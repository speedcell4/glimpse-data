{
  "https://openreview.net/forum?id=ALCpkWP8jw": {
    "title": "A Gaussian matrix graphical encoder in sports medicine diagnosis combining structured and unstructured data",
    "volume": "review",
    "abstract": "We study the integration of Electronic Medical Records (EMRs) from clinical study into a joint predictive model. Compared to the totally black-box models, a competitive model with explainable structure is much more desirable. To tackle this challenge, this paper introduces a novel Gaussian Matrix Graphical Encoder(GMGE) based on matrix normal graphical model to encode unstructured medical text and simultaneously learn the underlying conditional dependency graph of concepts. We further present DiMES, a Diagnostic Model with Explainable Structure, which integrates the concept graph generated by GMGE with structured data such as patient's physical examination measures. Utilizing Graph Convolutional Networks (GCNs), DiMES encodes patient features based on the concept graph for downstream tasks, providing clinicians with accurate predictive information to assist in diagnostic decisions and treatment plan design. The effectiveness of the proposed DiMES is validated through its application on four downstream diagnostic predictive tasks(ACL, PCL, MMI and PS)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Umh1zgd1pc": {
    "title": "Rethinking Pragmatics in Large Language Models: Towards Open-Ended Evaluation and Preference Tuning",
    "volume": "review",
    "abstract": "This study addresses the challenges of assessing and enhancing social-pragmatic inference in large language models (LLMs). We first highlight the inadequacy of current accuracy-based multiple choice question answering (MCQA) formats in assessing social-pragmatic reasoning, and propose the direct evaluation of models' free-form responses as measure, which - as our results show - correlates better with human judgement. Further, we explore the enhancement of pragmatic abilities in LLMs, proposing the use of preference optimization (PO) over supervised finetuning (SFT) since there's no ``gold'' answer in responding to a social situation. Our results indicate that preferential tuning significantly outperforms and proves more robust than SFT across pragmatic phenomena, and offers a near-free launch to enhance models' pragmatic ability without compromising generic abilities. Lastly, we delve into LLMs' internal space and demonstrate that the substantial boost of the model's pragmatic reasoning capabilities is linked to deeper layer representation, mirroring human's high-level thinking. Our experiments span multiple pragmatic and social reasoning data sources, covering diverse phenomena, as well as a image referential game requiring multimodal theory of mind (ToM). With our refined paradigms for evaluating and enhancing pragmatic inference, this paper offers key insights for developing more socially aware language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJqAmdfCPs": {
    "title": "Bayelemabaga: Creating Resources for Bambara NLP",
    "volume": "review",
    "abstract": "In low-resource settings, the problem is often not only the amount of data available, but also the quality, and in ways that are entirely foreign to high-resourced languages. For instance, many extreme low-resource languages have only recently acquired writing systems. This may result in multiple writing systems competing for dominance or, within a single writing system, non-standardized spelling. Translating to and from low-resource languages is a challenge for machine translation (MT) systems due to a lack of suitable parallel data. In this case study, we focus on the impact of manual data cleaning on the performance of learning machine translation models. We focus on Bambara, the vehicular language of Mali, and introduce the largest curated dataset for multilingual translation. We finetune six commonly used transformer-based language models, i.e., AfriMBART, AfriMT5, AfriM2M100, Mistral, Open-Llama-7B, and Meta-Llama3-8B on three existing Bambara-French language pair datasets and our curated dataset. We show that our new aligned and curated multilingual dataset enhances the translation quality of all studied models using the BLEU, CHRF++, and AfriCOMET evaluation metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E2jDWy4rIu": {
    "title": "Visualizing Entity States in Recipes by Generating Step Images",
    "volume": "review",
    "abstract": "Procedural texts, such as recipes and instruction manuals, are crucial for understanding processes involving multiple entities over time. Entity state tracking, which monitors the states of specific entities at each time step, is a key task in this domain. However, existing benchmarks heavily rely on manually annotated datasets, limiting scalability. We propose a novel task of step image generation in recipes, using step images as visual supervision for tracking entity states in procedural text without relying on manually annotated data. By generating step images, we can visualize the entity states in each step. For this task, we collect high-quality multimodal recipe datasets, theSpruceEats. Addressing the limitation of existing two-stage methods in achieving deep interaction between text and image, this paper introduces an explicit state modeling approach based on multimodal generative models. Experiments on theSpruceEats dataset demonstrate that our method enhance entity state tracking and image generation quality compared to existing methods, improving the CLIP similarity metric by 10.2% compared to existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EmWhceeJx9": {
    "title": "Arc Representation for Graph-based Dependency Parsing",
    "volume": "review",
    "abstract": "In this paper, we address the explicit representation of arcs in graph-based syntactic dependency parsing, departing from conventional approaches where parsing algorithms compute dependency arc scores directly from input token representations. We propose to augment the parser with an intermediate arc representation, arguing for two main advantages. Firstly, arc vectors encapsulate richer information, improving the capabilities of scoring functions. Secondly, by introducing refinement layers, we allow interactions between arc representations, facilitating interactions between arcs. We demonstrate the efficacy of this approach through evaluations on PTB and UD treebanks. Our approach achieves an LAS error rate reduction of 1.0\\% on the PTB test set, and 1.7\\% on UD, over the best SOTA model",
    "checked": false,
    "id": "b112a4a27c7e97229f9c12372efb7be047e10ff0",
    "semantic_title": "auxiliary tasks to boost biaffine semantic dependency parsing",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ivJRLEXIPs": {
    "title": "Refract ICL: Rethinking Example Selection in the Era of Million-Token Models",
    "volume": "review",
    "abstract": "The emergence of long-context large language models (LLMs) has enabled the use of hundreds, or even thousands, of demonstrations for in-context learning (ICL) â€“ a previously impractical regime. This paper investigates whether traditional ICL selection strategies, which balance the similarity of ICL examples to the test input (using a text retriever) with diversity within the ICL set, remain effective when utilizing a large number of demonstrations. Our experiments demonstrate that, while longer contexts can accommodate more examples, simply increasing the number of demonstrations does not guarantee improved performance. Smart ICL selection remains crucial, even with thousands of demonstrations. To further enhance ICL in this setting, we introduce Refract ICL, a novel ICL selection algorithm specifically designed to focus LLM attention on challenging examples by strategically repeating them within the context and incorporating zero-shot predictions as error signals. Our results show that Refract ICL significantly improves the performance of extremely long-context models such as Gemini 1.5 Pro, particularly on tasks with a smaller number of output classes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JPwP3EpeCM": {
    "title": "Dual-Phase Accelerated Prompt Optimization",
    "volume": "review",
    "abstract": "Gradient-free prompt optimization methods have made significant strides in enhancing the performance of closed-source Large Language Model (LLMs) across a wide range of tasks. However, existing approaches make light of the importance of high-quality prompt initialization and the identification of effective optimization directions, thus resulting in substantial optimization steps to obtain satisfactory performance. In this light, we aim to accelerate prompt optimization process to tackle the challenge of low convergence rate. We propose a dual-phase approach which starts with generating high-quality initial prompts by adopting a well-designed meta-instruction to delve into task-specific information, and iteratively optimize the prompts at the sentence level, leveraging previous tuning experience to expand prompt candidates and accept effective ones. Extensive experiments on eight datasets demonstrate the effectiveness of our proposed method, achieving a consistent accuracy gain over baselines with less than five optimization steps",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RBHRmQAvJ9": {
    "title": "Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions",
    "volume": "review",
    "abstract": "Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce: (i) a general ontology of perturbations for maths and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, MORE and CORE, respectively, of perturbed maths and coding problems to probe LLM capabilities in numeric reasoning and coding tasks. Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust problem solving skills and structured reasoning abilities in many areas, as defined by our ontology",
    "checked": true,
    "id": "1a1063e0e2707d8c540d88a49084bf683477c57e",
    "semantic_title": "evaluating llms' mathematical and coding competency through ontology-guided interventions",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=NyZtxWIj8L": {
    "title": "Vanilla Transformers are Transfer Capability Teachers",
    "volume": "review",
    "abstract": "Recently, Mixture of Experts (MoE) Transformers have garnered increasing attention due to their advantages in model capacity and computational efficiency. However, studies have indicated that MoE Transformers underperform vanilla Transformers in many downstream tasks, significantly diminishing the practical value of MoE models. To explain this issue, we propose that the pre-training performance and transfer capability of a model are joint determinants of its downstream task performance. MoE models, in comparison to vanilla models, have poorer transfer capability, leading to their subpar performance in downstream tasks. To address this issue, we introduce the concept of transfer capability distillation, positing that although vanilla models have weaker performance, they are effective teachers of transfer capability. The MoE models guided by vanilla models can achieve both strong pre-training performance and transfer capability, ultimately enhancing their performance in downstream tasks. We design a specific distillation method and conduct experiments on the BERT architecture. Experimental results show a significant improvement in downstream performance of MoE models, and many further evidences also strongly support the concept of transfer capability distillation. Finally, we attempt to interpret transfer capability distillation and provide some insights from the perspective of model feature",
    "checked": true,
    "id": "ecc547edb0e38b25e60a4200ec46c59d5947a064",
    "semantic_title": "vanilla transformers are transfer capability teachers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5uGSREmvi0": {
    "title": "MCQFormatBench: Robustness Tests for Multiple-Choice Questions",
    "volume": "review",
    "abstract": "Multiple-choice questions (MCQs) are often used to evaluate large language models (LLMs). They measure LLMs' general common sense and reasoning abilities, as well as their knowledge in specific domains such as medicine. However, the robustness of LLMs to a variety of question formats in MCQs has not been thoroughly evaluated. While there are studies on the sensitivity of LLMs to input variations, research into their responsiveness to different question formats is still limited. Therefore, in this study, we propose a method to construct tasks to comprehensively evaluate the robustness against format changes of MCQs by decomposing the answering process into several steps. Using this dataset, we evaluate six LLMs, such as Llama3-70B and Mixtral-8x7B. Consequently, the lack of robustness to differences in the format of MCQs becomes evident. It is crucial to consider whether the format of MCQs influences their evaluation scores when assessing LLMs using MCQ datasets",
    "checked": false,
    "id": "570e4fec8c8f1c96b76accbb07d40e0528aafb4a",
    "semantic_title": "large language models are not robust multiple choice selectors",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=2nPRjjdLyT": {
    "title": "Causal Inference in Large Language Model: A Survey",
    "volume": "review",
    "abstract": "Causal inference has been a pivotal challenge across diverse domains such as medicine and economics, demanding a complicated integration of human knowledge, numerical reasoning, and data processing capabilities. Recent advancements in natural language processing (NLP), particularly with the advent of large language models (LLMs), have introduced transformative opportunities for traditional causal inference tasks. This paper reviews recent progress in applying LLMs to causal inference, encompassing various tasks spanning different levels of causation. We summarize their causal problems, methodologies, and present comparison of their evaluation results in different scenarios. Furthermore, we discuss key findings, emerging trends, and outline directions for future research, underscoring the potential implications of integrating LLMs in advancing causal inference methodologies",
    "checked": false,
    "id": "94a3653fa6f468daf8e4a85f90525e76921b583b",
    "semantic_title": "causal inference with latent variables: recent advances and future prospectives",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y1tPrJWYFE": {
    "title": "NPC: Personalized Next Profile Crafting Using Previous Role-Play Bots",
    "volume": "review",
    "abstract": "Personalization and role-playing are two important research topics for the LLM community. However, the exploration in the direction of personalized role-playing especially rare. The primary obstacle in personalized role-playing is the absence of a dataset containing role-playing dialogues with personalized information. To overcome this obstacle, we introduce a new large-scale personalized role-playing dataset Multi-Bot Tailored Interaction Dataset (MBTI), which includes the entire interaction history from creating bot to deeply engaged conversation between 1238 users and 8477 Bots. More importantly, we propose a new pipeline called \"Next Profile Crafting (NPC)\" for crafting role profiles with cross-bot insights to achieve personalization before the conversation. This method is based on the bot persona link among historical bots that user has multi-turn interaction with. We conducted tests using both trained and untrained approaches, as well as open-source and proprietary large language models, highlighting significant disparities in the effectiveness of personalized crafting in the NPC task. Our findings indicate substantial room for improvement in current methodologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VkBZ42ouIB": {
    "title": "QReT: Quality Aware Token Count Reduction",
    "volume": "review",
    "abstract": "LLMs are widely used nowadays by several enterprises for various use cases. This is due to their general applicability and demonstrated success across multiple domains and tasks. However, there is a monetary cost associated with the use of commercially available inference APIs to LLMs. This cost generally depends on the number of input and output tokens and the cost parameters of the provider. In this work, we propose a framework QReT for reducing the input token count in prompts in a controllable quality aware manner. QReT first paraphrases the prompt to reduce token counts while maintaining quality measures. Secondly, it applies certain heuristics, again a controlled manner to reduce the final token count, without affecting the understanding by LLMs (hence, the output quality). We empirically validate QReT across several datasets and tasks and show its effectiveness",
    "checked": false,
    "id": "03f6e2a8d55c8d56ec3da47f5ed450d16332ba36",
    "semantic_title": "library aware power conscious realization of complementary boolean functions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DwRLo6toLN": {
    "title": "Defending Jailbreak Prompts via In-Context Adversarial Game",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Moreover, ICAG demonstrates remarkable transferability to other LLMs, indicating its potential as a versatile defense mechanism",
    "checked": true,
    "id": "50ceabc6aa41e08480fa5976342bfe04bb47bce3",
    "semantic_title": "defending jailbreak prompts via in-context adversarial game",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=40xgCdANT4": {
    "title": "Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace",
    "volume": "review",
    "abstract": "Instruction tuning is a burgeoning method to elicit the general intelligence of Large Language Models (LLMs). However, the understanding of its scaling properties remains underexplored. While some research advocates for expanding the number of instructions, others suggest that a small set of well-chosen examples is adequate. To understand such discrepancy, our work systematically studies the effectiveness of data volume, parameter size, and data construction methods on the development of each underlying ability of LLM, such as creative writing, code generation, and logical reasoning. Our study reveals three primary findings: (i) Despite these factors significantly influencing overall model performance, some abilities are more responsive to scaling, while others show high resistance. (ii) The sensitivity of different abilities to these factors can be explained by their Complexity and Transference, which indicate the relative importance of each factor in learning specific abilities. (iii) Tailoring data construction based on these sensitivities results in performance gains on two public benchmarks. Additionally, we curate a comprehensive dataset containing over 40k instances across ten abilities for our experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pqIpMxy70A": {
    "title": "Reward Modeling Requires Automatic Adjustment Based on Data Quality",
    "volume": "review",
    "abstract": "In Reinforcement Learning from Human Feedback (RLHF), the reward model plays a crucial role in aligning language model outputs with human values. The human preference data used to train the reward model consists of a prompt and a response pair, with humans annotating which response better aligns with human value preferences. Due to the complexity and subjectivity of the annotation task, multiple organizations including OpenAI and Anthropic report significant noise in the human preference datasets, leading to instability and deviation in reward model training from human values. We discover that the difference in scores assigned to response pairs by the reward model effectively indicates the quality of data, and data of varying qualities show significant distinctions in reward model training. We introduce a method that automatically adjusts reward modeling based on data quality, reducing the impact of noise and making full use of dataset. Experiments on multiple human preference datasets demonstrate that our method stabilizes reward model training and significantly enhances the alignment performance of RLHF",
    "checked": false,
    "id": "05d8876cba1208490b1e242becce321a45f511bb",
    "semantic_title": "gedss: a generic framework to enhance model robustness for intrusion detection on noisy data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wqs9Beej1n": {
    "title": "Gradient Localization Improves Lifelong Pretraining of Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) trained on web-scale text corpora have been shown to capture world knowledge in their parameters. However, the mechanism by which language models store different types of knowledge is poorly understood. In this work, we examine two types of knowledge relating to temporally sensitive entities and demonstrate that each type is localized to different sets of parameters within the LLMs. We hypothesize that the lack of consideration of the locality of knowledge in existing continual learning methods is responsible for failed uptake of new information and catastrophic forgetting of previously learned information. We demonstrate that targeted training to these relevant layers can improve the performance of continually learned language under temporal drift",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7SlzDFnejn": {
    "title": "Expanding Search Space with Diverse Prompting Agents: An Efficient Sampling Approach for LLM Mathematical Reasoning",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have exhibited remarkable capabilities in many complex tasks including mathematical reasoning. However, traditional approaches heavily rely on ensuring self-consistency within single prompting method, which limits the exploration of diverse problem-solving strategies. This study addresses these limitations by performing an experimental analysis of distinct prompting methods within the domain of mathematical reasoning. Our findings demonstrate that each method explores a distinct search space, and this differentiation becomes more evident with increasing problem complexity. To leverage this phenomenon, we applied efficient sampling process that uniformly combines samples from these diverse methods, which not only expands the maximum search space but achieves higher performance with fewer runs compared to single methods. Especially, within the subset of difficult questions of MATH dataset named MATH-hard, The maximum search space was achieved while utilizing approximately 43% fewer runs than single methods on average. These findings highlight the importance of integrating diverse problem-solving strategies to enhance the reasoning abilities of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J9vbZnEoxi": {
    "title": "Coarse and Fine-grained Confidence Calibration of LLM-based Text-to-SQL Generation",
    "volume": "review",
    "abstract": "Calibration plays a crucial role as LLMs are increasingly deployed to convert natural language questions into SQL over commercial databases. In this work, we study the calibration of the confidence attached to both the whole query, and for the first time, to sub-parts of the query. For whole queries, we demonstrate that the simple baseline of deriving confidence from model assigned whole sequence probability yields the best calibration surpassing recent self-check and verbalization methods. For fine-grained calibration, we propose a novel method of assigning confidence to nodes of a logical relational algebra tree representation of the SQL string. We present an extensive comparison spanning two popular Text-to-SQL benchmarks on multiple LLMs, and draw interesting insights about various calibration methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WoBLg8E8Sm": {
    "title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation",
    "volume": "review",
    "abstract": "The training process of large language models (LLMs) often involves varying degrees of test data contamination. Although current LLMs are achieving increasingly better performance on various benchmarks, their performance in practical applications does not always match their benchmark results. Leakage of benchmarks can prevent the accurate assessment of LLMs' true performance. However, constructing new benchmarks is costly, labor-intensive and still carries the risk of leakage. Therefore, in this paper, we ask the question Can we reuse these leaked benchmarks for LLM evaluation? We propose Inference-Time Decontamination (ITD) to address this issue by detecting and rewriting leaked samples without altering their difficulties. ITD can mitigate performance inflation caused by memorizing leaked benchmarks. Our proof-of-concept experiments demonstrate that ITD reduces inflated accuracy by 22.9\\% on GSM8K and 19.0\\% on MMLU. On MMLU, using Inference-time Decontamination can lead to a decrease in the results of Phi3 and Mistral by 6.7\\% and 3.6\\% respectively. We hope that ITD can provide more truthful evaluation results for large language models",
    "checked": true,
    "id": "a1d29e4da609746b0c927ef141f31445e769ec3c",
    "semantic_title": "inference-time decontamination: reusing leaked benchmarks for large language model evaluation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OfEDtLFtYh": {
    "title": "To Stay or Not to Stay: Insights on Factors in Role-playing Dialogs",
    "volume": "review",
    "abstract": "With the growing humanlike nature of dialog agents, people are now engaging in extended conversations that can stretch from brief moments to substantial periods of time. Understanding the factors that contribute to sustaining these interactions is crucial, yet existing studies primarily focusing on short-term simulations that rarely explore such prolonged and real conversations. In this paper, we investigate the factors influencing retention rates in real interactions with role-playing models. By analyzing a large dataset of interactions between real users and thousands of characters, we systematically examine multiple factors and assess their impact on user retention rate. Surprisingly, we find that the degree to which the bot embodies the roles it plays has limited influence on retention rates, while the length of each turn it speaks significantly affects retention rates. This study sheds light on the critical aspects of user engagement with role-playing models and provides valuable insights for future improvements in the development of large language models for role-playing purposes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WL0TCM2ijw": {
    "title": "Solving for X and Beyond: Can Large Language Models Solve Complex Math Problems with More-Than-Two Unknowns?",
    "volume": "review",
    "abstract": "Large Language Models have demonstrates remarkable performance in solving math problems, a hallmark of human intelligence. Despite high success rates on current benchmarks, however, these often feature simple problems with only one or two unknowns, which do not sufficiently challenge their reasoning capacities. This paper introduces a novel benchmark, BeyondX, designed to address these limitations by incorporating problems with multiple unknowns. Recognizing the challenges in proposing multi-unknown problems from scratch, we developed BeyondX using an innovative automated pipeline that progressively increases complexity by expanding the number of unknowns in simpler problems. Empirical study on BeyondX reveals that the performance of existing LLMs, even those fine-tuned specifically on math tasks, significantly decreases as the number of unknowns increases - with a performance drop of up to 70% observed in GPT-4. To tackle these challenges, we propose the Formulate-and-Solve strategy, a generalized prompting approach that effectively handles problems with an arbitrary number of unknowns. Our findings reveal that this strategy not only enhances LLM performance on the BeyondX benchmark but also provides deeper insights into the computational limits of LLMs when faced with more complex mathematical challenges",
    "checked": true,
    "id": "432b119c85f21462784f5be5d3b48b03ec423294",
    "semantic_title": "solving for x and beyond: can large language models solve complex math problems with more-than-two unknowns?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YBswWaS21w": {
    "title": "Retrieval-DPO: Retrieval-augmented preference optimization with non-paired preference data",
    "volume": "review",
    "abstract": "Aligning Large Language Models (LLMs) with human feedback is important and challenging. \\citet{rafailov2023direct} propose Direct Preference Optimization (DPO), a simple but effective alignment method which is reinforcement learning free. However, DPO requires paired preference data which is harder and more expensive to obtain compared to binary preference data. We propose a retrieval-based method named Retrieval-DPO to align LLMs under binary preference data situation. The core idea of our method is that learning how to align can be achieved with non-paired preference data of similar questions rather than strictly paired preference data considering the learning process of human. For instance, to teach the LLM to learn how to treat multiple perspectives, other comprehensive golden answers of similar question may have similar positive effects as the golden answer of the same question. Following this idea, we retrieve an example with opposite label from the retrieval database for a binary preference data in the training set. After the retrieval process, we get a pair of preference data but with possibly different questions and then adopt the DOVE \\cite{bansal2024comparing} optimization objective for the alignment. We compare Retrieval-DPO with other preference optimization algorithms which do not need paired preference data such as Kahneman-Tversky Optimization (KTO) and Unified Language Model Alignment (ULMA). We find that our method significantly outperforms KTO and ULMA on helpful-base subset of HH dataset (over 13\\%) and slightly outperforms KTO on harmless-base subset of HH dataset and controlled sentiment generation task. Besides, our method is not sensitive to the ratio of the number of positive examples to the number of negative examples without additional hyperparameter tuning",
    "checked": false,
    "id": "fc24091a9b44953c71084449b4fa0eabfb58abb3",
    "semantic_title": "knowpo: knowledge-aware preference optimization for controllable knowledge selection in retrieval-augmented language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PqH6FU8HOj": {
    "title": "On the Client Preference of LLM Fine-tuning in Federated Learning",
    "volume": "review",
    "abstract": "Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained large language model (LLM) using preference datasets, enabling the LLM to generate outputs that align with human preferences. Given the sensitive nature of these preference datasets held by various clients, there is a need to implement RLHF within a federated learning (FL) framework, where clients are reluctant to share their data due to privacy concerns. To address this, we introduce a feasible framework in which clients collaboratively train a binary selector with their preference datasets using our proposed FedBis. With a well-trained selector, we can further enhance the LLM that generates human-preferred completions. Meanwhile, we propose a novel algorithm, FedBiscuit, that trains multiple selectors by organizing clients into balanced and disjoint clusters based on their preferences. Compared to the FedBis, FedBisuit demonstrates superior performance in simulating human preferences for pairwise completions. Our extensive experiments on federated human preference datasets -- marking the first benchmark to address heterogeneous data partitioning among clients -- demonstrate that FedBisuit outperforms FedBis and even surpasses traditional centralized training",
    "checked": true,
    "id": "fc73b75cfc94f4955daf23cb4f6954351dfa76f7",
    "semantic_title": "on the client preference of llm fine-tuning in federated learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CiU8jOrPPD": {
    "title": "ModeLing: A Novel Dataset for Testing Linguistic Reasoning in Language Models",
    "volume": "review",
    "abstract": "We introduce ModeLing, a novel benchmark of Linguistics Olympiad-style puzzles which tests few-shot reasoning in AI systems. Solving these puzzles necessitates inferring aspects of a language's grammatical structure from a small number of examples. Such puzzles provide a natural testbed for language models, as they require compositional generalization and few-shot inductive reasoning. Consisting solely of new puzzles written specifically for this work, ModeLing has no risk of appearing in the training data of existing AI systems: this ameliorates the risk of data leakage, a potential confounder for many prior evaluations of reasoning. Evaluating several large open source language models and GPT on our benchmark, we observe non-negligible accuracy, demonstrating few-shot emergent reasoning ability which cannot merely be attributed to shallow memorization. However, imperfect model performance suggests that ModeLing can be used to measure further progress in linguistic reasoning",
    "checked": true,
    "id": "e35c5f517e547bde2ae4d77a535d3e5584805f02",
    "semantic_title": "modeling: a novel dataset for testing linguistic reasoning in language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=8bEuhpV2vH": {
    "title": "Enhancing Emotion Recognition in Conversations through Global Context: An Empirical Analysis",
    "volume": "review",
    "abstract": "According to multimodal and contextualized nature of the human conversation, correctly identifying an emotion for given utterance in the conversation has always been a challenging task. Recent research benefits from Graph Neural Networks by capturing implicit relationship of temporally proximate utterances. In this paper, we expand the structure of the graph exploited by these models reflecting the global context of the conversation and explore how leveraging conversational context and interactions can lead to more accurate emotion recognition. We empirically analyze the modules on Emotion Recognition in Conversation models, showing this approach enhances the performance of these models. Our experiments show that incorporating global conversational context has a positive effect on the performance of emotion recognition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qlts1JVWpX": {
    "title": "AntiSemRO: Studying the Romanian expression of Antisemitism",
    "volume": "review",
    "abstract": "With far-right ideology rising in popularity, online environment embodies hateful attitudes. The Covid-19 pandemic and the violent wars in Ukraine and Palestine contributed to a growth in antisemitic discourse. This study introduces an annotated dataset for the study of antisemitic hate speech in Romanian along with several baseline models using classical machine learning models and transformer models for the classification of antisemitic discourse in the online medium",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ymuD7hlrvu": {
    "title": "REPOPILOT: Software Agents To Resolve Software Engineering Tasks at Repository-Level Scale",
    "volume": "review",
    "abstract": "Coding assistants based on Large Language Models (LLMs) have recently surged in popularity. A significant challenge for LLMs is accurately responding to user queries at the scale of entire code repositories. We propose RepoPilot, a multi-agent-based system capable of effectively navigating through source code repositories to collect relevant information, editing code and execute programs. We demonstrate the effectiveness of RepoPilot through extensive evaluations on challenging benchmarks, including SWE-bench and an automatically collected code generation dataset. On SWE-bench Lite, RepoPilot achieves a 17\\% pass rate, establishing competitive results compared to the baseline while maintains low cost and also excels in other code intelligence tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dIkMv4eZQD": {
    "title": "Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias",
    "volume": "review",
    "abstract": "The rapid growth of Large Language Models (LLMs) has put forward the study of biases as a crucial field. It is important to assess the influence of different types of biases embedded in LLMs to ensure fair use in sensitive fields. Although there have been extensive works on bias assessment in English, such efforts are rare and scarce for a major language like Bangla. In this work, we examine two types of social biases in LLM generated outputs for Bangla language. Our main contributions in this work are: (1) bias studies on two different social biases for Bangla (2) a curated dataset for bias measurement benchmarking (3) two different probing techniques for bias detection in the context of Bangla. This is the first work of such kind involving bias assessment of LLMs for Bangla to the best of our knowledge. All our code and resources will be made publicly available for the progress of bias related research in Bangla NLP",
    "checked": true,
    "id": "b6d4eea463fc1717cbc45b4ac3ae55bddfb5090d",
    "semantic_title": "social bias in large language models for bangla: an empirical study on gender and religious bias",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=E8pNK3YAdO": {
    "title": "Hint-before-Solving: A Framework to Effectively Utilizing Inherent Knowledge of Large Language Model",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have recently showcased remarkable generalizability in various domains. Despite their extensive knowledge, LLMs still face challenges in efficiently utilizing encoded knowledge to develop accurate and logical reasoning. To mitigate this problem, we introduced the Hint-before-Solving framework (HinSo), which guides the model in generating hints (e.g., specific knowledge or key ideas) for solving the problem before the step-by-step solution. Our studies involving 5 LLMs across 7 datasets of mathematical and commonsense reasoning results indicated that introducing hints before problem-solving can significantly enhance the performance of CoT. To investigate whether LLMs can learn the HinSo pattern and improve their generalization ability, we constructed two large-scale and high-quality training datasets, HST-S and HST-L, containing 7.5k and 75k samples, respectively. The experimental results of supervised fine-tuning (SFT) showed that, under the same settings, the performance of model trained on the HinSo-formatted data improved significantly compared to CoT-formatted data, with a performance increase of 5.1% and 5.6% on the GSM8K, respectively. We make our code and dataset publicly available at \\url{https://github.com/sfhff216/hsp}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZI99Mbqyge": {
    "title": "Are Expert-Level Language Models Expert-Level Annotators?",
    "volume": "review",
    "abstract": "Data annotation refers to the labeling or tagging of textual data with relevant information. A large body of works have reported positive results on leveraging LLMs as an alternative to human annotators. However, existing studies focus on classic NLP tasks, and the extent to which LLMs as data annotators perform in domains requiring expert knowledge remains underexplored. In this work, we investigate comprehensive approaches across three highly specialized domains and discuss practical suggestions from a cost-effectiveness perspective. To the best of our knowledge, we present the first systematic evaluation of LLMs as expert-level data annotators",
    "checked": false,
    "id": "1eb1a8c7f88de27af224153f43ecdd41774600f2",
    "semantic_title": "promptagent: strategic planning with language models enables expert-level prompt optimization",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=S7he4lxunu": {
    "title": "Understanding the Relationship between Prompts and Response Uncertainty in Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) are widely used in decision-making, but their reliability, especially in critical tasks like healthcare, is not well-established. Therefore, understanding how LLMs reason and make decisions is crucial for their safe deployment. This paper investigates how the uncertainty of responses generated by LLMs relates to the information provided in the input prompt. Leveraging the insight that LLMs learn to infer latent concepts during pretraining, we propose a prompt-response concept model that explains how LLMs generate responses and helps understand the relationship between prompts and response uncertainty. We show that the uncertainty decreases as the prompt's informativeness increases, similar to epistemic uncertainty. Our detailed experimental results on real datasets validate our proposed model",
    "checked": false,
    "id": "d4838211d7f65628f56b9f6faab30a95ff7b51f8",
    "semantic_title": "for prediction city region re-weighting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wf3f7Fz3C7": {
    "title": "CodeGRAG: Bridging the Gap between Natural Language and Programming Language via Graphical Retrieval Augmented Generation",
    "volume": "review",
    "abstract": "Utilizing large language models to generate codes has shown promising meaning in software development revolution. Despite the intelligence shown by the general large language models, their specificity in code generation can still be improved due to the syntactic gap and mismatched vocabulary existing among natural language and different programming languages. In this paper, we propose CodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance the performance of LLMs. CodeGRAG builds the graphical view of code blocks based on the control flow and data flow of them to fill the gap between programming languages and natural language, which can facilitate natural language based LLMs for better understanding of code syntax and serve as a bridge among different programming languages. To take the extracted structural knowledge into the foundation models, we propose 1) a hard meta-graph prompt template to transform the challenging graphical representation into informative knowledge for tuning-free models and 2) a soft prompting technique that injects the domain knowledge of programming languages into the model parameters via finetuning the models with the help of a pretrained GNN expert model. CodeGRAG significantly improves the code generation ability of LLMs and can even offer performance gain for cross-lingual code generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WfHQCaUhi6": {
    "title": "VLV-Bench: A Comprehensive benchmark for very long-form videos understanding",
    "volume": "review",
    "abstract": "Understanding long videos, ranging from tens of minutes to several hours, presents unique challenges in video comprehension. Despite the increasing importance of long-form video content, existing benchmarks primarily focus on shorter clips. To address this gap, we introduce a comprehensive benchmark for Very Long Videos understanding (VLV-Bench), which presents 1) The longest video duration, averaging 76.34 minutes; 2) The largest number of question-answer pairs, 108.2K; 3) Diversity in questions that examine nine different skills and include both multiple-choice questions and open-ended questions; 4) Human-centric, as the video sources come from movies and daily TV shows, with specific human-level question designs such as Movie Spoiler Questions that require critical thinking and comprehensive understanding. Using VLV-Bench, we comprehensively evaluate existing Large Multi-Modality Models (LMMs) on each skill, including the commercial model Gemini 1.5 Flash and the open-source models. The evaluation shows significant challenges in our benchmark. Our results show that the best AI models such Gemini struggles to perform well with 42.72 % average accuracy and 2.71 out of 5 average score. We hope this benchmark will stimulate the LMMs community towards long video and human-level understanding. Our benchmark can be accessed at (https://vlv-bench.github.io/VLV-website/) and will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vUGoG1TM0x": {
    "title": "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension",
    "volume": "review",
    "abstract": "Referring Expression Comprehension (REC) is a crucial cross-modal task that objectively evaluates the capabilities of language understanding, image comprehension, and language-to-image grounding. Consequently, it serves as an ideal testing ground for Multi-modal Large Language Models (MLLMs). In pursuit of this goal, we have established a new REC dataset characterized by two key features: Firstly, it is designed with controllable varying levels of difficulty, necessitating multi-level fine-grained reasoning across object categories, attributes, and multi-hop relationships. Secondly, it includes negative text and images created through fine-grained editing and generation based on existing data, thereby testing the model's ability to correctly reject scenarios where the target object is not visible in the imageâ€”an essential aspect often overlooked in existing datasets and approaches. Utilizing this high-quality dataset, we conducted comprehensive evaluations of both state-of-the-art specialist models and MLLMs. Our findings indicate that there remains a significant gap in achieving satisfactory grounding performance. We anticipate that our dataset will inspire new approaches to enhance visual reasoning and develop more advanced cross-modal interaction strategies, ultimately unlocking the full potential of MLLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3p8FbDD1uI": {
    "title": "Evaluating LLMs' capability on Satisfying Lexical Constraint",
    "volume": "review",
    "abstract": "Lexical Constrained Generation (LCG) is a fundamental task in text generation. Recent advancement of large pretrained language models (LLMs) has enabled prompt-based controlling for LCG. Despite growing interest in assessing LLMs' capabilities in various aspects, there remains a lack of thorough investigation. To address this gap, we systematically analyze the performance of LLMs on satisfying lexical constraints with prompt-based controlling, as well as their efficacy in downstream applications (such as recipe generation, table-to-text, profile writing, etc). Through extensive experimentation, we identified several key observations that elucidate the limitations of LLMs in LCG, including (1) position bias, where LLMs tend to satisfy constraints that appear in specific positions within the input; (2) insensitive decoding parameters, which minimally impact the performance of LLMs; and (3) the inherent complexity of certain constraints (i.e. compound word). We conclude that there is a complexity bottleneck: LLMs still face significant challenges in consistently satisfying lexical constraints. Additionally, we introduce the Divide and Conquer Generation strategy, effective for both white-box and black-box LLMs, significantly enhancing their performance in LCG tasks. This strategy boosts LLMs' success rate by 93% in the most challenging LCG task, which is 40% more than the baseline. Our analysis aims to provide valuable insights into the performance of LLMs in LCG, and our proposed strategy offers a pathway to more sophisticated and customized text generation applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S3lYBpVDkQ": {
    "title": "ALERTS: Active Learning and Ensemble LLM Real-Time Switch for Real-World Data Drift Challenges",
    "volume": "review",
    "abstract": "In the rapidly changing real-world scenarios, data drift and ``cold-start'' issues present significant challenges for the development of machine learning models, along with the high cost and resource scarcity of domain experts. Traditional compact models fine-tuned on small number of domain-specific examples often outperform generic LLMs, despite the fine-tuned models struggling with rapid data changes. This study introduces ALERTS, an ensemble system designed to address these data challenges. The system comprises 1) an LLM to enhance early-stage performance and adapt to sudden data drifts, 2) an Active Learning (AL)-assisted compact model iteratively fine-tuned on annotations from daily human expert workflows, and 3) a switch mechanism that evaluates both models in real-time and selects the best-performing ones. We conducted empirical studies to understand the performance between LLMs and AL-assisted compact models, then evaluated our system's effectiveness through AL simulations of real-world scenarios. Our work offers a novel framework for developing robust language model systems across various dynamic real-world scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b1sX4irpzf": {
    "title": "Self-Harmonized Chain of Thought",
    "volume": "review",
    "abstract": "Chain-of-Thought (CoT) prompting reveals that large language models are capable of performing complex reasoning via intermediate steps. CoT prompting is primarily categorized into three approaches. The first approach utilizes straightforward prompts like ``Let's think step by step'' to generate a sequential thought process before yielding an answer. The second approach makes use of human-crafted, step-by-step demonstrations to guide the model's reasoning process. The third automates the generation of reasoned demonstrations with the 'Let's think step by step'.This approach sometimes leads to reasoning errors, highlighting the need to diversify demonstrations to mitigate its misleading effects. However, diverse demonstrations pose challenges for effective representations. In this work, we propose ECHO, a self-harmonized chain-of-thought prompting method. It consolidates diverse solution paths into a uniform and effective solution pattern.ECHO demonstrates the best overall performance across three reasoning domains",
    "checked": false,
    "id": "d36770f542facfa60896593ca02bb1ce680f0499",
    "semantic_title": "categorical matrix of vitacultural methodology: from thought-activity to canon",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=YG1TnDkc8O": {
    "title": "Query-based Cross-Modal Projector Bolstering Mamba Multimodal LLM",
    "volume": "review",
    "abstract": "The Transformer's quadratic complexity with input length imposes an unsustainable computational load on large language models (LLMs). In contrast, the Selective Scan Structured State-Space Model, or Mamba, addresses this computational challenge effectively. This paper explores a query-based cross-modal projector designed to bolster Mamba's efficiency for vision-language modeling by compressing visual tokens based on input through the cross-attention mechanism. This innovative projector also removes the need for manually designing the 2D scan order of original image features when converting them into an input sequence for Mamba LLM. Experimental results across various vision-language understanding benchmarks show that the proposed cross-modal projector enhances Mamba-based multimodal LLMs, boosting both performance and throughput",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zBJIOrpbPx": {
    "title": "Deciphering Multi-task Learning: Comparative Insights for Similar and Dissimilar Tasks",
    "volume": "review",
    "abstract": "Multi-task learning (MTL), which emerged as a powerful concept in the era of machine learning, employs a shared model trained to handle multiple tasks at the same time. Numerous advantages of this novel approach inspire us to investigate the insights of various tasks with similar (Identification of Sentiment, Emotion, Sarcasm, Irony, Hate and Offensive) and dissimilar (Identification of Sentiment, Claim, Language) genres and to analyze the change in their performances with respect to long and short head approaches. We shed light on the methods employed and critical observations to promote more efficient learning paradigm across similar and dissimilar tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0O7N7fTKGE": {
    "title": "Training Data Extraction Attack from Large Language Models in Federated Learning Through Frequent Sequence Mining",
    "volume": "review",
    "abstract": "Large language models (LLMs) are vulnerable to data extraction attacks due to their tendency to memorize precise training data. In contrast, Federated Learning (FL) has the potential to mitigate privacy leakage. This underscores the need for an assessment of the privacy risks associated with LLMs trained with FL algorithms, which remains an underexplored question. In this study, we evaluate the privacy leakage of LLMs trained with FL algorithms on the public datasets extended with automatically annotated Personally Identifiable Information (PII) to evaluate the leakage of PII and training example outputs. Through extensive experiments, we find out that FL algorithms indeed mitigate privacy leaks compared to their counterparts on centralized data. In addition, we discover a novel data extraction attack method, called cross-client security theft, which can recover up to 40\\% of unique PII mentions in target devices by accessing only one of the FL participants. These findings highlight the potential privacy risks of FL for LLMs and underscore the need to explore new protective mechanisms in future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n9dV9E7RVj": {
    "title": "LLM-Powered Multi-Agent Proactive Communication System for Embodied Intelligence",
    "volume": "review",
    "abstract": "We presents a novel multi-robot collaboration framework leveraging large language models (LLMs) for improved communication, planning, and execution. By integrating a centralized message pool and LLM-assisted decision-making, our system addresses limitations of existing multi-agent frameworks. Experiments in the MuJoCo simulation environment demonstrate significant improvements in task completion rates, communication effectiveness, and decision-making accuracy. Our proactive communication system reduces redundancy and enhances fault tolerance, enabling efficient handling of unexpected situations. Future work will focus on improving information synchronization and multi-system collaboration, further enhancing efficiency and scalability in complex environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AkIq6Mpx2G": {
    "title": "Human-Readable Representation for Graph Neural Networks",
    "volume": "review",
    "abstract": "This research presents an innovative method for representing nodes in graph neural networks (GNNs) using human-readable text in natural language, diverging from the traditional numerical embeddings. By employing a large language model (LLM) as a projector, we train GNNs to aggregate information from neighboring nodes and update node representations iteratively. Our experiments on the MovieLens dataset, widely used for recommendation tasks, demonstrate that human-readable representations effectively capture useful information for recommendations. This suggests that LLMs can successfully aggregate neighborhood information in a graph. Furthermore, fine-tuning the LLMs can improve their ability to generate more application-specific human-readable representations. This technique not only facilitates the incorporation of world knowledge into GNNs but also enhances their interpretability and allows for human intervention in their behavior. Our approach shows significant potential for making graph neural networks more understandable and controllable",
    "checked": false,
    "id": "67c0a7c64b62c2b6370c3788c685d00d610963d6",
    "semantic_title": "page: prototype-based model-level explanations for graph neural networks",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=GXkjWKYumX": {
    "title": "Visibility vs. Engagement: How Two Indian News Websites Reported on LGBTQ+ Individuals and Communities during the Pandemic",
    "volume": "review",
    "abstract": "Online news media outlets were an important source of information for people with digital access during the COVIDâ€“19 pandemic. In India, where \"transgender\" was legally recognised as a category only in 2014, and sameâ€“sex marriages are yet to be legalised, it becomes crucial to analyse whether and how news media reported the lived realities of vulnerable LGBTQ+ communities during the pandemic. This study analysed articles from online editions of two Englishâ€“language newspaper websites, which differed vastly in their circulation figuresâ€”The Times of India and The Indian Express. The results of our study suggest that these newspaper websites covered articles surrounding various aspects of the lives of LGBTQ+ individuals with a greater focus on transgender communities. However, they lacked quality and depth. Focusing on the period spanning March 2020 to August 2021, we analysed articles from The Times of India and The Indian Express using distilâ€“RoBERTaâ€“base and ChatGPTâ€“3.5 for sentiment analysis and BERTopic for topic modelling. We also compared our results to the period before the pandemic (January 2019â€“December 2019) to understand the shift in topics and sentiments across the two newspaper websites. Our topic modelling results indicate that The Times of India and The Indian Express primarily wrote soft news on LGBTQ+ communities. Similarly, our sentiment analysis results indicate a difference in prevailing sentiment as depicted by the two models. Furthermore, our manual analysis of the articles indicates that the language used in certain articles by The Times of India was transphobic and obsolete. Our study captures the visibility and representation of the LGBTQ+ communities in online Indian news media outlets, the language they use, and the narratives they follow",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=51Gq1Qw9vc": {
    "title": "CPTQuant - A Novel Mixed Precision Post-Training Quantization Techniques for Large Language Models",
    "volume": "review",
    "abstract": "Large language models have transformed the comprehension and generation of natural language tasks, but they come with substantial memory and computational requirements. Quantization techniques have emerged as a promising avenue for addressing these challenges while preserving accuracy and making energy efficient. We propose CPTQuant, a comprehensive strategy that introduces correlation-based (CMPQ), pruning-based (PMPQ), and Taylor decomposition-based (TDMPQ) mixed precision techniques. CMPQ adapts the precision level based on canonical correlation analysis of different layers. PMPQ optimizes precision layer-wise based on their sensitivity to sparsity. TDMPQ modifies precision using Taylor decomposition to assess each layer's sensitivity to input perturbation. These strategies allocate higher precision to more sensitive layers while diminishing precision to robust layers. CPTQuant assesses the performance across BERT, OPT-125M, OPT-350M, OPT-1.3B, and OPT-2.7B. We demonstrate up to 4x compression and a 2x-fold increase in efficiency with minimal accuracy drop compared to Hugging Face FP16. PMPQ stands out for achieving a considerably higher model compression. Sensitivity analyses across various LLMs show that the initial and final 30% of layers exhibit higher sensitivities than the remaining layers. PMPQ demonstrates an 11% higher compression ratio than other methods for classification tasks, while TDMPQ achieves a 30% greater compression ratio for language modeling tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fQI2qNhs0d": {
    "title": "Ask the experts: sourcing a high-quality nutrition counseling dataset through Human-AI collaboration",
    "volume": "review",
    "abstract": "Recent publicly available Large Language Models (LLMs) are being employed by end-users for various tasks, including sensitive ones such as health counseling, disregarding potential safety concerns. It is thus necessary to understand how adequately LLMs perform in such domains. We conduct a case study on ChatGPT in nutrition counseling, a popular use-case where the model supports a user with their dietary struggles. We crowd-source real-world diet-related struggles, then work with nutrition experts to generate supportive text using ChatGPT. Finally, experts evaluate the safety and text quality of ChatGPT's output. The result is the HAI-coaching dataset, containing ~2.4K crowdsourced dietary struggles and ~97K corresponding ChatGPT-generated and expert-annotated supportive texts. We analyse ChatGPT's performance, discovering potentially harmful behaviours, especially for sensitive topics like mental health. Finally, we use HAI-coaching to test open LLMs on various downstream tasks, showing that even the latest models struggle to achieve good performance. HAI-coaching is available at https://anonymous.4open.science/r/3z2",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IIYiBQraWe": {
    "title": "Solving the Inverse Alignment Problem for Efficient RLHF",
    "volume": "review",
    "abstract": "Collecting high-quality preference datasets for reinforcement learning from human feedback (RLHF) is resource-intensive and challenging. As a result, researchers often train reward models on extensive offline datasets which aggregate diverse generation sources and scoring/alignment policies. We hypothesize that this aggregation has an averaging effect on reward model scores, which limits signal and impairs the alignment process. Inspired by the field of inverse RL, we define the ``inverse alignment problem'' in language model training, where our objective is to optimize the critic's reward for a fixed actor and a fixed offline preference dataset. We hypothesize that solving the inverse alignment problem will improve reward model quality by providing clearer feedback on the policy's current behavior. To that end, we investigate whether repeatedly fine-tuning a reward model on subsets of the offline preference dataset aligned with a periodically frozen policy during RLHF improves upon vanilla RLHF. Our empirical results demonstrate that this approach facilitates superior alignment and faster convergence compared to using an unaligned or out-of-distribution reward model relative to the LLM policy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rDb0EUHIgu": {
    "title": "SedarEval: Automated Evaluation using Self-Adaptive Rubrics",
    "volume": "review",
    "abstract": "The evaluation paradigm of LLM-as-judge gains popularity due to its significant reduction in human labor and time costs. This approach utilizes one or more large language models (LLMs) to assess the quality of outputs from other LLMs. However, existing methods rely on generic scoring rubrics that fail to consider the specificities of each question and its problem-solving process, compromising precision and stability in assessments. Inspired by human examination scoring processes, we propose a new evaluation paradigm based on self-adaptive rubrics. Specifically, we create detailed scoring rubrics for each question, capturing the primary and secondary criteria in a structured format of scoring and deduction points that mimic a human evaluator's analytical process. Building on this paradigm, we further develop a novel benchmark called INSDA, which covers a range of domains including long-tail knowledge, mathematics, coding, and logical reasoning. INSDA consists of 1,000 meticulously crafted questions, each with its own self-adaptive rubric. To further streamline the evaluation, we train a specialized evaluator language model (evaluator LM) to supplant human graders. Using the same training data, our evaluator LM achieves a higher concordance rate with human grading results than other paradigms, including GPT-4, highlighting the superiority and efficiency of our approach",
    "checked": false,
    "id": "07ba6561ed3904d3e61a1750759066520a098b54",
    "semantic_title": "classroom evaluation of a gamified adaptive tutoring system",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=1xGA00uVtD": {
    "title": "Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models",
    "volume": "review",
    "abstract": "Causal graph recovery is traditionally done using statistical estimation-based methods or based on individual's knowledge about variables of interests. They often suffer from data collection biases and limitations of individuals' knowledge. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that leverages LLMs to deduce causal relationships in general causal graph recovery tasks. This method leverages knowledge compressed in LLMs and knowledge LLMs extracted from scientific publication database as well as experiment data about factors of interest to achieve this goal. Our method gives a prompting strategy to extract associational relationships among those factors and a mechanism to perform causality verification for these associations. Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets. More importantly, as causality among some factors may change as new research results emerge, our method show sensitivity to new evidence in the literature and can provide useful information for updating causal graphs accordingly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c4BKOnm6uN": {
    "title": "TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot",
    "volume": "review",
    "abstract": "With the rapid development of large language models (LLM), the evaluation of LLM becomes increasingly important. Measuring text gener- ation tasks such as summarization and article creation is very difficult. Especially in spe- cific application domains (e.g., to-business or to-customer service), in-house evaluation cri- teria have to meet not only general standards (correctness, helpfulness and creativity, etc.) but also specific needs of customers and busi- ness security requirements at the same time, making the evaluation more difficult. So far, the evaluation of LLM in business scenarios has mainly relied on manual, which is expensive and time-consuming. In this paper, we propose a model-based evaluation method: TALEC, which allows users to flexibly set their own evaluation criteria, and uses in-context learning (ICL) to teach judge model these in-house cri- teria. In addition, we try combining zero-shot and few-shot to make the judge model focus on more information. We also propose a prompt paradigm and an engineering approach to ad- just and iterate the shots ,helping judge model to better understand the complex criteria. We then compare fine-tuning with ICL, finding that fine-tuning can be replaced by ICL. TALEC demonstrates a strong capability to accurately reflect human preferences and achieves a cor- relation of over 80% with human judgments, outperforming even the inter-human correlation in some tasks",
    "checked": true,
    "id": "6eec3166b5c0920573cd838e6d308c416dc06da4",
    "semantic_title": "talec: teach your llm to evaluate in specific domain with in-house criteria by criteria division and zero-shot plus few-shot",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XEISCKCFtK": {
    "title": "On the Fragility of Active Learners for Text Classification",
    "volume": "review",
    "abstract": "Active learning (AL) techniques optimally utilize a labeling budget by iteratively selecting instances that are most valuable for learning. However, they lack \"prerequisite checks\", i.e., there are no prescribed criteria to pick an AL algorithm best suited for a dataset. A practitioner must pick a technique they \\emph{trust} would beat random sampling, based on prior reported results, and hope that it is resilient to the many variables in their environment: dataset, labeling budget and prediction pipelines. The important questions then are: how often on average, do we expect any AL technique to reliably beat the computationally cheap and easy-to-implement strategy of random sampling? Does it at least make sense to use AL in an ``Always ON'' mode in a prediction pipeline, so that while it might not always help, it never under-performs random sampling? How much of a role does the prediction pipeline play in AL's success? We examine these questions in detail for the task of text classification using pre-trained representations, which are ubiquitous today. Our primary contribution here is a rigorous evaluation of AL techniques, old and new, across setups that vary wrt datasets, text representations and classifiers. This unlocks multiple insights around warm-up times, i.e., number of labels before gains from AL are seen, viability of an ``Always ON'' mode and the relative significance of different factors. Additionally, we release a framework for rigorous benchmarking of AL techniques for text classification",
    "checked": true,
    "id": "a24650d2caf7a3bee9b17d2dc436f63f1629d205",
    "semantic_title": "on the fragility of active learners for text classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5kxm5l55Nv": {
    "title": "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer",
    "volume": "review",
    "abstract": "Language style is necessary for AI systems to accurately understand and generate diverse human language. However, previous text style transfer primarily focused on sentence-level data-driven approaches, limiting exploration of potential problems in large language models (LLMs) and the ability to meet complex application needs. To overcome these limitations, we introduce a novel task called Public-Speaking Style Transfer (PSST), which aims to simulate humans to transform passage-level, official texts into a public-speaking style. Grounded in the analysis of real-world data from a linguistic perspective, we decompose public-speaking style into key sub-styles to pose challenges and quantify the style modeling capability of LLMs. For such intricate text style transfer, we further propose a fine-grained evaluation framework to analyze the characteristics and identify the problems of stylized texts. Comprehensive experiments suggest that current LLMs struggle to generate public speaking texts that align with human preferences, primarily due to excessive stylization and loss of semantic information. We will release our data, code, and model upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FlnkfDLudm": {
    "title": "On the Universal Truthfulness Hyperplane Inside LLMs",
    "volume": "review",
    "abstract": "While large language models (LLMs) have demonstrated remarkable abilities across various fields, hallucination remains a significant challenge. Recent studies have explored hallucinations through the lens of internal representations, proposing mechanisms to decipher LLMs' adherence to facts. However, these approaches often fail to generalize to out-of-distribution data, leading to concerns about whether internal representation patterns reflect fundamental factual awareness, or only overfit spurious correlations on the specific datasets. In this work, we investigate whether a universal truthfulness hyperplane that distinguishes the model's factually correct and incorrect outputs exists within the model. To this end, we scale up the number of training datasets and conduct an extensive evaluation -- we train the truthfulness hyperplane on a diverse collection of over 40 datasets and examine its cross-task, cross-domain, and in-domain generalization. Our results indicate that increasing the diversity of the training datasets significantly enhances the performance in all scenarios, while the volume of data samples plays a less critical role. This finding supports the optimistic hypothesis that a universal truthfulness hyperplane may indeed exist within the model, offering promising directions for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UPRRMBSCVJ": {
    "title": "ECON: On the Detection and Resolution of Evidence Conflicts",
    "volume": "review",
    "abstract": "The rise of large language models (LLMs) has significantly influenced the quality of information in decision-making systems, leading to the prevalence of AI-generated content and challenges in detecting misinformation and managing conflicting information, or \"inter-evidence conflicts.\" This study introduces a method for generating diverse, validated evidence conflicts to simulate real-world misinformation scenarios. We evaluate conflict detection methods, including Natural Language Inference (NLI) models, factual consistency (FC) models, and LLMs, on these conflicts (\\textbf{RQ1}) and analyze LLMs' conflict resolution behaviors (\\textbf{RQ2}). Our key findings include: (1) NLI and LLM models exhibit high precision in detecting answer conflicts, though weaker models suffer from low recall; (2) FC models struggle with lexically similar answer conflicts, while NLI and LLM models handle these better; and (3) stronger models like GPT-4 show robust performance, especially with nuanced conflicts. For conflict resolution, LLMs often favor one piece of conflicting evidence without justification and rely on internal knowledge if they have prior beliefs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nG4y9gy0jn": {
    "title": "HALLUCHECK: An Efficient & Effective Fact-Based Approach Towards Factual Hallucination Detection Of LLMs Through Self-Consistency",
    "volume": "review",
    "abstract": "Large language models (LLMs) frequently generate inaccurate responses -- this can be particularly dangerous in sensitive areas like medicine and healthcare. Current methods for detecting hallucinations involve sampling answers multiple times, making them computationally intensive. In this study, we introduce HalluCheck, a novel hallucination detection module that identifies factual elements or atomic facts within a text. HalluCheck operates on the premise that responses to questions probing factual answers should be consistent both within a single LLM and across different LLMs. To improve system robustness, we incorporate a token-probability-based double-check mechanism. For hallucinated facts, inconsistencies or a lack of model confidence during generation will be evident. We evaluate our detection module on fact-based datasets such as NQ\\_Open, HotpotQA, and WebQ, by building upon open-source LLMs such as LLaMa-2 (7B)-Instruct and Mistral-7B-Instruct. Finally, we compare the generated output with the correct answers to determine sentence-level AUC-ROC scores for hallucination detection. Our results demonstrate that HalluCheck can (i) detect hallucinated facts and (ii) achieve significantly higher AUC-ROC scores compared to existing baselines that operate under similar conditions, specifically those that do not utilize external databases for hallucination detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=olGXnHrQSs": {
    "title": "Discourse Parsing in the Wild : Enhancing Discourse Parsing for Localized Structures from Social Media with LLM-Generated Data",
    "volume": "review",
    "abstract": "This study addresses challenges in discourse parsing for localized discourse structures on Social Media by using Large Language Models to generate synthetic data. We focus on a specific discourse structure in online immigration-related discussions, characterized by joint joint evaluation coherence relations. Our approach includes annotating and generating synthetic data, then evaluating two parsersâ€”bottom-up and a top-down parserâ€”trained on this data. We focus on parsing in real-world scenarios where edu segmentation is performed from scratch, reflecting practical parser use for discourse analysis beyond annotated RST corpora. We highlight the challenge of identifying coherence relations in longer texts, a task often overlooked in tree-based evaluations which typically assess the entire structure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=811DnH3ZqN": {
    "title": "Boosting Decision-Making Ability of LLMs with Speculative Reward Model Cost-Effectively",
    "volume": "review",
    "abstract": "Efficient decision-making in Large Language Models (LLMs) for intricate tasks is impeded by the substantial inference cost. The prevailing search algorithms tend to focus solely on performance enhancement, ignoring the cost-effectiveness trade-off. To address these challenges, we first utilize the 3E Criteria to systematically evaluate the cost-benefit balance of current search strategies. We found current methods typically sacrifice vast efficiency for marginal improvement of effectiveness. In order to empower LLMs effective decision-making capability with balanced efficiency, we present a plug-and-play framework with Speculative Reward Model (SRM), which leverages external reward assigner to speculate the preferential actions, not just rely on self-evaluation inside LLMs. Through speculative verification approach for pruning and efficient navigation toward more promising steps, SRM boosts decision-making ability of LLM cost-effectively. We evaluate SRM on several complex decision-making tasks including mathematical reasoning, planning and numerical reasoning for specific fields. Our results demonstrate the superiority of SRM for inference efficiency, which significantly lowers cost to a fraction of the original search framework's by 1/10 on average, without sacrificing effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jicM6vUE6E": {
    "title": "IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language",
    "volume": "review",
    "abstract": "Hate speech poses a significant threat to social harmony. Over the past two years, Indonesia has seen a ten-fold increase in the online hate speech ratio, underscoring the urgent need for effective detection mechanisms. However, progress is hindered by the limited availability of labeled data for Indonesian texts. The condition is even worse for marginalized minorities, such as Shia, LGBTQ, and other ethnic minorities because hate speech is underreported and less understood by detection tools. Furthermore, the lack of accommodation for subjectivity in current datasets compounds this issue. To address this, we introduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity classification dataset. Comprising 43,692 entries annotated by 19 diverse individuals, the dataset focuses on texts targeting vulnerable groups in Indonesia, specifically during the hottest political event in the country: the presidential election. We establish baselines for seven binary classification tasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet) fine-tuned for hate speech classification. Furthermore, we demonstrate how incorporating demographic information can enhance the zero-shot performance of the large language model, gpt-3.5-turbo. However, we also caution that an overemphasis on demographic information can negatively impact the fine-tuned model performance due to data fragmentation",
    "checked": true,
    "id": "3d4b5e6f59deba7a4fb76fa095ea6a562a8f5ee0",
    "semantic_title": "indotoxic2024: a demographically-enriched dataset of hate speech and toxicity types for indonesian language",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cU2EUwz0mv": {
    "title": "Mixture-of-Subspaces in Low-Rank Adaptation",
    "volume": "review",
    "abstract": "In this paper, we introduce a subspace-inspired Low-Rank Adaptation (LoRA) method, which is computationally efficient, easy to implement, and readily applicable to large language, multimodal, and diffusion models. Initially, we equivalently decompose the weights of LoRA into two subspaces, and find that simply mixing them can enhance performance. To study such a phenomenon, we revisit it through a fine-grained subspace lens, showing that such modification is equivalent to employing a fixed mixer to fuse the subspaces. To be more flexible, we jointly learn the mixer with the original LoRA weights, and term the method as Mixture-of-Subspaces LoRA (MoSLoRA). MoSLoRA consistently outperforms LoRA on tasks in different modalities, including commonsense reasoning, visual instruction tuning, and subject-driven text-to-image generation, demonstrating its effectiveness and robustness",
    "checked": true,
    "id": "e2403e398314d49c5f56e05105a420a6f93e3cb2",
    "semantic_title": "mixture-of-subspaces in low-rank adaptation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=MHyCm3J9C2": {
    "title": "Measure LLM knowledge via RIG (Raw Information Gain) over preliminarily crafted probes",
    "volume": "review",
    "abstract": "We propose a novel high-level approach to analyze models in a different way: we could estimate amount of information receipted by a model using a crafted set of control statements. We introduce a new metrics RIG (raw information gain) in order to do so. Any LLM (large language model) could be considered a \"black box\" of compressed information. It is hard to measure what amount of information is stored inside the model regarding any domain. The contrast between the size of a trained model of around 43GB compared to 15 trillion tokens of training data is staggering The other issue is to figure out where do the limits come from: is it an architectural constraint or is the limitation coming from the data used in training. So far the most common way to identify if the model is properly trained and contains necessary information is to put it through a list of benchmarks and the decision is based on either it's ranking or some educated guess of a score threshold. Keeping in mind that the most of those benchmarks become part of training data for upcoming models we face a vicious cycle of never ending benchmark creation. Taken into account constant size growth of both language models and datasets we face an challenge of losing a track of what is efficient and what is not to train models as well as simple scale of the datasets makes them almost impossible to supervise at all, what is an immense obstacle when we need to update any language model according to different environments those are implemented at and we need to bring ethical issues, actuality of the human knowledge and controversial statements altogether",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7G3BTM5BFQ": {
    "title": "Pairing Analogy-Augmented Generation with Procedural Memory for Procedural Q&A",
    "volume": "review",
    "abstract": "While LLMs in the RAG paradigm have shown remarkable performance on a variety of tasks, they still under-perform on unseen domains, especially on complex tasks like procedural question answering. In this work, we introduce a novel formalism and structure for manipulating text-based procedures. Based on this formalism, we further present a novel dataset called LCStep, scraped from the LangChain Python docs. Moreover, we extend the traditional RAG system to propose a novel system called Analogy Augmented Generation (AAG), that draws inspiration from human analogical reasoning and ability to assimilate past experiences to solve unseen problems. The proposed method uses a frozen language model with a custom procedure memory store to adapt to specialized knowledge. We demonstrate that AAG outperforms few-shot and RAG baselines on LCStep, RecipeNLG, and the CHAMP datasets under a pairwise LLM-based evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QzaK1U1S1w": {
    "title": "LogicPro: Logical Reasoning Enhanced with Program Examples",
    "volume": "review",
    "abstract": "In this paper, we present a novel approach, called \\textbf{LogicPro}, to enhance \\underline{Logic} reasoning through \\underline{Pro}gram examples to improve multiple complex reasoning tasks simultaneously. We do this effectively by simply utilizing widely available algorithmic problems and their code solutions. First, we constructed diverse input test samples based on algorithmic questions and code solutions. Then, we designed different logic reasoning questions based on the algorithmic problems and test samples. Finally, combining the intermediate variable outputs of the code solutions and the logic reasoning questions, we obtain the final reasoning path through a large language model. Based on this, we are able to construct very rich \\textit{SFT} data. At the same time, we construct a diverse and scalable dataset of logical reasoning evaluation by treating each algorithmic question as a reasoning rule. As a result, our approach achieves significant improvements on multiple models for BBH dataset (20+ subsets), GSM8K and HellSwag datasets, and significantly outperforms a wide range of existing logical reasoning datasets. In addition, our eval data distinguishes well between existing models and brings new challenges to the model",
    "checked": false,
    "id": "1b234c4a18f54329820a9ec2e8da0abec6e7149e",
    "semantic_title": "simply logical - intelligent reasoning by example",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=cojlzteFYT": {
    "title": "Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) offer the potential for automatic time series analysis and reporting, which is a critical task across many domains, spanning healthcare, finance, climate, energy, and many more. In this paper, we propose a framework for rigorously evaluating the capabilities of LLMs on time series understanding, encompassing both univariate and multivariate forms. We introduce a comprehensive taxonomy of time series features, a critical framework that delineates various characteristics inherent in time series data. Leveraging this taxonomy, we have systematically designed and synthesized a diverse dataset of time series, embodying the different outlined features, each accompanied by textual descriptions. This dataset acts as a solid foundation for assessing the proficiency of LLMs in comprehending time series. Our experiments shed light on the strengths and limitations of state-of-the-art LLMs in time series understanding, revealing which features these models readily comprehend effectively and where they falter. In addition, we uncover the sensitivity of LLMs to factors including the formatting of the data, the position of points queried within a series and the overall time series length",
    "checked": true,
    "id": "80b06bb6b5ab0e6e6de9eecf8d5829dec2f6df57",
    "semantic_title": "evaluating large language models on time series feature understanding: a comprehensive taxonomy and benchmark",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aJjVT5ri76": {
    "title": "A Narrative Framework for Analyzing Partisan Perspectives in Event Discourse",
    "volume": "review",
    "abstract": "Experts from several domains, especially political science, are interested in analyzing political discourse associated with real-world news events. This process would typically require researchers to manually analyze a large collection of news articles on a given event, in order to characterize the underlying partisan perspectives from each side of the political map. Instead, in this work, we propose a systematic approach to summarize partisan perspectives, in an automated manner. Our framework allows us to represent each news article with a predefined structure, comprising of talking points, which we then cluster to identify the repeating themes that collectively shape the narrative of an event. Then, we utilize the resulting clusters to generate a summary for each ideology, left and right, that indicates how each side discusses the event. We show the effectiveness of our framework in capturing partisan perspectives across automated proxy tasks, and human evaluation over a set of events. We release the dataset derived from our narrative framework to the research community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gpo4L9k1Ol": {
    "title": "From Instructions to Basic Human Values: A Survey of Alignment Goals for Big Model",
    "volume": "review",
    "abstract": "As big models demonstrate remarkable performance across diverse tasks, concerns about their potential risks and social harms are raised. Extensive efforts have been made towards aligning big models with humans to ensure their responsible development and human profits maximization. Nevertheless, the question `what to align with' remains largely unexplored. It is critical to precisely define the objectives for big models to pursue, since aligning with inappropriate goals could cause disaster, e.g., chatbots promote abusive or biased contents when only instructed to interact freely. This paper conducts a comprehensive survey of different alignment goals, tracing their evolution paths to identify the most appropriate goal for big models. Specifically, we categorize existing goals into four levels: human instructions, human preferences, value principles and basic values, revealing a learning process from basic abilities to intrinsic value concepts. For each goal, we elaborate its definition, limitation, how techniques are designed to achieve it and how to evaluate the alignment. Posing basic values as a promising goal, we discuss technical challenges and future research directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BqGUy8eT4C": {
    "title": "RepoHyper: Hybrid Retrieval for Repository-level Code Completion",
    "volume": "review",
    "abstract": "Code Large Language Models (CodeLLMs) have demonstrated impressive proficiency in code completion tasks. However, they often fall short of fully understanding the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies, which can result in less precise completions. To overcome these limitations, we present \\tool, a multifaceted framework designed to address the complex challenges associated with repository-level code completion. Central to \\tool is the {\\em Repo-level Semantic Graph} (RSG), a novel semantic graph structure that encapsulates the vast context of code repositories. Furthermore, \\tool leverages \\textit{Expand and Refine} retrieval method, including a graph expansion and a link prediction algorithm applied to the RSG, enabling the effective retrieval and prioritization of relevant code snippets. Our evaluations show that \\tool markedly outperforms existing techniques in repository-level code completion, showcasing enhanced accuracy across various datasets when compared to several strong baselines",
    "checked": false,
    "id": "d7fc8fc5510f31469ba263253ec54edd7821cf8d",
    "semantic_title": "repohyper: better context retrieval is all you need for repository-level code completion",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=xmG4oGCxJH": {
    "title": "Towards Expert Legal LLM Responses: Logical Structure and Semantic Information Integration",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated excellent performance across various fields. Nevertheless, they exhibit notable deficiencies when addressing legal questions. In the legal field, LLMs often provide generalized responses, lacking the necessary specificity for expert legal advice. Additionally, they tend to provide answers that appear correct but are unreliable due to issues with hallucination. Retrieval-Augmented Generation (RAG) is a popular approach to addressing these issues. However, existing methods often focus solely on semantic-level similarity, neglecting the logical structure relationships between different legal questions. In this paper, we propose a Logical-Semantic Integration Model (LSIM), which consists of three components. First, reinforcement learning is used to predict the fact-rule chain of thought for the given question. Secondly, the DSSM model that integrates logical structure and semantic information is used to retrieve the most relevant candidate questions from the database. Finally, in-context learning is used to generate the final answer. Experiments on a real-world legal QA dataset, using both automated evaluation metrics and human evaluation, demonstrate the effectiveness of the proposed method. The dataset will be released to the community to promote the development of the legal QA fieldã€‚",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eMUKOywNK3": {
    "title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
    "volume": "review",
    "abstract": "In this paper, we introduce a novel psychological benchmark, CPsyExam, constructed from questions sourced from Chinese examination systems. CPsyExam is designed to prioritize psychological knowledge and case analysis separately, recognizing the significance of applying psychological knowledge to real-world scenarios. We collect 22k questions from 39 psychology-related subjects across four Chinese examination systems. From the pool of 22k questions, we utilize 4k to create the benchmark that offers balanced coverage of subjects and incorporates a diverse range of case analysis techniques. Furthermore, we evaluate a range of existing large language models(LLMs), spanning from open-sourced to proprietary models. Our experiments and analysis demonstrate that CPsyExam serves as an effective benchmark for enhancing the understanding of psychology within LLMs and enables the comparison of LLMs across various granularities",
    "checked": true,
    "id": "10da92399630f5ee4e37f51ac21d278d829e9253",
    "semantic_title": "cpsyexam: a chinese benchmark for evaluating psychology using examinations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FVTPELeltD": {
    "title": "Direct Multi-Turn Preference Optimization for Language Agents",
    "volume": "review",
    "abstract": "Adapting Large Language Models (LLMs) for agent tasks is critical in developing language agents. Direct Preference Optimization (DPO) is a promising technique for this adaptation with the alleviation of compounding errors, offering a means to directly optimize Reinforcement Learning (RL) objectives. However, applying DPO to multi-turn tasks presents challenges due to the inability to cancel the partition function. Overcoming this obstacle involves making the partition function independent of the current state and addressing length disparities between preferred and dis-preferred trajectories. In this light, we replace the policy constraint with the state-action occupancy measure constraint in the RL objective and add length normalization to the Bradley-Terry model, yielding a novel loss function named DMPO for multi-turn agent tasks with theoretical explanations. Extensive experiments on three multi-turn agent task datasets confirm the effectiveness and superiority of the DMPO loss",
    "checked": true,
    "id": "3b5778b0aec88c1921fee7538400e49bed42a5fa",
    "semantic_title": "direct multi-turn preference optimization for language agents",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oFq2Pn8Xqr": {
    "title": "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models",
    "volume": "review",
    "abstract": "Multimodal Large Language Models (MLLMs) have shown promising results in various tasks, but their ability to perceive the visual world with deep, hierarchical understanding similar to humans remains uncertain. To address this gap, we introduce CONSTRUCTURE, a novel concept-level benchmark to assess MLLMs' hierarchical concept understanding and reasoning abilities. Our goal is to evaluate MLLMs across four key aspects: 1) Understanding atomic concepts at different levels of abstraction; 2) Performing upward abstraction reasoning across concepts; 3) Achieving downward concretization reasoning across concepts; and 4) Conducting multi-hop reasoning between sibling or common ancestor concepts. Our findings indicate that even state-of-the-art multimodal models struggle with concept structure reasoning (e.g., GPT-4o averages a score of 62.1\\%). We summarize key findings of MLLMs in concept structure reasoning evaluation. Morever, we provide key insights from experiments using CoT prompting and fine-tuning to enhance their abilities",
    "checked": false,
    "id": "681cee58cf7e54199191cf9e0baf6851d8356704",
    "semantic_title": "complex qa and language models hybrid architectures, survey",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=JVtQWJXang": {
    "title": "Chain of Condition: Construct, Verify and Solve Conditions for Conditional Question Answering",
    "volume": "review",
    "abstract": "Conditional question answering (CQA) is an important task that aims to find probable answers and identify conditions that need to be satisfied to support the answer. Existing approaches struggle with CQA due to two main challenges: (1) precisely identifying conditions and their logical relationship, and (2) verifying and solving the conditions. To address these challenges, we propose Chain of Condition, a novel prompting approach by firstly identifying all conditions and constructing their logical relationships explicitly according to the document, then verifying whether these conditions are satisfied, finally solving the logical expression by tools to indicate any missing conditions and generating the answer based on the resolved conditions. The experiments on two benchmark conditional question answering datasets shows chain of condition outperforms existing prompting baselines, establishing a new state-of-the-art. Furthermore, with backbone models like GPT-3.5-Turbo or GPT-4, it surpasses all supervised baselines with only few-shot settings",
    "checked": true,
    "id": "d8a3d1088540065be6bc41365750055125039a40",
    "semantic_title": "chain of condition: construct, verify and solve conditions for conditional question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YHUUBTF2HW": {
    "title": "Towards Strategic Persuasion: Unveiling Users' Susceptibility to Persuasive Strategies in Dialogues",
    "volume": "review",
    "abstract": "Generative AI's rapid evolution has made dialogue systems indispensable tools. While persuasive strategies have been incorporated in dialogue systems to provide personalized services, current research primarily focuses on studying persuasive strategies from persuader's perspective, with limited exploration of persuadee's susceptibility towards these strategies. To bridge this gap, we introduce a novel task called Susceptibility Strategy Detection, aimed at identifying the persuasive strategies that users are most susceptible to. To support this new task, we develop a refined dataset P4G+, and propose a dual attitude-sensitive framework to detect susceptibility strategy by analyzing the persuasive process, user interactions, and content within dialogues. Comprehensive experiments have demonstrated the efficacy of our approach in identifying users' susceptible strategies. The code and dataset will be made available upon acceptance of this paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wNvMf766ZJ": {
    "title": "Improving Language Model Self-Correction with Meta-Feedback",
    "volume": "review",
    "abstract": "Large language models (LLMs) are capable of self-correct their responses by generating feedback and refining the initial output. However, their performance may sometimes decline following self-correction, either because the feedback contains errors or because they unnecessarily attempt to refine an already accurate response. To address these limitations, we investigate whether LLMs can generate meta-feedback that pinpoints errors in the feedback rather than the response.While the ability of LLMs to generate self-feedback has been well-researched, their potential to provide constructive meta-feedback remains under-explored. We design a novel self-correction prompting framework, Feedback-on-Feedback (FoF), which leverages meta-feedback to improve the feedback before refining the response. Our framework first samples multiple feedbacks for the initial response, and prompts the LLM to generate a meta-feedback that analyze the inconsistency between these feedbacks. Based on the meta-feedback, the LLM generates a refined feedback that subsequently guides the revision of the response. Our FoF framework uniformly outperforms competitive baselines across two base models in different sizes and three datasets spanning arithmetic reasoning, machine translation and programming, with an improvement of up to 1.68% in GSM8K task by LLaMA3-8B model",
    "checked": false,
    "id": "df90ee11ed6378635f22e6d0061cf67dd0bacd13",
    "semantic_title": "meta-rewarding language models: self-improving alignment with llm-as-a-meta-judge",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=rvXzn73Gxm": {
    "title": "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA",
    "volume": "review",
    "abstract": "Long-context modeling capabilities of Large Language Models (LLMs) have garnered widespread attention, leading to the emergence of LLMs with ultra-context windows. Meanwhile, benchmarks for evaluating long-context language models are gradually catching up. However, existing benchmarks employ irrelevant noise texts to artificially extend the length of test cases, diverging from the real-world scenarios of long-context applications. To bridge this gap, we propose a novel long-context benchmark, Loong, aligning with realistic scenarios through extended multi-document question answering (QA). Unlike typical document QA, in Loong's test cases, each document is relevant to the final answer, ignoring any document will lead to the failure of the answer. Furthermore, Loong introduces four types of tasks with a range of context lengths: Spotlight Locating, Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic and comprehensive evaluation of long-context understanding. Extensive experiments indicate that existing long-context language models still exhibit considerable potential for enhancement. Retrieval augmented generation (RAG) achieves poor performance, demonstrating that Loong can reliably assess the model's long-context modeling capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=heAXhUzqYI": {
    "title": "FD-Bench: Evaluating the Decision-Making Capability of LLM Agents in Dynamic Scenarios through Fire Evacuation",
    "volume": "review",
    "abstract": "LLMs as general-purpose agents have gained widespread application in practical domains due to robust autonomous decision-making capabilities and extensive world knowledge. However, recent studies have highlighted that the dynamicity inherent in real-world environments poses significant challenges to the decision-making abilities of LLM agents, making the investigation of LLM decision-making boundaries in dynamic environments an urgent and formidable task. Current evaluation benchmarks primarily focus on assessing LLM decision-making performance in static and interactive environments, which significantly differ from the dynamicity present in real-world scenarios. Furthermore, existing evaluation frameworks lack fine-grained assessments of LLMs' capabilities, providing limited insights during the evaluation process and hindering a comprehensive understanding of the models' capabilities. To address these limitations, we propose FD-Bench, a framework for evaluating the decision-making abilities of LLM agents in dynamic scenarios. FD-Bench employs a fire evacuation scenario as a representative dynamic setting, offering a generalized evaluation framework and a step-wise evaluation strategy to comprehensively analyze and capture variations in decision-making abilities. This framework provides a comprehensive understanding of the decision-making capabilities and limitations of LLM agents in dynamic environments, laying the foundation for their potential deployment in more realistic physical scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1a5gtWNCAr": {
    "title": "Advancing Idiomatic Understanding: Evaluating GPTÂ­3.5 and Google Translate for PersianÂ­English Translations",
    "volume": "review",
    "abstract": "Figurative language, especially idiomatic expressions, poses significant translation challenges due to its cultural and contextual nuances. Large Language Models (LLMs) like GPT-3.5 have shown greater capability in translating figurative language compared to state-of-the-art neural machine translation (NMT) systems. However, the impact of different prompting methods and combining NMTs and LLMs on idiom translation remains unexplored. This paper introduces two parallel datasets for Persian\\rightarrow English and English\\rightarrow Persian translation to address these challenges. The Persian idiom examples are sampled from our PersianIdioms resource, which is compiled from an online dictionary and contains 2200 idioms with their meanings and popularity scores. Using these datasets, we evaluate GPT models, Google Translate, and their combination, focusing on idiom translation accuracy, fluency, and contextual relevance. Additionally, we assess existing automatic evaluation metrics and GPT-3.5 and GPT-4 for evaluating idiomatic translations. Our results indicate that while Google Translate shows superior fluency, GPT-3.5 excels in accurately translating idioms. We also show that models are better at translating English idioms than Persian ones, and different configurations of models perform differently depending on the direction of translation. We will release all our resources and annotations upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ilKqohjYS": {
    "title": "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities",
    "volume": "review",
    "abstract": "Task-oriented dialogue (TOD) systems aim to efficiently handle task-oriented conversations, including information gathering. How to utilize ToD accurately, efficiently and effectively for information gathering has always been a critical and challenging task. Recent studies have demonstrated that Large Language Models (LLMs) excel in dialogue, instruction generation, and reasoning, and can significantly enhance the performance of TOD through fine-tuning. However, current datasets primarily cater to user-led systems and are limited to predefined specific scenarios and slots, thereby necessitating improvements in the proactiveness, diversity, and capabilities of TOD. In this study, we present a detailed multi-domain task-oriented data construction process for conversations, and a Chinese dialogue dataset generated based on this process, \\textbf{TransferTOD}, which authentically simulates human-machine dialogues in 30 popular life service scenarios. Leveraging this dataset, we trained a \\textbf{TransferTOD-7B} model using full-parameter fine-tuning, showcasing notable abilities in slot filling and questioning. Our work has demonstrated its strong generalization capabilities in various downstream scenarios, significantly enhancing both data utilization efficiency and system performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X9lpoxnJLu": {
    "title": "Bayesian Calibration of Win Rate Estimation with LLM Evaluators",
    "volume": "review",
    "abstract": "Recent advances in large language models (LLMs) show the potential of using LLMs as evaluators for assessing the quality of generations from LLMs. However, applying LLM evaluators naively to compare different systems can lead to unreliable results due to inaccuracy and intrinsic bias of LLM evaluators. In order to mitigate this problem, we propose two calibration methods, Bayesian Win-Rate Sampling (BWRS) and Bayesian Dawid-Skene, which both leverage Bayesian inference to more accurately infer the true win rate of generative language models. We empirically validate our methods on seven datasets including story generation, summarization, and instruction following. We show that both our methods are effective in improving the accuracy of win rate estimation using LLMs as evaluators, offering a promising direction for reliable automatic text quality evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dfIZ6FQw3M": {
    "title": "EchoSight: Advancing Visual-Language Models with Wiki Knowledge",
    "volume": "review",
    "abstract": "Knowledge-based Visual Question Answering (KVQA) tasks require answering questions about images using extensive background knowledge. Despite significant advancements, generative models often struggle with these tasks due to the limited integration of external knowledge. In this paper, we introduce **EchoSight**, a novel multimodal Retrieval-Augmented Generation (RAG) framework that enables large language models (LLMs) to answer visual questions requiring fine-grained encyclopedic knowledge. To strive for high-performing retrieval, EchoSight first searches wiki articles by using visual-only information, subsequently, these candidate articles are further reranked according to their relevance to the combined text-image query. This approach significantly improves the integration of multimodal knowledge, leading to enhanced retrieval outcomes and more accurate VQA responses. Our experimental results on the E-VQA and InfoSeek datasets demonstrate that EchoSight establishes new state-of-the-art results in knowledge-based VQA, achieving an accuracy of 41.8% on E-VQA and 31.3% on InfoSeek",
    "checked": true,
    "id": "7242478180247ef7bf335112aed16d4fc5d2d133",
    "semantic_title": "echosight: advancing visual-language models with wiki knowledge",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dJyrh0FxYg": {
    "title": "Do Explanations Help or Hurt? Saliency Maps vs Natural Language Explanations in a Clinical Decision-Support Setting",
    "volume": "review",
    "abstract": "As AI models are becoming more powerful, their adoption is becoming more widespread, including in safety-critical domains. Explainable AI (XAI) has the aim of making these models safer to use, for instance by making their decision-making process more transparent. However, current explainability methods are seldom evaluated in the way they are intended to be used: by real-world end users. To address this, we conducted a large-scale user study with 85 clinicians in the context of human-AI collaborative chest X-ray analysis. We evaluated three types of explanations: saliency maps, natural language explanations, and their combination. We specifically examine how different explanation types influence users depending on whether the AI is correct. We find that the quality of explanations, i.e., how much correct information they entail, and how much this aligns with AI correctness, significantly impacts the usefulness of the different explanation types",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lD0mamNmAM": {
    "title": "Exploring Domain Adaptation with LLMs for Real-World Augmented Question Answer Generation (RA-QAG) in Children Storytelling",
    "volume": "review",
    "abstract": "In the real world, external domain-specific knowledge is commonly required, for instance, teachers often apply their expertise to ask preschoolers educational-crafted, story-inspired questions beyond the story content during interactive storytelling; however, existing storytelling systems could not effectively support such activity as the generated questions are mostly text-based. We formulate this type of common real-world application as a novel Real-World Augmented QAG (RA-QAG) task. This work aims to explore how well LLMs, equipped with various domain adaptation strategies (e.g., few-shot In-Context Learning, Chain-of-Thoughts, Retrieval-Augmented Generation), perform on the RA-QAG task in the context of children storytelling. We design and experiment with end-to-end and 2-Step QAG pipelines with different domain adaptation strategies to explore whether they can identify real-world knowledge and create QA pairs aligned with experts' annotation. Our automatic evaluation and human evaluation show that 1) RAG is a promising direction to approach real-world domain-specific tasks; 2) human experts still have more nuanced knowledge from which generic LLMs need to learn",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7nOqhiXc10": {
    "title": "TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data",
    "volume": "review",
    "abstract": "Instruction tuning has achieved unprecedented success in NLP, turning large language models into versatile chatbots. However, the increasing variety and volume of instruction datasets demand significant computational resources. To address this, it is essential to extract a small and highly informative subset (i.e., \\textit{Coreset}) that achieves comparable performance to the full dataset. Achieving this goal poses non-trivial challenges: 1) data selection requires accurate data representations that reflect the training samples' quality, 2) considering the diverse nature of instruction datasets, and 3) ensuring the efficiency of the coreset selection algorithm for large models. To address these challenges, we propose \\textit{\\textbf{T}ask-\\textbf{A}gnostic \\textbf{G}radient \\textbf{C}lustered C\\textbf{O}reset \\textbf{S}election} (\\textbf{\\ModelNameAbbre}). Specifically, we leverage sample gradients as the data representations, perform clustering to group similar data, and apply an efficient greedy algorithm for coreset selection. Experimental results show that our algorithm, selecting only 5\\% of the data, surpasses other unsupervised methods and achieves performance close to that of the full dataset",
    "checked": true,
    "id": "754989f5d425f7d5dd03dcc9ffc029b6df82384b",
    "semantic_title": "tagcos: task-agnostic gradient clustered coreset selection for instruction tuning data",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FGZdm3jagP": {
    "title": "Improving Automatic Speech Recognition with Decoder-Centric Regularisation in Encoder-Decoder Models",
    "volume": "review",
    "abstract": "This paper introduces $\\textbf{De}$coder-$\\textbf{C}$entric $\\textbf{R}$egularisation in $\\textbf{E}$ncoder-$\\textbf{D}$ecoder (DeCRED) architecture for automatic speech recognition, where auxiliary classifier(s) are introduced in layers of the decoder module. Leveraging these classifiers, we propose two decoding strategies that re-estimate the next token probabilities. Pilot experiments conducted on the independent in-domain datasets identify the suitable placement and weighting of the auxiliary classifiers, resulting in a consistent word-error-rate (WER) reduction of up to 9% relative across different model sizes. Further experiments on a collection of multi-domain English datasets showed that DeCRED obtained competitive WERs as compared to Whisper-medium and outperformed OWSM v3; while relying only on a fraction of training data and model size. Finally, we also study the generalisation capabilities of DeCRED by evaluating on out-of-domain datasets, where we show an absoulte reduction of 2.7 and 2.9 WERs on AMI and Gigaspeech datasets respectively",
    "checked": false,
    "id": "3c692fe94d9bcd9d73e15d98f517386eac92c457",
    "semantic_title": "anatomy of industrial scale multilingual asr",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=NSlnJ03iaY": {
    "title": "Dynamic Order Template Prediction for Generative Aspect-Based Sentiment Analysis",
    "volume": "review",
    "abstract": "Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific aspects within texts, resulting in detailed sentiment tuples. Previous ABSA models often use static templates to predict all of the elements in the tuples, and these models often fail to accurately capture dependencies between elements. Multi-view prompting method improves the performance of ABSA by predicting tuples with various templates and then ensembling the results. However, this method suffers from inefficiencies and out-of-distribution errors. In this paper, we propose a Dynamic Order Template (DOT) method for ABSA, which dynamically generates necessary views for each instance based on instance-level entropy. Ensuring the diverse and relevant view generation, our proposed method improves F1-scores on ASQP and ACOS datasets while significantly reducing inference time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1h8rggcwWE": {
    "title": "MiniConGTS: A Near Ultimate Minimalist Contrastive Grid Tagging Scheme for Aspect Sentiment Triplet Extraction",
    "volume": "review",
    "abstract": "Aspect Sentiment Triplet Extraction (ASTE) aims to co-extract the sentiment triplets in a given corpus. Existing approaches within the pretraining-finetuning paradigm tend to either meticulously craft complex tagging schemes and classification heads, or incorporate external semantic augmentation to enhance performance. In this study, we, for the first time, re-evaluate the redundancy in tagging schemes and the internal enhancement in pretrained representations. We propose a method to improve and utilize pretrained representations by integrating a minimalist tagging scheme and a novel token-level contrastive learning strategy. The proposed approach demonstrates comparable or superior performance compared to state-of-the-art techniques while featuring a more compact design and reduced computational overhead. Additionally, we are the first to formally evaluate GPT-4's performance in few-shot learning and Chain-of-Thought scenarios for this task. The results demonstrate that the pretraining-finetuning paradigm remains highly effective even in the era of large language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kadMXEYf0Z": {
    "title": "Language Models as Simulations of Early Language Acquisition: analysis of expressive vocabulary",
    "volume": "review",
    "abstract": "Large language models (LLMs) have been shown to develop linguistic competence from mere exposure to language content, making them a promising avenue for investigating infants' language learning processes \\citep{lavechin2023babyslm,chang2022word}. Nevertheless, LLMs typically require orders of magnitude more data than children, and language outcomes cannot be directly compared. Here, we introduce \\textit{machine-CDI}, a metric based on the learner's output to enable a direct comparison of machines and infants on their expressive vocabulary as a function of input quantity. This metric adapts the Communicative Development Inventories \\citep{fenson2007macarthur,frank2017wordbank}, a normalized inventory of words to quantify child language development, to the evaluation set of language models. We illustrate machine-CDI by comparing the expressive vocabulary in infants and character language models (LSTMs and Transformers) trained on English audiobooks. The results show that language models approximately match the children's learning curves, although Transformers are delayed compared to LSTMs. A further analysis show that the models are more impacted by word frequency than children, with a large delay in acquiring low frequency words for models. This delay is found to be linked to the more general phenomenon of long tail truncation observed in language models, which makes them unable to learn words based on few shot observations. These results shed new light on the principles of language acquisition, and highlights important divergences in how humans and modern algorithms learn to process natural language",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vaEBK5FCLu": {
    "title": "A Closer Look into Mixture-of-Experts in Large Language Models",
    "volume": "review",
    "abstract": "Mixture-of-experts (MoE) architecture is gaining increasing attention due to its unique properties and remarkable performance, especially for language tasks. By sparsely activating a subset of parameters for each token, MoE architecture could increase the model size without sacrificing computational efficiency, achieving a better trade-off between performance and training costs. However, the underlying mechanism of MoE still lacks further exploration, and its modularization degree remains questionable. In this paper, we make an initial attempt to understand the inner workings of MoE-based large language models (LLMs). Concretely, we comprehensively study the parametric and behavioural features of three recent MoE-based models and reveal some intriguing observations, including (1) Neurons act like fine-grained experts and (2) The router of MoE usually selects experts with larger output norms. (3) The expert diversity increases as the layer increases, while the last layer is an outlier. Based on these observations, we also provide suggestions for a broad spectrum of MoE practitioners, such as router design and expert allocation. We hope this work could shed light on future research on the MoE framework and other modular architectures",
    "checked": true,
    "id": "f9088e3759b3b92fa33656e694730d14d9946d19",
    "semantic_title": "a closer look into mixture-of-experts in large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=PuUESZkwMT": {
    "title": "Exploring the Potential of Foundation Models as Reliable AI Contact Centers",
    "volume": "review",
    "abstract": "The emergence of foundation models with high generalization performance increases expectations for their use in Contact Center (CC) applications (e.g., AI agents). However, research on foundation model-based AICCs has not been sufficiently explored, and there is also a lack of phone call-based customer service datasets to evaluate their performance fairly across various attributes. Reference-based evaluation metrics may not be suitable for evaluating AI agent responses because they fail to account for multiple valid responses generated by AI agents. While we consider human evaluation (e.g., surveys) to be the ideal metric for AICCs, it can generally be costly, and specifically challenging to conduct immediately after a conversation. In this study, we explore whether foundation models in AICCs can provide fair and reliable answers, as well as discuss the need for an automatic evaluation method suitable for phone call-based customer service. Specifically, (1) we present audio-text data to validate the reliability and fairness of the foundation model-based AI agent across different subgroups based on regions, genders, and ages. (2) We customize a system prompt by combining domain-specific information and response guidance to apply foundation models to AICCs. (3) We propose an automatic evaluation method using LLMs called a generative model-based hierarchical dialogue evaluation metric and compare it with human evaluators to further investigate the feasibility of using a foundation model-based evaluation method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XRuU1qscaR": {
    "title": "Beyond Token Generation: Adaptive Chunk-Distilled Language Modeling",
    "volume": "review",
    "abstract": "The remarkable capabilities of Large Language Models (LLMs) in text generation have been widely recognized. However, their inefficiency in generating text at the token level leaves room for improvement, and adapting these models to new data remains a challenging task. To tackle these challenges, we introduce a novel approach to language modeling -- Chunk-Distilled Language Modeling (CD-LM). By integrating deep neural networks with a straightforward retrieval module, our method allows the generation of text chunks containing fine-grained information through multiple tokens at a single decoding step. Our retrieval framework enables flexible construction of model- or domain-specific datastores, either leveraging the internal knowledge of pre-trained or fine-tuned models, or incorporating expert insights from human-annotated corpus. This adaptability allows for enhanced control over language model distribution without necessitating additional training. We present a formal formulation of our {\\name} framework, along with quantifiable performance metrics, demonstrating its efficacy in optimizing language model performance and efficiency across a diverse set of downstream tasks, including language modeling, text generation, and domain adaptation",
    "checked": false,
    "id": "fe99095df95638b6768e1c10682034d25353ed89",
    "semantic_title": "musegraph: graph-oriented instruction tuning of large language models for generic graph mining",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=IEAKog8TLo": {
    "title": "Iterative Context Vectors: Boost In-Context Learning within Activations",
    "volume": "review",
    "abstract": "In-context learning has become a standard learning paradigm for language models. However, current prompt engineering methods, which function within the token space, may restrict their effectiveness. We propose to explore the potential of activation space through Iterative Context Vectors (ICVs), a technique aimed at improving task performance without backpropagation. ICVs are employed by first extracting and iteratively refining activations within a language model, then applying them during inference with minimal computational and memory overhead. We evaluate ICVs across a range of tasks using various models and observe significant improvements. Our findings suggest that activation steering can serve as a promising direction for in-context learning, thereby opening new avenues for future research",
    "checked": false,
    "id": "5fbc97965c1221975e160193d64d220c21efaec9",
    "semantic_title": "non-iterative calculation of parameters of a linear classifier with a threshold activation function",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rTkhl8Z2Zz": {
    "title": "A Survey on Combating Hate Speech by Detection and Prevention",
    "volume": "review",
    "abstract": "With the rise in social media (SM) platforms that offer easy access, community formation, and online debate, the issue of hate speech has risen rapidly. The hate detection, and countering it becomes a growing challenge to society, researchers, companies, and policymakers. Hate speech is in the form of text or multimodal such as memes, GIFs, audio, or video. The scientific study of hate speech from a computer science view has gained attention in recent years. Mostly it is considered a supervised task where the annotated corpora and shared resources play a big role. To combat it, SM, employing modern AI tools is getting attention. This survey comprehensively examines the work done to combat hate in the English language so far. This structures the state-of-the-art methodologies employed for unimodal identification, studies conducted in multimodal hate identification, the role of Explainable AI, prevention of hate speech through style transfer, and counter-narrative generation for the English language. The efficacy and limitations are also discussed. Compared with the earlier surveys this paper concisely gives a well-organized presentation of the methods to combat hate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4VzHv5s0Hp": {
    "title": "Personalized Preference Optimization for Text-to-Image Generation using Large Language Models",
    "volume": "review",
    "abstract": "Preference optimization is a crucial aspect of generative models, ensuring that the generated content aligns with users' preferences. While previous research has focused on optimizing for average preferences, text-to-image tasks require a personalized approach due to the diversity of individual preferences. In this study, we propose a two-stage framework for personalized preference optimization in text-to-image generation. The first stage, personalized image aesthetic assessment (PIAA), learns user preferences from a small amount of user image rating data. The second stage, prompt optimization, optimizes the text-to-image model's prompt to generate images that receive high scores from the learned preference model. We employ Large Language Models (LLMs) for the prompt optimization process. Through extensive experimentation with various configurations in the PIAA and prompt optimization stages, we demonstrate that our approach can generate novel images that align with individual user preferences, even with limited user data. Our research lays the foundation for future work on personalized content generation",
    "checked": false,
    "id": "cfb9eba1b5c55bb0052df41eaaff8716f9c420bd",
    "semantic_title": "pmg : personalized multimodal generation with large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=StzzP3HLho": {
    "title": "An Empirical Study of Gendered Stereotypes in Emotional Attributes for Bangla in Multilingual Large Language Models",
    "volume": "review",
    "abstract": "With the rapid growth of Large Language models, more and more jobs are being automated by using LLMs. Therefore, it is very important to assess the fairness of LLMs. Studies reveal the reflection of societal norms and biases in LLMs, which creates a risk of propagating societal stereotypes in downstream tasks. Numerous works have been done in various NLP applications regarding bias exhibition via LLMs and more so on gender bias. However there is a gap on the study of bias in emotional attributes, although human emotion and gender are closely related in societal discourse for almost all societies. The gap is even larger for a low resource language like Bangla. Historically women were more associated with emotional responses like empathy, fear or guilt, whereas men were more associated with anger, bravado, authority etc. This resonates with the societal system in areas where Bangla is prevalent. We offer the first thorough investigation of gendered emotion attribution in Bangla for both closed and open source LLMs in this work. Our aim is to elucidate the intricate societal relationship between gender and emotion specifically within the context of Bangla. All of our resources including code and data will be publicly available to support future research on Bangla NLP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p9o4HVN12w": {
    "title": "DOLMA: Visual Instruction Tuning for Document AI",
    "volume": "review",
    "abstract": "The rapid expansion of Vision-Language Models (VLMs) has spurred research into their applicability across various domains. While VLMs excel in understanding environmental contexts, their effectiveness declines with visually-rich scanned documents. Although some VLMs use Optical Character Recognition (OCR) to mitigate this, OCR alone is insufficient for the complex textual and visual insights required. Developing tailored models for Document AI applications also demands substantial labeled data and high training costs. To address these challenges, we conducted experiments with various models, data types, architectures, and training methodologies. Based on our findings, we introduce DOLMA, an OCR-free vision-language model designed for diverse Document AI applications in a zero-shot setting. Despite having a moderate parameter count of 7 billion, DOLMA performs on par with models ten times larger on numerous Document AI benchmarks. The complete model, including weights, training data, and code, is publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UslFMTBEFc": {
    "title": "Towards One-to-Many Visual Question Answering",
    "volume": "review",
    "abstract": "Most existing Visual Question Answering (VQA) systems are constrained to support domain-specific questions, i.e., to train different models separately for different VQA tasks, thus generalizing poorly to others. For example, models trained on the reasoning-focused dataset GQA struggle to effectively handle samples from the knowledge-emphasizing dataset OKVQA. Meanwhile, in real-world scenarios, it is user-unfriendly to restrict the domain of questions. Therefore, this paper proposes a necessary task: One-to-Many Visual Question Answering, of which the ultimate goal is to enable a single model to answer as many different domains of questions as possible by the effective integration of available VQA resources. To this end, we first investigate into ten common VQA datasets, and break the task of VQA down into the integration of three key abilities. Then, considering assorted questions rely on different VQA abilities, this paper proposes a novel dynamic Mixture of LoRAs (MoL) strategy. MoL mixes three individually trained LoRA adapters (corresponding to each VQA ability) dynamically for different samples demanding various VQA abilities. The proposed MoL strategy is verified to be highly effective by experiments, establishing SOTAs on four datasets. In addition, MoL generalizes well to three extra zero-shot datasets. Data and codes will be released",
    "checked": false,
    "id": "b2fcce1e640e761522030e55f3f9ab53e3b14963",
    "semantic_title": "toward unsupervised realistic visual question answering",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=WpJj2Dh1me": {
    "title": "Head-wise Shareable Attention for Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices. Weight sharing is one promising solution that encourages weight reuse, effectively reducing memory usage with less performance drop. However, current weight sharing techniques primarily focus on small-scale models like BERT and employ coarse-grained sharing rules, e.g., layer-wise. This becomes limiting given the prevalence of LLMs and sharing an entire layer or block obviously diminishes the flexibility of weight sharing. In this paper, we present a perspective on head-wise shareable attention for large language models. We further propose two memory-efficient methods that share parameters across attention heads, with a specific focus on LLMs. % to reduce the memory usage for large PLMs. Both of them use the same dynamic strategy to select the shared weight matrices. The first method directly reuses the pre-trained weights without retraining, denoted as $\\textbf{DirectShare}$. The second method first post-trains with constraint on weight matrix similarity and then shares, denoted as $\\textbf{PostShare}$. Experimental results reveal our head-wise shared models still maintain satisfactory capabilities, demonstrating the feasibility of fine-grained weight sharing applied to LLMs",
    "checked": true,
    "id": "ee92723f03475fcfbb37ce8b8c888c497453ceb9",
    "semantic_title": "head-wise shareable attention for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QDEQWe4mqm": {
    "title": "ShareLoRA: Less Tuning, More Performance for LoRA Fine-tuning of LLMs",
    "volume": "review",
    "abstract": "Due to the prohibitively expensive full fine-tuning costs of large language models (LLMs), various popular parameter-efficient fine-tuning (PEFT) methods have been developed. These methods majorly rely on fine-tuning few add-on modules, popularly referred to as {\\it adapters}, that corresponds to only \\textit{small fraction of LLM parameters}. In specific, low rank adaptation (LoRA), has demonstrated impressive parameter efficiency while yielding performance close to the full fine-tuning. However, classical LoRA may still fine-tune more parameters as compared to the inherent rank of the pre-trained weights \\cite{aghajanyan2020intrinsic}, leaving room for further parameter reduction. To mitigate this, only recently, few researches had proposed various freezing strategy of LoRA projection matrices, however, at the cost of additional FLOPs. To improve fine-tuning efficiency, in this work, we present ShareLoRA, that leverages a novel approach to use the redundancy in pre-trained model weights and share LoRA modules to significantly reduce the trainable parameter counts. In specific, \\autolora{} automatically finds the redundancy of the pre-trained weights and determines which LoRA adapters can share parameters. For this, \\autolora{} uses the similarity between representations to assess the information redundancy and a greedy algorithm to maximize the sharing of LoRA modules. We performed extensive evaluations with LLaMA family LLMs across various tasks. In specific, at reduced PEFT parameter count of up to \\textbf{23}$\\%$, ShareLoRA performs similar or better that of the existing PEFT alternatives",
    "checked": false,
    "id": "e923cdf6414b38fc639b015af43ac8aa82109411",
    "semantic_title": "iapt: instruction-aware prompt tuning for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6F7jCU8yeA": {
    "title": "Disentangle to Decay: Linear Attention with Trainable Decay Factor",
    "volume": "review",
    "abstract": "Linear attention enhances inference efficiency of Transformer and has attracted research interests as an efficient backbone of language models. Existing linear attention based models usually exploit decay factor based positional encoding (PE), where attention scores decay exponentially with increasing relative distance. However, most work manually designs a non-trainable base of exponential calculation (decay factor), which limits further optimization. Our analysis reveals that direct training decay factor is unstable. To address this problem, we propose a novel PE for linear attention named Disentangle to Decay (D2D). D2D disentangles decay factor into two parts to achieve further optimization and stable training. Moreover, D2D can be transformed into recurrent form for efficient inference. Experiments demonstrate that D2D achieves stable training of decay factor, and enhances performance of linear attention in both normal context length and length extrapolation scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IqUrqS3k3b": {
    "title": "Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay Scoring Methods based on Linguistically-informed Counterfactuals",
    "volume": "review",
    "abstract": "While current automated essay scoring (AES) methods demonstrate high scoring agreement with human raters, their decision-making mechanisms are not fully understood. Our proposed method, using counterfactual intervention assisted by Large Language Models (LLMs), reveals that BERT-like models primarily focus on sentence-level features, whereas LLMs such as GPT-3.5, GPT-4 and Llama-3 are sensitive to conventions \\& accuracy, language complexity, and organization, indicating a more comprehensive rationale alignment with scoring rubrics. Moreover, LLMs can discern counterfactual interventions when giving feedback on essays. Our approach improves understanding of neural AES methods and can also apply to other domains seeking transparency in model-driven decisions. Access codes and data at anonymous repo during review: \\url{https://anonymous.4open.science/r/beyond-agreement-aes-2024-8321}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nPymxunXjT": {
    "title": "Exploring the Role of Semantic Parsing on Downstream Tasks for Large Language Models",
    "volume": "review",
    "abstract": "Semantic Parsing focuses on converting sentences into structured forms. While previous studies show its benefits for smaller models, the impact on Large Language Models (LLMs) remains under explored. Our paper explores whether integrating Semantic Parsing can enhance LLMs' performance in downstream tasks. Unlike prior approaches, we propose SENSE, adding semantic parsing hint instead results into prompt and find that this approach consistently improves performance across tasks, highlighting the potential of semantic information integration in enhancing LLM capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t9kPsStbHo": {
    "title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
    "volume": "review",
    "abstract": "The concept of *persona*, originally adopted in dialogue literature, has re-surged as a promising framework for tailoring large language models (LLMs) to specific context (*e.g.*, personalized search, LLM-as-a-judge). However, the growing research on leveraging persona in LLMs is relatively disorganized and lacks a systematic taxonomy. To close the gap, we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) *LLM Role-Playing*, where personas are assigned to LLMs, and (2) *LLM Personalization*, where LLMs take care of user personas. Additionally, we introduce existing methods for LLM personality evaluation. To the best of our knowledge, we present the first survey for role-playing and personalization in LLMs under the unified view of persona. We continuously maintain a paper collection to foster future endeavors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GooteJNbmR": {
    "title": "HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild",
    "volume": "review",
    "abstract": "Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains. Recent benchmarks designed to assess LLM hallucinations within conventional NLP tasks, such as knowledge-intensive question answering (QA) and summarization, are insufficient for capturing the complexities of user-LLM interactions in dynamic, real-world settings. To address this gap, we introduce HaluEval-Wild, the first benchmark specifically designed to evaluate LLM hallucinations in the wild. We meticulously collect challenging (adversarially filtered by Alpaca) user queries from ShareGPT, an existing real-world user-LLM interaction datasets, to evaluate the hallucination rates of various LLMs. Upon analyzing the collected queries, we categorize them into five distinct types, which enables a fine-grained analysis of the types of hallucinations LLMs exhibit, and synthesize the reference answers with the powerful GPT-4 model and retrieval-augmented generation (RAG). Our benchmark offers a novel approach towards enhancing our comprehension of and improving LLM reliability in scenarios reflective of real-world interactions",
    "checked": true,
    "id": "03cfdde24c6b9837ad8933cb535a2c4a7c0fd971",
    "semantic_title": "halueval-wild: evaluating hallucinations of language models in the wild",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=VPFgn7AEu1": {
    "title": "TableBench: A Capability-Based Table Benchmark for Large Language Models",
    "volume": "review",
    "abstract": "The rapid advancement of techniques in large language models (LLMs) for processing tabular data necessitates improvements in evaluation benchmarks. However, most of existing table benchmarks offer evaluation from a singular task-based perspective, failing to provide a comprehensive and meticulous assessment of the LLMs' table-related capabilities. To address this gap, we introduce TableBench, a capability-based benchmark tailored to evaluate the performance of LLMs on tabular data. Our framework intricately outlines 10 essential capabilities required from the point a model receives a table-related input to the generation of an output, with each capability tested across 6 table formats. We evaluate 20 models using TableBench and observe that GPT-4 and GPT-4o achieve the highest scores, while phi3-small outperform other open-source models of similar scale. Drawing from our evaluation, we present a series of valuable insights, which can serve as a pivotal reference for future table-related LLM research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q0OBwmXQHd": {
    "title": "$SusGen-GPT$: A Data-Centric LLM for Financial NLP and Sustainability Report Generation",
    "volume": "review",
    "abstract": "The rapid growth of the financial sector and the increasing emphasis on Environmental, Social, and Governance (ESG) considerations have highlighted the need for advanced natural language processing (NLP) tools. Despite significant advancements, there remains a lack of open-source Large Language Models (LLMs) proficient across both general finance and ESG domains, such as generating ESG reports. To address this gap, we propose $SusGen-30k$, a high-quality, category-balanced dataset that comprises seven financial NLP tasks and ESG report generation. Additionally, we propose $TCFD-Bench$, a benchmark designed to enhance the evaluation of sustainability report generation. Employing a data-centric methodology, we developed a suite of models, referred to as $SusGen-GPT$. When trained on our curated dataset, these suites of models achieved state-of-the-art performance, surpassing the benchmarks set by models of significantly larger size. By doing so, we introduce a data-centric approach to effectively address the aforementioned existing challenges, aiming to foster continual development in the financial and ESG research community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PJo8pRbiLy": {
    "title": "Leveraging Cross-Lingual Knowledge from Pre-Trained Models for Low-Resource Neural Machine Translation",
    "volume": "review",
    "abstract": "Neural machine translation (NMT) quality significantly depends on large parallel corpora, making low-resource language translation a challenge. This paper introduces a novel approach that leverages cross-lingual alignment knowledge from multilingual pre-trained language models (PLMs) to enhance low-resource NMT. Our method segments the translation model into source encoding, target encoding, and alignment modules, each initialized with different pre-trained BERT models. Experiments on four translation directions with two low-resource language pairs demonstrate significant BLEU score improvements, validating the efficacy of our approach",
    "checked": false,
    "id": "5495e6f7990423e0f4c73cab1bfdc63b1970eb20",
    "semantic_title": "pre-training synthetic cross-lingual decoder for multilingual samples adaptation in e-commerce neural machine translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N4391IEP0c": {
    "title": "Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation",
    "volume": "review",
    "abstract": "With the rapid advancement of large language models (LLMs) for handling complex language tasks, an increasing number of studies are employing LLMs as agents to emulate the sequential decision-making processes of humans often represented as Markov decision-making processes (MDPs). The actions in MDPs adhere to specific probability distributions and require iterative sampling. This arouses curiosity regarding the capacity of LLM agents to comprehend probability distributions, thereby guiding the agent's behavioral decision-making through probabilistic sampling and generating behavioral sequences. To answer the above question, we divide the problem into two main aspects: sequence simulation with known probability distribution and sequence simulation with unknown probability distribution. Our analysis indicates that LLM agents can understand probabilities, but they struggle with probability sampling. Their ability to perform probabilistic sampling can be improved to some extent by integrating coding tools, but this level of sampling precision still makes it difficult to simulate human behavior as agents",
    "checked": true,
    "id": "1d03bc20b66001f702c163f7abeac2121649ef8b",
    "semantic_title": "do llms play dice? exploring probability distribution sampling in large language models for behavioral simulation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eE2eVW4JFG": {
    "title": "How Do Your Code LLMs perform? Empowering Code Instruction Tuning with Really Good Data",
    "volume": "review",
    "abstract": "Recently, there has been a growing interest in studying how to construct better code instruction tuning data. However, we find Code models trained with these datasets exhibit high performance on HumanEval but perform worse on other benchmarks such as LiveCodeBench. Upon further investigation, we discover many datasets suffer from significant data leakage. After cleaning up most of the leaked data, we find that some datasets previously considered high-quality perform poorly. This discovery reveals a new challenge: identifying which dataset genuinely qualify as high-quality code instruction data. To address this, we propose an efficient code data selection strategy for selecting samples. Our approach is based on three dimensions: instruction complexity, response quality, and instruction diversity. Based on our selected data, we present XCoder, a family of models finetuned from LLaMA3. Experiments show Xcoder achieves new state-of-the-art performance using fewer training data, which verify the effectiveness of our data strategy. Moreover, we perform a comprehensive analysis on the data composition and find existing code datasets have different characteristics according to their construction methods, which provide new insights for future code LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k5skat8pmi": {
    "title": "Controlled Cloze-test Question Generation with Surrogate Models for IRT Assessment",
    "volume": "review",
    "abstract": "Item difficulty plays a crucial role in adaptive testing. However, few works have focused on generating questions of varying difficulty levels, especially for multiple-choice (MC) cloze tests. We propose training pre-trained language models (PLMs) as surrogate models to enable item response theory (IRT) assessment, avoiding the need for human test subjects. We also propose two strategies to control the difficulty levels of the distractors using ranking rules to reduce invalid distractors. Experimentation on a benchmark dataset demonstrates that our proposed framework and methods can effectively control and evaluate the difficulty levels of MC cloze tests",
    "checked": false,
    "id": "416d9636d9cee2bc5013d8202ccfe8adfbd24e31",
    "semantic_title": "controlling cloze-test question item difficulty with plm-based surrogate models for irt assessment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VcXuAzw62v": {
    "title": "YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal Information Extraction",
    "volume": "review",
    "abstract": "The difficulty of the information extraction task lies in dealing with the task-specific label schemas and heterogeneous data structures. Recent work has proposed methods based on large language models to uniformly model different information extraction tasks. However, these existing methods are deficient in their information extraction capabilities for Chinese languages other than English. In this paper, we propose an end-to-end chat-enhanced instruction tuning framework for universal information extraction (YAYI-UIE), which supports both Chinese and English. Specifically, we utilize dialogue data and information extraction data to enhance the information extraction performance jointly. Experimental results show that our proposed framework achieves state-of-the-art performance on Chinese datasets while also achieving comparable performance on English datasets under both supervised settings and zero-shot settings",
    "checked": true,
    "id": "0670e318213958cab2c22f2917768cb33465350e",
    "semantic_title": "yayi-uie: a chat-enhanced instruction tuning framework for universal information extraction",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=bHKYYqJqoo": {
    "title": "Towards Robust Cross-Prompt Essay Trait Scoring: A Generative Model Framework with Ranking Loss",
    "volume": "review",
    "abstract": "Automated Essay Scoring (AES) aims to evaluate the overall quality of essays, while essay trait scoring provides a detailed assessment by assigning separate scores to specific traits. Prompt-specific AES models have shown success, but their application to ``unseen'' prompts remains challenging due to limited prompt and essay diversity, hindering the generalization ability. This paper introduces GenAES, a generative model framework for cross-prompt essay trait scoring, leveraging large language models (LLMs) to augment prompts and essays. GenAES further develops a prompt encoder to manage representations of unseen prompts and introduces a ranking loss to evaluate the similarity of unlabeled generated essays with the source essays. Experimental results show GenAES improves generalization, achieving state-of-the-art performance on the ASAP++ dataset, with 6.5% and 7.3% gains in average QWK scores over prompts and traits, respectively. The generated prompts and essays are released to facilitate future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YpB7vxyFXl": {
    "title": "ZK-GenMed: A Zero-shot Knowledge Generative Medical Large Language Model",
    "volume": "review",
    "abstract": "Advancements in Natural Language Processing (NLP) have led to the development of Large Language Models (LLMs), which have demonstrated remarkable capabilities in various tasks, domains, and settings. These models have demonstrated efficacy in various training and evaluation scenarios, including zero-shot learning and instruction settings. They have been effectively applied to reasoning, summarizing, and answering questions. Moreover, LLMs have been used in various industries, including the medical profession, where they have been used for jobs requiring accuracy, such as answering questions. However, much research hasn't been done on LLMs' potential for resolving medical questions in a zero-shot manner. To close this knowledge gap, we provide a novel framework called ZK-GenMed, which uses LLMs' advantages to produce the information needed to answer medical questions in a zero-shot scenario. This framework combines the generated knowledge with ranking strategies to extract relevant information, meaningfully enabling the model to answer medical questions. Experimental results demonstrate significant improvements, with marginal gains of over 10\\% on various datasets, highlighting the potential of ZK-GenMed for medical question-answering applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OEn7Z6JQdY": {
    "title": "SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance",
    "volume": "review",
    "abstract": "As the development of large language models (LLMs) rapidly advances, securing these models effectively without compromising their utility has become a pivotal area of research. However, current defense strategies against jailbreak attacks (i.e., efforts to bypass security protocols) often suffer from limited adaptability, restricted general capability, and high cost. To address these challenges, we introduce SafeAligner, a methodology implemented at the decoding stage to fortify defenses against jailbreak attacks. We begin by developing two specialized models: the Sentinel Model, which is trained to foster safety, and the Intruder Model, designed to generate riskier responses. SafeAligner leverages the disparity in security levels between the responses from these models to differentiate between harmful and beneficial tokens, effectively guiding the safety alignment by altering the output token distribution of the target model. Extensive experiments show that SafeAligner can increase the likelihood of beneficial tokens, while reducing the occurrence of harmful ones, thereby ensuring secure alignment with minimal loss to generality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yE1NZQy5BA": {
    "title": "Chain of Evidences and Evidence to Generate: Prompting for Context Grounded and Retrieval Augmented Reasoning",
    "volume": "review",
    "abstract": "While chain-of-thoughts (CoT) prompting has revolutionized how LLMs perform reasoning tasks, its current methods and variations (e.g, Self-consistency, ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR) etc.,) suffer from limitations like limited context grounding, hallucination/inconsistent output generation, and iterative sluggishness. To overcome these challenges, we introduce a novel mono/dual-step prompting framework built upon two unique strategies \\textbf{Chain of Evidences (CoE)} and \\textbf{Evidence to Generate (E2G)}. Instead of unverified reasoning claims, our innovative approaches leverage the power of \"evidence for decision making\" by first focusing exclusively on the thought sequences explicitly mentioned in the context which then serve as extracted evidence, guiding the LLM's output generation process with greater precision and efficiency. This simple yet potent approach unlocks the full potential of chain-of-thoughts prompting, facilitating faster, more reliable, and contextually aware reasoning in LLMs. Our framework consistently achieves remarkable results across various knowledge-intensive reasoning and generation tasks, surpassing baseline approaches with state-of-the-art LLMs. For instance, (i) on the LogiQA benchmark using GPT-4, CoE achieves a new state-of-the-art accuracy of 53.8\\%, surpassing CoT by 18\\%, ToT by 11\\%, and CR by 9\\%; (ii) CoE with PaLM-2 outperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points, achieving an F1 score of 83.3 on DROP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SAYWfha12M": {
    "title": "Exploring the Practicality of Generative Retrieval on Dynamic Corpora",
    "volume": "review",
    "abstract": "Benchmarking the performance of information retrieval (IR) is mostly conducted with a fixed set of documents (static corpora). However, in realistic scenarios, this is rarely the case and the documents to be retrieved are constantly updated and added. In this paper, we focus on Generative Retrievals (GR), which apply autoregressive language models to IR problems, and explore their adaptability and robustness in dynamic scenarios. We also conduct an extensive evaluation of computational and memory efficiency, crucial factors for real-world deployment of IR systems handling vast and ever-changing document collections. Our results on the StreamingQA benchmark demonstrate that GR is more adaptable to evolving knowledge (+ 4 -- 11\\%), robust in learning knowledge with temporal information, and efficient in terms of inference FLOPs ($\\times 2$), indexing time ($\\times 6$), and storage footprint ($\\times 4$) compared to Dual Encoders (DE), which are commonly used in retrieval systems. Our paper highlights the potential of GR for future use in practical IR systems within dynamic environments",
    "checked": true,
    "id": "5e52c1c4fc90870df11efd1348f54bc56407d773",
    "semantic_title": "exploring the practicality of generative retrieval on dynamic corpora",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=yGXVC2BqaY": {
    "title": "Commonsense Knowledge Editing Based on Free-Text in LLMs",
    "volume": "review",
    "abstract": "Knowledge editing technology is crucial for maintaining the accuracy and timeliness of large language models (LLMs) . However, the setting of this task overlooks a significant portion of commonsense knowledge based on free-text in the real world, characterized by broad knowledge scope, long content and non instantiation. The editing objects of previous methods (e.g., MEMIT) were single token or entity, which were not suitable for commonsense knowledge in free-text form. To address the aforementioned challenges, we conducted experiments from two perspectives: knowledge localization and knowledge editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT) method, revealing the challenges associated with the distribution of commonsense knowledge in MLP and Attention layers, as well as in decentralized distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which utilizes a Dynamics-aware Module to locate the parameter positions corresponding to commonsense knowledge, and uses Knowledge Editing Module to update knowledge. The DEM method fully explores the potential of the MLP and Attention layers, and successfully edits commonsense knowledge based on free-text. The experimental results indicate that the DEM can achieve excellent editing performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fmum61rEUg": {
    "title": "Beyond Task-Specific Fine-tuning: Exploring the Impact of Diverse Data on LLM Performance",
    "volume": "review",
    "abstract": "This study explores the efficacy of fine-tuning large language models (LLMs) using non-task-specific datasets, challenging the traditional reliance on task-specific datasets. Based on the SUP-NATINST and Stanford Alpaca dataset, this research explores some schemes to enhance the application capabilities of LLMs in specific domains through strategic data selection. The main contributions include an innovative fine-tuning strategy that emphasizes the importance of dataset selection by comparing non-task-specific datasets with task-specific datasets, demonstrating the potential advantages of unconventional datasets in improving model task performance. The study also reveals the model's performance sensitivity to data ratios, challenges the concept of optimal data ratios, and explores the impact of data volume changes on model accuracy. Furthermore, it deepens the understanding of how different task types affect model performance fine-tuning. By analyzing the performance across different tasks and dimensions and dissecting the impact of data ratio changes, the research indicates that non-specific task data allocation can achieve better performance than specific task datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jum5GgEV0h": {
    "title": "The influence of pretraining model's life of orientation leaning on mental health disease detection",
    "volume": "review",
    "abstract": "With the popularity of the Internet, online social media has increasingly become a platform for people to share their attitudes towards life, such as optimism and pessimism about the future. While the Internet embraces various views, it also quietly deepens the formation of impressions on different views and attitudes. A large part of these texts will form the corpus of the pre-trained model, and the model may learn the tendency of life attitudes in the corpus. Our work develops new methods to (1) measure life attitude biases in LMs trained on such corpora and (2) measure the judgement impact of downstream models trained on different life attitude corpus. We focus on mental health disorder detection, aiming to empirically quantify the effects of life attitude (optimism, pessimism) leaning in pretraining data on the influence of risk-related tasks. Our findings reveal that pretrained LMs do have life attitude leanings that reinforce the polarization present in pretraining corpora, propagating life attitude biases into mental health disorder detection.We discussed strategies that might mitigate or leverage models of different life attitude leaning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hIEXCK91Vl": {
    "title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents",
    "volume": "review",
    "abstract": "Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on the harmlessness of LLM-generated content in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging and identifying safety risks given agent interaction records. R-Judge comprises 569 records of multi-turn agent interaction, encompassing 27 key risk scenarios among 5 application categories and 10 risk types. It is of high-quality curation with annotated safety labels and risk descriptions. Evaluation of 11 LLMs on R-Judge shows considerable room for enhancing the risk awareness of LLMs: The best-performing model, GPT-4o, achieves 74.42% while no other models significantly exceed the random. Moreover, we reveal that risk awareness in open agent scenarios is a multi-dimensional capability involving knowledge and reasoning, thus challenging for LLMs. With further experiments, we find that fine-tuning on safety judgment significantly improve model performance while straightforward prompting mechanisms fail. R-Judge is publicly available at Annoymous",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S3hKLXwOZ5": {
    "title": "Key Verbatim Extraction from Clinical Notes: A Hierarchical Multimodal Cross-Attention Approach",
    "volume": "review",
    "abstract": "Clinical notes are essential for physicians to accurately assess patient conditions, particularly in oncology where records are extensive. Efficient and effective information extraction from these notes is crucial for effective treatment. This is not a trivial task due to the lengthy and specialized content in the notes. Current methods that capture token-level or sentence-level relations, which are context-dependent, are sometimes insufficient for knowledge-intensive tasks such as information extraction from EHR that require external knowledge. To address this, we introduce a knowledge-enhanced hierarchical multimodal cross-attention approach. This method employs a cross-attention mechanism to integrate textual knowledge with patient network knowledge, aiming to synthesize information across multiple data levels, including word, sentence, note, and patient levels. This approach can efficiently highlight key sentences in clinical notes. We validate our method using extensive experiments on a large real-world dataset. The results demonstrate that our proposed model outperforms baseline models by up to 4.17% and 2.79% regarding F1 and accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DanhjcTL2T": {
    "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment",
    "volume": "review",
    "abstract": "The use of Large Language Models (LLMs), which demonstrate impressive capabilities in natural language understanding and reasoning, in Embodied AI is a rapidly developing area. As a part of an embodied agent, LLMs are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task disambiguation have been proposed. However, it is difficult to compare them because they work with different data. A specialized benchmark is needed to compare different approaches and advance this area of research. We propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 500 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (human preference, common sense knowledge, safety), with environment descriptions, clarifying questions and answers, and task plans, for a total of 1000 tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MtQiTZCtjq": {
    "title": "Should multiple defendants and charges be treated separately in legal judgment prediction: An exploratory study and dataset",
    "volume": "review",
    "abstract": "Legal judgment prediction offers a compelling method to aid legal practitioners and researchers. However, the research question remains relatively under-explored: Should multiple defendants and charges be treated separately in Legal judgment prediction? To address this, we introduce a new dataset namely multi-person multi-charge prediction MPMCP, and seek the answer by evaluating the performance of several prevailing legal \\acp{LLM} on four practical legal judgment scenarios: (S1) single defendant with a single charge, (S2) single defendant with multiple charges, (S3) multiple defendants with a single charge, and (S4) multiple defendants with multiple charges. We evaluate the dataset across two Legal judgment prediction tasks, i.e., charge prediction and penalty term prediction. We have conducted extensive experiments and found that the scenario involving multiple defendants and multiple charges (S4) poses the greatest challenges, followed by S2, S3, and S1. The impact varies significantly depending on the model. For example, in S4 compared to S1, InternLM2 achieves approximately 4.5\\% lower F1-score and 2.8\\% higher LogD, while Lawformer demonstrates around 19.7\\% lower F1-score and 19.0\\% higher LogD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zbMg3gM1De": {
    "title": "Interpretable Composition Attribution Enhancement for Visio-linguistic Compositional Understanding",
    "volume": "review",
    "abstract": "Contrastively trained vision-language models such as CLIP have achieved remarkable progress in vision and language representation learning. Despite the promising progress, their proficiency in compositional reasoning over attributes and relations (eg, distinguishing between \"the car is underneath the person\" and \"the person is underneath the car\") remains notably inadequate. We investigate the cause for this deficient behavior is the composition attribution issue, where the attribution scores (eg, attention scores or GradCAM scores) for relations (eg, underneath) or attributes (eg, red) in text are substantially lower than those for object terms. In this work, we show such issue is mitigated via a novel framework called CAE (Composition Attribution Enhancement). This generic framework incorporates various interpretable attribution methods to encourages the model to pay greater attention on composition words denoting relationships and attributes within the text. Detailed analysis shows that our approach enables the models to adjust and rectify the attribution on the texts. Extensive experiments across seven benchmarks reveal that our framework significantly enhances the ability to discern intricate details and construct more sophisticated interpretations of combined visual and linguistic elements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l8Oql9HsyK": {
    "title": "POMEM: In-Context Knowledge Post Editing on Massive-Editing Memory in Language Language Models",
    "volume": "review",
    "abstract": "Parameter updating (PU), while being widely used in $\\textit{knowledge editing}$, has still shown limited performances in terms of generalization and locality metrics, likely due to the catastrophic forgetting, the riffle effects, or the unseen contexts. This paper proposes a novel $\\textit{in-context post-editing}$, which is subsequently applied to the PU-based prediction results, namely $\\textbf{POMEM}$ -- In-context knowledge $\\textbf{po}$st $\\textbf{e}$diting on $\\textbf{m}$assive-$\\textbf{e}$diting $\\textbf{m}$emory -- which consists of two different types of in-context post-editing prompting method, divided into the \"in-scope\" and \"out-of-scope\" post-editing methods, shortly referred to as Copier and Recaller, respectively; 1) $\\textbf{Copier}$ is specially designed for in-scope cases, mainly aiming to further enhance the generalization editing ability; 2) $\\textbf{Recaller}$ is designed for out-of-scope cases, which involves a novel \"recalling\" prompt which aims to recover the prediction result of \"original pre-edited\" model under using the PU-based \"edited\" model. Experiment results on Counterfact dataset show that POMEM leads to the state-of-the-art performances. Our codes are publicly available at \\url{https://github.com/XXX/XXX}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dHLogaGMts": {
    "title": "BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models",
    "volume": "review",
    "abstract": "Safety backdoor attacks in large language models (LLMs) enable harmful behaviors to be stealthily triggered while evading detection during normal interactions. The high dimensionality of the trigger search space and the diverse range of potential malicious behaviors in LLMs make this a critical open problem. This paper presents BEEAR, a novel mitigation method based on a key insight: backdoor triggers induce a uniform drift in the model's embedding space, irrespective of the trigger's form or targeted behavior. Leveraging this observation, we introduce a bi-level optimization approach. The inner level identifies universal perturbations to the decoder's embeddings that steer the model towards defender-defined unwanted behaviors; the outer level fine-tunes the model to reinforce safe behaviors against these perturbations. Our experiments demonstrate the effectiveness of this approach, reducing the success rate of safety backdoor attacks from over 95\\% to $<$1\\% for general harmful behaviors and from 47\\% to 0\\% for Sleeper Agents, without compromising the model's helpfulness. Notably, our method relies only on defender-defined sets of safe and unwanted behaviors without any assumptions about the trigger location or attack mechanism. This work represents the first practical framework to counter safety backdoors in LLMs and provides a foundation for future advancements in AI safety and security",
    "checked": true,
    "id": "b15493107b8e3193aae28d818ef265f0e8790f49",
    "semantic_title": "beear: embedding-based adversarial removal of safety backdoors in instruction-tuned language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=4ViDfPhtNJ": {
    "title": "Synergizing Foundation Models and Federated Learning: A Survey",
    "volume": "review",
    "abstract": "The recent development of Foundation Models (FMs), represented by large language models, vision transformers, and multimodal models, has been making a significant impact on both academia and industry. Compared with small-scale models, FMs have a much stronger demand for high-volume data during the pre-training phase. Although general FMs can be pre-trained on data collected from open sources such as the Internet, domain-specific FMs need proprietary data, posing a practical challenge regarding the amount of data available due to privacy concerns. Federated Learning (FL) is a collaborative learning paradigm that breaks the barrier of data availability from different participants. Therefore, it provides a promising solution to customize and adapt FMs to a wide range of domain-specific tasks using distributed datasets whilst preserving privacy. This survey paper discusses the potentials and challenges of synergizing FL and FMs and summarizes core techniques, future directions, and applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ijOjOtCulk": {
    "title": "COFFEE-GYM: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code",
    "volume": "review",
    "abstract": "This paper presents Coffee-Gym, a comprehensive RL environment for training models that provide feedback on code editing. Coffee-Gym includes two major components: (1) Coffee, a dataset containing humans' code edit traces for coding questions and human-written feedback for editing erroneous code; (2) CoffeeEval, a reward function that faithfully reflects the helpfulness of feedback by assessing the performance of the revised code in unit tests. With them, Coffee-Gym addresses the unavailability of high-quality datasets for training feedback models with RL, and provides more accurate rewards than the SOTA reward model (i.e., GPT-4). By applying Coffee-Gym, we elicit feedback models that outperform baselines in enhancing open-source code LLMs' code editing, making them comparable with closed-source LLMs. We make the dataset and the model checkpoint publicly available in https://huggingface.co/spaces/Coffee-Gym/Project-Coffee-Gym",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZzAGwGFC6d": {
    "title": "MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction",
    "volume": "review",
    "abstract": "Molecular property prediction (MPP) is a fundamental and crucial task in drug discovery. However, prior methods are limited by the requirement for a large number of labeled molecules and their restricted ability to generalize for unseen and new tasks, both of which are essential for real-world applications. To address these challenges, we present MolecularGPT for few-shot MPP. From a perspective on instruction tuning, we fine-tune large language models (LLMs) based on curated molecular instructions spanning over 1000 property prediction tasks. This enables building a versatile and specialized LLM that can be adapted to novel MPP tasks without any fine-tuning through zero- and few-shot in-context learning (ICL). MolecularGPT exhibits competitive in-context reasoning capabilities across 10 downstream evaluation datasets, setting new benchmarks for few-shot molecular prediction tasks. More importantly, with just two-shot examples, MolecularGPT can outperform standard supervised graph neural network methods on 4 out of 7 datasets. It also excels state-of-the-art LLM baselines by up to 16.6\\% increase on classification accuracy and decrease of 199.17 on regression metrics (e.g., RMSE) under zero-shot. This study demonstrates the potential of LLMs as effective few-shot molecular property predictors. Our model and curated instruction set will be open-sourced",
    "checked": true,
    "id": "443e5490e7f9fedf912a330fc7a0456d4247ab3d",
    "semantic_title": "moleculargpt: open large language model (llm) for few-shot molecular property prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EYKzVrsXPt": {
    "title": "LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation",
    "volume": "review",
    "abstract": "The rise of multimodal misinformation on social platforms poses significant challenges for individuals and societies. Its increased credibility and broader impact make detection more complex, requiring robust reasoning across diverse media types and profound knowledge for accurate verification. The emergence of Large Vision Language Model (LVLM) offers a potential solution to this problem. Leveraging their proficiency in processing visual and textual information, LVLM demonstrates promising capabilities in recognizing complex information and exhibiting strong reasoning skills. We investigate the potential of LVLM on multimodal misinformation detection and find that even though LVLM has a superior performance compared to LLMs, its profound reasoning may present limited power with a lack of evidence. Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation. LEMMA leverages LVLM intuition and reasoning capabilities while augmenting them with external knowledge to enhance the accuracy of misinformation detection. Our external knowledge extraction module adopts multi-query generation and image source tracing to enhance the rigor and comprehensiveness of LVLM's reasoning. We observed that LEMMA improves the accuracy over the top baseline LVLM by 9% and 13% on Twitter and Fakeddit datasets respectively",
    "checked": true,
    "id": "00c6a500726a85090e1b2e7e8f4c5226ff56d86d",
    "semantic_title": "lemma: towards lvlm-enhanced multimodal misinformation detection with external knowledge augmentation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=01F6uQeZeN": {
    "title": "Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups",
    "volume": "review",
    "abstract": "Complex Word Identification (CWI) is an important step in the lexical simplification task and has recently become a task on its own. Some variations of this binary classification task have emerged, such as lexical complexity prediction (LCP) and complexity evaluation of multi-word expressions (MWE). Large language models (LLMs) recently became popular in the Natural Language Processing community because of their versatility and capability to solve unseen tasks in zero/few-shot settings. Our work investigates LLM usage, specifically open-source models such as Llama 2, Llama 3, and Vicuna v1.5, and closed-source, such as ChatGPT-3.5-turbo and GPT-4o, in the CWI, LCP, and MWE settings. We evaluate zero-shot, few-shot, and fine-tuning settings and show that LLMs struggle in certain conditions or achieve comparable results against existing methods. In addition, we provide some views on meta-learning combined with prompt learning. In the end, we conclude that the current state of LLMs cannot or barely outperform existing methods, which are usually much smaller",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iMNstxxsWo": {
    "title": "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown promising performance in text-to-SQL, which involves translating natural language questions into SQL queries. However, current text-to-SQL LLMs are computationally expensive and challenging to deploy in real-world applications, highlighting the importance of compressing them. To achieve this goal, knowledge distillation (KD) is a common approach, which aims to distill the larger teacher model into a smaller student model. While numerous KD methods for autoregressive LLMs have emerged recently, it is still under-explored whether they work well in complex text-to-SQL scenarios. To this end, we conduct a series of analyses and reveal that these KD methods generally fall short in balancing performance and efficiency. In response to this problem, we propose to improve the KD with imperfect data, namely KID, which effectively boosts the performance without introducing much training budget. The core of KID is to efficiently mitigate the training-inference mismatch by simulating the cascading effect of inference in the imperfect training data. Extensive experiments on 5 text-to-SQL benchmarks show that, KID can not only achieve consistent and significant performance gains (up to +5.83% average score) across all model types and sizes, but also effectively improve the training efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CE07aE1kTR": {
    "title": "GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization",
    "volume": "review",
    "abstract": "News summarization in today's global scene can be daunting with its flood of multilingual content and varied viewpoints from different sources. However, current studies often neglect such real-world scenarios as they tend to focus solely on either single-language or single-document tasks. To bridge this gap, we aim to unify Multi-lingual, Cross-lingual and Multi-document Summarization into a novel task, i.e., MCMS, which encapsulates the real-world requirements all-in-one. Nevertheless, the lack of a benchmark inhibits researchers from adequately studying this invaluable problem. To tackle this, we have meticulously constructed the GLOBESUMM dataset by first collecting a wealth of multilingual news reports and restructuring them into event-centric format. Additionally, we introduce the method of protocol-guided prompting for high-quality and cost-effective reference annotation. In MCMS, we also highlight the challenge of conflicts between news reports, in addition to the issues of redundancies and omissions, further enhancing the complexity of GLOBESUMM. Through extensive experimental analysis, we validate the quality of our dataset and elucidate the inherent challenges of the task. We firmly believe that GLOBESUMM, given its challenging nature, will greatly contribute to the multilingual communities and the evaluation of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yvqWdJqYN1": {
    "title": "Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have been revolutionizing a myriad of natural language processing tasks with their diverse zero-shot capabilities. Indeed, existing work has shown that LLMs can be used to great effect for many tasks, such as information retrieval (IR), and passage ranking. However, current state-of-the-art results heavily lean on the capabilities of the LLM being used. Currently, proprietary, and very large LLMs such as GPT-4 are the highest performing passage re-rankers. Hence, users without the resources to leverage top of the line LLMs, or ones that are closed source, are at a disadvantage. In this paper, we investigate the use of a pre-filtering step before passage re-ranking in IR. Our experiments show that by using a small number of human generated relevance scores, coupled with LLM relevance scoring, it is effectively possible to filter out irrelevant passages before re-ranking. Our experiments also show that this pre-filtering then allows the LLM to perform significantly better at the re-ranking task. Indeed, our results show that smaller models such as Mixtral can become competitive with much larger proprietary models (e.g., ChatGPT and GPT-4)",
    "checked": true,
    "id": "8678ac94e115f67cb0cef698f573942882cfbc5c",
    "semantic_title": "re-ranking step by step: investigating pre-filtering for re-ranking with large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uH2AUUscub": {
    "title": "Large Language Model is a Better Context Extractor for Aspect-Based Sentiment Analysis",
    "volume": "review",
    "abstract": "Previous Aspect-Based Sentiment Analysis (ABSA) studies have often incorporated syntactic information to connect contextual details with the designated aspect. These methods rely on complex model design to obtain syntactic structure information, further acquiring crucial semantic insights. Considering the potent contextualization abilities of the Large Language Model (LLM), we present the Low-Rank Adaptation plus In-domain Dynamic Examplar (LoRA-IDE) framework. This framework effectively aligns the task and sentence context information with the target aspect, leveraging the power of LLM. Specifically, we employ the LoRA training strategy to enable LLM to learn the context information of ABSA and promote the model's understanding of the connection between sentence context and aspects through the use of curated, designed instructions with IDE. Experimental results demonstrate that our proposed approach not only improves the performance of LLM on ABSA but also outperforms the current state-of-the-art model on two benchmarks at a large scale. The codes will be released upon the acceptance of this paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fzhUuW3kzO": {
    "title": "Augmenting Document-level Relation Extraction with Efficient Multi-Supervision",
    "volume": "review",
    "abstract": "Despite its popularity in sentence-level relation extraction, distantly supervised data is rarely utilized by existing work in document-level relation extraction due to its noisy nature and low information density. Among its current applications, distantly supervised data is mostly used as a whole for pertaining, which is of low time efficiency. To fill in the gap of efficient and robust utilization of distantly supervised training data, we propose Efficient Multi-Supervision for document-level relation extraction, in which we first select a subset of informative documents from the massive dataset by combining distant supervision with expert supervision, then train the model with Multi-Supervision Ranking Loss that integrates the knowledge from multiple sources of supervision to alleviate the effects of noise. The experiments demonstrate the effectiveness of our method in improving the model performance with higher time efficiency than existing baselines",
    "checked": true,
    "id": "5bc1c611be14665d4c7086f7571323fa60c3d5c7",
    "semantic_title": "augmenting document-level relation extraction with efficient multi-supervision",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ozR7qitndD": {
    "title": "{MOPO: Multi-Objective Prompt Optimization for Affective Text Generation",
    "volume": "review",
    "abstract": "Emotions are expressed differently and in different context and domain. On Twitter, for instance, an author might simply use the hashtag \\#anger to express this affective state, while in a news headline, such an emotion needs to be written in a polite manner. To enable conditional text generation models to create appropriate texts, users need a parameter that allows them to choose the appropriate way to express an emotion. To achieve this, we introduce MOPO, a Multi-Objective Prompt Optimization methodology. MOPO optimizes prompts according to multiple objectives and outputs a set of prompts for different preference weightings of the objectives. Users can then choose the most appropriate prompt for their context. We evaluate MOPO using three objectives, determined by various domain-specific emotion classifiers. Our findings indicate that MOPO improves performance by up to 15 pp across all objectives with only a minimal loss (1--2 pp) for any single objective compared to single-objective optimization. These minor performance losses are offset by a broader generalization across multiple objectives -- which is not possible with single-objective optimization. Additionally, MOPO reduces computational resources by simultaneously optimizing for multiple objectives, eliminating separate optimizations for each objective",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zbLxByF6Sg": {
    "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "Recent advancements in Large Language Models have transformed ML/AI development, necessitating a reevaluation of AutoML principles for the Retrieval-Augmented Generation (RAG) systems. To address the challenges of hyper-parameter optimization and online adaptation in RAG, we propose the AutoRAG-HP framework, which formulates the hyper-parameter tuning as an online multi-armed bandit (MAB) problem and introduces a novel two-level Hierarchical MAB (Hier-MAB) method for efficient exploration of large search spaces. We conduct extensive experiments on tuning hyper-parameters, such as top-k retrieved documents, prompt compression ratio, and embedding methods, using the ALCE-ASQA and Natural Questions datasets. Our evaluation from jointly optimization all three hyper-parameters demonstrate that MAB-based online learning methods can achieve Recall@5 $\\approx 0.8$ for scenarios with prominent gradients in search space, using only $\\sim20$% of the LLM API calls required by the Grid Search approach. Additionally, the proposed Hier-MAB approach outperforms other baselines in more challenging optimization scenarios",
    "checked": true,
    "id": "758881985475e137439da465fadf968aead68c4c",
    "semantic_title": "autorag-hp: automatic online hyper-parameter tuning for retrieval-augmented generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3sFwiyoWNg": {
    "title": "LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models",
    "volume": "review",
    "abstract": "In prompt tuning, a prefix or suffix text is added to the prompt, and the embeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix are optimized to gain more control over language models for specific tasks. This approach eliminates the need for hand-crafted prompt engineering or explicit model fine-tuning. Prompt tuning is significantly more parameter-efficient than model fine-tuning, as it involves optimizing partial inputs of language models to produce desired outputs. In this work, we aim to further reduce the amount of trainable parameters required for a language model to perform well on specific tasks. We propose Low-rank Prompt Tuning (LoPT), a low-rank model for prompts that achieves efficient prompt optimization. The proposed method demonstrates similar outcomes to full parameter prompt tuning while reducing the number of trainable parameters by a factor of 5. It also provides promising results compared to the state-of-the-art methods that would require 10 to 20 times more parameters",
    "checked": true,
    "id": "43033574192c5e37a2ea11f192cb1518ce6a06dc",
    "semantic_title": "lopt: low-rank prompt tuning for parameter efficient language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i2zEZWQJAY": {
    "title": "Dynamic Prefix as Instructor for Incremental Named Entity Recognition: A Unified Seq2Seq Generation Framework",
    "volume": "review",
    "abstract": "The Incremental Named Entity Recognition (INER) task aims to update a model to extract entities from an expanding set of entity type candidates due to concerns related to data privacy and scarcity. However, conventional incremental learning methods for INER often suffer from the catastrophic forgetting problem, which leads to the degradation of the model's performance on previously encountered entity types. In this paper, we propose a parameter-efficient dynamic prefix method and formalize INER as a unified seq2seq generation task. By employing the dynamic prefix as a task instructor to guide the generative model, our approach can preserve task-invariant knowledge while adapting to new entities with minimal parameter updates, making it particularly effective in low-resource scenarios. Additionally, we design a generative label augmentation strategy and a novel self-entropy loss to balance the stability and plasticity of the model. Empirical experiments on NER benchmarks demonstrate the effectiveness of our proposed method in addressing the challenges associated with INER",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F0rlA94Rj2": {
    "title": "A View of Large Language Models in HPC: Challenges and Opportunities",
    "volume": "review",
    "abstract": "There is a growing interest in using machine learning techniques to automate and improve the process of generating code. With the rapid development of large language models (LLMs), various models have been created to help write and optimize code. However, they do not yet meet the stringent requirements of high-performance computing (HPC), where highly optimized and efficient code is essential. This paper explores the research direction of adapting and using LLMs for HPC code generation. We present the reasoning behind our position and suggest how existing ideas can be adapted and enhanced to meet the demands of HPC applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9E8IZ8dOuA": {
    "title": "MC-MKE: A Fine-Grained Multimodal Knowledge Editing Benchmark Emphasizing Modality Consistency",
    "volume": "review",
    "abstract": "Multimodal large language models (MLLMs) are prone to non-factual or outdated knowledge issues, which can manifest as misreading and misrecognition errors due to the complexity of multimodal knowledge. Previous benchmarks have not systematically analyzed the performance of editing methods in correcting these two error types. To better represent and correct these errors, we decompose multimodal knowledge into its visual and textual components. Different error types correspond to different editing formats, which edits distinct part of the multimodal knowledge. We present MC-MKE, a fine-grained **M**ultimodal **K**nowledge **E**diting benchmark emphasizing **M**odality **C**onsistency. Our benchmark facilitates independent correction of misreading and misrecognition errors by editing the corresponding knowledge component. We evaluate three multimodal knowledge editing methods on MC-MKE, revealing their limitations, particularly in terms of modality consistency. Our work highlights the challenges posed by multimodal knowledge editing and motivates further research in developing effective techniques for this task",
    "checked": true,
    "id": "8efecf2a2192c81be47fb0214c14b76aa9472ced",
    "semantic_title": "mc-mke: a fine-grained multimodal knowledge editing benchmark emphasizing modality consistency",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v6ivlRk0i5": {
    "title": "Claim Check-Worthiness Detection: How Well do LLMs Grasp Annotation Guidelines?",
    "volume": "review",
    "abstract": "The rising threat of disinformation underscores the need to fully or partially automate the fact-checking process. Identifying text segments requiring fact-checking is known as claim detection (CD) and claim check-worthiness detection (CW), the latter incorporating complex domain-specific criteria of worthiness and often framed as a ranking task. Zero- and few-shot LLM prompting is an attractive option for both tasks, as it bypasses the need for labeled datasets and allows verbalized claim and worthiness criteria to be directly used for prompting. We evaluate the LLMs' predictive accuracy and accuracy on five CD/CW datasets from diverse domains, each utilizing a different worthiness criterion. We examine two key aspects: (1) how to best distill factuality and worthiness criteria into a prompt, and (2) how much context to provide for each claim. To this end, we experiment with different levels of prompt verbosity and varying amounts of contextual information given to the model. We additionally evaluate the top-performing models with ranking metrics, resembling prioritization done by fact-checkers. Our results show that optimal prompt verbosity varies, meta-data alone adds more performance boost than co-text, and confidence scores can be directly used to produce reliable check-worthiness rankings",
    "checked": true,
    "id": "295f118d7ebfa0f0ee4066249fe4c8277b6ad042",
    "semantic_title": "claim check-worthiness detection: how well do llms grasp annotation guidelines?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=XX29vP3tfY": {
    "title": "PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning",
    "volume": "review",
    "abstract": "Large language models (LLMs) have played a fundamental role in various natural language processing tasks with powerful prompt techniques. However, in real-world applications, there are often similar prompt components for repeated queries, which causes significant computational burdens during inference. Existing prompt compression and direct fine-tuning methods aim to tackle these challenges, yet they frequently struggle to strike an optimal balance between cost-efficiency and performance effectiveness, especially in complex tasks such as NL2Code. In this paper, we propose a novel method namely PromptIntern to internalize the prompt knowledge into model parameters via progressive fine-tuning. Our method enables LLMs to emulate the human learning process for a new task, where detailed templates and examples in a prompt are gradually internalized and phased out progressively as the model grows accustomed to the task. Extensive experiments demonstrate that our method can reduce the inference tokens by 67-90\\%, saves 39-90\\% cost, and speedups inference by 1.1-5.1x",
    "checked": true,
    "id": "e7e35bf7e359535b75344e9ba7fb9c6224ffd77b",
    "semantic_title": "promptintern: saving inference costs by internalizing recurrent prompt during large language model fine-tuning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lT9xK9pfLp": {
    "title": "Towards Multimodal Question Answering in Educational Domain",
    "volume": "review",
    "abstract": "The proliferation of educational videos on the Internet has changed the educational landscape by enabling students to learn complex concepts at their own pace. Our work outlines the vision of an automated tutor â€“ a multimodal QA system to answer questions from students watching a video. This can make doubt resolution faster and further improve learning experience. In this work, we take first steps towards building such a QA system. We curate and release a dataset named EduVidQA, with 3,158 videos and 18,474 QA-pairs. However, building and evaluating a QA system proves challenging, because (1) existing evaluation metrics do not correlate with human judgments, and (2) a student question could be answered in many different ways, and training on a single gold answer often confuses the model and makes it worse. We conclude with important research questions to develop this research area further",
    "checked": false,
    "id": "b5c62408df08da36d9cdfd38886193b451157a3b",
    "semantic_title": "eduvqa â€“ visual question answering: an educational perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bk8hNalWin": {
    "title": "Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection",
    "volume": "review",
    "abstract": "The robustness of AI-content detection models against sophisticated adversarial strategies, such as paraphrasing or word switching, is a rising concern in natural language generation (NLG) applications. This study proposes a novel token-ensemble generation strategy to challenge the robustness of current AI-content detection approaches by utilizing multiple sets of candidate generative large language models (LLMs). By randomly sampling token(s) from candidate language model sets, we find the token-ensemble approach significantly drops the performance of AI-content detection models. We evaluate the text quality produced under different token-ensemble settings based on annotations from hired human experts. We proposed a fine-tuned Llama2 model to distinguish the token-ensemble-generated text more accurately. Our findings underscore our proposed text generation approach's great potential in deceiving and improving detection models. This study's datasets, codes, and annotations are open-sourced",
    "checked": true,
    "id": "658c852761dda1372c50d0698093532518c92387",
    "semantic_title": "token-ensemble text generation: on attacking the automatic ai-generated text detection",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=vWYvYGLm3z": {
    "title": "CAMEL: Counterfactuals As a Means for EvaLuating faithfulness of attribution methods in causal language models",
    "volume": "review",
    "abstract": "Despite the widespread adoption of decoder-only autoregressive language models, explainability evaluation research has predominantly focused on encoder-only models, specifically masked language models (MLMs). Evaluating the faithfulness of an explanation methodâ€”how accurately the method explains the inner workings and decision-making of the modelâ€”is very challenging because it is very hard to separate the model from its explanation. Most faithfulness evaluation techniques corrupt or remove some input tokens considered important according to a particular attribution (feature importance) method and observe the change in the model's output. While these faithfulness evaluation techniques are suitable for MLMs, as they involve corrupted or masked inputs during pretraining, they create out-of-distribution inputs for CLMs due to the fundamental difference in their training objective of next token prediction. In this study, we propose a technique that leverages counterfactual generation to evaluate the faithfulness of attribution methods for autoregressive language modeling scenarios. Our technique creates natural, fluent, and in-distribution counterfactuals, something that we show is important for a faithfulness evaluation method. We apply our method to several attribution methods and evaluate their faithfulness in predicting the important tokens of a few large language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=57QSR3rrZo": {
    "title": "Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization",
    "volume": "review",
    "abstract": "Large language models (LLMs) have revolutionized the role of AI, yet pose potential social risks. To steer LLMs towards human preference, alignment technologies have been introduced and gained increasing attention. Nevertheless, existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy positive responses that are barely distinguishable from negative ones. Given recent LLMs' proficiency in generating helpful responses, this work pivots towards a new research question: **can we achieve alignment using solely human-annotated negative samples, preserving helpfulness while reducing harmfulness?** For this purpose, we propose Distributional Dispreference Optimization (D$^2$O), which maximizes the discrepancy between dispreferred responses and the generated non-negative ones. In this way, D$^2$O effectively eschews harmful information without incorporating noisy positive samples, while avoiding collapse using self-generated responses as anchors. We demonstrate that D$^2$O can be regarded as learning a distributional preference model reflecting human dispreference against negative responses, which is theoretically an upper bound of the instance-level DPO. Extensive experiments manifest that our method achieves comparable generation quality and surpasses the latest strong baselines in producing less harmful and more informative responses with better training stability and faster convergence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TUb54qvNV9": {
    "title": "Tokenization Falling Short: The Curse of Tokenization",
    "volume": "review",
    "abstract": "Language models typically tokenize raw text into sequences of subword identifiers from a predefined vocabulary, a process inherently sensitive to typographical errors, length variations, and largely oblivious to the internal structure of tokensâ€”issues we term the curse of tokenization. In this study, we delve into these drawbacks and demonstrate that large language models (LLMs) remain susceptible to these problems. This study systematically investigates these challenges and their impact on LLMs through three critical research questions: (1) complex problem solving, (2) token structure probing, and (3) resilience to typographical variation. Our findings reveal that scaling model parameters can mitigate the issue of tokenization; however, LLMs still suffer from biases induced by typos and other text format variations. Our experiments show that subword regularization such as BPE-dropout can mitigate this issue. We will release our code and data to facilitate further research",
    "checked": true,
    "id": "b81281c0a5d340e908a7e348b0a0521c069131f2",
    "semantic_title": "tokenization falling short: the curse of tokenization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ti4SbPA3a2": {
    "title": "A LLM-based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation",
    "volume": "review",
    "abstract": "The proliferation of misinformation and harmful narratives in online discourse has underscored the critical need for effective Counter Narrative (CN) generation techniques. However, existing automatic evaluation methods often lack interpretability and fail to capture the nuanced relationship between generated CNs and human perceptions. Aiming to achieve a higher correlation with human judgments, this paper proposes a novel approach to asses generated CNs that consists on the use of a Large Language Model (LLM) as a evaluator. By comparing generated CNs pairwise in a tournament-style format, we establish a model ranking pipeline that achieves a correlation of 0.88 with human preference. As an additional contribution, we leverage LLMs as zero-shot CN generators and conduct a comparative analysis of chat, instruct, and base models, exploring their respective strengths and limitations. Through meticulous evaluation, including fine-tuning experiments, we elucidate the differences in performance and responsiveness to domain-specific data. We conclude that chat-aligned models in zero-shot are the best option for carrying out the task, provided they do not refuse to generate an answer due to security concerns",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U338AuBmpo": {
    "title": "MedThink: Explaining Medical Visual Question Answering via Multimodal Decision-Making Rationale",
    "volume": "review",
    "abstract": "Medical Visual Question Answering (MedVQA) provides language responses to image-based medical inquiries, facilitating more accurate diagnoses. However, existing MedVQA methods lack interpretability and transparency. To address this, we introduce a semi-automated annotation process and create new benchmark datasets, R-RAD and R-SLAKE, incorporating multimodal language models and human annotations. Additionally, we develop a framework, MedThink, to fine-tune lightweight generative models with medical decision-making rationales. This framework employs three distinct strategies to generate decision outcomes and corresponding rationales, effectively showcasing the medical decision-making process during reasoning. MedThink achieves 83.5\\% accuracy on R-RAD and 86.3\\% on R-SLAKE, outperforming current baselines. Dataset and code will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yt0GQNJNDs": {
    "title": "CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays",
    "volume": "review",
    "abstract": "Existing rhetorical understanding and generation datasets or corpora primarily focus on single coarse-grained categories or fine-grained categories, neglecting the common interrelations between different rhetorical devices by treating them as independent sub-tasks. In this paper, we propose the $\\textbf{C}$hinese $\\textbf{E}$ssay $\\textbf{R}$hetoric $\\textbf{D}$ataset (CERD), consisting of 4 commonly used coarse-grained categories including metaphor, personification, hyperbole and parallelism and 23 fine-grained categories across both form and content levels. CERD is a manually annotated and comprehensive Chinese rhetoric dataset with five interrelated sub-tasks. Unlike previous work, our dataset aids in understanding various rhetorical devices, recognizing corresponding rhetorical components, and generating rhetorical sentences under given conditions, thereby improving the author's writing proficiency and language usage skills. Extensive experiments are conducted to demonstrate the interrelations between multiple tasks in CERD, as well as to establish a benchmark for future research on rhetoric. The experimental results indicate that Large Language Models achieve the best performance across most tasks, and jointly fine-tuning with multiple tasks further enhances performance. The dataset and code will be released in a future version",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aEZgQVIFaP": {
    "title": "Holistic Evaluation for Interleaved Text-and-Image Generation",
    "volume": "review",
    "abstract": "Interleaved text-and-image generation has been an intriguing research direction, where the models are required to generate both images and text pieces in an arbitrary order. Despite the emerging advancements in interleaved generation, the progress in its evaluation still significantly lags behind. Existing evaluation benchmarks do not support arbitrarily interleaved images and text for both inputs and outputs, and they only cover a limited number of domains and use cases. Also, current works predominantly use similarity-based metrics which fall short in assessing the quality in open-ended scenarios. To this end, we introduce InterleavedBench, the first benchmark carefully curated for the evaluation of interleaved text-and-image generation. InterleavedBench features a rich array of tasks to cover diverse real-world use cases. In addition, we present InterleavedEval, a strong reference-free metric powered by GPT-4o to deliver accurate and explainable evaluation. We carefully define five essential evaluation aspects for InterleavedEval, including text quality, perceptual quality, image coherence, text-image coherence, and helpfulness, to ensure a comprehensive and fine-grained assessment. Through extensive experiments and rigorous human evaluation, we show that our benchmark and metric can effectively evaluate the existing models with a strong correlation with human judgments surpassing previous reference-based metrics. We also provide substantial findings and insights to foster future research in interleaved generation and its evaluation",
    "checked": true,
    "id": "1b6f04a3cfab8804a1d16b83f1853fdd0b682865",
    "semantic_title": "holistic evaluation for interleaved text-and-image generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=KMb5KB4jDq": {
    "title": "C-LLM: Learn to Check Chinese Spelling Errors Character by Character",
    "volume": "review",
    "abstract": "Chinese Spell Checking (CSC) aims to detect and correct spelling errors in sentences. Despite Large Language Models (LLMs) exhibit robust capabilities and are widely applied in various tasks, their performance on CSC is often unsatisfactory. We find that LLMs fail to meet the Chinese character-level constraints of the CSC task, namely equal length and phonetic similarity, leading to a performance bottleneck. Further analysis reveal that this issue stems from the granularity of tokenization, as current mixed character-word tokenization struggles to satisfy these character-level constraints. To address this issue, we propose C-LLM, a Large Language Model-based Chinese Spell Checking method that learns to check errors Character by Character. Character-level tokenization enables the model to learn character-level alignment, effectively mitigating issues related to character-level constraints. Furthermore, CSC is simplified to replication-dominated and substitution-supplemented tasks. Experiments on two CSC benchmarks demonstrate that C-LLM achieves a 2.1\\% enhancement in general scenarios and a significant 12\\% improvement in vertical domain scenarios compared to existing methods, establishing state-of-the-art performance",
    "checked": true,
    "id": "4b6749c981de4c3f519ef18749ecd3059abbff32",
    "semantic_title": "c-llm: learn to check chinese spelling errors character by character",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8QvzkkuM1T": {
    "title": "CoAct: A Global-Local Hierarchy for Autonomous Agent Collaboration",
    "volume": "review",
    "abstract": "Existing LLMs exhibit remarkable performance on various NLP tasks, but still struggle with complex real-world tasks, even equipped with advanced strategies like CoT and ReAct. In this work, we propose the CoAct framework, which transfers the hierarchical planning and collaboration patterns in human society to LLM systems. Specifically, our CoAct framework involves two agents: (1) A global planning agent, to comprehend the problem scope, formulate macro-level plans and provide detailed sub-task descriptions to local execution agents, which serves as the initial rendition of a global plan. (2) A local execution agent, to operate within the multi-tier task execution structure, focusing on detailed execution and implementation of specific tasks within the global plan. Experimental results on the WebArena benchmark show that CoAct can re-arrange the process trajectory when facing failures, and achieves superior performance over baseline methods on long-horizon web tasks",
    "checked": true,
    "id": "e93f1fdaecd63298d60fd2574ae4f241912edb37",
    "semantic_title": "coact: a global-local hierarchy for autonomous agent collaboration",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nMQgHmhKDD": {
    "title": "LegalViz: Legal Text Visualization by Text To Diagram Generation",
    "volume": "review",
    "abstract": "Legal documents including judgments, court orders, government ordinances, professional papers, and textbooks of judicial examinations require highly sophisticated legal knowledge for understanding. To disclose expert knowledge for non-experts, we explore the problem of visualizing legal texts with easy-to-understand diagrams and propose a novel dataset of LegalViz with 23 languages and 5,580 cases of legal document and visualization pairs, using the DOT graph description language of Graphviz. LegalViz provides a simple diagram from a complicated legal corpus identifying legal entities, rules, statements, and transactions at a glance, that are important in each judgment. In addition, we provide a new evaluation approach for the legal diagram visualization by considering the graph and text similarities. We conducted empirical studies on few-shot and finetuning large language models for generating legal diagrams and evaluated them with the graph and text evaluation metrics by each model in 23 languages and confirmed the effectiveness of our dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XHLVUIbnJh": {
    "title": "Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems",
    "volume": "review",
    "abstract": "End-to-end Task-Oriented Dialog (TOD) systems typically require extensive training datasets to perform well. In contrast, large language model (LLM) based TOD systems can excel even with limited data due to their ability to learn tasks through in-context exemplars. However, these models lack alignment with the style of responses in training data and often generate comprehensive responses, making it difficult for users to grasp the information quickly. In response, we propose *SyncTOD* that synergizes LLMs with task-specific hints to improve alignment in low-data settings. *SyncTOD* employs small auxiliary models to provide hints and select exemplars for in-context prompts. With *ChatGPT*, *SyncTOD* achieves superior performance compared to LLM-based baselines and SoTA models in low-data settings, while retaining competitive performance in full-data settings",
    "checked": true,
    "id": "d4dcd6eaab30e47ed3ba526663bdaa99e74a16e4",
    "semantic_title": "synergizing in-context learning with hints for end-to-end task-oriented dialog systems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t9dGSDTtFx": {
    "title": "Enhancing Zero-shot Emotion Perception in Conversation through the Internal-to-External Chain-of-Thought",
    "volume": "review",
    "abstract": "An excellent emotional dialogue model needs to rapidly adapt to new scenarios and perform emotion analysis to meet rapidly changing demands. Therefore, enhancing the model's zero-shot emotion-related capabilities in the dialogue domain has become a new challenge. However, current research shows that large language models (LLMs) perform poorly in zero-shot emotion-related tasks and the Emotion Recognition in Conversations (ERC) task alone doesn't comprehensively reflect the model's emotion understanding capabilities. In this paper, we propose an Emotion Perception in Conversation (EPC) task, which includes both ERC and Emotion Inference in Conversations (EIC), to evaluate the model's emotion perception capabilities in dialogue comprehensively. We propose an Internal-to-External Chain-of-Thought (IoECoT) method for the EPC task. This is a plug-and-play method that first extracts personality information of the dialogue participants from the dialogue history as internal factors influencing emotions, and then uses the sentiment polarity of the historical utterances as external factors. Finally, emotions are perceived by combining internal and external factors. Additionally, we conduct extensive experiments, and the results show that IoECoT significantly outperforms other baselines across multiple models and datasets, demonstrating that IoECoT effectively enhances the emotion perception capabilities of LLMs in zero-shot scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8eMZBseusC": {
    "title": "Playing with News Context for Algorithmic Trading",
    "volume": "review",
    "abstract": "The application of reinforcement learning for algorithmic trading in the spot market using numerical data is a well-studied problem. However, news data consists of hard-to-quantify information which the investors use to base their trading decisions. Thus factoring in news data for algorithmic trading can improve the trading performance of the RL agent. This paper proposes an RL-based framework that performs algorithmic trading in the futures market by combining news data and price data. We propose two approaches for representing the context of the news data: sentiment-aware approach and context-aware approach. We investigate the effect of these approaches on the trading performance of the RL agent. We further compare the performance of on-policy and off-policy RL algorithms. The models are evaluated by trading in the NIFTY 50 index. The evaluation of the models show that using context-aware approach for representation of news data significantly improves the return (\\%) and also reduces the maximum drawdown of the trading model during a trading session",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KtaD0vl1e7": {
    "title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across various tasks, but their performance is highly sensitive to the prompts utilized. This variability poses challenges for accurate assessment and user satisfaction. Current research frequently overlooks instance-level prompt variations and their implications on subjective evaluations. To address these shortcomings, we introduce \\textbf{ProSA}, a framework designed to evaluate and comprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity metric, \\texttt{PromptSensiScore}, and leverages decoding confidence to elucidate underlying mechanisms. Our extensive study, spanning multiple tasks, uncovers that prompt sensitivity fluctuates across datasets and models, with larger models exhibiting enhanced robustness. We observe that few-shot examples can alleviate this sensitivity issue, and subjective evaluations are also susceptible to prompt sensitivities, particularly in complex, reasoning-oriented tasks. Furthermore, our findings indicate that higher model confidence correlates with increased prompt robustness. We believe this work will serve as a helpful tool in studying prompt sensitivity of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=94L33aYLJd": {
    "title": "MASAI: Modular Architecture for Software-engineering AI Agents",
    "volume": "review",
    "abstract": "A common method to solve complex problems in software engineering, is to divide the problem into multiple sub-problems. Inspired by this, we propose a Modular Architecture for Software-engineering AI (MASAI) agents, where different LLM-powered sub-agents are instantiated with well-defined objectives and strategies tuned to achieve those objectives. Our modular architecture offers several advantages: (1) employing and tuning different problem-solving strategies across sub-agents, (2) enabling sub-agents to gather information from different sources scattered throughout a repository, and (3) avoiding unnecessarily long trajectories which inflate costs and add extraneous context. MASAI enabled us to achieve the highest performance (28.33% resolution rate) on the popular and highly challenging SWE-bench Lite dataset consisting of 300 GitHub issues from 11 Python repositories. We conduct a comprehensive evaluation of MASAI relative to other agentic methods and analyze the effects of our design decisions and their contribution to the success of MASAI",
    "checked": true,
    "id": "c0b6149a7ff72817d5cdb008566dc4f0ca9b9379",
    "semantic_title": "masai: modular architecture for software-engineering ai agents",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=4U9A3aR5pJ": {
    "title": "Detecting Infrastructure Bias in LLM Generated Text",
    "volume": "review",
    "abstract": "In this study, we explore potential biases in large language models (LLMs) from a novel perspective. We focus on detecting racial bias in texts generated by these models that describe the physical environments of diverse racial communities and the narratives of their inhabitants. Our study reveals statistically significant infrastructure biases in popular LLMs, including ChatGPT, Gemma and Llama 3, suggesting potential racial biases linked to built environment features",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=igWWeJesIr": {
    "title": "Double-Checker: Large Language Model as a Checker for Few-shot Named Entity Recognition",
    "volume": "review",
    "abstract": "Recently, few-shot Named Entity Recognition (NER) has attracted significant attention due to the high cost of obtaining high-quality labeled data. Decomposition-based methods have demonstrated remarkable performance on this task, which initially train a type-independent span detector and subsequently classify the detected spans based on their types. However, this framework has an evident drawback as a domain-agnostic detector cannot ensure the identification of only those entity spans that are specific to the target domain. To address this issue, we propose Double-Checker, which leverages collaboration between Large Language Models (LLMs) and small models. Specifically, we employ LLMs to verify candidate spans predicted by the small model and eliminate any spans that fall outside the scope of the target domain. Extensive experiments validate the effectiveness of our method, consistently yielding improvements over two baseline approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cpzV5YpC1h": {
    "title": "QuanSIRA: The Quantitative Investment Risk Modeling in Stock Markets with Large Language Models",
    "volume": "review",
    "abstract": "Stock market analysis is important for investors to make financial decisions. Stock price prediction is widely investigated in the natural language processing area due to the superiority of large language models. Recent works have developed several datasets for stock price predictions. However, investment risk, considered an essential factor for investors, is rarely discussed in NLP applications, and there are limited datasets for investment risk analysis. In this work, we propose methods to quantify investment risk and introduce the dataset QuanSIRA. Using this benchmark, we investigate the applications of large language models in tackling quantitative investment risk analysis. The experimental results show the difficulty of investment risk analysis. The model built on pre-trained large language models obtained F1 scores of 68.07 and 65.01 in the in-stock benchmark and the cross-stock benchmark of investment risk prediction task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i7KbJCPlOL": {
    "title": "Quis custodiet ipsos custodes?' Who will watch the watchmen? On Detecting AI-generated peer-reviews",
    "volume": "review",
    "abstract": "The integrity of the peer-review process is vital for maintaining scientific rigor and trust within the academic community. With the steady increase in the usage of large language models (LLMs) like ChatGPT in academic writing, there is a growing concern that AI-generated texts could compromise the scientific publishing including peer-reviews. Previous works have focused on generic AI-generated text detection or have presented an approach for estimating the fraction of peer-reviews that can be AI-generated. Our focus here is to solve a real-world problem by assisting the editor or chair in determining whether a review is written by ChatGPT or not. To address this, we introduce the Term Frequency (TF) model, which posits that AI often repeats tokens, and the Review Regeneration (RR) model which is based on the idea that ChatGPT generates similar outputs upon re-prompting. We stress test these detectors against token attack and paraphrasing. Finally we propose an effective defensive strategy to reduce the effect of paraphrasing on our models. Our findings suggest both our proposed methods perform better than other AI text detectors. Our RR model is more robust, although our TF model performs better than the RR model without any attacks. We make our code, dataset, model public",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rD2HwXN7bx": {
    "title": "Refining Multilingual Pronunciation through G2P and ASR Integration",
    "volume": "review",
    "abstract": "Pronunciation dictionaries are indispensable for applications in speech synthesis and language learning, providing word pronunciations across diverse languages. Grapheme-to-Phoneme (G2P) models are pivotal in creating these dictionaries. However, variations in pronunciation can arise due to language, context, dialect, and acoustic conditions, potentially introducing inaccuracies. To address this, we introduce an approach to refine G2P model outputs by utilizing an alignment and weighting algorithm to integrate results from an acoustic phone recognizer across several high and low-resource languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2kYbu7XQxI": {
    "title": "Skill based framework for harnessing emergent abilities of LLMs for knowledge management",
    "volume": "review",
    "abstract": "This paper introduces a skill-based framework for enhancing the emergent abilities of Large Language Models (LLMs) within knowledge management applications, leveraging Retrieval-Augmented Generation (RAG). LLMs exhibit emergent abilities that can significantly impact their performance in complex tasks. Our approach explores and harnesses these abilities by defining skills, optimizing model performance through the DSPy framework, and assessing impact using a combination of discrete and continuous metrics. We conducted experiments on LLMs of varying scales, focusing on models like GPT-3.5 and Mistral 7B, across skill associated datasets (Emotion-based, fact-based persona, persona emotional state, crisp answers). Our results indicate that the DSPy optimization enhances LLM performance, particularly in generating contextually rich responses while reducing operational costs. This study not only sheds light on the mechanisms through which emergent abilities develop in LLMs but also illustrates how skill-based frameworks can systematically leverage these properties to improve efficiency and effectiveness in real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U1AaWjiFH6": {
    "title": "Self-Distillation Across Modalities: Enhancing Cross-Modal Correlation Perception for Effective Multimodal Fake News Detection",
    "volume": "review",
    "abstract": "The proliferation of multimodal fake news content has garnered increasing attention. Existing multimodal fake news detection methods have significantly contributed to the automation of fake news detection. However, these methods often rely on explicit cross-modal consistency, while the impact of consistency on the veracity of news is not absolute. In this paper, we propose a novel Self-Distillation Across Modalities (SDAM) method for multimodal fake news detection. Our approach leverages internal self-distillation from multimodal representations to the unimodal, implicitly capturing cross-modal correlation. Additionally, SDAM employs a structured segmentation of text based on human biological habits, capturing global textual information while minimally affecting the number of input tokens. Extensive experiments demonstrate that SDAM exhibits superior performance in multimodal fake news detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hPHe91dZ9H": {
    "title": "WIKIGENBENCH: Exploring Full-length Wikipedia Generation for New Events",
    "volume": "review",
    "abstract": "Generating comprehensive and accurate Wikipedia articles for newly emerging real-world events presents significant challenges. Previous efforts have often fallen short by focusing only on short snippets, neglecting verifiability, or ignoring the impact of the pre-training corpus. In this paper, we simulate a real-world scenario where structured, full-length Wikipedia articles with citations are generated for new events using input documents from web sources. To minimize data leakage in Large Language Models (LLMs), we select recent events and construct a new benchmark, WIKIGENBENCH, consisting of 1320 events paired with their corresponding related web documents. We also design a comprehensive set of systematic metrics and LLM-based baseline methods to evaluate the capability of LLMs in generating factual, full-length Wikipedia articles. The data and code will be released upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f3TRCoTDME": {
    "title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating exceptional reasoning, tool usage, and memory capabilities. As their applications expand into multi-agent environments, there arises a need for a comprehensive evaluation framework that captures LLMs' reasoning, planning, collaboration, and other social abilities. This work introduces a novel competition-based benchmark framework specifically designed to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality. To create diverse environments, we utilize two social deduction games alongside three game theory scenarios. Our frame is fortified with the probabilistic graphic modeling (PGM) method, enhancing the LLMs' capabilities in navigating complex social and cognitive dimensions. We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT-4, and the weakest, Llama-2-70B. It also confirms that our PGM enhancement boosts the abilities of all selected models by an average of 37\\%. Our codes are in the anonymous link https://anonymous.4open.science/r/magic_anonym-5366",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ld5fGYUK9n": {
    "title": "MOCK: Can LLMs Really Understand Humor-Sarcasm?",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated the capacity to engage in interaction with humans, employing humor and sarcasm. However, their true comprehension of humor and sarcasm remains a subject of inquiry. This work introduces the hu$\\textbf{M}$or-sarcasm c$\\textbf{O}$mprehension ben$\\textbf{C}$hmar$\\textbf{K}$, named MOCK, to systematically evaluate LLMs' abilities to detect, match, and explain humor-sarcasm across diverse scenes, including cartoon, post, and comedy. Our comprehensive assessment reveals significant gap between the performance of LLMs and human on humor-sarcasm comprehension. To bridge this gap, we propose a Chain-of-Task approach that integrates the three comprehension sub-tasks (\\ie detecting, matching and explaining), leveraging their interrelatedness to enhance humor-sarcasm comprehension. Additionally, we propose a novel humor-sarcasm generation task and explore the potential of MOCK to improve LLMs' humor-sarcasm generation capabilities. The evaluation results verify that humor-sarcasm comprehension can significantly enhance humor-sarcasm generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RCeC68JlL1": {
    "title": "ConceptPsy: A Benchmark Suite with Conceptual Comprehensiveness in Psychology",
    "volume": "review",
    "abstract": "The effective incorporation of Large Language Models (LLMs) into the field of psychology necessitates a comprehensive domain benchmark to guide their development and adaptation. Existing Chinese benchmarks in the style of MMLU, such as CMMLU, do include psychology subjects, but their concept coverage is far from exhaustive. The number of questions in each domain is just in the hundreds, and an uneven question sampling process can lead to a ``concept bias'' issue. This bias, stemming from using a question set with a low concept coverage rate to represent a subject, can potentially lead to skewed results. To address this, we present ConceptPsy, a Chinese conceptual benchmark specifically designed for evaluating LLMs' complex reasoning and knowledge in psychology. ConceptPsy encompasses 12 core subjects and 1,383 concepts from official exams. To avoid copyright issues, we prompt \\texttt{GPT-4} to generate questions for each of the concepts, which are then validated by psychology professionals to ensure high quality. Besides the overall scores, we annotate each question with a chapter label to provide fine-grained results. We evaluate a range of LLMs on ConceptPsy and the results show significant performance differences across psychology concepts, even among models from the same series. We anticipate the comprehensive concept coverage and the fine-grained strengths and weaknesses identified by ConceptPsy can facilitate the development and growth of the Chinese psychology domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VOV0ymQGk2": {
    "title": "Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning",
    "volume": "review",
    "abstract": "Despite significant advancements, there is a limited understanding of how large language models (LLMs) utilize knowledge for reasoning. To address this, we propose a method that deconstructs complex real-world questions into a graph, representing each question as a node with parent nodes of background knowledge needed to solve the question. We develop the DepthQA set, deconstructing questions into three depths: (i) recalling conceptual knowledge, (ii) applying procedural knowledge, and (iii) analyzing strategic knowledge. Based on a hierarchical graph, we quantify forward discrepancy, discrepancies in LLMs' performance on simpler sub-problems versus complex questions. We also measure backward discrepancy, where LLMs answer complex questions but struggle with simpler ones. Our analysis shows that smaller models have more discrepancies than larger models. Additionally, guiding models from simpler to complex questions through multi-turn interactions improves performance across model sizes, highlighting the importance of structured intermediate steps in knowledge reasoning. This work enhances our understanding of LLM reasoning and suggests ways to improve their problem-solving abilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NjbjefmfAB": {
    "title": "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment",
    "volume": "review",
    "abstract": "Large language models (LLMs) are still struggling in aligning with human preference in complex tasks and scenarios. They are prone to overfit into the unexpected patterns or superficial styles in the training data. We conduct an empirical study that only selects the top-10\\% most updated parameters in LLMs for alignment training, and see improvements in the convergence process and final performance. It indicates the existence of redundant neurons in LLMs for alignment training. To reduce its influence, we propose a low-redundant alignment method named $\\textbf{ALLO}$, focusing on optimizing the most related neurons with the most useful supervised signals. Concretely, we first identify the neurons that are related to the human preference data by a gradient-based strategy, then identify the alignment-related key tokens by reward models for computing loss. Besides, we also decompose the alignment process into the forgetting and learning stages, where we first forget the tokens with unaligned knowledge and then learn aligned knowledge, by updating different ratios of neurons, respectively. Experimental results on 10 datasets have shown the effectiveness of ALLO. Our code and data will be publicly released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eX0cMTH433": {
    "title": "WTU-EVAL: A Whether-or-Not Tool Usage Evaluation Benchmark for Large Language Models",
    "volume": "review",
    "abstract": "Although Large Language Models (LLMs) excel in NLP tasks, they still need external tools to extend their ability. Current research on tool learning with LLMs often assumes mandatory tool use, which does not always align with real-world situations, where the necessity for tools is uncertain, and incorrect or unnecessary use of tools can damage the general abilities of LLMs. Therefore, we propose to explore whether LLMs can discern their ability boundaries and use tools flexibly. We then introduce the Whether-or-not tool usage Evaluation benchmark (WTU-Eval) to assess LLMs with eleven datasets, where six of them are tool-usage datasets, and five are general datasets. LLMs are prompted to use tools according to their needs. The results of eight LLMs on WTU-Eval reveal that LLMs frequently struggle to determine tool use in general datasets, and LLMs' performance in tool-usage datasets improves when their ability is similar to ChatGPT. In both datasets, incorrect tool usage significantly impairs LLMs' performance. To mitigate this, we also develop the finetuning dataset to enhance tool decision-making. Fine-tuning Llama2-7B results in a 14\\% average performance improvement and a 16.8\\% decrease in incorrect tool usage. We will release the WTU-Eval benchmark",
    "checked": true,
    "id": "c8a5e1b37c448b0e17720b3c4e65d5b36c695638",
    "semantic_title": "wtu-eval: a whether-or-not tool usage evaluation benchmark for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dI5asbPk6H": {
    "title": "SCoPe: Submodular Combinatorial Prototype Learner for Continuous Speech Keyword Spotting",
    "volume": "review",
    "abstract": "Keyword Spotting (KwS) in the continuous speech setting encapsulates localization and recognition of keywords amongst a large volume of non-keyword tokens, further exemplified by variation in speakers and presence of rare keywords. Our paper presents a novel Submodular Combinatorial Prototype (SCoPe) learning framework that not only contrasts between target keywords but also ensures sufficient separation of keywords with non-keyword tokens. Additionally, our work proposes a weakly-supervised training strategy, utilizing forced-alignment on phoneme level embeddings to guide a windowing function to correctly localize keywords of interest. We evaluate our model on the popular LibriSpeech and L2-Arctic datasets under varying numbers of keywords demonstrating a class-imbalanced distribution, and show that our proposed architecture consistently outperforms existing baselines by upto 1.8%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EjzgVCaP80": {
    "title": "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models",
    "volume": "review",
    "abstract": "Existing works examining Vision Language Models (VLMs) for social biases predominantly focus on a limited set of documented bias associations, such as gender-profession or race-crime. This narrow scope often overlooks a vast range of unexamined implicit associations, restricting the identification and, hence, mitigation of such biases. We address this gap by probing VLMs to (1) uncover hidden, implicit associations across 9 bias dimensions. We systematically explore diverse input and output modalities and (2) demonstrate how biased associations vary in their negativity, toxicity, and extremity. Our work (3) identifies subtle and extreme biases that are typically not recognized by existing methodologies. We make the **D**ataset **o**f **r**etrieved **a**ssociations, (__Dora__), publicly available",
    "checked": true,
    "id": "69f58648780d6de2fe79745790a5b974927735f3",
    "semantic_title": "biasdora: exploring hidden biased associations in vision-language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jOHp3SYZJm": {
    "title": "Automated Grammar Error Correction for Urdu using Deep Learning",
    "volume": "review",
    "abstract": "Automated Grammar Error Correction (GEC) is an active area of research within the field of Natural Language Processing (NLP), yet its scope remains restricted to English and other resource-rich languages. Urdu is a language that is widely spoken in South Asia. However, due to the lack of annotated datasets no work has been in field of GEC for Urdu language. This paper presents an GEC model for Urdu. In addition, we also present a dataset that contains 1200 pairs of grammatically correct and incorrect sentences in Urdu that was manually curated from children books. Moreover, we also scrapped 400 children stories from Rekhta, an Urdu Literary website, and introduced errors probabilistically to create a dataset with 36,000 pairs of grammatically correct and incorrect sentences. The model that we used was mT5, which is a multilingual version of T5 transformer based model presented by Google. We trained the model in two stages. First, we trained the model on the manually curated dataset. Then, we trained the same model on the dataset that was scrapped from web. Finally, we tested the model by on Wikipedia Edit History dataset containing only grammatical errors which were identified using ERRANT. F0.5 Score, GLEU, Recall and Precision were used as evaluation criteria. The F0.5 scores for the test dataset after fine tuning the MT5 Base model on Raw + Synthetic Dataset are: NOUN INFL 0.63, ADP INFL 0.76, VERB INFL 0.73, VERB FORM 0.66, ADJ INFL 0.76, and PRON INFL 0.74. Additionally, our study is the first to focus on GEC systems, as to the best of our knowledge, no prior work has been done in this field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X7KPsMrYtL": {
    "title": "Not all Layers of LLMs are Necessary during Inference",
    "volume": "review",
    "abstract": "The inference phase of Large Language Models (LLMs) is very expensive. An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability). In this paper, we try to answer the question, \"During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?\" To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer can achieve an average of 17.8% pruning ratio, even up to 43% on sentiment tasks while maintaining comparable performance with minimal loss (<1%). Additionally, this method is orthogonal to other model acceleration techniques, potentially boosting inference efficiency further",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MPBrmuOOX7": {
    "title": "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging",
    "volume": "review",
    "abstract": "Reinforcement learning from human feedback (RLHF) is a popular strategy for aligning large language models (LLMs) with desired behaviors. Reward modeling is a crucial step in RLHF. However, collecting paired preference data for training reward models is often costly and time-consuming, especially for domain-specific preferences requiring expert annotation. To address this challenge, we propose the \\textbf{Do}main knowled\\textbf{ge} merged \\textbf{R}eward \\textbf{M}odel (DogeRM), a novel framework that integrates domain-specific knowledge into a general reward model by model merging. The experiments demonstrate that DogeRM enhances performance across different benchmarks and provide a detailed analysis showcasing the effects of model merging, showing the great potential of facilitating model alignment",
    "checked": true,
    "id": "6f8d26bef8ac26f747a8bd9919109798f61eb0ed",
    "semantic_title": "dogerm: equipping reward models with domain knowledge through model merging",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J8Nzr2T5eJ": {
    "title": "ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets",
    "volume": "review",
    "abstract": "This work addresses the timely yet underexplored problem of performing inference and finetuning of a proprietary LLM owned by a model provider entity on the confidential/private data of another data owner entity, in a way that ensures the confidentiality of both the model and the data. Hereby, the finetuning is conducted offsite, i.e., on the computation infrastructure of a third-party cloud provider. We tackle this problem by proposing \\texttt{ObfuscaTune}, a novel, efficient and fully utility-preserving approach that combines a simple yet effective obfuscation technique with an efficient usage of confidential computing (only $~5\\%$ of the model parameters are placed on TEE). We empirically demonstrate the effectiveness of \\texttt{ObfuscaTune} by validating it on GPT-2 models with different sizes on four NLP benchmark datasets. Finally, we compare to a naive version of our approach to highlight the necessity of using random matrices with low condition numbers in our approach to reduce errors induced by the obfuscation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6DOAoQANIB": {
    "title": "Robust AI-Generated Text Detection by Restricted Embeddings",
    "volume": "review",
    "abstract": "Growing amount and quality of AI-generated texts makes detecting such content more difficult. In most real-world scenarios, the domain (style and topic) of generated data and the generator model are not known in advance. In this work, we focus on the robustness of classifier-based detectors of AI-generated text, namely their ability to transfer to unseen generators or semantic domains. We investigate the geometry of the embedding space of Transformer-based text encoders and show that clearing out harmful linear subspaces helps to train a robust classifier, ignoring domain-specific spurious features. We investigate several subspace decomposition and feature selection strategies and achieve significant improvements over state of the art methods in cross-domain and cross-generator transfer. Our best approaches for head-wise and coordinate-based subspace removal increase the mean out-of-distribution (OOD) classification score by 9% and 14% for RoBERTa and BERT embeddings respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILJTa37aUN": {
    "title": "LAR: LLM Assisted Retrieval",
    "volume": "review",
    "abstract": "Large language models (LLMs), have demonstrated significant success in natural language understanding and generation tasks. In this work, we propose LAR (Large language model Assisted Retrieval) to harness LLMs towards enhancing the effectiveness of retrieval models, thereby improving the relevance of information retrieval from datasets. Our approach augments a retriever engine by incorporating a subsequent refinement step to the query, utilizing an LLM. This approach showcases the potential of combining retrieval models with LLMs to advance information retrieval systems. We demonstrate the efficacy of LAR through extensive evaluations, specifically showing enhanced performance on the BEIR retrieval benchmark. Furthermore, our methodology exhibits notable improvements on downstream tasks such as question answering, as demonstrated on the NarrativeQA dataset",
    "checked": false,
    "id": "db22b645cb9d213095089a9ba88d02d18e6543a6",
    "semantic_title": "avatar: optimizing llm agents for tool-assisted knowledge retrieval",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O65aiPtB1t": {
    "title": "LightCache: Efficient Inference for Transformers via KV Cache Compression in Feature Dimension",
    "volume": "review",
    "abstract": "KV Cache Optimization is a crucial topic in improving the inference efficiency and length extrapolation of Transformer-based Large Language Models (LLMs). Previous KV Cache Optimization approaches often focus on pruning or compressing the sequence dimension, leading to an irreversible loss of contextual information. In this work, we propose LightCache, a novel KV Cache optimization approach that operates on the feature dimension. LightCache employs parameter-aware compression and full-context cache selection, allowing it to reduce memory usage and enhance computational efficiency without sacrificing contextual information. Importantly, LightCache enables a training-free integration with LLMs. Experiments demonstrate that LightCache outperforms classic extrapolation, quantization, and context-pruning methods in long-context evaluation. In terms of efficiency, LightCache reduces the KV Cache size by over 60\\% and achieves 1.7$\\sim$2.4$\\times$ memory efficiency as well as 1.5$\\sim$3.6$\\times$ speedup in 32K context length",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7BsrRLQQxt": {
    "title": "Representation Alignment and Adversarial Networks for Cross-lingual Dependency Parsing",
    "volume": "review",
    "abstract": "Thanks to the strong representation capability of pre-trained language models, dependency parsing in rich-resource language has achieved remarkable improvements. However, the parsing accuracy drops sharply when the model is transferred to low-resource language due to distribution shifts. To alleviate this issue, we propose a representation alignment and adversarial model to filter out useful knowledge from rich-resource language and ignore useless ones. Our proposed model consists of two components, i.e., an alignment network in the input layer for selecting useful language-specific representation features and an adversarial network in the encoder layer for augmenting the language-invariant contextualized features. Experiments on the benchmark datasets show that our proposed model outperforms RoBARTa-enhanced strong baseline models by 1.37 LAS and 1.34 UAS. Detailed analysis shows that both alignment and adversarial networks are equally important in alleviating the distribution shifts problem and can benefit from each other. In addition, the comparative experiments demonstrate that both the alignment and adversarial networks can substantially facilitate extracting and utilizing relevant target language features, thereby increasing the adaptation capability of our proposed model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rNzTEAwhnY": {
    "title": "Reverse Thinking in Large Language Models",
    "volume": "review",
    "abstract": "Humans are accustomed to reading and writing in a forward manner, and this natural bias extends to text understanding in auto-regressive large language models (LLMs). This paper investigates whether LLMs, like humans, struggle with reverse thinking, specifically with reversed text inputs. We found that publicly available pre-trained LLMs cannot understand such inputs. However, LLMs trained from scratch with both forward and reverse texts can understand them equally well during inference. Our case study shows that different-content texts result in different losses if input (to LLMs) in different directions---some get lower losses for forward while some for reverse. This leads us to a simple and nice solution for data selection based on the loss differences between forward and reverse directions. Using our selected data in continued pretraining can boost LLMs' performance by a large margin for the task of Massive Multitask Language Understanding",
    "checked": false,
    "id": "3ae618cf5c264dcbf6c4d30ce40c4315502fe6de",
    "semantic_title": "re-thinking inverse graphics with large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=oriK8vHGQe": {
    "title": "HAF-RM: A Hybrid Alignment Framework for Reward Model Training",
    "volume": "review",
    "abstract": "The reward model has become increasingly important in alignment, assessment, and data construction for large language models (LLMs). Most existing researchers focus on enhancing reward models through data improvements, following the conventional training framework for reward models that directly optimizes the predicted rewards. In this paper, we propose a hybrid alignment framework **HaF-RM** for reward model training by introducing an additional constraint on token-level policy probabilities in addition to the reward score. It can simultaneously supervise the internal preference model at the token level and optimize the mapping layer of the reward model at the sequence level. Theoretical justifications and experiment results on five datasets show the validity and effectiveness of our proposed hybrid framework for training a high-quality reward model. By decoupling the reward modeling procedure and incorporating hybrid supervision, our **HaF-RM** framework offers a principled and effective approach to enhancing the performance and alignment of reward models, a critical component in the responsible development of powerful language models. We release our code at [https://haf-rm-anonymized.github.io](https://haf-rm-anonymized.github.io)",
    "checked": true,
    "id": "581d831fcea84501bec33161c716d5bf94a8a345",
    "semantic_title": "haf-rm: a hybrid alignment framework for reward model training",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vmcyBaTX4r": {
    "title": "Aligning Translation-Specific Understanding to General Understanding in Large Language Models",
    "volume": "review",
    "abstract": "Large Language models (LLMs) have exhibited remarkable abilities in understanding complex texts, offering a promising path towards human-like translation performance. However, this study reveals the misalignment between the translation-specific understanding and the general understanding inside LLMs. This understanding misalignment leads to LLMs mistakenly or literally translating some complicated concepts that they accurately comprehend in the general scenarios (e.g., QA). To align the translation-specific understanding to the general one, we propose a novel translation process, DUAT (Difficult words Understanding Aligned Translation), explicitly incorporating the general understanding on the complicated content incurring inconsistent understandings to guide the translation. Specifically, DUAT performs cross-lingual interpretation for the difficult-to-translate words and enhances the translation with the generated interpretations. Furthermore, we reframe the external tools to improve DUAT in detecting difficult words and generating helpful interpretations. We conduct experiments on the self-constructed benchmark Challenge-WMT, consisting of samples that are prone to mistranslation. Human evaluation results on high-resource and low-resource language pairs indicate that DUAT significantly facilitates the understanding alignment, which improves the translation quality (up to +3.85 COMET) and reduces translation literalness by -25% âˆ¼ -51%",
    "checked": true,
    "id": "a23a89855e3af2e6cec7fd4a01e12cacdf6c727f",
    "semantic_title": "aligning translation-specific understanding to general understanding in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qjgx06oQDl": {
    "title": "Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Solvers for Math Word Problems",
    "volume": "review",
    "abstract": "Chain-of-Thought (CoT) prompting has enhanced the performance of Large Language Models (LLMs) across various reasoning tasks. However, CoT still falls short in dealing with complex math word problems, as it usually suffers from three pitfalls: semantic misunderstanding errors, calculation errors and step-missing errors. Prior studies involve addressing the calculation errors and step-missing errors, but neglect the semantic misunderstanding errors, which is the major factor limiting the LLMs' performance. To this end, we propose a simple-yet-effective method, namely Deeply Understanding the Problems (DUP), to improve the LLMs' math problem-solving ability by addressing semantic misunderstanding errors. The core of our method is to encourage the LLMs to deeply understand the problems and extract the key problem-solving information used for better reasoning. Extensive experiments on 10 diverse reasoning benchmarks show that our DUP method consistently outperforms the other counterparts by a large margin. More encouragingly, DUP achieves a new SOTA result on the GSM8K benchmark, with an accuracy of 97.1% under zero-shot setting",
    "checked": false,
    "id": "dd3dd5e7e3a2718336039112a3f734347da5c1f4",
    "semantic_title": "achieving>97% on gsm8k: deeply understanding the problems makes llms better solvers for math word problems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z4OYxFi4s7": {
    "title": "A âˆ§ B â‡” B âˆ§ A: Evaluating and Improving Logical Reasoning Ability of Large Language Models",
    "volume": "review",
    "abstract": "We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT-4. Despite their prowess in tasks like writing assistance, code generation, and machine translation, assessing LLMs' ability to reason has been challenging. Traditional evaluations often prioritize accuracy on downstream tasks over direct assessments of reasoning processes. LogicAsker addresses this gap by employing a set of atomic reasoning skills grounded in propositional and predicate logic to systematically examine and improve the reasoning prowess of LLMs. Our methodology reveals significant gaps in LLMs' learning of logical rules, with identified reasoning failures ranging from 29% to 90% across different models. Moreover, we leverage these findings to construct targeted demonstration examples and fine-tune data, notably enhancing logical reasoning in models like GPT-4o by up to 5%. To our knowledge, this is the first effort to utilize test case outcomes to effectively refine LLMs' formal reasoning capabilities. We will make our code, data, and results publicly available to facilitate further research and replication of our findings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2KtL13evFm": {
    "title": "Multilingual Fact-Checking using LLMs",
    "volume": "review",
    "abstract": "Due to the recent rise in digital misinformation, there has been great interest shown in using LLMs for fact-checking and claim verification. In this paper, we answer the question: Do LLMs know multilingual facts and can they use this knowledge for effective fact-checking? To this end, we create a benchmark by filtering multilingual claims from the X-fact dataset and evaluating the multilingual fact-checking capabilities of five LLMs across five diverse languages: Spanish, Italian, Portuguese, Turkish, and Tamil on our benchmark. We employ three different prompting techniques: Zero-Shot, English Chain-of-Thought, and Cross-Lingual Prompting, using both greedy and self-consistency decoding. We extensively analyze our results and find that GPT-4o achieves the highest accuracy, but zero-shot prompting with self-consistency was the most effective overall. We also show that techniques like Chain-of-Thought and Cross-Lingual Prompting, which are designed to improve reasoning abilities, do not necessarily improve the fact-checking abilities of LLMs. Interestingly, we find a strong negative correlation between model accuracy and the amount of internet content for a given language. This suggests that LLMs are better at fact-checking from knowledge in low-resource languages. We hope that this study will encourage more work on multilingual fact-checking using LLMs",
    "checked": false,
    "id": "a56cc09fd52b40203c5f38b01b20592f21a48548",
    "semantic_title": "multilingual detection of check-worthy claims using world languages and adapter fusion",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=ZQ8ahykTmc": {
    "title": "Adaptive Prompts for Efficient RLHF",
    "volume": "review",
    "abstract": "The alignment problem, ensuring AI systems adhere to human values, remains a significant challenge despite the collection of increasingly high-quality and expensive datasets. Reinforcement Learning from Human Feedback (RLHF) offers a promising solution, leveraging human judgment during training. However, standard RLHF often relies on static prompts, potentially wasting resources and neglecting areas needing improvement. This work proposes a novel approach for efficient and effective RLHF fine-tuning of large language models (LLMs). We introduce a dynamic prompt generation system that adapts based on the model's intermediate performance. This allows the model to focus on areas requiring the most human guidance, leading to faster and more targeted alignment. We evaluate our method by comparing three models trained with the same resources: a standard RLHF baseline, a Starts-On-Policy (SOP) model with static prompts based on initial performance, and our Always-On-Policy (AOP) model with dynamically generated prompts. Results demonstrate that AOP significantly outperforms all other models showcasing the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2qr92Iex3u": {
    "title": "DISC: Plug-and-Play Decoding Intervention with Similarity of Characters for Chinese Spelling Check",
    "volume": "review",
    "abstract": "One of the key challenges in Chinese spelling check (CSC) is ensuring that modifications remain faithful to the original intent of the sentence. Confusion sets are commonly used to mitigate this issue; however, it is challenging to construct high-quality confusion sets and integrate them into the model. In this paper, we propose a plug-and-play DISC (Decoding Intervention with Similarity of Characters) module for CSC models to address these challenges. DISC measures phonetic and glyph similarities between characters and incorporates this similarity information in the decoding stage. This method can be easily integrated into various existing CSC models, such as ReaLiSe, SCOPE, and ReLM, without additional training costs. Experiments on three CSC benchmarks demonstrate that our proposed method significantly improves model performance, approaching and even surpassing the current state-of-the-art models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ex0hyw6R8": {
    "title": "Fine-tuning Large Language Models with Human-inspired Learning Strategies in Medical Question Answering",
    "volume": "review",
    "abstract": "Training Large Language Models (LLMs) incurs substantial data-related costs, motivating the development of data-efficient training methods through optimised data ordering and selection. Human-inspired learning strategies, such as curriculum learning, offer possibilities for efficient training by ordering data according to common human learning practices. Despite evidence that curriculum learning improves performance of natural language understanding tasks in fine-tuning LLMs, its application to domain-specific question-answering remains underexplored. In this work, we comprehensively examine the effectiveness of human-inspired learning strategies for fine-tuning LLMs in medical question answering. Our work complements previous studies by extending the evaluation to non-curriculum-based learning across multiple language models, using both human-defined and automated data labels. Our results show moderate impact in using human-inspired learning strategies for fine-tuning LLMs, with maximum accuracy gains of 1.77\\% per model and 1.81\\% per dataset. However, the effectiveness of these learning strategies varies significantly across different model-dataset combinations, suggesting caution in generalising human-inspired strategies for fine-tuning language models. We also find that curriculum learning using LLM-defined question difficulty outperformed human-defined difficulty, highlighting the potential of using model-generated metrics in optimal curriculum design",
    "checked": true,
    "id": "560631f2889bdebf695d1012e55caf95d421a039",
    "semantic_title": "fine-tuning large language models with human-inspired learning strategies in medical question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dZ0VyV7adK": {
    "title": "LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) require continual knowledge updates to stay abreast of the ever-changing world facts, prompting the formulation of lifelong model editing task. While recent years have witnessed the development of various techniques for single and batch editing, these methods either fail to apply or perform sub-optimally when faced with lifelong editing. In this paper, we introduce LEMoE, an advanced Mixture of Experts (MoE) adaptor for lifelong model editing. We first analyze the factors influencing the effectiveness of conventional MoE adaptor in lifelong editing, including catastrophic forgetting, inconsistent routing and order sensitivity. Based on these insights, we propose a tailored module insertion method to achieve lifelong editing, incorporating a novel KV anchor routing to enhance routing consistency between training and inference stage, along with a concise yet effective clustering-based editing order planning. Experimental results demonstrate the effectiveness of our method in lifelong editing, surpassing previous model editing techniques while maintaining outstanding performance in batch editing task. Our code will be available",
    "checked": true,
    "id": "6202c20b152b6b9f379aa1714bbd01594dcc990a",
    "semantic_title": "lemoe: advanced mixture of experts adaptor for lifelong model editing of large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=t7onBnDHYW": {
    "title": "The Reason behind Good or Bad: Towards a Better Mathematical Verifier with Natural Language Feedback",
    "volume": "review",
    "abstract": "Mathematical verfier achieves success in mathematical reasoning tasks by validating the correctness of solutions. However, existing verifiers are trained with binary classification labels, which are not informative enough for the model to accurately assess the solutions. To mitigate the aforementioned insufficiency of binary labels, we introduce step-wise natural language feedbacks as rationale labels (i.e., the correctness of the current step and the explanations). In this paper, we propose **Math-Minos**, a natural language feedback enhanced verifier by constructing automatically-generated training data and a two-stage training paradigm for effective training and efficient inference. Our experiments reveal that a small set (30k) of natural language feedbacks can significantly boost the performance of the verifier by the accuracy of 1.6% (86.6% â†’ 88.2%) on GSM8K and 0.8% (37.8% â†’ 38.6%) on MATH",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lPJqEBRQD0": {
    "title": "Sparse Contrastive Learning of Sentence Embeddings",
    "volume": "review",
    "abstract": "Recently, SimCSE, a simple contrastive learning framework for sentence embeddings, has shown the feasibility of contrastive learning in training sentence embeddings and illustrates its expressiveness in spanning an aligned and uniform embedding space. However, prior studies have shown that dense models could contain harmful parameters that affect the model performance. This prompted us to consider whether SimCSE might also have similar harmful parameters. To tackle the problem, parameter sparsification is applied, where alignment and uniformity scores are used to measure the contribution of each parameter to the overall quality of sentence embeddings. Drawing from a preliminary study, we hypothesize that parameters with minimal contributions are detrimental, and sparsifying them would result in an improved model performance. Accordingly, a sparsified SimCSE (SparseCSE) is proposed. To systematically explore the ubiquity of detrimental parameters and the removal of them, extensive experiments are conducted on the standard semantic textual similarity (STS) tasks and transfer learning tasks. The results show that the proposed SparseCSE significantly outperform SimCSE. Furthermore, through an in-depth analysis, we establish the validity and stability of our sparsification method, showcasing that the embedding space generated by SparseCSE exhibits an improved alignment compared to that produced by SimCSE. Importantly, the uniformity remains uncompromised",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LOrTiRUy2a": {
    "title": "AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging",
    "volume": "review",
    "abstract": "As an effective alternative to the direct fine-tuning on target tasks in specific languages, cross-lingual transfer addresses the challenges of limited training data by decoupling \"task ability\" and \"language ability\", achieved by fine-tuning on the target task in the source language and another selected task in the target language, respectively. However, they fail to fully separate the task ability from the source language or the language ability from the chosen task. In this paper, we acknowledge the mutual reliance between task ability and language ability and direct our attention toward the gap between the target language and the source language on tasks. As the gap removes the impact of tasks, we assume that it remains consistent across tasks. Based on this assumption, we propose a new cross-lingual transfer method called $\\texttt{AdaMergeX}$ that utilizes adaptive adapter merging. By introducing a reference task, we can determine that the divergence of adapters fine-tuned on the reference task in both languages follows the same distribution as the divergence of adapters fine-tuned on the target task in both languages. Hence, we can obtain target adapters by combining the other three adapters. Furthermore, we propose a structure-adaptive adapter merging method. Our empirical results demonstrate that our approach yields new and effective cross-lingual transfer, outperforming existing methods across all settings",
    "checked": true,
    "id": "bba9778411de8940585fc730a2836d45e2292875",
    "semantic_title": "adamergex: cross-lingual transfer with large language models via adaptive adapter merging",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=2U97bAEame": {
    "title": "Religious Bias Landscape in Language and Text-to-Image Models: Analysis, Detection, and Debiasing Strategies",
    "volume": "review",
    "abstract": "The increasing utilization of language models necessitates critical examinations of their inherent biases, particularly concerning religion. This study thoroughly examines religious bias across a wide range of language models, encompassing pre-trained models like BERT, RoBERTa, ALBERT, and DistilBERT, alongside diverse open-source large language models such as GPT-2, Llama-3, Mixtral-8x7B, Vicuna-13B, and closed-source models including GPT-3.5 and GPT-4, along with DALLÂ·E 3 for image generation. Using diverse methodologies like mask filling, prompt completion, and image generation, we assess each model's handling of content related to different religions to uncover any underlying biases. We also investigate cross-domain bias concerning gender, age, and nationality within the context of religious content. Furthermore, this paper explores the effectiveness of targeted debiasing techniques, employing corrective prompts to mitigate identified biases. Our findings indicate that language models continue to exhibit biases in both text and image generation. However, the use of debiasing prompts has proven effective in mitigating these biases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0YJERBwdYc": {
    "title": "KGLens: Towards Efficient and Effective Knowledge Probing of Large Language Models with Knowledge Graphs",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) might hallucinate facts, while curated Knowledge Graph (KGs) are typically factually reliable especially with domain-specific knowledge. Measuring the alignment between KGs and LLMs can effectively probe the factualness and identify the knowledge blind spots of LLMs. However, verifying the LLMs over extensive KGs can be expensive. In this paper, we present KGLens, a Thompson-sampling-inspired framework aimed at effectively and efficiently measuring the alignment between KGs and LLMs. KGLens features a graph-guided question generator for converting KGs into natural language, along with a carefully designed importance sampling strategy based on parameterized KG structure to expedite KG traversal. Our simulation experiment compares the brute force method with KGLens under six different sampling methods, demonstrating that our approach achieves superior probing efficiency. Leveraging KGLens, we conducted in-depth analyses of the factual accuracy of ten LLMs across three large domain-specific KGs from Wikidata, composing over 19K edges, 700 relations, and 21K entities. Human evaluation results indicate that KGLens can assess LLMs with a level of accuracy nearly equivalent to that of human annotators, achieving 95.7% of the accuracy rate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=elE5RA9TDB": {
    "title": "Thread: A Logic-Based Data Organization Paradigm for How-To Question Answering with Retrieval Augmented Generation",
    "volume": "review",
    "abstract": "Current question answering systems leveraging retrieval augmented generation perform well in answering factoid questions but face challenges with non-factoid questions, particularly how-to queries requiring detailed step-by-step instructions and explanations. In this paper, we introduce Thread, a novel data organization paradigm that transforms documents into logic units based on their inter-connectivity. Extensive experiments across open-domain and industrial scenarios demonstrate that Thread outperforms existing data organization paradigms in RAG-based QA systems, significantly improving the handling of how-to questions",
    "checked": true,
    "id": "35adc58a2d9d3affa1a8d211bd1536d129658a35",
    "semantic_title": "thread: a logic-based data organization paradigm for how-to question answering with retrieval augmented generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GtbhHl56Ht": {
    "title": "From Semantic Alignment to LLM Hallucination Origins: An Algorithmic Approach",
    "volume": "review",
    "abstract": "We introduce a new text alignment algorithm that can produce fine-grained alignment between a query document and database documents. Our work explores two under-explored directions: i) alignment granularity at text segment level as opposed to traditional entire document retrieval, and ii) alignment directed by semantic similarity instead of exact matches. We utilize text embeddings produced by Large Language Models (LLM) and perform efficient queries through nearest neighbor data structures. We also introduce an attack strategy exploiting temporal inconsistencies to induce hallucinations in Large Language Models (LLMs) and apply our alignment algorithm to trace these hallucinations back to their possible origins in training data. By creating a database of relevant web documents using keyword filtering on Common Crawl data, our approach demonstrates the effectiveness of identifying candidate origins of LLM hallucinations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w1jGpGzcyQ": {
    "title": "Lifelong Event Detection via Optimal Transport",
    "volume": "review",
    "abstract": "Continual Event Detection (CED) poses a formidable challenge due to the catastrophic forgetting phenomenon, where learning new tasks (with new coming event types) hampers performance on previous ones. In this paper, we introduce a novel approach, Lifelong Event Detection via Optimal Transport (**LEDOT**), that leverages optimal transport principles to align the optimization of our classification module with the intrinsic nature of each class, as defined by their pre-trained language modeling. Our method integrates replay sets, prototype latent representations, and an innovative Optimal Transport component. Extensive experiments on MAVEN and ACE datasets demonstrate LEDOT's superior performance, consistently outperforming state-of-the-art baselines. The results underscore LEDOT as a pioneering solution in continual event detection, offering a more effective and nuanced approach to addressing catastrophic forgetting in evolving environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dDCdV3VcrY": {
    "title": "Pamba: Transplanting Transformer Blocks with Mamba",
    "volume": "review",
    "abstract": "To combine the Transformer's long-context copying and retrieval capabilities and Mamba's low-memory requirement, we introduce Pamba-1.1B, a hybrid architecture by replacing top Pythia-1.4B Transformer blocks with trained-from-scratch Mamba-1.4B blocks and healing the model through a two-stage fine-tuning process. Pamba-1.1B demonstrates enhanced language modeling abilities and 30\\% memory savings compared to the baseline Transformer model (Pythia-1.4B), as well as superior long-context copying capabilities compared to Mamba-1.4B. Furthermore, compared to other hybrid models like RecurrentGemma-2B (Griffin), Pamba-1.1B excels in specific long-context copying tasks, while also being more memory-efficient. Our code is available at: https://anonymous.4open.science/r/Transformer_Mamba_Transplantation-3C51/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U6h4JPvX9M": {
    "title": "Exploring Synthetic Data Generation Techniques for Employment Type Classification in Job Advertisements",
    "volume": "review",
    "abstract": "The classification of employment types in online job advertisements (OJAs) is crucial for labor market analysis and recruitment. This study addresses the limitations of manual data annotation by leveraging synthetic data generation (SDG) techniques using large language models (LLMs). We evaluate four SDG methodsâ€”plain prompting, sampling, precise attributes, and adjective attributesâ€”to generate synthetic job ads and assess their impact on classification model performance. Our analysis focuses on the balance between dataset size, data diversity and label-fit, and we explore the use of Natural Language Inference (NLI) filtering to enhance data quality. Results show that models trained on synthetic data can effectively classify real-world job ads, achieving competitive performance. However, we observed significant volatility in outcomes, which we could not fully explain. By making our code and data publicly available, we provide the research community with opportunities to further investigate SDG techniques. By publishing our best models, we offer researchers tools capable of achieving up to 96% F1 on a real-world dataset for classifying German OJAs by employment type",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=klIxQ7VWBl": {
    "title": "Assessment and manipulation of latent constructs in pre-trained language models using psychometric scales",
    "volume": "review",
    "abstract": "Human-like personality traits have recently been discovered in large language models, raising the hypothesis that their (known and as yet undiscovered) biases conform with human latent psychological constructs. While large conversational models may be tricked into answering psychometric questionnaires, the latent psychological constructs of thousands of simpler transformers, trained for other tasks, cannot be assessed because appropriate psychometric methods are currently lacking. Here, we show how standard psychological questionnaires can be reformulated into natural language inference prompts, and we provide a code library to support the psychometric assessment of arbitrary models. We demonstrate, using a sample of 88 publicly available models, the existence of human-like mental health-related constructsâ€”including anxiety, depression, and the sense of coherenceâ€”which conform with standard theories in human psychology and show similar correlations and mitigation strategies. The ability to interpret and rectify the performance of language models by using psychological tools can boost the development of more explainable, controllable, and trustworthy models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xXu4txKpBI": {
    "title": "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification",
    "volume": "review",
    "abstract": "Emotion significantly influences human behavior and decision-making processes. We propose a labeling methodology grounded in Plutchik's Wheel of Emotions theory for emotion classification. Furthermore, we employ a Mixture of Experts (MoE) architecture to evaluate the efficacy of this labeling approach, by identifying the specific emotions that each expert learns to classify. Experimental results reveal that our methodology improves the performance of emotion classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xgOmjXHPA7": {
    "title": "RoundTripOCR: A Data Generation Technique for Enhancing OCR Error Correction in Low-Resource Devanagari Languages",
    "volume": "review",
    "abstract": "Optical Character Recognition (OCR) technology has revolutionized the digitization of printed text, enabling efficient data extraction and analysis across various domains. Just like Machine Translation systems, OCR systems are prone to errors stemming from factors such as poor image quality, diverse fonts, and language variations. In this work, we address the challenge of data generation and post-OCR error correction, specifically for low-resource languages. We propose a novel approach for synthetic data generation for Devanagari languages, RoundTripOCR, that tackles the scarcity of the OCR Error Correction dataset. In this work, we release a post-OCR text correction dataset for Hindi, Marathi, Bodo, Nepali, Konkani and Sanskrit. We also present a novel approach for OCR error correction by leveraging techniques from machine translation. Our method involves translating the erroneous OCR output into a corrected form by treating the OCR errors as mistranslations in a parallel text corpus. We employ a state-of-the-art pre-trained transformer model, mBART, to learn the mapping from erroneous to correct text pairs, effectively correcting OCR errors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fwIEViaMuW": {
    "title": "Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing",
    "volume": "review",
    "abstract": "Large Language Models have recently revolutionized the NLP field, while they still fall short in some specific down-stream tasks. In the work, we focus on utilizing LLMs to perform machine translation, where we observe that two patterns of errors frequently occur and drastically affect the translation quality: language mismatch and repetition. The work sets out to explore the potential for mitigating these two issues by leveraging model editing methods, e.g., by locating FFN neurons or something that are responsible for the errors and deactivating them in the inference time. We find that directly applying such methods either limited effect on the targeted errors or has significant negative side-effect on the general translation quality, indicating that the located components may also be crucial for ensuring machine translation with LLMs on the rails. To this end, we propose to refine the located components by fetching the intersection of the locating results under different language settings, filtering out the aforementioned information that is irrelevant to targeted errors. The experiment results empirically demonstrate that our methods can effectively reduce the language mismatch and repetition ratios and meanwhile enhance or keep the general translation quality in most cases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uabufUkyQl": {
    "title": "HalluAttack: Mitigating Hallucinations in LLMs via Counterfactual Instruction Fine Tuning",
    "volume": "review",
    "abstract": "LLMs encapsulate a vast range of world knowledge with huge mount of pretraining data. While these models have demonstrated remarkable capabilities in various applications, they are prone to generating content infused with hallucinations, compromising the trustworthiness of their output. This phenomenon raises concerns of LLM applications, particularly when the dissemination of misleading information can have detrimental impacts. In this paper, we propose a simple yet effective method called \\textsc{HalluAttack} which generates high quality counterfactual instruction data in order to reduce the hallucinations. We observe that these counterfactual instruction data can unlock the self-reflection ability of LLMs, and the LLMs will use knowledge learnt from pretraining phase more accurately. We conducted experiments across multiple open-source LLMs to evaluate the effectiveness of our proposed approach\\footnote{The data we used for fine-tuning is publicly available in \\url{https://github.com/oldstree/halluattack}}. Results consistently demonstrate that, through counterfactual attack and subsequent fine-tuning, we are able to significantly improve the model performance on hallucination benchmarks (e.g. TruthfulQA and HalluQA). Moreover, we also find that the LLMs fine-tuned with counterfactual instruction data can also achieve gains on public general benchmarks like C-Eval, MMLU and GSM8K, which also demonstrate the effectiveness of our approach on hallucination mitigation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=erqoeO6lFu": {
    "title": "Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models",
    "volume": "review",
    "abstract": "Data selection for fine-tuning Large Language Models (LLMs) aims to select a high-quality subset from a given candidate dataset to train a Pending Fine-tune Model (PFM) into a Selective-Enhanced Model (SEM). It can improve the model performance and accelerate the training process. Although a few surveys have investigated related works of data selection, there is a lack of comprehensive comparison between existing methods due to their various experimental settings. To address this issue, we first propose a three-stage scheme for data selection and comprehensively review existing works according to this scheme. Then, we design a unified comparing method with ratio-based efficiency indicators and ranking-based feasibility indicators to overcome the difficulty of comparing various models with diverse experimental settings. After an in-depth comparative analysis, we find that the more targeted method with data-specific and model-specific quality labels has higher efficiency, but the introduction of additional noise information should be avoided when designing selection algorithms. Finally, we summarize the trends in data selection and highlight the short-term and long-term challenges to guide future research",
    "checked": true,
    "id": "2a88c2b6a7d838f5635e707070440353b8980b33",
    "semantic_title": "take the essence and discard the dross: a rethinking on data selection for fine-tuning large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=12fPx9tG61": {
    "title": "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering",
    "volume": "review",
    "abstract": "The fusion of language models (LMs) and knowledge graphs (KGs) is widely used in commonsense question answering, but generating faithful explanations remains challenging. Current methods often overlook path decoding faithfulness, leading to divergence between graph encoder outputs and model predictions. We identify confounding effects and LM-KG misalignment as key factors causing spurious explanations. To address this, we introduce the LM-KG Fidelity metric to assess KG representation reliability and propose the LM-KG Distribution-aware Alignment (LKDA) algorithm to improve explanation faithfulness. Without ground truth, we evaluate KG explanations using the proposed Fidelity-Sparsity Trade-off Curve. Experiments on CommonsenseQA and OpenBookQA show that LKDA significantly enhances explanation fidelity and model performance, highlighting the need to address distributional misalignment for reliable commonsense reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CABNvm4tQ5": {
    "title": "MEMLLM: Finetuning LLMs to Use Explicit Read-Write Memory",
    "volume": "review",
    "abstract": "While current large language models (LLMs) perform well on many knowledge-related tasks, they are limited by relying on their parameters as an implicit storage mechanism. As a result, they struggle with memorizing rare events and with updating their memory as facts change over time. In addition, the uninterpretable nature of parametric memory makes it challenging to prevent hallucination. Model editing and augmenting LLMs with parameters specialized for memory are only partial solutions. In this paper, we introduce MemLLM, a novel method of enhancing LLMs by integrating a structured and explicit read-and-write memory module. MemLLM tackles the aforementioned challenges by enabling dynamic interaction with the memory and improving the LLM's capabilities in using stored knowledge. Our experiments indicate that MemLLM enhances the LLM's performance and interpretability, in language modeling in general and knowledge-intensive tasks in particular. We see MemLLM as an important step towards making LLMs more grounded and factual through memory augmentation",
    "checked": false,
    "id": "47c8f0d7232f52f1a48e933e32309dc35ad85f49",
    "semantic_title": "memllm: finetuning llms to use an explicit read-write memory",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=BM5O1r4T2r": {
    "title": "Benchmarking Foundation Models on Exceptional Cases: Dataset Creation and Validation",
    "volume": "review",
    "abstract": "Foundation models (FMs) have achieved significant success across various tasks, leading to research on benchmarks for commonsense and reasoning abilities. However, there is a lack of studies on FMs performance in exceptional scenarios. This paper addresses these cases for the first time, developing a novel dataset for comprehensive FMs evaluation across multiple modalities, including graphic novels, calligraphy, news articles, and lyrics. It includes tasks for instance classification, character recognition, token prediction, and text generation. The paper also proposes prompt engineering techniques like Chain-of-Thought (CoT) and CoT+Few-Shot to enhance performance. Validation of FMs using various methods revealed improvements. The code repository is accessible at: https://anonymous.4open.science/r/Exceptional-Dataset-for-FMs/README.md",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=np6hrTv7aW": {
    "title": "Induction Heads as a Primary Mechanism for Pattern Matching in In-context Learning",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown a remarkable ability to learn and perform complex tasks through in-context learning (ICL). However, a comprehensive understanding of its internal mechanisms is still lacking. This paper explores the role of induction heads in a few-shot ICL setting. We analyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract pattern recognition and NLP tasks. Our results show that even a minimal ablation of induction heads leads to ICL performance decreases of up to ~32\\% for abstract pattern recognition tasks, bringing the performance close to random. For NLP tasks, this ablation substantially decreases the model's ability to benefit from examples, bringing few-shot ICL performance close to that of zero-shot prompts. We further use attention knockout to disable specific induction patterns, and present fine-grained evidence for the role that induction mechanism plays in ICL",
    "checked": false,
    "id": "6d84377d5765229d621c4b2a209e44479ef33516",
    "semantic_title": "induction heads as an essential mechanism for pattern matching in in-context learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FHmsbqAiH0": {
    "title": "Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors",
    "volume": "review",
    "abstract": "Multiple-choice visual question answering (VQA) is to automatically choose a correct answer from a set of choices after reading an image. Existing efforts have been devoted to a separate generation of an image-related question, a correct answer, or challenge distractors. By contrast, we turn to a holistic generation and optimization of questions, answers, and distractors (QADs) in this study. This integrated generation strategy eliminates the need for human curation and guarantees information consistency. Furthermore, we first propose to put the spotlight on different image regions to diversify QADs. Accordingly, a novel framework ReBo is formulated in this paper. ReBo cyclically generates each QAD based on a recurrent multimodal encoder, and each generation is focusing on a different area of the image compared to those already concerned by the previously generated QADs. In addition to traditional VQA comparisons with state-of-the-art approaches, we also validate the capability of ReBo in generating augmented data to benefit VQA models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l6y6Ei4TjA": {
    "title": "Temporal Knowledge Graph Question Answering: A Survey",
    "volume": "review",
    "abstract": "Knowledge Base Question Answering (KBQA) has been a long-standing field to answer questions based on knowledge bases. Recently, the evolving dynamics of knowledge have attracted a growing interest in Temporal Knowledge Graph Question Answering (TKGQA), an emerging task to answer temporal questions. However, this field grapples with ambiguities in defining temporal questions and lacks a systematic categorization of existing methods for TKGQA. In response, this paper provides a thorough survey from two perspectives: the taxonomy of temporal questions and the methodological categorization for TKGQA. Specifically, we first establish a detailed taxonomy of temporal questions engaged in prior studies. Subsequently, we provide a comprehensive review of TKGQA techniques of two categories: semantic parsing-based and TKG embedding-based. Building on this review, the paper outlines potential research directions aimed at advancing the field of TKGQA. This work aims to serve as a comprehensive reference for TKGQA and to stimulate further research",
    "checked": true,
    "id": "2b55fc06077910c9f83137513764248388aac051",
    "semantic_title": "temporal knowledge graph question answering: a survey",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OhozicmObs": {
    "title": "Unlocking the Global Synergies in Low-Rank Adapters",
    "volume": "review",
    "abstract": "Low-rank adaption (LoRA) has been the de-facto parameter-efficient fine-tuning technique for large language models. We present HeteroLoRA, a light-weight search algorithm that leverages zero-cost proxies to allocate the limited LoRA trainable parameters across the model for better fine-tuned performance. In addition to the allocation for the standard LoRA-adapted models, we also demonstrate the efficacy of HeteroLoRA by performing the allocation in a more challenging search space that includes LoRA modules and LoRA-adapted shortcut connections. Experiments show that HeteroLoRA enables improvements in model performance given the same parameter budge. For example, on RTE, we see an improvement of 6.7% in accuracy with a similar training parameter budget compared to a variety of state-of-the-art methods. We will open-source our algorithm once the paper is accepted",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EmzhaCoNIv": {
    "title": "Adversarial Winograd Schema Challenge",
    "volume": "review",
    "abstract": "While Large Language Models (LLMs) have showcased remarkable proficiency in reasoning, there is still a concern about hallucinations and unreliable reasoning issues due to semantic associations and superficial logical chains. To evaluate the extent to which LLMs perform robust reasoning instead of relying on superficial logical chains, we propose a new evaluation dataset, the Adversarial Winograd Schema Challenge (AWSC), based on the famous Winograd Schema Challenge (WSC) dataset. By simply replacing the entities with those that are more associated with the wrong answer, we find that the performance of LLMs drops significantly despite the rationale of reasoning remaining the same. Furthermore, we propose Abstraction-of-Thought (AoT), a novel prompt method for recovering adversarial cases to normal cases to improve LLMs' robustness and consistency in reasoning, as demonstrated by experiments on AWSC",
    "checked": false,
    "id": "b4355875a250258eddea5e7abc979f23e8209a4a",
    "semantic_title": "who killed the winograd schema challenge?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4HOMjp5q6M": {
    "title": "FASST: Fast LLM-based Simultaneous Speech Translation",
    "volume": "review",
    "abstract": "Simultaneous speech translation (SST) takes streaming speech input and generates text translation on the fly. Existing methods either have high latency due to recomputation of input representations, or fall behind of offline ST in translation quality. In this paper, we propose FASST, a fast large language model based method for streaming speech translation. We propose blockwise-causal speech encoding and consistency mask, so that streaming speech input can be encoded incrementally without recomputation. Furthermore, we develop a two-stage training strategy to optimize FASST for simultaneous inference. We evaluate FASST and multiple strong prior models on MuST-C dataset. Experiment results show that FASST achieves the best quality-latency tradeoff. It outperforms the previous best model by an average of 1.5 BLEU under the same latency for English to Spanish translation",
    "checked": true,
    "id": "39cb370560705ae11184a10c28e9a0fb787b470f",
    "semantic_title": "fasst: fast llm-based simultaneous speech translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=obohYdeCYf": {
    "title": "ExpertEase: A Multi-Agent Framework for Grade-Specific Document Simplification with Large Language Models",
    "volume": "review",
    "abstract": "Text simplification is crucial for making texts more accessible, yet current research primarily focuses on sentence-level simplification, neglecting document-level simplification and the different reading levels of target audiences. To bridge these gaps, we introduce ExpertEase, a multi-agent framework for grade-specific document simplification using Large Language Models (LLMs). ExpertEase simulates real-world text simplification by introducing expert, teacher, and student agents that cooperate on the task and rely on external tools for calibration. Experiments demonstrate that this multi-agent approach significantly enhances LLMs' ability to simplify reading materials for diverse audiences. Furthermore, we evaluate the performance of LLMs varying in size and type, and compare LLM-generated texts with human-authored ones, highlighting their potential in educational resource development and guiding future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FAILS1a5pf": {
    "title": "GrammaMT: Improving Machine Translation with Grammar-Informed In-Context Learning",
    "volume": "review",
    "abstract": "We introduce GrammaMT, a grammatically-aware prompting approach for machine translation that uses Interlinear Glossed Text (IGT), a common form of linguistic annotation describing lexical and functional morphemes of source sentences. GrammaMT proposes two prompting strategies: gloss-shot and chain-gloss. Both are training-free, require only a few examples, and involve minimal effort to collect, making them well-suited for low-resource setups. Experiments and ablation studies on open-source instruction-tuned LLMs, across three different benchmarks, demonstrate the benefits of leveraging interlinear gloss resources for machine translation. GrammaMT improves the translation performance for various low-resource to high-resource languages in the largest existing corpus of IGT data, on the challenging 2023 SIGMORPHON Shared Task data across rarely-seen, endangered languages, and even in an out-of-domain setting within FLORES",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=39KtrP3axP": {
    "title": "CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation in RAG",
    "volume": "review",
    "abstract": "Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large Language Models (LLMs) by referencing external documents. However, the misinformation in external documents may mislead LLMs' generation. To address this issue, we explore the task of \"credibility-aware RAG\", in which LLMs automatically adjust the influence of retrieved documents based on their credibility scores to counteract misinformation. To this end, we introduce a plug-and-play method named $\\textbf{Cr}$edibility-aware $\\textbf{A}$ttention $\\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in LLMs and adjusts their attention scores based on the credibility of the documents, thereby reducing the impact of low-credibility documents. Experiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and Qwen-7B show that CrAM improves the RAG performance of LLMs against misinformation pollution by over 20%, even surpassing supervised fine-tuning methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gHRYAU6Wrd": {
    "title": "FinLlama: LLM-Based Financial Sentiment Analysis for Algorithmic Trading",
    "volume": "review",
    "abstract": "Online sources of financial news have a profound influence on both market movements and trading decisions. Standard sentiment analysis employs a lexicon-based approach to aid financial decisions, but struggles with context sensitivity and word ordering. On the other hand, Large Language Models (LLMs) are powerful, but are not finance-specific and require significant computational resources. To this end, we introduce a finance specific LLM framework, based on the Llama 2 7B foundational model, in order to benefit from its generative nature and comprehensive language manipulation. Such a generator-discriminator scheme, referred to as FinLlama, both classifies sentiment valence and quantifies its strength, offering a nuanced insight into financial news. The FinLlama model is fine-tuned on supervised financial sentiment analysis data, to make it handle the complexities of financial lexicon and context, and is equipped with a neural network-based decision mechanism. The subsequent parameter-efficient fine-tuning optimises trainable parameters, thus minimising computational and memory requirements without sacrificing accuracy. Simulation results demonstrate the ability of FinLlama to increase market returns in portfolio management scenarios, yielding high-return and resilient portfolios, even during volatile periods",
    "checked": false,
    "id": "15b0a6ccb198b2936e36266be992da78a29953fd",
    "semantic_title": "finllama: financial sentiment classification for algorithmic trading applications",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=6qioVR5ecT": {
    "title": "Multi-property Steering of Large Language Models with Dynamic Activation Composition",
    "volume": "review",
    "abstract": "Activation steering methods were shown to be effective in conditioning language model generation by additively intervening over models' intermediate representations. However, the evaluation of these techniques has so far been limited to single conditioning properties and synthetic settings. In this work, we conduct a comprehensive evaluation of various activation steering strategies, highlighting the property-dependent nature of optimal parameters to ensure a robust effect throughout generation. To address this issue, we propose Dynamic Activation Composition, an information-theoretic approach to modulate the steering intensity of one or more properties throughout generation. Our experiments on multi-property steering show that our method successfully maintains high conditioning while minimizing the impact of conditioning on generation fluency",
    "checked": true,
    "id": "cdab286b5b28f03aa9dda2b818f30f8a32a809d4",
    "semantic_title": "multi-property steering of large language models with dynamic activation composition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cg7sGNXwRQ": {
    "title": "Improving LLM Pretraining by Filtering Out Advertisements",
    "volume": "review",
    "abstract": "Data has been recognized as a vital factor for Large Language Models (LLMs), prompting the development of various data selection methods to optimize pretraining data. Among these, the loss-based filtering method has gained popularity due to its straightforwardness. However, our empirical findings suggest that this approach may lead to performance degradation on knowledge-intensive benchmarks, such as the MMLU. To address this issue, we propose filtering out low-information text, particularly advertisements, which constitute a significant portion of internet content. We employed a 100M parameter proxy model to compare these two methods. Despite its smaller size, the proxy model's results accurately predict the downstream metrics when scaled to 3B models. This study demonstrates that a 100M parameter proxy model is sufficient for comparing different data selection strategies, and our experiments across various benchmarks confirm the effectiveness of eliminating advertisements from pretraining data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g9MgAxi6ix": {
    "title": "SiRA: Sparse Mixture of Low Rank Adaptation",
    "volume": "review",
    "abstract": "Parameter Efficient Tuning (PET) techniques such as Low-rank Adaptation~(LoRA) are effective methods to adapt Large Language Models to downstream tasks. We propose Sparse mixture of low Rank Adaption~(SiRA), which uses Sparse Mixture of Experts (SMoE) by enforcing conditional computation with top k LoRA weights. SiRA is optimized through a combination of training techniques, including an auxiliary loss encouraging load balancing, a capacity limit which restricts the maximum number of tokens each expert can process, and novel expert dropout on top of the gating network. Through extensive experiments, we show that SiRA performs better than LoRA and other mixture of expert approaches across different single-task and multiple-task settings. Results show SiRA has more orthogonal low rank spaces and consumes less computing resources compared to other MoE variants",
    "checked": true,
    "id": "525cd5d5c7d823d0b2ad4eaf086622940d45ed6d",
    "semantic_title": "sira: sparse mixture of low rank adaptation",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=QoSLasuLAa": {
    "title": "COMPACT: Compressing Retrieved Documents Actively for Question Answering",
    "volume": "review",
    "abstract": "Retrieval-augmented generation supports language models to strengthen their factual groundings by providing external context. However, language models often face challenges in locating and integrating extensive information, diminishing their effectiveness in solving complex questions. Query-focused compression tackles this issue by filtering out information irrelevant to the query, but current methods still struggle in realistic scenarios where crucial information may not be located with a single-step approach. To overcome this limitation, we introduce COMPACT, a novel framework that employs an active strategy to condense extensive documents without losing key information. COMPACT flexibly operates as a cost-efficient plug-in module with any off-the-shelf retriever or reader model, achieving extremely high compression rates (44x). Our experiments demonstrate that COMPACT brings significant improvements in both compression rate and QA performance on multi-hop question-answering datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OG8oRwEREg": {
    "title": "FinanceGPT-B: A Large Language Model with Multi-Stage Structure for Financial Breakout Detection",
    "volume": "review",
    "abstract": "Trading range breakout is a key method in the technical analysis of financial trading, widely employed by traders in financial markets such as stocks, futures, and foreign exchange. However, distinguishing between true and false breakout and providing the correct rationale cause significant challenges to investors. Traditional quantitative methods require large amounts of data and cannot directly present the reasoning process to users, making them less than perfect in this field. Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar. The reason is that the unique data and specific knowledge are required in breakout detection. To address these issues, we created the first financial breakout dataset and introduce FinanceGPT-B, the premier large language model for financial breakout detection. Furthermore, we have developed a novel framework for large language models, namely multi-stage structure, effectively reducing mistakes in downstream applications. Experimental results indicate that compared to GPT-3.5, FinanceGPT-B improves the average accuracy of answers and rational by 49.97\\%, with the multi-stage structure contributing 9.72\\% to the improvement. Additionally, it outperforms ChatGPT-4 by 37.30\\%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YEPygp6YTS": {
    "title": "A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models",
    "volume": "review",
    "abstract": "Due to the continuous emergence of new data, version updates have become an indispensable requirement for Large Language Models (LLMs). The training paradigms for version updates of LLMs include pre-training from scratch (PTFS) and continual pre-training (CPT). Preliminary experiments demonstrate that PTFS exhibits better pre-training performance, while the training cost of CPT is lower. Moreover, their performance and training cost gaps gradually widen with the version updates processing. To investigate the underlying reasons for this phenomenon, we analyze the effect of learning rate adjustments during the two stages of CPT: preparing an initialization checkpoint and conducting pre-training based on this checkpoint. We find that a large learning rate in the first stage and a complete learning rate decaying process in the second stage are crucial for version updates of LLMs. Hence, we propose a learning rate path switching training paradigm. Our paradigm comprises one main path, where we pre-train a LLM with the maximal learning rate, and multiple branching paths, each of which corresponds to an update of the LLM with newly-added training data. Compared with PTFS, when training four versions of LLMs, our paradigm can reduce the total training cost to 58% while maintaining comparable pre-training performance. In addition, we also validate the generalization of our paradigm, further proving its practicability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ex4LKiSAKc": {
    "title": "Ground Every Sentence: Improving Retrieval-Augmented LLMs with Interleaved Reference-Claim Generation",
    "volume": "review",
    "abstract": "Retrieval-Augmented Generation (RAG) has been widely adopted to enhance Large Language Models (LLMs) in knowledge-intensive tasks. Recently, Attributed Text Generation (ATG) has attracted growing attention, which provides citations to support the model's responses in RAG, so as to enhance the credibility of LLM-generated content and facilitate verification. Prior methods mainly adopt coarse-grained attributions, linking to passage-level references or providing paragraph-level citations. However, these methods still fall short in verifiability and require certain time costs for fact checking. This paper proposes a fine-grained ATG method called ReClaim (Refer & Claim), which alternates the generation of references and answers step by step. Unlike traditional coarse-grained attribution, ReClaim allows the model to add sentence-level fine-grained citations to each answer sentence in long-form question-answering tasks. Our experiments encompass various training and inference methods and multiple LLMs, verifying the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XwJFHV5C8l": {
    "title": "Continual Learning for Large Language Models: A Survey",
    "volume": "review",
    "abstract": "Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task",
    "checked": true,
    "id": "bd0cd89337cc40d39d3a4cbe9c8709e06e877f3e",
    "semantic_title": "continual learning for large language models: a survey",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=K7hZCc3d95": {
    "title": "On the Reliability of Psychological Scales on Large Language Models",
    "volume": "review",
    "abstract": "Recent research has extended beyond assessing the performance of Large Language Models (LLMs) to examining their characteristics from a psychological standpoint, acknowledging the necessity of understanding their behavioral characteristics. The administration of personality tests to LLMs has emerged as a noteworthy area in this context. However, the suitability of employing psychological scales, initially devised for humans, on LLMs is a matter of ongoing debate. Our study aims to determine the reliability of applying personality assessments to LLMs, explicitly investigating whether LLMs demonstrate consistent personality traits. Analyzing responses under 2,500 settings reveals that various LLMs show consistency in responses to the Big Five Inventory, indicating a high degree of reliability. Furthermore, our research explores the potential of gpt-3.5-turbo to emulate diverse personalities and represent various groupsâ€”a capability increasingly sought after in social sciences for substituting human participants with LLMs to reduce costs. Our findings reveal that LLMs have the potential to represent different personalities with specific prompt instructions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ahh5eXkKKc": {
    "title": "To Know or Not To Know? Analyzing Self-Consistency of Large Language Models under Ambiguity",
    "volume": "review",
    "abstract": "One of the major aspects contributing to the striking performance of large language models (LLMs) is the vast amount of factual knowledge accumulated during pre-training. Yet, many LLMs suffer from self-inconsistency, which raises doubts about their trustworthiness and reliability. In this paper, we focus on entity type ambiguity and analyze current state-of-the-art LLMs for their proficiency and consistency in applying their factual knowledge when prompted for entities under ambiguity. To do so, we propose an evaluation protocol that disentangles knowing from applying knowledge and test state-of-the-art LLMs on 49 entities. Our experiments reveal that LLMs perform poorly with ambiguous prompts, achieving only 80% accuracy. Our results further demonstrate systematic discrepancies in LLM behavior and their failure to consistently apply information, indicating that the models can exhibit knowledge without being able to utilize it, significant biases for preferred readings, as well as self-inconsistencies. Our study highlights the importance of handling entity ambiguity in future for more trustworthy LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K0ajSlez2E": {
    "title": "Towards Verifiable Text Generation with Evolving Memory and Self-Reflection",
    "volume": "review",
    "abstract": "Despite the remarkable ability of large language models (LLMs) in language comprehension and generation, they often suffer from producing factually incorrect information, also known as hallucination. A promising solution to this issue is verifiable text generation, which prompts LLMs to generate content with citations for accuracy verification. However, verifiable text generation is non-trivial due to the focus-shifting phenomenon, the intricate reasoning needed to align the claim with correct citations, and the dilemma between the precision and breadth of retrieved documents. In this paper, we present VTG, an innovative framework for Verifiable Text Generation with evolving memory and self-reflection. VTG introduces evolving long short-term memory to retain both valuable documents and recent documents. A two-tier verifier equipped with an evidence finder is proposed to rethink and reflect on the relationship between the claim and citations. Furthermore, active retrieval and diverse query generation are utilized to enhance both the precision and breadth of the retrieved documents. We conduct extensive experiments on five datasets across three knowledge-intensive tasks and the results reveal that VTG significantly outperforms baselines",
    "checked": true,
    "id": "69e2aa1723b46e4df0445e8ecb838636b0911cd7",
    "semantic_title": "towards verifiable text generation with evolving memory and self-reflection",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=QyEBiBFC7C": {
    "title": "Beyond English: Examining the Impact of Prompt Translation Strategies in Multilingual Natural Language Tasks",
    "volume": "review",
    "abstract": "Despite advances in the multilingual capabilities of Large Language Models (LLMs) across diverse Natural Language Processing (NLP) tasks, English remains the dominant language for LLM research and development. This has led to the widespread practice of pre-translation, i.e., translating the task prompt into English before inference. Selective pre-translation, a more surgical approach, focuses on translating specific prompt components. However, its current use lacks a systematic research foundation. Consequently, the optimal pre-translation strategy for various multilingual settings and tasks remains unclear. In this work, we aim to uncover the optimal setup for pre-translation by systematically assessing its modes of use. Specifically, we view the prompt as a modular entity, composed of four functional parts: instruction, context, examples (zero-shot / few-shot), and output, either of which could be translated or not. We evaluate pre-translation strategies across 35 languages covering both low and high-resource languages, and assessing various capabilities including Question Answering (QA), Natural Language Inference (NLI), Named Entity Recognition (NER), and Abstractive Summarization. Our experiments uncover the impact of factors as translation quality, similarity to English, and the size of pre-trained data, on the model performance with pre-translation. Finally, we suggest practical guidelines for choosing the optimal strategy in various multilingual scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JMzb5fRm2k": {
    "title": "PreAlign: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment",
    "volume": "review",
    "abstract": "Large language models demonstrate reasonable multilingual abilities, despite predominantly English-centric pretraining. However, the spontaneous multilingual alignment in these models is shown to be weak, leading to unsatisfactory cross-lingual transfer and knowledge sharing. Previous works attempt to address this issue by explicitly injecting multilingual alignment information during or after pretraining. Thus for the early stage in pretraining, the alignment is weak for sharing information or knowledge across languages. In this paper, we propose PreAlign, a framework that establishes multilingual alignment prior to language model pretraining. PreAlign injects multilingual alignment by initializing the model to generate similar representations of aligned words and preserves this alignment using a code-switching strategy during pretraining. Extensive experiments in a synthetic English to English-Clone setting demonstrate that \\method significantly outperforms standard multilingual joint training in language modeling, zero-shot cross-lingual transfer, and cross-lingual knowledge application. Further experiments in real-world scenarios further validate PreAlign's effectiveness across various model sizes",
    "checked": true,
    "id": "a3ca19771db42fa926f61d79eac4958b8a2e2b66",
    "semantic_title": "prealign: boosting cross-lingual transfer by early establishment of multilingual alignment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SxGFTTQwCm": {
    "title": "Zero-Shot Fact Verification via Natural Logic and Large Language Models",
    "volume": "review",
    "abstract": "The recent development of fact verification systems with natural logic has enhanced their explainability by aligning claims with evidence through set-theoretic operators, providing faithful justifications. Despite these advancements, such systems often rely on a large amount of training data annotated with natural logic. To address this issue, we propose a zero-shot method that utilizes the generalization capabilities of instruction-tuned large language models. To comprehensively assess the zero-shot capabilities of our method and other fact verification systems, we evaluate all models on both artificial and real-world claims, including datasets in Danish and Mandarin Chinese. We compare our method against other fact verification systems in two setups. First, in the zero-shot generalization setup, our approach outperforms other systems that were not specifically trained on natural logic data, achieving an average accuracy improvement of 8.61 points over the best-performing baseline. Second, in the zero-shot transfer setup, we demonstrate that current natural-logic-based systems do not generalize well to other domains. Our method performs better on all datasets with real-world claims compared to systems that were trained on datasets with artificial claims",
    "checked": false,
    "id": "09238228c405323bcb20232f424258a8872f150c",
    "semantic_title": "few-shot image classification by generating natural language rules",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=og2LJuXqmP": {
    "title": "Using Ordinal Labels for Text Augmentation and Simultaneous Contrastive Learning for DownStream Task",
    "volume": "review",
    "abstract": "Leveraging large language models, advancements in text augmentation and embedding models for downstream tasks have shown promise, However yet challenges remain in distinguishing texts with similar meanings. The proposed scheme, incorporating ordered labels to enhance sequence information, employs an integrated technique combining Contrastive and Downstream Learning. The proposed scheme outperforms Full Fine-Tuning methods using only classfication learning in text classification because it effectively uses ordered labels to train the model to distinguish similar texts with greater accuracy. our method boosts data diversity and model accuracy by refining the model's sensitivity to nuances, utilizing strongly hard-negative samples in generated texts to further enhance Contrastive Learning outcomes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eUCxcWsha8": {
    "title": "Tutor-ICL: Guiding Large Language Models for Improved In-Context Learning Performance",
    "volume": "review",
    "abstract": "There has been a growing body of work focusing on the in-context learning (ICL) abilities of large language models (LLMs). However, it is an open question of how effective ICL can be. This paper presents Tutor-ICL, a simple prompting method that guides LLMs through the ICL process, inspired by how effective instructors might engage their students in learning a task. Specifically, we propose presenting exemplar answers in a comparative format rather than the traditional single-answer format. We also show that including the test instance before the exemplars can improve performance, making it easier for LLMs to focus on relevant exemplars. Lastly, we include a summarization step before attempting the test, following a common human practice. Experiments on various classification tasks, conducted across both decoder-only LLMs (Llama 2, 3) and encoder-decoder LLMs (Flan-T5-XL, XXL), show that Tutor-ICL consistently boosts performance, achieving up to a 13.76% increase in accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vm0KCWkLHb": {
    "title": "Mitigating Hallucination Caused by Excessive Reliance on LLM within MLLM instead of Images",
    "volume": "review",
    "abstract": "In the domain of multimodal generation and comprehension, multimodal large language models (MLLMs), which integrate visual encoders with large language models, have garnered significant success. However, solely relying on modal connection layers/modules to unify these models can lead to a neglect of image information, resulting in visual hallucinations. This manifests as generated text that is independent of the image content, such as descriptions of objects not present within the image. To mitigate this issue, we introduce a fine-tuning approach: Adversarial Contrast Dual Fine-tuning (ACD for short). This approach leverages the MLLM itself and employs the Fast Gradient Sign Method (FGSM) to generate adversarial image samples. During fine-tuning, both the original and adversarial images are utilized to perform dual contrastive fine-tuning on the MLLM. The experimental results show that our method significantly reduces hallucinations without any external annotations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jL5iBVC1xq": {
    "title": "CharSS: Character-Level Transformer Model for Sanskrit Word Segmentation",
    "volume": "review",
    "abstract": "Subword tokens in Indian languages inherently carry meaning, and isolating them can enhance NLP tasks, making sub-word segmentation a crucial process. Segmenting Sanskrit and other Indian languages into subtokens is not straightforward, as it may include sandhi, which may lead to changes in the word boundaries. We propose a new approach of utilizing a CharSS: Character-level Transformer model for Sanskrit Word Segmentation. We perform experiments on three benchmark datasets to compare the performance of our method against existing methods. On the UoH+SandhiKosh dataset, our method outperforms the current state-of-the-art system by an absolute gain of 6.72 points in split prediction accuracy. On the hackathon dataset our method achieves a gain of 2.27 points over the current SOTA system in terms of perfect match metric. We also propose a use-case of Sanskrit-based segments for a linguistically informed translation of technical terms to lexically similar low-resource Indian languages. In two separate experimental settings for this task, we achieve an average improvement of 8.46 and 6.79 chrF++ scores, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s90ShMOcYq": {
    "title": "Can Frozen Large Language Models Solve Visual Reasoning?",
    "volume": "review",
    "abstract": "We present ReasonLM, a simple framework which utilizes a pre-trained, frozen large language model (LLM) for visual reasoning tasks, and achieves competitive performance on ACRE and MEWL. We demonstrate for the first time that a frozen LLM serves as a task-agnostic reasoning machine for diverse reasoning tasks that involve object recognition, causal induction, and relation modeling. ReasonLM does not rely on synthesizing symbolic programs or self-supervised visual representation learning. Rather, it learns an object-centric, light-weight visual encoder from scratch. Via its simplified design, we investigate the essential design choices for strong visual reasoning performance. Code and model will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zfAmHkODTf": {
    "title": "NYK-MS: A Well-annotated Multi-modal Metaphor and Sarcasm Understanding Benchmark on Cartoon-Caption Dataset",
    "volume": "review",
    "abstract": "Metaphor and sarcasm are common figurative expressions in people's communication, especially on the Internet or the memes popular among teenagers. We create a new benchmark named NYK-MS (NewYorKer for Metaphor and Sarcasm), which contains 1,583 samples for metaphor understanding tasks and 1,578 samples for sarcasm understanding tasks. These tasks include whether it contains metaphor/sarcasm, which word or object contains metaphor/sarcasm, what does it satirize and why does it contains metaphor/sarcasm, all of the 7 tasks are well-annotated by at least 3 annotators. We annotate the dataset for several rounds to improve the consistency and quality, and use GUI and GPT-4V to raise our efficiency. Based on the benchmark, we conduct plenty of experiments. In the zero-shot experiments, we show that Large Language Models (LLM) and Large Multi-modal Models (LMM) can't do classification task well, and as the scale increases, the performance on other 5 tasks improves. In the experiments on traditional pre-train models, we show the enhancement with augment and alignment methods, which prove our benchmark is consistent with previous dataset and requires the model to understand both of the two modalities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2p3FI7YZHu": {
    "title": "SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support",
    "volume": "review",
    "abstract": "Developing specialized dialogue systems for mental health support requires multi-turn conversation data, which has recently garnered increasing attention. However, gathering and releasing large-scale, real-life multi-turn conversations to facilitate advancements in mental health presents challenges due to data privacy protection, as well as the time and cost involved. To address the challenges related to data scarcity, we introduce SMILE, a single-turn to multi-turn inclusive language expansion technique with a solid theoretical foundation that prompts ChatGPT to transform public single-turn dialogues into multi-turn ones. Our study first focuses on basic characteristics, dialogue diversity, and quality among four large language models, verifying that our proposed method is superior to other baseline methods and that GPT-4o is the optimal option. Thus, we employ our method to generate a large-scale, diverse, and high-quality dialogue dataset named SmileChat, consisting of 13k dialogues. Finally, we utilize SmileChat to fine-tune six large language models, giving birth to mental health chatbots, MeChat. Empirical evaluations demonstrate that MeChat excels in generating empathic, professional, helpful, and safe responses in mental health support, showing its high quality and practicality",
    "checked": true,
    "id": "83bc360fa9874564cf8ccbd3a00e6e14dbe4e4e6",
    "semantic_title": "smile: single-turn to multi-turn inclusive language expansion via chatgpt for mental health support",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=SgPmlIRgBC": {
    "title": "Enhance Reasoning of Large Language Models via Macro-Micro Self-Training",
    "volume": "review",
    "abstract": "Decomposing complex problems into smaller stages has proven to be highly effective in enhancing the reasoning capabilities of Large Language Models (LLMs). However, as the reasoning process becomes more intricate, uncertainties and errors tend to accumulate, making it challenging to achieve precise final outcomes. Overcoming this challenge and addressing uncertainty in multi-step reasoning necessitates innovative approaches. In this regard, we propose a novel macro-micro self-training method. Our approach leverages self-evaluation and self-modification to enable LLMs to continuously refine their outputs. Through self-evaluation, LLMs assess the accuracy of their generated outputs, while the critical aspect of self-modification allows for iterative refinement of these outputs. To ensure comprehensive refinement, we combine macro evaluation and modification of the entire code structure with micro analysis, where each line of code is individually assessed and refined in line with the problem statement. This dual approach ensures coherent handling of both syntax and semantics. Empirically, our results demonstrate the effectiveness of our approach, as it outperforms existing methods across all settings. Our method enables LLMs to achieve new levels of reasoning capability, providing superior performance in various tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QelMGP8yie": {
    "title": "Demostrations Aren't All You Need For Long-form Generation! Learning Task-Inherent Attribute Guidelines For Large Language Models",
    "volume": "review",
    "abstract": "We study the sufficiency of demonstrations in enabling pre-trained large language models (LLMs) to implicitly learn the underlying task distribution for long-form generation. We prove the answer is no. For any long-form generation task, we show that if an LLM fails to initially grasp the task's language distribution, demonstrations alone are insufficient. This gap is caused by a lack of explicit task-language distribution characterization exposed to the model. Addressing this by capturing these distributions explicitly through task guidelines enhances model performance. We then present LongGuide, the first efficient algorithm that generates two types of guidelines as additional instructions for LLMs: (i) Metric Guideline (MG) that instructs models to optimize for selected metrics; and (ii) Output Constraint Guideline (OCG) that constrains generation at both the token and sentence levels. LongGuide automatically selects the most useful combination of guidelines, improving strong open- and closed-source LLMs by 5.39% and 6.58% under zero- and few-shot settings across seven tasks. Furthermore, LongGuide enhances LLMs beyond demonstrations, is learnable by weaker models to enhance stronger ones, and synergistically combines with prompt optimizers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dPELTS1rUK": {
    "title": "Iterative Data Augmentation with Large Language Models for Aspect-based Sentiment Analysis",
    "volume": "review",
    "abstract": "Aspect-based Sentiment Analysis (ABSA) is an important sentiment analysis task, which aims to determine the sentiment polarity towards an aspect in a sentence. Due to the expensive and limited labeled data, data augmentation (DA) has become the standard for improving the performance of ABSA. However, current DA methods usually have some shortcomings: 1) poor fluency and coherence, 2) lack of diversity of generated data, and 3) reliance on some existing labeled data, hindering its applications in real-world scenarios. In response to these problems, we propose a systematic iterative data augmentation framework, namely IterD, to boost the performance of ABSA. The core of IterD is to leverage the powerful ability of large language models (LLMs) to iteratively generate more fluent and diverse synthetic labeled data, starting from an unsupervised sentence corpus. Extensive experiments on 4 widely-used ABSA benchmarks show that IterD brings consistent and significant performance gains among 5 baseline ABSA models. More encouragingly, the synthetic data generated by IterD can achieve comparable or even better performance against the manually annotated data",
    "checked": true,
    "id": "84334c25e410e1e2998f1cdb6c490aab5d32855d",
    "semantic_title": "iterative data augmentation with large language models for aspect-based sentiment analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t4xLFxSsjr": {
    "title": "Graph Elicitation for Guiding Multi-Step Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "Chain-of-Thought (CoT) prompting along with sub-question generation and answering has enhanced multi-step reasoning capabilities of Large Language Models (LLMs). However, prompting the LLMs to directly generate sub-questions is suboptimal since they sometimes generate redundant or irrelevant questions. To deal with them, we propose a GE-Reasoning method, which directs LLMs to generate proper sub-questions and corresponding answers. Concretely, given an input question, we first prompt the LLM to generate knowledge triplets, forming a graph representation of the question. Unlike conventional knowledge triplets, our approach allows variables as head or tail entities, effectively representing a question as knowledge triplets. Second, for each triplet, the LLM generates a corresponding sub-question and answer along with using knowledge retrieval. If the prediction confidence exceeds a threshold, the sub-question and prediction are incorporated into the prompt for subsequent processing. This approach encourages that sub-questions are grounded in the extracted knowledge triplets, reducing redundancy and irrelevance. Our experiments demonstrate that our approach outperforms previous CoT prompting methods and their variants on multi-hop question-answering benchmark datasets",
    "checked": true,
    "id": "5baaa716bbd880b940028ad3dd4f13e71b9cd856",
    "semantic_title": "graph elicitation for guiding multi-step reasoning in large language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=it6eZSe3TG": {
    "title": "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension",
    "volume": "review",
    "abstract": "Large language models are playing an increasingly significant role in molecular research, yet existing models often generate erroneous information. Traditional evaluations fail to assess a model's factual correctness. To rectify this absence, we present MoleculeQA, a novel question answering (QA) dataset which possesses 62K QA pairs over 23K molecules. Each QA pair, composed of a manual question, a positive option and three negative options, has consistent semantics with a molecular description from authoritative corpus. MoleculeQA is not only the first benchmark to evaluate molecular factual correctness but also the largest molecular QA dataset. A comprehensive evaluation on MoleculeQA for existing molecular LLMs exposes their deficiencies in specific aspects and pinpoints crucial factors for molecular modeling. Furthermore, we employ MoleculeQA in reinforcement learning to mitigate model hallucinations, thereby enhancing the factual correctness of generated information",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rw3Gzlopou": {
    "title": "Navigating Hallucinations for Reasoning of Unintentional Activities",
    "volume": "review",
    "abstract": "In this work we present a novel task of understanding unintentional human activities in videos. We formalize this problem as a reasoning task under zero-shot scenario, where given a video of an unintentional activity we want to know why it transitioned from intentional to unintentional. We first evaluate the effectiveness of current state-of-the-art Large Multimodal Models on this reasoning task and observe that they suffer from hallucination. We further propose a novel prompting technique, termed as Dream of Thoughts (DoT), which allows the model to navigate through hallucinated thoughts to achieve better reasoning. To evaluate the performance on this task, we also introduce three different specialized metrics designed to quantify the models reasoning capability. We perform our experiments on three datasets, OOPs, UCF-Crimes, and ReUAct, and our findings show that DOT prompting technique is able to outperform standard prompting, while minimizing hallucinations",
    "checked": true,
    "id": "f12b2e22ed2530e2b87054e5735208449616bf3c",
    "semantic_title": "navigating hallucinations for reasoning of unintentional activities",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Fe8vovPrCt": {
    "title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
    "volume": "review",
    "abstract": "Recent methods have demonstrated that Large Language Models (LLMs) can solve reasoning tasks better when they are encouraged to solve subtasks of the main task first. In this paper we devise a similar strategy that breaks down reasoning tasks into a problem decomposition phase and a problem solving phase and show that the strategy is able to outperform a single stage solution. Further, we hypothesize that the decomposition should be easier to distill into a smaller model compared to the problem solving because the latter requires large amounts of domain knowledge while the former only requires learning general problem solving strategies. We propose methods to distill these two capabilities and evaluate their impact on reasoning outcomes and inference cost. We find that we can distill the problem decomposition phase and at the same time achieve good generalization across tasks, datasets, and models. However, it is harder to distill the problem solving capability without losing performance and the resulting distilled model struggles with generalization. These results indicate that by using smaller, distilled problem decomposition models in combination with problem solving LLMs we can achieve reasoning with cost-efficient inference and local adaptation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xa4GYUSvhW": {
    "title": "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation",
    "volume": "review",
    "abstract": "Adapting pre-trained language models (PLMs) for cross-task generalization is a crucial research area within the field of NLP. While fine-tuning and in-context learning are effective approaches for adapting LMs to emerging tasks, they can be costly and inefficient. Recently, some researchers have focused on achieving efficient task adaptation via hypernetwork, which is a meta network that generates task-specific weights based on task-oriented information without any optimization. However, the training of hypernetworks often lacks stability since the optimization signal is not straightforward, and the task information is not adequately representative. Moreover, previous works train hypenetworks with the general corpus, which is struggling with few-shot adaptation. To address these issues, we introduce HyperLoRA, a hypernetwork for LoRA parameters generation involving hypernetwork pre-training on instruction-following data and generalization fine-tuning on sparse task data. Furthermore, we utilize a constrained training loss and a gradient-based demonstration selection strategy to enhance the training stability and performance. Experimental results and analysis across four benchmark datasets (P3, S-NI, BBH, and SuperGLUE) demonstrate the proposed approach has flexible generalization ability and superior performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IBPBukWkGx": {
    "title": "AIGT: AI Generative Table Based on Prompt Enhancement",
    "volume": "review",
    "abstract": "Tabular data, comprising over 80\\% of enterprise data assets, is crucial across various fields. With growing concerns about privacy protection and data-sharing restrictions, generating high-quality synthetic tabular data has become essential. Recent advancements demonstrate that large language models (LLMs) can effectively generate realistic tabular data by leveraging semantic information and avoiding the curse of dimensionality inherent in one-hot encoding of high-dimensional data. However, current methods do not fully exploit the rich information within tables. To address this, we introduce $\\underline{AI} ~\\underline{G}enerative ~\\underline{T}able$ based on prompt enhancement, a novel approach that utilizes metadata information, such as table descriptions and schemas, as prompts to generate ultra-high-quality synthetic data. To circumvent the token limit constraints of LLMs, we propose long-token partitioning algorithms that enable \\ours{} to model tables of arbitrary scale. AIGT achieves state-of-the-art performance on 20 public datasets and multi-scenario datasets within the Alipay risk control system. The source code, data, and other artifacts are available in supplementary materials",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3PvvmeHJAc": {
    "title": "LaCo: Large Language Model Pruning via Layer Collapse",
    "volume": "review",
    "abstract": "Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the model internal structure. In this paper, we propose a concise layer-wise structured pruner called \\textit{Layer Collapse (LaCo)}, in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80\\% at pruning ratios of 25-30\\%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the \\textit{LaCo} effectively inherits the parameters of the original model. Additionally, we perform ablation studies on various settings of \\textit{LaCo}. Finally, we discuss our motivation from the perspective of layer-wise similarity and evaluate the performance of the pruned LLMs across various pruning ratios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2cRuzDyVcd": {
    "title": "Evaluating Image Review Ability of Vision Language Models",
    "volume": "review",
    "abstract": "Large-scale Vision-Language Models (LVLMs) can process both images and text, demonstrating advanced capabilities in multimodal tasks like image captioning and visual question answering (VQA). However, it remains unclear whether they have an ability to understand and evaluate images, particularly in capturing the nuanced impressions and evaluations. To address this, we propose an image review evaluation method using rank correlation analysis. Our method asks a model to rank five review texts for an image. We then compare the model's rankings with human rankings to measure correlation. This enables effective evaluation of review texts that do not have a single correct answer. We validate this approach with a benchmark dataset of images from 15 categories, each with five review texts and annotated rankings in English and Japanese, resulting in over 2,000 data instances. Our experiments show that LVLMs excel at distinguishing between high-quality and low-quality reviews",
    "checked": true,
    "id": "c1e88bd7fe8c3174a697c6814a044f899f04db4e",
    "semantic_title": "evaluating image review ability of vision language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ndjti4Nxkh": {
    "title": "SNAP: Unlearning Selective Knowledge in Large Language Models with Negative Instructions",
    "volume": "review",
    "abstract": "Instruction-following large language models (LLMs), such as ChatGPT, have become increasingly popular with the general audience, many of whom are incorporating them into their daily routines. However, these LLMs inadvertently disclose personal or copyrighted information, which calls for a machine unlearning method to remove selective knowledge. Previous attempts sought to forget the link between the target information and its associated entities, but it rather led to generating undesirable responses about the target, compromising the end-user experience. In this work, we propose SNAP, an innovative framework designed to selectively unlearn information by 1) training an LLM with \\textit{negative instructions} to generate obliterated responses, 2) augmenting hard positives to retain the original LLM performance, and 3) applying the novel Wasserstein regularization to ensure adequate deviation from the initial weights of the LLM. We evaluate our framework on various NLP benchmarks and demonstrate that our approach retains the original LLM capabilities, while successfully unlearning the specified information",
    "checked": true,
    "id": "dcdf8c252362752911c6b928e2683c5e698a2d1b",
    "semantic_title": "snap: unlearning selective knowledge in large language models with negative instructions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XAciMT4JvK": {
    "title": "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages",
    "volume": "review",
    "abstract": "LLMs have become a go-to solution not just for text generation, but also for natural language understanding (NLU) tasks. Acquiring extensive knowledge through language modeling on web-scale corpora, they excel on English NLU, yet struggle to extend their NLU capabilities to underrepresented languages. In contrast, machine translation models (MT) produce excellent multilingual representations, resulting in strong translation performance even for low-resource languages. MT encoders, however, lack the knowledge necessary for comprehensive NLU that LLMs obtain through language modeling training on immense corpora. In this work, we get the best both worlds by integrating MT encoders directly into LLM backbones via sample-efficient self-distillation. The resulting MT-LLMs preserve the inherent multilingual representational alignment from the MT encoder, allowing lower-resource languages to tap into the rich knowledge embedded in English-centric LLMs. Merging the MT encoder and LLM in a single model, we mitigate the propagation of translation errors and inference overhead of MT decoding inherent to discrete translation-based cross-lingual transfer (e.g., translate-test). Evaluation spanning three prominent NLU tasks and 127 predominantly low-resource languages renders MT-LLMs highly effective in cross-lingual transfer. MT-LLMs substantially and consistently outperform translation-test based on the same MT model, showing that we truly unlock multilingual language understanding for LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rU9hzxdjIB": {
    "title": "Zero-shot Multi-Hop Question Answering via Monte-Carlo Tree Search with Large Language Models",
    "volume": "review",
    "abstract": "Recent advances in large language models (LLMs) have significantly impacted the domain of multi-hop question answering (MHQA), where systems are required to aggregate information and infer answers from disparate pieces of text. However, the autoregressive nature of LLMs inherently poses a challenge as errors may accumulate if mistakes are made in the intermediate reasoning steps. This paper introduces **M**onte-Carlo tree search for **Z**ero-shot multi-hop **Q**uestion **A**nswering (MZQA), a framework based on Monte-Carlo tree search (MCTS) to identify optimal reasoning paths in MHQA tasks, mitigating the error propagation from sequential reasoning processes. Unlike previous works, we propose a zero-shot prompting method, which relies solely on instructions without the support of hand-crafted few-shot examples that typically require domain expertise. We also introduce a behavioral cloning approach (MZQA-BC) trained on self-generated MCTS inference trajectories, achieving an over 10-fold increase in reasoning speed with bare compromise in performance. The efficacy of our method is validated on standard benchmarks such as HotpotQA, 2WikiMultihopQA, and MuSiQue, demonstrating that it outperforms existing frameworks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nQTpIJjcfX": {
    "title": "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion",
    "volume": "review",
    "abstract": "During pre-training, the Text-to-Image (T2I) diffusion models encode factual knowledge into their parameters. These parameterized facts enable realistic image generation, but they may become obsolete over time, thereby misrepresenting the current state of the world. Knowledge editing techniques aim to update model knowledge in a targeted way. However, facing the dual challenges posed by inadequate editing datasets and unreliable evaluation criterion, the development of T2I knowledge editing encounter difficulties in effectively generalizing injected knowledge. In this work, we design a T2I knowledge editing framework by comprehensively spanning on three phases: First, we curate a dataset \\textbf{CAKE}, comprising paraphrase and multi-object test, to enable more fine-grained assessment on knowledge generalization. Second, we propose a novel criterion, \\textbf{adaptive CLIP threshold}, to effectively filter out false successful images under the current criterion and achieve reliable editing evaluation. Finally, we introduce \\textbf{MPE}, a simple but effective approach for T2I knowledge editing. Instead of tuning parameters, MPE precisely recognizes and edits the outdated part of the conditioning text-prompt to accommodate the up-to-date knowledge. A straightforward implementation of MPE (Based on in-context learning) exhibits better overall performance than previous model editors. We hope these efforts can further promote faithful evaluation of T2I knowledge editing methods.\\footnote{Our code will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IVgiXnhIbt": {
    "title": "An Evaluation Mechanism of LLM-based Agents on Manipulating APIs",
    "volume": "review",
    "abstract": "LLM-based agents can greatly extend the abilities of LLMs and thus attract sharply increased studies. An ambitious vision -- serving users by manipulating massive API-based tools -- has been proposed and explored. However, we find a widely accepted evaluation mechanism for generic agents is still missing. This work aims to fill this gap. We decompose tool use capability into seven aspects and form a thorough evaluation schema. In addition, we design and release an instruction dataset and a toolset -- the two sides that the agents bridge between -- following the principle of reflecting real-world challenges. Furthermore, we evaluate multiple generic agents. Our findings can inspire future research in improving LLM-based agents and rethink the philosophy of API design",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jilpfuICJn": {
    "title": "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering",
    "volume": "review",
    "abstract": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., \"In which city was Silvio Berlusconi's first wife born?\", leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., \"Do I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?\" unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable. In response, we propose Right for Right Reasons (R^3), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples. Through experimental evaluations across three different tasksâ€”question answering, claim verification, and preference matchingâ€”our findings showcase R^3 as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and reasoning errors",
    "checked": true,
    "id": "59395cf4f9346ef4ccb37499a3a7e52c2978fc61",
    "semantic_title": "right for right reasons: large language models for verifiable commonsense knowledge graph question answering",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=H06ZGKJJ4C": {
    "title": "Revisiting the Knowledge Recall and Selection in Chinese Spelling Correction",
    "volume": "review",
    "abstract": "Chinese Spelling Correction (CSC) task is very challenging in the natural language processing area. However, the performance improvement is quite limited, primarily because the infusion of knowledge is limited. Previous work involved confusion sets as additional knowledge, but the size was too small and served only as a role of additional feature. To address this, we propose a knowledge recall and selection network (ReSC). First through four recall methods to achieve an average recall rate above 93\\%, with individual character recall of around 150 related characters/words. Subsequently, we proposed a Knowledge Selection Algorithm, choosing the appropriate characters or words from numerous recall sets. The knowledge selection network is highly efficient, as the F1 score nearly reached 100\\%. Extensive experiments have proven ReSC is able to inject substantial amount of entities with even a lower False Positive Rate. This novel network acheves the new SOTA results across three domain-specific datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qCdRbiOqWV": {
    "title": "Learning from Errors: A Data-Efficient Adaptation Method of Large Language Models for Code Generation",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have achieved substantial advances in code generation tasks, but they still struggle in specific code generation scenarios. These scenarios often require LLMs to be adapted to meet specific needs, but the limited training data available in practice leads to poor code generation performance. Therefore, how to effectively adapt LLMs to new scenarios with less training data is a major challenge for current code generation. In this paper, we propose a novel and effective adaptation method DEED, which stands for Data-Efficient adaptation based on Error-Driven learning for code generation. DEED leverages the errors made by LLM as learning opportunities and overcomes its own shortcomings through error revision, thereby achieving efficient learning. Specifically, DEED includes identifying the erroneous code generated by LLM, using Self-revise for code revision, optimizing the model with the revised code, and iteratively adapting the process for continuous improvement. Experimental results show that DEED achieves superior performance compared with mainstream fine-tuning and prompting methods using only a small amount of training data, with an average relative improvement of 54.7% on Pass@1 on multiple code generation datasets. We also verify the effectiveness of Self-revise, which generates revised code that optimizes the model more efficiently compared to the code samples from datasets. Moreover, DEED consistently shows strong performance across various LLMs, highlighting its generalizability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zhBbq1RfFf": {
    "title": "Logic Consistency Makes Large Language Models Personalized Reasoning Teachers",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have advanced natural language processing significantly with Chain-of-Thought (CoT) reasoning and In-Context Learning (ICL), but their deployment is limited by high computational and operational costs. This paper introduces Personalized Chain-of-Thought Distillation (PeCoTD), a novel approach to transfer reasoning capabilities from LLMs to smaller, more deployable models. Recognizing the comprehension difficulties small LMs face with LLM-generated rationales, we first develop a metric called Self Logic Consistency (SLC) to assess rationale quality. This refinement process ensures the maintenance of semantic equivalence with the original LLM rationales, facilitating more effective fine-tuning and avoiding distribution shifts. This approach, focusing on data quality in Knowledge Distillation (KD), mitigates comprehension variability in small LMs and extends the applicability of CoT KD strategies. Our experiments show that PeCoTD significantly improves the reasoning abilities of small models across diverse datasets",
    "checked": false,
    "id": "58fce438f817b46d37d072f8af7dfa4fd2dcd866",
    "semantic_title": "can language models teach weaker agents? teacher explanations improve students via personalization",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=qVYFTgVOLn": {
    "title": "Knowledge Distillation of Black-Box Large Language Models",
    "volume": "review",
    "abstract": "Given the exceptional performance of proprietary large language models (LLMs) like GPT-4, recent research has increasingly focused on boosting the capabilities of smaller models through knowledge distillation (KD) from these powerful yet black-box teachers. While leveraging the high-quality outputs of these teachers is advantageous, the inaccessibility of their internal states often limits effective knowledge transfer. To overcome this limitation, we introduce Proxy-KD, a novel method that uses a proxy model to facilitate the efficient transfer of knowledge from black-box LLMs to smaller models. Our experiments show that Proxy-KD not only enhances the performance of KD from black-box teacher models but also surpasses traditional white-box KD techniques.~This approach presents a compelling new avenue for distilling knowledge from advanced LLMs",
    "checked": false,
    "id": "f5359f596e0306599b4aa4157e6fe03567b35c01",
    "semantic_title": "knowledge distillation of large language models",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=bjtLHkVsv2": {
    "title": "TTQA-RS- A break-down prompting approach for Multi-hop Table-Text Question Answering with Reasoning and Summarization",
    "volume": "review",
    "abstract": "Question answering (QA) over tables and text has gained much popularity over the years. Multi-hop table-text QA requires multiple hops between the table and text, making it a challenging QA task. Although several works have attempted to solve the table-text QA task, most involve training the models and requiring labeled data. In this paper, we have proposed a model - \"TTQA-RS: A break-down prompting approach for Multi-hop Table-Text Question Answering with Reasoning and Summarization. Our model uses augmented knowledge including table-text summary with decomposed sub-question with answer for a reasoning-based table-text QA. Using open-source language models our model outperformed all existing prompting methods for table-text QA tasks on existing table-text QA datasets like HybridQA and OTT-QA's development set. Our results are comparable with the training-based state-of-the-art models, demonstrating the potential of prompt-based approaches using open-source LLMs. Additionally, by using GPT-4 with LLaMA3-70B, our model achieved state-of-the-art performance for prompting-based methods on multi-hop table-text QA",
    "checked": true,
    "id": "2a8686ced6bf48e404560ab7d09e3d0af6d5149d",
    "semantic_title": "ttqa-rs- a break-down prompting approach for multi-hop table-text question answering with reasoning and summarization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4rxo5Hk2QJ": {
    "title": "Identifying Nuances of Multi-Task Learning for Bengali and English Emotional Texts",
    "volume": "review",
    "abstract": "In this paper, we present a multi-task learning (MTL) model to classify sentiment and emotion in Bengali and English languages. For this multi-task learning work, different Bengali and English datasets were collected from publicly available sources and developed two MTL models by utilizing pre-trained mBERT and MuRIL models. Our proposed MTL model outperforms their corresponding standalone classifiers with an average F1-score of 0.5728 (+0.041) and 0.7590 (+0.046) for Bengali sentiment, emotion and English sentiment, emotion classification tasks respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UwM5gX2wHN": {
    "title": "Span-based Multi-grained Word Segmentation with Natural Annotations",
    "volume": "review",
    "abstract": "Multi-grained word segmentation (MWS) differs from traditional single-grained word segmentation (SWS) by dividing a sentence into multiple word sequences at varying granularities. The scarcity of annotated MWS data has led previous studies to use automatically generated pseudo MWS data and treat MWS as a tree parsing task. However, this method is limited by the low quality of the pseudo data. In this work, we directly utilize multiple single-grained datasets and implement multi-task learning for MWS. To better address conflicts arising from words segmented at different granularities, we employ a span-based word segmentation model. Additionally, we incorporate naturally annotated BAIKE data to improve model performance in cross-domain applications. Experimental results demonstrate that our method achieved an F1 score improvement of 0.83 on the NEWS dataset and 4.8 on the BAIKE dataset. Furthermore, by employing data augmentation, we obtained an additional F1 score improvement of 2.23 on the BAIKE dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dPYUU61lIE": {
    "title": "Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models",
    "volume": "review",
    "abstract": "Emotions play important epistemological and cognitive roles in our lives, revealing our values and guiding our actions. Previous work has shown that LLMs display biases in emotion attribution along gender lines. However, unlike gender, which says little about our values, religion, as a socio-cultural system, prescribes a set of beliefs and values for its followers. Religions, therefore, cultivate certain emotions. Moreover, these rules are explicitly laid out and interpreted by religious leaders. Using emotion attribution, we explore how different religions are represented in LLMs. We find that: Major religions in the US and European countries are represented with more nuance, displaying a more shaded model of their beliefs. Eastern religions like Hinduism and Buddhism are strongly stereotyped. Judaism and Islam are stigmatized -- the models' refusal skyrocket. We ascribe these to cultural bias in LLMs and the scarcity of NLP literature on religion. In the rare instances where religion is discussed, it is often in the context of toxic language, perpetuating the perception of these religions as inherently toxic. This finding underscores the urgent need to address and rectify these biases. Our research underscores the crucial role emotions play in our lives and how our values influence them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q3pRHLxMDc": {
    "title": "IntCoOp: Interpretability-Aware Vision-Language Prompt Tuning",
    "volume": "review",
    "abstract": "Image-text contrastive models such as CLIP learn transferable and robust representations for zero-shot transfer to a variety of downstream tasks. However, to obtain strong downstream performances, prompts need to be carefully curated, which can be a tedious engineering task. To address the issue of manual prompt engineering, prompt-tuning is used where a set of contextual vectors are learned by leveraging information from the training data. Despite their effectiveness, existing prompt-tuning frameworks often lack interpretability, thus limiting their ability to understand the compositional nature of images. In this work, we first identify that incorporating compositional attributes (e.g., a \"green\" tree frog) in the design of manual prompts can significantly enhance image-text alignment scores. Building upon this observation, we propose a novel and interpretable prompt-tuning method named IntCoOp, which learns to jointly align attribute-level inductive biases and class embeddings during prompt-tuning. To assess the effectiveness of our approach, we evaluate IntCoOp across two representative tasks in a few-shot learning setup: generalization to novel classes, and unseen domain shifts. Through extensive experiments across 10 downstream datasets on CLIP, we find that introducing attribute-level inductive biases leads to superior performance against state-of-art prompt tuning frameworks. Notably, in a 16-shot setup, IntCoOp improves CoOp by 7.35% in average performance across 10 diverse datasets",
    "checked": true,
    "id": "eff8782850cca1291fec91bbb5607b054cbbe892",
    "semantic_title": "intcoop: interpretability-aware vision-language prompt tuning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fyIE0G23ob": {
    "title": "Tool Graph Retriever: Exploring Dependency Graph-based Tool Retrieval for Large Language Models",
    "volume": "review",
    "abstract": "With the remarkable advancement of AI agents, the number of their equipped tools is increasing rapidly. However, integrating all tool information into the limited model context becomes impractical, highlighting the need for efficient tool retrieval methods. In this regard, dominant methods primarily rely on semantic similarities between tool descriptions and user queries to retrieve relevant tools. However, they often consider each tool independently, overlooking dependencies between tools, which may lead to the omission of prerequisite tools for successful task execution. To deal with this defect, in this paper, we propose Tool Graph Retriever (TGR), which exploits the dependencies among tools to learn better tool representations for retrieval. First, we construct a dataset termed TDI300K to train a discriminator for identifying tool dependencies. Then, we represent all candidate tools as a tool dependency graph and use graph convolution to integrate the dependencies into their representations. Finally, these updated tool representations are employed for online retrieval. Experimental results on several commonly used datasets show that our TGR can bring a performance improvement to existing dominant methods, achieving SOTA performance. Moreover, in-depth analyses also verify the importance of tool dependencies and the effectiveness of our TGR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nvVv9udFyo": {
    "title": "Mitigating Biases of Large Language Models in Stance Detection with Calibration",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved remarkable progress in many natural language processing tasks. However, our experiment reveals that, in stance detection tasks, LLMs may generate biased stances due to sentiment-stance spurious correlations and preference towards certain individuals and topics, thus harming their performance. Therefore, in this paper, we propose to Mitigate Biases of LLMs in stance detection with Calibration (MB-Cal). To be specific, a novel calibration network is devised to calibrate potential bias in the stance prediction of LLMs. Further, to address the challenge of effectively learning bias representations and the difficulty in the generalizability of debiasing, we construct counterfactual augmented data. This approach enhances the calibration network, facilitating the debiasing and out-of-domain generalization. Experimental results on in-target and zero-shot stance detection tasks show that the proposed MB-Cal can effectively mitigate biases of LLMs, achieving state-of-the-art results",
    "checked": true,
    "id": "7b7853b6d8659d9027590f14639ccf5a616b2bca",
    "semantic_title": "mitigating biases of large language models in stance detection with calibration",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=tbMeasOfX9": {
    "title": "Causal Discovery Inspired Unsupervised Domain Adaptation for Emotion-Cause Pair Extraction",
    "volume": "review",
    "abstract": "This paper tackles the task of emotion-cause pair extraction in the unsupervised domain adaptation setting. The problem is challenging as the distributions of the events causing emotions in target domains are dramatically different than those in source domains, despite the distributions of emotional expressions between domains are overlapped. Inspired by causal discovery, we propose a novel deep latent model in the variational autoencoder (VAE) framework, which not only captures the underlying latent structures of data but also utilizes the easily transferable knowledge of emotions as the bridge to link the distributions of events in different domains. To facilitate knowledge transfer across domains, we also propose a novel variational posterior regularization technique to disentangle the latent representations of emotions from those of events in order to mitigate the damage caused by the spurious correlations related to the events in source domains. Through extensive experiments, we demonstrate that our model outperforms the strongest baseline by approximately 11.05\\% on a Chinese benchmark and 2.45\\% on a English benchmark in terms of weighted-average F1 score. The source code will be publicly available upon acceptance",
    "checked": true,
    "id": "7710fe5287a50ce6b0f4f12ed617f9b5f057738e",
    "semantic_title": "causal discovery inspired unsupervised domain adaptation for emotion-cause pair extraction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YSpiATIH97": {
    "title": "Navigating Alignment Pitfalls: Assessing Suggestions to Combat Sycophancy",
    "volume": "review",
    "abstract": "Sycophancy causes models to produce answers that cater to user expectations rather than providing truthful responses. Previous research has found that model scaling, instruction tuning, and human feedback may increase sycophancy. However, these studies primarily focused on closed-source models and used indirect analysis to demonstrate the influence of human feedback. Our study focuses on sycophancy in open-source models, which are commonly used for specialized domain applications. We investigated the impact of human feedback on sycophancy by directly comparing models aligned with human feedback to those not aligned. To address sycophancy, we proposed assessing the user's expected answer rather than ignoring it. Consequently, we developed the Assessing Suggested Answer Preferences (ASAP) dataset and demonstrated that ASAP can enhance the model's assessment ability and reduce sycophancy across tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1twynEXRxS": {
    "title": "Resource-Efficient and Model-Independent Data Selection Framework for Instruction Fine-Tuning",
    "volume": "review",
    "abstract": "Large language models (LLMs) possess powerful capabilities and play a crucial role in daily life. Instruction fine-tuning is essential for training LLMs, enabling them to understand human instructions and produce the desired output. Selecting appropriate data for instruction fine-tuning is essential but challenging, existing data selection methods struggle to balance effectiveness and efficiency in real-world scenarios. Given that instruction fine-tuning requires models to respond to a wide variety of questions, we focus on output quality to assess the quality of instruction fine-tuning samples. In this work, we propose a novel data selection framework that evaluates data from unknown sources based on its output. To guide the model in distinguishing instruction fine-tuning data, we train a discriminator that uses outputs from models of varying quality as supervision signals. We establish principles to evaluate model quality, asserting that a model's quality is higher if it is a newer version, has more parameters, and achieves higher scores on well-known benchmarks. This way, the discriminator learns the differences between outputs from different models, enabling it to categorize unknown data into the most similar model outputs. We conduct experiments to prove that our method is resource-efficient and model-independent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=blEJ6z1bvu": {
    "title": "Focused Large Language Models are Stable Many-Shot Learners",
    "volume": "review",
    "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL does not necessarily scale well in many-shot (demonstration) settings. We hypothesize that the reason lies in more demonstrations dispersing the model attention from the query, hindering its understanding of key content, which we validate both theoretically and experimentally. Inspired by how humans learn from examples, we propose a training-free method FocusICL, which conducts triviality filtering to avoid attention being diverted by unimportant contents at token-level and operates hierarchical attention to further ensure sufficient attention towards current query at demonstration-level. We also design an efficient hyperparameter searching strategy for FocusICL based on model perplexity of demonstrations. Comprehensive experiments validate that FocusICL achieves an average performance improvement of 5.2% over vanilla ICL and scales well with many-shot demonstrations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rwhZhPgj0V": {
    "title": "RaTEScore: A Metric for Entity-Aware Radiology Text Similarity",
    "volume": "review",
    "abstract": "This paper proposes a new entity-aware lightweight metric for assessing accuracy of generated medical free-form text from AI models. Our metric, termed as Radiological Report Text Evaluation (RaTEScore), is designed to focus on key medical entities, such as diagnostic outcomes, anatomies, while demonstrating robustness against complex medical synonyms and sensitivity to negation expressions. Technically, we establish a new large-scale medical NER dataset RaTE-NER and train an NER model on it. Leveraging it, we decompose complex radiological reports into medical entities. We define the final metric by comparing the similarity based on the entity embeddings computed from language model and their corresponding types, forcing the metrics to focus on clinically critical statements. In experiments, our score demonstrates superior performance on aligning with human preference than other metrics, both on the existing public benchmarks and our new proposed RaTE-Eval benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZyRyfvPUhs": {
    "title": "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception",
    "volume": "review",
    "abstract": "The pervasive spread of misinformation and disinformation in social media underscores the critical importance of detecting media bias. While robust Large Language Models (LLMs) have emerged as foundational tools for bias prediction, concerns about inherent biases within these models persist. In this work, we investigate the presence and nature of bias within LLMs and its consequential impact on media bias detection. Departing from conventional approaches that focus solely on bias detection in media content, we delve into biases within the LLM systems themselves. Through meticulous examination, we probe whether LLMs exhibit biases, particularly in political bias prediction and text continuation tasks. Additionally, we explore bias across diverse topics, aiming to uncover nuanced variations in bias expression within the LLM framework. Importantly, we propose debiasing strategies, including prompt engineering and model fine-tuning. Extensive analysis of bias tendencies across different LLMs sheds light on the broader landscape of bias propagation in language models. This study advances our understanding of LLM bias, offering critical insights into its implications for bias detection tasks and paving the way for more robust and equitable AI systems",
    "checked": true,
    "id": "0fd73d32176189f7980847206a9d797c3b0f4e1d",
    "semantic_title": "investigating bias in llm-based bias detection: disparities between llms and human perception",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=cqu4AWqdqH": {
    "title": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning",
    "volume": "review",
    "abstract": "Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Extensive experiments on instruction-following datasets show that PromptKD achieves state-of-the-art performance while adding only 0.0007% of the teacher's parameters as prompts. Further analysis suggests that distilling student-friendly knowledge alleviates exposure bias effectively throughout the entire training process, leading to performance enhancements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=42W3lwlZY6": {
    "title": "Improving Translation between Spanish and Mapudungun through Transfer Learning",
    "volume": "review",
    "abstract": "Neural Machine Translation (NMT) systems for lower-resource languages like Mapudungun face significant challenges due to limited training data and linguistic complexities. This project aims to improve translation between Spanish and Mapudungun through transfer learning, leveraging pre-trained models on Spanish-English and Spanish-Finnish language pairs. Our contributions include demonstrating the effectiveness of transfer learning in this context and providing a comparative analysis of different parent models. Our main findings show that transfer learning enhances translation performance, with not much of a difference between the Spanish-English and Spanish-Finnish pre-trained model performance. This suggests that factors beyond morphological similarity, such as data quality or tokenization methods, play a crucial role in transfer learning success. These insights hope to pave the way for future research into optimizing translation tools for low-resource languages and involving communities in the development process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JsqTM04zoa": {
    "title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",
    "volume": "review",
    "abstract": "Current evaluations of large language models (LLMs) often overlook non-determinism, typically focusing on a single output per example. This limits our understanding of LLM performance variability in real-world applications. Our study addresses this issue by exploring key questions about the performance differences between greedy decoding and sampling, identifying benchmarks' consistency regarding non-determinism, and examining unique model behaviors. Our findings reveal and quantify significant performance gaps between greedy and sampling methods across various benchmarks, with sampling excelling in creative tasks and greedy decoding favoring deterministic tasks. We also observe consistent performance across different LLM sizes and alignment methods, noting that alignment can reduce sampling variance. Moreover, our best-of-N sampling approach demonstrates that smaller LLMs can match or surpass larger models such as GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This research shows the importance of considering non-determinism in LLM evaluations and provides insights for future LLM development and evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JyjbGaYs2e": {
    "title": "SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation",
    "volume": "review",
    "abstract": "Adaptive Retrieval-Augmented Generation (RAG) is an effective strategy to alleviate hallucination of large language models (LLMs). It dynamically determines whether LLMs need external knowledge for generation and invokes retrieval accordingly. This paper introduces **Self-aware Knowledge Retrieval** (SeaKR), a novel adaptive RAG model that extracts self-aware uncertainty of LLMs from their internal states. SeaKR activates retrieval when the LLMs present high self-aware uncertainty for generation. To effectively integrate retrieved knowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty to preserve the snippet that reduces their uncertainty to the utmost. To facilitate solving complex tasks that require multiple retrievals, SeaKR utilizes their self-aware uncertainty to choose among different reasoning strategies. Our experiments on both complex and simple Question Answering datasets show that SeaKR outperforms existing adaptive RAG methods",
    "checked": true,
    "id": "2a7720c878c20ed21608978e31507ca9d9936d1c",
    "semantic_title": "seakr: self-aware knowledge retrieval for adaptive retrieval augmented generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=kBaTgEPgcl": {
    "title": "Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation",
    "volume": "review",
    "abstract": "The COVID-19 pandemic causes severe social and economic disruption around the world, raising various subjects that are discussed or argued over on social media. Identifying pandemic-related named entities as expressed on social media is fundamental and important for understanding the discussions on the pandemic. However, there is limited work on named entity recognition on this topic due to the following challenges: 1) annotated data is rare and insufficient to train a robust recognition model, and 2) named entity recognition in COVID-19 requires extensive knowledge of the pandemic. To address this, we propose a novel entity knowledge augmentation for named entity recognition systems in COVID-19 tweets. Experiments carried out on the COVID-19 tweets dataset show that our proposed entity knowledge augmentation improves NER performance, achieving an F1 score of 84.10",
    "checked": false,
    "id": "8431f99edcc5abb7527dfc993af7916f437b50fc",
    "semantic_title": "mets-cov: a dataset of medical entity and targeted sentiment on covid-19 related tweets",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=JMwjgXHLCj": {
    "title": "FunLMs: Methods for Fine-tuning LLMs to Generate Humor",
    "volume": "review",
    "abstract": "This paper explores advancements in computational humor through the fine-tuning of large language models (LLMs) on curated datasets of English and Russian jokes. Experiments with fine-tuning on humorous data are conducted to see if generative humor is a viable idea. Different LLMs, sampling techniques and data curation are implemented for enhancing the coherence and effectiveness of humor generation providing a light computational approach for such tasks. We test the proposed methods on different formats and languages and conclude that generating humorous texts with LLMs is feasible though it requires substantial efforts in data preparation and evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=imbmw0VVRC": {
    "title": "Analyzing and Evaluating Correlation Measures in NLG Meta-Evaluation",
    "volume": "review",
    "abstract": "The correlation between NLG automatic evaluation metrics and human evaluation is the most critical criterion for assessing the capability of an evaluation metric. However, different grouping methods and choices of correlation coefficients result in at least 12 types of correlation measures. For a long time, little has been known about their characteristics. Therefore, this paper illustrates the relationships between different correlation measures and demonstrates how the degree of data discretization affects their values through statistical simulations. Additionally, we designed algorithms to evaluate the discriminative power and ranking consistency of 12 correlation measures using empirical data from 6 datasets and 32 evaluation metrics, uncovering many interesting conclusions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U5qeRWEOkQ": {
    "title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
    "volume": "review",
    "abstract": "Solving grid puzzles involves a significant amount of logical reasoning. Hence, it is a good domain to evaluate reasoning capability of a model which can then guide us to improve the reasoning ability of models. However, most existing works evaluate only the final predicted answer of a puzzle, without delving into an in-depth analysis of the LLMs' reasoning chains (such as where they falter) or providing any finer metrics to evaluate them. Since LLMs may rely on simple heuristics or artifacts to predict the final answer, it is crucial to evaluate the generated reasoning chain beyond overall correctness measures, for accurately evaluating the reasoning abilities of LLMs. To this end, we first develop GridPuzzle, an evaluation dataset comprising of 274 grid-based puzzles with different complexities. Second, we propose a new error taxonomy derived from manual analysis of reasoning chains from LLMs including GPT-4, Claude-3, Gemini, Mistral, and Llama-2. Then, we develop a LLM-based framework for large-scale subjective evaluation (i.e., identifying errors) and an objective metric, PuzzleEval, to evaluate the correctness of reasoning chains. Evaluating reasoning chains from LLMs leads to several interesting findings. We further show that existing prompting methods used for enhancing models' reasoning abilities do not improve performance on GridPuzzle. This highlights the importance of understanding fine-grained errors and presents a challenge for future research to enhance LLMs' puzzle-solving abilities by developing methods that address these errors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xIowecGhF3": {
    "title": "LLM-based Affective Text Generation Quality Based on Different Quantization Values",
    "volume": "review",
    "abstract": "Large language models in Natural Language Processing (NLP) exhibit a remarkable capacity for tasks such as language generation, translation, and contextual comprehension. In order to achieve these results, the model uses a large number of parameters, requiring a significant number of computational resources for training or utilization. Reducing the precision bits makes the models smaller, resulting in fewer computational resources needed to use the models, at the cost of lowering overall accuracy. This paper addresses the trade-off between different quantization values, GPU RAM utilization, and text quality in affective text generation (e.g., ``I really enjoy running in the snow-covered forest''). To evaluate, we use an emotion classifier and ten seed prompts to generate affective text. We test three setups of precision bits (8, 16, and 32) across two open weight language models. Our findings demonstrate that bit reductions leads to memory savings, achieving a reduction of 76 \\%. However, this optimization comes with a trade-off, leading to a decrease of up to 10 pp in F$_1$ score for larger models and an increase of 10 pp for smaller models, along with roughly double the inference time. In terms of text quality, larger models at lower quantization levels generally outperform smaller, higher-precision models -- while requiring similar memory",
    "checked": false,
    "id": "a48a3cfde9e9a6f02821ea28698012e4d3e1cd73",
    "semantic_title": "qaq: quality adaptive quantization for llm kv cache",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=frpq5dUxhW": {
    "title": "Robust Text Classification: Analyzing Prototype-Based Networks",
    "volume": "review",
    "abstract": "Downstream applications often require text classification models to be accurate and robust. While the accuracy of the state-of-the-art Language Models (LMs) approximates human performance, they often exhibit a drop in performance on noisy data found in the real world. This lack of robustness can be concerning, as even small perturbations in the text, irrelevant to the target task, can cause classifiers to incorrectly change their predictions. A potential solution can be the family of Prototype-Based Networks (PBNs) that classifies examples based on their similarity to prototypical examples of a class (prototypes) and has been shown to be robust to noise for computer vision tasks. In this paper, we study whether the robustness properties of PBNs transfer to text classification tasks under both targeted and static adversarial attack settings. Our results show that PBNs, as a mere architectural variation of vanilla LMs, offer more robustness compared to vanilla LMs under both targeted and static settings. We showcase how PBNs' interpretability can help us to understand PBNs' robustness properties. Finally, our ablation studies reveal the sensitivity of PBNs' robustness to how strictly clustering is done in the training phase, as tighter clustering results in less robust PBNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UgDosMbD7E": {
    "title": "LAiW: A Chinese Legal Large Language Models Benchmark",
    "volume": "review",
    "abstract": "General and legal domain LLMs have demonstrated strong performance in various tasks of LegalAI. However, their current evaluations lack alignment with the fundamental logic of legal reasoning, the legal syllogism. This hinders trust and understanding from legal experts. To bridge this gap, we introduce LAiW, the first Chinese legal LLM benchmark structured around the legal syllogism. We evaluate legal LLMs across three levels of capability, each reflecting a progressively more complex stage of legal syllogism: fundamental information retrieval, legal principle inference, and advanced legal applications, and encompassing a wide range of tasks in different legal scenarios. Our automatic evaluation reveals that LLMs, despite their ability to answer complex legal questions, lack the inherent logical processes of the legal syllogism. This limitation poses a barrier to acceptance by legal professionals. Furthermore, manual evaluation with legal experts confirms this issue and highlights the importance of pre-training to enhance the legal syllogism of LLMs. Future research may prioritize addressing this gap to unlock the full potential of LLMs in legal applications",
    "checked": false,
    "id": "c96b62643d08afa7cb852b7371c08ddc2a86b080",
    "semantic_title": "laiw: a chinese legal large language models benchmark a technical report",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5x6rnuzzUy": {
    "title": "Unsupervised Human Preference Learning",
    "volume": "review",
    "abstract": "Large language models demonstrate impressive reasoning abilities but struggle to provide personalized content due to their lack of individual user preference information. Existing methods, such as in-context learning and parameter-efficient fine-tuning, fall short in capturing the complexity of human preferences, especially given the small, personal datasets individuals possess. In this paper, we propose a novel approach utilizing small parameter models as preference agents to generate natural language rules that guide a larger, pre-trained model, enabling efficient personalization. Our method involves a small, local \"steering wheel\" model that directs the outputs of a much larger foundation model, producing content tailored to an individual's preferences while leveraging the extensive knowledge and capabilities of the large model. Importantly, this personalization is achieved without the need to fine-tune the large model. Experimental results on email and article datasets, demonstrate that our technique significantly outperforms baseline personalization methods. By allowing foundation models to adapt to individual preferences in a data- and compute-efficient manner, our approach paves the way for highly personalized language model applications",
    "checked": false,
    "id": "02b78ca6de66023924cffe33bae338de144f47fa",
    "semantic_title": "unsupervised domain adaptation for preference learning based speech emotion recognition",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=TMFGQCXTTZ": {
    "title": "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases",
    "volume": "review",
    "abstract": "The rapid development of social media has led to an increase in online harassment and offensive speech, posing significant challenges for effective content moderation. Existing automated detection models often exhibit a bias towards predicting offensive speech based on specific vocabulary, which not only compromises model fairness but also potentially exacerbates biases against vulnerable and minority groups. Addressing these issues, this paper proposes a bias self-awareness and data self-iteration framework for mitigating model biases.This framework aims to \"giving control back to models: enabling offensive language detection models to autonomously identify and mitigate biases\" through bias self-awareness algorithms and self-iterative data augmentation method. Experimental results demonstrate that the proposed framework effectively reduces the false positive rate of models in both in-distribution and out-of-distribution tests, enhances model accuracy and fairness, and shows promising performance improvements in detecting offensive speech on larger-scale datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WVWetUqnk0": {
    "title": "Prompt Engineering for Domain-Specific Geo-spatial Named Entity Disambiguation",
    "volume": "review",
    "abstract": "Despite the scarcity of employing transformer approaches for toponym resolution, this study leverages oral and transcribed text data to address the disambiguation of diverse named entities, including place names such as camps, ghettos, and streets. We utilise generative AI techniques, incorporating prompt engineering, to effectively disambiguate these named entities within geographical contexts. Our methodology aims to demonstrate how leveraging prompt engineering from general large language models (LLMs) can be effectively employed for less commonly addressed topics, such as toponym resolution in the field of Natural Language Processing (NLP). We have evaluated the few-shot chain of thought (COT) prompting approach combining the knowledge base (KB) as a retriever to provide the fewshots required for the reasoning process of LLM. This technique illustrates the efficacy of these advanced approaches in accurately identifying and resolving toponyms in complex textual datasets, thereby contributing valuable insights to the field of geographic information systems and digital humanities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hmk7Bf6HFg": {
    "title": "$R3$-NL2GQL: A Model Coordination and Knowledge Graph Alignment Approach for NL2GQL",
    "volume": "review",
    "abstract": "While current tasks of converting natural language to SQL (NL2SQL) using Foundation Models have shown impressive achievements, adapting these approaches for converting natural language to Graph Query Language (NL2GQL) encounters hurdles due to the distinct nature of GQL compared to SQL, alongside the diverse forms of GQL. Moving away from traditional rule-based and slot-filling methodologies, we introduce a novel approach, $R^3$-NL2GQL, integrating both small and large Foundation Models for ranking, rewriting, and refining tasks. This method leverages the interpretative strengths of smaller models for initial ranking and rewriting stages, while capitalizing on the superior generalization and query generation prowess of larger models for the final transformation of natural language queries into GQL formats. Addressing the scarcity of datasets in this emerging field, we have developed a bilingual dataset, sourced from graph database manuals and selected open-source Knowledge Graphs (KGs). Our evaluation of this methodology on this dataset demonstrates its promising efficacy and robustness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eQiQOOJ2OR": {
    "title": "LexMatcher: Dictionary-centric Data Curation for LLM-based Machine Translation",
    "volume": "review",
    "abstract": "The fine-tuning of open-source large language models (LLMs) for machine translation has recently received considerable attention, marking a shift towards data-centric research from traditional neural machine translation. However, the area of data collection for instruction fine-tuning in machine translation remains relatively underexplored. In this paper, we present LexMatcher, a simple yet effective method for data curation, the design of which is driven by the coverage of senses found in bilingual dictionaries. The construction process comprises data retrieval from an existing corpus and data augmentation that supplements the infrequent senses of polysemous words. Utilizing LLaMA2 as our base model, our approach outperforms the established baselines on the WMT2022 test sets and also exhibits remarkable performance in tasks related to word sense disambiguation and specialized terminology translation. These results underscore the effectiveness of LexMatcher in enhancing LLM-based machine translation",
    "checked": false,
    "id": "b5c70c2f9ae398b2458e1cf79cbf75d6131cfe6c",
    "semantic_title": "lexmatcher: dictionary-centric data collection for llm-based machine translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ft1N1XC7EK": {
    "title": "Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) excel in various natural language processing tasks but struggle with hallucination issues. Existing solutions have considered utilizing LLMs' inherent reasoning abilities to alleviate hallucination, such as self-correction and diverse sampling methods. However, these methods often overtrust LLMs' initial answers due to inherent biases. The key to alleviating this issue lies in overriding LLMs' inherent biases for answer inspection. To this end, we propose a CounterFactual Multi-Agent Debate (CFMAD) framework. CFMAD presets the stances of LLMs to override their inherent biases by compelling LLMs to generate justifications for a predetermined answer's correctness. The LLMs with different predetermined stances are engaged with a skeptical critic for counterfactual debate on the rationality of generated justifications. Finally, the debate process is evaluated by a third-party judge to determine the final answer. Extensive experiments on four datasets of three tasks demonstrate the superiority of CFMAD over existing methods",
    "checked": true,
    "id": "53dcdf3c6e0640bef458f97332063bbf7389721b",
    "semantic_title": "counterfactual debating with preset stances for hallucination elimination of llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=373APLOCkq": {
    "title": "Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference",
    "volume": "review",
    "abstract": "The demand for high-quality question-answering (QA) datasets has surged with the proliferation of language models and conversational agents in various emerging domains. As these models become ever more capable, the possibility of applying them to more challenging tasks is growing. Manual dataset annotation is costly and time-consuming, necessitating a more efficient approach. Automatically generated questions often suffer from a lack of quality or difficulty; hence, we propose a methodology to increase the difficulty of automatically generated questions using synthetic preference data, derived from SQuAD, to fine tune a question generation model using reinforcement learning. We empirically show an improvement in question difficulty over a supervised-finetuned model with minimal impact on question validity and perform an extensive error analysis. We believe our methodology provides a feasible approach to creating high quality synthetic datasets in emerging domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AuEHTm0CDy": {
    "title": "Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering",
    "volume": "review",
    "abstract": "Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question. With augmentation of retrieval module, open-source Large Language Models~(LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis. In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide *more correct* and *more comprehensive* answers for open-ended QA, although they are not strong enough individually. Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers. We will release our data and code for further research",
    "checked": true,
    "id": "12fa32d4ba0da26372747eaff7b34f353e26e838",
    "semantic_title": "chain-of-discussion: a multi-model framework for complex evidence-based question answering",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=wf1WhsmwFJ": {
    "title": "``It takes two to tango\": A Combination of Closed-Domain and Open-Domain Few-Shot Prompting for Claim Verification",
    "volume": "review",
    "abstract": "The widespread use of social media platforms has resulted in the swift dissemination of misinformation and fake news, creating a critical need for the development of computational models for automated fact-checking. Existing work on claim verification mainly relies on supervised learning from manually annotated claim-evidence pairs, which is resource-intensive and prone to biases, limiting their generalization across domains. To address this gap, we investigate zero-shot domain adaptation for claim verification, where no labeled training data is available for the target domain. We propose a hybrid approach that combines utilizing labeled training data from a source domain via in-context learning, along with topically relevant contexts from target document collections such as Wikipedia by means of RAG. We conduct experiments to evaluate zero-shot domain adaptation of claim verification for three target domains, namely climate change, scientific publications, and COVID-19 with the training set of the FEVER dataset as the source domain. We find that our proposed approach outperforms supervised models for domain adaptation, several LLM prompting-based models including zero-shot, and few-shot prompting from the source domain, and an RAG-based approach over a target collection of Wikipedia",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DgC6HolAhm": {
    "title": "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models",
    "volume": "review",
    "abstract": "This paper presents a way of enhancing the reliability of Large Multi-modal Models (LMMs) in addressing hallucination, where the models generate cross-modal inconsistent responses. Without additional training, we propose Counterfactual Inception, a novel method that implants counterfactual thinking into LMMs using self-generated counterfactual keywords. Our method is grounded in the concept of counterfactual thinking, a cognitive process where human considers alternative realities, enabling more extensive context exploration. Bridging the human cognition mechanism into LMMs, we aim for the models to engage with and generate responses that span a wider contextual scene understanding, mitigating hallucinatory outputs. We further introduce Plausibility Verification Process (PVP), a simple yet robust keyword constraint that effectively filters out sub-optimal keywords to enable the consistent triggering of counterfactual thinking in the model responses. Comprehensive analyses across various LMMs, including both open-source and proprietary models, corroborate that counterfactual thinking significantly reduces hallucination and helps to broaden contextual understanding based on true visual clues",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TNOnM4CQsl": {
    "title": "RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models",
    "volume": "review",
    "abstract": "The escalating challenge of misinformation, particularly in political discourse, requires advanced fact-checking solutions; this is even clearer in the more complex scenario of multimodal claims. We tackle this issue using a multimodal large language model in conjunction with retrieval-augmented generation (RAG), and introduce two novel reasoning techniques: Chain of RAG (CoRAG) and Tree of RAG (ToRAG). They fact-check multimodal claims by extracting both textual and image content, retrieving external information, and reasoning subsequent questions to be answered based on prior evidence. We achieve a weighted F1-score of 0.85, surpassing a baseline reasoning technique by 0.14 points. Human evaluation confirms that the vast majority of our generated fact-check explanations contain all information from gold standard data",
    "checked": true,
    "id": "f89ed27318cb930ae884af0c62be37f0355571b5",
    "semantic_title": "ragar, your falsehood radar: rag-augmented reasoning for political fact-checking using multimodal large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QZKn0P5dAE": {
    "title": "Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning",
    "volume": "review",
    "abstract": "Efficient fine-tuning plays a fundamental role in modern large models, with low-rank adaptation emerging as a particularly promising approach. However, the existing variants of LoRA are hampered by limited expressiveness, a tendency to overfit, and sensitivity to hyperparameter settings. This paper presents LoRA Slow Cascade Learning (LoRASC), an innovative technique designed to enhance LoRA's expressiveness and generalization capabilities while preserving its training efficiency. Our approach augments expressiveness through a cascaded learning strategy that enables a mixture-of-low-rank adaptation, thereby increasing the model's ability to capture complex patterns. Additionally, we introduce a slow-fast update mechanism and cascading noisy tuning to bolster generalization. The extensive experiments on various language and vision datasets, as well as robustness benchmarks, demonstrate that the proposed method not only significantly outperforms existing baselines, but also mitigates overfitting, enhances model stability, and improves OOD robustness",
    "checked": true,
    "id": "2ca52ea75fc02a3f08988794241cad9e30444db4",
    "semantic_title": "expressive and generalizable low-rank adaptation for large models via slow cascaded learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=djmJmRxHd4": {
    "title": "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing",
    "volume": "review",
    "abstract": "Large Language Models have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CoCoST framework, which enhances complex code generation by online searching for more information with planned queries and correctness testing for code refinement. Moreover, CoCoST serializes the complex inputs and outputs to improve comprehension and generates test cases to ensure the adaptability for real-world applications. CoCoST is validated through rigorous experiments on the DS-1000 and ClassEval datasets. Experimental results show that CoCoST substantially improves the quality of complex code generation, highlighting its potential to enhance the practicality of LLMs in generating complex code",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mX0QrIco2B": {
    "title": "A Retrieval Augmentation Approach for Aligning to Pluralistic Values",
    "volume": "review",
    "abstract": "Aligning LLM outputs to human preferences and values is important for reducing harms of AI deployments. However, human values are pluralistic with different population groups and communities having potentially conflicting preferences. Existing fine-tuning and prompting approaches have primarily focused around alignment towards shared values. In this paper, we propose a new approach for pluralistic alignment that uses retrieval-based in-context examples to augment alignment prompts. We introduce a framework, SPICA, consisting of three components to facilitate this: ``scenario banks'', group-informed retrieval measures, and contrastive prompts. We evaluate SPICA with human participants reflecting groups with different values, and find that SPICA outperforms relevance metrics like semantic similarity, selecting few-shot examples that better match group preferences (22.1\\% lower RMSE). In an end-to-end setting, we also find that SPICA produces more preferable responses when explicitly aligning to group preferences (+0.07 / 5-point scale)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=24MpT2xc5X": {
    "title": "LM2: A Simple Society of Language Models Solves Complex Reasoning",
    "volume": "review",
    "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems elicits more robustness in LLM reasoning -- a decomposer generates the subproblems, and a solver solves each of these subproblems. However, these techniques fail to accommodate coordination between the decomposer and the solver modules (either in a single model or different specialized ones) -- the decomposer does not keep track of the ability of the solver to follow the decomposed reasoning. In this paper, we propose LM2 to address these challenges. LM2 modularizes the decomposition, solution, and verification into three different language models. The decomposer module identifies the key concepts necessary to solve the problem and generates step-by-step subquestions according to the reasoning requirement. The solver model generates the solution to the subproblems that are then checked by the verifier module; depending upon the feedback from the verifier, the reasoning context is constructed using the subproblems and the solutions. These models are trained to coordinate using policy learning. Exhaustive experimentation suggests the superiority of LM2 over existing methods on in- and out-domain reasoning problems, outperforming the best baselines by 8.1% on MATH, 7.71% on JEEBench, and 9.7% on MedQA problems",
    "checked": true,
    "id": "448c54a59b18b6eada98be058d42378891e211e6",
    "semantic_title": "lm2: a simple society of language models solves complex reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VkuIeJNrVM": {
    "title": "Finding Safety Neurons in Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) excel in various capabilities but also pose safety risks such as generating harmful content and misinformation, even after safety alignment. In this paper, we explore the inner mechanisms of safety alignment from the perspective of mechanistic interpretability, focusing on identifying and analyzing *safety neurons* within LLMs that are responsible for safety behaviors. We propose generation-time activation contrasting to locate these neurons and dynamic activation patching to evaluate their causal effects. Experiments on multiple recent LLMs show that: (1) Safety neurons are sparse and effective. We can restore $90$\\% safety performance with intervention only on about $5$\\% of all the neurons. (2) Safety neurons encode transferrable mechanisms. They exhibit consistent effectiveness on different red-teaming datasets. The finding of safety neurons also interprets ''alignment tax''. We observe that the identified key neurons for safety and helpfulness significantly overlap, but they require different activation patterns of the shared neurons. Furthermore, we demonstrate an application of safety neurons in detecting unsafe outputs before generation. Our findings may promote further research on understanding LLM alignment. The source codes will be publicly released to facilitate future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0KEarh7j3Q": {
    "title": "DPPA: Merging Large Language Model using Dynamic Pruning and Partition Amplification",
    "volume": "review",
    "abstract": "Model merging aims to combine models with different capabilities into a single unified model, providing multiple capabilities without the necessity of retraining with the original training data. However, as distinctions between fine-tuned and base models grow, especially for large language models, current methods suffer significant performance drops, hindering true multi-domain capabilities. In this study, we propose a two-stage method, called Dynamic Pruning and Partition Amplification (DPPA), to address the challenge of merging models with significant distinctions. First, we introduce Dynamic Pruning (DP) to discover significant parameters and remove redundant ones. Subsequently, we propose Dynamic Partition Amplification (DPA) to restore the capability in the domain. Experimental results demonstrate that our approach performs outstandingly, improving model merging performance by almost 20\\%",
    "checked": false,
    "id": "08920921ce6f4efe0dd92f6005a755b07d4ce760",
    "semantic_title": "dppa: pruning method for large language model to model merging",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=26QCPvQ0hg": {
    "title": "Prefix-VAE: Efficient and Consistent Short-Text Topic Modeling with LLMs",
    "volume": "review",
    "abstract": "Topic models are compelling methods for discovering latent semantics in a document collection. However, it assumes that a document has sufficient co-occurrence information to be effective. However, in short texts, co-occurrence information is minimal, which results in feature sparsity in document representation. Therefore, existing topic models- whether probabilistic or neural- mostly struggle to mine patterns from them to generate coherent topics. In this paper, we first explore the capability of large language models (LLMs) to generate longer texts from shorter ones before applying them to traditional topic modeling. To further improve the efficiency and solve the problem of the semantic inconsistency from LLM-generated texts, we propose to use prefix tuning to train a smaller language model coupled with a variational autoencoder for short-text topic modeling. Extensive experiments on multiple real-world datasets under extreme data sparsity scenarios show that our models can generate high-quality topics that outperform state-of-the-art models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AoC7R3yrZJ": {
    "title": "M$^{2}$Chat: Empowering VLM for Multimodal LLM Interleaved Text-Image Generation",
    "volume": "review",
    "abstract": "In this paper, we propose \\textbf{$M^{2}Chat$}, a novel unified multimodal LLM framework for generating interleaved text-image conversation across various scenarios. Specifically, we propose an $M^{3}Adapter$ that efficiently integrates granular low-level visual information and high-level semantic features from multi-modality prompts. Upon the well-aligned fused feature, $M^{3}Adapter$ tailors a learnable gating strategy to balance the model creativity and consistency across various tasks adaptively. Moreover, to further enhance the effectiveness of $M^{3}Adapter$ while preserving the coherence of semantic context comprehension, we introduce a two-stage $M^{3}FT$ fine-tuning strategy. This strategy optimizes disjoint groups of parameters for image-text alignment and visual-instruction respectively. Extensive experiments demonstrate our $M^{2}Chat$ surpasses state-of-the-art counterparts across diverse benchmarks, showcasing its prowess in interleaving generation, storytelling, and multimodal dialogue systems",
    "checked": true,
    "id": "320f152689743a4e977ef85e4e43485033fcc0a0",
    "semantic_title": "m$^{2}$chat: empowering vlm for multimodal llm interleaved text-image generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=1C1LSKfoyc": {
    "title": "Mixture of Cluster-Conditional LoRA Experts for Vision-Language Instruction Tuning",
    "volume": "review",
    "abstract": "Instruction tuning of Large Vision-language Models (LVLMs) has revolutionized the development of versatile models with zero-shot generalization across a wide range of downstream vision-language tasks. However, the diversity of training tasks of different sources and formats would lead to inevitable task conflicts, where different tasks conflict for the same set of model parameters, resulting in sub-optimal instruction-following abilities. To address that, we propose the Mixture of Cluster-conditional LoRA Experts (MoCLE), a novel Mixture of Experts (MoE) architecture designed to activate the task-customized model parameters based on the instruction clusters. A separate universal expert is further incorporated to improve generalization capabilities of MoCLE for novel instructions. Extensive experiments on InstructBLIP and LLaVA demonstrate the effectiveness of MoCLE",
    "checked": true,
    "id": "2d4a853affeb0b164fc1134df612aea658f36459",
    "semantic_title": "mixture of cluster-conditional lora experts for vision-language instruction tuning",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=E05bFZcUQG": {
    "title": "Data in Formation: Structuring Data to Solve Linguistic Analogical Tasks",
    "volume": "review",
    "abstract": "This study explores the impact of structured synthetic data on the performance of linguistic tasks in limited data or low-quality data scenarios. Using Blackbird Language Matrices (BLMs), linguistic analogues of Raven's Progressive Matrices, as a case study task, we explore how data size and structure influence task solving. This multiple-choice puzzle consists of structured contexts and answer sets of sentences. We devise a context that has two properties: analogical internal structure and implicit annotation of properties needed to find the solution. The answer set elicits contrastive learning and supports careful error analysis. We also propose a method to semi-automatically create synthetic data for the BLM task. Our experiments show that structured data improves performance even with small training sizes. They also show that both analogically-structured contexts and implicit annotation are useful. These results suggest that careful data structuration of small amount of data can be used to mitigate the typical resource demands in NLP tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7O4Bpy8MKj": {
    "title": "Are Large Language Models Meta Reasoners?",
    "volume": "review",
    "abstract": "In this paper, we introduce Meta-Reasoning Prompting (MRP), a novel approach inspired by human meta-reasoning to enhance the flexibility and generality of large language models (LLMs). Traditional in-context learning techniques, such as Tree-of-Thoughts, show promise but lack consistent state-of-the-art performance across diverse tasks due to their specialized nature. MRP addresses this limitation by dynamically selecting and applying different reasoning methods based on the specific requirements of each task, optimizing both performance and computational efficiency. The MRP framework operates in two phases: initially, the LLM selects the most appropriate reasoning method using task input cues and objective descriptions of available methods; subsequently, it applies the chosen method to complete the task. This dynamic strategy mirrors human meta-reasoning, allowing the model to excel in a wide range of problem domains. We evaluate the effectiveness of MRP through comprehensive benchmarks. The results demonstrate that MRP achieves or approaches state-of-the-art performance across these diverse tasks. MRP represents a significant advancement in enabling LLMs to autonomously select suitable reasoning methods, enhancing their ability to handle diverse and complex problem domains efficiently",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rCMNqF8Oeq": {
    "title": "CASE: Efficient Curricular Data Pre-training for Building Assistive Psychology Expert Models",
    "volume": "review",
    "abstract": "The limited availability of psychologists necessitates efficient identification of individuals requiring urgent mental healthcare. This study explores the use of Natural Language Processing (NLP) pipelines to analyze text data from online mental health forums used for consultations. By analyzing forum posts, these pipelines can flag users who may require immediate professional attention. A crucial challenge in this domain is data privacy and scarcity. To address this, we propose utilizing readily available curricular texts used in institutes specializing in mental health for pre-training the NLP pipelines. This helps us mimic the training process of a psychologist. Our work presents CASE-BERT that flags potential mental health disorders based on forum text. CASE-BERT demonstrates superior performance compared to existing methods, achieving an f1 score of 0.91 for Depression and 0.88 for Anxiety, two of the most commonly reported mental health disorders. Our code is publicly available \\footnote{https://anonymous.4open.science/r/CASE-FB26/README.md}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LU91EkeNru": {
    "title": "FSM: A Finite State Machine Based Zero-Shot Prompting Paradigm for Multi-Hop Question Answering",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) with chain-of-thought (COT) prompting have demonstrated impressive abilities on simple nature language inference tasks. However, they tend to perform poorly on Multi-hop Question Answering (MHQA) tasks due to several challenges, including hallucination, error propagation and limited context length. We propose a prompt method Finite State Machine (FSM) to enhance the reasoning capabilities of LLM for complex tasks in addition to improved effectiveness and trustworthiness. Different from chain-of-thought (COT) methods, FSM addresses MHQA by iteratively decomposing a question into multi-turn sub-questions, and self-correcting in time, improving the accuracy of answers in each step. Specifically, FSM addresses one sub-question at a time and decides on the next step based on its current result and state, in an automaton-like format. Experiments on benchmarks show the effectiveness of our method. Although our method performs on par with the baseline on relatively simpler datasets, it excels on challenging datasets like Musique. Moreover, this approach mitigates the hallucination phenomenon, wherein the correct final answer can be recovered despite errors in the intermediate reasoning steps. Furthermore, our method improves LLMs' ability to follow specified output format requirements, significantly reducing the difficulty of answer interpretation and the need for reformatting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sV75BwXVtt": {
    "title": "Major Entity Identification: A Generalizable Alternative to Coreference Resolution",
    "volume": "review",
    "abstract": "The limited generalization of coreference resolution (CR) models has been a major bottleneck in the task's broad application. Prior work has identified annotation differences, especially for mention detection, as one of the main reasons for the generalization gap and proposed using additional annotated target domain data. Rather than relying on this additional annotation, we propose an alternative formulation of the CR task, Major Entity Identification (MEI), where we: (a) assume the target entities to be specified in the input, and (b) limit the task to only the frequent entities. Through extensive experiments, we demonstrate that MEI models generalize well across domains on multiple datasets with supervised models and LLM-based few-shot prompting. Additionally, the MEI task fits the classification framework, which enables the use of classification-based metrics that are more robust than the current CR metrics. Finally, MEI is also of practical use as it allows a user to search for all mentions of a particular entity or a group of entities of interest",
    "checked": true,
    "id": "eecc71949047e5d69c0fdc41911fd4583b7e64b2",
    "semantic_title": "major entity identification: a generalizable alternative to coreference resolution",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GP5hWLoxBv": {
    "title": "Scaling Behavior for Numeral System: Tokenize Your Numbers into $1$-digit",
    "volume": "review",
    "abstract": "Though Large Language Models (LLMs) have shown remarkable abilities in mathematics reasoning, they are still struggling with performing numeric operations accurately, such as addition and multiplication. Numbers can be tokenized into tokens in various ways by different LLMs and affect the numeric operations performance. Currently, there are two representatives: 1) Tokenize into $1$-digit, and 2) Tokenize into $1\\sim 3$ digit. The difference is roughly equivalent to using different numeral systems (namely base $10$ or base $10^{3}$). In light of this, we study the scaling behavior of different numeral systems in the context of transformer-based large language models. We empirically show that a base $10$ system is consistently more data-efficient than a base $10^{2}$ or $10^{3}$ system across training data scale, model sizes under from-scratch training settings, while different number systems have very similar performances when fine-tuned. Through thorough analysis and experiments, we conclude that tokenizing numbers into $1$-digit is more favorable for LLMs in numerical operations. Additionally, we reveal \\textit{extrapolation} behavior patterns on addition and multiplication that sheds light on the mechanism learnt by the models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LWGnAptrul": {
    "title": "Multi-Loss Fusion: Angular and Contrastive Integration for Machine-Generated Text Detection",
    "volume": "review",
    "abstract": "Modern natural language generation (NLG) systems have led to the development of synthetic human-like open-ended texts, posing concerns as to who the original author of a text is. To address such concerns, we introduce DeB-Ang: the utilisation of a custom DeBERTa model with angular loss and contrastive loss functions for effective class separation in neural text classification tasks. We expand the application of this model on binary machine-generated text detection and multi-class neural authorship attribution. We demonstrate improved performance on many benchmark datasets whereby the accuracy for machine-generated text detection was increased by as much as 38.04\\% across all datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=il0rzVS0Wk": {
    "title": "When Parts are Greater Than Sums: Individual LLM Components Can Outperform Full Models",
    "volume": "review",
    "abstract": "This paper studies in-context learning (ICL) by decomposing the output of large language models into the individual contributions of attention heads and MLPs (components). We observe curious components: good-performing ones that individually do well on a classification task, even when the model performs poorly; bad-performing ones that do much worse than chance; and label-biased components that always predict the same label. We find that component accuracies are well-correlated across different demonstration sets and perturbations of prompt templates, even when the full-model accuracy varies greatly. Based on our findings, we propose component reweighting, which learns to linearly re-scale the component activations from a few labeled examples. Given 24 labeled examples, our method improves by an average of 6.0% accuracy points over 24-shot ICL across 8 tasks on Llama-2-7B. Overall, this paper both enriches our understanding of ICL and provides a practical method for improvement by examining model internals",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ar6EquF1PB": {
    "title": "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data",
    "volume": "review",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has proven effective in aligning large language models with human intentions, yet it often relies on complex methodologies like Proximal Policy Optimization (PPO) that require extensive hyper-parameter tuning and present challenges in sample efficiency and stability. In this paper, we introduce Inverse-Q*, an innovative framework that transcends traditional RL methods by optimizing token-level reinforcement learning without the need for additional reward or value models. Inverse-Q* leverages direct preference optimization techniques but extends them by estimating the conditionally optimal policy directly from the model's responses, facilitating more granular and flexible policy shaping. Our approach reduces reliance on human annotation and external supervision, making it especially suitable for low-resource settings. We present extensive experimental results demonstrating that Inverse-Q* not only matches but potentially exceeds the effectiveness of PPO in terms of convergence speed and the alignment of model responses with human preferences. Our findings suggest that Inverse-Q* offers a practical and robust alternative to conventional RLHF approaches, paving the way for more efficient and adaptable model training approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CD1fVpd3tr": {
    "title": "Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices",
    "volume": "review",
    "abstract": "In this paper, we present \\textbf{Delta-LoRA}, which is a novel parameter-efficient approach to fine-tune large language models (LLMs). In contrast to LoRA and other low-rank adaptation methods such as AdaLoRA, Delta-LoRA not only updates the low-rank matrices $A$ and $B$, but also propagate the learning to the pre-trained weights $W$ via updates utilizing the delta of the product of two low-rank matrices ($A^{(t+1)}B^{(t+1)} - A^{(t)}B^{(t)}$). Such a strategy effectively addresses the limitation that the incremental update of low-rank matrices is inadequate for learning representations capable for downstream tasks. Moreover, as the update of $W$ does not need to compute the gradients of $W$ and store their momentums, Delta-LoRA shares comparable memory requirements and computational costs with LoRA. Extensive experiments show that Delta-LoRA significantly outperforms existing low-rank adaptation methods. We further support these results with comprehensive analyses that underscore the effectiveness of Delta-LoRA",
    "checked": true,
    "id": "587d0627031c165985c69036f62d5d21fc38e3f7",
    "semantic_title": "delta-lora: fine-tuning high-rank parameters with the delta of low-rank matrices",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=IFlWg1wfiu": {
    "title": "SafetyQuizzer: Evaluating the Safety of LLMs in a More Sustained Manner",
    "volume": "review",
    "abstract": "As the expansion of application of Large Language Models (LLMs), concerns about the safety of LLMs have grown among researchers. Numerous previous studies demonstrated the potential risks of LLMs to generate harmful contents and proposed various safety assessment benchmarks aimed at evaluating the safety risks. However, the evaluation questions in current benchmarks are not only too straightforward to be easily rejected by target LLMs, but also difficult to update questions with practical significance due to their lack of correlation with real-world events, thereby making these benchmarks challenging to sustainably apply in continuous evaluaton tasks. To address these limitations, we propose SafetyQuizzer, a question generation framework for evaluating the safety of LLMs in a more sustained manner. SafetyQuizzer leverages fine-tuned LLM and jailbreaking attack templates to generate weakly offensive questions and so reduces the decline rate. Additionally, by employing retrieval-augmented generation, SafetyQuizzer incorporates the latest events into evaluation questions, overcoming the challenge of question updates and introducing a new dimension of event relevance to enhance the quality of evaluation questions. Our experiments show that evaluation questions generated by SafetyQuizzer significantly reduce the decline rate compared to other benchmarks while still maintaining comparable attack success rate. Warning: this paper contains examples that may be offensive or upsetting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xv4CnSdNJM": {
    "title": "Distilling Opinions at Scale: Incremental Opinion Summarization using XL-OPSUMM",
    "volume": "review",
    "abstract": "Opinion summarization in e-commerce encapsulates the collective views of numerous users about a product based on their reviews. Typically, a product on an e-commerce platform has thousands of reviews, each review comprising around 10-15 words. While Large Language Models (LLMs) have shown proficiency in summarization tasks, they struggle to handle such a large volume of reviews due to context limitations. To address this, we propose a scalable framework called XL-OPSUMM, that generates summaries incrementally with the help of an Aspect Dictionary (Refer to Section 3). However, the existing test set, AMASUM, has only 560 reviews per product on average. Due to the lack of a test set with thousands of reviews, we created a new test set called XL-FLIPKART by gathering data from the Flipkart website and generating summaries using GPT-4. Through various automatic evaluations and extensive analysis, we evaluated the framework's efficiency on two datasets, AMASUM and Xl-Flipkart. Experimental results show that our framework, XL-OPSUMM, powered by Llama-3-8B-8k, achieves an average ROUGE-1 F1 gain of 4.38% and a ROUGE-L F1 gain of 3.70% over the next best-performing model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CTFm5LlSnG": {
    "title": "Automatically Enhanced Instruction-following Capabilities of Large Language Models via Execution Feedback",
    "volume": "review",
    "abstract": "One core capability of large language models (LLMs) is to follow natural language instructions. However, the issue of automatically constructing high-quality training data to enhance the complex instruction-following abilities of LLMs without manual annotation remains unresolved. In this paper, we introduce AutoIF, the first scalable and reliable method for automatically generating instruction-following training data. AutoIF transforms the validation of instruction-following data quality into code verification, requiring LLMs to generate instructions, the corresponding code to check the correctness of the instruction responses, and unit test samples to verify the code's correctness. Then, execution feedback-based rejection sampling can generate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) training. AutoIF achieves significant improvements across three training algorithms, SFT, Offline DPO, and Online DPO, when applied to the top open-source LLMs, Qwen2 and Llama3, in self-alignment and strong-to-weak distillation settings",
    "checked": false,
    "id": "50650e66ce7c454595862aac70c2a3e9dde23387",
    "semantic_title": "self-play with execution feedback: improving instruction-following capabilities of large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=xsxmKbOA5f": {
    "title": "Ladder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level",
    "volume": "review",
    "abstract": "General-purpose Large Language Models (LLMs) like GPT-4 have achieved remarkable advancements in machine translation (MT) by leveraging extensive web content. On the other hand, translation-specific LLMs are built by pre-training on domain-specific monolingual corpora and fine-tuning with human-annotated translation data. Despite the superior performance, these methods either demand an unprecedented scale of computing and data or substantial human editing and annotation efforts. In this paper, we develop Ladder, a novel model-agnostic and cost-effective tool to refine the performance of general LLMs for MT. Ladder is trained on pseudo-refinement triplets which can be easily obtained from existing LLMs without additional human cost. During training, we propose a hierarchical fine-tuning strategy with an easy-to-hard schema, improving Ladder's refining performance progressively. The trained Ladder can be seamlessly integrated with any general-purpose LLMs to boost their translation performance. By utilizing Gemma-2B/7B as the backbone, Ladder-2B can elevate raw translations to the level of top-tier open-source models (e.g., refining BigTranslate-13B with +6.91 BLEU and +3.52 COMET for XXâ†’En), and Ladder-7B can further enhance model performance to be on par with the state-of-the-art GPT-4. Extensive ablation and analysis corroborate the effectiveness of Ladder in diverse settings. Data and code will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ezz0bOHOhg": {
    "title": "Large Language Models Encode Geoscience Knowledge",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shed light on potential inter-discipline applications to foster scientific discoveries of a specific domain by using artificial intelligence (AI for science, AI4S). In this study, we introduce A Data-centric Recipe for advancing the application of Large Language Models (LLMs) in the realm of geoscience. Leveraging the versatility of LLMs and their potential for interdisciplinary applications, particularly in Artificial Intelligence for Science (AI4S), we propose a methodology to tailor an open-source LLM to the geoscience domain, with potential for broader interdisciplinary use. This involves further pre-training the model with a comprehensive geoscience text corpus and fine-tuning it using a custom instruction tuning dataset. Our efforts culminate in multiple size of LLM specialized for geoscience tasks. Through rigorous evaluation on geoscience examinations and open-domain questions, our model exhibits state-of-the-art performance across a diverse array of Natural Language Processing tasks within the geoscience domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6ZKHA0ZgU4": {
    "title": "DDPC: Dual Dynamic Presentation with Contrastive Learning for Robust Temporal Knowledge Graph Completion",
    "volume": "review",
    "abstract": "Temporal knowledge graph completion has made significant progress, but several research gaps persist. This study addresses the challenges of temporal changes by proposing DDP and DDPC, novel dual-perspective learning frameworks that integrate static and temporal knowledge using a dual-layer embedding mechanism and a contrastive learning-enhanced version, respectively. This approach effectively captures both dynamic changes and time-invariant properties of entities and relations, optimizing the completeness and accuracy of information. Additionally, a perturbation learning mechanism is introduced to enhance the model's robustness to anomalous data and noise by simulating data perturbations during training, improving adaptability and stability in changing environments. DDPC achieves state-of-the-art results on multiple standard evaluation datasets, experimentally verifying the effectiveness of the proposed theories and methods. This study contributes to advancing the field of temporal knowledge graph completion by developing an innovative framework that integrates temporal and static perspectives, enhances robustness, and undergoes rigorous evaluations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v9LcSZuTDz": {
    "title": "Sentium: Sentiment Evaluation through Neurosymbolic Taxonomy - an Interpretable and Understandable Model",
    "volume": "review",
    "abstract": "Sentiment analysis has seen rapid progress driven by deep learning, but the opaque black-box nature of these models hinders trustworthy deployment in high-stakes domains where interpretability is crucial. We propose \\textbf{Sentium} (\\textbf{S}entiment \\textbf{E}valuation through \\textbf{N}eurosymbolic \\textbf{T}axonomy, an \\textbf{I}nterpretable and \\textbf{U}nderstandable \\textbf{M}odel), a cognitively-inspired architecture that closely emulates human sentiment comprehension processes. Sentium takes a hybrid approach by combining structured sentiment knowledge with neural models, achieving state-of-the-art performance while maintaining transparency through explicit compositional reasoning over semantic propositions. Compared to state-of-the-art financial language models, Sentium showed substantially lower misclassification rates for predicting true negatives as positive (Sentium=1.97\\%; FLANG-BERT \\citep{shah2022flue} =6.78\\%, FinBERT \\citep{araci2019finbert} =10.17\\%). The code are available at: https://github.com/anonymous-submission",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lRYgIWNbFL": {
    "title": "ValueDCG: Measuring Comprehensive Human Value Understanding Ability of Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "53898f11fa6bad10a2faf480e741cfc76a020558",
    "semantic_title": "valuedcg: measuring comprehensive human value understanding ability of language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=uTfn2rGEgl": {
    "title": "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization",
    "volume": "review",
    "abstract": "RL-based techniques can be employed to search for prompts that, when fed into a target language model, maximize a set of user-specified reward functions. However, in many target applications, the natural reward functions are in tension with one another -- for example, content preservation vs. style matching in style transfer tasks. Current techniques focus on maximizing the average of reward functions, which does not necessarily lead to prompts that achieve balance across rewards -- an issue that has been well-studied in the multi-objective and robust optimization literature. In this paper, we conduct an empirical comparison of several existing multi-objective optimization techniques adapted to this new setting: RL-based discrete prompt optimization. We compare two methods optimizing the volume of the Pareto reward surface and one method that chooses an update direction that benefits all rewards simultaneously. We evaluate performance on two NLP tasks: style transfer and machine translation, each using three competing reward functions. Our experiments demonstrate that multi-objective methods that directly optimize the volume of the Pareto reward surface perform better and achieve a better balance of all rewards than those that attempt to find monotonic update directions",
    "checked": true,
    "id": "564bffb42edb6a24be8b144f22eec97e0579028b",
    "semantic_title": "morl-prompt: an empirical analysis of multi-objective reinforcement learning for discrete prompt optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=bRQEN4N4qo": {
    "title": "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention",
    "volume": "review",
    "abstract": "Using prompt-based \"diversity interventions\" is a typical way to improve diversity for Text-to-Image models to depict individuals with various racial or gender traits. However, this strategy might result in nonfactual demographic distribution, especially when generating real historical figures. In this work, we propose **DemOgraphic FActualIty Representation (DoFaiR)**, a benchmark to quantify the trade-off between using diversity interventions and preserving demographic factuality in Text-to-Image models. DoFaiR consists of 756 test instances, various diversity prompts, and evaluation metrics to reveal the factuality tax of diversity instructions through an automated, fact-checked, and evidence-supported evaluation pipeline. Experiments with DALLE-3 on DoFaiR unveil that diversity-oriented instructions improve the number of different gender and racial groups in generated images at the cost of accurate historical demographic distributions. To resolve this issue, we propose **Fact-Augmented Intervention** (FAI), which instructs a Large Language Model (LLM) to reflect on factual information about gender and racial compositions of generation subjects in history and incorporate it into the generation context of T2I models. By orienting model generations using the reflected historical truths, FAI remarkably preserves demographic factuality under diversity interventions, while also boosting diversity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kzYxvGY9au": {
    "title": "AggregHate: An Efficient Aggregative Approach for the Detection of Hatemongers on Social Platforms",
    "volume": "review",
    "abstract": "The automatic detection of online hate speech serves as a crucial step in the detoxification of the online discourse. Moreover, accurate classification can promote a better understanding of the proliferation of hate as a social phenomenon. While most prior work focus on the detection of hateful utterances, we argue that focusing on the user level is as important, albeit challenging. In this paper we consider a multimodal aggregative approach for the detection of hate-mongers, taking into account the potentially hateful texts, user activity, and the user network. We evaluate our methods on three unique datasets X (Twitter), Gab, and Parler showing that a processing a user's texts in her social context significantly improves the detection of hate mongers, compared to previously used text and graph-based methods. Our method can be then used to improve the classification of coded messages, dog-whistling, and racial gas-lighting, as well as inform intervention measures. Moreover, our approach is highly efficient even for very large datasets and networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TOoUxtY6us": {
    "title": "Intention Analysis Makes LLMs A Good Jailbreak Defender",
    "volume": "review",
    "abstract": "Aligning large language models (LLMs) with human values, particularly in the face of complex and stealthy jailbreak attacks, presents a formidable challenge. In this study, we present a simple yet highly effective defense strategy, i.e., Intention Analysis ($\\mathbb{IA}$). The principle behind $\\mathbb{IA}$ is to trigger LLMs' inherent self-correct and improve ability through a two-stage process: 1) analyzing essential intention of the user input, and 2) providing final policy-aligned response based on the first round conversation. Notably, $\\mathbb{IA}$ is an inference-only method, thus could enhance the safety of LLMs without compromising their helpfulness. Extensive experiments on varying jailbreak benchmarks across ChatGLM, Llama2, Llama3, Vicuna, MPT, DeepSeek, and GPT-3.5 show that $\\mathbb{IA}$ could consistently and significantly reduce the harmfulness in responses (averagely -48.2\\% attack success rate) without compromising the general helpfulness. Encouragingly, with the help of our $\\mathbb{IA}$, Vicuna-7B even outperforms GPT-3.5 in terms of attack success rate. We empirically demonstrate that, to some extent, $\\mathbb{IA}$ is robust to errors in generated intentions. Further analyses present some insights into how $\\mathbb{IA}$ mechanism works and suggest two directions to improve its performance. The code will be released",
    "checked": true,
    "id": "8fd29e810540c40846cddce3cbdf5060cd59fb57",
    "semantic_title": "intention analysis makes llms a good jailbreak defender",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=WGFxKLHIAB": {
    "title": "Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models for Russian",
    "volume": "review",
    "abstract": "There has been a surge in the development of various Large Language Models (LLMs). However, text generation for languages other than English often faces significant challenges, including poor generation quality and the reduced computational performance due to the disproportionate representation of tokens in model's vocabulary. In this work, we address these issues and introduce Vikhr, a new state-of-the-art open-source instruction-tuned LLM designed specifically for the Russian language. ``Vikhr'' refers to the name of the Mistral LLM series and means ``strong gust of wind.'' Unlike previous efforts for Russian that utilize computationally inexpensive LoRA adapters on top of English-oriented models, Vikhr features an adapted tokenizer vocabulary and undergoes the continued pre-training and instruction tuning of all weights. This approach not only enhances the model's performance but also significantly improves its computational and contextual efficiency. The remarkable performance of Vikhr across various Russian-language benchmarks can also be attributed to our efforts in expanding instruction datasets and corpora for continued pre-training. Vikhr not only sets the new state of the art among open-source LLMs for Russian, but even outperforms some proprietary closed-source models on certain benchmarks. The model weights, instruction sets, and code are publicly available",
    "checked": true,
    "id": "c9128e2b411ab8779abd83a7e2ff9ae0af935b33",
    "semantic_title": "vikhr: the family of open-source instruction-tuned large language models for russian",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=c4MnHb4UHb": {
    "title": "Step-by-Step Evaluation of Gender Bias in Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) tend to internalize and reproduce discriminatory societal biases. A natural language reasoning process provided by Chain-of-Thought (CoT) prompting helps determine whether the LLM is reasoning based on correct grasp. However, it is not clarified whether such information provided by CoT leads to accurately evaluating the LLM's social biases. In this paper, we introduce a benchmark to evaluate gender-related social biases based on the step-by-step process using CoT prompts. We construct the benchmark for an English reasoning task where the LLM is given a list of words comprising feminine, masculine, and gendered occupational words, and is required to count the number of feminine and masculine words. Our CoT prompts require the LLM to explicitly indicate whether each word in the word list is feminine or masculine. Experimental results show that considering both the step-by-step process and predictions of LLMs improves the quality of bias evaluation. Furthermore, despite the simplicity of the task of counting words, our benchmark produces evaluations of gender-related social biases that are comparable to existing human-scratched benchmarks",
    "checked": false,
    "id": "5da06eb3a746932acfe36b81c7c640c3d969ae70",
    "semantic_title": "evaluating gender bias in large language models via chain-of-thought prompting",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=0t5MiMx3ji": {
    "title": "Debiasing Transformer Models through Weight Masking: Addressing Gender Confounding Shift in Dementia Detection",
    "volume": "review",
    "abstract": "Deep language models are often described as \"black-box\" systems due to their opaque inference procedures. This presents a challenge in understanding the information they capture, and how it is encoded within transformer networks, raising the possibility that encoded biases may remain undetected. This work addresses confounding bias learned during model fine-tuning, when a pretrained language model is adapted to downstream domains and tasks. Building on previous methodologies, we extend them by proposing the Extended Confounding Filter and the Dual Filter. These methods aim to isolate and address weights within the transformer network that are associated with confounding variables through distinct training phases. We evaluate these methods on the \\textit{DementiaBank} dataset, a first-person narrative dataset that contains language of patients with cognitive impairment and healthy controls. We aim to demonstrate the applicability of the proposed methods in the domain of dementia detection as a means to correct for gender-related disparities in class distribution at training time. Our results show that transformer models can overfit to the subpopulation distribution in the training data. By disrupting the weights associated with known confounders, we show that fairer models can be achieved with reduced prediction bias towards specific subgroups. Moreover, our findings highlight resilience of the model against weights deletion and show a trade-off between model performance in dementia detection and the reduction of disparities across gender groups",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NkXvCeorhW": {
    "title": "AppAgent: Multimodal Agents as Smartphone Users",
    "volume": "review",
    "abstract": "Recent advancements in large language models (LLMs) have led to the creation of intelligent agents capable of performing complex tasks. This paper introduces a novel LLM-based multimodal agent framework designed to operate smartphone applications. Our framework enables the agent to operate smartphone applications through a simplified action space, mimicking human-like interactions such as tapping and swiping. This novel approach bypasses the need for system back-end access, thereby broadening its applicability across diverse apps. Central to our agent's functionality is its innovative learning method. The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations. This process generates a knowledge base that the agent refers to for executing complex tasks across different applications. To demonstrate the practicality of our agent, we conducted extensive testing over 50 tasks in 10 different applications, including social media, email, maps, shopping, and sophisticated image editing tools. The results affirm our agent's proficiency in handling a diverse array of high-level tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0kqZox6BPU": {
    "title": "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation",
    "volume": "review",
    "abstract": "Distractor generation task focuses on generating incorrect but plausible options for objective questions such as fill-in-the-blank and multiple-choice questions. This task is widely utilized in educational settings across various domains and subjects The effectiveness of these questions in assessments relies on the quality of the distractors, as they challenge examinees to select the correct answer from a set of misleading options. The evolution of artificial intelligence (AI) has transitioned the task from traditional methods to the use of neural networks and pre-trained language models. This shift has established new benchmarks and expanded the use of advanced deep learning methods in generating distractors. This survey explores distractor generation tasks, datasets, methods, and current evaluation metrics for English objective questions, covering both text-based and multi-modal domains. It also evaluates existing AI models and benchmarks and discusses potential future research directions",
    "checked": false,
    "id": "2f949cb7375c7bf8a884d492018d61fceefdb847",
    "semantic_title": "distractor generation for multiple-choice questions: a survey of methods, datasets, and evaluation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=mUmFgGtWni": {
    "title": "Atomic Self-Consistency for Better Long Form Generations",
    "volume": "review",
    "abstract": "Recent work has aimed to improve LLM generations by filtering out hallucinations, thereby improving the precision of the information in responses. Correctness of a long-form response, however, also depends on the recall of multiple pieces of information relevant to the question. In this paper, we introduce Atomic Self-Consistency (ASC), a technique for improving the recall of relevant information in an LLM response. ASC follows recent work, Universal Self-Consistency (USC) in using multiple stochastic samples from an LLM to improve the long-form response. Unlike USC which only focuses on selecting the best single generation, ASC picks authentic subparts from the samples and merges them into a superior composite answer. Through extensive experiments and ablations, we show that merging relevant subparts of multiple samples performs significantly better than picking a single sample. ASC demonstrates significant gains over USC on multiple factoids and open-ended QA datasets - ASQA, QAMPARI, QUEST, ELI5 with ChatGPT and Llama3. Our analysis also reveals untapped potential for enhancing long-form generations using approach of merging multiple samples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v7s4HVQ9bt": {
    "title": "AutoScraper: A Progressive Understanding Web Agent for Web Scraper Generation",
    "volume": "review",
    "abstract": "Web scraping is a powerful technique that extracts data from websites, enabling automated data collection, enhancing data analysis capabilities, and minimizing manual data entry efforts. Existing methods, wrappers-based methods suffer from limited adaptability and scalability when faced with a new website, while language agents, empowered by large language models (LLMs), exhibit poor reusability in diverse web environments. In this work, we introduce the paradigm of generating web scrapers with LLMs and propose AutoScraper, a two-stage framework that can handle diverse and changing web environments more efficiently. AutoScraper leverages the hierarchical structure of HTML and similarity across different web pages for generating web scrapers. Besides, we propose a new executability metric for better measuring the performance of web scraper generation tasks. We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework. Our work is now open-source",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=COyVDgtnxp": {
    "title": "Simulating Classroom Education with LLM-Empowered Agents",
    "volume": "review",
    "abstract": "Large language models (LLMs) have been employed in various intelligent educational tasks to assist teaching. While preliminary explorations have focused on independent LLM-empowered agents for specific educational tasks, the potential for LLMs within a multi-agent collaborative framework to simulate a classroom with real user participation remains unexplored. In this work, we propose SimClass, a multi-agent classroom simulation framework involving user participation. We recognize representative class roles and introduce a novel class control mechanism for automatic classroom teaching, and conduct user experiments in two real-world courses. Utilizing the Flanders Interactive Analysis System and Community of Inquiry theoretical frame works from educational analysis, we demonstrate that LLMs can simulate traditional classroom interaction patterns effectively while enhancing user's experience. We also observe emergent group behaviors among agents in SimClass, where agents collaborate to create enlivening interactions in classrooms to improve user learning process. We hope this work pioneers the application of LLM-empowered multi-agent systems in virtual classroom teaching",
    "checked": true,
    "id": "a7a4aafe038f0c78b8b5320b537bdd3fb8c2b28d",
    "semantic_title": "simulating classroom education with llm-empowered agents",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=bRXFB375r8": {
    "title": "Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning",
    "volume": "review",
    "abstract": "The emergence of large language models~(LLMs) has opened up unprecedented possibilities for automating complex tasks that are often comparable to human performance. Despite their capabilities, LLMs still encounter difficulties in completing tasks that require high levels of accuracy and complexity due to their inherent limitations in handling multifaceted problems single-handedly. This paper introduces `Smurfs', a cutting-edge multi-agent framework designed to revolutionize the application of LLMs. By seamlessly transforming a conventional LLM into a synergistic multi-agent ensemble, Smurfs can enhance the model's ability to solve complex tasks at no additional cost. This is achieved through innovative prompting strategies that allocate distinct roles within the model, thereby facilitating collaboration among specialized agents and forming an intelligent multi-agent system. Our empirical investigation on both open-ended task of StableToolBench and closed-ended task on HotpotQA showcases Smurfs' superior capability in intricate tool utilization scenarios. Notably, Smurfs outmatches all the baseline methods in both experiments, setting new state-of-the-art performance. Furthermore, through comprehensive ablation studies, we dissect the contribution of the core components of the multi-agent framework to its overall efficacy. This not only verifies the effectiveness of the framework, but also sets a route for future exploration of multi-agent LLM systems",
    "checked": true,
    "id": "4df39291087ea56c5c11a3f5e007c13b523bb97a",
    "semantic_title": "smurfs: leveraging multiple proficiency agents with context-efficiency for tool planning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eOrss1nk6d": {
    "title": "Scaling Laws for Linear Complexity Language Models",
    "volume": "review",
    "abstract": "The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their scalability. Specifically, we examine the scaling behaviors of three efficient linear architectures. These include TNL, a linear attention model with data-independent decay; HGRN2, a linear RNN with data-dependent decay; and cosFormer2, a linear attention model without decay. We also include LLaMA as a baseline architecture for softmax attention for comparison. These models were trained with six variants, ranging from 70M to 7B parameters on a 300B-token corpus, and evaluated with a total of 1,376 intermediate checkpoints on various downstream tasks. These tasks include validation loss, commonsense reasoning, and information retrieval and generation. The study consumes over 200k H100/H800 GPU hours and reveals that existing linear complexity language models exhibit similar scaling capabilities as conventional transformer-based models while also demonstrating superior linguistic proficiency and knowledge retention",
    "checked": true,
    "id": "685283f452856d3d70902860e5b0634e695d9a42",
    "semantic_title": "scaling laws for linear complexity language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jf5IAH5Ht2": {
    "title": "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment",
    "volume": "review",
    "abstract": "Recent studies have explored the working mechanisms of In-Context Learning (ICL). However, they mainly focus on classification and simple generation tasks, limiting their broader application to more complex generation tasks in practice. To address this gap, we investigate the impact of demonstrations on token representations within the practical alignment tasks. We find that the transformer embeds the task function learned from demonstrations into the separator token representation, which plays an important role in the generation of prior response tokens. Once the prior response tokens are determined, the demonstrations become redundant. Motivated by this finding, we propose an efficient Progressive In-Context Alignment (PICA) method consisting of two stages. In the first few-shot stage, the model generates several prior response tokens via standard ICL while concurrently extracting the ICL vector that stores the task function from the separator token representation. In the following zero-shot stage, this ICL vector guides the model to generate responses without further demonstrations. Extensive experiments demonstrate that our PICA not only surpasses vanilla ICL but also achieves comparable performance to other alignment tuning methods. The proposed training-free method reduces the time cost (e.g., 5.45Ã—) with improved alignment performance (e.g., 6.57+). Consequently, our work highlights the application of ICL for alignment and calls for a deeper understanding of ICL for complex generations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9LdWG0PCsh": {
    "title": "The Role of Role Design in In-Context Learning for Large Language Models",
    "volume": "review",
    "abstract": "In-context learning (ICL) enables Large Language Models (LLMs) to generate predictions based on prompts without additional fine-tuning. While prompt engineering has been widely studied, the impact of role design within prompts remains underexplored. This study examines the influence of role configurations in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from OpenAI and Llama2-7b Llama2-13b from Meta. We evaluate the models' performance across datasets, focusing on tasks like sentiment analysis, text classification, and question answering. F1 scores are used to measure the effectiveness of different role designs. Our findings highlight the potential of role-based prompt structuring to enhance LLM performance, offering new insights for optimizing prompt design strategies in natural language processing tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nvh98zb6yp": {
    "title": "Self-assessment, Exhibition, and Recognition: a Review of Personality in Large Language Models",
    "volume": "review",
    "abstract": "As large language models (LLMs) appear to behave increasingly human-like in text-based interactions, more and more researchers become interested in investigating personality in LLMs. However, the diversity of psychological personality research and the rapid development of LLMs have led to a broad yet fragmented landscape of studies in this interdisciplinary field. Extensive studies across different research focuses, different personality psychometrics, and different LLMs make it challenging to have a holistic overview and further pose difficulties in applying findings to real-world applications. In this paper, we present a comprehensive review by categorizing current studies into three research problems: self-assessment, exhibition, and recognition, based on the intrinsic characteristics and external manifestations of personality in LLMs. For each problem, we provide a thorough analysis and conduct in-depth comparisons of their corresponding solutions. Besides, we summarize research findings and open challenges from current studies and further discuss their underlying causes. We also collect extensive publicly available resources to facilitate interested researchers and developers. Lastly, we discuss the potential future research directions and application scenarios. Our paper is the first comprehensive survey of up-to-date literature on personality in LLMs. By presenting a clear taxonomy, in-depth analysis, promising future directions, and extensive resource collections, we aim to provide a better understanding and facilitate further advancements in this emerging field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TcI0p8NvZd": {
    "title": "Optimizing Code Retrieval: High-Quality and Scalable Dataset Annotation through Large Language Models",
    "volume": "review",
    "abstract": "Code retrieval aims to identify code from extensive codebases that semantically aligns with a given query code snippet. Collecting a broad and high-quality set of query and code pairs is crucial to the success of this task. However, existing data collection methods struggle to effectively balance scalability and annotation quality. In this paper, we first analyze the factors influencing the quality of function annotations generated by Large Language Models (LLMs). We find that the invocation of intra-repository functions and third-party APIs plays a significant role. Building on this insight, we propose a novel annotation method that enhances the annotation context by incorporating the content of functions called within the repository and information on third-party API functionalities. Additionally, we integrate LLMs with a novel sorting method to address the multi-level function call relationships within repositories. Furthermore, by applying our proposed method across a range of repositories, we have developed the Query4Code dataset. The quality of this synthesized dataset is validated through both model training and human evaluation, demonstrating high-quality annotations. Moreover, cost analysis confirms the scalability of our annotation method",
    "checked": false,
    "id": "d44031f253668c61ac6d68b95bbe9cac57730d51",
    "semantic_title": "soft prompt tuning for augmenting dense retrieval with large language models",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=CAbTWafsgh": {
    "title": "MemDPT: Differential Privacy for Memory Efficient Language Models",
    "volume": "review",
    "abstract": "Large language models have consistently demonstrated remarkable performance across a wide spectrum of applications. Nonetheless, the deployment of these models can inadvertently expose user privacy to potential risks. The substantial memory demands of these models during training represent a significant resource consumption challenge. The sheer size of these models imposes a considerable burden on memory resources, which is a matter of significant concern in practice. In this paper, we present an innovative training framework MemDPT that not only reduces the memory cost of large language models but also places a strong emphasis on safeguarding user data privacy. MemDPT provides edge network and reverse network designs to accommodate various differential privacy memory-efficient fine-tuning schemes. Our approach not only achieves $2 \\sim 3 \\times$ memory optimization but also provides robust privacy protection, ensuring that user data remains secure and confidential. Extensive experiments have demonstrated that MemDPT can effectively provide differential privacy efficient fine-tuning across various task scenarios",
    "checked": false,
    "id": "4f71f3ba99d63848868c56a021568157670c1813",
    "semantic_title": "dp-memarc: differential privacy transfer learning for memory efficient language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IDXjH7Q8T4": {
    "title": "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation",
    "volume": "review",
    "abstract": "Large language models (LLMs) have become increasingly prevalent in our daily lives, leading to an expectation for LLMs to be trustworthy ---- both accurate and well-calibrated (the prediction confidence should align with its ground truth correctness likelihood). Nowadays, fine-tuning has become the most popular method for adapting a model to practical usage by significantly increasing accuracy on downstream tasks. Despite the great accuracy it achieves, we found fine-tuning is still far away from satisfactory trustworthiness due to \"tuning-induced mis-calibration\". In this paper, we delve deeply into why and how mis-calibration exists in fine-tuned models, and how distillation can alleviate the issue. Then we further propose a brand new method named Efficient Trustworthy Distillation (FIRST), which utilizes a small portion of teacher's knowledge to obtain a reliable language model in a cost-efficient way. Specifically, we identify the \"concentrated knowledge\" phenomenon during distillation, which can significantly reduce the computational burden. Then we apply a \"trustworthy maximization\" process to optimize the utilization of this small portion of concentrated knowledge before transferring it to the student. Experimental results demonstrate the effectiveness of our method, where better accuracy (+2.3\\%) and less mis-calibration (-10\\%) are achieved on average across both in-domain and out-of-domain scenarios, indicating better trustworthiness",
    "checked": false,
    "id": "fa85ded1c64dbcb7c641e93943c2af2256091c80",
    "semantic_title": "combining machine learning, patient reported outcomes and value based healthcare: a protocol for scoping reviews (preprint)",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zhwylVPqZn": {
    "title": "Expert Margin Optimization: Enhancing Multi-Domain Translation Capabilities of LLM with MoE-LoRA",
    "volume": "review",
    "abstract": "In the realm of machine translation utilizing Large Language Models (LLMs), the standard workflow involves Cross-Lingual Alignment learning followed by Instruction-tuning. Low-Rank Adaptation (LoRA) has been a widely-used and effective method for fine-tuning LLMs. However, LoRA alone exhibits limited benefits when confronted with multi-task or multi-domain scenarios. Given the prevalent existence of multi-domain challenges in machine translation, this paper focuses on enhancing the Multi-Domain Translation Capabilities of LLMs. We extend LoRA to Mixture of Experts (MoE) architecture, defined as MoE-LoRA, to address domain conflicts in multi-domain settings. Our approach involves introducing MoE-LoRA solely at higher layers to target specific domain-related knowledge acquisition, preceded by General Cross-Lingual Alignment during the training process. Particularly, we propose a methodology called Expert Margin Optimization to facilitate the transfer of additional knowledge from other domains to enhance the inputs specific to a domain. Experimental validations conducted on the English-to-German and English-to-Chinese translation directions using the Llama2-7B and Llama3-8B models demonstrate consistent improvements in BLEU and COMET scores, highlighting the efficacy of our proposed approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KY3roODQ47": {
    "title": "LLMs for Extremely Low-Resource Finno-Ugric Languages",
    "volume": "review",
    "abstract": "The advancement of large language models (LLMs) has predominantly focused on high-resource languages, leaving low-resource languages, such as those in the Finno-Ugric family, significantly underrepresented. This paper addresses this gap by focusing on VÃµro, Livonian, and Komi. We cover almost the entire cycle of LLM creation, from data collection to instruction tuning and evaluation. Our contributions include developing multilingual base and instruction-tuned models; creating evaluation benchmarks, including the SMUGRI-MT-Bench multi-turn conversational benchmark; and conducting human evaluation. We intend for this work to promote linguistic diversity, ensuring that lesser-resourced languages can benefit from advancements in NLP",
    "checked": false,
    "id": "9e5f2a3ad112b3b8c25be3961dde6955eb00c301",
    "semantic_title": "building an extremely low resource language to high resource language machine translation system from scratch",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=QeZwkjuq4J": {
    "title": "CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification",
    "volume": "review",
    "abstract": "Deploying large language models (LLMs) on edge devices presents significant challenges due to the substantial computational overhead and memory requirements. Activation sparsification can mitigate these challenges by reducing the number of activated neurons during inference. Existing methods typically employ thresholding-based sparsification based on the statistics of activation tensors. However, these methods do not model the impact of activation sparsification on performance, resulting in significant performance degradation. To address this issue, this paper reformulates the activation sparsification problem and proposes , a general activation sparsification approach via \\textbf{CH}annel-wise thr\\textbf{E}sholding and \\textbf{S}elective \\textbf{S}parsification. First, channel-wise thresholding assigns a unique threshold to each activation channel in FFN layers. Then, selective sparsification involves choosing specific layers in the attention modules to apply thresholding-based activation sparsification. Finally, this paper shows the detailed implementation of sparse kernels to accelerate the LLM inference. Experimental results demonstrate that the proposed CHESS achieves lower performance degradation over 8 downstream tasks while activating fewer parameters, thus speeding up the LLM inference by up to 1.27x",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SzRQxGCQor": {
    "title": "KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions",
    "volume": "review",
    "abstract": "Recent studies have demonstrated that large language models (LLMs) are susceptible to being misled by false premise questions (FPQs), leading to errors in factual knowledge, know as factuality hallucination. Existing benchmarks that assess this vulnerability primarily rely on manual construction, resulting in limited scale and lack of scalability. In this work, we introduce an automated, scalable pipeline to create FPQs based on knowledge graphs (KGs). The first step is modifying true triplets extracted from KGs to create false premises. Subsequently, utilizing the state-of-the-art capabilities of GPTs, we generate semantically rich FPQs. Based on the proposed method, we present a comprehensive benchmark, the Knowledge Graph-based False Premise Questions (KG-FPQ), which contains approximately 178k FPQs across three knowledge domains, at six levels of confusability, and in two task formats. Using KG-FPQ, we conduct extensive evaluations on several representative LLMs and provide valuable insights",
    "checked": true,
    "id": "000d4c828826fb786bcb466f88db1fb0b9cec0f2",
    "semantic_title": "kg-fpq: evaluating factuality hallucination in llms with knowledge graph-based false premise questions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jrXVmwaFIr": {
    "title": "IoA: Linking Collaborative Agent Efforts with the Internet of Agents",
    "volume": "review",
    "abstract": "The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate efficient collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. We believe that this direction holds potential for better multi-agent systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pL85yVw6Yp": {
    "title": "Efficient LLM Pruning with Token-Dependency Awareness and Hardware-Adapted Inference",
    "volume": "review",
    "abstract": "Structured pruning removes entire components, like attention heads or hidden dimensions to yield faster dense large language models. However, previous methods are time-consuming and inference speedup is bottlenecked by inefficient GPU parallel processing due to mismatch in pruned weight block dimensions with tensor cores. Moreover, pruning of heads in grouped query attentions is not widely attempted due to challenges with their interdependencies. To address these limitations, we propose (1) a structured pruning method for LLMs with grouped-query attention (GQA) that learn appropriate key, value and shared query heads to retain according to their importance for accurate prediction. (2) a post-pruning weight update to better retain the performance of pruned LLMs. (3) a post-pruning dimension adaptation step to enhance GPU utilization of pruned models and significantly speed up inference. Our method speeds up inference by up to 60% over previous approaches. Evaluated on several language benchmarks using variants of LLaMA models and Mistral, our method shows a reduction in pruning time by upto 90% with higher inference speed and performance over a range of sparsity ratios. Additionally, our findings suggest that pruning can alleviate prediction confusion in certain scenarios",
    "checked": false,
    "id": "58700f3740105e3422eb030305372b6d8bc44986",
    "semantic_title": "hardware-aware parallel prompt decoding for memory-efficient acceleration of llm inference",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=DGjv5L30cn": {
    "title": "Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models",
    "volume": "review",
    "abstract": "Accurate assessment of personality traits is crucial for effective psycho-counseling, yet traditional methods like self-report questionnaires are time-consuming and biased. This study exams whether Large Language Models (LLMs) can predict the Big Five personality traits directly from counseling dialogues and introduces an innovative framework to perform the task. Our framework applies role-play and questionnaire-based prompting to condition LLMs on counseling sessions, simulating client responses to the Big Five Inventory. We evaluated our framework on 853 real-world counseling sessions, finding a significant correlation between LLM-predicted and actual Big Five traits, proving the validity of framework. Moreover, ablation studies highlight the importance of role-play simulations and task simplification via questionnaires in enhancing prediction accuracy. Meanwhile, our fine-tuned Llama3-8B model, utilizing Direct Preference Optimization with Supervised Fine-Tuning, achieves a 130.95\\% improvement, surpassing the state-of-the-art Qwen1.5-110B by 36.94\\% in personality prediction validity. In conclusion, LLMs can predict personality based on counseling dialogues. Our code and model are publicly available at \\url{https://github.com/Anonymous-gwFabfaH/BigFive-LLM-Predictor}, providing a valuable tool for future research in computational psychometrics",
    "checked": true,
    "id": "922e122e11d6c60249e96bf2c89235d8f30e66a2",
    "semantic_title": "predicting the big five personality traits in chinese counselling dialogues using large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hn6CZ6m7QO": {
    "title": "Reasoning Robustness of LLMs to Adversarial Typographical Errors",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning using Chain-of-Thought (CoT) prompting. However, CoT can be biased by users' instruction. In this work, we study the reasoning robustness of LLMs to typographical errors, which can naturally occur in users' queries. We design an Adversarial Typo Attack ($\\texttt{ATA}$) algorithm that iteratively samples typos for words that are important to the query and selects the edit that is most likely to succeed in attacking. It shows that LLMs are sensitive to minimal adversarial typographical changes. Notably, with 1 character edit, Mistral-7B's accuracy drops from 43.7\\% to 38.6\\% on GSM8K, while with 8 character edits the performance further drops to 19.2\\%. To extend our evaluation to larger and closed-source LLMs, we develop the $\\texttt{R$^2$ATA}$ benchmark, which assesses models' $\\underline{R}$easoning $\\underline{R}$obustness to $\\underline{\\texttt{ATA}}$. It includes adversarial typographical questions derived from three widely-used reasoning datasetsâ€”GSM8K, BBH, and MMLUâ€”by applying $\\texttt{ATA}$ to open-source LLMs. $\\texttt{R$^2$ATA}$ demonstrates remarkable transferability and causes notable performance drops across multiple super large and closed-source LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MumfvVOHTQ": {
    "title": "Unveiling Wisdom: Inspirational Quote Extraction using a Retrieval Augmented Multi-Task Reader",
    "volume": "review",
    "abstract": "Inspirational quotes from famous individuals are often used to convey thoughts in news articles, essays, and everyday conversations. In this paper, we propose a novel context-based quote extraction system that aims to predict the most relevant quote from a long text. We formulate this quote extraction as an open domain question answering problem first by employing a vector-store based retriever and then applying a multi-task reader. We curate three context-based quote extraction dataset and introduce a novel multi-task framework that improves the state-of-the-art performance, achieving a maximum improvement of 5.08% in BoW F1-score",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BTOfSCbjC5": {
    "title": "CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures",
    "volume": "review",
    "abstract": "Explaining Artificial Intelligence (AI) decisions is a major challenge nowadays in AI, in particular when applied to sensitive scenarios like medicine and law. However, the need to explain the rationale behind decisions is a main issues also for human-based deliberation as it is important to justify why a certain decision has been taken. Resident medical doctors for instance are required not only to provide a (possibly correct) diagnosis, but also to explain how they reached a certain conclusion. Developing new tools to aid residents to train their explanation skills is therefore a central objective of AI in education. In this paper, we follow this direction, and we present, to the best of our knowledge, the first multilingual dataset for Medical Question Answering where correct and incorrect diagnoses for a clinical case are enriched with a natural language explanation written by doctors. These explanations have been manually annotated with argument components (i.e., premise, claim) and argument relations (i.e., attack, support). The Multilingual CasiMedicos-arg dataset consists of 558 clinical cases (English, Spanish, French, Italian) with explanations, where we annotated 5021 claims, 2313 premises, 2431 support relations, and 1106 attack relations. We conclude by showing how competitive baselines perform over this challenging dataset for the argument mining task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uQDOxlH9LO": {
    "title": "Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach",
    "volume": "review",
    "abstract": "Susceptibility to misinformation describes the degree of belief in unverifiable claims, a latent aspect of individuals' mental processes that is not observable. Existing susceptibility studies heavily rely on self-reported beliefs, which can be subject to bias, expensive to collect, and challenging to scale for downstream applications. To address these limitations, in this work, we propose a computational approach to efficiently model users' latent susceptibility levels. As shown in previous work, susceptibility is influenced by various factors (e.g., demographic factors, political ideology), and directly influences people's reposting behavior on social media. To represent the underlying mental process, our susceptibility modeling incorporates these factors as inputs, guided by the supervision of people's sharing behavior. Using COVID-19 as a testbed, our experiments demonstrate a significant alignment between the susceptibility scores estimated by our computational modeling and human judgments, confirming the effectiveness of this latent modeling approach. Furthermore, we apply our model to annotate susceptibility scores on a large-scale dataset and analyze the relationships between susceptibility with various factors. Our analysis reveals that political leanings, etc., psychological factors exhibit varying degrees of association with susceptibility to COVID-19 misinformation, and shows that susceptibility is unevenly distributed across different professional and geographical backgrounds",
    "checked": false,
    "id": "43cbfe896a3b3e5194f43a2161f40e29463ff29d",
    "semantic_title": "decoding susceptibility: modeling misbelief to misinformation through a computational approach (preprint)",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Txi2KnUmiH": {
    "title": "On the Role of Entity and Event Level Conceptualization in Generalizable Reasoning: A Survey of Tasks, Methods, Applications, and Future Directions",
    "volume": "review",
    "abstract": "Entity- and event-level conceptualization, as fundamental elements of human cognition, plays a pivotal role in generalizable reasoning. This process involves abstracting specific instances into higher-level concepts and forming abstract knowledge that can be applied in unfamiliar or novel situations, which can enhance models' inferential capabilities and support the effective transfer of knowledge across various domains. Despite its significance, there is currently a lack of a systematic overview that comprehensively examines existing works in the definition, execution, and application of conceptualization to enhance reasoning tasks. In this paper, we address this gap by presenting the first comprehensive survey of 150+ papers, categorizing various definitions, resources, methods, and downstream applications related to conceptualization into a unified taxonomy, with a focus on the entity and event levels. Furthermore, we shed light on potential future directions in this field and hope to garner more attention from the community",
    "checked": true,
    "id": "d70a1f45665fcef448ce4daedeac34f5f4befaa2",
    "semantic_title": "on the role of entity and event level conceptualization in generalizable reasoning: a survey of tasks, methods, applications, and future directions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YI8HSqgwjp": {
    "title": "Benchmarking the Ability of Large Language Models to Reason about Event Sequences",
    "volume": "review",
    "abstract": "The ability to reason about events and their temporal relations is a key aspect in Natural Language Understanding. In this paper, we investigate the ability of Large Language Models to resolve temporal references with respect to longer event sequences. Given that events rarely occur in isolation, it is crucial to determine the extent to which Large Language Models can reason about longer sequences of events. Towards this goal, we introduce a novel synthetic benchmark dataset comprising of 2200 questions to test the abilities of LLMs to reason about events using a Question Answering task as proxy. We compare the performance of 4 state of the art LLMs on the benchmark, analyzing their performance in dependence of the length of the event sequence considered as well as of the explicitness of the temporal reference. Our results show that, while the benchmarked LLMs can answer questions over event sequences with a handful of events and explicit temporal references successfully, performance clearly deteriorates with larger event sequence length and when temporal references get less explicit",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lMd0iIOwyX": {
    "title": "Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown significant achievements in solving a wide range of tasks. Recently, LLMs' capability to store, retrieve and infer with symbolic knowledge has drawn a great deal of attention, showing their potential to understand structured information. However, it is not yet known whether LLMs can understand Description Logic (DL) ontologies. In this work, we empirically analyze the LLMs' capability of understanding DL-Lite ontologies covering 6 representative tasks from syntactic and semantic aspects. With extensive experiments, we demonstrate both the effectiveness and limitations of LLMs in understanding DL-Lite ontologies. We find that LLMs can understand formal syntax and model-theoretic semantics of concepts and roles. However, LLMs struggle with understanding TBox NI transitivity and handling ontologies with large ABoxes. We hope that our experiments and analyses provide more insights into LLMs and inspire to build more faithful knowledge engineering solutions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U8HPrsnLcb": {
    "title": "Identify High-Risk Suicidal Posts and Psychological Risk Factors on Social Media Using a Two-Stage Deep Learning Model",
    "volume": "review",
    "abstract": "Our study aims to utilize psychological risk factors to detect articles on social media that are at high risk for suicidal content. We propose a two-stage model structure: the first stage labels each sentence in an article with risk factors, and the second stage uses this information as features to predict the crisis level of the article. Our models were trained using a dataset that we developed, which consists of social media posts from Dcard. These posts were labeled by psychological professionals and will be publicly released. Our approach achieved an accuracy and F1-score of 0.96 in classifying high-crisis-level articles. Our research facilitates the automatic detection of high-crisis-level articles for further analysis of risk factors, enhancing interdisciplinary collaboration between natural language processing, deep learning, and psychology",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tBaSkqpU1S": {
    "title": "Learnability of Indirect Evidence in Language Models",
    "volume": "review",
    "abstract": "What kinds of and how much data is necessary for language models to acquire grammatical knowledge to judge sentence acceptability? Recent language models still have much room for improvement in their data efficiency compared to humans. In this paper, we investigate whether language models efficiently use indirect data (indirect evidence), from which they infer sentence acceptability. In contrast, humans use indirect evidence efficiently, which is considered one of the inductive biases contributing to efficient language acquisition. To explore this question, we inject synthetic instances with newly coined \"wug\" words into pretraining data and explore the model's behavior on evaluation data that assess grammatical acceptability regarding those words. We prepare the injected instances by varying their levels of indirectness and quantity. Our experiments surprisingly show that language models do not acquire grammatical knowledge even after repeated exposure to instances with the same structure but differing only in lexical items from evaluation instances in certain language phenomena. Our findings suggest a potential direction for future research: developing models that use latent indirect evidence to acquire grammatical knowledge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tb9te4v9KK": {
    "title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
    "volume": "review",
    "abstract": "$\\textit{``An idea is nothing more nor less than a new combination of old elements\"}$ (Young, J.W.). The widespread adoption of Large Language Models (LLMs) and publicly available ChatGPT have marked a significant turning point in the integration of Artificial Intelligence (AI) into people's everyday lives. This study explores the capability of LLMs in generating novel research ideas based on information from research papers. We conduct a thorough examination of 4 LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and Physics). We found that the future research ideas generated by Claude-2 and GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini. We also found that Claude-2 generates more diverse future research ideas than GPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the novelty, relevancy, and feasibility of the generated future research ideas. This investigation offers insights into the evolving role of LLMs in idea generation, highlighting both its capability and limitations. Our work contributes to the ongoing efforts in evaluating and utilizing language models for generating future research ideas. We make our datasets and codes publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vFf7ZDoLA2": {
    "title": "Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention Steering",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance across various real-world tasks. However, recent studies reveal that LLMs often struggle to fully comprehend and effectively utilize their input contexts, resulting in responses that lack faithfulness or suffer from hallucination. This difficulty becomes particularly evident when the contexts are lengthy or contain distracting information, which can divert LLMs from fully capturing essential evidence. Most prior work focuses on designing effective prompts to guide LLMs in utilizing contextual information more faithfully. For instance, iterative prompting highlights key information through two high-level prompting steps that first ask the LLM to identify important pieces of context and then derive answers accordingly. However, prompting methods are constrained to highlighting key information implicitly in token space, which is often insufficient to fully steer the model's attention. To improve model faithfulness more reliably, we propose AutoPASTA, a method that automatically identifies contextual key information and explicitly highlights it by steering the model's attention scores. Similar to prompting, AutoPASTA is applied at inference time and does not require changing any model parameters. Our experiments on open-book QA demonstrate that AutoPASTA can effectively guide models to grasp essential contextual information, leading to substantially improved model faithfulness and performance, e.g., an average improvement of 11.26% for LLAMA3-8B-Instruct. Code will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X9h5qaKqqT": {
    "title": "Hallucination Mitigating for Medical Report Generation",
    "volume": "review",
    "abstract": "In the realm of medical report generation (MRG), the integration of natural language processing has emerged as a vital tool to alleviate the workload of radiologists. Despite the impressive capabilities demonstrated by large vision language models (LVLMs) in understanding natural language, their susceptibility to generating plausible yet inaccurate claims, known as ``hallucinations'', raises concernsâ€”especially in the nuanced and critical field of medical. In this work, we introduce a framework, Knowledge-Enhanced with Fine-Grained Reinforced Rewards Medical Report Generation (KERM), to tackle the issue. Our approach refines the input to the LVLM by first utilizing MedCLIP for knowledge retrieval, incorporating relevant lesion fact sentences from a curated knowledge corpus. We then introduce a novel purification module to ensure the retrieved knowledge is contextually relevant to the patient's clinical context. Subsequently, we employ fine-grained rewards to guide these models in generating highly supportive and clinically relevant descriptions, ensuring the alignment of model's outputs with desired behaviors. Experimental results on IU-Xray and MIMIC-CXR datasets validate the effectiveness of our approach in mitigating hallucinations and enhancing report quality",
    "checked": false,
    "id": "f818e71f883aa8eb515331b01e24ba3530968664",
    "semantic_title": "cross-modal causal intervention for medical report generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=t04AUXMwIx": {
    "title": "Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG) via Pure Synthetic Data",
    "volume": "review",
    "abstract": "Retrieval-augmented generation (RAG) enhances the outputs of language models by integrating relevant information retrieved from external knowledge sources. However, when the retrieval process involves private data, RAG systems may face severe privacy risks, potentially leading to the leakage of sensitive information. To address this issue, we propose using synthetic data as a privacy-preserving alternative for the retrieval data. We propose SAGE, a novel two-stage synthetic data generation paradigm. In the stage-1, we employ an attribute-based extraction and generation approach to preserve key contextual information from the original data. In the stage-2, we further enhance the privacy properties of the synthetic data through an agent-based iterative refinement process. Extensive experiments demonstrate that using our synthetic data as the retrieval context achieves comparable performance to using the original data while substantially reducing privacy risks. Our work takes the first step towards investigating the possibility of generating high-utility and privacy-preserving synthetic data for RAG, opening up new opportunities for the safe application of RAG systems in various domains",
    "checked": true,
    "id": "dcdcf97a86050202d0a804202e56e5878c167951",
    "semantic_title": "mitigating the privacy issues in retrieval-augmented generation (rag) via pure synthetic data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rhdC4p4ujD": {
    "title": "Understanding the Therapeutic Relationship between Counselors and Clients in Online Text-based Counseling using LLMs",
    "volume": "review",
    "abstract": "Robust therapeutic relationships between counselors and clients are fundamental to counseling effectiveness. The assessment of therapeutic alliance is well-established in traditional face-to-face therapy but may not directly translate to text-based settings. With millions of individuals seeking support through online text-based counseling, understanding the relationship in such contexts is crucial. In this paper, we present an automatic approach using large language models (LLMs) to understand the development of therapeutic alliance in text-based counseling. We develop a theoretically grounded framework with detailed guidelines for characterizing the alliance. We collect a comprehensive counseling dataset and conduct multiple expert evaluations on a subset based on this framework. Our LLM-based approach, combined with guidelines and simultaneous extraction of supportive evidence underlying its predictions, demonstrates effectiveness in identifying the therapeutic alliance. Through further LLM-based evaluations on additional conversations, our findings underscore the challenges counselors face in cultivating strong online relationships with clients. Furthermore, we demonstrate the potential of LLM-based feedback mechanisms to enhance counselors' ability to build relationships, supported by a small-scale proof-of-concept",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KoKV0CFGTB": {
    "title": "WundtGPT: Shaping Large Language Models To Be An Empathetic, Proactive Psychologist",
    "volume": "review",
    "abstract": "Large language models (LLMs) are raging over the medical domain, and their momentum has carried over into the mental health domain, leading to the emergence of few mental health LLMs. Although such mental health LLMs could provide reasonable suggestions for psychological counseling, how to develop an authentic and effective doctor-patient relationship (DPR) through LLMs is still an important problem. To fill this gap, we dissect DPR into two key attributes, i.e., the psychologist's empathy and proactive guidance. We thus present WundtGPT, an empathetic and proactive mental health large language model that is acquired by fine-tuning it with instruction and real conversation between psychologists and patients. It is designed to assist psychologists in diagnosis and help patients who are reluctant to communicate face-to-face understand their psychological conditions. Its uniqueness lies in that it could not only pose purposeful questions to guide patients in detailing their symptoms but also offer warm emotional reassurance. In particular, WundtGPT incorporates \\textbf{Collection of Questions}, \\textbf{Chain of Psychodiagnosis}, and \\textbf{Empathy Constraints} into a comprehensive prompt for eliciting LLMs' questions and diagnoses. Additionally, WundtGPT proposes a reward model to promote alignment with empathetic mental health professionals, which encompasses two key factors: cognitive empathy and emotional empathy. We offer a comprehensive evaluation of our proposed model. Based on these outcomes, we further conduct the manual evaluation based on proactivity, effectiveness, professionalism and coherence. We notice that WundtGPT can offer professional and effective consultation. The model is available at huggingface",
    "checked": true,
    "id": "6a7246a8cf065cf2858fee9d9ccca298e9ae4546",
    "semantic_title": "wundtgpt: shaping large language models to be an empathetic, proactive psychologist",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dBbWg7LQWK": {
    "title": "KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view Knowledge Graph Contrastive Learning",
    "volume": "review",
    "abstract": "Autoregressive large language models (LLMs) pre-trained by next token prediction are inherently proficient in generative tasks. However, their performance on knowledge-driven tasks such as factual knowledge querying remains unsatisfactory. Knowledge graphs (KGs), as high-quality structured knowledge bases, can provide reliable knowledge for LLMs, potentially compensating for their knowledge deficiencies. Aligning LLMs with explicit, structured knowledge from KGs has been a challenge; previous attempts either failed to effectively align knowledge representations or compromised the generative capabilities of LLMs, leading to less-than-optimal outcomes. This paper proposes \\textbf{KaLM}, a \\textit{Knowledge-aligned Language Modeling} approach, which fine-tunes autoregressive LLMs to align with KG knowledge via the joint objective of explicit knowledge alignment and implicit knowledge alignment. The explicit knowledge alignment objective aims to directly optimize the knowledge representation of LLMs through dual-view knowledge graph contrastive learning. The implicit knowledge alignment objective focuses on incorporating textual patterns of knowledge into LLMs through triple completion language modeling. Notably, our method achieves a significant performance boost in evaluations of knowledge-driven tasks, specifically embedding-based knowledge graph completion and generation-based knowledge graph question answering",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BjtAS1vMxV": {
    "title": "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian",
    "volume": "review",
    "abstract": "Norwegian, spoken by only 5 million population, is under-representative within the most impressive breakthroughs in NLP tasks. To the best of our knowledge, there has not yet been a comprehensive evaluation of the existing language models (LMs) on Norwegian generation tasks during the article writing process. To fill this gap, we 1) compiled the existing Norwegian dataset and pre-trained 4 Norwegian Open Language Models varied from parameter scales and architectures, collectively called NorGLM; 2) introduced a comprehensive benchmark, NLEBench, for evaluating natural language generation capabilities in Norwegian, encompassing translation and human annotation. Based on the investigation, we find that: 1) the mainstream, English-dominated LM GPT-3.5 has limited capability in understanding the Norwegian context; 2) the increase in model parameter scales demonstrates limited impact on the performance of downstream tasks when the pre-training dataset is constrained in size; 3) smaller models also demonstrate the reasoning capability through Chain-of-Thought; 4) a multi-task dataset that includes synergy tasks can be used to verify the generalizability of LLMs on natural language understanding and, meanwhile, test the interconnectedness of these NLP tasks. We share our resources and code for reproducibility under a CC BY-NC 4.0 license",
    "checked": true,
    "id": "0ad16e2c1c30d8ed5b63970e5fb3459a08218ea3",
    "semantic_title": "nlebench+norglm: a comprehensive empirical analysis and benchmark dataset for generative language models in norwegian",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=atQoQJa3Sz": {
    "title": "Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation",
    "volume": "review",
    "abstract": "Despite recent advancements in language and vision modeling, integrating rich multimodal knowledge into recommender systems continues to pose significant challenges. This is primarily due to the need for efficient recommendation, which requires adaptive and interactive responses. In this study, we focus on sequential recommendation and introduce a lightweight framework called full-scale Matryoshka representation learning for multimodal recommendation (fMRLRec). Our fMRLRec captures item features at different granularities, learning informative representations for efficient recommendation across multiple dimensions. To integrate item features from diverse modalities, fMRLRec employs a simple mapping to project multimodal item features into an aligned feature space. Additionally, we design an efficient linear transformation that embeds smaller features into larger ones, substantially reducing memory requirements for large-scale training on recommendation data. Combined with improved state space modeling techniques, fMRLRec scales to different dimensions and only requires one-time training to produce multiple models tailored to various granularities. We demonstrate the effectiveness and efficiency of fMRLRec on multiple benchmark datasets, which consistently achieves superior performance over state-of-the-art baseline methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tf8PJtuaGq": {
    "title": "KG-TRICK: Unifying Textual and Relational Information Completion of Knowledge for Multilingual Knowledge Graphs",
    "volume": "review",
    "abstract": "Multilingual knowledge graphs (KGs) provide high-quality relational and textual information for various NLP applications but they are often incomplete, especially in non-English languages. Previous research has shown that combining information from several knowledge graphs in different languages aids both Knowledge Graph Completion (KGC), the task of predicting of missing relations between entities, and Knowledge Graph Enhancement (KGE), the task of predicting missing textual information for entities. While previous efforts have considered KGC and KGE as independent tasks, we hypothesize that they are interdependent and mutually beneficial. To this end, we introduce KG-TRICK, a novel sequence-to-sequence framework that unifies the tasks of textual and relational information completion for multilingual knowledge graphs. KG-TRICK demonstrates that i) it is possible to unify the tasks of KGC and KGE into one single framework, and ii) combining textual information from multiple languages is beneficial to improve the completeness of a KG. As part of our contributions, we also introduce WikiKGE++, the largest manually-curated benchmark for textual information completion of KGs, which features over 30,000 instances across 10 diverse languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TWGjxLNW7g": {
    "title": "SSP: Story-Space Prompting Improves the Reader Immersion in Long Story Generation",
    "volume": "review",
    "abstract": "Generating long-form stories with neural network models, even the large language models (LLMs), e.g., GPT, has always been criticized for lacking interestingness and coherence, thus greatly diminishing the reader's sense of immersion. In this paper, we present a novel \"story space\" prompting (SSP) solution, which provides a coherent and consistent background to support long-term storytelling. Specifically, we first define the story space intricately connected to the given story premise. Then, Our framework systematically generates the story space by progressively constructing it from an abstract representation to a more informative and detailed one. Empirically, we implement our plug-in method upon an existing advanced story generation framework (Yang et al., 2023) and evaluate its impact on both interestingness and coherence. Our findings emphasize the significance of our SSP in enhancing reader enjoyment and immersion, contributing to advancements in long-form story generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bCAxEwwmBr": {
    "title": "Fact-Level Confidence Calibration: Empowering Confidence-Guided LLM Self-Correction",
    "volume": "review",
    "abstract": "Confidence calibration in LLMs, i.e., aligning their self-assessed confidence with the actual accuracy of their responses, enabling them to self-evaluate the correctness of their outputs. However, current calibration methods for LLMs typically estimate two scalars to represent overall response confidence and correctness, which is inadequate for long-form generation where the response includes multiple atomic facts and may be partially confident and correct. These methods also overlook the relevance of each fact to the query. To address these challenges, we propose a Fact-Level Calibration framework that operates at a finer granularity, calibrating confidence to relevance-weighted correctness at the fact level. Furthermore, comprehensive analysis under the framework inspired the development of Confidence-guided Fact-level self-correction (ConFact), which uses high-confidence facts within a response as additional knowledge to improve low-confidence ones. Extensive experiments across four datasets and six models demonstrate that ConFact effectively mitigates hallucinations without requiring external knowledge sources such as retrieval systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nvvg466R6g": {
    "title": "Hybrid Generative and Commonsense Knowledge for Script Event Prediction",
    "volume": "review",
    "abstract": "Script event prediction aims to predict subsequent events given contextual events, which requires inferring correlations between contexts and candidate events. Current research focuses on improving script event prediction using external knowledge and pre-trained language models, but faces the problems of sparse event-level correlation knowledge and separation of word-level correlation knowledge. In this paper, we propose a novel model CoGen-Predictor based on hybrid generative and commonsense knowledge that combines explicit event-level and implicit word-level correlation knowledge for prediction. CoGen-Predictor constructs event-level correlations through a commonsense knowledge base and updates the event representations using graph neural networks, then learns word-level contextual event correlations through a generative approach. Experimental results on the multi-choice narrative cloze (MCNC) task demonstrate the effectiveness of the model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2HYsVlsGfX": {
    "title": "Reconstruct Your Previous Conversations! Comprehensively Investigating Privacy Leakage Risks in Conversations with GPT Models",
    "volume": "review",
    "abstract": "Significant advancements have been made in the field of large language models recently, represented by GPT models. Users frequently have multi-round private conversations with cloud-hosted GPT models for task optimization. Yet, this operational paradigm introduces additional attack surfaces, particularly in custom GPTs and hijacked chat sessions. In this paper, we introduce a straightforward yet potent Conversation Reconstruction Attack, that employs malicious prompts to query GPT models to leak previous conversations. Our comprehensive examination of privacy risks during GPT interactions under this attack reveals GPT-4's considerable resilience. We present two advanced attacks targeting improved reconstruction of past conversations, demonstrating significant privacy leakage across all models under these advanced techniques. Evaluating various defense mechanisms, we find them ineffective against these attacks. Our findings highlight the ease with which privacy can be compromised in interactions with GPT models, urging the community to safeguard against potential abuses of these models' capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SzzfKS6rnJ": {
    "title": "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models",
    "volume": "review",
    "abstract": "It is imperative for Large language models (LLMs) to follow instructions with elaborate requirements (i.e. Complex Instructions Following}). Yet, it remains under-explored how to enhance the ability of LLMs to follow complex instructions with multiple constraints. To bridge the gap, we initially study what training data is effective in enhancing complex constraints following abilities. We found that training LLMs with instructions containing multiple constraints enhances their understanding of complex instructions, especially those with lower complexity levels. Additionally, we further propose methods addressing how to obtain and utilize the effective training data. Finally, we conduct extensive experiments to prove the effectiveness of our methods in terms of overall performance and training efficiency. We also demonstrate that our methods improve models' ability to follow instructions generally and generalize effectively across out-of-domain, in domain, and adversarial settings, while maintaining general capabilities",
    "checked": true,
    "id": "ad5c359094dbfb68b1e09b22e2be3ce6dac33e3d",
    "semantic_title": "from complex to simple: enhancing multi-constraint complex instruction following ability of large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=JFdKwzKSrY": {
    "title": "Dynamic Multi-granularity Attribution Network for Aspect-based Sentiment Analysis",
    "volume": "review",
    "abstract": "Aspect-based sentiment analysis (ABSA) aims to predict the sentiment polarity of a specific aspect within a given sentence. Most existing methods predominantly leverage semantic or syntactic information based on attention scores, which are susceptible to interference caused by irrelevant contexts and often lack sentiment knowledge at a data-specific level. In this paper, we propose a novel Dynamic Multi-granularity Attribution Network (DMAN) from the perspective of attribution. Initially, we leverage Integrated Gradients to dynamically extract importance scores for each token, which contain underlying reasoning knowledge for sentiment analysis. Subsequently, we aggregate attribution representations from multiple semantic granularities in natural language, enhancing profound understanding of the semantics. Finally, we integrate attribution scores with syntactic information to more accurately capture the relationships between aspects and their relevant contexts during the sentence understanding process. Extensive experiments on five benchmark datasets demonstrate the effectiveness of our proposed method",
    "checked": false,
    "id": "972cb3b6691d6915db719ad576d04be822723f8e",
    "semantic_title": "green supplier selection based on sequential group three-way decision making",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=iLhpEOXBdZ": {
    "title": "Discrete Diffusion Language Model for Long Text Summarization",
    "volume": "review",
    "abstract": "While diffusion models excel at conditional generating high-quality images, prior works in discrete diffusion models were not evaluated on conditional long-text generation. In this work, we address the limitations of prior discrete diffusion models for conditional long-text generation, particularly in long sequence-to-sequence tasks such as abstractive summarization. Despite fast decoding speeds compared to autoregressive methods, previous diffusion models failed on the abstractive summarization task due to the incompatibility between the backbone architectures and the random noising process. To overcome these challenges, we introduce a novel semantic-aware noising process that enables Transformer backbones to handle long sequences effectively. Additionally, we propose CrossMamba, an adaptation of the Mamba model to the encoder-decoder paradigm, which integrates seamlessly with the random absorbing noising process. Our approaches achieve state-of-the-art performance on three benchmark summarization datasets: Gigaword, CNN/DailyMail, and Arxiv, outperforming existing discrete diffusion models on ROUGE metrics as well as possessing much faster speed in inference compared to autoregressive models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4hlnbLp0k4": {
    "title": "Latent Segment Language Models",
    "volume": "review",
    "abstract": "Tokenization is a critical step in every NLP system, yet most works treat it as an isolated component separate from the models they are building. In this paper, we present a framework to jointly learn next-token prediction and segmentation from a sequence of characters or bytes. We evaluate our model on language modeling benchmarks in English, Chinese, and Japanese using both character and byte vocabularies. Our model consistently outperforms baselines on Chinese benchmarks with character vocabulary and shows significant improvements with byte vocabulary. Further latency improvements are achieved by adapting different pooling strategies while maintaining comparable results to the best models. Our main contributions are threefold: we propose a language model that learns to segment the input sequence, conforming to the desired segmentation prior; we demonstrate that our model achieves shorter latency than baselines in token generation; and we show that our model can be applied to three different languagesâ€”English, Chinese, and Japaneseâ€”demonstrating its potential for wider NLP applications. Our source code will be released on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NnpVpW7A3x": {
    "title": "IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization",
    "volume": "review",
    "abstract": "In this work, we address the problem of text anonymization where the goal is to prevent adversaries from correctly inferring private attributes of the author, while keeping the text utility, i.e., meaning and semantics. We propose IncogniText, a technique that anonymizes the text to mislead a potential adversary into predicting a wrong private attribute value. Our empirical evaluation shows a reduction of private attribute leakage by more than $90\\%$. Finally, we demonstrate the maturity of IncogniText for real-world applications by distilling its anonymization capability into a set of LoRA parameters associated with an on-device model",
    "checked": true,
    "id": "70acacd0c50c7fc30f7e7ede1546b945918a3c93",
    "semantic_title": "incognitext: privacy-enhancing conditional text anonymization via llm-based private attribute randomization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RgqajywZa5": {
    "title": "XL$^2$Bench: A Benchmark for Extremely Long Context Understanding with Long-range Dependencies",
    "volume": "review",
    "abstract": "Recently, various efforts have been proposed to expand the context window size of large language models (LLMs). Meanwhile, building high-quality benchmarks with much longer text lengths and more demanding tasks to provide comprehensive evaluations is of immense practical interest to facilitate long context understanding research of LLMs. However, prior benchmarks create datasets that ostensibly cater to long-text comprehension by expanding the input of traditional tasks, which falls short to exhibit the unique characteristics of long-text understanding, including long dependency tasks and longer text length compatible with modern LLMs' context window size. In this paper, we introduce a benchmark for extremely long context understanding with long-range dependencies, XL$^2$Bench, which includes three scenariosâ€”Fiction Reading, Paper Reading, and Law Readingâ€”and four tasks of increasing complexity: Memory Retrieval, Detailed Understanding, Overall Understanding, and Open-ended Generation, covering 27 subtasks in English and Chinese. It has an average length of 100K+ words (English) and 200K+ characters (Chinese). Evaluating seven leading LLMs on XL$^2$Bench, we find that their performance significantly lags behind human levels. Moreover, the observed decline in performance across both the original and enhanced datasets underscores the efficacy of our approach to mitigating data contamination",
    "checked": true,
    "id": "d9db8114b3c79f96ee27a9f16e83430448ad3e68",
    "semantic_title": "xl$^2$bench: a benchmark for extremely long context understanding with long-range dependencies",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=DdGqSrG3oI": {
    "title": "Communicating with Speakers and Listeners of Different Pragmatic Levels",
    "volume": "review",
    "abstract": "This paper explores the impact of variable pragmatic competence on communicative success through simulating language learning and conversing between speakers and listeners with different levels of reasoning abilities. Through studying this interaction, we hypothesize that matching levels of reasoning between communication partners would create a more beneficial environment for communicative success and language learning. Our research findings indicate that learning from more explicit, literal language is advantageous, irrespective of the learner's level of pragmatic competence. Furthermore, we find that integrating pragmatic reasoning during language learning, not just during evaluation, significantly enhances overall communication performance. This paper provides key insights into the importance of aligning reasoning levels and incorporating pragmatic reasoning in optimizing communicative interactions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4OaaSHmdVM": {
    "title": "Memorize Step by Step: Efficient Long-Context Prefilling with Incremental Memory and Decremental Chunk",
    "volume": "review",
    "abstract": "The evolution of Large Language Models (LLMs) has led to significant advancements, with models like Claude and Gemini capable of processing contexts up to 1 million tokens. However, efficiently handling long sequences remains challenging, particularly during the prefilling stage when input lengths exceed GPU memory capacity. Traditional methods often segment sequence into chunks and compress them iteratively with fixed-size memory. However, our empirical analysis shows that the fixed-size memory results in wasted computational and GPU memory resources. Therefore, we introduces Incremental Memory (IM), a method that starts with a small memory size and gradually increases it, optimizing computational efficiency. Additionally, we propose Decremental Chunk based on Incremental Memory (IMDC), which reduces chunk size while increasing memory size, ensuring stable and lower GPU memory usage. Our experiments demonstrate that IMDC is consistently faster (1.45x) and reduces GPU memory consumption by 23.3\\% compared to fixed-size memory, achieving comparable performance on the LongBench Benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fJk1Iv91zQ": {
    "title": "Mining Word Boundaries from Speech for Cross-domain Chinese Word Segmentation",
    "volume": "review",
    "abstract": "Inspired by early research on exploring naturally annotated data for Chinese Word Segmentation (CWS), and also by recent research on integration of speech and text processing, this work for the first time proposes to explicitly mine word boundaries from parallel speech-text data. We employ the Montreal Forced Aligner (MFA) toolkit to perform character-level alignment on speech-text data, giving pauses as candidate word boundaries. Based on detailed analysis of collected pauses, we propose an effective probability-based strategy for filtering unreliable word boundaries. To more effectively utilize word boundaries as extra training data, we also propose a robust complete-then-train (CTT) strategy. We conduct cross-domain CWS experiments on two target domains, i.e., ZX and AISHELL2. We have also annotated about 900 sentences as the evaluation data of AISHELL2. Experiments demonstrate the effectiveness of our proposed approach",
    "checked": false,
    "id": "827adfd441ed4df68ada97b89638cb002e593835",
    "semantic_title": "mining word boundaries in speech as naturally annotated word segmentation data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sl5MjsXQnB": {
    "title": "Can LLMs Patch Security Issues?",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown impressive proficiency in code generation. Unfortunately, these models share a weakness with their human counterparts: producing code that inadvertently has security vulnerabilities. These vulnerabilities could allow unauthorized attackers to access sensitive data or systems, which is unacceptable for safety-critical applications. %In this paper, we evaluate LLMs' ability to generate vulnerable code on existing datasets and approaches, discuss the limitations, and propose a new dataset and novel approach to address these limitations. We propose Feedback-Driven Security Patching (FDSP), where LLMs automatically refine generated, vulnerable code. Our approach leverages automatic static code analysis to empower the LLM to generate and implement potential solutions to address vulnerabilities. We address the research community's needs for safe code generation by introducing a large-scale dataset, PythonSecurityEval, covering the diversity of real-world applications, including databases, websites and operating systems. We empirically validate that FDSP outperforms prior work that uses self-feedback from LLMs by up to 17.6\\% through our procedure that injects targeted, external feedback. Code and data are attached",
    "checked": true,
    "id": "f1c93c2f2e0fad83f323db5a6b6ac1aec9dbd830",
    "semantic_title": "can llms patch security issues?",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=CXcaPRwKYz": {
    "title": "What Makes Two Language Models Think Alike?",
    "volume": "review",
    "abstract": "Do architectural differences significantly affect the way models represent and process language? We propose a new approach, based on metric-learning encoding models (MLEMs), as a first step to answer this question. The approach provides a feature-based comparison of how any two layers of any two models represent linguistic information. We apply the method to BERT, GPT-2 and Mamba. Unlike previous methods, MLEMs offer a transparent comparison, by identifying the specific linguistic features responsible for similarities and differences. More generally, the method uses formal, symbolic descriptions of a domain, and use these to compare neural representations. As such, the approach can straightforwardly be extended to other domains, such as speech and vision, and to other neural systems, including human brains",
    "checked": true,
    "id": "5c889532c401e719beabc45d2df5bcea1367839f",
    "semantic_title": "what makes two language models think alike?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B5BlmsErGE": {
    "title": "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature",
    "volume": "review",
    "abstract": "Recent advancements in large language models (LLMs) have achieved promising performances across various applications. Nonetheless, the ongoing challenge of integrating long-tail knowledge continues to impede the seamless adoption of LLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic Co-Augmentation of LLMs and KG, to address this limitation and demonstrate its ability on studying Alzheimer's Disease (AD), a specialized sub-field in biomedicine and a global health priority. With a synergized framework of LLM and KG mutually enhancing each other, we first leverage LLM to construct an evolving AD-specific knowledge graph (KG) sourced from AD-related scientific literature, and then we utilize a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the KG to augment LLM inference capabilities. The experimental results, conducted on our constructed AD question answering (ADQA) benchmark, underscore the efficacy of DALK. Additionally, we perform a series of detailed analyses that can offer valuable insights and guidelines for the emerging topic of mutually enhancing KG and LLM",
    "checked": true,
    "id": "f0a3cc74df84803589b8e8850fa3434d06e0c8eb",
    "semantic_title": "dalk: dynamic co-augmentation of llms and kg to answer alzheimer's disease questions with scientific literature",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=buHxwAaG2x": {
    "title": "TokenVerse: Unifying Speech and NLP Tasks via Transducer-based ASR",
    "volume": "review",
    "abstract": "In traditional conversational intelligence from speech, a cascaded pipeline is used, involving tasks such as voice activity detection, diarization, transcription, and subsequent processing with different NLP models for tasks like semantic endpointing and named entity recognition (NER). Our paper introduces TokenVerse, a single Transducer-based model designed to handle multiple tasks. This is achieved by integrating task-specific tokens into the reference text during ASR model training, streamlining the inference and eliminating the need for separate NLP models. In addition to ASR, we conduct experiments on 3 different tasks: speaker change detection, endpointing, and NER. Our experiments on a public and a private dataset show that the proposed method improves ASR by up to 7.7% in relative WER while outperforming the cascaded pipeline approach in individual task performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hdZYHlDMez": {
    "title": "Self-Enhanced Reasoning Training: Activating Latent Reasoning in Small Models for Effective Knowledge Distillation",
    "volume": "review",
    "abstract": "The deployment of large language models (LLMs) in resource-constrained environments is hindered by their high computational demands. Traditional knowledge distillation methods struggle with significant distributional mismatches between the reasoning outputs of large teacher models and smaller student models. This study introduces Self-Enhanced Reasoning Training (SERT), a novel approach that activates the latent reasoning capabilities of small models prior to distillation. By leveraging inherent reasoning paths, SERT prepares these models for more effective knowledge transfer. Empirical results show that SERT not only narrows the generation gap between large and small models but also improves their reasoning performance significantly. SERT-activated models produce outputs with greater format consistency and reduced repetition, achieving higher accuracy than those trained via direct distillation methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ISCsjScVW": {
    "title": "What Makes Cryptic Crosswords Challenging for LLMs?",
    "volume": "review",
    "abstract": "Cryptic crosswords are puzzles that rely on general knowledge and the solver's ability to manipulate language on different levels, dealing with various types of wordplay. Previous research suggests that solving such puzzles is a challenge even for modern NLP models. However, the abilities of large language models (LLMs) have not yet been tested on this task. In this paper, we establish the benchmark results for two popular LLMs: {\\tt LLaMA3} and {\\tt ChatGPT}, showing that their performance on this task is still far from that of humans. We also investigate why the models struggle to achieve superior performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=57q5tBHu6X": {
    "title": "Counter Turing Test ($CT^2$): Investigating AI-Generated Text Detection for Hindi - Ranking LLMs based on Hindi AI Detectability Index ($ADI_{hi}$)",
    "volume": "review",
    "abstract": "The widespread adoption of large language models (LLMs) and awareness around multilingual LLMs have raised concerns regarding the potential risks and repercussions linked to the misapplication of AI-generated text, necessitating increased vigilance. While these models are primarily trained for English, their extensive training on vast datasets covering almost the entire web, equips them with capabilities to perform well in numerous other languages. AI-Generated Text Detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by the emergence of techniques to bypass detection. In this paper, we report our investigation on AGTD for an indic language Hindi. Our major contributions are in four folds: i) examined 26 LLMs to evaluate their proficiency in generating Hindi text, ii) introducing the AI-generated news article in Hindi (AG\\textsubscript{hi}) dataset, iii) evaluated the effectiveness of five recently proposed AGTD techniques: ConDA, J-Guard, RADAR, RAIDAR and Intrinsic Dimension Estimation for detecting AI-generated Hindi text, iv) proposed Hindi AI Detectability Index ($ADI_{hi}$) which shows a spectrum to understand the evolving landscape of eloquence of AI-generated text in Hindi. To encourage further research in this field, we will be making the models and datasets available. The code and dataset can be found \\href{https://anonymous.4open.science/r/AGTD_Hindi-BC43/}{here}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FqHlUgDir4": {
    "title": "DeconICA: Deconfounding the Dataset Bias for Domain Generalization",
    "volume": "review",
    "abstract": "Domain generalization provides a research spot for enhancing the generalization capability of the machine learning model. We focus on a causal perspective for the domain generalization task. In causal theory, a confounder is a factor that affects both the cause and the effect. The confounder is often hidden, which causes problems in correctly performing the intervention. The Deconfounder approach indicates that a factorized multiple causes could be considered a substitute confounder. We choose a non-linear ICA method to factorize the data features to represent the confounder. The confounder is considered to represent the background, and domain biases. Empirical results on text and image classification domain generalization validate the proposed methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MBfnmeBFlC": {
    "title": "Representational Isomorphism and Alignment of Multilingual Large Language Models",
    "volume": "review",
    "abstract": "In this paper, we investigate the capability of large language models (LLMs) to represent texts in multilingual contexts. Our findings reveal that sentence representations derived from LLMs exhibit a high degree of isomorphism across languages. This existing isomorphism facilitates representational alignments in few-shot or even zero-shot settings. Specifically, by applying a contrastive objective at the representation level with only a small number (e.g., 100) of translation pairs, we significantly improve models' performance on Semantic Textual Similarity (STS) tasks across languages. This representation-level approach proves to be more efficient and effective for semantic alignment than continued pretraining or instruction tuning. Interestingly, we also observe substantial STS improvements within individual languages, even without a monolingual objective specifically designed for this purpose",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3wCpk0bnFU": {
    "title": "Leveraging Domain Knowledge for Efficient Reward Modeling in RLHF: A Case-Study in E-Commerce Opinion Summarization",
    "volume": "review",
    "abstract": "E-Commerce Opinion Summarization is the task of summarizing users' opinions expressed on a product (such as laptop, book, etc.). Prior approaches have failed to impart the human-desirable properties within an opinion summary. Recently, Reinforcement Learning from Human Feedback (RLHF) has become a dominant strategy in aligning Language Models (LMs) with human values. This motivates us to leverage RLHF for our task. The key to the strategy is learning a reward model ($\\varphi$), which can reflect the latent reward model of humans. The training process for $\\varphi$ requires sizeable human preference data, usually in the order of tens of thousands. However, human goals are subjective, and vary from task-to-task, hindering us from using a general purpose off-the-shelf reward model. This necessitates a large-scale preference annotation for our task, which is expensive and time-consuming. To address this challenge and still leverage RLHF, we propose a novel approach to infuse domain knowledge into $\\varphi$, which reduces the amount of preference annotation required ($21\\times$), while advancing SOTA ($\\sim4$-point \\rouge{L} improvement, $68\\%$ of times preferred by humans over SOTA). Our technique also omits Alignment Tax and provides some interpretability. We release our code: [anon.4open.science/efficient-rlhf](https://anonymous.4open.science/r/reward-approx-social-choice-opp-summ-B380)",
    "checked": false,
    "id": "c14b58a49667fb0990759905ea3b874d50a983d1",
    "semantic_title": "leveraging domain knowledge for efficient reward modelling in rlhf: a case-study in e-commerce opinion summarization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F0hrLeNX6d": {
    "title": "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models",
    "volume": "review",
    "abstract": "Recently, very large language models (LLMs) have shown exceptional performance on several English NLP tasks with just in-context learning (ICL), but their utility in other languages is still underexplored. We investigate their effectiveness for NLP tasks in low-resource languages (LRLs), especially in the setting of *zero-labelled* cross-lingual transfer (0-CLT), where no labelled training data for the target language is available -- however training data from one or more related medium-resource languages (MRLs) is utilized, alongside the available unlabeled test data for a target language. We introduce Self-Supervised Prompting (SSP), a novel ICL approach tailored for the 0-CLT setting. SSP is based on the key observation that LLMs output more accurate labels if in-context exemplars are from the target language (even if their labels are slightly noisy). To operationalize this, since target language training data is not available in 0-CLT, SSP operates in two stages. In Stage I, using source MRL training data, target language's test data is noisily labeled. In Stage II, these noisy test data points are used as exemplars in ICL for further improved labelling. Additionally, our implementation of SSP uses a novel Integer Linear Programming (ILP)-based exemplar selection that balances similarity, prediction confidence (when available) and label coverage. Experiments on three tasks and eleven LRLs (from three regions) demonstrate that SSP strongly outperforms existing SOTA fine-tuned and prompting-based baselines in 0-CLT setup",
    "checked": true,
    "id": "4e298242fd18904c09c67729d8f417546b0d02d0",
    "semantic_title": "ssp: self-supervised prompting for cross-lingual transfer to low-resource languages using large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jQKR6UQd9u": {
    "title": "Subjective Topic meets LLMs: Unleashing Comprehensive, Reflective and Creative Thinking through the Negation of Negation",
    "volume": "review",
    "abstract": "Large language models (LLMs) exhibit powerful reasoning capacity, as evidenced by prior studies focusing on objective topics that with unique standard answers such as arithmetic and commonsense reasoning. However, the reasoning to definite answers emphasizes more on logical thinking, and falls short in effectively reflecting the comprehensive, reflective, and creative thinking that is also critical for the overall reasoning prowess of LLMs. In light of this, we build a dataset SJTP comprising diverse SubJective ToPics with free responses, as well as three evaluation indicators to fully explore LLM's reasoning ability. We observe that a sole emphasis on logical thinking falls short in effectively tackling subjective challenges. Therefore, we introduce a framework grounded in the principle of the Negation of Negation (NeoN) to unleash the potential comprehensive, reflective, and creative thinking abilities of LLMs.Comprehensive experiments on SJTP demonstrate the efficacy of NeoN, and the enhanced performance on various objective reasoning tasks unequivocally underscores the benefits024 of stimulating LLM's subjective thinking in augmenting overall reasoning capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qQoWZ0C3ML": {
    "title": "TRACE: TRansformer-based Attribution using Contrastive Embeddings in LLMs",
    "volume": "review",
    "abstract": "The rapid evolution of large language models (LLMs) represents a substantial leap forward in natural language understanding and generation. However, alongside these advancements come significant challenges related to the accountability and transparency of LLM outputs. Reliable source attribution is essential to adhering to stringent legal and regulatory standards, including those set forth by the General Data Protection Regulation. Despite the well-established methods in source attribution within the computer vision domain, the application of robust attribution frameworks to natural language processing remains underexplored. To bridge this gap, we propose a novel and versatile TRansformer-based Attribution framework using Contrastive Embeddings called TRACE that, in particular, exploits contrastive learning for source attribution. We perform an extensive empirical evaluation to demonstrate the performance and efficiency of TRACE in various settings and show that TRACE significantly improves the ability to attribute sources accurately, making it a valuable tool for enhancing the reliability and trustworthiness of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DLhsUyK4F5": {
    "title": "Formality Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge",
    "volume": "review",
    "abstract": "Having been trained on massive pretraining data, large language models have shown excellent performance on many knowledge-intensive tasks. However, pretraining data tends to contain misleading and even conflicting information, and it is intriguing to understand how LLMs handle these noisy data during training. In this study, we systematically analyze LLMs' learning preferences for data with conflicting knowledge. We find that pretrained LLMs establish learning preferences similar to humans, i.e., preferences towards formal texts and texts with fewer spelling errors, resulting in faster learning and more favorable treatment of knowledge in data with such features when facing conflicts. This finding is generalizable across models and languages and is more evident in larger models. An in-depth analysis reveals that LLMs tend to trust data with features that signify consistency with the majority of data, and it is possible to instill new preferences and erase old ones by manipulating the degree of consistency with the majority data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MdydT5NSmq": {
    "title": "Non-Asymptotic Convergence Bounds for Cross-Entropy Estimation between Neural Auto-Regressive Language Models: Theoretical Analysis",
    "volume": "review",
    "abstract": "Cross-entropy (CE) represents a central metric in evaluating the performance and other characteristics of Neural Auto-Regressive Language Models (NARLMs). Despite its importance, the convergence properties of its estimation remain relatively underexplored from a theoretical perspective, primarily due to the complex structure of modern language model architectures. This article aims at investigating this issue by providing a formal theoretical analysis of the covergence properties of the CE estimation between different families of NARLMs. When the test distribution is modeled by a LSTM/GRU, we will show that CE estimation exhibits a non-vacuous convergence rate, which depends linearly on the norm of the output matrix of the test model and logarithmically on the alphabet size. Additionaly, we provide a variance-based convergence bound applicable to large families of NARLM, including Decoder-only Transformer-based models and LSTMs/GRUs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e0aS6qUefY": {
    "title": "Handling the Follow-up Question: Conversational Explanations for Image Classification",
    "volume": "review",
    "abstract": "Explainable AI (XAI) aims to provide insights into decisions made by deep neural networks. To date, most XAI approaches provide only one-time, static explanations, which cannot cater to users' diverse knowledge levels and information needs. Conversational explanations have been proposed as an effective method to customize XAI explanations. However, building conversational explanation systems is hindered by the scarcity of training data. Training with synthetic data faces two main challenges: lack of data diversity and hallucination in the generated data. To alleviate these issues, we introduce a repetition penalty to promote data diversity and exploit a hallucination detector to filter out untruthful synthetic conversation turns. The proposed system, fEw-shot Multi-round ConvErsational Explanation (EMCEE), achieves relative improvements of 81.6\\% in BLEU and 80.5\\% in ROUGE compared to the baselines. EMCEE also mitigates the degeneration of data quality caused by training on synthetic data. In human evaluations, EMCEE outperforms baseline models in improving users' comprehension, acceptance, trust, and collaboration with static explanations by large margins. To the best of our knowledge, this is the first conversational explanation method that can answer arbitrary user questions that follow from static explanations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HMg3RmxR96": {
    "title": "CKBP v2: Better Annotation and Reasoning for Commonsense Knowledge Base Population",
    "volume": "review",
    "abstract": "Commonsense Knowledge Bases (CSKB) Population, which aims at automatically expanding knowledge in CSKBs with external resources, is an important yet hard task in NLP. Fang et al. (2021a) proposed a CSKB Population (CKBP) framework with an evaluation set CKBP v1. However, CKBP v1 relies on crowdsourced annotations that suffer from a considerable number of mislabeled answers, and the evaluation set lacks alignment with the external knowledge source due to random sampling. In this paper, we introduce CKBP v2, a new high-quality CSKB Population evaluation set that addresses the two aforementioned issues by employing domain experts as annotators and incorporating diversified adversarial samples to make the evaluation data more representative. We show that CKBP v2 serves as a challenging and representative evaluation dataset for the CSKB Population task, while its development set aids in selecting a population model that leads to improved knowledge acquisition for downstream commonsense reasoning. A better population model can also help acquire more informative commonsense knowledge as additional supervision signals for both generative commonsense inference and zero-shot commonsense question answering. Specifically, the question-answering model based on DeBERTa-v3-large (He et al., 2023b) even outperforms powerful large language models in a zero-shot setting, including ChatGPT and GPT-3.5",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=27scPv7iSP": {
    "title": "Synthetic Data Generation with Large Language Models for Personalized Community Question Answering",
    "volume": "review",
    "abstract": "Personalization in Information Retrieval (IR) is a topic studied by the community for a long time. However, the collection and curation of high-quality training data requires significant costs and time investment, especially for collecting user-related information. In this paper we explore the usefulness of Large Language Models (LLMs) in generating synthetic documents tailored to user's personal interests using user-related information. We introduce a new dataset, Sy-SE-PQA, to study the effectiveness of models fine-tuned on LLM-generated data and study how the complexity of personalization impacts model performances. We build Sy-SE-PQA based on an existing dataset, SE-PQA, which consists of questions and answers posted on the popular StackExchange communities. Starting from questions in SE-PQA, we generate synthetic answers using different prompt techniques and LLMs. Our findings suggest that LLMs have high potential in generating training data, tailored to user's needs, for neural retrieval models and it can be used to replace training data. The code is publicly available",
    "checked": false,
    "id": "b9194cf73bfa63aabe69532af6a58cee9e541872",
    "semantic_title": "ua-llm: advancing context-based question answering in ukrainian through large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cEZBq67YOz": {
    "title": "Towards Explainable Chinese Native Learner Essay Fluency Assessment: Dataset, Tasks, and Method",
    "volume": "review",
    "abstract": "Grammatical Error Correction (GEC) is a crucial technique in Automated Essay Scoring (AES) for evaluating the fluency of essays. However, in Chinese, existing GEC datasets often fail to consider the importance of specific grammatical error types within compositional scenarios, lack research on data collected from native Chinese speakers, and largely overlook cross-sentence grammatical errors. Furthermore, the measurement of the overall fluency of an essay is often overlooked. To address these issues, we present CEFA (Chinese Essay Fluency Assessment), an extensive corpus that is derived from essays authored by native Chinese-speaking primary and secondary students and encapsulates essay fluency scores along with both coarse and fine-grained grammatical error types and corrections. Experiments employing various benchmark models on CEFA substantiate the challenging nature of our dataset. Our findings further highlight the significance of fine-grained annotations in fluency assessment and the mutually beneficial relationship between error types and corrections. We will make the corpus and related codes available for research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XOK0jVBlok": {
    "title": "States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) exhibit various emergent abilities. Among these abilities, some might reveal the internal working mechanisms of models. In this paper, we uncover a novel emergent capability in models: the intrinsic ability to perform extended sequences of calculations without relying on chain-of-thought step-by-step solutions. Remarkably, the most advanced models are capable of directly outputting the results of two-digit number additions with lengths extending up to 15 addends. We hypothesize that the model emerges discrete representations of symbols within its hidden states and performs symbolic calculations internally. To test this hypothesis, we design a sequence of experiments that look into the hidden states. Specifically, we first confirm that Implicit Discrete State Representations (IDSRs) exist. Then, we provide interesting observations about the formation of IDSRs from layer, digit, and sequence perspectives. Finally, we confirm that models indeed use IDSRs to produce the final answers. However, we also discover that the state representations are far from lossless in current open-sourced models, leading to inaccuracies in final performance. Our work presents a novel exploration of LLMs' symbolic calculation abilities and the underlying mechanisms",
    "checked": true,
    "id": "504f3713a74c66d61a46fe658988861302eb088d",
    "semantic_title": "states hidden in hidden states: llms emerge discrete state representations implicitly",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BSWRClgew8": {
    "title": "Can Tool-augmented Large Language Models be Aware of Incomplete Conditions?",
    "volume": "review",
    "abstract": "Recent advancements in integrating large language models (LLMs) with tools have allowed the models to interact with real-world environments. However, these tool-augmented LLMs often encounter incomplete scenarios when users provide partial information or the necessary tools are unavailable. Recognizing and managing such scenarios is crucial for LLMs to ensure their reliability, but this exploration remains understudied. This study examines whether LLMs can identify incomplete conditions and appropriately determine when to refrain from using tools. To this end, we address a dataset by manipulating instances from two datasets by removing necessary tools or essential information for tool invocation. We confirm that most LLMs are challenged to identify the additional information required to utilize specific tools and the absence of appropriate tools. Our research can contribute to advancing reliable LLMs by addressing scenarios that commonly arise during interactions between humans and LLMs. Our code and dataset will be made publicly available",
    "checked": true,
    "id": "7b31f2d677677afb401773f2660581c1ec5c650d",
    "semantic_title": "can tool-augmented large language models be aware of incomplete conditions?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cdLL7qJN0T": {
    "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
    "volume": "review",
    "abstract": "Pretrained language models memorize vast amounts of information, including private and copyrighted data, raising significant safety concerns. Retraining these models after excluding sensitive data is prohibitively expensive, making machine unlearning a viable, cost-effective alternative. Previous research has focused on machine unlearning for monolingual models, but we find that unlearning in one language does not necessarily transfer to others. This vulnerability makes models susceptible to low-resource language attacks, where sensitive information remains accessible in less dominant languages. This paper presents a pioneering approach to machine unlearning for multilingual language models, selectively erasing information across different languages while maintaining overall performance. Specifically, our method employs an adaptive unlearning scheme that assigns language-dependent weights to address different language performances of multilingual language models. Empirical results demonstrate the effectiveness of our framework compared to existing unlearning baselines, setting a new standard for secure and adaptable multilingual language models",
    "checked": true,
    "id": "04da681a5cd508826c134f0a65c5c12acd0edd46",
    "semantic_title": "cross-lingual unlearning of selective knowledge in multilingual language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BCC3EfGzyw": {
    "title": "Bridging Law and Data: Augmenting Reasoning via a Semi-Structured Dataset with IRAC methodology",
    "volume": "review",
    "abstract": "The effectiveness of Large Language Models (LLMs) in legal reasoning is often limited due to the unique legal terminologies and the necessity for highly specialized knowledge. These limitations highlight the need for high-quality data tailored for complex legal reasoning tasks. This paper introduces LEGALSEMI, a benchmark specifically curated for legal scenario analysis. LEGALSEMI comprises 54 legal scenarios, each rigorously annotated by legal experts, based on the comprehensive IRAC (Issue, Rule, Application, Conclusion) framework. In addition, LEGALSEMI is accompanied by a structured knowledge graph (SKG). A series of experiments were conducted to assess the usefulness of LEGALSEMI for IRAC analysis. The experimental results demonstrate the effectiveness of incorporating the SKG for issue identification, rule retrieval, application and conclusion generation using four different LLMs. LEGALSEMI will be publicly available upon acceptance of this paper",
    "checked": true,
    "id": "1c6c7162e8a87b0b5814beec36911bb998cbc89f",
    "semantic_title": "bridging law and data: augmenting reasoning via a semi-structured dataset with irac methodology",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m56BGyFBB2": {
    "title": "StorySpark: Expert-Annotated QA Pairs with Real-World Knowledge for Children Storytelling",
    "volume": "review",
    "abstract": "Interactive storytelling between parents and children is a common activity in the real world, in which parents expect to teach children both language skills and real-world knowledge beyond the story narratives. While increasing AI-assisted storytelling systems have been developed and used in children's story-based interaction and learning scenarios, existing systems often fall short of generating real-world knowledge infused conversation to meet parents' practical expectation of interactive storytelling, with the foremost reason of existing question-answering (QA) datasets these systems build on focusing mainly on the knowledge answerable within the story content. To bridge this gap, we designed an annotation framework empowered by real-world knowledge graph to facilitate experts' annotations while collecting their mental procedures. Further, we leveraged this annotation framework to build StorySpark, a dataset of 5,868 expert-annotated QA pairs with real-world knowledge beyond story context. A comprehensive benchmarking experiment, including both automated and human expert evaluation within various QA pair generation (QAG) settings, demonstrates the usability of our StorySpark on the story-based knowledgeable QAG task. Worth mentioning that a traditional compact model fine-tuned on StorySpark can reliably outperform robust LLMs. This further highlights the complexity of such real-world tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FhBWQJGlDW": {
    "title": "UBENCH: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions",
    "volume": "review",
    "abstract": "The rapid development of large language models (LLMs) has shown promising practical results. However, their low interpretability often leads to errors in unforeseen circumstances, limiting their utility. Many works have focused on creating comprehensive evaluation systems, but previous benchmarks have primarily assessed problem-solving abilities while neglecting the response's uncertainty, which may result in unreliability. Recent methods for measuring LLM reliability are resource-intensive and unable to test black-box models. To address this, we propose UBENCH, a comprehensive benchmark for evaluating LLM reliability. UBENCH includes 3,978 multiple-choice questions covering knowledge, language, understanding, and reasoning abilities. Experimental results show that UBENCH has achieved state-of-the-art performance, while its single-sampling method significantly saves computational resources compared to baseline methods that require multiple samplings. Additionally, based on UBENCH, we evaluate the reliability of 15 popular LLMs, finding GLM4 to be the most outstanding, closely followed by GPT-4. We also explore the impact of Chain-of-Thought prompts, role-playing prompts, option order, and temperature on LLM reliability, analyzing the varying effects on different LLMs. Our implementation available at https://anonymous.4open.science/r/UBench",
    "checked": true,
    "id": "c14963a444405a9caca3c3c3aa5b16d1f5e1fb54",
    "semantic_title": "ubench: benchmarking uncertainty in large language models with multiple choice questions",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=QKsn2QE1Sw": {
    "title": "LeanQuant: Accurate Large Language Model Quantization with Loss-Error-Aware Grid",
    "volume": "review",
    "abstract": "Large language models (LLMs) have numerous applications across various domains, but their high computational and memory demands pose significant deployment challenges. Weight quantization is an effective technique for reducing the decoding latency and memory requirements of LLMs. Existing approaches primarily aim to maintain the quality of quantized models by preserving outliers in input features, but they still suffer significant quality loss at lower bit widths. Our approach builds on Optimal Brain Quantization (OBQ), an iterative weight-update-based quantization framework. We identify a key limitation of OBQ, specifically that its uniform quantization grid is suboptimal for maintaining model quality, as it introduces large errors to the task loss. To address this, we propose LeanQuant, which learns a loss-error-aware quantization grid by leveraging the inverse diagonal Hessian. Extensive experimental results demonstrate that our method compares favorably against competitive baselines and effectively accelerates LLM inference",
    "checked": true,
    "id": "9c580c162f028c20ad0516c220bbca32d9f8edf9",
    "semantic_title": "leanquant: accurate large language model quantization with loss-error-aware grid",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0N9nj2a0MH": {
    "title": "Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations",
    "volume": "review",
    "abstract": "Despite recent advancements in Large Language Models (LLMs), their performance on tasks involving long contexts remains sub-optimal. In-Context Learning (ICL) with few-shot examples may be an appealing solution to enhance LLM performance in this scenario too. However, na\\\"ively adding ICL examples with long context faces challenges, including domain shifts between demonstrations and the target query and substantial token overhead added for each example. In this work, we propose to automatically generate few-shot examples for long context QA tasks by recycling contexts. Specifically, given a long input context (1-3k tokens) and a query, we generate additional query-output pairs from the given context as few-shot examples. This ensures that the demonstrations come from the same domain as the target query and only add a small number of tokens to the prompt. Furthermore, we enhance each demonstration example by instructing the model to \\textit{explicitly} identify the relevant paragraphs before the answer. This approach acts as a structured Chain of Thought and provides fine-grained attribution to the answer. We apply our method on multiple models and obtain a substantial improvement on various QA datasets with long context, especially when the answer lies within the middle of the text. Surprisingly, despite introducing only single-passage ICL examples, LLMs successfully generalize to multi-hop long-context QA using our approach",
    "checked": true,
    "id": "48a00907faea2ccb56ee45495aaef8953ccf6535",
    "semantic_title": "can few-shot work in long-context? recycling the context to generate demonstrations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=55pXzoTgFS": {
    "title": "Towards Truthful Multilingual Large Language Models: Benchmarking and Alignment Strategies",
    "volume": "review",
    "abstract": "In the era of large language models (LLMs), building multilingual large language models (MLLMs) that can serve users worldwide holds great significance. However, existing research seldom focuses on the truthfulness of MLLMs. Meanwhile, contemporary multilingual aligning technologies struggle to balance massive languages and often exhibit serious truthfulness gaps across different languages, especially those that differ greatly from English. In our work, we construct a benchmark for truthfulness evaluation in multilingual scenarios and explore the ways to align facts across languages to enhance the truthfulness of MLLMs. Furthermore, we propose *Fact-aware Multilingual Selective Synergy* (FaMSS) to optimize the data allocation across a large number of languages and different data types. Experimental results demonstrate that our approach can effectively reduce the multilingual representation disparity and enhance the multilingual capabilities of LLMs",
    "checked": true,
    "id": "90f780915df90786428f9a76db3eb61b99177b35",
    "semantic_title": "towards truthful multilingual large language models: benchmarking and alignment strategies",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vYXgey0drZ": {
    "title": "Match, Compare, or Select? An Investigation of Large Language Models for Entity Matching",
    "volume": "review",
    "abstract": "Entity matching (EM) is a critical step in entity resolution. Recently, entity matching based on large language models (LLMs) has shown great promise. However, current LLM-based entity matching approaches typically follow a binary matching paradigm that ignores the global consistency between record relationships. In this paper, we investigate various methodologies for LLM-based entity matching that incorporate record interactions from different perspectives. Specifically, we comprehensively compare three representative strategies: matching, comparing, and selecting, and analyze their respective advantages and challenges in diverse scenarios. Based on our findings, we further design a compound entity matching framework (ComEM) that leverages the composition of multiple strategies and LLMs. ComEM benefits from the advantages of different sides and achieves improvements in both effectiveness and efficiency. Experimental results verify that ComEM not only achieves significant performance gains on various datasets, but also reduces the cost of LLM-based entity matching for practical applications",
    "checked": true,
    "id": "3042f56baf64b06ba5f236f8237ec10174ba6ae2",
    "semantic_title": "match, compare, or select? an investigation of large language models for entity matching",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Umrjf5EVkl": {
    "title": "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages",
    "volume": "review",
    "abstract": "Neural dependency parsing has achieved remarkable performance for low resource morphologically rich languages. It has also been well-studied that morphologically rich languages exhibit relatively free word order. This prompts a fundamental investigation: *Is there a way to enhance dependency parsing performance, making the model robust to word order variations utilizing the relatively free word order nature of morphologically rich languages?* In this work, we examine the robustness of graph-based parsing architectures on 7 relatively free word order languages. We focus on scrutinizing essential modifications such as data augmentation and the removal of position encoding required to adapt these architectures accordingly. To this end, we propose a contrastive self-supervised learning method to make the model robust to word order variations. Furthermore, our proposed modification demonstrates a substantial average gain of 3.03/2.95 points in 7 relatively free word order languages, as measured by the UAS/LAS Score metric when compared to the best performing baseline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PYCkzS8WJr": {
    "title": "Leveraging Multimodal Fusion for Advanced Fake News Detection",
    "volume": "review",
    "abstract": "Detecting multimodal fake news is imperative for maintaining social media security and safeguarding community well-being. Existing detection approaches often fall short in adequately considering the nuanced context of social media and fail to fully utilize various modalities such as metadata, resulting in a significant gap. In this paper, we propose a novel and efficient model that integrates both textual global features and local features. This model captures semantic relationships within the text and utilizes a global corpus representation to align with the complex context of social media. We further enhance feature connectivity by employing a multilevel fusion technique that integrates visual and metadata information. Extensive experiments demonstrate that our method achieves state-of-the-art performance across all classification tasks using Fakeddit, the largest multimodal fake news dataset, underscoring its effectiveness",
    "checked": false,
    "id": "2efc32cb1ba8a49dee41e4f2f4ca6850ff57a7f7",
    "semantic_title": "a mutual attention based multimodal fusion for fake news detection on social network",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=xyswzRGYu1": {
    "title": "Towards Tool Use Alignment of Large Language Models",
    "volume": "review",
    "abstract": "Recently, tool use with LLMs has become one of the primary research topics as it can help LLM generate truthful and helpful responses. Existing studies on tool use with LLMs primarily focus on enhancing the tool-calling ability of LLMs. In practice, like chat assistants, LLMs are also required to align with human values in the context of tool use. Specifically, LLMs should refuse to answer unsafe tool use relevant instructions and insecure tool responses to ensure their reliability and harmlessness. At the same time, LLMs should demonstrate autonomy in tool use to reduce the costs associated with tool calling. To tackle this issue, we first introduce the principle that LLMs should follow in tool use scenarios: H2A. The goal of H2A is to align LLMs with $\\textbf{helpfulness}$, $\\textbf{harmlessness}$, and $\\textbf{autonomy}$. In addition, we propose ToolAlign, a dataset comprising instruction-tuning data and preference data to align LLMs with the H2A principle for tool use. Based on ToolAlign, we develop LLMs by supervised fine-tuning and preference learning, and experimental results demonstrate that the LLMs exhibit remarkable tool-calling capabilities, while also refusing to engage with harmful content, and displaying a high degree of autonomy in tool utilization. The code and datasets are avaliable at: https://anonymous.4open.science/r/ToolAlign",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=crEmQ7Gfuy": {
    "title": "Turn Waste into Worth: Rectifying Top-$k$ Router of MoE",
    "volume": "review",
    "abstract": "Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are empty, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectification effectively handle dropped tokens and padding, respectively. Furthermore, the combination of them achieves superior performance, surpassing the accuracy of the vanilla top-1 router by 4.7\\%",
    "checked": false,
    "id": "656f4a76bcbc1a3a032ef5cf284909ef1bb58156",
    "semantic_title": "turn waste into worth: rectifying top-k router of moe",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=SH2sMWCLSe": {
    "title": "Thoughts to Target: Enhance Planning for Target-driven Conversation",
    "volume": "review",
    "abstract": "In conversational AI, large-scale models excel in various tasks but struggle with target-driven conversation planning. Current methods, such as chain-of-thought reasoning and tree-search policy learning techniques, either neglect plan rationality or require extensive human simulation procedures. Addressing this, we propose a novel two-stage framework, named EnPL, to improve the LLMs' capability in planning conversations towards designated targets, including (1) distilling natural language plans from target-driven conversation corpus and (2) generating new plans with demonstration-guided in-context learning. Specifically, we first propose a filter approach to distill a high-quality plan dataset, ConvPlan(Resources of this paper can be found at https://anonymous.4open.science/r/ConvPlan-2023). With the aid of corresponding conversational data and support from relevant knowledge bases, we validate the quality and rationality of these plans. Then, these plans are leveraged to help guide LLMs to further plan for new targets. Empirical results demonstrate that our method significantly improves the planning ability of LLMs, especially in target-driven conversations. Furthermore, EnPL is demonstrated to be quite effective in creating large-scale target-driven conversation datasets, paving the way for constructing extensive target-driven conversational models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xEYAiRmKMc": {
    "title": "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues",
    "volume": "review",
    "abstract": "We develop assistive agents based on Large Language Models (LLMs) that aid interlocutors in business negotiations. Specifically, we simulate business negotiations by letting two LLM-based agents engage in role play. A third LLM acts as a remediator agent to rewrite utterances violating norms for improving negotiation outcomes. We introduce a simple tuning-free and label-free In-Context Learning (ICL) method to identify high-quality ICL examples for the remediator, where we propose a novel select criteria, called \\textit{value impact}, to measure the quality of the negotiation outcomes. We provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different negotiation topics. The source code and the generated dataset will be publicly available upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zp1BNUJnMM": {
    "title": "MuPT: A Generative Symbolic Music Pretrained Transformer",
    "volume": "review",
    "abstract": "In this paper, we explore the application of Large Language Models (LLMs) to the pre-training of music. While the prevalent use of MIDI in music modeling is well-established, our findings suggest that LLMs are inherently more compatible with ABC Notation, which aligns more closely with their design and strengths, thereby enhancing the model's performance in musical composition. To address the challenges associated with misaligned measures from different tracks during generation, we propose the development of a $\\underline{S}$ynchronized $\\underline{M}$ulti-$\\underline{T}$rack ABC Notation ($\\textbf{SMT-ABC Notation}$), which aims to preserve coherence across multiple musical tracks. Our contributions include a series of models capable of handling up to 8192 tokens, covering 90% of the symbolic music data in our training set. Furthermore, we explore the implications of the $\\underline{S}$ymbolic $\\underline{M}$usic $\\underline{S}$caling Law ($\\textbf{SMS Law}$) on model performance. The results indicate a promising direction for future research in music generation, offering extensive resources for community-led research through our open-source contributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IUbIseO8rW": {
    "title": "OfficeBench: Benchmarking Language Agents across Multiple Applications for Office Automation",
    "volume": "review",
    "abstract": "Office automation significantly enhances human productivity by automatically finishing routine tasks in the workflow. Beyond the basic information extraction studied in much of the prior document AI literature, the office automation research should be extended to more realistic office tasks which require to integrate various information sources in the office system and produce outputs through a series of decision-making processes. We introduce OfficeBench, one of the first office automation benchmarks for evaluating current LLM agents' capability to address the office tasks in realistic office workflows. OfficeBench requires LLM agents to perform feasible long-horizon planning, proficiently switch between applications in a timely manner, and accurately ground their actions within a large combined action space, based on the contextual demands of the workflow. Applying our customized evaluation methods on each task, we find that GPT-4 Omni achieves the highest pass rate of 47.00%, demonstrating a decent performance in handling office tasks. However, this is still far below the human performance and accuracy standards required by real-world office workflows. We further observe that most issues are related to operation redundancy and hallucinations, as well as limitations in switching between multiple applications, which may provide valuable insights for developing effective agent frameworks for office automation. Code and data will be released upon acceptance",
    "checked": true,
    "id": "09710b95adf178e6db154d1a54212d4b111b7a6a",
    "semantic_title": "officebench: benchmarking language agents across multiple applications for office automation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AbuBYZw0Tk": {
    "title": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments",
    "volume": "review",
    "abstract": "The rapid obsolescence of information in Large Language Models (LLMs) has driven the development of various techniques to incorporate new facts. However, existing methods for knowledge editing still face difficulties with multi-hop questions that require accurate fact identification and sequential logical reasoning, particularly among numerous fact updates. To tackle these challenges, this paper introduces Graph Memory-based Editing for Large Language Models (GMeLLo), a straitforward and effective method that merges the explicit knowledge representation of Knowledge Graphs (KGs) with the linguistic flexibility of LLMs. Beyond merely leveraging LLMs for question answering, GMeLLo employs these models to convert free-form language into structured queries and fact triples, facilitating seamless interaction with KGs for rapid updates and precise multi-hop reasoning. Our results show that GMeLLo significantly surpasses current state-of-the-art knowledge editing methods in the multi-hop question answering benchmark, MQuAKE, especially in scenarios with extensive knowledge edits",
    "checked": false,
    "id": "4308208fac24626e0c927ee728038aadc4e87266",
    "semantic_title": "hipporag: neurobiologically inspired long-term memory for large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ByUlikMupd": {
    "title": "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets",
    "volume": "review",
    "abstract": "Large language models (LLMs) can now generate and recognize text in a wide range of styles and genres, including highly specialized, creative genres like poetry. Poetry is a lightning rod for the marketing and popular imagination of LLM capabilities because it is a signifier of human creativity and complexity, as well as a popular and culturally significant art form. But what do LLMs really know about poetry? What can they know about poetry? We develop a task to evaluate how well LLMs recognize one aspect of English-language poetry, poetic form, which captures many different poetic features, including rhyme scheme, meter, and word or line repetition. We use this task to reflect on LLMs' current poetic capabilities, as well as the challenges and pitfalls of creating NLP benchmarks for poetry and for other creative tasks. In particular, we use this task to audit and reflect on the poems included in popular pretraining datasets. Our findings have implications for NLP researchers interested in model evaluation, digital humanities and cultural analytics research, and cultural heritage collections",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CbjKOrrQxO": {
    "title": "Tools Fail: Detecting Silent Errors in Faulty Tools",
    "volume": "review",
    "abstract": "Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not in their weights, to perform tasks on the web, and even to control robots. However, most ontologies and surveys of tool-use have assumed the core challenge for LLMs is choosing the tool. Instead, we introduce a framework for tools more broadly which guides us to explore a model's ability to detect \"silent\" tool errors, and reflect on how to plan. This more directly aligns with the increasingly popular use of models as tools. We provide an initial approach to failure recovery with promising results both on a controlled calculator setting and embodied agent planning",
    "checked": true,
    "id": "491e129c10bca2f9e50c9c4859b3cb825d217d37",
    "semantic_title": "tools fail: detecting silent errors in faulty tools",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hi5jqzoqpI": {
    "title": "Enhancing Controllable Generation with Improved Control Module Representations",
    "volume": "review",
    "abstract": "Controllable generation (CG) has been widely used in large language models (LLMs) for a wide range of language tasks, such as multi-task learning and human preference alignment. For example, prompt-based CG uses curated prompts as inputs (such as system prompts) to control LLMs behaviors. Finetuning-based CG is widely adopted when training data is available; it trains control modules and controls LLM behaviors by plugging these modules into LLMs (e.g., trainable prompts or LoRA weights). Finetuning-based CG can freeze LLMs and only train control modules for efficiency or train LLMs together with control models for effectiveness. We argue that fine-tuning control modules together with LLMs directly is not the optimal optimization strategy since their representations are often initialized irrelevantly to LLMs' representations, which adds more difficulty for optimization. A better optimization should first align control modules with the LLM's representation space and then optimize them together. To this end, we propose a simple yet effective Two-step Freezing-then-Tuning framework (TFT) to achieve better optimization results for finetuning-based CG. Concretely, we first freeze LLMs and only optimize control modules to align their representations with LLMs, and then optimize control modules together with LLMs to ensure performance. Experiment results on two popular human preference alignment datasets and one multi-task learning dataset show that our approach significantly improves the controllable generation qualities compared with one-step optimization widely used in related works, and achieves better or on-par performance compared with other kinds of baselines, such as direct preference optimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FxYoajVJnW": {
    "title": "KeyInst: Keyword Instruction for Improving SQL Formulation in Text-to-SQL",
    "volume": "review",
    "abstract": "Text-to-SQL parsing involves the translation of natural language queries (NLQs) into their corresponding SQL commands. A principal challenge within this domain is the formulation of SQL queries that are not only syntactically correct but also semantically aligned with the natural language input. However, the intrinsic disparity between the NLQ and the SQL poses a significant challenge. In this research, we introduce Keyword Instruction (KeyInst), a novel method designed to enhance SQL formulation by Large Language Models (LLMs). KeyInst essentially provides guidance on pivotal SQL keywords likely to be part of the final query, thus facilitates a smoother SQL query formulation process. We explore two strategies for integrating KeyInst into Text-to-SQL parsing: a pipeline strategy and a single-pass strategy. The former first generates KeyInst for question, which are then used to prompt LLMs. The latter employs a fine-tuned model to concurrently generate KeyInst and SQL in one step. We developed StrucQL, a benchmark specifically designed for the evaluation of SQL formulation. Extensive experiments on StrucQL and other benchmarks demonstrate that KeyInst significantly improves upon the existing Text-to-SQL prompting techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=si4RqKfaBS": {
    "title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models",
    "volume": "review",
    "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the quality of this data is challenging due to its sheer volume and the absence of sample-level quality annotations and enhancements. In this paper, we introduce DecorateLM, a data engineering method designed to refine the pretraining corpus through data rating, tagging and editing. Specifically, DecorateLM rates texts against quality criteria, tags texts with hierarchical labels, and edits texts into a more formalized format. Due to the massive size of the pretraining corpus, adopting an LLM for decorating the entire corpus is less efficient. Therefore, to balance performance with efficiency, we curate a meticulously annotated training corpus for DecorateLM using a large language model and distill data engineering expertise into a compact 1.2 billion parameter small language model (SLM). We then apply DecorateLM to enhance 100B billion tokens of the training corpus, selecting 45 billion tokens that exemplify high quality and diversity for the further training of another 1.2 billion parameter LLM. Our results demonstrate that employing such high-quality data can significantly boost model performance, showcasing a powerful approach to enhance the quality of the pretraining corpus",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HzYw72kdn4": {
    "title": "On Demonstration Selection for Language Model Fairness in Decision-Making",
    "volume": "review",
    "abstract": "Recently, there has been a surge in deploying Large Language Models (LLMs) for decision-making tasks, such as income prediction and crime risk assessments. Due to bias in the pre-training data, LLMs generally present unfairness and discrimination against underprivileged groups. However, traditional fairness enhancement methods are generally impractical for LLMs due to the computational cost of fine-tuning and the black-box nature of powerful LLMs. To deal with this, In-Context Learning (ICL) offers a promising strategy for enhancing LLM fairness through input-output pairs, without the need for extensive retraining. Nevertheless, the efficacy of ICL is hindered by the inherent bias in both data and the LLM itself, leading to the potential exaggeration of existing societal disparities. In this study, we investigate the unfairness problem in LLMs and propose a novel demonstration selection strategy to address data and model biases when applying ICL. Extensive experiments on various tasks and datasets validate the superiority of our strategy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P8URqRlQD0": {
    "title": "AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories",
    "volume": "review",
    "abstract": "Fine-tuning on agent-environment interaction trajectory data holds significant promise for surfacing generalized agent capabilities in open-source large language models (LLMs). In this work, we introduce AgentBank, by far the largest trajectory tuning data collection featuring more than 50k diverse high-quality interaction trajectories which comprises 16 tasks covering five distinct agent skill dimensions. Leveraging a novel annotation pipeline, we are able to scale the annotated trajectories and generate a trajectory dataset with minimized difficulty bias. Furthermore, we fine-tune LLMs on AgentBank to get a series of agent models, Samoyed. Our comparative experiments demonstrate the effectiveness of scaling the interaction trajectory data to acquire generalized agent capabilities. Additional studies also reveal some key observations regarding trajectory tuning and agent skill generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ol4QiyoFbq": {
    "title": "Iterative or Innovative? A Problem-Oriented Perspective for Code Optimization",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated strong capabilities in solving a wide range of programming tasks. However, LLMs have rarely been explored for code optimization. In this paper, we explore code optimization with a focus on performance enhancement, specifically aiming to optimize code for minimal execution time. The recently proposed first PIE dataset for performance optimization constructs program optimization pairs based on iterative submissions from the same programmer for the same problem. However, this approach restricts LLMs to local performance improvements, neglecting global algorithmic innovation. Therefore, we adopt a completely different perspective by reconstructing the optimization pairs into a problem-oriented approach. This allows for the integration of various ingenious ideas from different programmers tackling the same problem. Experimental results demonstrate that adapting LLMs to problem-oriented optimization pairs significantly enhances their optimization capabilities. Meanwhile, we identified performance bottlenecks within the problem-oriented perspective. By employing model merge, we further overcame bottlenecks and ultimately elevated the program optimization ratio (51.76\\% $\\rightarrow$ 76.65\\%) and speedup ($2.65\\times\\rightarrow5.09\\times$) to new levels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JYjhTl4LSK": {
    "title": "Rethinking Jailbreaking through the Lens of Representation Engineering",
    "volume": "review",
    "abstract": "The recent surge in jailbreaking methods has revealed the vulnerability of Large Language Models (LLMs) to malicious inputs. While earlier research has primarily concentrated on increasing the success rates of jailbreaking attacks, the underlying mechanism for safeguarding LLMs remains underexplored. This study investigates the vulnerability of safety-aligned LLMs by uncovering specific activity patterns within the representation space generated by LLMs. We propose a novel approach to identify such \"safety patterns'\" using only a few pairs of contrastive queries. Surprisingly, these safety patterns function as \"keys'' (used as a metaphor for security defense capability) that can be used to open or lock Pandora's Box of LLMs. Extensive experiments demonstrate that the robustness of LLMs against jailbreaking can be lessened or augmented by attenuating or strengthening the identified safety patterns. These findings deepen our understanding of jailbreaking phenomena and call for the LLM community to address the potential misuse of open-source LLMs",
    "checked": true,
    "id": "90538881baa3ff6461ab6edc756832337219f9b4",
    "semantic_title": "rethinking jailbreaking through the lens of representation engineering",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=iIWujvj8Kg": {
    "title": "Is long context helpful for dialog response generation?",
    "volume": "review",
    "abstract": "Personalization has been a key challenge in building engaging conversational agents, necessitating models to effectively utilize long-range context to maintain coherence and consistency over extended interactions. In this work, we investigate the potential of large language models (LLMs) to generate coherent and personalized responses in long-term human-human conversations. We experiment with $\\textit{fixed context}$ and $\\textit{retrieval-based}$ approaches to use the dialogue history between two speakers. We evaluate our methods and perform analysis on four long-term conversational datasets. Our results indicate that including only a few preceding utterances is generally sufficient for response generation. Retrieval or more extended contexts from past dialogues provide minimal benefits for personalizing model responses. Further analysis of instances that benefited most from retrieval reveals that these cases typically involve either explicit references to previously shared information or scenarios requiring stylistic consistency, such as farewell messages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CrF1t2Qqyi": {
    "title": "Mitigating Hallucination in Fictional Character Role-Play",
    "volume": "review",
    "abstract": "Role-playing has wide-ranging applications in customer support, embodied agents, computational social science, etc. The influence of parametric world knowledge of large language models (LLMs) often causes role-playing characters to act out of character and hallucinate about things outside the scope of their knowledge. In this work, we focus on the evaluation and mitigation of hallucination in fictional character role-play. We introduce a dataset with more than 2,000 characters and 72,000 interviews, including 18,000 adversarial questions. We propose RoleFact, a role-playing method that mitigates hallucination by modulating the influence of parametric knowledge using a pre-calibrated confidence threshold. Experiments show that the proposed method improves the factual precision of generated responses by 18% for adversarial questions with a 44% reduction in temporal hallucination for time-sensitive interviews. We will make the dataset and code publicly available for the research community upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z9Ux2V9lWR": {
    "title": "Skip-Layer Attention: Bridging Abstract and Detailed Dependencies in Transformers",
    "volume": "review",
    "abstract": "The Transformer architecture has significantly advanced deep learning, particularly in natural language processing, by effectively managing long-range dependencies. However, as the demand for understanding complex relationships grows, refining the Transformer's architecture becomes critical. This paper introduces Skip-Layer Attention (SLA) to enhance Transformer models by enabling direct attention between non-adjacent layers. This method improves the model's ability to capture dependencies between high-level abstract features and low-level details. By facilitating direct attention between these diverse feature levels, our approach overcomes the limitations of current Transformers, which often rely on suboptimal intra-layer attention. Our implementation extends the Transformer's functionality by enabling queries in a given layer to interact with keys and values from both the current layer and one preceding layer, thus enhancing the diversity of multi-head attention without additional computational burden. Extensive experiments demonstrate that our enhanced Transformer model achieves superior performance in language modeling tasks, highlighting the effectiveness of our skip-layer attention mechanism",
    "checked": true,
    "id": "937a377d5ac0f6a835b843615c5e94e71a34176e",
    "semantic_title": "skip-layer attention: bridging abstract and detailed dependencies in transformers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wg0PkjMWkx": {
    "title": "InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery",
    "volume": "review",
    "abstract": "The rapid evolution of artificial intelligence in drug discovery encounters challenges with generalization and extensive training, yet Large Language Models (LLMs) offer promise in reshaping interactions with complex molecular data. Our novel contribution, InstructMol, a multi-modal LLM, effectively aligns molecular structures with natural language via an instruction-tuning approach, utilizing a two-stage training strategy that adeptly combines limited domain-specific data with molecular and textual information. InstructMol showcases substantial performance improvements in drug discovery-related molecular tasks, surpassing leading LLMs and significantly reducing the gap with specialists, thereby establishing a robust foundation for a versatile and dependable drug discovery assistant",
    "checked": true,
    "id": "2b3554a8fea6f123fc04bd3e120f2293f227e1b2",
    "semantic_title": "instructmol: multi-modal integration for building a versatile and reliable molecular assistant in drug discovery",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=eqdRlangte": {
    "title": "Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning",
    "volume": "review",
    "abstract": "Natural language is a powerful complementary modality of communication for data visualizations, such as bar and line charts. To facilitate chart-based reasoning using natural language, various downstream tasks have been introduced recently such as chart question answering, chart summarization, and fact-checking with charts. These tasks pose a unique challenge, demanding both vision-language reasoning and a nuanced understanding of chart data tables, visual encodings, and natural language instructions. Despite the recent success of Large Language Models (LLMs) across diverse NLP tasks, their abilities and limitations in the realm of data visualization remain under-explored, possibly due to their lack of multi-modal capabilities. To bridge the gap, this paper presents one of the first comprehensive evaluations of the recently developed large vision language models (LVLMs) for chart understanding and reasoning tasks. Our evaluation includes a comprehensive assessment of both closed and open-sourced LVLMs across five major chart reasoning tasks. Furthermore, we perform a qualitative evaluation of LVLMs' performance on a diverse range of charts, aiming to provide a thorough analysis. Our findings reveal that while LVLMs demonstrate impressive abilities in generating fluent texts covering high-level data insights, they also encounter common problems like hallucinations, factual errors, and data bias. We highlight the key strengths and limitations of LVLMs in chart comprehension tasks, offering insights for future research\\footnote{We will make all our prompts as well as LVLMs' responses open source for future research",
    "checked": false,
    "id": "97ec0d508265f43d3d011a09ffbf599df9ea2b0d",
    "semantic_title": "are large vision language models up to the challenge of chart comprehension and reasoning? an extensive investigation into the capabilities and limitations of lvlms",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=WTCDeps5ym": {
    "title": "Automatic Detection of Parental Interference Behaviors during Bilingual Child Language Assessment",
    "volume": "review",
    "abstract": "Recent clinical research has developed novel protocols that enable children to participate in bilingual language assessment remotely with parents to assist in this process. However, since parents are not trained clinicians, they often perform interference behaviors---actions that could compromise the validity of the assessment (e.g., providing hints). In this paper, we study whether language models can help automate the detection and categorization of parental interference behaviors during bilingual English-Mandarin child language assessment. Such a system would reduce the burden on clinicians, who must otherwise rely on transcribing video recordings and checking them manually for signs of interference. We release a new, expert-annotated dataset for this task, and evaluate multiple state-of-the-art large language models. While these models achieve non-trivial accuracy, they currently lag far behind human annotators. We find that understanding Mandarin and code-mixed text are key challenges these models need to overcome. We hope that our new dataset inspires modeling advances that could improve the practice of bilingual child language assessment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X6kcA8rIKX": {
    "title": "I Never Said That\": A dataset, taxonomy and baselines on response clarity classification",
    "volume": "review",
    "abstract": "Equivocation and ambiguity in public speech are well-studied discourse phenomena, especially in political science and analysis of political interviews. Inspired by the well-grounded theory on equivocation, we aim to resolve the closely related problem of response clarity in questions extracted from political interviews, leveraging the capabilities of Large Language Models (LLMs) and human expertise. To this end, we introduce a novel taxonomy that frames the task of detecting and classifying response clarity and a corresponding clarity classification dataset which consists of question-answer (QA) pairs drawn from political interviews and annotated accordingly. Our proposed two-level taxonomy addresses the clarity of a response in terms of the information provided for a given question (high-level) and also provides a fine-grained taxonomy of evasion techniques that relate to unclear, ambiguous responses (lower-level). We combine ChatGPT and human annotators to collect, validate and annotate discrete QA pairs from political interviews, to be used for our newly introduced response clarity task. We provide a detailed analysis and conduct several experiments with different model architectures, sizes and adaptation methods to gain insights and establish new baselines over the proposed dataset and task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HgJy3CLtRe": {
    "title": "Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment",
    "volume": "review",
    "abstract": "As the capabilities of large language models (LLMs) have expanded dramatically, aligning these models with human values presents a significant challenge. Traditional alignment strategies rely heavily on human intervention, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), or on the self-alignment capacities of LLMs, which usually require a strong LLM's emergent ability to improve its original bad answer. To address these challenges, we propose a novel self-alignment method that utilizes a Chain of Thought (CoT) approach, termed AlignCoT. This method encompasses stages of Question Analysis, Answer Guidance, and Safe Answer production. It is designed to enable LLMs to generate high-quality, safe responses throughout various stages of their development. Furthermore, we introduce the Mixture of insighTful Experts (MoTE) architecture, which applies mixture of experts to enhance each component of the AlignCoT process, markedly increasing alignment efficiency. The MoTE approach not only outperforms existing methods in aligning LLMs with human values but also highlights the benefits of using self-generated data, revealing the dual benefits of improved alignment and training efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OVGk9vvWf1": {
    "title": "MatchTime: Towards Automatic Soccer Game Commentary Generation",
    "volume": "review",
    "abstract": "Soccer is a globally popular sport with a vast audience, in this paper, we consider to construct an automatic soccer game commentary model to improve the audiences' viewing experience. In general, we make the following contributions: *First*, observing the prevalent video-text misalignment in existing datasets, we manually annotate timestamps for 49 matches, establishing a more robust benchmark for soccer game commentary generation, termed as ***SN-Caption-test-align***; *Second*, we propose a multimodal temporal alignment pipeline to automatically correct and filter the existing dataset at scale, creating a higher-quality soccer game commentary dataset for training, denoted as ***MatchTime***; *Third*, based on our curated dataset, we train an automatic commentary generation model, named ***MatchVoice***. Extensive experiments and ablation studies have demonstrated the effectiveness of our alignment pipeline, and training model on the curated datasets achieves state-of-the-art performance for commentary generation, showcasing that better alignment can lead to significant performance improvements in downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aFhDuLzqc9": {
    "title": "XLLaMA2: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable translation capabilities in high-resource language tasks, yet their performance in low-resource languages is hindered by insufficient multilingual data during pre-training. To mitigate this, we continued pre-train LLaMA2-7B to support translation across more than 100 languages. Following a thorough analysis of training strategies, including vocabulary expansion and data augmentation, we apply extensive multilingual continued pre-training to the LLaMA series model, resulting in XLLaMA2. Without loss of the generality ability, the translation performance of XLLaMA2 significantly surpassed existing LLMs and is on par with that of a specialized translation model (M2M-100-12B) on the Flores-101 benchmark. Specifically, XLLaMA2 achieves an average spBLEU score improvement of over 10 points compared to the original LLaMA2 model. Further testing XLLaMA2 on Flores-200, XLLaMA2 exhibited notable performance gains even for languages not included in the training set. We will make the code and model publicly available",
    "checked": false,
    "id": "d3ad0931e5c6e0e7152485eb103a7301836b57cf",
    "semantic_title": "llamax: scaling linguistic horizons of llm by enhancing translation capabilities beyond 100 languages",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=aYcIUbchin": {
    "title": "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions",
    "volume": "review",
    "abstract": "Modern language models, while sophisticated, exhibit some inherent shortcomings, particularly in conversational settings. We claim that many of the observed shortcomings can be attributed to violation of one or more conversational principles. By drawing upon extensive research from both the social science and AI communities, we propose a set of maxims -- quantity, quality, relevance, manner, benevolence, and transparency -- for describing effective human-AI conversation. We first justify the applicability of the first four maxims (from Grice) in the context of human-AI interactions. We then argue that two new maxims, benevolence (concerning the generation of, and engagement with, harmful content) and transparency (concerning recognition of one's knowledge boundaries, operational constraints, and intents), are necessary for addressing behavior unique to modern human-AI interactions. We evaluate the degree to which various language models are able to understand these maxims and find that models possess an internal prioritization of principles that can significantly impact accurate interpretability of the maxims",
    "checked": true,
    "id": "5dcc26649414295ec3d1d9a274d41b2759e53f8e",
    "semantic_title": "language models in dialogue: conversational maxims for human-ai interactions",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=JGtxrFQ2pg": {
    "title": "Communicate to Play: Pragmatic Reasoning for Efficient Cross-Cultural Communication",
    "volume": "review",
    "abstract": "In this paper, we study how culture leads to differences in common ground and how this influences communication. During communication, cultural differences in common ground during communication may result in pragmatic failure and misunderstandings. We develop our method Rational Speech Acts for Cross-Cultural Communication (RSA+C3) to resolve cross-cultural differences in common ground. To measure the success of our method, we study RSA+C3 in the collaborative referential game of Codenames Duet and show that our method successfully improves collaboration between simulated players of different cultures. Our contributions are threefold: (1) creating Codenames players using contrastive learning of an embedding space and LLM prompting that are aligned with human patterns of play, (2) studying culturally induced differences in common ground reflected in our trained models, and (3) demonstrating that our method RSA+C3 can ease cross-cultural communication in gameplay by inferring sociocultural context from interaction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yaNXERza7h": {
    "title": "A Survey on Natural Language Counterfactual Generation",
    "volume": "review",
    "abstract": "Natural Language Counterfactual generation aims to minimally modify a given text such that the modified text belongs to a different class. The generated counterfactuals provide insights into the reasoning behind a model's predictions by highlighting which words significantly influence the outcomes. Additionally, they can be used to detect model fairness issues or augment the training data to enhance the model's robustness. A substantial amount of research has been conducted to generate counterfactuals for various NLP tasks, employing different models and methodologies. With the rapid growth of studies in this field, a systematic review is crucial to guide future researchers and developers. To bridge this gap, this survey comprehensively overview textual counterfactual generation methods, particularly including those based on Large Language Models. We propose a new taxonomy that categorizes the generation methods into four groups and systematically summarize the metrics for evaluating the generation quality. Finally, we discuss ongoing research challenges and outline promising directions for future work",
    "checked": true,
    "id": "8d0bcbbc3c3505e7e7d9399fac404c14b6a17cb1",
    "semantic_title": "a survey on natural language counterfactual generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sTxK9Ne4uY": {
    "title": "Amuro and Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models",
    "volume": "review",
    "abstract": "The development of large language models leads to the formation of a pre-train-then-align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream tasks. In this work, we investigate the relationship between pre-training and fine-tuning by fine-tuning multiple intermediate pre-trained model checkpoints, we find that i) continual pre-training improves the model in a latent way that unveils after fine-tuning; ii) with extra fine-tuning, the datasets that the model does not demonstrate capability gain much more than those that the model performs well during the pre-training stage; iii) although model benefits significantly through supervised fine-tuning, it may forget previously known domain knowledge and the tasks that are not seen during fine-tuning; iv) the supervised fine-tuned model resembles high sensitivity to few-shot evaluation prompts, but this sensitivity can be alleviated by more pre-training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NnCI2HLTAz": {
    "title": "Securing Multi-turn Conversational Language Models Against Distributed Backdoor Triggers",
    "volume": "review",
    "abstract": "The security of multi-turn conversational large language models (LLMs) is understudied despite it being one of the most popular LLM utilization. Specifically, LLMs are vulnerable to data poisoning backdoor attacks, where an adversary manipulates the training data to cause the model to output malicious responses to predefined triggers. Specific to the multi-turn dialogue setting, LLMs are at the risk of even more harmful and stealthy backdoor attacks where the backdoor triggers may span across multiple utterances, giving lee-way to context-driven attacks. In this paper, we explore a novel distributed backdoor trigger attack that serves to be an extra tool in an adversary's toolbox that can interface with other single-turn attack strategies in a plug and play manner. Results on two representative defense mechanisms indicate that distributed backdoor triggers are robust against existing defense strategies which are designed for single-turn user-model interactions, motivating us to propose a new defense strategy for the multi-turn dialogue setting that is more challenging. To this end, we also explore a novel contrastive decoding based defense that is able to mitigate the backdoor with a low computational tradeoff",
    "checked": true,
    "id": "a9e3d4d0ce275564387a5a0265dcc1f1d75ca341",
    "semantic_title": "securing multi-turn conversational language models against distributed backdoor triggers",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=GCkhHWFdDW": {
    "title": "EPT: Explosive Prompt Tuning For Parameter-Efficient with Large Norm Prompt",
    "volume": "review",
    "abstract": "Prompt tuning introduces additional learnable tokens, known as $\\textit{soft prompts}$, to frozen pre-trained language models for parameter-efficient tuning. Unlike fine-tuning, only these soft prompts are trained on downstream tasks rather than all model parameters. While recent prompt tuning approaches that introduce a reparameterization network have shown comparable performance to fine-tuning, they still require a large number of parameters for the soft prompts. In this paper, we empirically show the characteristics of the recent prompt tuning methods, such as the large norm of trained soft prompts and their significant similarity to each other. Inspired by these observations, we propose simple yet effective modifications to the reparameterization network for efficient prompt tuning, which involves inducing large norm, replacing overparameterization with under-parameterization, and focusing on a single prompt. This approach preserves the advantageous characteristics of the soft prompts while significantly reducing the number of parameters. Our comprehensive experiments across 21 diverse NLP datasets show that our method called EPT: Explosive Prompt Tuning, significantly outperforms prompt Tuning and achieves comparable performance full fine-tuning or other parameter-efficient tuning, with only 2.3K parameters during training on T5-base",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1lMN0xV5tg": {
    "title": "InternalInspector $I^2$: Robust Confidence Estimation in LLMs through Internal States",
    "volume": "review",
    "abstract": "Despite their vast capabilities, Large Language Models (LLMs) often struggle with generating reliable outputs, frequently producing high-confidence inaccuracies known as hallucinations. Addressing this challenge, our research introduces InternalInspector, a novel framework designed to enhance confidence estimation in LLMs by leveraging contrastive learning on internal states including attention states, feed-forward states, and activation states of all layers. Unlike existing methods that primarily focus on the final activation state, InternalInspector conducts a comprehensive analysis across all internal states of every layer to accurately identify both correct and incorrect prediction processes. By benchmarking InternalInspector against existing confidence estimation methods across various natural language understanding and generation tasks, including factual question answering, commonsense reasoning, and reading comprehension, InternalInspector achieves significantly higher accuracy in aligning the estimated confidence scores with the correctness of the LLM's predictions and lower calibration error. Furthermore, InternalInspector excels at HaluEval, a hallucination detection benchmark, outperforming other internal-based confidence estimation methods in this task",
    "checked": false,
    "id": "2d5b8eed2fdf9f7ba237f986946c573d2b6aa258",
    "semantic_title": "internalinspector i2: robust confidence estimation in llms through internal states",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6gcX05a9aW": {
    "title": "ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models",
    "volume": "review",
    "abstract": "Emotion Support Conversation (ESC) is a crucial application, which aims to reduce human stress, offer emotional guidance, and ultimately enhance human mental and physical well-being. With the advancement of Large Language Models (LLMs), many researchers have employed LLMs as the ESC models. However, the evaluation of these LLM-based ESCs remains uncertain. In detail, we first re-organize 2,801 role-playing cards from seven existing datasets to define the roles of the role-playing agent. Second, we train a specific role-playing model called ESC-Role which behaves more like a confused person than GPT-4. Third, through ESC-Role and organized role cards, we systematically conduct experiments using 14 LLMs as the ESC models, including general AI-assistant LLMs (e.g., ChatGPT) and ESC-oriented LLMs (e.g., ExTES-Llama). We conduct comprehensive human annotations on interactive multi-turn dialogues of different ESC models. The results show that ESC-oriented LLMs exhibit superior ESC abilities compared to general AI-assistant LLMs, but there is still a gap behind human performance. Moreover, to automate the scoring process for future ESC models, we developed ESC-RANK, which trained on the annotated data, achieving a scoring performance surpassing 35 points of GPT-4",
    "checked": true,
    "id": "202f3a1341a9e614c2611de4809dd12fe7850f36",
    "semantic_title": "esc-eval: evaluating emotion support conversations in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AwD1id99dz": {
    "title": "Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved remarkable performance in various natural language processing tasks, especially in dialogue systems. However, LLM may also pose security and moral threats, especially in multi round conversations where large models are more easily guided by contextual content, resulting in harmful or biased responses.In this paper, we present a novel method to attack LLMs in multi-turn dialogues, called CoA (Chain of Attack). CoA is a semantic-driven contextual multi-turn attack method that adaptively adjusts the attack policy through contextual feedback and semantic relevance during multi-turn of dialogue with a large model, resulting in the model producing unreasonable or harmful content.We evaluate CoA on different LLMs and datasets, and show that it can effectively expose the vulnerabilities of LLMs, and outperform existing attack methods. Our work provides a new perspective and tool for attacking and defending LLMs, and contributes to the security and ethical assessment of dialogue systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2skGwpaB4x": {
    "title": "Selfâ€“Boost: Boosting LLMs with Iterative Self-Generated Data",
    "volume": "review",
    "abstract": "Instruction finetuned large language models (LLMs) have shown impressive performance solving a diverse range of natural language processing (NLP) tasks involving classification and reasoning. However, this can be particularly challenging in low-data regimes. Recent methods have shown boosting via iterative full finetuning to be an effective method to augment the training data by using the incorrect examples to generate synthetic data using a teacher LLM. However, data generation at scale using a teacher LLM can be costly, and full finetuning can be computationally expensive. To address this, we introduce Selfâ€“Boost, an iterative data augmentation and instruction finetuning strategy that has no external dependence on any teacher models. Selfâ€“Boost uses parameter efficient finetuning (PEFT) with Llama 3 8B to instruction finetune a model using the seed data, uses the same model to generate examples similar to the misclassifications, and also the same model to verify and filter the generated examples. Our experiments show that performance on TREC, GSM8K, and CaseHOLD improves by 21.6\\%, 5.6\\% and 1.3\\% respectively, when compared to our baseline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e7KRWhZJo7": {
    "title": "Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors",
    "volume": "review",
    "abstract": "Large language models have become increasingly prominent, also signaling a shift towards multimodality as the next frontier in artificial intelligence, where their embeddings are harnessed as prompts to generate textual content. Vision-language models (VLMs) stand at the forefront of this advancement, offering innovative ways to combine visual and textual data for enhanced understanding and interaction. However, this integration also enlarges the attack surface. Patch-based adversarial attack is considered the most realistic threat model in physical vision applications, as demonstrated in many existing literature. In this paper, we propose to address patched visual prompt injection, where adversaries exploit adversarial patches to generate target content in VLMs. Our investigation reveals that patched adversarial prompts exhibit sensitivity to pixel-wise randomization, a trait that remains robust even against adaptive attacks designed to counteract such defenses. Leveraging this insight, we introduce SmoothVLM, a defense mechanism rooted in smoothing techniques, specifically tailored to protect VLMs from the threat of patched visual prompt injectors. Our framework significantly lowers the attack success rate to a range between 0% and 5.0% on two leading VLMs, while achieving around 67.3% to 95.0% context recovery of the benign images, demonstrating a balance between security and usability",
    "checked": true,
    "id": "f41109b24c24ecccf26cda9ceacad9167d197028",
    "semantic_title": "safeguarding vision-language models against patched visual prompt injectors",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p7AOfQVfec": {
    "title": "Enhancing Legal Case Retrieval via Scaling High-quality Synthetic Query-Candidate Pairs",
    "volume": "review",
    "abstract": "Legal case retrieval (LCR) aims to provide similar cases as references for a given fact description. This task is crucial for promoting consistent judgments in similar cases, effectively enhancing judicial fairness and improving work efficiency for judges. However, existing works face two main challenges for real-world applications: existing works mainly focus on case-to-case retrieval using lengthy queries, which does not match real-world scenarios; and the limited data scale, with current datasets containing only hundreds of queries, is insufficient to satisfy the training requirements of existing data-hungry neural models. To address these issues, we introduce an automated method to construct synthetic query-candidate pairs and build the largest LCR dataset to date, LEAD, which is hundreds of times larger than existing datasets. This data construction method can provide ample training signals for LCR models. Experimental results demonstrate that model training with our constructed data can achieve state-of-the-art results on two widely-used LCR benchmarks. Besides, the construction method can also be applied to civil cases and achieve promising results. The code and dataset used in this paper will be released to promote the development of LCR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KGcCDvchuY": {
    "title": "Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering",
    "volume": "review",
    "abstract": "To address the issues of insufficient knowledge and hallucination in Large Language Models (LLMs), numerous studies have explored integrating LLMs with Knowledge Graphs (KGs). However, these methods are typically evaluated on conventional Knowledge Graph Question Answering (KGQA) with complete KGs, where all factual triples required for each question are entirely covered by the given KG. In such cases, LLMs primarily act as an agent to find answer entities within the KG, rather than effectively integrating the internal knowledge of LLMs and external knowledge sources such as KGs. In fact, KGs are often incomplete to cover all the knowledge required to answer questions. To simulate these real-world scenarios and evaluate the ability of LLMs to integrate internal and external knowledge, we propose leveraging LLMs for QA under Incomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the factual triples for each question, and construct corresponding datasets. To handle IKGQA, we propose a training-free method called Generate-on-Graph (GoG), which can generate new factual triples while exploring KGs. Specifically, GoG performs reasoning through a Thinking-Searching-Generating framework, which treats LLM as both Agent and KG in IKGQA. Experimental results on two datasets demonstrate that our GoG outperforms all previous methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YRjqvFpEwx": {
    "title": "Bayesian Example Selection Improves In-Context Learning for Speech, Text, and Visual Modalities",
    "volume": "review",
    "abstract": "Large language models (LLMs) can adapt to new tasks through in-context learning (ICL) based on a few examples presented in dialogue history without any model parameter update. Despite such convenience, the performance of ICL heavily depends on the quality of the in-context examples presented, which makes the in-context example selection approach a critical choice. This paper proposes a novel Bayesian in-Context example Selection method (ByCS) for ICL. Extending the inference probability conditioned on in-context examples based on Bayes' theorem, ByCS focuses on the inverse inference conditioned on test input. Following the assumption that accurate inverse inference probability (likelihood) will result in accurate inference probability (posterior), in-context examples are selected based on their inverse inference results. Diverse and extensive cross-tasking and cross-modality experiments are performed with speech, text, and image examples. Experimental results show the efficacy and robustness of our ByCS method on various models, tasks and modalities",
    "checked": true,
    "id": "729c6421c049d47350e04e32256a14d6fb74f9f1",
    "semantic_title": "bayesian example selection improves in-context learning for speech, text, and visual modalities",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=6TMh1qcTAN": {
    "title": "On the Empirical Complexity of Reasoning and Planning in LLMs",
    "volume": "review",
    "abstract": "Chain-of-thought (CoT), tree-of-thought (ToT), and related techniques work surprisingly well in practice for some complex reasoning tasks with Large Language Models (LLMs), but why? This work seeks the underlying reasons by conducting experimental case studies and linking the performance benefits to well-established sample and computational complexity principles in machine learning. We experimented with 6 reasoning tasks, ranging from grade school math, air travel planning, $\\ldots$, to Blocksworld. The results suggest that (i) both CoT and ToT benefit significantly from task decomposition, which breaks a complex reasoning task into a sequence of steps with low sample and computational complexity and explicitly outlines the reasoning structure, and (ii) for computationally hard reasoning tasks, the more sophisticated tree structure of ToT outperforms the linear structure of CoT. These findings provide useful guidelines for the use of LLM in solving reasoning tasks in practice",
    "checked": true,
    "id": "cb4ce9d960c0c91aa72b23fdf4b4de27dc0bd1db",
    "semantic_title": "on the empirical complexity of reasoning and planning in llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cojHhAjzVy": {
    "title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
    "volume": "review",
    "abstract": "Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs' abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Llama3-8B surpasses other LLMs with the comparable size by more than 8.0\\% in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT",
    "checked": true,
    "id": "60126292c0b31dfc8628d99001e057b9f8355000",
    "semantic_title": "sciagent: tool-augmented language models for scientific reasoning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=SqDrBaZVqn": {
    "title": "LONGAGENT: Achieving Question Answering for 128k-Token-Long Documents through Multi-Agent Collaboration",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved tremendous success in understanding language and processing text. However, question-answering (QA) on lengthy documents faces challenges of resource constraints and a high propensity for errors, even for the most advanced models such as GPT-4 and Claude2. In this paper, we introduce \\textsc{LongAgent}, a multi-agent collaboration method that enables efficient and effective QA over $128k$-token-long documents. \\textsc{LongAgent} adopts a \\textit{divide-and-conquer} strategy, breaking down lengthy documents into shorter, more manageable text chunks. A leader agent comprehends the user's query and organizes the member agents to read their assigned chunks, reasoning a final answer through multiple rounds of discussion. Due to members' hallucinations, it's difficult to guarantee that every response provided by each member is accurate. To address this, we develop an \\textit{inter-member communication} mechanism that facilitates information sharing, allowing for the detection and mitigation of hallucinatory responses. Experimental results show that a LLaMA-2 7B driven by \\textsc{LongAgent} can effectively support QA over $128k$-token documents, achieving $16.42\\%$ and $1.63\\%$ accuracy gains over GPT-4 on single-hop and multi-hop QA settings, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ioC3T9FYAW": {
    "title": "Exploring Unified Training Framework for Multimodal User Profiling",
    "volume": "review",
    "abstract": "With the emergence of social media and e-commerce platforms, accurate \\emph{user profiling} has become increasingly vital for recommendation systems and personalized services. Recent studies have focused on generating detailed user profiles by extracting various aspects of user attributes from textual reviews. Nevertheless, these investigations have not fully exploited the potential of the abundant multimodal data at hand. In this study, we propose a novel task called \\emph{multimodal user profiling}. This task emphasizes the utilization of both review texts and their accompanying images to create comprehensive user profiles. By integrating textual and visual data, we leverage their complementary strengths, enabling the generation of more holistic user representations. Additionally, we explore a unified joint training framework with various multimodal training strategies that incorporate users' historical review texts and images for user profile generation. Our experimental results underscore the significance of multimodal data in enhancing user profile generation and demonstrate the effectiveness of the proposed unified joint training approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pwXH7ldmt3": {
    "title": "DataNarrative: Automated Data-Driven Storytelling with Visualizations and Texts",
    "volume": "review",
    "abstract": "Data-driven storytelling is a powerful method for conveying insights by combining narrative techniques with visualizations and text. These stories integrate visual aids, such as highlighted bars and lines in charts, along with textual annotations explaining insights. However, creating such stories requires a deep understanding of the data and meticulous narrative planning, often necessitating human intervention, which can be time-consuming and mentally taxing. While Large Language Models (LLMs) excel in various NLP tasks, their ability to generate coherent and comprehensive data stories remains underexplored. In this work, we introduce a novel task for data story generation and a benchmark containing 1,449 stories from diverse sources. To address the challenges of crafting coherent data stories, we propose a multi-agent framework employing two LLM agents designed to replicate the human storytelling process: one for understanding and describing the data (Reflection), generating the outline, and narration, and another for verification at each intermediary step. While our agentic framework generally outperforms non-agentic counterparts in both model-based and human evaluations, the results also reveal unique challenges in data story generation",
    "checked": true,
    "id": "bf746159ec6008fa8e4d4134c848f8611066d62d",
    "semantic_title": "datanarrative: automated data-driven storytelling with visualizations and texts",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kYT8fIcRtW": {
    "title": "CELM: A Dataset for Chinese Ethico-Legal Alignment in Large Language Models",
    "volume": "review",
    "abstract": "Existing Chinese datasets for aligning large language models (LLMs) with human preferences often reflect U.S.-centric values due to their annotation process, reducing their effectiveness for developing safe and culturally appropriate LLMs for China, one of the largest LLM markets in the world. In this work, we introduce ``CELM'', a comprehensive Chinese-centric dataset, for i) training LLMs with the Chinese module aligning with corresponding societal values and ii) assessing their safety in the Chinese context. This dataset includes 17 important scenarios, three of which are unique to China. It includes 1,337 instances innovatively annotated with Chinese legal and ethical norms for fine-tuning, and 46,633 instances judged according to the safety preference of native Chinese crowdworkers for reinforcement learning. It includes 2,111 evaluation examples produced using human-in-the-loop red teaming to rigorously examine the safety levels of LLMs in the Chinese cultural context. Our studies show that models trained on CELM produce safer and more culturally relevant responses for China than those trained on datasets biased towards U.S. norms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HBwy94oRhi": {
    "title": "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?",
    "volume": "review",
    "abstract": "The adaption of multilingual pre-trained LLMs into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models instruction-tuned on different language compositions on parallel instruction-tuning benchmarks across a selection of the most spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized and a large, multilingual LLMs by instruction-tuning them on parallel instruction-tuning datasets. Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 9.9%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning datasets. Finally, we conduct a human annotation study to understand the alignment between human-based and GPT-4-based evaluation within multilingual chat scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z4zVpx8OLs": {
    "title": "SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding",
    "volume": "review",
    "abstract": "As large language models (LLMs) become integral to advancing NLP tasks, their sequential decoding becomes a bottleneck to achieving more efficient inference. Multi-Draft Speculative Decoding (MDSD) emerges as a promising solution, where a small draft model produces a tree of tokens with each path as a draft predicting the target LLM's outputs, which is then verified by the target LLM in parallel. However, current methods rely on Recursive Rejection Sampling (RRS) and its variants, which suffer from low acceptance rates in proceeding drafts, diminishing the merits of multiple drafts. In this work, we investigate this critical inefficiency and sub-optimality through an optimal transport (OT) formulation that aims to maximize the acceptance rate by optimizing the joint distribution $\\pi(x_{1:k},y)$ of $k$-draft tokens $x_{1:k}$ and an accepted token $y$. We show that the OT can be greatly simplified to a much smaller Linear Programming (LP) focusing on a few probabilities in $\\pi(x_{1:k},y)$. Moreover, our analysis of different choices for the marginal distribution $Q(x_{1:k})$ reveals its importance to the sampling effectiveness and efficiency. Motivated by the new insight, we introduce SpecHub, which adopts a special design of $Q(x_{1:k})$ that significantly accelerates the LP and provably achieves a higher acceptance rate than existing strategies. SpecHub can be seamlessly integrated into existing MDSD frameworks, improving their acceptance rate while only incurring linear computational overhead. In extensive experiments, Spechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step than RRS with and without replacement and achieves equivalent batch efficiency with half as much concurrency. We attach our code at \\url{anonymous.4open.science/r/SpecHub}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vPsA7QJWln": {
    "title": "Taxonomy-Guided Zero-Shot Recommendations with LLMs",
    "volume": "review",
    "abstract": "With the emergence of large language models (LLMs) and their ability to perform a variety of tasks, their application in recommender systems (RecSys) has shown promise. However, we are facing significant challenges when deploying LLMs into RecSys, such as limited prompt length, unstructured item information, and un-constrained generation of recommendations, leading to sub-optimal performance. To address these issues, we propose a novel method using a taxonomy dictionary. This method provides a systematic framework for categorizing and organizing items, improving the clarity and structure of item information. By incorporating the taxonomy dictionary into LLM prompts, we achieve efficient token utilization and controlled feature generation, leading to more accurate and contextually relevant recommendations. Our Taxonomy-guided Recommendation (TaxRec) approach features a two-step process: one-time taxonomy categorization and LLM-based recommendation, enabling zero-shot recommendations without the need for domain-specific fine-tuning. Experimental results demonstrate TaxRec significantly enhances recommendation quality compared to traditional zero-shot approaches, showcasing its efficacy as personal recommender with LLMs. Code is available at https://anonymous.4open.science/r/TaxRec",
    "checked": true,
    "id": "7a7da5e38c5b917b99c794436c8da58f4474cefe",
    "semantic_title": "taxonomy-guided zero-shot recommendations with llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9JFlJG9yZZ": {
    "title": "Thought2Text: Text Generation from EEG Brain Activities through Large Language Models",
    "volume": "review",
    "abstract": "Decoding and expressing brain activities in a comprehensible form is a challenging frontier in AI. This paper presents Thought2Text, which uses instruction-tuned LLMs fine-tuned with EEG data to achieve this goal. The approach involves three stages: (1) training an EEG encoder for visual feature extraction, (2) fine-tuning LLMs on image and text data, enabling multimodal description generation, and (3) further fine-tuning on EEG embeddings to generate text directly from EEG during inference. Experiments on a public EEG dataset collected for six subjects with image stimuli demonstrate the efficacy of multimodal LLMs (LLaMa-v3, Mistral-v0.3, Phi-v3) validated using BLEU, METEOR, ROUGE, BertScore, and GPT-4 based evaluations. This approach marks a significant advancement towards portable, low-cost \"thoughts-to-text\" technology, applicable to neuroscience and NLP",
    "checked": false,
    "id": "f9e5844a6bcff29f0e54374c345818331dae4d36",
    "semantic_title": "mindsemantix: deciphering brain visual experiences with a brain-language model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nAqcuIOXr8": {
    "title": "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models",
    "volume": "review",
    "abstract": "Vision-language models (VLMs) like CLIP have demonstrated remarkable applicability across a variety of downstream tasks, including zero-shot image classification. Recently, the use of prompts or adapters for efficient transfer learning (ETL) has gained significant attention for effectively adapting to downstream tasks. However, previous studies have overlooked the challenge of varying transfer difficulty of downstream tasks. In this paper, we empirically analyze how each ETL method behaves with respect to transfer difficulty. Our observations indicate that utilizing vision prompts and text adapters is crucial for adaptability and generalizability in domains with high difficulty. Also, by applying an adaptive ensemble approach that integrates task-adapted VLMs with pre-trained VLMs and strategically leverages more general knowledge in low-difficulty and less in high-difficulty domains, we consistently enhance performance across both types of domains. Based on these observations, we propose an adaptive ensemble method that combines visual prompts and text adapters with pre-trained VLMs, tailored by transfer difficulty, to achieve optimal performance for any target domain. Upon experimenting with extensive benchmarks, our method consistently outperforms all baselines, particularly on unseen tasks, demonstrating its effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o9GRwNogsB": {
    "title": "A Collaborative Reasoning Framework Powered by Reinforcement Learning and Large Language Models for Complex Questions Answering over Knowledge Graph",
    "volume": "review",
    "abstract": "Knowledge Graph Question Answering (KGQA) aims to automatically answer natural language questions by reasoning across multiple triples in knowledge graphs (KGs). Reinforcement learning (RL)-based methods are introduced to enhance model interpretability. Nevertheless, when addressing complex questions requiring long-term reasoning, the RL agent is usually misled by aimless exploration, as it lacks common learning practices with prior knowledge. Recently, large language models (LLMs) have been proven to encode vast amounts of knowledge about the world and possess remarkable reasoning capabilities. However, they often encounter challenges with hallucination issues, failing to address complex questions that demand deep and deliberate reasoning. In this paper, we propose a collaborative reasoning framework (CRF) powered by RL and LLMs to answer complex questions based on the knowledge graph. Our approach leverages the common sense priors contained in LLMs while utilizing RL to provide learning from the environment, resulting in a hierarchical agent that uses LLMs to solve the complex KGQA task. By combining LLMs and the RL policy, the high-level agent accurately identifies constraints encountered during reasoning, while the low-level agent conducts efficient path reasoning by selecting the most promising relations in KG. Extensive experiments conducted on four benchmark datasets clearly demonstrate the effectiveness of the proposed model, which surpasses state-of-the-art approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yr8UoBpmwM": {
    "title": "Dual-Space Knowledge Distillation for Large Language Models",
    "volume": "review",
    "abstract": "Knowledge distillation (KD) is known as a promising solution to compress large language models (LLMs) via transferring their knowledge to smaller models. During this process, white-box KD methods usually minimize the distance between the output distributions of the two models so that more knowledge can be transferred. However, in the current white-box KD framework, the output distributions are from the respective output spaces of the two models, using their own prediction heads. We argue that the space discrepancy will lead to low similarity between the teacher model and the student model on both representation and distribution levels. Furthermore, this discrepancy also hinders the KD process between models with different vocabularies, which is common for current LLMs. To address these issues, we propose a dual-space knowledge distillation (DSKD) framework that unifies the output spaces of the two models for KD. On the basis of DSKD, we further develop a cross-model attention mechanism, which can automatically align the representations of the two models with different vocabularies. Thus, our framework is not only compatible with various distance functions for KD (e.g., KL divergence) like the current framework, but also supports KD between any two LLMs regardless of their vocabularies. Experiments on task-agnostic instruction-following benchmarks show that DSKD significantly outperforms the current white-box KD framework with various distance functions, and also surpasses existing KD methods for LLMs with different vocabularies",
    "checked": true,
    "id": "84c567316b2eb804fb673871ae89748c3990a558",
    "semantic_title": "dual-space knowledge distillation for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xX0uhgvexQ": {
    "title": "Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training",
    "volume": "review",
    "abstract": "Existing speculative decoding methods typically require additional model structure and training processes to assist the model for draft token generation. This makes the migration of acceleration methods to the new model more costly and more demanding on device memory. To address this problem, we propose the Make Some Noise (MSN) training framework as a replacement for the supervised fine-tuning stage of the large language model. The training method simply introduces some noise at the input for the model to learn the denoising task. It significantly enhances the parallel decoding capability of the model without affecting the original task capability. In addition, we propose a tree-based retrieval-augmented Jacobi (TR-Jacobi) decoding strategy to further improve the inference speed of MSN models. Experiments in both the general and code domains have shown that MSN can improve inference speed by 2.3-2.7x times without compromising model performance. The MSN model also achieves comparable acceleration ratios to the SOTA model with additional model structure on Spec-Bench",
    "checked": true,
    "id": "a03cb5e9806e532966a13e4fab7cef9c8d81dec5",
    "semantic_title": "make some noise: unlocking language model parallel inference capability through noisy training",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=aJdfCtCOlq": {
    "title": "Is Nomenclature Beneficial to Language Models for Chemistry?",
    "volume": "review",
    "abstract": "Most existing research in language model applications for chemistry employs the Simplified Molecular Input Line Entry System (SMILES) nomenclature, designed to encode molecular structure in a string format as both input and output. In contrast, machine learning approaches using human-readable IUPAC (International Union of Pure and Applied Chemistry) nomenclature remain underexplored. IUPAC names are widely used in the chemical literature, providing opportunities to train large language models on a vast corpus with contextual expertise. We are motivated to compare these two nomenclatures across various language-molecule scenarios. We found that simply switching to IUPAC names in challenging downstream tasks such as molecular generation, captioning, and editing results in a performance improvement of up to 4 times. Additionally, catastrophic forgetting during fine-tuning is reduced by half when using IUPAC names compared to SMILES",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cfMRTgLYSf": {
    "title": "TKGT: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented LLMs",
    "volume": "review",
    "abstract": "The task of text-to-table receives widespread attention, but its importance and difficulty are underestimated. Existing works use simple datasets like those from table-to-text tasks and employ methods that ignore domain structures. As a bridge between raw text and statistical analysis, the text-to-table task faces challenges from more complex semi-structured texts that refer to certain domain topics in the real world with obvious entities and events, especially from those of social sciences. In this paper, we analyse the limitation of previous datasets with methods and redefine the text-to-table task, based on which we propose a new dataset called CPL (Chinese Private Lending) of case judgments from a real world legal academic project. We further propose TKGT (Text-KG-Table), a two stages domain-aware pipeline, which firstly generates domain knowledge graphs (KGs) classes semi-automatically from raw text with the mixed information extraction (Mixed-IE) method, then adopts the hybrid retrieval augmented generation (Hybird-RAG) method to transform it to tables for downstream needs under the guidance of KGs classes. Experiment results show that TKGT achieves state-of-the-art (SOTA) performance on both traditional datasets and the CPL. Our code and data are available at https://anonymous.4open.science/r/TKGT-4755",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A6JKBhc8aT": {
    "title": "Exploring the Diversity of Opinions on Affirmative Action Through Extended Stance Detection Among Reddit Users",
    "volume": "review",
    "abstract": "Affirmative Action (AA) has remained a controversial topic in the U.S. for several decades. While previous research has extensively explored AA from legal, social, and ethical aspects, there is a lack of computational work to investigate this topic from the lens of users on social media. By collecting over 2,500 posts from 23 prominent Reddit communities, our study attempts to gain a better understanding of how online users view AA. We build upon the previous work on stance detection, by introducing a new set of stance categories that can efficiently reflect a diverse range of opinions on AA. Finally, we explore the performance of three LLM-based classifiers, powered with four carefully designed prompts to classify these categories and run several analyses to enhance our understanding of the AA discourse. Our findings show that GPT-4 with instruction and examples is the most efficient for classifying stance. We found opposition towards AA more common than support in our dataset with discussions on college admission, and race more prevalent. This study enhances the field by presenting a novel dataset of stances derived from Reddit data and initiates a conversation on broadening binary evaluations of viewpoints on controversial subjects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AYzd2W2ZEG": {
    "title": "MTChat: A Multimodal Time-Aware Dataset and Framework for Conversation",
    "volume": "review",
    "abstract": "Understanding temporal dynamics is critical for applications ranging from conversations and multimedia content analysis to decision-making. However, time-aware datasets, particularly for conversations, are still limited, which narrows their scope and diminishes their complexity. To overcome these limitations, we introduce MTChat, a multimodal time-aware dialogue dataset that integrates linguistic, visual, and temporal elements in dialogue and persona memory. Based on MTChat, we design two time-sensitive tasks, Temporal Next Response Prediction (TNRP) and Temporal Grounding Memory Prediction (TGMP), utilizing implicit temporal cues and dynamic aspects to challenge model's temporal awareness. Furthermore, we present an innovative framework with an adaptive temporal module to integrate these multimodal streams and build interconnections effectively. The experimental results confirm that novel challenges of MTChat and effectiveness of our framework in multimodal time-sensitive scenarios. The codes are publicly available at \\href{https://anonymous.4open.science/r/MTChat-F83B/.} and MTChat is submitted to ARR system",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3PABAHvV6H": {
    "title": "Revisiting Automated Evaluation for Long-form Table Question Answering in the Era of Large Language Models",
    "volume": "review",
    "abstract": "In the era of data-driven decision-making, Long-Form Table Question Answering (LFTQA) is essential for integrating structured data with complex reasoning. Despite recent advancements in Large Language Models (LLMs) for LFTQA, evaluating their effectiveness remains a significant challenge. We introduce LFTQA-Eval, a meta-evaluation dataset comprising 6,400 human-annotated examples, to rigorously assess the efficacy of current automated metrics in assessing LLM-based LFTQA systems, with a focus on faithfulness and comprehensiveness. Our findings reveal that existing automatic metrics poorly correlate with human judgments and fail to consistently differentiate between factually accurate responses and those that are coherent but factually incorrect. Additionally, our in-depth examination of the limitations associated with automated evaluation methods provides essential insights for the improvement of LFTQA automated evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RifFJNhGFS": {
    "title": "DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework",
    "volume": "review",
    "abstract": "Current video generation models excel at creating short, realistic clips, but struggle with longer, multi-scene videos. We introduce \\texttt{DreamFactory}, an LLM-based framework that tackles this challenge. \\texttt{DreamFactory} leverages multi-agent collaboration principles and a Key Frames Iteration Design Method to ensure consistency and style across long videos. It utilizes Chain of Thought (COT) to address uncertainties inherent in large language models. \\texttt{DreamFactory} generates long, stylistically coherent, and complex videos. Evaluating these long-form videos presents a challenge. We propose novel metrics such as Cross-Scene Face Distance Score and Cross-Scene Style Consistency Score. To further research in this area, we contribute the Multi-Scene Videos Dataset containing over 150 human-rated videos. \\texttt{DreamFactory} paves the way for utilizing multi-agent systems in video generation. We will make our framework and datasets public after paper acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lr5kxVrfqc": {
    "title": "Collaborative Tasks with Heterogenous LLM Students",
    "volume": "review",
    "abstract": "Advances in LLMs offer hope of corresponding advances in agent participation in teamwork, while also posing new challenges in designing multi-agent benchmarks for evaluating these agents and integrating them effectively into hybrid teams in real-world situations. While prior work has demonstrated that LLMs can operate in multi-agent settings, they often oversimplify the complexity of collaboration in critical dimensions, such as restricting evaluation to in-domain and single episode tasks amongst homogeneous LLM groups. To bridge this gap, we propose a new cooperative multi-agent task, Kitchen-Alien Rush, which includes both out-of-domain multi-episode evaluation, as well as evaluates the effectiveness of hybrid groups in collaboration. Our findings reveal that our evaluation exposes gaps in multi-agent collaboration, as LLM agents struggle to perform in the out-of-domain task and show inconsistent improvement over multiple episodes in hybrid teams. By identifying these gaps, we motivate the need for future work in addressing weaknesses of hybrid multi-agents systems for out-of-domain multi-episode tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LA49aG7kLd": {
    "title": "SEAL: Entangled White-box Watermarks on Low-Rank Adaptation",
    "volume": "review",
    "abstract": "Watermarking is a promising copyright protection method for Deep Neural Networks (DNNs). It works by embedding a secret identity message into the DNN during training, and extracting it later when copyright is disputed. Prior work has proposed various techniques that can embed secret identity messages into different layers of a DNN. We observe that models nowadays are frequently created and distributed in the form of Low-Rank Adaptation (LoRA) weights, because of its significant savings in training cost. We propose SEAL (SEcure wAtermarking on LoRA weights), the first watermarking method tailored for LoRA weights. Unlike existing methods that focus on specific layers and are unsuitable for LoRA's unique structure, SEAL embeds a secret, non-trainable matrix between trainable LoRA weights, serving as a passport to claim ownership. SEAL then entangles this passport with the LoRA weights through finetuning, and distributes the finetuned weights after hiding the passport. We demonstrate that SEAL is robust against a variety of known attacks, and works without compromising the performance of watermarked models on various NLP tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=brG8gXxSNI": {
    "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have highlighted the necessity of effective {unlearning} mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility beyond the scope of unlearning. While interest in studying LLM unlearning is growing, the impact of the optimizer choice for LLM unlearning remains unexplored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between second-order optimization and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order optimization-based LLM unlearning framework, termed Second-Order UnLearning (SOUL), which extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, indicating that second-order optimization offers an effective and broadly applicable solution for LLM unlearning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vu4d5PWRxb": {
    "title": "Unveiling the Flaws: Exploring Imperfections in Synthetic Data and Mitigation Strategies for Large Language Models",
    "volume": "review",
    "abstract": "Synthetic data has been proposed as a solution to address the issue of high-quality data scarcity in the training of large language models (LLMs). Studies have shown that synthetic data can effectively improve the performance of LLMs on downstream benchmarks. However, despite its potential benefits, our analysis suggests that there may be inherent flaws in synthetic data. The uniform format of synthetic data can lead to pattern overfitting and cause significant shifts in the output distribution, thereby reducing the model's instruction-following capabilities. Our work delves into these specific flaws associated with question-answer (Q-A) pairs, a prevalent type of synthetic data, and presents a method based on unlearning techniques to mitigate these flaws. The empirical results demonstrate the effectiveness of our approach, which can reverse the instruction-following issues caused by pattern overfitting without compromising performance on benchmarks at relatively low cost. Our work has yielded key insights into the effective use of synthetic data, aiming to promote more robust and efficient LLM training",
    "checked": true,
    "id": "4e528581e8b45b7d955455f825079825eeb66701",
    "semantic_title": "unveiling the flaws: exploring imperfections in synthetic data and mitigation strategies for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=aReecSfloe": {
    "title": "MultiLogicNMR(er): A Benchmark and Neural-Symbolic Framework for Non-monotonic Reasoning Tasks with Multiple Extensions",
    "volume": "review",
    "abstract": "Non-monotonic reasoning is a classic paradigm widely used in daily life and legal reasoning. The $\\delta$-$NLI$ and LogicNMR proposed in the existing work have only preliminary explored the non-monotonic reasoning ability of the pre-trained language models (LMs) in natural language. However, the performance of large language models (LLMs) on complex non-monotonic reasoning tasks with multiple extensions has not yet been explored. An extension can be interpreted as a set of plausible conclusions. In this paper, we automatically synthesized a non-monotonic reasoning dataset with multiple extensions, MultiLogicNMR. Then, we systematically evaluated prompt-based and fine-tuned LLMs using skeptical and credulous reasoning, respectively. Skeptical reasoning only believes in common facts in all extensions, while credulous reasoning believes in facts in any one extension. In addition, inspired by classic symbolic solvers, we propose a neural-symbolic framework, MultiLogicNMRer, to improve the model's non-monotonic reasoning ability. Experimental results show that the accuracy of MultiLogicNMRer based on ChatGPT3.5 is about 23.1\\% higher $(46.2\\% \\rightarrow 69.3\\%)$ than the corresponding prompt-based LLMs. The proposed MultiLogicNMR dataset and MultiLogicNMRer framework are expected to promote the research of LLMs on non-monotonic reasoning in natural language",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OMhPSDMWFX": {
    "title": "Augment before You Try: Knowledge-Enhanced Table Question Answering via Table Expansion",
    "volume": "review",
    "abstract": "Table question answering is a popular task that assesses a model's ability to understand and interact with structured data. However, the given table often does not contain sufficient information to answer the question, necessitating the integration of external knowledge. Existing methods either convert both the table and external knowledge into text, which neglects the structured nature of the table; or they embed queries for external sources in the interaction with the table, which complicates the process. In this paper, we propose a simple yet effective method to integrate external information in a given table. Our method first constructs an augmenting table containing the missing information and then generates a SQL query over the two tables to answer the question. Experiments show that our method outperforms strong baselines on three table QA benchmarks",
    "checked": true,
    "id": "f818966c3a1b1745246efa1b33c98747a9c02922",
    "semantic_title": "augment before you try: knowledge-enhanced table question answering via table expansion",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=yGuJKNOJ1u": {
    "title": "Advancing Test-Time Adaptation in Wild Acoustic Test Settings",
    "volume": "review",
    "abstract": "Acoustic foundation models, fine-tuned for Automatic Speech Recognition (ASR), suffer from performance degradation in wild acoustic test settings when deployed in real-world scenarios. Stabilizing online Test-Time Adaptation (TTA) under these conditions remains an open and unexplored question. Existing wild vision TTA methods often fail to handle speech data effectively due to the unique characteristics of high-entropy speech frames, which are unreliably filtered out even when containing crucial semantic content. Furthermore, unlike static vision data, speech signals follow short-term consistency, requiring specialized adaptation strategies. In this work, we propose a novel wild acoustic TTA method tailored for ASR fine-tuned acoustic foundation models. Our method, Confidence-Enhanced Adaptation, performs frame-level adaptation using a confidence-aware weight scheme to avoid filtering out essential information in high-entropy frames. Additionally, we apply consistency regularization during test-time optimization to leverage the inherent short-term consistency of speech signals. Our experiments on both synthetic and real-world datasets demonstrate that our approach outperforms existing baselines under various wild acoustic test settings, including Gaussian noise, environmental sounds, accent variations, and sung speech",
    "checked": false,
    "id": "9529de56904818eeb9ce3ad8aaa5595d93d3bd91",
    "semantic_title": "(socio-)ecological tools and insights for a changing climate",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JqC3JoeL3l": {
    "title": "Multimodal Customized Review Generation",
    "volume": "review",
    "abstract": "In this study, we introduce a new task called \\emph{customized review generation}. This task aims to generate a personalized review that a specific user would give to a product that they have not yet reviewed. This can help users write high-quality reviews for products they have not previously reviewed, providing them with valuable insights. Additionally, customized reviews can offer a tailored summary of all reviews for a product, catering to the individual preferences of the reader. To achieve this goal, we explore the use of multimodal information for customized review generation. Specifically, we utilize a \\emph{multimodal pre-trained language model} that takes a picture of a product and a set of words as input and generates a customized review using both visual and textual information. Our experimental results demonstrate the effectiveness of the proposed model in generating customized reviews that are often of high quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ui6mIdPCLy": {
    "title": "A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are trained on massive web-crawled corpora. An increasing issue is LLMs generating content based on leaked data, and the need to detect and suppress such generated results, including personal information, copyrighted text, and benchmark datasets. A fundamental cause of this issue is leaked data in the training dataset. However, existing research has not sufficiently clarified the relationship between leaked instances in the training data and the ease of output and detection of these leaked instances by LLMs. In this paper, we conduct an experimental survey to elucidate the relationship between the rate of leaked instances in the training dataset and the generation and detection of LLMs in relation to the leakage of personal information, copyrighted texts, and benchmark data. Our experiments reveal that LLMs generate leaked information in most cases despite there being little such data in the training set. Furthermore, the lower the rate of leaked instances, the more difficult it becomes to detect the leakage. When addressing the leakage problem in the training dataset, we must be careful as reducing leakage instances does not necessarily lead to only positive effects. Finally, we demonstrate that explicitly defining the leakage detection task using examples in LLMs can help mitigate the impact of the rate of leakage instances in the training data on detection",
    "checked": true,
    "id": "4c641b4b177024943c5b3cc527bf58632e9a4f41",
    "semantic_title": "a little leak will sink a great ship: survey of transparency for large language models from start to finish",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=hHHkx08YVV": {
    "title": "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Heuristic-based Sampling",
    "volume": "review",
    "abstract": "Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework PROMST that incorporates human-designed feedback rules to automatically offer direct suggestions for improvement. We also use an extra learned heuristic model that predicts prompt performance to efficiently sample from prompt candidates. This approach significantly outperforms both human-engineered prompts and several other prompt optimization methods across 11 representative multi-step tasks (an average 10.6%-29.3% improvement to current best methods on five LLMs respectively). We believe our work can serve as a benchmark for automatic prompt optimization for LLM-driven multi-step tasks",
    "checked": true,
    "id": "ef9645900019453c9908b6a4878e458b961bb7c3",
    "semantic_title": "prompt optimization in multi-step tasks (promst): integrating human feedback and heuristic-based sampling",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=2UlLGNpvwP": {
    "title": "Defending Jailbreak Attack in VLMs via Cross-modality Information Detector",
    "volume": "review",
    "abstract": "Vision Language Models (VLMs) extend the capacity of LLMs to comprehensively understand vision information, achieving remarkable performance in many vision-centric tasks. Despite that, recent studies have shown that these models are susceptible to jailbreak attacks, which refer to an exploitative technique where malicious users can break the safety alignment of the target model and generate misleading and harmful answers. This potential threat is caused by both the inherent vulnerabilities of LLM and the larger attack scope introduced by vision input. To enhance the security of VLMs against jailbreak attacks, researchers have developed various defense techniques. However, these methods either require modifications to the model's internal structure or demand significant computational resources during the inference phase. Multimodal information is a double-edged sword. While it increases the risk of attacks, it also provides additional data that can enhance safeguards. Inspired by this, we propose $\\underline{\\textbf{C}}$ross-modality $\\underline{\\textbf{I}}$nformation $\\underline{\\textbf{DE}}$tecto$\\underline{\\textbf{R}}$ ($\\textit{CIDER})$, a plug-and-play jailbreaking detector designed to identify maliciously perturbed image inputs, utilizing the cross-modal similarity between harmful queries and adversarial images. This simple yet effective cross-modality information detector, $\\textit{CIDER}$, is independent of the target VLMs and requires less computation cost. Extensive experimental results demonstrate the effectiveness and efficiency of $\\textit{CIDER}$, as well as its transferability to both white-box and black-box VLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VhwKsW47FA": {
    "title": "medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced Clinical Diagnosis on EMRs",
    "volume": "review",
    "abstract": "Electronic Medical Records (EMRs), while integral to modern healthcare, present challenges for clinical reasoning and diagnosis due to their complexity and information redundancy. To address this, we proposed medIKAL (\\textbf{I}ntegrating \\textbf{K}nowledge Graphs as \\textbf{A}ssistants of \\textbf{L}LMs), a framework that combines Large Language Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic capabilities. medIKAL assigns weighted importance to entities in medical records based on their type, enabling precise localization of candidate diseases within KGs. It innovatively employs a residual network approach, allowing initial diagnosis by the LLM without external knowledge before merging with KG search results. A path-based reranking algorithm and a specialized prompt template further refine the diagnostic process. We validated medIKAL's effectiveness through extensive experiments on a newly introduced open-sourced Chinese EMR dataset, demonstrating its potential to improve clinical diagnosis and decision-making in real-world settings",
    "checked": true,
    "id": "4d7ba29d00d7497f7c74a2f2556e70e9af50645f",
    "semantic_title": "medikal: integrating knowledge graphs as assistants of llms for enhanced clinical diagnosis on emrs",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CtHsalE9mx": {
    "title": "When Evolution Strategy Meets Language Models Tuning",
    "volume": "review",
    "abstract": "Supervised Fine-tuning has been pivotal in training autoregressive language models, yet it introduces exposure bias. To mitigate this, Post Fine-tuning, including on-policy and off-policy methods, has emerged as a solution to enhance models further, though each has its limitations regarding performance enhancements and susceptibility to overfitting. In this paper, we introduce a novel on-policy approach, called \\textbf{Evolution Strategy Optimization} (ESO), which is designed by harnessing the principle of biological evolution, namely \\emph{survival of the fittest}. Particularly, we consider model tuning as an evolution process, and each output sentence generated by the model can provide a perturbation signal to the model parameter space. Then, the fitness of perturbation signals is quantified by the difference between its score and the averaged one offered by a reward function, steering optimization process. Empirically, the proposed method can achieve superior performance in various tasks and comparable performance in the human alignment one. The code will be publicly available",
    "checked": false,
    "id": "1215d4c5248124f6a0b133760c4c5b6d65804abf",
    "semantic_title": "when large language models meet evolutionary algorithms",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=Ezwl7hBpf2": {
    "title": "The Eyes Don't Lie: Text Transcriptions Can Hide Dementia Presentation that Gaze Reveals",
    "volume": "review",
    "abstract": "Current methods used to diagnose or monitor dementia-related cognitive decline predominantly rely on audio recordings. Such audio recordings can leak personally identifiable information and create new risks given deep fake technology. We introduce generative likelihood-based approaches to identify differences in healthy versus dementia-diagnosed participants via gaze tracking and text transcriptions during a standard diagnostic image description task without relying on sensitive audio information. Contrasting conventional wisdom, we find that text transcriptions alone are not a reliable measure of cognitive impairment in this task, finding gaze tracking to be more reliable, and suggesting existing results in language-based dementia detection rely primarily on audio signals",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vkLeYVwzlQ": {
    "title": "Surgical-LLaVA: Toward Surgical Scenario Understanding via Large Language and Vision Models",
    "volume": "review",
    "abstract": "Conversation agents powered by large language models are revolutionizing the way we interact with visual data. Recently, large vision-language models (LVLMs) have been extensively studied for both images and videos. However, these studies typically focus on common scenarios. In this work, we introduce an LVLM specifically designed for surgical scenarios. We integrate visual representations of surgical images and videos into the language feature space. Consequently, we establish a LVLM model, Surgical-LLaVA, fine-tuned on instruction following data of surgical scenarios. Our experiments demonstrate that Surgical-LLaVA exhibits impressive multi-modal chat abilities in surgical contexts, occasionally displaying multi-modal behaviors on unseen instructions. We conduct a quantitative evaluation of visual question-answering datasets for surgical scenarios. The results show superior performance compared to previous works, indicating the potential of our model to tackle more complex surgery scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pU4Bzqr9oh": {
    "title": "CVLUE: A New Benchmark Dataset for Chinese Vision-Language Understanding Evaluation",
    "volume": "review",
    "abstract": "Despite the rapid development of Chinese vision-language models (VLMs), most existing Chinese vision-language (VL) datasets are constructed on Western-centric images from existing English VL datasets. The cultural bias in the images makes these datasets unsuitable for evaluating VLMs in Chinese culture. To remedy this issue, we present a new Chinese Vision-Language Understanding Evaluation (CVLUE) benchmark dataset, where the selection of object categories and images is entirely driven by Chinese native speakers, ensuring that the source images are representative of Chinese culture. The benchmark contains four distinct VL tasks ranging from image-text retrieval to visual question answering, visual grounding and visual dialogue. We present a detailed statistical analysis of CVLUE and provide a baseline performance analysis with several open-source multilingual VLMs on CVLUE and its English counterparts to reveal their performance gap between English and Chinese. Our in-depth category-level analysis reveals a lack of Chinese cultural knowledge in existing VLMs. We also find that fine-tuning on Chinese culture-related VL datasets effectively enhances VLMs' understanding of Chinese culture",
    "checked": true,
    "id": "910b69e67f4906a1578f1598344f917fbbdc00cc",
    "semantic_title": "cvlue: a new benchmark dataset for chinese vision-language understanding evaluation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fE6InnWvQS": {
    "title": "SynthesizRR: Generating Diverse Datasets with Retrieval Augmentation",
    "volume": "review",
    "abstract": "It is often desirable to distill the capabilities of large language models (LLMs) into smaller student models due to compute and memory constraints. One way to do this for classification tasks is via dataset synthesis, which can be accomplished by generating examples of each label from the LLM. Prior approaches to synthesis use few-shot prompting, which relies on the LLM's parametric knowledge to generate usable examples. However, this leads to issues of repetition, bias towards popular entities, and stylistic differences from human text. In this work, we propose Synthesize by Retrieval and Refinement (SynthesizRR), which uses retrieval augmentation to introduce variety into the dataset synthesis process: as retrieved passages vary, the LLM is seeded with different content to generate its examples. We empirically study the synthesis of six datasets, covering topic classification, sentiment analysis, tone detection, and humor, requiring complex synthesis strategies. We find SynthesizRR greatly improves lexical and semantic diversity, similarity to human-written text, and distillation performance, when compared to 32-shot prompting and four prior approaches",
    "checked": true,
    "id": "386c1852329d981eca785f986d93cbf993dbaae4",
    "semantic_title": "synthesizrr: generating diverse datasets with retrieval augmentation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=34tVBCpUQM": {
    "title": "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging",
    "volume": "review",
    "abstract": "Supervised fine-tuning (SFT) is crucial for adapting Large Language Models (LLMs) to specific tasks. In this work, we demonstrate that the order of training data can lead to significant training imbalances, potentially resulting in performance degradation. Consequently, we propose to mitigate this imbalance by merging SFT models fine-tuned with different data orders, thereby enhancing the overall effectiveness of SFT. Additionally, we introduce a novel technique, \"parameter-selection merging,\" which outperforms traditional weighted-average methods on five datasets. Further, through analysis and ablation studies, we validate the effectiveness of our method and identify the sources of performance improvements",
    "checked": false,
    "id": "92c361d02caa966769b6888d4eff1080929e9dbb",
    "semantic_title": "llm-based federated recommendation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=rN1cAMNWIj": {
    "title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
    "volume": "review",
    "abstract": "Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization. While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with others to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ksK00x9MW4": {
    "title": "Bit-level BPE: Below the byte boundary",
    "volume": "review",
    "abstract": "Byte-level fallbacks for subword tokenization have become a common practice in large language models. In particular, it has been demonstrated to be incredibly effective as a pragmatic solution for preventing OOV, especially in the context of larger models. However, breaking a character down to individual bytes significantly increases the sequence length for long-tail tokens in languages such as Chinese, Japanese, and Korean (CJK) and other character-diverse contexts such as emoji. The increased sequence length results in longer computation during both training and inference. In this work, we propose a simple compression technique that reduces the sequence length losslessly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O6mNdNKhGy": {
    "title": "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries",
    "volume": "review",
    "abstract": "Plain biomedical summaries generation with Large Language Models (LLMs) can enhance the accessibility of biomedical knowledge to the public. However, how faithful the generated summaries are remains an open yet critical question. To address this, we propose FaReBio, a benchmark dataset with expert-annotated Faithfulness and Reasoning on plain Biomedical Summaries. This dataset consists of 175 plain summaries, including 1445 sentences generated by 7 different LLMs, paired with PubMed articles. Based on our dataset, we identify the performance gap of LLMs in generating faithful plain biomedical summaries and show the impact of abstractiveness on faithfulness. We show that current faithfulness metrics do not transfer well in the biomedical domain. To better understand the faithfulness judgements, we further benchmark LLMs in retrieving supporting evidence. Going beyond the binary faithfulness labels, coupled with the annotation of supporting sentences, our dataset could further contribute to the understanding of faithfulness evaluation and reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qXDppd1Xy3": {
    "title": "Trainable Attention-based Conditional Dependency for Uncertainty Quantification of Large Language Models",
    "volume": "review",
    "abstract": "Uncertainty quantification (UQ) is a perspective approach to detecting Large Language Model (LLM) hallucinations and low quality output. In this work, we address one of the challenges of UQ in generation tasks that arises from the conditional dependency between the generation steps of a LLM. We propose to learn this dependency from data. We train a regression model, which target variable is the gap between the conditional and the unconditional generation confidence. During LLM inference, we use this learned conditional dependency model to modulate the uncertainty of the current generation step based on the uncertainty of the previous step. Our experimental evaluation on nine datasets and three LLMs shows that the proposed method is highly effective for uncertainty quantification, achieving substantial improvements over rivaling approaches",
    "checked": false,
    "id": "ff4f8ad7853b81f189186ba97d63a609e1eface1",
    "semantic_title": "unconditional truthfulness: learning conditional dependency for uncertainty quantification of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6FXotcWvO2": {
    "title": "A Nested Watermark for Large Language Models",
    "volume": "review",
    "abstract": "The rapid development of large language models (LLMs) has raised concerns about the potential misuse of these models for generating fake news and misinformation. To mitigate this risk, watermarking techniques for auto-regressive language models have been proposed as a means of detecting text generated by LLMs. However, this method assumes that the target text, which is watermarked, contains a sufficient number of tokens, and the detection accuracy decreases as the number of tokens in the text becomes smaller. To address this issue, we introduce a novel nested watermark that embeds two watermarks in a nested structure. Our method ensures that high detection accuracy can be achieved even with fewer tokens compared to conventional approaches. Our experiments show that the nested watermark outperformed the single watermark in terms of embedding success ratio and text quality when dealing with short text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uHlOvD1sTt": {
    "title": "Dual Stream Alignment with Hierarchical Bottleneck Fusion For Multimodal Sentiment Analysis",
    "volume": "review",
    "abstract": "Multimodal sentiment analysis (MSA) leverages different modalities, such as text, image, and audio, for a comprehensive understanding of sentiment but faces challenges like temporal misalignment and modality heterogeneity. We propose a Dual-stream Alignment with Hierarchical Bottleneck Fusion (DAHB) method to address these issues. Our approach achieves comprehensive alignment through temporal alignment by cross-attention and semantic alignment via contrastive learning, ensuring alignment in time dimension and feature space. Moreover, Supervised contrastive learning is applied to refine these features. For modality fusion, we employ a hierarchical bottleneck method, progressively reducing bottleneck tokens to compress information and using bi-directional cross-attention to learn interactive between modalities. We conducted experiments on MOSI, MOSEI and CH-SIMS and results show that DAHB achieves state-of-the-art performance on a range of metrics. Ablation studies demonstrates the effectiveness of our methods. The code are available at url",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jshsqUvXXB": {
    "title": "BaFair: Backdoored Fairness Attacks with Group-conditioned Triggers",
    "volume": "review",
    "abstract": "Deep learning models have become essential in pivotal sectors such as healthcare, finance, and recruitment. However, they are not without risks; biases and unfairness inherent in these models could harm those who depend on them. Although there are algorithms designed to enhance fairness, the resilience of these models against hostile attacks, especially the emerging threat of Trojan (aka backdoor) attacks, is not thoroughly investigated. To bridge this research gap, we present *BajFair*, a Trojan fairness attack methodology. BaFair stealthily crafts a model that operates with accuracy and fairness under regular conditions but, when activated by certain triggers, discriminates and produces incorrect results for specific groups. This type of attack is particularly stealthy and dangerous as it circumvents existing fairness detection methods, maintaining an appearance of fairness in normal use. Our findings reveal that BaFair achieves a remarkable success rate of 88.7% in attacks aimed at targeted groups on average, while only incurring a minimal average accuracy loss of less than 1.2%. Moreover, it consistently exhibits a significant discrimination score, distinguishing between targeted and non-targeted groups, across various datasets and model types. **Content Warning**: This article only analyzes offensive language for academic purposes. Discretion is advised",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WzyoaiSfWc": {
    "title": "LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation",
    "volume": "review",
    "abstract": "Low-Rank Adaptation (LoRA) is currently the most commonly used Parameter-efficient fine-tuning (PEFT) method, it introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources. However, it still faces resource consumption challenges during training when scaling up to larger models. Most previous studies have tackled this issue by using pruning techniques, which involve removing LoRA parameters deemed unimportant. Nonetheless, these efforts only analyze LoRA parameter features to evaluate their importance, such as parameter count, size, and gradient. In fact, the output of LoRA (product of LoRA parameter and hidden state), directly impacts the final results. Preliminary experiments indicate that a fraction of LoRA elements possesses significantly high output values, substantially influencing the layer output. Motivated by the observation, we propose LoRA-drop. Concretely, LoRA-drop evaluates the importance of LoRA based on the LoRA output. Then we retain LoRA for important layers and the other layers share the same LoRA. We conduct abundant experiments with models of different scales on NLU and NLG tasks. Results demonstrate that LoRA-drop can achieve performance comparable to full fine-tuning and LoRA, while retaining 50\\% of the LoRA parameters on average",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F9p3fT9Wix": {
    "title": "Character is Destiny: Can Role-Playing Language Agents Make Persona-Driven Decisions?",
    "volume": "review",
    "abstract": "Can Large Language Models~(LLMs) simulate humans in making important decisions? Recent research has unveiled the potential of using LLMs to develop role-playing language agents (RPLAs), mimicking mainly the knowledge and tones of various characters. However, imitative decision-making necessitates a more nuanced understanding of personas. In this paper, we benchmark the ability of LLMs in persona-driven decision-making. Specifically, we investigate whether LLMs can predict characters' decisions provided by the preceding stories in high-quality novels. Leveraging character analyses written by literary experts, we construct a dataset LIFECHOICE comprising 1,462 characters' decision points from 388 books. Then, we conduct comprehensive experiments on LIFECHOICE, with various LLMs and RPLA methodologies. The results demonstrate that state-of-the-art LLMs exhibit promising capabilities in this task, yet substantial room for improvement remains. Hence, we further propose the CHARMAP method, which adopts persona-based memory retrieval and significantly advances RPLAs on this task, achieving 5.03% increase in accuracy. We will make our dataset and code publicly available",
    "checked": false,
    "id": "f52c7d32739afdee794159e78af74dc5a3386b95",
    "semantic_title": "character is destiny: can large language models simulate persona-driven decisions in role-playing?",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=J3LfXaS9Fa": {
    "title": "Task Oriented In-Domain Data Augmentation",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown superior performance in various applications and fields. To achieve better performance on specialized domains such as law and advertisement, LLMs are often continue pre-trained on in-domain data. However, existing approaches suffer from two major issues. First, in-domain data are scarce compared with general domain-agnostic data. Second, data used for continual pre-training are not task-aware, such that they may not be helpful to downstream applications. We propose TRAIT, a task-oriented in-domain data augmentation framework. Our framework is divided into two parts: in-domain data selection and task-oriented synthetic passage generation. The data selection strategy identifies and selects a large amount of in-domain data from general corpora, and thus significantly enriches domain knowledge in the continual pre-training data. The synthetic passages contain guidance on how to use domain knowledge to answer questions about downstream tasks. By training on such passages, the model aligns with the need of downstream applications. We adapt LLMs to two domains: advertisement and math. On average, TRAIT improves LLM performance by 8% in the advertisement domain and 7.5% in the math domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q3oNU7aafv": {
    "title": "Inter-Batch Cross-Attention: See More to Forget Less",
    "volume": "review",
    "abstract": "Our paper presents a simple training strategy to help prevent catastrophic forgetting in continual learners, named Inter-Batch Cross-Attention (IBCA). We discover that adding an IBCA module at the input level can significantly increase the model's continual learning performance, with minimum memory and performance overhead. Our method makes minimum changes to existing transformer-based model architectures and can be used in parallel with other continual learning strategies. We demonstrate its effectiveness on class-incremental classification tasks on the 20 Newsgroups dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l3FbLMvW3P": {
    "title": "Long Context Alignment with Short Instructions and Synthesized Positions",
    "volume": "review",
    "abstract": "Effectively handling instructions with extremely long context remains a challenge for Large Language Models (LLMs), typically necessitating high-quality long data and substantial computational resources. This paper introduces Step-Skipping Alignment (SkipAlign), a new technique designed to enhance the long-context capabilities of LLMs in the phase of alignment without the need for additional efforts beyond training with original data length. SkipAlign is developed on the premise that long-range dependencies are fundamental to enhancing an LLM's capacity of long context. Departing from merely expanding the length of input samples, SkipAlign synthesizes long-range dependencies from the aspect of positions indices. This is achieved by the strategic insertion of skipped positions within instruction-following samples, which utilizes the semantic structure of the data to effectively expand the context. Through extensive experiments on base models with a variety of context window sizes, SkipAlign demonstrates its effectiveness across a spectrum of long-context tasks. Particularly noteworthy is that with a careful selection of the base model and alignment datasets, SkipAlign with only 6B parameters achieves it's best performance and comparable with strong baselines like GPT-3.5-Turbo-16K on LongBench. The code and SkipAligned models will be open-sourced",
    "checked": true,
    "id": "ebc746fe7d2d580912498a98ec76a1e60020c95d",
    "semantic_title": "long context alignment with short instructions and synthesized positions",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=OildGoeWDP": {
    "title": "Preference Tuning For Toxicity Mitigation Generalizes Across Languages",
    "volume": "review",
    "abstract": "Detoxifying multilingual Large Language Models (LLMs) has become crucial due to their increasing global use. In this work, we explore zero-shot cross-lingual generalization of preference tuning in detoxifying LLMs. Unlike previous studies that show limited cross-lingual generalization for other safety tasks, we demonstrate that Direct Preference Optimization (DPO) training with only English data can significantly reduce toxicity in multilingual open-ended generations. For example, the probability of mGPT-1.3B generating toxic continuations drops from 46.8% to 3.9% across 17 different languages after training. Our results also extend to other multilingual LLMs, such as BLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal intervention and activation analysis, we identified the dual multilinguality property of MLP layers in LLMs, which explains the cross-lingual generalization of DPO. Finally, we show that bilingual sentence retrieval can predict the cross-lingual transferability of DPO preference tuning",
    "checked": true,
    "id": "6789a2f409af02915e48223e4be3e451ae7aa008",
    "semantic_title": "preference tuning for toxicity mitigation generalizes across languages",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=laBa2cmS1N": {
    "title": "The Impact of Auxiliary Patient Data on Automated Chest X-Ray Report Generation and How to Incorporate It",
    "volume": "review",
    "abstract": "This study investigates the integration of diverse patient data sources into multimodal language models for automated chest X-ray (CXR) report generation. Traditionally, CXR report generation relies solely on CXR images and limited radiology data, overlooking valuable information from patient health records, particularly from emergency departments. Utilising the MIMIC-CXR and MIMIC-IV-ED datasets, we incorporate detailed patient information such as aperiodic vital signs, medications, and clinical history to enhance diagnostic accuracy. We introduce a novel approach to transform these heterogeneous data sources into embeddings that prompt a multimodal language model, significantly enhancing the diagnostic accuracy of generated radiology reports. Our comprehensive evaluation demonstrates the benefits of using a broader set of patient data, underscoring the potential for enhanced diagnostic capabilities and better patient outcomes through the integration of multimodal data in CXR report generation",
    "checked": true,
    "id": "2fd9836f3848f8514875cdc0d4d7455737e32b22",
    "semantic_title": "the impact of auxiliary patient data on automated chest x-ray report generation and how to incorporate it",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=1qzSGLm6wI": {
    "title": "Unified Approach for more Generalizable Medical Language Understanding through Instruction Tuning",
    "volume": "review",
    "abstract": "Large language models (LLMs) such as ChatGPT are fine-tuned on large and diverse instruction-following corpora, and can generalize to new tasks. However, those instruction-tuned LLMs often perform poorly in specialized medical natural language understanding (NLU) tasks that require domain knowledge, granular text comprehension, and structured data extraction. To bridge the gap, we: (1) propose a unified prompting format for 7 important NLU tasks , (2) curate an instruction-tuning dataset, MNLU-Instruct, utilizing diverse existing open-source medical NLU corpora, and (3) develop BioMistral-NLU, a generalizable medical NLU model, through fine-tuning BioMistral on MNLU-Instruct. We evaluate BioMistral-NLU in a zero-shot setting, across 6 important NLU tasks, from two widely adopted medical NLU benchmarks: BLUE and BLURB. Our experiments show that our BioMistral-NLU outperforms the original BioMistral, as well as the proprietary LLMs - ChatGPT and GPT-4. Our dataset-agnostic prompting strategy and instruction tuning step over diverse NLU tasks enhance LLMs' generalizability across diverse medical NLU tasks. Our ablation experiments show that instruction-tuning on a wider variety of tasks, even when the total number of training instances remains constant, enhances downstream zero-shot generalization",
    "checked": false,
    "id": "1cb566743319bbb09cd11b45c51989c8dff25180",
    "semantic_title": "medxchat: a unified multimodal large language model framework towards cxrs understanding and generation",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=9yeUsKoDe0": {
    "title": "Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models",
    "volume": "review",
    "abstract": "We present Multi-expert Prompting, an enhanced extension of ExpertPrompting (Xu et al., 2023), which efficiently guides a large language model (LLM) to fulfill an input instruction by simulating multiple expert behaviors. Multi-expert Prompting synthesizes and evaluates responses from these experts, selecting the best among individual and combined responses in a coherent chain of thoughts through our seven carefully designed subtasks based on the Nominal Group Technique (Ven and Delbecq, 1974). It is the pioneer in addressing the challenge of aggregating long-form answers from LLM expert agents within a single turn. Our evaluations demonstrate that Multi-expert Prompting significantly outperforms ExpertPrompting and comparable baselines in enhancing the truthfulness, factuality, informativeness, and usefulness of responses while reducing toxicity and hurtfulness. It further achieves state-of-the-art truthfulness by outperforming the best baseline by 8.69% with ChatGPT. Moreover, it is efficient, explainable, and highly adaptable to diverse scenarios, eliminating the need for manual prompt construction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LJ2mqF2xUd": {
    "title": "You Gotta be a Doctor, Lin\": An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations",
    "volume": "review",
    "abstract": "Social science research has shown that candidates with names indicative of certain races or genders often face discrimination in employment practices. Similarly, Large Language Models (LLMs) have demonstrated racial and gender biases in various applications. In this study, we utilize GPT-3.5-Turbo and Llama 3-70B-Instruct to simulate hiring decisions and salary recommendations for candidates with 320 first names that strongly signal their race and gender, across over 750,000 prompts. Our empirical results indicate a preference among these models for hiring candidates with White female-sounding names over other demographic groups across 40 occupations. Additionally, even among candidates with identical qualifications, salary recommendations vary by as much as 5% between different subgroups. A comparison with real-world labor data reveals inconsistent alignment with U.S. labor market characteristics, underscoring the necessity of risk investigation of LLM-powered systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zm5Cn6Yfcm": {
    "title": "Consecutive Batch Model Editing with HooK Layers",
    "volume": "review",
    "abstract": "As the typical retraining paradigm is unacceptably time- and resource-consuming, researchers are turning to model editing to find an effective way that supports both consecutive and batch scenarios to edit the model behavior directly. Despite all these practical expectations, existing model editing methods fail to realize all of them. Furthermore, the memory demands for such sequential model editing approaches tend to be prohibitive, frequently necessitating an external memory that grows incrementally over time. To cope with these challenges, we propose CoachHooK, a model editing method that simultaneously supports sequential and batch editing. CoachHooK is memory-friendly as it only needs a small amount of it to store several hook layers whose size remains unchanged over time. Experimental results demonstrate the superiority of our method over other batch-supportive model editing methods under both single-round and consecutive batch editing scenarios. Extensive analyses of CoachHooK have been conducted to verify the stability of our method over 1) the number of consecutive steps and 2) the number of editing instances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JRxfpIpb2W": {
    "title": "Multilingual Knowledge Editing with Language-Agnostic Factual Neurons",
    "volume": "review",
    "abstract": "Multilingual knowledge editing (MKE) aims to simultaneously revise factual knowledge across multilingual languages within large language models (LLMs). However, most existing MKE methods just adapt existing monolingual editing methods to multilingual scenarios, overlooking the deep semantic connections of the same factual knowledge between different languages, thereby limiting edit performance. To address this issue, we first investigate how LLMs represent multilingual factual knowledge and discover that the same factual knowledge in different languages generally activates a shared set of neurons, which we call language-agnostic factual neurons. These neurons represent the semantic connections between multilingual knowledge and are mainly located in certain layers. Inspired by this finding, we propose a new MKE method by locating and modifying Language-Agnostic Factual Neurons (LAFN) to simultaneously edit multilingual knowledge. Specifically, we first generate a set of paraphrases for each multilingual knowledge to be edited to precisely locate the corresponding language-agnostic factual neurons. Then we optimize the update values for modifying these located neurons to achieve simultaneous modification of the same factual knowledge in multiple languages. Experimental results on Bi-ZsRE and MzsRE benchmarks demonstrate that our method outperforms existing MKE methods and achieves remarkable edit performance, indicating the importance of considering the semantic connections among multilingual knowledge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ktws1BPlKg": {
    "title": "MMOE: Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts",
    "volume": "review",
    "abstract": "Advances in multimodal models have greatly improved how interactions relevant to various tasks are modeled. Today's models mainly focus on the correspondence between images and text, using this for tasks like image captioning and image-text retrieval. However, this covers only a subset of real-world interactions. Novel interactions, such as sarcasm expressed through opposing spoken words and gestures or figurative descriptions of images, remain challenging. In this paper, we introduce an approach to enhance multimodal models, which we call Multimodal Mixtures of Experts (MMoE). The key idea in MMoE is to train separate expert models for each type of interaction, such as redundancy present in both modalities, uniqueness in one modality, or varying degrees of synergy that emerge when both modalities are fused. On two multimodal sarcasm datasets, we obtain new state-of-the-art results. MMoE also provides the opportunity to design smaller specialized experts, and improves the transparency of the modeling process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4bRTAKd1zz": {
    "title": "Mitigating Bias in LLMs via EquiSync: A Multi-Objective Optimization Perspective",
    "volume": "review",
    "abstract": "The field of Natural Language Processing (NLP) has seen remarkable advancements in Large Language Models (LLMs). Despite these advancements, a persistent challenge remains: LLMs often produce biased outputs. This paper introduces EquiSync, a novel method designed to mitigate social bias in LLMs without significantly compromising their performance. EquiSync utilizes a multi-agent framework, incorporating three agents that employ a two-phase approach: Attributes Masking and Attributes Balancing. This method aligns with human values transparently and reduces disparities between social groups. Unlike traditional debiasing techniques, which often lead to performance degradation, EquiSync achieves substantial bias reduction while maintaining or even improving accuracy in downstream tasks. Our experiments demonstrate that EquiSync reduces bias scores by up to 87.7%, with only a marginal performance degradation of up to 6.8% in the BBQ dataset. Additionally, it significantly enhances the multi-objective metric icat in the stereoset dateset by up to 56.98%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8OrxBZI2zO": {
    "title": "LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users",
    "volume": "review",
    "abstract": "While state-of-the-art Large Language Models (LLMs) have shown impressive performance on many tasks, there has been extensive research on undesirable model behavior such as hallucinations and bias. In this work, we investigate how the quality of LLM responses changes in terms of information accuracy, truthfulness, and refusals depending on three user traits: English proficiency, education level, and country of origin. We present extensive experimentation on three state-of-the-art LLMs and two different datasets targeting truthfulness and factuality. Our findings suggest that undesirable behaviors in state-of-the-art LLMs occur disproportionately more for users with lower English proficiency, of lower education status, and originating from outside the US, rendering these models unreliable sources of information towards their most vulnerable users",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xFOowEWHUJ": {
    "title": "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations",
    "volume": "review",
    "abstract": "Large Language Models prompting, such as using in-context demonstrations, is a mainstream technique for invoking LLMs to perform high-performance and solid complex reasoning (e.g., mathematical reasoning, commonsense reasoning), and has the potential for further human-machine collaborative scientific findings. However, current LLMs are delicate and elusive in prompt words and styles. And there is an unseen gap between LLM understanding and human-written prompts. This paper introduces **AlignedCoT**, an LLM-acquainted prompting technique that includes proficient \"native-speaking\" in in-context learning for the LLMs. Specifically, it achieves consistent and correct step-wise prompts in zero-shot scenarios by progressively probing, refining, and formatting the LLM chain of thoughts so that free from handcrafted few-shot demonstrations while maintaining the prompt quality. We conduct experiments on mathematical reasoning and commonsense reasoning. We find that LLMs with **AlignedCoT** perform significantly superior to them with human-crafted demonstrations. We further apply **AlignedCoT** for rewriting the GSM8k training set, resulting in a *GSM8k-Align* dataset. We observe its benefits for retrieval augmented generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vYgIRMRn03": {
    "title": "On the In-context Generation of Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) are found to have the ability of in-context generation (ICG): when they are fed with an in-context prompt containing a somehow similar examples, they can implicitly discover the pattern of them and then complete the prompt in the same pattern. ICG is curious, since language models are not completely trained in the way same as the in-context prompt, and the distribution of examples in the prompt differs from that of sequences in the pretrained corpora. This paper provides a systematic study of the ICG ability of language models, covering discussions about its source and influential factors, in the view of both theory and empirical experiments. Concretely, we first propose a plausible latent variable model to describe the distribution of the pretrained corpora, and then formalize ICG as a problem of next topic prediction. With this framework, we can prove that the repetition nature of a few topics ensures the ICG ability on them theoretically. Then, we use this controllable pretrained distribution to generate several medium-scale synthetic datasets (token scale: 2.1B-3.9B) and experiment with different settings of Transformer architectures (parameter scale: 4M-234M). Our experimental results further offer insights into how factors of data and model architectures influence ICG",
    "checked": false,
    "id": "be331430d8c2ed63e0e31375524f6d2118746b82",
    "semantic_title": "automated inspection report generation using multimodal large language models and set-of-mark prompting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NkBPuFOyPe": {
    "title": "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization",
    "volume": "review",
    "abstract": "Recent breakthroughs in preference alignment have significantly improved Large Language Models' ability to generate texts that align with human preferences and values. However, current alignment metrics typically emphasize the post-hoc overall improvement, while overlooking a critical aspect: $\\textit{regression}$, which refers to the backsliding on previously correctly-handled data after updates. This potential pitfall may arise from excessive fine-tuning on already well-aligned data, which subsequently leads to over-alignment and degeneration. To address this challenge, we propose $\\textit{FlipGuard}$, a constrained optimization approach to detect and mitigate update regression with focal attention. Specifically, FlipGuard identifies performance degradation using a customized reward characterization and strategically enforces a constraint to encourage conditional congruence with the pre-aligned model during training. Comprehensive experiments demonstrate that FlipGuard effectively alleviates update regression while demonstrating excellent overall performance, with the added benefit of knowledge preservation while aligning preferences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3zDPZm7YNY": {
    "title": "Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey",
    "volume": "review",
    "abstract": "Rationality is the quality of being guided by reason, characterized by decision-making aligned with evidence and logical rules. This quality is essential for effective problem-solving, as it ensures that solutions are well-founded and consistently derived. Despite the advancements of large language models (LLMs) in generating human-like texts with remarkable accuracy, they present limited knowledge space, inconsistency across contexts, and difficulty understanding complex scenarios. Therefore, recent research focuses on building multi-modal and multi-agent systems to achieve considerable progress with enhanced consistency and reliability, instead of relying on a single LLM as the sole planning or decision-making agent. To that end, this paper aims to understand whether multi-modal and multi-agent systems are advancing toward rationality by surveying the state-of-the-art works, identifying advancements over single-agent and single-modal systems in terms of rationality, and discussing open problems and future directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uZ4NJWOuwA": {
    "title": "BCoQA: Benchmark and Resources for Bangla Context-based Conversational Question Answering",
    "volume": "review",
    "abstract": "Developing a Bangla Context-based Conversational Question Answering (CCQA) system presents unique challenges, including limited domain-specific data, inadequate translation methods, and a lack of pretrained language models. In this work, we address these obstacles by constructing a robust Bangla CCQA dataset through quality controlled machine translation and LLM based augmentation of established English CCQA datasets, followed by partitioning into training, validation and test splits. We finetune and then evaluate the performance of various existing sequence-to-sequence models using the train and test split respectively, by appending conversation history into the input prompt to preserve context. The entire dataset and the testing script have been made publicly available on GitHub for benchmarking future models. This initiative marks a significant step in advancing conversational AI for Bangla, setting a foundation for further research and development in the field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VZ6IwTdiZV": {
    "title": "Fast Forwarding Low-Rank Training",
    "volume": "review",
    "abstract": "Parameter efficient finetuning methods like low-rank adaptation (LoRA) aim to reduce the computational costs of finetuning pretrained Language Models (LMs). Enabled by these low-rank settings, we propose an even more efficient optimization strategy: Fast Forward, a simple and effective approach to accelerate large segments of SGD training. In a Fast Forward stage, we repeat the most recent optimizer step until the loss stops improving on a tiny validation set. By alternating between regular optimization steps and Fast Forward stages, Fast Forward provides up to an 87\\% reduction in FLOPs over standard SGD with Adam. We validate Fast Forward by finetuning various models on different tasks and demonstrate that it speeds up training without compromising model performance. Additionally, we analyze when and how to apply Fast Forward",
    "checked": false,
    "id": "ac5d0faa5b07c884e21f423b8c4f22b3c1c3a01a",
    "semantic_title": "lora-ga: low-rank adaptation with gradient approximation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=bixDcyuC3w": {
    "title": "Out-of-Distribution Detection through Soft Clustering with Non-Negative Kernel Regression",
    "volume": "review",
    "abstract": "As language models become more general purpose, increased attention needs to be paid to detecting out-of-distribution (OOD) instances, i.e., those not belonging to any of the distributions seen during training. Existing methods for detecting OOD data are computationally complex and storage-intensive. We propose a novel soft clustering approach for OOD detection based on non-negative kernel regression. Our approach greatly reduces computational and space complexities (up to $11\\times $ improvement in inference time and 87% reduction in storage requirements) and outperforms existing approaches by up to 4 AUROC points on four different benchmarks. We also introduce an entropy-constrained version of our algorithm, which leads to further reductions in storage requirements (up to 97\\% lower than comparable approaches) while retaining competitive performance. Our soft clustering approach for OOD detection highlights its potential for detecting tail-end phenomena in extreme-scale data settings",
    "checked": true,
    "id": "12f04ecb1c9a76bed28656e1cb178c1b97eb7506",
    "semantic_title": "out-of-distribution detection through soft clustering with non-negative kernel regression",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Ysv9j5nMw": {
    "title": "Sing it, Narrate it: Quality Musical Lyrics Translation",
    "volume": "review",
    "abstract": "Translating lyrics for musicals presents unique challenges due to the need to ensure high translation quality while adhering to singability requirements such as length and rhyme. Existing song translation approaches often prioritize these singability constraints at the expense of translation quality, which is crucial for musicals. This paper aims to enhance translation quality while maintaining key singability features. Our method consists of three main components. First, we create a dataset to train reward models for the automatic evaluation of translation quality. Second, to enhance both singability and translation quality, we implement a two-stage training process with filtering techniques. Finally, we introduce an inference-time optimization framework for translating entire songs. Extensive experiments, including both automatic and human evaluations, demonstrate significant improvements over baseline methods and validate the effectiveness of each component in our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nIf4iC9dXR": {
    "title": "Grounding Language in Multi-Perspective Referential Communication",
    "volume": "review",
    "abstract": "We introduce a task and dataset for referring expression generation and comprehension in multi-agent embodied environments. In this task, two agents in a shared scene must take into account one another's visual perspective, which may be different from their own, to both produce and understand references to objects in a scene and the spatial relations between them. We collect a dataset of 2,970 human-written referring expressions, each paired with human comprehension judgments, and evaluate the performance of automated models as speakers and listeners paired with human partners, finding that model performance in both reference generation and comprehension lags behind that of pairs of human agents. Finally, we experiment training an open-weight speaker model with evidence of communicative success when paired with a listener, resulting in an improvement from 59.7 to 69.2\\% in communicative success and even outperforming the strongest proprietary model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fYEaVbm1VV": {
    "title": "Boosting Translation Capabilities of Large Language Models with Code-Switching Pretraining",
    "volume": "review",
    "abstract": "Recently, there has been significant attention on adapting the translation capabilities of Large Language Models. Represented by ALMA, a two-stage training recipe has been developed: first, utilizing a large amount of monolingual data for pretraining to enhance proficiency in non-English languages, followed by fine-tuning with a small amount of high-quality bilingual data. However, in the pretraining process, explicit cross-lingual alignment information is not provided, and excessive use of bilingual data can lead to catastrophic forgetting issues, both of which hinder the further advancement of the model's translation abilities. In this article, we address this issue by introducing a new pretraining process based on Code-Switching pretraining data. In this stage of pretraining, we can provide rich cross-lingual alignment information while ensuring that the training data is semantically coherent documents, which helps alleviate catastrophic forgetting. Moreover, the training process relies solely on monolingual data and a pair of traditional machine translation models, making it highly versatile. Experimental results show that our method has improved the translation quality, achieving state-of-the-art results in similar works",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=daU3GzIQ58": {
    "title": "LLaSA: Large Multimodal Agent for Human Activity Analysis Through Wearable Sensors",
    "volume": "review",
    "abstract": "Integrating inertial measurement units (IMUs) with large language models (LLMs) advances multimodal AI by enhancing human activity understanding. We introduce SensorCaps, a dataset of 26,288 IMU-derived activity narrations, and OpenSQA, an instruction-following dataset with 257,562 question-answer pairs. Combining LIMU-BERT and LLaMA, we develop LLaSA, a Large Multimodal Agent capable of interpreting and responding to activity and motion analysis queries. Our evaluation demonstrates LLaSA's effectiveness in activity classification and question answering, highlighting its potential in healthcare, sports science, and human-computer interaction. These contributions advance sensor-aware language models and open new research avenues",
    "checked": true,
    "id": "973967ce7f4c011794100119dcc7ed76476c6b22",
    "semantic_title": "llasa: large multimodal agent for human activity analysis through wearable sensors",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=KG93mcIsYv": {
    "title": "A Critical Survey on LLM Deployment Paradigms: Assessing Usability and Cognitive Behavioral Aspects",
    "volume": "review",
    "abstract": "Over the last decade, a wide range of training and deployment strategies for Large Language Models (LLMs) have emerged. Among these, the prompting paradigms of Auto-Regressive LLMs (AR-LLMs) have catalyzed a significant surge. This paper embarks on a quest to unravel the underlying factors behind the triumph of AR-LLMs' prompting paradigm. This study summarizes and focuses on six distinct task-oriented channels, e.g., numeric prefixes and free-form text, across diverse deployment paradigms By pivoting our focus onto these channels, we can assess these paradigms across crucial dimensions, such as task customizability, transparency, and complexity to gauge LLMs. The results emphasize the significance of utilizing free-form contexts as user-directed channels for downstream deployment. Moreover, we examine the stimulation of diverse cognitive behaviors in LLMs through the adoption of free-form, verbal outputs and inputs as contexts. We detail four common cognitive behaviors to underscore how AR-LLMs' prompting successfully imitates human-like behaviors under the free-form modality and channel",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kLwY5X4ovF": {
    "title": "Global Prefix-Tuning: Extremely Efficient Fine-Tuning for Shallow Alignment Using One Token",
    "volume": "review",
    "abstract": "We introduce \\textit{Global-Token Tuner}, an extremely parameter-efficient fine-tuning (PEFT) method for adapting Large Language Models (LLMs) that uses only a few or just one learnable token, regardless of model size. Global-Token Tuner employs a unique design that constructs a globally shared set of tunable tokens that modify the attention of every layer. Therefore no matter how base model change the tunable parameter remains relatively constant. We showed that our method can attain comparable performance with LoRA across plenty of common tasks while reducing parameter requirements from multiple millions or more to as few as 5 thousand. We also believe the discovery that even one token can effectively finetune LLMs illuminates the inner workings of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IEB3MU0t3j": {
    "title": "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs",
    "volume": "review",
    "abstract": "In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. Localizing and bringing users' attention to the specific problematic content is also paramount, instead of providing simple blanket labels. In this paper, we present ClaimVer, a human-centric framework tailored to meet users' informational and verification needs by generating rich annotations and thereby reducing cognitive load. Designed to deliver comprehensive evaluations of texts, it highlights each claim, verifies it against a trusted knowledge graph (KG), presents the evidence, and provides succinct, clear explanations for each claim prediction. Finally, our framework introduces an attribution score, enhancing applicability across a wide range of downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PUBMcARDwJ": {
    "title": "PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes",
    "volume": "review",
    "abstract": "Multimodal Large Language Models (MLLMs) have seen growing adoption across various scientific disciplines. These advancements encourage the investigation of molecule-text modeling within synthetic chemistry, a field dedicated to designing and conducting chemical reactions to synthesize new compounds with desired properties and applications. Current approaches, however, often neglect the critical role of multi-molecule graph interaction in understanding chemical reactions, leading to suboptimal performance in synthetic chemistry tasks. This study introduces PRESTO (Progressive Pretraining Enhances Synthetic Chemistry Outcomes), a new framework that bridges the molecule-text modality gap by integrating a comprehensive benchmark of pretraining strategies and dataset configurations. It progressively improves multimodal LLMs through cross-modal alignment and multi-graph understanding. Our extensive experiments demonstrate that PRESTO offers competitive results in downstream synthetic chemistry tasks. The code can be found at https://anonymous.4open.science/r/presto",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5LL5TyTMfV": {
    "title": "SmellDetector: Multi-Label Code Smell Detection and Refactoring with Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in many tasks such as code generation and automated program repair. However, code LLMs have ignored another important task in programmers' daily development work, which is to improve the maintainability, readability, and scalability of the program. All of these characteristics are related to code smells and we study how to improve them by detecting and removing code smells. Most works on code smells still rely on using measures formulated by experts as features, but lack of use of the rich prior knowledge contained in code LLMs. In this paper, we propose SmellDetector, a comprehensive model for both code smell detection and refactoring opportunities detection in Java. We train the model with the designed prompt which contains both code smells of class-level and method-level in the same code snippet, including more than 20 types. We achieve state-of-the-art performance on the code smell detection task and change the basic paradigm of code smell detection from binary classification problem to multi-label classification. Finally, it has been verified through experiments that good code smell detection helps to detect refactoring opportunities",
    "checked": false,
    "id": "ae75767ae41abca0186d9e6ef10d34d3602c0e27",
    "semantic_title": "smelldetector: code smell detection and refactoring with large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvdrkuUbUg": {
    "title": "Studying Differential Mental Health Expressions in India",
    "volume": "review",
    "abstract": "Psychosocial stressors and the symptomatology of mental disorders vary across cultures. Mental health expressions on social media, however, are primarily informed by studies in the WEIRD (Western, Educated, Industrial, Rich, and Democratic) contexts. In this paper, we analyze mental health posts on Reddit made by individuals in India, to identify variations in online depression language specific to the Indian context compared to users from the Rest of the World (ROW). Unlike in Western samples, mental health discussions in India additionally express sadness, use negation, are present-focused, and are related to work and achievement. Illness is exclusively correlated to India, reaffirming the link between somatic symptoms and mental disorders in Indian patients. Two clinical psychologists validated the findings from social media posts and found 95% of the top-20 topics associated with mental health discussions as prevalent in Indians. Significant linguistic variations in online mental health-related language in India compared to ROW, highlight the need for precision culturally-aware mental health models. These findings have important implications for designing culturally appropriate interventions to reduce the growing diagnosis and treatment gap for mental disorders in India",
    "checked": true,
    "id": "7798dc4f3fd72807d90219df0b95407a174be29a",
    "semantic_title": "studying differential mental health expressions in india",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=51faGPIG0o": {
    "title": "Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on pre-defined task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose GeM-CoT, a Generalizable CoT prompting mechanism in Mixed-task scenarios where the type of input questions is unknown. GeM-CoT first categorizes the question type and subsequently samples or constructs demonstrations from the corresponding data pool in an automatic pattern. With this technical design, GeM-CoT simultaneously enjoys superior generalization capabilities and remarkable performances on 10 public reasoning tasks and 23 BBH tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VnDTbu0WB0": {
    "title": "From Traits to Empathy: Personality-Aware Multimodal Empathetic Response Generation",
    "volume": "review",
    "abstract": "Empathetic dialogue systems improve the user experience across various domains. Existing approaches mainly focus on acquiring affective and cognitive information from text, often neglecting the unique personality traits of individuals and the inherently multimodal nature of human conversation. To this end, we propose enhancing dialogue systems with the ability to generate customized empathetic responses, considering the diverse personality traits of speakers, and we advocate for the incorporation of multimodal data analysis to gain a more detailed comprehension of speakers' emotional states and context. Specifically, we initially identify the speaker's trait across the context. The dialogue system then comprehends the speaker's emotion and situation by emotion perception through the analysis of multimodal inputs. Finally, the response generator models the correlations among the captured personality, emotion, and multimodal data, thereby generating empathetic responses. Extensive experiments are conducted utilizing the MELD dataset and the IEMOCAP dataset to investigate the influence of personality traits on empathetic response generation and validate the effectiveness of the proposed approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3HMbUj3bo4": {
    "title": "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering",
    "volume": "review",
    "abstract": "While large pre-trained visual-language models have shown promising results on traditional visual question answering benchmarks, it is still challenging for them to answer complex VQA problems which requires diverse world knowledge. Motivated by the research of retrieval-augmented generation in the field of natural language processing, we use Dense Passage Retrieval (DPR) to retrieve related knowledge to help the model answer questions. However, DPR conduct retrieving in natural language space, which may not ensure comprehensive acquisition of image information. Thus, the retrieved knowledge is not truly conducive to helping answer the question, affecting the performance of the overall system. To address this issue, we propose a novel framework that leverages the visual-language model to select the key knowledge retrieved by DPR and answer questions. The framework consists of two modules: Selector and Answerer, where both are initialized by the MLLM and parameter-efficiently finetuned by self-bootstrapping: find key knowledge in the retrieved knowledge documents using the Selector, and then use them to finetune the Answerer to predict answers; obtain the pseudo-labels of key knowledge documents based on the predictions of the Answerer and weak supervision labels, and then finetune the Selector to select key knowledge; repeat. Our framework significantly enhances the performance of the baseline on the challenging open-domain Knowledge-based VQA benchmark, OK-VQA, achieving a state-of-the-art accuracy of 62.83\\%",
    "checked": true,
    "id": "6b1da517a8699f0a62368d032fda3311871372c5",
    "semantic_title": "self-bootstrapped visual-language model for knowledge selection and question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3r04gGOnxd": {
    "title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding",
    "volume": "review",
    "abstract": "Graphical User Interfaces (GUIs) are central to our interaction with digital devices. Recently, growing efforts have been made to build models for various GUI understanding tasks. However, these efforts largely overlook an important GUI-referring task: screen reading based on user-indicated points, which we name the Screen Point-and-Read (SPR) task. This task is predominantly handled by rigid accessible screen reading tools, in great need of new models driven by advancements in Multimodal Large Language Models (MLLMs). In this paper, we propose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism, to address the SPR task. Based on the input point coordinate and the corresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout Tree. Based on the tree, our ToL agent not only comprehends the content of the indicated area but also articulates the layout and spatial relationships between elements. Such layout information is crucial for accurately interpreting information on the screen, distinguishing our ToL agent from other screen reading tools. We also thoroughly evaluate the ToL agent against other baselines on a newly proposed SPR benchmark, which includes GUIs from mobile, web, and operating systems. Last but not least, we test the ToL agent on mobile GUI navigation tasks, demonstrating its utility in identifying incorrect actions along the path of agent execution trajectories",
    "checked": true,
    "id": "50a436fed0a4a996c7dd4edc084343645e8d4dcd",
    "semantic_title": "read anywhere pointed: layout-aware gui screen reading with tree-of-lens grounding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zCHTrb9c1B": {
    "title": "GoldCoin: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory",
    "volume": "review",
    "abstract": "Privacy issues arise prominently during the inappropriate transmission of information between entities. Existing research primarily studies privacy by exploring various privacy attacks, defenses, and evaluations within narrowly predefined patterns, while neglecting that privacy is not an isolated, context-free concept limited to traditionally sensitive data (e.g., social security numbers), but intertwined with intricate social contexts that complicate the identification and analysis of potential privacy violations. The advent of Large Language Models (LLMs) offers unprecedented opportunities for incorporating the nuanced scenarios outlined in privacy laws to tackle these complex privacy issues. However, the scarcity of open-source relevant case studies restricts the efficiency of LLMs in aligning with specific legal statutes. To address this challenge, we introduce a novel framework, GoldCoin, designed to efficiently ground LLMs in privacy laws for judicial assessing privacy violations. Our framework leverages the theory of contextual integrity as a bridge, creating numerous synthetic scenarios grounded in relevant privacy statutes (e.g., HIPAA), to assist LLMs in comprehending the complex contexts for identifying privacy risks in the real world. Extensive experimental results demonstrate that GoldCoin markedly enhances LLMs' capabilities in recognizing privacy risks across real court cases, surpassing the baselines on different judicial tasks",
    "checked": true,
    "id": "acc79fcf42b64234c3aa48ba7eb4b21cd9271f51",
    "semantic_title": "goldcoin: grounding large language models in privacy laws via contextual integrity theory",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=9J0gFCp8At": {
    "title": "The Generation Gap: Exploring Age Bias Underlying in the Value Systems of Large Language Models",
    "volume": "review",
    "abstract": "We explore the alignment of values in Large Language Models (LLMs) with specific age groups, leveraging data from the World Value Survey across thirteen categories. Through a diverse set of prompts tailored to ensure response robustness, we find a general inclination of LLM values towards younger demographics, especially in the US. Additionally, we explore the impact of incorporating age identity information in prompts and observe challenges in mitigating value discrepancies with different age cohorts. Our findings highlight the age bias in LLMs and provide insights for future work. Materials for our analysis will be available via \\url{anonymous.github.com}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YqSXeZfXCg": {
    "title": "Evaluating Fairness in Large Vision-Language Models Across Diverse Demographic Attributes and Prompts",
    "volume": "review",
    "abstract": "Large vision-language models (LVLMs) have recently achieved significant progress, demonstrating strong capabilities in open-world visual understanding. However, it is not yet clear how LVLMs address demographic biases in real life, especially the disparities across attributes such as gender, skin tone, and age. In this paper, we empirically investigate \\emph{visual fairness} in several mainstream LVLMs and audit their performance disparities across sensitive demographic attributes, based on public fairness benchmark datasets (e.g., FACET). To disclose the visual bias in LVLMs, we design a fairness evaluation framework with direct questions and single-choice question-instructed prompts on visual question-answering/classification tasks. The zero-shot prompting results indicate that, despite enhancements in visual understanding, both open-source and closed-source LVLMs exhibit prevalent fairness issues across different instruct prompts and demographic attributes",
    "checked": true,
    "id": "84838d69335e690d63dde98175fa76f87028af6a",
    "semantic_title": "evaluating fairness in large vision-language models across diverse demographic attributes and prompts",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fbhlQzY350": {
    "title": "Indic-MULAN: A Study of Fact Mutability in Language Models for Low-resource Indian languages",
    "volume": "review",
    "abstract": "Large language models(LLMs) open up new possibilities for Low-resource languages(LRLs) by showing impressive results in various classification and generation tasks with zero/few-shot inference. Due to pre-training from large datasets, including web-based corpora, these LLMs have the knowledge of factual information, and factual knowledge can both be time-dependent and independent. Having said that, LLM's capability of extracting factual information from LLMs involving LRLs is an interesting task, though not much explored. In this work, we present Indic-MULAN, a benchmark dataset to evaluate LLM's capability to extract time-aware factual knowledge involving one-to-one and one-to-many relations. Our dataset comprises 34 relations and $\\sim$30K queries covering 11 Indian languages. We experimented with two LLMs, GPT-4(proprietary) and Llama-3(open-source). We find performance is poor when queried with native languages but improves when translated to English. Then, we do a brief analysis of the embedding space using t-SNE plots, which leads to some interesting observations. We hope Indic-MULAN will help future studies of LLMs involving time-aware factual knowledge in Indian languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EchbXYM0DN": {
    "title": "Multi-Granularity History and Entity Similarity Learning for Temporal Knowledge Graph Reasoning",
    "volume": "review",
    "abstract": "Temporal Knowledge Graph (TKG) reasoning, aiming to predict future unknown facts based on historical information, has attracted considerable attention due to its great practical value. Insight into history is the key to predict the future. However, most existing TKG reasoning models singly capture repetitive history, ignoring the entity's multi-hop neighbour history which can provide valuable background knowledge for TKG reasoning. In this paper, we propose $\\textbf{M}$ulti-$\\textbf{G}$ranularity History and $\\textbf{E}$ntity $\\textbf{S}$imilarity $\\textbf{L}$earning (MGESL) model for Temporal Knowledge Graph Reasoning, which models historical information from both coarse-grained and fine-grained history. Since similar entities tend to exhibit similar behavioural patterns, we also design a hypergraph convolution aggregator to capture the similarity between entities. Furthermore, we introduce a more realistic setting for the TKG reasoning, where candidate entities are already known at the timestamp to be predicted. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed model",
    "checked": true,
    "id": "d9cbe6cdf9bc397d53f196b5846966960d4336ad",
    "semantic_title": "multi-granularity history and entity similarity learning for temporal knowledge graph reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K3cOWu0pVC": {
    "title": "Image-conditioned human language comprehension and psychometric benchmarking of visual language models",
    "volume": "review",
    "abstract": "Large language model (LLM)s' next-word predictions have shown impressive performance in capturing human expectations during real-time language comprehension. This finding has enabled a line of research on psychometric benchmarking of LLMs against human language-comprehension data in order to reverse-engineer humans' linguistic subjective probability distributions and representations. However, to date this work has exclusively involved unimodal (language-only) comprehension data, whereas much human language use takes place in rich multimodal contexts. Here we extend psychometric benchmarking to visual language models (VLMs). We develop a novel experimental paradigm, $\\text{\\emph{Image-Conditioned Maze Reading}}$, in which participants first view an image and then read a text describing an image within the Maze paradigm, yielding word-by-word reaction-time measures with high signal-to-noise ratio and good localization of expectation-driven language processing effects. We find a large facilitatory effect of correct image context on language comprehension, not only for words such as concrete nouns that are directly grounded in the image but even for ungrounded words in the image descriptions. Furthermore, we find that VLM surprisal captures most to all of this effect. We use these findings to benchmark a range of VLMs, showing that models with lower perplexity generally have better psychometric performance, but that among the best VLMs tested perplexity and psychometric performance dissociate. Overall, our work offers new possibilities for connecting psycholinguistics with multimodal LLMs for both scientific and engineering goals",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VmEFM6Jqtf": {
    "title": "Adapters Mixup: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers",
    "volume": "review",
    "abstract": "Existing works show that augmenting the training data of pre-trained language models (PLMs) for classification tasks fine-tuned via parameter-efficient fine-tuning methods (PEFT) using both clean and adversarial examples can enhance their robustness under adversarial attacks. However, this adversarial training paradigm often leads to performance degradation on clean inputs and requires frequent re-training on the entire data to account for new, unknown attacks. To overcome these challenges while still harnessing the benefits of adversarial training and the efficiency of PEFT, this work proposes a novel approach, called AdpMixup, that combines two paradigms: (1) fine-tuning through adapters and (2) adversarial augmentation via mixup to dynamically leverage existing knowledge from a set of pre-known attacks for robust inference. Intuitively, AdpMixup fine-tunes PLMs with multiple adapters with both clean and pre-known adversarial examples and intelligently mixes them up in different ratios during prediction. Our experiments show AdpMixup achieves the best trade-off between training efficiency and robustness under both pre-known and unknown attacks, compared to existing baselines on five downstream tasks across six varied black-box attacks and 2 PLMs. All source code will be available",
    "checked": true,
    "id": "e80da784ce4f4de96a1f215b751bc7efe38f6406",
    "semantic_title": "adapters mixup: mixing parameter-efficient adapters to enhance the adversarial robustness of fine-tuned pre-trained text classifiers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JGSh8i3LIr": {
    "title": "Discovering Meaningful Units with Visually Grounded Semantics from Image Captions",
    "volume": "review",
    "abstract": "Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oTLbk834Ph": {
    "title": "LLM-powered Context Augmentation for Heterogeneous Citation Networks",
    "volume": "review",
    "abstract": "Recent advances in large language models (LLMs) such as ChatGPT and Llama have driven significant progress in natural language processing and diverse AI applications. In this paper, we explore how LLMs can enhance the construction of heterogeneous citation networks by integrating rich contextual information derived from LLMs. We propose a novel approach that augments content-based feature engineering with context-aware techniques. Specifically, we queried the contents within the metadata using Llama3 to extract context, encoded this knowledge-rich context using the LLM encoder DeBERTa, and constructed a knowledge-rich heterogeneous citation network. Experimental results demonstrate that our LLM-powered context augmentation improves author classification by 2% to 24% and author clustering by 6% to 33%, compared with existing feature engineering approaches. The dataset and source code are available at https://anonymous.4open.science/r/LLM-citation-252F/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ULjRcdsEiU": {
    "title": "LLMs as NLP Researchers: Paper (Meta-)Reviewing as a Testbed",
    "volume": "review",
    "abstract": "This work is motivated by two key trends. On one hand, large language models (LLMs) have shown remarkable versatility in various generative tasks such as writing, drawing, and question answering, significantly reducing the time required for many routine tasks. On the other hand, researchers, whose work is not only time-consuming but also highly expertise-demanding, face increasing challenges as they have to spend more time reading, writing, and reviewing papers. This raises the question: how can LLMs potentially assist researchers in alleviating their heavy workload? This study focuses on the topic of LLMs as NLP Researchers, particularly examining how effectively LLMs can perform paper (meta-)reviewing. To address this, we constructed the ReviewCritique dataset, which includes two types of information: (i) NLP papers (initial submissions rather than camera-ready) with both human-written and LLM-generated reviews, and (ii) each review comes with \"deficiency\" labels and corresponding explanations for individual segments, annotated by experts. Using ReviewCritique, this study explores two threads of research questions: (i) \"LLMs as Reviewers\", how do reviews generated by LLMs compare with those written by humans in terms of quality and distinguishability? (ii) \"LLMs as Metareviewers\", how effectively can LLMs identify potential issues, such as Deficient or unprofessional review segments, within individual paper reviews? To our knowledge, this is the first work to provide such a comprehensive analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MHYcgeMGg9": {
    "title": "Beyond English-Centric Machine Translation by Multilingual Instruction Tuning Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on Machine Translation (MT) among various natural languages. However, many LLMs are English-dominant and only support some high-resource languages, they will fail on the non-English-Centric translation task. In this work, we propose a Multilingual Instruction Tuning (MLIT) method to improve the LLMs on non-English-Centric translation. We design a multilingual instruction method which leverage the English sentence as reference to help LLMs understand the source sentence. In order to solve the problem of difficulty in obtaining multilingual parallel corpora of low-resource languages, we train a to-English LLM to generate English reference so that our MLIT method only needs bilingual data. We experiment on LLaMA2 foundation and extensive experiments show that MLIT outperforms the baselines and some large-scale language models. We further demonstrate the importance of English reference in both training and inference processes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=naLNksUMDd": {
    "title": "Is In-Context Learning a Type of Gradient-Based Learning? Evidence from the Inverse Frequency Effect in Structural Priming",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown the emerging capability of in-context learning (ICL). One line of research has explained ICL as functionally performing gradient descent. In this paper, we introduce a new way of diagnosing whether ICL is functionally equivalent to gradient-based learning. Our approach is based on the inverse frequency effect (IFE) -- a phenomenon in which an error-driven learner is expected to show larger updates when trained on infrequent examples than frequent ones. The IFE has previously been studied in psycholinguistics because humans show this effect in the context of structural priming (the tendency for people to produce sentence structures they have encountered recently); the IFE has been used as evidence that human structural priming must involve error-driven learning mechanisms. In our experiments, we simulated structural priming within ICL and found that LLMs display the IFE, with the effect being stronger in larger models. We conclude that ICL is indeed a type of gradient-based learning, supporting the hypothesis that a gradient component is implicitly computed in the forward pass during ICL. Our results suggest that both humans and LLMs make use of gradient-based, error-driven processing mechanisms",
    "checked": true,
    "id": "5bbe613edf32189a662274213de06ff800c3f177",
    "semantic_title": "is in-context learning a type of gradient-based learning? evidence from the inverse frequency effect in structural priming",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=kWlUDTGCb9": {
    "title": "Cerberus: Efficient Inference with Adaptive Parallel Decoding and Sequential Knowledge Enhancement",
    "volume": "review",
    "abstract": "Large language models (LLMs) often face a bottleneck in inference speed due to their reliance on auto-regressive decoding. Recently, parallel decoding has shown significant promise in enhancing inference efficiency. However, we have identified two key issues with existing parallel decoding frameworks: (1) decoding heads fail to balance prediction accuracy and the parallelism of execution, and (2) parallel decoding is not a universal solution, as it can bring unnecessary overheads at some challenging decoding steps. To address these issues, we propose Cerberus, an adaptive parallel decoding framework introduces the gating mechanism to enable the LLMs to adaptively choose appropriate decoding approaches at each decoding step, along with introducing a new paradigm of decoding heads that introduce the sequential knowledge while maintaining execution parallelism. The experiment results demonstrate that the Cerberus can achieve up to 2.12x speed up compared to auto-regressive decoding, and outperforms one of the leading parallel decoding frameworks, Medusa, with a 10% - 30% increase in acceleration and superior generation quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ECCTVtJCae": {
    "title": "PRDetect: Perturbation-Robust LLM-generated Text Detection Based on Syntax Tree",
    "volume": "review",
    "abstract": "As LLM-generated text becomes increasingly prevalent on the internet, which may contain hallucinations or biases, detecting such content has emerged as a critical area of research. Recent methods have demonstrated impressive performance in detecting text generated entirely by LLMs. However, in real-world scenarios, users often make perturbations to the LLM-generated text, and the robustness of existing detection methods to these perturbations has not been sufficiently explored. This paper empirically investigates this question and finds that even minor perturbation can severely degrade the performance of current detection methods. To address this issue, we find that the syntactic tree is minimally affected by disturbances and exhibits differences between human-written text and LLM-generated text. Therefore, we propose a detection method based on syntactic trees, which can capture features invariant under perturbations. It demonstrate significantly improved robustness against perturbation on the HC3 and GPT-3.5-mixed datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oxYT7ztOt1": {
    "title": "PretextTrans: Investigating Medical Factual Knowledge Mastery of LLMs with Predicate-text Dual Transformation",
    "volume": "review",
    "abstract": "In the study, we aim to investigate current LLMs' mastery of medical factual knowledge with a dynamic evaluation schema, which can automatically generate multiple test samples for each medical factual knowledge point. Test samples produced directly by LLMs always introduce factual errors and lack diversity in the manner of knowledge expression. To overcome the drawbacks, here we propose a novel evaluation method, Predicate-text Dual Transformation (PretextTrans), by introducing predicate transformations into the dynamic evaluation schema. Specifically, each medical knowledge point is firstly transformed into a predicate expression; then, the predicate expression derives a series of variants through predicate transformations; lastly, the produced predicate variants are transformed back into textual expressions, resulting in a series of test samples with both factual reliability and expression diversity. Using the proposed PretextTrans method, we systematically investigate 12 well-known LLMs' mastery of medical factual knowledge based on two medical datasets. The comparison results show that current LLMs still have significant deficiencies in fully mastering medical knowledge, which may illustrate why current LLMs still perform unsatisfactorily in real-world medical scenarios despite having achieved considerable performance on public benchmarks. Our proposed method serves as an effective solution for evaluation of LLMs in medical domain and offers valuable insights for developing medical-specific LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5bHyRdZkZX": {
    "title": "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication",
    "volume": "review",
    "abstract": "Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs. Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication. Our code will be released to facilitate further exploration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=24CXJ2UEeB": {
    "title": "Reference (In-)Determinacy in Natural Language Inference",
    "volume": "review",
    "abstract": "Natural Language Inference (NLI) provides a general task format for evaluating the semantic relations between two pieces of text, which can be useful for various applications such as fact verification and text attribution. However, existing datasets for NLI and models trained on these datasets make assumptions about the context from which the premise and hypothesis are sampled. In this paper, we revisit this reference determinacy (RD) assumption in NLI, i.e., the premise and hypothesis are assumed to refer to the same context when human raters annotate a label. While RD is a practical assumption for constructing a new NLI dataset, we observe that current NLI modelsâ€”which are typically trained solely on hypothesis-premise pairs created with the RD assumptionâ€”fail in many practical settings in which the premise and hypothesis may refer to different contexts. To highlight the impact of this phenomenon in real-world use cases, we introduce the ReFNLI, a diagnostic benchmark for identifying reference ambiguity in NLI examples. In ReFNLI, the premise is retrieved from a knowledge source (i.e., Wikipedia) and does not necessarily refer to the same context as the hypothesis. With ReFNLI, we demonstrate that finetuned NLI models and few-shot prompted LLMs both fail to recognize context mismatch, leading to > 80% false contradiction and > 50% entailment predictions. We discover that the existence of reference ambiguity in NLI examples can in part explain the inherent human disagreements in NLI, and provide insight into how the RD assumption impacts NLI dataset creation process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qUKjgYIE05": {
    "title": "Behavioral Bias of Vision-Language Models: A Behavioral Finance View",
    "volume": "review",
    "abstract": "Large Vision-Language Models (LVLMs) evolve rapidly as Large Language Models (LLMs) was equipped with vision modules to create more human-like models. However, we should carefully evaluate their applications in different domains, as harmful biases may occur. Our work studies the potential behavioral biases of LVLMs from a behavioral finance perspective, an interdisciplinary area that jointly considers finance and psychology. We propose an end-to-end framework, from data collection to new evaluation metrics, to assess LVLM's reasoning capabilities and dynamic behaviors manifested in two established human financial behavioral biases: recency bias and authority bias. Our evaluations find that recent open-source LVLMs such as LLaVA-NeXT, MobileVLM-V2, Mini-Gemini, MiniCPM-Llama3-V 2.5 and Phi-3-vision suffer significantly from these two biases, while the proprietary model GPT-4o is negligibly impacted. This highlights a direction in which open-source models can improve",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1XnMteXc8T": {
    "title": "Residualized Similarity Prediction for Maintaining Interpretability in Authorship Verification",
    "volume": "review",
    "abstract": "Responsible use of authorship verification systems not only requires high accuracies but also interpretable solutions. Neural methods achieve high accuracies, but their representations lack direct interpretability, whereas methods using interpretable linguistic features generally perform worse than neural methods. In this paper, we introduce residualized similarity prediction (RSP), a novel method of supplementing systems using interpretable features with a neural network to improve their performance while maintaining interpretability. The key idea is to use the neural network to predict a residual similarity, i.e. the error in the similarity predicted by the interpretable system. Our evaluation on three datasets shows that using RSP improves authorship verification predictions over a fully interpretable system, multiple neural models, as well as weighted ensembles of these two (RSP yields gains in 17 of the 24 combinations), all while maintaining interpretability as measured using a new interpretability confidence metric",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JEHGDzuJLM": {
    "title": "Leveraging a Cognitive Model to Measure Subjective Similarity of Human and GPT-4 Written Content",
    "volume": "review",
    "abstract": "Cosine similarity between two documents can be computed using token embeddings formed by Large Language Models (LLMs) such as GPT-4, and used to categorize those documents across a range of uses. However, these similarities are ultimately dependent on the corpora used to train these LLMs, and may not reflect subjective similarity of individuals or how their biases and constraints impact similarity metrics. This lack of cognitively-aware personalization of similarity metrics can be particularly problematic in educational and recommendation settings where there is a limited number of individual judgements of category or preference, and biases can be particularly relevant. To address this, we rely on an integration of an Instance-Based Learning (IBL) cognitive model with LLM embeddings to develop the Instance-Based Individualized Similarity (IBIS) metric. This similarity metric is beneficial in that it takes into account individual biases and constraints in a manner that is grounded in the cognitive mechanisms of decision making. To evaluate the IBIS metric, we also introduce a dataset of human categorizations of emails as being either dangerous (phishing) or safe (ham). This dataset is used to demonstrate the benefits of leveraging a cognitive model to measure the subjective similarity of human participants in an educational setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sYeakNu6Ll": {
    "title": "Unveiling and Mitigating Bias in Mental Health Analysis with Large Language Models",
    "volume": "review",
    "abstract": "The advancement of large language models (LLMs) has demonstrated strong capabilities across various applications, including mental health analysis. However, existing studies have focused on predictive performance, leaving the critical issue of fairness underexplored, posing significant risks to vulnerable populations. Despite acknowledging potential biases, previous works have lacked thorough investigations into these biases and their impacts. To address this gap, we systematically evaluate biases across seven social factors (e.g., gender, age, religion) using ten LLMs with different prompting methods on eight diverse mental health datasets. Our results show that GPT-4 achieves the best overall balance in performance and fairness among LLMs, although it still lags behind domain-specific models like MentalRoBERTa in some cases. Additionally, our tailored fairness-aware prompts can effectively mitigate bias in mental health predictions, highlighting the great potential for fair analysis in this field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sVnBCIjMxr": {
    "title": "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning",
    "volume": "review",
    "abstract": "Path planning is a fundamental scientific problem in robotics and autonomous navigation, requiring the derivation of efficient routes from starting to destination points while avoiding obstacles. Traditional algorithms like A* and its variants are capable of ensuring path validity but suffer from significant computational and memory inefficiencies as the state space grows. Conversely, large language models (LLMs) excel in broader environmental analysis through contextual understanding, providing global insights into environments. However, they fall short in detailed spatial and temporal reasoning, often leading to invalid or inefficient routes. In this work, we propose LLM-A*, an new LLM based route planning method that synergistically combines the precise pathfinding capabilities of A* with the global reasoning capability of LLMs. This hybrid approach aims to enhance pathfinding efficiency in terms of time and space complexity while maintaining the integrity of path validity, especially in large-scale scenarios. By integrating the strengths of both methodologies, LLM-A* addresses the computational and memory limitations of conventional algorithms without compromising on the validity required for effective pathfinding",
    "checked": true,
    "id": "8bb5b517012530244497beb4d1b7257d3c76661c",
    "semantic_title": "llm-a*: large language model enhanced incremental heuristic search on path planning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H3QYUHI99M": {
    "title": "ARISE: Automatic Rule Induction and Filtering for Few-shot Text Classification",
    "volume": "review",
    "abstract": "We propose ARISE, a framework that combines weak supervision, synthetic data generation and contrastive representation learning for few-shot text classification (FSTC). Weak supervision forms a major novelty in ARISE. Here, we propose an automatic rule induction component to induce rules from syntactic-ngrams using inductive generalisation. The rules we induce capture syntactic information, often not explicitly captured by state of the art neural models. While these rules can be noisy, they are used to learn a label aggregation model with data programming. Subsequently, we jointly train the base classifier along with the label aggregation model to update their parameters. Unlike, past work that employ data programming to label unlabeled data-points, we use it for verifying synthetically generated labeled data. Finally, we combine synthetic data generation and automatic rule induction, via bootstrapping, to iteratively filter the generated rules and data. Our experiments with nine FSTC datasets over diverse domains, and multilingual experiments on seven languages, show consistent and statistically significant improvements for our proposed approach over other state-of-the-art approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iyvA2me61E": {
    "title": "Eliminating Retrieval Knowledge Conflicts: Cross-Validated Re-ranking with Large Language Models",
    "volume": "review",
    "abstract": "In retrieval-augmented generation systems, employing large language models for re-ranking has proven effective. However, existing work often prioritizes passage relevance over reliability, leading to the utilization of conflicting information and the generation of ambiguous answers. This is particularly problematic when dealing with inter-context knowledge conflicts, where candidate documents contain opposing information that can mislead the model. To address this issue, we introduce a novel cross-validation re-ranking technique that specifically resolves these inter-context knowledge conflicts during retrieval. We develope a new dataset, ContraPRT, specifically to test the model's ability to rank sets of passages containing conflicting knowledge. Results with GPT-4 and LlaMA3-70B demonstrate that our approach not only successfully filters out conflicting information but also ensures that the passage rankings are accurate, thus providing reliable supplementary knowledge for the generation module",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UlE0F3wfga": {
    "title": "Belief Revision: The Adaptability of Large Language Models Reasoning",
    "volume": "review",
    "abstract": "The capability to reason from text is crucial for real-world NLP applications. Real-world scenarios often involve incomplete or evolving data. In response, individuals update their beliefs and understandings accordingly. However, most existing evaluations assume that language models (LMs) operate with consistent information. We introduce Belief-R, a new dataset designed to test LMs' belief revision ability when presented with new evidence. Inspired by how humans suppress prior inferences, this task assesses LMs within the newly proposed delta reasoning ($\\Delta R$) framework. Belief-R features sequences of premises designed to simulate scenarios where additional information could necessitate prior conclusions drawn by LMs. We evaluate $\\sim$30 LMs across diverse prompting strategies and found that LMs generally struggle to appropriately revise their beliefs in response to new information. Further, models adept at updating often underperformed in scenarios without necessary updates, highlighting a critical trade-off. These insights underscore the importance of improving LMs' adaptiveness to changing information, a step toward more reliable AI systems",
    "checked": true,
    "id": "64e50707a5398ad0d0cb08cf4b372000f3e14562",
    "semantic_title": "belief revision: the adaptability of large language models reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0AKfifWzSK": {
    "title": "Generation with Dynamic Vocabulary",
    "volume": "review",
    "abstract": "Vocabulary is a crucial component of language models. Traditional language models generate text by selecting tokens from a fixed vocabulary. In this paper, we introduce a novel dynamic setting for the vocabulary. Under this setting, vocabulary can include arbitrary text spans on demand. These text spans act as basic bricks, akin to tokens in the fixed vocabulary. Our proposed model can be deployed in a way of plug-and-play. Extensive experimental results demonstrate that our approach yields superior generation quality. For instance, compared to the standard language model, the MAUVE metric increases from 20.47 $\\%$ to 25.69$\\%$. We also demonstrate that dynamic vocabulary can be effectively applied to different domains in a training-free manner, and it also helps to generate reliable citations in question answering tasks (substantially enhancing citation results without compromising answer accuracy)",
    "checked": false,
    "id": "09a0f9d84ff6102f924782b5286c4fbe91d7a7a0",
    "semantic_title": "drug discovery with dynamic goal-aware fragments",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=b9jsMszNCi": {
    "title": "Do they mean us? Interpreting Referring Expressions in Intergroup Bias",
    "volume": "review",
    "abstract": "The variations between in-group and out-group speech (intergroup bias) are subtle and could underlie many social phenomena like stereotype perpetuation and implicit bias. In this paper, we model the intergroup bias as a tagging task on English sports comments from forums dedicated to fandom for NFL teams. We curate a unique dataset of over 6 million game-time comments from opposing perspectives (the teams in the game), each comment grounded in a non-linguistic description of the events that precipitated these comments (live win probabilities for each team). Expert and crowd annotations justify modeling the bias through tagging of implicit and explicit referential expressions and reveal the rich, contextual understanding of language and the world required for this task. For large-scale analysis of intergroup variation, we use LLMs for automated tagging, and discover that some LLMs perform best when prompted with linguistic descriptions of the win probability at the time of the comment, rather than numerical probability. Further, large-scale tagging of comments using LLMs uncovers linear variations in the form of referent across win probabilities that distinguish in-group and out-group utterances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FEyvVBgOxy": {
    "title": "Extending Multilingual Machine Translation through Behavioral Cloning",
    "volume": "review",
    "abstract": "Despite the growing variety of languages supported by existing multilingual neural machine translation (MNMT) models, most of the world's languages are still being left behind. We aim to extend large-scale MNMT models to a new language, allowing for translation between the newly added and all of the already supported languages in a challenging scenario: using only a parallel corpus between the new language and English. Previous approaches, such as continued training on parallel data including the new language, suffer from catastrophic forgetting (i.e., performance on other languages is reduced). Our novel approach BC-MNMT treats the task as an behavioral cloning process, which mimicks the behavior of an expert, a well-known technique widely used in the computer vision area, but not well explored in NLP. More specifically, we construct a pseudo multi-parallel corpus of the new and the original languages by pivoting through English in an online mode, and imitate the output distribution of the original MNMT model. Extensive experiments show that our approach significantly improves the translation performance between the new and the original languages, without severe catastrophic forgetting",
    "checked": false,
    "id": "3354771e0d79d3479357d7d9c54a2b717db67f9e",
    "semantic_title": "extending multilingual machine translation through imitation learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=M15yEj20uq": {
    "title": "Dependency Parsing with the Structuralized Prompt Template",
    "volume": "review",
    "abstract": "Dependency parsing is a crucial task in natural language processing that involves identifying syntactic dependencies to construct a structural tree of a sentence. Traditional models conduct dependency parsing by constructing embeddings and utilizing additional layers for prediction. We propose a novel method for performing dependency parsing using only a pre-trained encoder model with a text-to-text training approach. To facilitate this, we define the structured prompt template that effectively captures the structural information of the dependency tree. Our experimental results demonstrate that the proposed method achieves outstanding performance when comparing to traditional models, in spite of relying solely on an encoder model. Moreover, this method can be easily adapted to various encoder models that are suitable for different target languages or training environments, and it easily embody special features into the encoder models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5loBBDD3c3": {
    "title": "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models",
    "volume": "review",
    "abstract": "Phrases are fundamental linguistic units through which humans convey semantics. This study critically examines the capacity of API-based large language models (LLMs) to comprehend phrase semantics, utilizing three human-annotated datasets. We assess the performance of LLMs in executing phrase semantic reasoning tasks guided by natural language instructions and explore the impact of common prompting techniques, including few-shot demonstrations and Chain-of-Thought reasoning. Our findings reveal that LLMs greatly outperform traditional embedding methods across the datasets; however, they do not show a significant advantage over fine-tuned methods. The effectiveness of advanced prompting strategies shows variability. We conduct detailed error analyses to interpret the limitations faced by LLMs in comprehending phrase semantics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P8PF2e5EXf": {
    "title": "Knowledge Conflicts for LLMs: A Survey",
    "volume": "review",
    "abstract": "This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GsEYwqRfgj": {
    "title": "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing",
    "volume": "review",
    "abstract": "Recent work using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we show that disabling edits are an artifact of irregularities in the implementation of ROME. With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that model collapse is no longer observed when making large scale sequential edits with r-ROME, while further improving generalization and locality of model editing compared to the original implementation of ROME",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ypkARmcboA": {
    "title": "Threshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval",
    "volume": "review",
    "abstract": "This paper revisits dynamic pruning through rank score thresholding in cluster-based sparse retrieval to skip the index partially at cluster and document levels during inference. It proposes a two-parameter pruning control scheme called ASC with a probabilistic guarantee on rank-safeness competitiveness. ASC uses cluster-level maximum weight segmentation to improve accuracy of rank score bound estimation and threshold-driven pruning, and is targeted for speeding up retrieval applications requiring high relevance competitiveness. The experiments with MS MARCO and BEIR show that ASC improves the accuracy and safeness of pruning for better relevance while delivering a low latency on a single-threaded CPU",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xw3LPqLYKT": {
    "title": "Bridging Gaps with Multimodal Data: A Comprehensive Dataset for Pharmacovigilance Analysis in Ovarian Cancer",
    "volume": "review",
    "abstract": "Ovarian cancer is a highly fatal type of gynecologic cancer, with over 70\\% of cases diagnosed at an advanced stage due to mild and nonspecific symptoms. This delayed diagnosis involves intensive treatments, such as surgery and chemotherapy. These treatments widely use platinum-based compounds and taxanes, which are highly effective but can cause serious adverse reactions. Identifying adverse drug reactions (ADRs) efficiently is essential in managing these side effects and ensuring that patients receive the most effective and safest medical care possible. In this work, we present $\\textit{OvaCer}$, a novel multi-labelled multimodal dataset thoroughly developed for ovarian cancer pharmacovigilance. This dataset includes 1500 records containing vital details such as drug name, duration of drug use, adverse effects, severity levels, post-effect actions, and reference images used during ovarian cancer treatment. In order to further enhance its adaptability for pharmacovigilance objectives, we have incorporated gold-standard summaries of patient experiences. Recognizing the potential of large language models (LLMs) in summarization, we conducted a comprehensive evaluation of several pre-trained models, including GPT-3.5, T5, BART, FlanT5, and clinical models like PMC LLaMA in medical summarization. Our results show that LLMs demonstrate varying degrees of effectiveness in clinical summarization tasks, with GPT-3.5 significantly outperforming other models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k3ibnAmzKG": {
    "title": "RAG vs. Long Context: Examining Frontier Large Language Models for Environmental Review Document Comprehension",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have been applied to many research problems across various domains. One of the applications of LLMs is providing question-answering systems that cater to users from different fields. The effectiveness of LLM-based question-answering systems has already been established at an acceptable level for users posing questions in popular and public domains such as trivia and literature. However, it has not often been established in niche domains that traditionally require specialized expertise. To this end, we construct the NEPAQuAD1.0 benchmark to evaluate the performance of three frontier LLMs, Claude Sonnet, Gemini, and GPT-4, when answering questions originating from Environmental Impact Statements prepared by U.S. federal government agencies in accordance with the National Environmental Environmental Act (NEPA). We specifically measure the ability of LLMs to understand the nuances of legal, technical, and compliance-related information present in NEPA documents in different contextual scenarios. For example, we test the LLMs' internal prior NEPA knowledge by providing questions without any context, as well as assess how LLMs synthesize the contextual information present in long NEPA documents to facilitate the question/answering task. We compare the performance of the long context LLMs and RAG powered models in handling different types of questions (e.g., problem-solving, divergent). Our results suggest that RAG powered models significantly outperform the long context models in the answer accuracy regardless of the choice of the frontier LLM. Our further analysis reveals that many models perform better answering closed questions than divergent and problem-solving questions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7UQUQudozb": {
    "title": "Two-level SVM model with language markers for (early) detection of Alzheimer's Disease",
    "volume": "review",
    "abstract": "This study presents a novel two-level SVM (Support Vector Machine) model for the automatic (early) detection of Alzheimer's Disease (AD) using language markers that are independent of lexical semantics. We avoid lexical semantic features because they are subject to high individual variation, thus limiting their predictive power for unseen data. Instead, we focus on morphosyntactic, syntactic, and sentence-level features, which are more stable and potentially allow for easier generalization of the model to other datasets, languages, and individuals. We constructed SVMs at both the sentence level and the subject level, applying language features extracted from automatically parsed transcriptions from the Pitt and Delaware corpora in DementiaBank. Our model demonstrated that the subject-level SVM significantly improved classification accuracy. The model yields high performance across all evaluation metrics on the test set for both AD and Mild Cognitive Impairment statuses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1v26J8FMdi": {
    "title": "The Current State of the NLP in Sub-Saharan Africa - A Position Paper",
    "volume": "review",
    "abstract": "In this study, we present our position on the current state of the NLP in Sub-Saharan Africa. Our position comes from surveys of literature we conducted on NLP activities (research and publications) in Sub-Saharan African languages. We discussed issues of NLP research outcomes for Sub-Saharan Africa based on the results of the survey. These issues include low-quality results of NLP studies, insufficient access to research funding, and lack of an interdisciplinary approach to NLP research in the region's languages. Results of the study reveal that most of the NLP work done for Sub-Saharan African languages from 2020-2023 was centered around corpus development, language modeling, and sentiment analysis. About 61% of the NLP work in sub-Saharan Africa does not have access to funding. Funding sources are mainly NGOs with 66.7% of work that received funding being multilingual studies. However, 64% of NLP activities in the region are monolingual. We finalize our position by providing recommendations on addressing issues raised and discovered based on the survey",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4MAACM0Hy6": {
    "title": "Avoiding Copyright Infringement via Machine Unlearning",
    "volume": "review",
    "abstract": "Pre-trained Large Language Models (LLMs) have demonstrated remarkable capabilities but also pose risks by learning and generating copyrighted material, leading to significant legal and ethical concerns. To address these issues, it is critical for model owners to be able to unlearn copyrighted content at various time steps. We explore the setting of sequential unlearning, where copyrighted content is removed over multiple time stepsâ€”a scenario that has not been rigorously addressed. To tackle this challenge, we propose Stable Sequential Unlearning (SSU), a novel unlearning framework for LLMs, designed to have a more stable process to remove copyrighted content from LLMs throughout different time steps using task vectors, by incorporating additional random labeling loss and applying gradient-based weight saliency mapping. Experiments demonstrate that SSU finds a good balance between unlearning efficacy and maintaining model's general knowledge compared to existing baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sjMqvDwq6A": {
    "title": "Robust Claim Verification Through Fact Detection",
    "volume": "review",
    "abstract": "Claim verification can be a challenging task. In this paper, we present a method to enhance the robustness and reasoning capabilities of automated claim verification by extracting short facts from evidence. Our novel approach, FactDetect, leverages Large Language Models (LLMs) to generate concise factual statements from evidence and label these facts based on their semantic relevance to the claim and evidence. The generated facts are then combined with the claim and evidence. To train a lightweight supervised model, we incorporate a fact-detection task into the claim verification process as a multitasking approach to improve both performance and explainability. We also show that augmenting FactDetect in the claim verification prompt enhances performance in zero-shot claim verification using LLMs. Our method demonstrates competitive results in the supervised claim verification model by 15% on the F1 score when evaluated for challenging scientific claim verification datasets. We also demonstrate that FactDetect can be augmented with claim and evidence for zero-shot prompting (AugFactDetect) in LLMs for verdict prediction. We show that AugFactDetect outperforms the baseline with statistical significance on three challenging scientific claim verification datasets with an average of029 17.3% performance gain compared to the best performing baselines",
    "checked": true,
    "id": "b321b31298ae20b5ef66f3c74d8a4f5b11635d26",
    "semantic_title": "robust claim verification through fact detection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aj7CqlDYeG": {
    "title": "Studying and Mitigating Biases in Sign Language Understanding Models",
    "volume": "review",
    "abstract": "Crowdsourced sign datasets collected with the involvement of deaf communities, such as the ASL Citizen dataset, represent an important step towards improved accessibility and documentation of signed languages. However, it is important to ensure that these resources benefit people in an equitable manner. Thus, there is a need to understand the potential biases that may result from models trained on sign language datasets. In this work, we utilize the rich information about participant demographics and lexical features present in the ASL Citizen dataset to study and document the biases that may result from models trained on crowdsourced sign datasets. Further, we apply several bias mitigation techniques during model training, and discuss the results and relative success of these techniques. In addition to our analyses and machine learning experiments, with the publication of this work we release the demographic information about the participants in the ASL Citizen dataset to encourage future work in this space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4qHLzBY62c": {
    "title": "Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective",
    "volume": "review",
    "abstract": "Vision-language models (VLMs) pre-trained on extensive datasets can inadvertently learn biases by correlating gender information with specific objects or scenarios. Current methods, focusing on modifying inputs and monitoring changes in the model's output probability scores, often struggle to comprehensively understand bias from the perspective of model components. We propose a framework that incorporates causal mediation analysis to measure and map the pathways of bias generation and propagation within VLMs. This approach allows us to identify the direct effects of interventions on model bias and the indirect effects of interventions on bias mediated through different model components. Our results show that image features are the primary contributors to bias, with significantly higher impacts than text features, specifically accounting for 32.57% and 12.63% of the bias in the MSCOCO and PASCAL-SENTENCE datasets, respectively. Additionally, the image encoder's contribution notably surpasses that of the text encoder and the deep fusion encoder. Further experimentation confirms that contributions from both language and vision modalities are aligned and non-conflicting. Consequently, focusing on blurring gender representations within the image encoder which contributes most to the model bias, reduces bias efficiently by 22.03% and 9.04% in the MSCOCO and PASCAL-SENTENCE datasets, respectively, with minimal performance loss or increased computational demands",
    "checked": true,
    "id": "d7552c281caf7e2be5818e8f74119600644a41cb",
    "semantic_title": "images speak louder than words: understanding and mitigating bias in vision-language model from a causal mediation perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qKa0jAOKyi": {
    "title": "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation",
    "volume": "review",
    "abstract": "The fashion domain encompasses a variety of real-world multimodal tasks, including multimodal retrieval and multimodal generation. The rapid advancements in artificial intelligence generated content, particularly in technologies like large language models for text generation and diffusion models for visual generation, have sparked widespread research interest in applying these multimodal models in the fashion domain. However, tasks that use embeddings, such as image-to-text or text-to-image retrieval, have been largely ignored from this perspective due to the diverse nature of the multimodal fashion domain. And current research on multi-task single models lack focus on image generation. In this work, we present UniFashion, a unified framework that simultaneously tackles the challenges of multimodal generation and retrieval tasks within the fashion domain, integrating image generation with retrieval tasks and text generation tasks. UniFashion unifies embedding and generative tasks by integrating a diffusion model and LLM, enabling controllable and high-fidelity generation. Our model significantly outperforms previous single-task state-of-the-art models across diverse fashion tasks, and can be readily adapted to manage complex vision-language tasks. This work demonstrates the potential learning synergy between multimodal generation and retrieval, offering a promising direction for future research in the fashion domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NO2eAN3FQz": {
    "title": "Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search",
    "volume": "review",
    "abstract": "Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context. The common approach of *rewrite-then-retrieve* aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results. To overcome this limitation, we present a novel framework ***RetPO*** (**Ret**riever's **P**reference **O**ptimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems. The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers' preferences. Through the process, we construct a large-scale dataset called ***RF collection***, containing **R**etrievers' **F**eedback on over 410K query rewrites across 12K conversations. Furthermore, we fine-tune a smaller LM using this dataset to align it with the retrievers' preferences as feedback. The resulting model demonstrates superiority on two benchmarks, surpassing the previous state-of-the-art performance of *rewrite-then-retrieve* approaches, including GPT-3.5. Code and dataset will be available",
    "checked": true,
    "id": "27a8f308ee592b8c664b7592b51efb655111dde9",
    "semantic_title": "ask optimal questions: aligning large language models with retriever's preference in conversational search",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=dinFs1ajvR": {
    "title": "SELECTLLM: A Framework for Quality Aware Cost Efficient LLM Usage",
    "volume": "review",
    "abstract": "Generative AI and LLMs in particular are heavily used nowadays for various document processing tasks such as question answering and document summarization. Enterprises are incurring huge costs of operating or using LLMs for their respective use cases. In this work, we propose optimizing the usage costs of LLMs in a quality aware manner for document summarization tasks. Specifically, we propose to exploit the variability of LLM performances across different types and formats of data to maximize the output quality while maintaining expected costs under a budget and latency within a threshold. This presents two challenges: 1) estimating the output quality of LLMs at runtime without invoking each LLM, 2) optimally allocating queries to LLMs such that the objectives are optimized and constraints are satisfied. We propose a model to predict the output quality of LLMs on text summarization, followed by an LP rounding algorithm to optimize the selection of LLMs. We study the problems both theoretically and empirically. Our methods reduce costs by $40\\%- 90\\%$ while improving quality by $4\\%-7\\%$. In addition to the quantitative results, we further show that our model quality estimation aligns majorly with human preferences through a user study. We release the annotated open source datasets to the community for further research and exploration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zr2pXxtEgn": {
    "title": "DLoRA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model",
    "volume": "review",
    "abstract": "To enhance the performance of large language models (LLM) on downstream tasks, one solution is to fine-tune certain LLM parameters and make it better align with the characteristics of the training dataset. This process is commonly known as parameter-efficient fine-tuning (PEFT). Due to the scale of LLM, PEFT operations are usually executed in the public environment (e.g., cloud server). This necessitates the sharing of sensitive user data across public environments, thereby raising potential privacy concerns. To tackle these challenges, we propose a distributed PEFT framework called DLoRA. DLoRA enables scalable PEFT operations to be performed collaboratively between the cloud and user devices. Coupled with the proposed Kill and Revive algorithm, the evaluation results demonstrate that DLoRA can significantly reduce the computation and communication workload over the user devices while achieving superior accuracy and privacy protection",
    "checked": true,
    "id": "3037efe59991db85cbb7e17f41e196f09c74ba6d",
    "semantic_title": "dlora: distributed parameter-efficient fine-tuning solution for large language model",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=k21Zww3Oon": {
    "title": "RUPBench: Benchmarking Reasoning Under Perturbations for Robustness Evaluation in Large Language Models",
    "volume": "review",
    "abstract": "With the increasing use of large language models (LLMs), ensuring reliable performance in diverse, real-world environments is essential. Despite their remarkable achievements, LLMs often struggle with adversarial inputs, significantly impacting their effectiveness in practical applications. To systematically understand the robustness of LLMs, we present RUPBench, a comprehensive benchmark designed to evaluate LLM robustness across diverse reasoning tasks. Our benchmark incorporates 15 reasoning datasets, categorized into commonsense, arithmetic, logical, and knowledge-intensive reasoning, and introduces nine types of textual perturbations at lexical, syntactic, and semantic levels. By examining the performance of state-of-the-art LLMs such as GPT-4o, Llama3, Phi-3, and Gemma on both original and perturbed datasets, we provide a detailed analysis of their robustness and error patterns. Our findings highlight that larger models tend to exhibit greater robustness to perturbations. Additionally, common error types are identified through manual inspection, revealing specific challenges faced by LLMs in different reasoning contexts. This work provides insights into areas where LLMs need further improvement to handle diverse and noisy inputs effectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=knQo5dGvc7": {
    "title": "Building a Multi-Platform, BERT Classifier for Detecting Connective Language",
    "volume": "review",
    "abstract": "This study presents an approach for detecting connective language---defined as language that facilitates engagement, understanding, and conversation---from social media discussions. We developed and evaluated two types of classifiers: BERT and GPT-3.5 turbo. Our results demonstrate that the BERT classifier significantly outperforms GPT-3.5 turbo in detecting connective language. Furthermore, our analysis confirms that connective language is distinct from related concepts measuring discourse qualities, such as politeness and toxicity. We also explore the potential of BERT-based classifiers for platform-agnostic tools. This research advances our understanding of the linguistic dimensions of online communication and proposes practical tools for detecting connective language across diverse digital environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cZKqp0etP6": {
    "title": "GovAIEc: A Lexical Complexity Corpus for Spanish in Ecuadorian public documents",
    "volume": "review",
    "abstract": "In this article, we present GovAI\\textit{Ec}, a new annotated corpus of complex lexicon created with institutional texts in Ecuadorian Spanish, and we detail the process of compiling and annotating this corpus. With the aim of providing a valuable resource to the scientific community to advance research in the field of Lexical Simplification in the Spanish language, we carried out several complex word prediction experiments using this corpus. The complex word labeling process was carried out with a group of annotators with different levels of literacy, in order to ensure a comprehensive evaluation. We use Lexical Complexity metrics as units of analysis, and apply advanced multilingual language models such as XLM-RoBERTa-Base, RoBERTa-large-BNE, XLM-RoBERTa-Large and BERT to evaluate the corpus. This corpus is invaluable for identifying words that represent barriers in the reading comprehension of users who interact with bureaucratic procedures of various entities in Ecuador",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1WvIFEPNG1": {
    "title": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs",
    "volume": "review",
    "abstract": "The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. Difficulties lie in assessing the factuality of free-form responses in open domains. Also, different papers use disparate evaluation benchmarks and measurements, which renders them hard to compare and hampers future progress. To mitigate these issues, we propose OpenFactCheck, a unified factuality evaluation framework for LLMs. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims, (ii) LLMEVAL, a unified evaluation framework assesses LLM's factuality ability from various perspectives fairly, and (iii) CHECKEREVAL is an extensible solution for gauging the reliability of automatic fact-checkers' verification results using human-annotated datasets. OpenFactCheck is publicly released at URL withheld",
    "checked": true,
    "id": "9741eb61739f2221c186634663876e2c1024c746",
    "semantic_title": "openfactcheck: a unified framework for factuality evaluation of llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=aHGpeLFdL4": {
    "title": "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning",
    "volume": "review",
    "abstract": "Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets. Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning",
    "checked": true,
    "id": "a3d2c1c932da66f48af673bcb860ba0b8fb335d1",
    "semantic_title": "making reasoning matter: measuring and improving faithfulness of chain-of-thought reasoning",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=kRipwoUR6R": {
    "title": "Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles",
    "volume": "review",
    "abstract": "Recent works leverage LLMs to roleplay realistic social scenarios, aiding novices in practicing their social skills. However, simulating sensitive interactions, such as in mental health, is challenging. Privacy concerns restrict data access, and collecting expert feedback, although vital, is laborious. To address this, we develop Roleplay-doh, a novel human-LLM collaboration pipeline that elicits qualitative feedback from a domain-expert, which is transformed into a set of principles, or natural language rules, that govern an LLM-prompted roleplay. A focal the domain of mental health with counselors customizing AI patients as simulated practice partners for novice counselors. After uncovering issues in GPT-4 simulations not adhering to expert-defined principles, we also introduce a novel principle-adherence prompting pipeline which shows 30\\% improvements in response quality and principle following for the downstream task. Via a user study with 25 counseling experts, we demonstrate that the pipeline makes it easy and effective to create AI patients that more faithfully resemble real patients, as judged by creators and third-party counselors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Za1qX4ShsK": {
    "title": "From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models",
    "volume": "review",
    "abstract": "Despite recent advancements in vision-language models, their performance remains suboptimal on images from non-western cultures due to underrepresentation in training datasets. Various benchmarks have been proposed to test models' cultural inclusivity. Still, they have limited coverage of cultures and do not adequately assess cultural diversity across universal and culture-specific local concepts. To address these limitations, we introduce the GlobalRG benchmark, comprising two challenging tasks: retrieval across universals and cultural visual grounding. The former task entails retrieving culturally diverse images for universal concepts from 50 countries, while the latter aims at grounding culture-specific concepts within images from 15 countries. Our evaluation across a wide range of models reveals that the performance varies significantly across cultures -- underscoring the necessity for enhancing multicultural understanding in vision-language models",
    "checked": true,
    "id": "5fecfeb9455af484cded3d9d48cd9a3da8277f7f",
    "semantic_title": "from local concepts to universals: evaluating the multicultural understanding of vision-language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vILFJ8fJrn": {
    "title": "Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation",
    "volume": "review",
    "abstract": "Large language models (LLMs) exhibit enhanced reasoning at larger scales, driving efforts to distill these capabilities into smaller models via teacher-student learning. Previous works simply fine-tune student models on teachers' generated Chain-of-Thoughts (CoTs) data. Although these methods enhance in-domain (IND) reasoning performance, they struggle to generalize to out-of-domain (OOD) tasks. We believe that the widespread spurious correlations between questions and answers may lead the model to preset a specific answer which restricts the diversity and generalizability of its reasoning process. In this paper, we propose \\textbf{Cas}cading Decomposed \\textbf{Co}Ts \\textbf{D}istillation (CasCoD) to address these issues by decomposing the traditional single-step learning process into two cascaded learning steps. Specifically, by restructuring the training objectivesâ€”removing the answer from outputs and concatenating the question with the rationale as inputâ€”CasCoD's two-step learning process ensures that students focus on learning rationales without interference from the preset answers, thus improving reasoning generalizability. Extensive experiments demonstrate the effectiveness of CasCoD on both IND and OOD benchmark reasoning datasets\\footnote{Code can be found at \\url{https://anonymous.4open.science/r/ef334sf-FB92}}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HTK9Efk8rB": {
    "title": "Symbolic Working Memory Enhances Language Models for Complex Rule Application",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown remarkable reasoning performance but struggle with multi-step deductive reasoning involving a series of rule application steps, especially when rules are presented non-sequentially. Our preliminary analysis shows that while LLMs excel in single-step rule application, their performance drops significantly in multi-step scenarios due to the challenge in rule grounding. It requires anchoring the applicable rule and supporting facts at each step, amidst multiple input rules, facts, and inferred facts. To address this, we propose augmenting LLMs with external working memory and introduce a neurosymbolic framework for rule application. The memory stores facts and rules in both natural language and symbolic forms, enabling precise tracking. Utilizing this memory, our framework iteratively performs symbolic rule grounding and LLM-based rule implementation. The former matches predicates and variables of symbolic rules and facts to ground applicable rules at each step. Experiments indicate our framework's effectiveness in rule application and its robustness across various steps and settings",
    "checked": false,
    "id": "f5549b92ac63d24bec4ef14b615ea98e9072f651",
    "semantic_title": "natural language processing and neurosymbolic ai: the role of neural networks with knowledge-guided symbolic approaches",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bhWiQWQOGc": {
    "title": "Securing Author Privacy using Large Language Models",
    "volume": "review",
    "abstract": "Sophisticated machine learning models can determine the author of a given document using stylometric features or contextualized word embeddings. In response, researchers have developed Authorship Obfuscation methods to disguise these identifying characteristics. Despite the growing popularity of large language models like GPT-4, their utility for this purpose has not been previously studied. In this work, we explore the application of popular large language models to the task of author obfuscation, and show that they can outperform a state-of-the-art approach. We analyze their behavior and suggest a personalized prompting technique for improving performance on more difficult authors. Our code and experiments will be made publicly available",
    "checked": false,
    "id": "553d85e202fdbdd4101673b9205135b8eb94811d",
    "semantic_title": "recovering from privacy-preserving masking with large language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=a95JWLYbIi": {
    "title": "Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation",
    "volume": "review",
    "abstract": "Legal case retrieval for sourcing similar cases is critical in upholding judicial fairness. Different from general web search, legal case retrieval involves processing lengthy, complex, and highly specialized legal documents. Existing methods in this domain often overlook the incorporation of legal expert knowledge, which is crucial for accurately understanding and modeling legal cases, leading to unsatisfactory retrieval performance. This paper introduces KELLER, a legal knowledge-guided case reformulation approach based on large language models (LLMs) for effective and interpretable legal case retrieval. By incorporating professional legal knowledge about crimes and law articles, we enable large language models to accurately reformulate the original legal case into concise sub-facts of crimes, which contain the essential information of the case. Extensive experiments on two legal case retrieval benchmarks demonstrate superior retrieval performance and robustness on complex legal case queries of KELLER over existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DA4XQ3VciF": {
    "title": "The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs",
    "volume": "review",
    "abstract": "Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs' robustness in the presence of misleading images",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4LwI0Y646O": {
    "title": "Topic-XICL: Demonstration Selection with Topic Inference for Cross-lingual In-context Learning",
    "volume": "review",
    "abstract": "Cross-lingual in-context learning (XICL) shows promise for adapting large language models (LLMs) to low-resource languages. Previous methods rely on off-the-shelf or task-specific retrievers based on LLM feedback signals for demonstration selection. However, these approaches often neglect factors beyond semantic similarity and can be resource-intensive. To address these challenges, we propose a novel approach called Topic-XICL, which leverages a latent topic model to select demonstrations for XICL. We assume that latent topic variables encapsulate information that more accurately characterizes demonstrations. By training this topic model on rich-resource language data with a small-parameter LLM, we obtain more informative demonstrations through topic inference and utilize them for in-context learning across various LLMs. Our method is tested on three multilingual tasks (XNLI, XCOPA, and TyDiQA-GoldP) and three models with approximately 7 billion parameters, including two multilingual LLMs (BLOOM and XGLM), and an English-centric model, Llama2. Comparative evaluations against baselines of random selection, semantic similarity selection, and clustering-based selection show consistent improvements in multilingual performance with our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SOWcC2cCMT": {
    "title": "LLM with Relation Classifier for Document-Level Relation Extraction",
    "volume": "review",
    "abstract": "Large language models (LLMs) create a new paradigm for natural language processing. Despite their advancement, LLM-based methods still lag behind traditional methods in document-level relation extraction (DocRE), a critical task for understanding complex entity relations. To address this issue, this paper first investigates the causes of the performance gap, identifying the dispersion of attention by LLMs due to entity pairs without relations as a primary factor. We then introduce a novel classifier-LLM approach to DocRE. The proposed approach begins with a classifier specifically designed to select entity pair candidates exhibiting potential relations and thereby feeds them to LLM for the final relation extraction. This method ensures that during inference, the LLM's focus is directed primarily at entity pairs with relations. Experiments on DocRE and Re-DocRE benchmarks reveal that our method significantly outperforms recent LLM-based DocRE methods",
    "checked": false,
    "id": "3aa7bb07d192239ec270a680b3f85a5db80f0415",
    "semantic_title": "llm with relation classifier for document-level relation extraction anonymous acl submission",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1dA1DetuTk": {
    "title": "Cultural Conditioning or Placebo? On the Effectiveness of Socio-Demographic Prompting",
    "volume": "review",
    "abstract": "Socio-demographic prompting is a commonly employed approach to study cultural biases in LLMs as well as for aligning models to certain cultures. In this paper, we systematically probe four LLMs (Llama 3, Mistral v0.2, GPT-3.5 Turbo and GPT4) with prompts that are conditioned on culturally sensitive and non-sensitive cues, on datasets that are supposed to be culturally sensitive (EtiCor and CALI) or neutral (MMLU and ETHICS). We observe that all models except GPT4 show significant variations in their responses on both kinds of datasets for both kinds of prompts, casting doubt on the robustness of the culturally-conditioned prompting as a method for eliciting cultural bias in models that are not sufficiently stable with respect to arbitrary prompting cues. Further, we also show that some of the supposedly culturally neutral datasets have a non-trivial fraction of culturally sensitive questions/tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tx7B7Gi52p": {
    "title": "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering",
    "volume": "review",
    "abstract": "Large-scale language models (LLMs) like ChatGPT have demonstrated impressive abilities in generating responses based on human instructions. However, their use in the medical field can be challenging due to their lack of specific, in-depth knowledge. In this study, we present a system called LLMs Augmented with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in specialized domains. LLM-AMT integrates authoritative medical textbooks into the LLMs' framework using plug-and-play modules. These modules include a Query Augmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together, they incorporate authoritative medical knowledge. Additionally, an \\textit{LLM Reader} aids in contextual understanding. Our experimental results on three medical QA tasks demonstrate that LLM-AMT significantly improves response quality, with accuracy gains ranging from 11.6\\% to 16.6\\%. Notably, with GPT-4-Turbo as the base model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on a massive amount of medical corpus by 2-3\\%. We found that despite being 100$\\times$ smaller in size, medical textbooks as a retrieval corpus are proven to be a more effective knowledge database than Wikipedia in the medical domain, boosting performance by 7.8\\%-13.7\\%. We will open-source the code for this work",
    "checked": false,
    "id": "5e761e9f5cd9672a181b256299cd2916a8079461",
    "semantic_title": "augmenting black-box llms with medical textbooks for clinical question answering",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=q7BNj09ljG": {
    "title": "MVP-Bench: Can Large Vision-Language Models Conduct Multi-level Visual Perception Like Humans?",
    "volume": "review",
    "abstract": "Humans perform visual perception at multiple levels, including low-level object recognition and high-level semantic interpretation such as behavior understanding. Subtle differences in low-level details can lead to substantial changes in high-level perception. For example, substituting the shopping bag held by a person with a gun suggests violent behavior, implying criminal or violent activity. Despite significant advancements in various multimodal tasks, Large Visual Language Models (LVLMs) remain unexplored in their capabilities to conduct such multi-level visual perceptions. To investigate the perception gap between LVLMs and humans, we introduce MVP-Bench, the first visual--language benchmark systematically evaluating both low- and high-level visual perception of LVLMs. We construct MVP-Bench across natural and synthetic images to investigate how manipulated content influences model perception. Using MVP-Bench, we diagnose the visual perception of 10 open-source and 2 closed-source LVLMs, showing that high-level perception tasks significantly challenge existing LVLMs. The state-of-the-art GPT-4o only achieves an accuracy of 56% on Yes/No questions, compared with 74% in low-level scenarios. Furthermore, the performance gap between natural and manipulated images indicates that current LVLMs do not generalize in understanding the visual semantics of synthetic images as humans do",
    "checked": false,
    "id": "3e75ac0db303e9acdf3ba4b524a1f69c22824ac8",
    "semantic_title": "a benchmark for multi-modal foundation models on low-level vision: from single images to pairs",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=2MAlLcfx5p": {
    "title": "Guideline Compliance in Task-Oriented Dialogue: The Chained Prior Approach",
    "volume": "review",
    "abstract": "Task-oriented dialogue (TOD) systems are widely used across various domains, including customer service, appointment scheduling, and technical support. In real-world scenarios, such systems must adhere to given operational guidelines. However, existing solutions based on large language models often cannot achieve strict guideline compliance, even when fine-tuned with domain knowledge. To address this issue, we introduce a novel TOD system named GuidedTOD, which explicitly considers domain-specific guidelines by integrating a policy module. This module employs a Markov Chain, termed Chained Prior, to efficiently encode and dynamically update guideline knowledge. During inference, the Chained Prior re-ranks outputs from the domain-expert language model using beam search, ensuring guideline adherence. Experimental results show that GuidedTOD significantly improves guideline compliance, achieving approximately 20% better action prediction accuracy than state-of-the-art solutions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9gahhf20Xx": {
    "title": "Debate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction",
    "volume": "review",
    "abstract": "We propose a multi-agent debate as optimization (DAO) system for event extraction, where the primary objective is to iteratively refine the large language models (LLMs) outputs through debating without parameter tuning. In DAO, we introduce two novel modules: the Diverse-RAG (DRAG) module and the Adaptive Conformal Prediction (AdaCP) module. DRAG systematically retrieves supporting information that best fits the debate discussion, while AdaCP enhances the accuracy and reliability of event extraction by effectively rejecting less promising answers. Experimental results demonstrate a significant reduction in the performance gap between supervised approaches and tuning-free LLM-based methods by 18.1\\% and 17.8\\% on ACE05 and 17.9\\% and 15.2\\% on CASIE for event detection and argument extraction respectively",
    "checked": true,
    "id": "fcbd3eaa5ac589303b6634152f11d79b04faa4ec",
    "semantic_title": "debate as optimization: adaptive conformal prediction and diverse retrieval for event extraction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n5c4fpprwg": {
    "title": "DocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction",
    "volume": "review",
    "abstract": "Event extraction aims to identify events and then extract the arguments involved in those events. In recent years, there has been a gradual shift from sentence-level event extraction to document-level event extraction research. Despite the significant success achieved in English domain event extraction research, event extraction in Chinese still remains largely unexplored. However, a major obstacle to promoting Chinese document-level event extraction is the lack of fine-grained, wide domain coverage datasets for model training and evaluation. In this paper, we propose DocEE-zh, a new Chinese document-level event extraction dataset comprising over 36,000 events and more than 210,000 arguments. DocEE-zh is an extension of the DocEE dataset, utilizing the same event schema, and all data has been meticulously annotated by human experts. We highlight two features: focus on high-interest event types and fine-grained argument types. Experimental results indicate that state-of-the-art models still fail to achieve satisfactory performance (F1 score of 68\\%), revealing that Chinese DocEE remains an unresolved challenge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s9y1iT2UFj": {
    "title": "Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style",
    "volume": "review",
    "abstract": "The ability of large language models (LLMs) to generate evidence-based and stylistic counter-arguments is crucial for enhancing online discussions. However, there is a research gap in evaluating these models' practical effectiveness in real-world applications. Previous studies often overlook the balance between evidentiality and stylistic elements necessary for persuasive arguments. We created and audited Counterfire, a new dataset of 32,000 counter-arguments generated by non- and finetuned-LLMs with varying prompts for evidence use and argumentative style. We audited models like GPT-3.5, PaLM 2, and Koala, evaluating their rhetorical quality and persuasive abilities. Our findings showed that while GPT-3.5 Turbo excelled in argument quality and style adherence, it still fell short of human standards, emphasizing the need for further refinement in LLM outputs",
    "checked": true,
    "id": "8ef52d7cf4fa71c33bbc6973b38954e04322e0c4",
    "semantic_title": "auditing counterfire: evaluating advanced counterargument generation with evidence and style",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gVFrVw2z1R": {
    "title": "Voices Unheard: NLP Resources and Models for YorÃ¹bÃ¡ Regional Dialects",
    "volume": "review",
    "abstract": "Yoruba---an African language with roughly 47 million speakers---encompasses a continuum with several dialects. Recent efforts to develop NLP technologies for African languages have focused on their standard dialects, resulting in disparities for dialects and varieties for which there are little to no resources or tools. We take steps towards bridging this gap by introducing a new high-quality parallel text and speech corpus; YORULECT across three domains and four regional yoruba dialects. To develop this corpus, we engaged native speakers, traveling to communities where these dialects are spoken, to collect text and speech data. Using our newly created corpus, we conducted extensive experiments on (text) machine translation, automatic speech recognition, and speech-to-text translation. Our results reveal substantial performance disparities between standard yoruba and the other dialects across all tasks. However, we also show that with dialect-adaptive finetuning, we are able to narrow this gap. We believe our dataset and experimental analysis will contribute greatly to developing NLP tools for Yoruba and its dialects, and potentially for other African languages, by improving our understanding of existing challenges and offering a high-quality dataset for further development. We will release YORULECT dataset and models publicly under an open license",
    "checked": true,
    "id": "92e9acc55013a7408559d5e203eb913378563377",
    "semantic_title": "voices unheard: nlp resources and models for yorÃ¹bÃ¡ regional dialects",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FAUDgLi6RO": {
    "title": "RAG-Logic: Enhance Neuro-symbolic Approaches for Logical Reasoning with Retrieval-augmented Generation",
    "volume": "review",
    "abstract": "Deductive reasoning over complex natural language poses significant challenges, necessitating the integration of large language models (LLMs). Benchmarks like ProofWriter and FOLIO highlight these challenges and demonstrate the need for advanced reasoning methods. Current approaches range from direct reasoning methods like zero-shot, few-shot, and chain-of-thought learning to hybrid models integrating LLMs with symbolic solvers. However, these methods often rely on static examples, limiting their adaptability. This paper introduces RAG-Logic, a dynamic example-based framework using Retrieval-Augmented Generation (RAG), which enhances LLMs' logical reasoning capabilities by providing contextually relevant examples. This approach conserves resources by avoiding extensive fine-tuning and reduces the propensity for hallucinations in traditional models. Our results across the ProofWriter and FOLIO datasets demonstrate the effectiveness of our framework, marking an advancement in logical reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KqlSYhDbxe": {
    "title": "Learning to Correct for QA Reasoning with Black-box LLMs",
    "volume": "review",
    "abstract": "An open challenge in recent machine learning is about how to improve the reasoning capability of large language models (LLMs) in a black-box setting, i.e., without access to detailed information such as output token probabilities. Existing approaches either rely on accessibility (which is often unrealistic) or involve significantly increased train- and inference-time costs. This paper addresses those limitations or shortcomings by proposing a novel approach, namely CoBB (Correct for improving QA reasoning of Black-Box LLMs). It uses a trained adaptation model to perform a seq2seq mapping from the often-imperfect reasonings of the original black-box LLM to the correct or improved reasonings. Specifically, the adaptation model is initialized with a relatively small open-source LLM and adapted over a collection of sub-sampled training pairs. To select the representative pairs of correct and incorrect reasonings, we formulated the dataset construction as an optimization problem that minimizes the statistical divergence between the sampled subset and the entire collection, and solved it via a genetic algorithm. We then train the adaptation model over the sampled pairs by contrasting the likelihoods of correct and incorrect reasonings. Our experimental results demonstrate that CoBB significantly improves reasoning accuracy across various QA benchmarks, compared to the best-performing adaptation baselines",
    "checked": true,
    "id": "41e347649d5563d4af2e8ceeea9ff36b5c7faf8f",
    "semantic_title": "learning to correct for qa reasoning with black-box llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=MiCF0g3E4u": {
    "title": "Attacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have introduced novel opportunities for text comprehension and generation. Yet, they are vulnerable to adversarial perturbations and data poisoning attacks, particularly in tasks like text classification and translation. However, the adversarial robustness of abstractive text summarization models remains less explored. In this work, we unveil a novel approach by exploiting the inherent lead bias in summarization models, to perform adversarial perturbations. Furthermore, we introduce an innovative application of influence functions, to execute data poisoning, which compromises the model's integrity. This approach not only shows a skew in the models' behavior to produce desired outcomes but also shows a new behavioral change, where models under attack tend to generate extractive summaries rather than abstractive summaries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DPCBUZqn1h": {
    "title": "Tune-n-Batch: Fine-Tuning LLMs for Batch Prompting",
    "volume": "review",
    "abstract": "The growing capabilities of Large Language Models (LLMs) have enabled batch prompting (BP), the technique of concatenating multiple questions into one prompt and answering all questions in one inference pass. However, current batch prompting techniques require lengthy prompts that need few-shot examples and formatting instructions, reporting decreased accuracy per question as the batch size grows. In this paper, we show that this accuracy loss can be mitigated by fine-tuning models for batch prompting. We aggregate training data for batch prompting by selecting batches from nine datasets with varying batch sizes. We demonstrate that after limited fine-tuning of LLaMA-3-8B-Instruct on our batch prompting dataset, BP can be performed effectively without in-prompt examples or repetitive formatting. Our fine-tuned LLaMA-3-8B-Instruct model exhibits consistent performance across various batch sizes on tasks seen and unseen during training",
    "checked": false,
    "id": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e",
    "semantic_title": "starcoder: may the source be with you!",
    "citation_count": 423,
    "authors": []
  },
  "https://openreview.net/forum?id=syE0kw1JSE": {
    "title": "Glue pizza and eat rocks\" - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models",
    "volume": "review",
    "abstract": "Retrieval-Augmented Generative (RAG) models enhance Large Language Models (LLMs) by integrating external knowledge bases, improving their performance in applications like fact-checking and information searching. In this paper, we demonstrate a security threat where adversaries can exploit the openness of these knowledge bases by injecting deceptive content into the retrieval database, intentionally changing the model's behavior. This threat is critical as it mirrors real-world usage scenarios where RAG systems interact with publicly accessible knowledge bases, such as web scrapings and user-contributed data pools. To be more realistic, we target a realistic setting where the adversary has no knowledge of users' queries, knowledge base data, and the LLM parameters. We demonstrate that it is possible to exploit the model successfully through crafted content uploads with access to the retriever. Our findings emphasize an urgent need for security measures in the design and deployment of RAG systems to prevent potential manipulation and ensure the integrity of machine-generated content",
    "checked": true,
    "id": "50d65e1bf5cee30b24c3872afc6aabae87f44e66",
    "semantic_title": "glue pizza and eat rocks\" - exploiting vulnerabilities in retrieval-augmented generative models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=oWbkowJg1W": {
    "title": "PERSONA: A Reproducible Testbed for Pluralistic Alignment",
    "volume": "review",
    "abstract": "The rapid advancement and adoption of language models (LMs) has highlighted critical challenges in aligning these models with the diverse values and preferences of global users. Existing reinforcement learning from human feedback (RLHF) approaches often fail to capture the plurality of user opinions, instead reinforcing majority viewpoints and marginalizing minority perspectives. To address this, we introduce PERSONA, a comprehensive and reproducible test bed designed to evaluate and improve pluralistic alignment in language models. Our approach utilizes synthetic personas, crafted through a combination of US census data and procedural generation, to simulate a wide array of user profiles with diverse demographic and idiosyncratic attributes. We present a detailed methodology for constructing a representative demographic of 1,586 personas, each enriched with individualistic personality traits and core values. Leveraging this synthetic demographic, we generate a large-scale preference dataset containing 3,868 prompts and 317,200 pairs of diverse feedback. This dataset enables the evaluation of language models' ability to align with both group-level and individual preferences across various controversial and value-laden topics. Our contributions include a systematic evaluation of current LM capabilities in role-playing diverse users, verified through human judges, and the establishment of a benchmark for pluralistic alignment approaches. Our work aims to facilitate the development of more inclusive and representative language models, paving the way for future research in global pluralistic alignment. The full dataset is available here \\href{https://sites.google.com/view/pluralistic}{https://sites.google.com/view/pluralistic}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U3ssobFgDL": {
    "title": "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance",
    "volume": "review",
    "abstract": "The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. This paper investigates the novel challenge of defending MLLMs against such attacks. Compared to large language models (LLMs), MLLMs include an additional image modality. We discover that images act as a \"foreign language\" that is not considered during safety alignment, making MLLMs more prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover all possible scenarios. This vulnerability is exacerbated by the fact that most state-of-the-art MLLMs are fine-tuned on limited image-text pairs that are much fewer than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during safety fine-tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play strategy that solves two subtasks: 1) identifying harmful responses via a lightweight harm detector, and 2) transforming harmful responses into harmless ones via a detoxifier. This approach effectively mitigates the risks posed by malicious visual inputs without compromising the original performance of MLLMs. Our results demonstrate that MLLM-Protector offers a robust solution to a previously unaddressed aspect of MLLM security",
    "checked": true,
    "id": "8d28d2ef602e8b518b7daecc39a0f2f8d2caaa09",
    "semantic_title": "mllm-protector: ensuring mllm's safety without hurting performance",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=WjlXdjwUYl": {
    "title": "Large Language Models Can Self-Correct with Minimal Effort",
    "volume": "review",
    "abstract": "Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple yet effective verification method can unleash inherent capabilities of the LLMs. That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response. The condition can be an entity in an open-domain question or a numeric value in a math question, which requires minimal effort (via prompting) to identify. We propose an iterative verify-then-correct framework to progressively identify and correct (probably) false responses, named ProCo. We conduct experiments on three reasoning tasks. On average, ProCo, with GPT-3.5-Turbo-1106 as the backend LLM, yields $+6.8$ exact match on four open-domain question answering datasets, $+14.1$ accuracy on three arithmetic reasoning datasets, and $+9.6$ accuracy on a commonsense reasoning dataset, compared to Self-Correct",
    "checked": true,
    "id": "99d4f4430461a38241617df52bc05717fdb851df",
    "semantic_title": "large language models can self-correct with minimal effort",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=3MzURC5YF4": {
    "title": "Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts",
    "volume": "review",
    "abstract": "This study presents a multi-modal multi-granularity tokenizer specifically designed for analyzing ancient Chinese scripts, focusing on the Chu bamboo slip (CBS) script used during the Spring and Autumn and Warring States period (771-256 BCE) in Ancient China. Considering the complex hierarchical structure of ancient Chinese scripts, where a single character may be a combination of multiple sub-characters, our tokenizer first adopts character detection to locate character boundaries, and then conducts character recognition at both the character and sub-character levels. Moreover, to support the academic community, we have also assembled the first large-scale dataset of CBSs with over 100K annotated character image scans. On the part-of-speech tagging task built on our dataset, using our tokenizer gives a 5.5\\% relative improvement in F1-score compared to mainstream sub-word tokenizers. Our work not only aids in further investigations of the specific script but also has the potential to advance research on other forms of ancient Chinese scripts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CpbflPKd0Z": {
    "title": "EditRoom: LLM-parameterized Graph Diffusion for Composable 3D Room Layout Editing",
    "volume": "review",
    "abstract": "Language-guided 3D scene editing has emerged as a pivotal technology in fields such as virtual reality, augmented reality, gaming, architecture, and film production. Traditional methods of 3D scene editing require extensive expertise and time due to the complexity of 3D environments. Recent advancements in language-guided 3D scene editing offer promising solutions, but existing approaches either limit editing to generated scenes or focus on appearance modifications without supporting comprehensive scene layout changes. In this work, we propose EditRoom, a novel framework for language-guided 3D room layout editing that addresses these limitations. EditRoom leverages Large Language Models (LLMs) for command planning and a graph diffusion-based method for executing six editing types: rotate, translate, scale, replace, add, and remove. In addition, we introduce EditRoom-DB, a large-scale dataset with 83k editing pairs, for training and evaluation purposes. Our approach significantly improves the accuracy and coherence of scene editing, effectively handling complex commands with multiple operations. Experimental results demonstrate EditRoom's superior performance in both single and complex editing scenarios, highlighting its potential for practical applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xtpwoThoBn": {
    "title": "OpenForecast: An Open-Domain and Large-Scale Dataset for LLM-Based Event Forecasting",
    "volume": "review",
    "abstract": "Event forecasting, which aims to forecast the future based on past events, continually attracted research interest and show potential with large language model (LLMs). However, existing studies and benchmarks treat it as a closed-domain problem, which restricts progress in the era of LLMs. Additionally, the diverse responses of LLMs in open-domain tasks make the evaluation challenging. In this work, we introduce OpenForecast, an open-domain event forecasting benchmark with three advantages: (1) large scale, encompassing 44692 significant historical events spanning from 1950 to 2024, and avoiding knowledge leakage for LLMs; (2) comprehensive forecasting tasks, including 3 open-domain tasks and 3 close-domain tasks; (3) automatic construction, proposing an automatic construction pipeline by using open-source language models. Furthermore, we conduct a thorough analysis on existing open-domain evaluation metrics and propose the Retrieval-Augmented Evaluation (RAE) to address the incomplete labeling issues. Experiments demonstrate that OpenForecast is quite challenging and reveal the problems of LLMs and existing evaluation methods. The OpenForecast and codes will be publicly released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KdOvmXA1ba": {
    "title": "Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models",
    "volume": "review",
    "abstract": "The burgeoning interest in Multimodal Large Language Models (MLLMs), such as OpenAI's GPT-4V(ision), has significantly impacted both academic and industrial realms. These models enhance Large Language Models (LLMs) with advanced visual understanding capabilities, facilitating their application in a variety of multimodal tasks. Recently, Google introduced Gemini, an advanced MLLM for multimodal integration. Despite its advancements, preliminary benchmarks indicate that Gemini lags behind GPT models in commonsense reasoning tasks. However, these assessments, using a limited dataset like HellaSWAG, may not fully reflect Gemini's true potential in commonsense reasoning. To address this gap, our study undertakes a thorough evaluation of Gemini's performance in complex reasoning tasks that necessitate the integration of commonsense knowledge across modalities. We carry out a comprehensive analysis of 12 commonsense reasoning datasets, ranging from general to domain-specific tasks. This includes 11 datasets focused solely on language, as well as one that incorporates multimodal elements. Our experiments across ten LLMs and two MLLMs demonstrate Gemini's competitive commonsense reasoning capabilities. We also highlight common challenges faced by current LLMs and MLLMs in commonsense reasoning, emphasizing the need for further advancements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=njgRRuqJNn": {
    "title": "PATIENT-Î¨: Using Large Language Models to Simulate Patients for Training Mental Health Professionals",
    "volume": "review",
    "abstract": "Mental illness remains one of the most critical public health issues. Despite its importance, many mental health professionals highlight a disconnect between their training and actual real-world patient practice. To help bridge this gap, we propose PATIENT-Î¨, a novel patient simulation framework for cognitive behavior therapy (CBT) training. To build PATIENT-Î¨, we construct diverse patient cognitive models based on CBT principles and use large language models (LLMs) programmed with these cognitive models to act as a simulated therapy patient. We propose an interactive training scheme, PATIENT-Î¨-TRAINER, for mental health trainees to practice a key skill in CBT -- formulating the cognitive model of the patient -- through role-playing a therapy session with PATIENT-Î¨. To evaluate PATIENT-Î¨, we conducted a comprehensive user study of 13 mental health trainees and 20 experts. The results demonstrate that practice using PATIENT-Î¨-TRAINER enhances the perceived skill acquisition and confidence of the trainees beyond existing forms of training such as textbooks, videos, and role-play with non-patients. Based on the experts' perceptions, PATIENT-Î¨ is perceived to be closer to real patient interactions than GPT-4, and PATIENT-Î¨-TRAINER holds strong promise to improve trainee competencies. We will release all our code and data upon acceptance of this paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DF9I6swsCa": {
    "title": "Training Task Experts through Retrieval Based Distillation",
    "volume": "review",
    "abstract": "One of the most reliable ways to create deployable models for specialized tasks is to obtain an adequate amount of high-quality task-specific data. However, for specialized tasks, often such datasets do not exist.Existing methods address this by creating such data from large language models (LLMs) and then distilling such knowledge into smaller models. However, these methods are limited by the quality of the LLMs output, and tend to generate repetitive or incorrect data. In this work, we present Retrieval Based Distillation (ReBase), a method that first retrieves data from rich online sources and then transforms them into domain-specific data. This method greatly enhances data diversity. Moreover, ReBase generates Chain-of-Thought reasoning and distills the reasoning capacity of LLMs. We test our method on 4 benchmarks and shows that our method significantly improves performance by up to 10.76% on SQuAD, 1.37% on MNLI, and 1.94% on BBH",
    "checked": true,
    "id": "044557354021be61a268c8f5b6da8fdf92270bc3",
    "semantic_title": "training task experts through retrieval based distillation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Acvvg21lst": {
    "title": "Adaptive Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization",
    "volume": "review",
    "abstract": "In recent years, large language models (LLMs) have driven advances in natural language processing. Still, their growing scale has increased the computational burden, necessitating a balance between efficiency and performance. Low-rank compression, a promising technique, reduces non-essential parameters by decomposing weight matrices into products of two low-rank matrices. Yet, its application in LLMs has not been extensively studied. The key to low-rank compression lies in low-rank factorization and low-rank dimensions allocation. To address the challenges of low-rank compression in LLMs, we conduct empirical research on the low-rank characteristics of large models. We propose a low-rank compression method suitable for LLMs. This approach involves precise estimation of feature distributions through pooled covariance matrices and a Bayesian optimization strategy for allocating low-rank dimensions. Experiments on the LLaMA-2 models demonstrate that our method outperforms existing strong structured pruning and low-rank compression techniques in maintaining model performance at the same compression ratio.\\footnote{The implementation code and model checkpoints are available at \\url{https://github.com/anonymous}.}",
    "checked": false,
    "id": "af3f455570b673be8ba8a51075f7f5955646a1e7",
    "semantic_title": "feature-based low-rank compression of large language models via bayesian optimization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=LzHaRaTT71": {
    "title": "An Investigation on Group Query Hallucination Attacks",
    "volume": "review",
    "abstract": "With the widespread use of large language models (LLMs), understanding their potential failure modes during user interactions is essential. In practice, users often pose multiple questions in a single conversation with LLMs. Therefore, in this study, we propose Group Query Attack, a technique that simulates this scenario by presenting groups of queries to LLMs simultaneously. We investigate how the accumulated context from consecutive prompts influences the outputs of LLMs. Specifically, we observe that Group Query Attack significantly degrades the performance of models fine-tuned on specific tasks. Moreover, we demonstrate that \\Tech\\ induces a risk of triggering potential backdoors of LLMs. Besides, Group Query Attack is also effective in tasks involving reasoning, such as mathematical reasoning and code generation for pre-trained and aligned models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qlVi4p8zD1": {
    "title": "Can we teach language models to gloss endangered languages?",
    "volume": "review",
    "abstract": "Interlinear glossed text (IGT) is a popular format in language documentation projects, where each morpheme is labeled with a descriptive annotation. Automating the creation of interlinear glossed text can be desirable to reduce annotator effort and maintain consistency across annotated corpora. Prior research has explored a number of statistical and neural methods for automatically producing IGT. As large language models (LLMs) have showed promising results across multilingual tasks, even for rare, endangered languages, it is natural to wonder whether they can be utilized for the task of generating IGT. We explore whether LLMs can be effective at the task of interlinear glossing with in-context learning, without any traditional training. We propose new approaches for selecting examples to provide in-context, observing that targeted selection can significantly improve performance. We find that LLM-based methods beat standard transformer baselines, despite requiring no training at all. These approaches still underperform state-of-the-art supervised systems for the task, but are highly practical for researchers outside of the NLP community, requiring minimal effort to use",
    "checked": true,
    "id": "cce4e698001b8902d0f2945fabefccee680406cb",
    "semantic_title": "can we teach language models to gloss endangered languages?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JD6fJzmFfm": {
    "title": "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking",
    "volume": "review",
    "abstract": "The rapid development of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has exposed vulnerabilities to various adversarial attacks. This paper provides a comprehensive overview of jailbreaking research targeting both LLMs and MLLMs, highlighting recent advancements in evaluation benchmarks, attack techniques and defense strategies. Compared to the more advanced state of unimodal jailbreaking, multimodal domain remains underexplored. We summarize the limitations and potential research directions of multimodal jailbreaking, aiming to inspire future research and further enhance the robustness and security of MLLMs",
    "checked": true,
    "id": "aab95a2479ae7a9dd168ef32314cd654ba8f590c",
    "semantic_title": "from llms to mllms: exploring the landscape of multimodal jailbreaking",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=gVnZwOcIgE": {
    "title": "Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora",
    "volume": "review",
    "abstract": "Media storms, dramatic outbursts of attention to a story, are central components of media dynamics and the attention landscape. Despite their importance, there has been little systematic and empirical research on this concept due to issues of measurement and operationalization. We introduce an iterative human-in-the-loop method to identify media storms in a large-scale corpus of news articles. The text is first transformed into signals of dispersion based on several textual characteristics. In each iteration, we apply unsupervised anomaly detection to these signals; each anomaly is then validated by an expert to confirm the presence of a storm, and those results are then used to tune the anomaly detection in the next iteration. We make available the resulting media storm dataset. Both the method and dataset provide a basis for comprehensive empirical study of media storms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hnj2MNpOKr": {
    "title": "Learning How to Prompt with Large Language Models",
    "volume": "review",
    "abstract": "The remarkable performance of large language models (LLMs) heavily depends on the prompts they receive. Inappropriate prompts can significantly hinder their performance or trigger undesirable behaviors, such as the amplification of societal biases. Traditional methods for addressing these issues often overlook valuable information from LLMs' pre-training phases and handle training data one by one, leading to a loss of crucial information. This paper presents an innovative framework called Learning to Prompt (L2P), which combines an LLM-based optimizer with meta-learning and the chain of thought mechanism. L2P enables effective optimization for each individual prompt and generalizes to new prompt optimization, significantly improving LLM performance. Our extensive evaluations confirm the superior performance of L2P over state-of-the-art methods",
    "checked": false,
    "id": "7397f9861a3c8110540a0ca55072d3d6ea54cba5",
    "semantic_title": "rewriting math word problems with large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=TIi1hD7G5A": {
    "title": "Advancing Vision-Language Models with Adapter Ensemble Strategies",
    "volume": "review",
    "abstract": "CLIP revolutes vision-language pretraining by using contrastive learning on paired web data. However, the sheer size of these pretrained models makes full-model finetuning exceedingly costly. One common solution is the \"adapter\", which finetunes a few additional parameters while freezing the backbone. It harnesses the heavy-duty backbone while offering a light finetuning for small downstream tasks. This synergy prompts us to explore the potential of augmenting large-scale backbones with traditional machine learning techniques. Often employed in traditional fields and overlooked in the large-scale era, these techniques could provide valuable enhancements. Herein, we delve into the \"adapter ensembles\" in the realm of large-scale pretrained vision-language models. We begin with a proof-of-concept study to establish the efficacy of combining multiple adapters. We then present extensive evidence showing these ensembles excel in a variety of settings, particularly when employing a Multi-Scale Attention (MSA) approach thoughtfully integrated into the ensemble framework. We further incorporate the LoRA to mitigate the additional parameter burden. We focus on vision-language retrieval, using different backbones under constraints of minimal data, parameters, and finetuning budgets. This research paves the way for a synergistic blend of traditional, yet effective, strategies with modern large-scale networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ykAqmbjDOg": {
    "title": "A 3D-ResNet Combined with BRNN: Application in the Auxiliary Diagnosis of ADHD",
    "volume": "review",
    "abstract": "Attention Deficit/Hyperactivity Disorder (ADHD) is a common mental disorder that exhibits a high incidence rate in children and adolescents, and it is also observed in adults. Currently, there is a lack of objective diagnostic methods for ADHD. Therefore, a three-dimensional residual network (3D-ResNet) deep learning method based on feature extraction from rs-fMRI images for assisting in the diagnosis of ADHD based on resting-state functional magnetic resonance imaging (rs-fMRI) and deep learning models was proposed in this paper. Taking into consideration the temporal characteristics of rs-fMRI, we constructed a 3D-ResNet model based on four-dimensional image. The model utilized TimeDistributed to encapsulate residual blocks which allowed the model to extract spatial features from rs-fMRI while preserving its temporal sequence information. We constructed four different hierarchical structures of 3D-ResNet which are subsequently combined with two different bidirectional recurrent neural networks (BRNNs) to extract sequence features. And BRNNs includes bidirectional long short-term memory (Bi-LSTM) and bidirectional gated recurrent unit (Bi-GRU). The proposed method utilized the ADHD-200 Consortium's public dataset for training and was validated by 5-fold cross-validation. The experimental results indicated that the proposed method in this study demonstrated superior performance on the dataset compared to traditional methods (Accuracy: 76.56%, Sensitivity: 80.16%, Specificity: 90.22%). Therefore, adopting this method can further enhance the accuracy of assisting in the diagnosis of ADHD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FnLMVYdXJe": {
    "title": "Detecting Online Community Practices with Large Language Models: A Case Study of Pro-Ukrainian Publics on Twitter",
    "volume": "review",
    "abstract": "Communities on social media display distinct patterns of linguistic expression and behaviour, collectively referred to as practices. These practices can be traced in textual exchanges, and reflect the intentions, knowledge, values, and norms of users and communities. This paper introduces a comprehensive methodological workflow for computational identification of such practices within social media texts. By focusing on supporters of Ukraine during the Russia-Ukraine war in (1) the activist collective NAFO and (2) the Eurovision Twitter community, we present a gold-standard data set capturing their unique practices. Using this corpus, we perform practice prediction experiments with both open-source baseline models and OpenAI's large language models (LLMs). Our results demonstrate that closed-source models, especially GPT-4, achieve superior performance, particularly with prompts that incorporate salient features of practices, or utilize Chain-of-Thought prompting. This study provides a detailed error analysis and offers valuable insights into improving the precision of practice identification, thereby supporting context-sensitive moderation and advancing the understanding of online community dynamics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UA3e4bVxU8": {
    "title": "Leveraging LEXICAL and GRAMMATICAL Errors: Extending ASR Error Measurements through NLP",
    "volume": "review",
    "abstract": "This paper addresses the limitations of current Automatic Speech Recognition (ASR) evaluation metrics by highlighting the inadequacies of overall error rates, particularly Word Error Rate. While this offers a broad assessment, it lacks the granularity needed to discern specific linguistic categories affected by errors. We offer an NLP-driven metric based on parts of speech and grammatical categories, to provide a more in-depth analysis of ASR errors. Using the Whisper ASR system on English, Japanese, and Spanish, within the CommonVoice 15 dataset, we analyze GRAMMATICAL and LEXICAL error rates. Results show that GRAMMATICAL words trigger less errors than LEXICAL words across all languages, and Proper Nouns in Japanese combined with case markers are related to higher accuracy. By categorizing errors based on these linguistic attributes, our methodology aims to enhance the explanatory power of error analysis in ASR, contributing to a more precise evaluation of system performance based on NLP approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y5xrfZ47if": {
    "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
    "volume": "review",
    "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of \"tit for tat\" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of \"tit for tat\" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents",
    "checked": true,
    "id": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
    "semantic_title": "encouraging divergent thinking in large language models through multi-agent debate",
    "citation_count": 189,
    "authors": []
  },
  "https://openreview.net/forum?id=O73iutFBl9": {
    "title": "Large Language Models are Biased to Detect Hallucination across Languages",
    "volume": "review",
    "abstract": "Hallucinations in generative AI, particularly in large language models (LLMs), have emerged as a significant concern. These models often facilitate multilingual operations, including querying and conversation. Yet, few research efforts have been devoted to understanding hallucinations in a multilingual context, specifically regarding the equitable treatment of supported languages, due to the lack of available benchmarks. Addressing this gap, this paper first proposes Poly-FEVER, a large-scale publicly accessible multilingual fact extraction and verification dataset for hallucination detection that covers 11 languages and more than 800K fact claims with diverse topics. We utilize Poly-FEVER to evaluate the hallucination detection capabilities of ChatGPT and LLaMA-2 series. Our investigation extends to exploring hallucination causes, employing Latent Dirichlet Allocation (LDA) for topic distribution analysis and web searches to assess resource imbalances. Furthermore, we propose a mitigation approach combining linguistic adjustments and resource-oriented strategies, including a trained LDA model and the Retrieval Augmented Generation (RAG) approach, to enhance the robustness and reliability of multilingual information verification in LLMs. Our findings highlight the critical need for multilingual benchmarks Poly-FEVER and demonstrate the potential of mitigation strategy to address biased detection abilities on hallucinations, thus contributing to the development of more equitable and reliable multilingual LLMs",
    "checked": false,
    "id": "2479fe3a646762c24230e6d97270bac4de2da600",
    "semantic_title": "litcab: lightweight language model calibration over short- and long-form responses",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=ZgVD1BnR6f": {
    "title": "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers",
    "volume": "review",
    "abstract": "The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present Factcheck-Bench, a holistic end-to-end framework for annotating and evaluating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels for fact-checking and correcting not just the final prediction, but also the intermediate steps that a fact-checking system might need to take. Based on this framework, we construct an open-domain factuality benchmark in three-levels of granularity: claim, sentence, and document. We further propose a system, Factcheck-GPT, which follows our framework, and we show that it outperforms several popular LLM fact-checkers. We make our annotation tool, annotated data, benchmark, and code available at \\url{http://anonymous.for.review}",
    "checked": true,
    "id": "72c62b3a2280e66499d5918fadc3c31474425768",
    "semantic_title": "factcheck-bench: fine-grained evaluation benchmark for automatic fact-checkers",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=x7fx4gVqrN": {
    "title": "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation",
    "volume": "review",
    "abstract": "As large language models achieve impressive scores on traditional benchmarks, an increasing number of researchers are becoming concerned about benchmark data leakage during pre-training, commonly known as the data contamination problem. To ensure fair evaluation, recent benchmarks release only the training and validation sets, keeping the test set labels closed-source. They require anyone wishing to evaluate his language model to submit the model's predictions for centralized processing and then publish the model's result on their leaderboard. However, this submission process is inefficient and prevents effective error analysis. To address this issue, we propose to variabilize benchmarks and evaluate language models dynamically. Specifically, we extract variables from each test case and define a value range for each variable. For each evaluation, we sample new values from these value ranges to create unique test cases, thus ensuring a fresh evaluation each time. We applied this variable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, and TruthfulQA, which cover mathematical generation and multiple-choice tasks. Our experimental results demonstrate that this approach provides a more accurate assessment of the true capabilities of language models, effectively mitigating the contamination problem",
    "checked": true,
    "id": "dfe926e52168827976a0b616b0f6aacd9de50267",
    "semantic_title": "varbench: robust language model benchmarking through dynamic variable perturbation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AyYIheGWkw": {
    "title": "Automatic Generation of In-Context Math Examples Using Multi-Modal Consistency",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have advanced Natural Language Processing (NLP) tasks but limited in mathematical reasoning. To address this, few-shot examples are used in prompts for in-context learning. However, existing methods require annotated datasets, resulting in higher computational costs and lower quality examples. To mitigate these limitations, we propose APMath, a framework that automatically generates high-quality in-context examples to enhance LLMs' mathematical reasoning. APMath ensures consistency across different modalities (e.g., Chain-of-Thought (CoT), code snippets, and equations) by generating and selecting mutations that improve response consistency. Evaluated on four math problem datasets, APMath outperforms six baselines, with LLM accuracy ranging from 87.0% to 99.3%. It surpasses the state-of-the-art in-context example retrieval method in three of the four datasets by 1.9% to 4.4%, without relying on an external annotated dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5MAJGeFRIV": {
    "title": "LongWanjuan: Towards Systematic Measurement for Long Text Quality",
    "volume": "review",
    "abstract": "The quality of training data are crucial for enhancing the long-text capabilities of foundation models. Despite existing efforts to refine data quality through heuristic rules and evaluations based on data diversity and difficulty, there's a lack of systematic approaches specifically tailored for assessing long texts. Addressing this gap, our work systematically measures the quality of long texts by evaluating three fundamental linguistic dimensions: coherence, cohesion, and complexity. Drawing inspiration from the aforementioned three dimensions, we introduce a suite of metrics designed to evaluate the quality of long texts, encompassing both statistical and pre-trained language model-based ones. Leveraging these metrics, we present LongWanjuan, a bilingual dataset specifically tailored to enhance the training of language models for long-text tasks with over 160B tokens. In LongWanjuan, we categorize long texts into holistic, aggregated, and chaotic types, enabling a detailed analysis of long-text quality. Furthermore, we devise a data mixture recipe that strategically balances different types of long texts within LongWanjuan, leading to significant improvements in model performance on long-text tasks",
    "checked": true,
    "id": "82f041ed71f8ef1cf462fa03a7e732e440259bd7",
    "semantic_title": "longwanjuan: towards systematic measurement for long text quality",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=DHtyB3ZblF": {
    "title": "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners",
    "volume": "review",
    "abstract": "This study introduces a hypothesis-testing framework to assess whether large language models (LLMs) possess genuine reasoning abilities or primarily depend on token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to investigate their token bias in solving logical reasoning tasks. Specifically, we develop carefully controlled synthetic datasets, featuring conjunction fallacy and syllogistic problems. Our framework outlines a list of hypotheses where token biases are readily identifiable, with all null hypotheses assuming genuine reasoning capabilities of LLMs. The findings in this study suggest, with statistical guarantee, that most LLMs still struggle with logical reasoning. While they may perform well on classic problems, their success largely depends on recognizing superficial patterns with strong token bias, thereby raising concerns about their actual reasoning and generalization abilities",
    "checked": true,
    "id": "00e88a0c006296e53bb7d4cfc90a134883ad34fd",
    "semantic_title": "a peek into token bias: large language models are not yet genuine reasoners",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=T16jwd5e4T": {
    "title": "Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Data",
    "volume": "review",
    "abstract": "Role-playing agents (RPA) have been a popular application area for large language models (LLMs), attracting significant interest from both industry and academia. While existing RPAs well portray the characters' knowledge and tones, they face challenges in capturing their minds, especially for small role-playing language models (RPLMs). In this paper, we propose to enhance RPLMs via personality-indicative data. Specifically, we leverage questions from psychological scales and distill advanced RPAs to generate dialogues that grasp the minds of characters. Experimental results validate that RPLMs trained with our dataset exhibit advanced role-playing capabilities for both general and personality-related evaluations",
    "checked": true,
    "id": "38fd1616d4e1e2e94ed3b5713ecfd60a8442a3bf",
    "semantic_title": "capturing minds, not just words: enhancing role-playing language models with personality-indicative data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JUlTFCPMuJ": {
    "title": "SymBa: Symbolic Backward Chaining for Structured Natural Language Reasoning",
    "volume": "review",
    "abstract": "While Large Language Models (LLMs) have demonstrated remarkable reasoning ability lately, providing a structured, explainable proof to ensure explainability, i.e. structured reasoning, still remains challenging. Among two directions of structured reasoning, we specifically focus on backward chaining, where the query is recursively decomposed to subgoals by applying inference rules. We point out that current popular backward chaining implementations (Least-to-most prompting and LAMBADA) fail to implement the necessary features of backward chaining, such as arbitrary-depth recursion and binding propagation. To this end, we propose a novel backward chaining framework, SymBa (Symbolic Backward Chaining). In SymBa, a symbolic solver controls the whole proof process, and an LLM searches for the relevant natural language premises and translates them into a symbolic form for the solver. By this LLM-solver integration, while producing a completely structured proof that is symbolically verified, SymBa achieves significant improvement in performance, proof accuracy, and efficiency in diverse structured reasoning benchmarks compared to baselines",
    "checked": true,
    "id": "9f64c85d2f857d2950ca57c1ec352c7557c78e80",
    "semantic_title": "symba: symbolic backward chaining for structured natural language reasoning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=3TIsPV9PO6": {
    "title": "Improving Zero-shot LLM Re-Ranker with Risk Minimization",
    "volume": "review",
    "abstract": "In the Retrieval-Augmented Generation (RAG) system, advanced Large Language Models (LLMs) have emerged as effective Query Likelihood Models (QLMs) in an unsupervised way, which re-rank documents based on the probability of generating the query given the content of a document. However, directly prompting LLMs to approximate QLMs inherently is biased, where the estimated distribution might diverge from the actual document-specific distribution. In this study, we introduce a novel framework, $\\mathrm{UR^3}$, which leverages Bayesian decision theory to both quantify and mitigate this estimation bias. Specifically, $\\mathrm{UR^3}$ reformulates the problem as maximizing the probability of document generation, thereby harmonizing the optimization of query and document generation probabilities under a unified risk minimization objective. Our empirical results indicate that $\\mathrm{UR^3}$ significantly enhances re-ranking, particularly in improving the Top-1 accuracy. It benefits the QA tasks by achieving higher accuracy with fewer input documents",
    "checked": true,
    "id": "c224c9a8490769bcdd7c7798008e67fdfaa350c4",
    "semantic_title": "improving zero-shot llm re-ranker with risk minimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P7bxwEk8Xo": {
    "title": "FATH: Authentication-based Test-time Defense against Indirect Prompt Injection Attacks",
    "volume": "review",
    "abstract": "Large language models (LLMs) have been widely deployed as the backbone with additional tools and text information for real-world applications. However, integrating external information into LLM-integrated applications raises significant security concerns. Among these, prompt injection attacks are particularly threatening, where malicious instructions injected in the external text information can exploit LLMs to generate answers as the attackers desire. While both training-time and test-time defense methods have been developed to mitigate such attacks, the unaffordable training costs associated with training-time methods and the limited effectiveness of existing test-time methods make them impractical. This paper introduces a novel test-time defense strategy, named Formatting AuThentication with Hash-based tags (FATH). Unlike existing approaches that prevent LLMs from answering additional instructions in external text, our method implements an authentication system, requiring LLMs to answer all received instructions but selectively filter out responses to user instructions as the final output. To achieve this, we utilize hash-based authentication tags to label each response, facilitating accurate identification of responses according to the user's instructions and improving the robustness against adaptive attacks. Comprehensive experiments demonstrate that our defense method can effectively defend the indirect prompt injection attacks, achieving state-of-the-art performance under Llama3 and GPT3.5 models across various attack methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XM7dMQnqCC": {
    "title": "VQ-TEGAN: Data Augmentation with Text Embeddings for Few-shot Learning",
    "volume": "review",
    "abstract": "Data augmentation is crucial for the fine-tuning of pre-trained models and the optimization of limited data utilization, particularly within the realm of few-shot learning. Traditionally, these techniques have been applied at the word and sentence levels, with little research conducted within the embedding space. This paper introduces VQ-TEGAN, a novel data augmentation approach designed to generate embeddings specifically for a few-shot learning. VQ-TEGAN generates embeddings that augment the few-shot dataset by training directly within the PLMs' word embedding, employing a customized loss function. Empirical valildation on GLUE benchmark datasets demonstrates that VQ-TEGAN markedly improves text classification performance. Additionally, we investigate the application of VQ-TEGAN with RoBERTa-large and BERT-large, offering insight for further application",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r6Eh3FbMZF": {
    "title": "UNO Arena for Evaluating Sequential Decision-Making Capability of Large Language Models",
    "volume": "review",
    "abstract": "Sequential decision-making refers to algorithms that take into account the dynamics of the environment, where early decisions affect subsequent decisions. With large language models (LLMs) demonstrating powerful capabilities between tasks, we can't help but ask: Can Current LLMs Effectively Make Sequential Decisions? In order to answer this question, we propose the UNO Arena based on the card game UNO to evaluate the sequential decision-making capability of LLMs and explain in detail why we choose UNO. In UNO Arena, We evaluate the sequential decision-making capability of LLMs dynamically with novel metrics based Monte Carlo methods. We set up random players, DQN-based reinforcement learning players, and LLM players (e.g. GPT-4, Gemini-pro) for comparison testing. Furthermore, in order to improve the sequential decision-making capability of LLMs, we propose the TUTRI player, which can involves having LLMs reflect their own actions wtih the summary of game history and the game strategy. Numerous experiments demonstrate that the TUTRI player achieves a notable breakthrough in the performance of sequential decision-making compared to the vanilla LLM player",
    "checked": true,
    "id": "833a2fb379e9d7faabcb55650fc6db39653277a1",
    "semantic_title": "uno arena for evaluating sequential decision-making capability of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B3Zv7qzEbX": {
    "title": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user's known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs' cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs",
    "checked": true,
    "id": "023a98af94a3e7e8e538a6183da8ec05024fec56",
    "semantic_title": "how well do llms represent values across cultures? empirical analysis of llm responses based on hofstede cultural dimensions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hXQy5jmVu6": {
    "title": "MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification",
    "volume": "review",
    "abstract": "To advance the evaluation of multimodal math reasoning in large multimodal models (LMMs), this paper introduces a novel benchmark, MM-MATH. MM-MATH consists of 5,929 open-ended middle school math problems with visual contexts, with fine-grained classification across difficulty, grade level, and knowledge points. Unlike existing benchmarks relying on binary answer comparison, MM-MATH incorporates both outcome and process evaluations. Process evaluation employs LMM-as-a-judge to automatically analyze solution steps, identifying and categorizing errors into specific error types. Extensive evaluation of ten models on MM-MATH reveals significant challenges for existing LMMs, highlighting their limited utilization of visual information and struggles with higher-difficulty problems. The best-performing model achieves only 31% accuracy on MM-MATH, compared to 82% for humans. This highlights the challenging nature of our benchmark for existing models and the significant gap between the multimodal reasoning capabilities of current models and humans. Our process evaluation reveals that diagram misinterpretation is the most common error, accounting for more than half of the total error cases, underscoring the need for improved image comprehension in multimodal reasoning",
    "checked": true,
    "id": "c49df7a70fd13060353d81e6d1cdc1bbbfd58357",
    "semantic_title": "mm-math: advancing multimodal math evaluation with process evaluation and fine-grained classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1w6HKalqgi": {
    "title": "Contrastive Classification via Linear Layer Extrapolation",
    "volume": "review",
    "abstract": "Early-exiting predictions in a deep Transformer network evolve from layer to layer in a somewhat smooth process. This has been exploited in language modeling to improve factuality (Chuang et al., 2023), with the observation that factual associations emerge in later layers. We find a similar process multiway emotion classification, motivating Linear Layer Extrapolation, which finds stable improvements by recasting contrastive inference as linear extrapolation. Experiments across multiple models and emotion classification datasets find that Linear Layer Extrapolation outperforms standard classification on fine-grained emotion analysis tasks",
    "checked": false,
    "id": "9624fc969ddeb81753b17fb96b174a59b537ffbb",
    "semantic_title": "efficient data generation for stroke classification via multilayer perceptron",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ZqHbcan6DG": {
    "title": "Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs",
    "volume": "review",
    "abstract": "Extractive summarization plays a pivotal role in natural language processing due to its wide-range applications in summarizing diverse content efficiently, while also being faithful to the original content. Despite significant advancement achieved in extractive summarization by Large Language Models (LLMs), these summaries frequently exhibit incoherence. An important aspect of the coherent summary is its readability for intended users. Although there have been many datasets and benchmarks proposed for creating coherent extractive summaries, none of them currently incorporate user intent to improve coherence in extractive summarization. Motivated by this, we propose a systematically created human-annotated dataset consisting of coherent summaries for five publicly available datasets and natural language user feedback, offering valuable insights into how to improve coherence in extractive summaries. We utilize this dataset for aligning LLMs through supervised fine-tuning with natural language human feedback to enhance the coherence of their generated summaries. Preliminary experiments with Falcon-40B and Llama-2-13B show significant performance improvements (~10% Rouge-L) in terms of producing coherent summaries. We further utilize human feedback to benchmark results over instruction-tuned models such as FLAN-T5 which resulted in several interesting findings",
    "checked": true,
    "id": "7b98ec261e39b1fe7310243a9d7bfb76c566f2ca",
    "semantic_title": "towards enhancing coherence in extractive summarization: dataset and experiments with llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8LdqmNyP0H": {
    "title": "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models",
    "volume": "review",
    "abstract": "Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders its generality. To overcome the limitation, this work proposes GLaPE, a gold label-agnostic prompt evaluation method to alleviate dependence on gold labels. GLaPE is composed of two critical aspects: self-consistency evaluation of a single prompt and mutual-consistency refinement across multiple prompts. Experimental results on 8 widely-recognized reasoning tasks demonstrate that GLaPE can produce more effective prompts, achieving performance comparable to those derived from manually annotated gold labels. Analysis shows that GLaPE provides reliable evaluations aligned with accuracy, even in the absence of gold labels. Code is publicly available at **Anonymous**",
    "checked": false,
    "id": "efa45e3772be3665d364a27a8ae022714efbbaa1",
    "semantic_title": "glape: gold label-agnostic prompt evaluation and optimization for large language model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jyirCJP6GV": {
    "title": "SoupLM: Model Integration in Large Language and Multi-Modal Models",
    "volume": "review",
    "abstract": "Training large language models (LLMs) and multimodal LLMs necessitates significant computing resources, and existing publicly available LLMs are typically pre-trained on diverse, privately curated datasets spanning various tasks. For instance, LLaMA, Vicuna, and LLaVA are three LLM variants trained with LLaMA base models using very different training recipes, tasks, and data modalities. The training cost and complexity for such LLM variants grow rapidly. In this work, we propose to use a soup strategy to assemble these LLM variants into a single well-generalized multimodal LLM (SoupLM) in a cost-efficient manner. Assembling these LLM variants efficiently brings knowledge and specialities trained from different domains and data modalities into an integrated one (e.g., chatbot speciality from user-shared conversations for Vicuna, and visual capacity from vision-language data for LLaVA), therefore, to avoid computing costs of repetitive training on several different domains. We propose series of soup strategies to systematically benchmark performance gains across various configurations, and probe the soup behavior across base models in the interpolation space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fpXjPbLxUH": {
    "title": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging",
    "volume": "review",
    "abstract": "Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with $\\textbf{TreeInstruct}$, an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student's conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting. In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixes-- all carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct's state-of-the-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct's ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning",
    "checked": true,
    "id": "4d9d8635c4a849b84db9d95e3c1b3249c05c1193",
    "semantic_title": "instruct, not assist: llm-based multi-turn planning and hierarchical questioning for socratic code debugging",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uDhgufpecX": {
    "title": "The Privileged Students: On the Value of Initialization in Multilingual Knowledge Distillation",
    "volume": "review",
    "abstract": "Knowledge distillation (KD) has proven to be a successful strategy to improve the performance of a smaller model in many NLP tasks. However, most of the work in KD only explores monolingual scenarios. In this paper, we investigate the value of KD in multilingual settings. We find the significance of KD and model initialization by analyzing how well the student model acquires multilingual knowledge from the teacher model. Our proposed method emphasizes copying the teacher model's weights directly to the student model to enhance initialization. Our finding shows that model initialization using copy-weight from the fine-tuned teacher contributes the most compared to the distillation process itself across various multilingual settings. Furthermore, we demonstrate that efficient weight initialization preserves multilingual capabilities even in low-resource scenarios",
    "checked": true,
    "id": "fa2e9f92ff515eb8d9287afe5765e16336741c82",
    "semantic_title": "the privileged students: on the value of initialization in multilingual knowledge distillation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w2Tkz8gKIQ": {
    "title": "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models",
    "volume": "review",
    "abstract": "Smaller-scale Vision-Langauge Models (VLMs) often claim to perform on par with larger models in general-domain visual grounding and question-answering benchmarks while offering advantages in computational efficiency and storage. However, their ability to handle rare objects, which fall into the long tail of data distributions, is less understood. To rigorously evaluate this aspect, we introduce the \"Uncontextualized Uncommon Objects\" (UOUO) benchmark. This benchmark focuses on systematically testing VLMs with both large and small parameter counts on rare and specialized objects. Our comprehensive analysis reveals that while smaller VLMs maintain competitive performance on common datasets, they significantly underperform on tasks involving uncommon objects. We also propose an advanced, scalable pipeline for data collection and cleaning, ensuring the UOUO benchmark provides high-quality, challenging instances. These findings highlight the need to consider long-tail distributions when assessing the true capabilities of VLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rdh264rHhe": {
    "title": "Schema-Driven Information Extraction from Heterogeneous Tables",
    "volume": "review",
    "abstract": "In this paper, we explore the question of whether large language models can support cost-efficient information extraction from tables. We introduce schema-driven information extraction, a new task that transforms tabular data into structured records following a human-authored schema. To assess various LLM's capabilities on this task, we present a benchmark comprised of tables from four diverse domains: machine learning papers, chemistry literature, material science journals, and webpages. We use this collection of annotated tables to evaluate the ability of open-source and API-based language models to extract information from tables covering diverse domains and data formats. Our experiments demonstrate that surprisingly competitive performance can be achieved without requiring task-specific pipelines or labels, achieving F1 scores ranging from 74.2 to 96.1, while maintaining cost efficiency. Moreover, through detailed ablation studies and analyses, we investigate the factors contributing to model success and validate the practicality of distilling compact models to reduce API reliance",
    "checked": true,
    "id": "983adfc42734a7ec2595a6b62352de967ab9000a",
    "semantic_title": "schema-driven information extraction from heterogeneous tables",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=QIzVVM3YOT": {
    "title": "Words Matter: Reducing Stigma in Online Conversations about Substance Use with Large Language Models",
    "volume": "review",
    "abstract": "Stigma is a barrier to treatment for individuals struggling with substance use disorders (SUD), which leads to significantly lower treatment engagement rates. With only 7% of those affected receiving any form of help, societal stigma not only discourages individuals with SUD from seeking help but isolates them, hindering their recovery journey and perpetuating a cycle of shame and self-doubt. This study investigates how stigma manifests on social media, particularly Reddit, where anonymity can exacerbate discriminatory behaviors. We analyzed over 1.2 million posts, identifying 3,207 that exhibited stigmatizing language towards people who use substances (PWUS). Using Informed and Stylized LLMs, we develop a model for de-stigmatization of these expressions into empathetic language, resulting in 1,649 reformed phrase pairs. Our paper contributes to the field by proposing a computational framework for analyzing stigma and destigmatizing online content, and delving into the linguistic features that propagate stigma towards PWUS. Our work not only enhances understanding of stigma's manifestations online but also provides practical tools for fostering a more supportive digital environment for those affected by SUD. Code and data will be made publicly available upon acceptance",
    "checked": true,
    "id": "d1e1becf715ecb792e242513934577ff8bf7caa1",
    "semantic_title": "words matter: reducing stigma in online conversations about substance use with large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5PmyXcU6Tp": {
    "title": "Large Language Models Can Plan Your Travels Rigorously with Formal Verification Tools",
    "volume": "review",
    "abstract": "In Xie et al. (2024), the authors proposed TravelPlanner, a U.S. domestic travel planning benchmark, and showed that LLMs themselves cannot make travel plans that satisfy user requirements with a best success rate of 0.6\\%. The state-of-the-art methods that combine LLMs with external critics, verifiers, and humans can only improve the success rate to 20\\% (Kambhampati et al.). In this work, we propose a framework that enables LLMs to formally formulate and solve combinatorial search problems such as the travel planning problem as a satisfiability modulo theory (SMT) problem, and use SMT solvers to automatically and interactively solve them. The SMT solvers guarantee to find a plan when input constraints are satisfiable. When the input constraints cannot be satisfiable, our LLM-based framework can interactively and adaptively offer modification suggestions to users using SMT solvers' capability of identifying the unsatisfiable core. We evaluate our framework with TravelPlanner and achieve a success rate of 97\\% for satisfiable queries. We also create a separate dataset that contains international travel benchmarks and show that when initial user queries are unsatisfiable, our interactive planning framework can generate valid plans with an average success rate of 78.6\\% for the international travel benchmark and 85.0\\% for TravelPlanner according to diverse humans preferences. We show that our framework could achieve zero-shot generalization to unseen constraints in travel planning problems. In addition, we introduce four new combinatorial optimization tasks and show that our framework could generalize well to new domains in a zero-shot manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AnCTnj6bR6": {
    "title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models",
    "volume": "review",
    "abstract": "The instruction-following ability of large language models enables humans to interact with AI agents in a natural way.However, when required to generate responses of a specific length, large language models often struggle to meet users' needs due to their inherent difficulty in accurately perceiving numerical constraints. To explore the ability of large language models to control the length of generated responses, we propose the Target Length Generation task (TLG) and design two metrics, Precise Match (PM) and Flexible Match (FM) to evaluate the model's performance in adhering to specified response lengths. Furthermore, we introduce a novel, model-agnostic approach called Ruler, which employs Meta Length Tokens (MLTs) to enhance the instruction-following ability of large language models under length-constrained instructions. Specifically, Ruler equips LLMs with the ability to generate responses of a specified length based on length constraints within the instructions. Moreover, Ruler can automatically generate appropriate MLT when length constraints are not explicitly provided, demonstrating excellent versatility and generalization. Comprehensive experiments show the effectiveness of Ruler across different LLMs on Target Length Generation Task, e.g., 28.25 average gain on FM, 18.40 average gain on PM. In addition, we conduct extensive ablation experiments to further substantiate the efficacy and generalization of Ruler. Our code and data will be made publicly available upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KY21Lzjpbx": {
    "title": "CRQBench: A Benchmark of Code Reasoning Questions",
    "volume": "review",
    "abstract": "Large Language Models have demonstrated exceptional proficiency on coding tasks, but it is challenging to precisely evaluate their code reasoning ability. Existing benchmarks are insufficient as they are unrealistic and conflate semantic reasoning ability with performance on software engineering tasks. We introduce CRQBench, a benchmark of 100 C++ code reasoning questions and answers derived from contextualized code review comments. To curate CRQBench, we use an LLM assistant alongside human inspection, reducing manual effort. We conduct an evaluation of GPT-4 on CRQBench and find that it produces correct responses grounded in the given context for 65 of the 100 questions",
    "checked": true,
    "id": "55067f59e0b9bd080c826276228297f5049cbd00",
    "semantic_title": "crqbench: a benchmark of code reasoning questions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IwKGTI5KTF": {
    "title": "Coverage-based Fairness in Multi-document Summarization",
    "volume": "review",
    "abstract": "Fairness in multi-document summarization (MDS) measures whether a system can generate a summary fairly representing information from documents with different social attribute values. Fairness in MDS is crucial since a fair summary can offer readers a comprehensive view. Previous works focus on quantifying summary-level fairness using Proportional Representation, a fairness measure based on Statistical Parity. However, Proportional Representation does not consider redundancy in input documents and overlooks corpus-level unfairness. In this work, we propose a new summary-level fairness measure, Equal Coverage, which is based on coverage of documents with different social attribute values and considers the redundancy within documents. To detect the corpus-level unfairness, we propose a new corpus-level measure, Coverage Parity. Our human evaluations show that our measures align with the human perception of fairness. Using our measures, we evaluate the fairness of ten different LLMs. We find that Llama2 is the fairest among all evaluated LLMs. We also find that almost all LLMs overrepresent different social attribute values",
    "checked": false,
    "id": "f28bad7ffbfc23493ca882326fb55b80883a4e6e",
    "semantic_title": "understanding position bias effects on fairness in social multi-document summarization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8adXCDiX8x": {
    "title": "LLMs Are Biased Towards Output Formats! Systematically Evaluating and Mitigating Output Format Bias of LLMs",
    "volume": "review",
    "abstract": "We present the first systematic evaluation examining format bias in performance of large language models (LLMs). Our approach distinguishes between two categories of an evaluation metric under format constraints to reliably and accurately assess performance: one measures performance when format constraints are adhered to, while the other evaluates performance regardless of constraint adherence. We then define a metric for measuring the format bias of LLMs and establish effective strategies to reduce it. Subsequently, we present our empirical format bias evaluation spanning four commonly used categories---multiple-choice question-answer, wrapping, list, and mapping---covering $15$ widely-used formats. Our evaluation on eight generation tasks uncovers significant format bias across state-of-the-art LLMs. We further discover that improving the format-instruction following capabilities of LLMs across formats potentially reduces format bias. Based on our evaluation findings, we study prompting and fine-tuning with synthesized format data techniques to mitigate format bias. Our methods successfully reduce the variance in ChatGPT's performance among wrapping formats from $235.33$ to $0.71$ (%$^2$)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NFKWnxtTSH": {
    "title": "Mitigation and Evaluation for Gender Stereotype under Unfair Escape",
    "volume": "review",
    "abstract": "Gender bias and stereotypes have long been concerns in language models. The training data for the model is derived from social products, which inevitably introduces potential unfairness. Existing datasets and methods lack concerns on diversified insight and mitigation efficiency. Based on the above issues, we propose an integrated and closed-loop framework for the data construction, mitigation method and evaluation for this task. Through this framework, we develop a diversified generative evaluation dataset that encompasses various perspectives on gender prejudice and the unfair escape ability LLMs possess. Further, we propose balanced prompting to effectively alleviate the inherent bias of the model. To evaluate the unbiased capability of the LLMs, we introduce the opinion consistency evaluation method. We demonstrate the effectiveness of the proposed framework through extensive experiments. Our code and datasets will release in \\href{https://anonymous.4open.science/r/Bias_dataset-8565}{https://anonymous.4open.science/r/Bias\\_dataset-8565}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eqkyfdseNp": {
    "title": "EASIER: Relevance-Boosted Captioning and Structural Information Extraction for Zero-Shot Video-Text Retrieval",
    "volume": "review",
    "abstract": "While recent progress in video-text retrieval (VTR) has been advanced by the exploration of supervised representation learning, in this paper, we present a novel zero-shot VTR framework, EASIER, to retrieve video/text with off-the-shelf captioning methods, large language models (LLMs), and text retrieval methods. Specifically, we first map videos into captions and then retrieve video captions and text using text retrieval methods, without any model training or fine-tuning. However, due to the limited power of captioning methods, the captions often miss important content in the video, resulting in unsatisfactory retrieval performance. To translate more information into video captions, we designed a novel relevance-boosted caption generation method, bringing extra relevant details into video captions by LLMs. Moreover, to emphasize key information and reduce the noise brought by imagination, we extract key visual tokens from captions and design different templates for structuring these tokens with the proposed structural information extraction, further boosting the retrieval performance. Benefiting from the enriched captions and structuralized information, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of EASIER over existing fine-tuned and pretraining methods without any data. A comprehensive study with both human and automatic evaluations shows that the enriched captions capture the key details and barely bring noise to the captions. Codes and data will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HpIX4JDFM8": {
    "title": "How to Leverage Digit Embeddings to Represent Numbers?",
    "volume": "review",
    "abstract": "Apart from performing arithmetic operations, understanding numbers themselves is still a challenge for existing language models. Simple generalisations, such as solving 100+200 instead of 1+2, can substantially affect model performance (Sivakumar and Moosavi, 2023). Among various techniques, character-level embeddings of numbers have emerged as a promising approach to improve number representation. However, this method has limitations as it leaves the task of aggregating digit representations to the model, which lacks direct supervision for this process. In this paper, we explore the use of mathematical priors to compute aggregated digit embeddings and explicitly incorporate these aggregates into transformer models. This can be achieved either by adding a special token to the input embeddings or by introducing an additional loss function to enhance correct predictions. We evaluate the effectiveness of incorporating this explicit aggregation, analysing its strengths and shortcomings, and discuss future directions to better benefit from this approach. Our methods, while simple, are compatible with any pretrained model and require only a few lines of code, which we have made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TPYhQQX0Ab": {
    "title": "AutoClean: LLMs Can Prepare Their Own Training Corpus",
    "volume": "review",
    "abstract": "Recent studies highlight the reliance of Large Language Models (LLMs) on high-quality, diverse data for optimal performance. The data sourced from the Internet often aggregated into datasets like the Common Crawl corpus, presents significant quality variability and necessitates extensive cleaning. Moreover, specific domain knowledge is usually presented in HTML, but there is a lack of effective methods to clean them into the training corpus automatically. Traditional cleaning methods involve either labor-intensive human teams that lack scalability or static heuristics that lead to suboptimal outcomes and are unable to be applied to specific target domains. In this paper, inspired by the recent progress in employing LLMs as versatile agents for diverse tasks, we take the initiative to explore the potential of these agents in automating data-cleaning methodologies. By configuring LLMs as an agent team that imitates the human data-cleaning team, we can automatically generate cleaning rules that traditionally require the involvement of data-cleaning experts. These rules are developed using a limited number of data samples and can then be applied broadly to substantial portions of raw data from the same domain. We demonstrate the efficiency and effectiveness of AutoClean on both pre-train scale corpora such as Common Crawl and specific target websites. Both automatic and human evaluations of the quality of the cleaned content highlight the feasibility of using LLMs to prepare their training corpus",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WzL8o3g9hG": {
    "title": "SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs",
    "volume": "review",
    "abstract": "Synthetic data generation has gained significant attention recently for its utility in training large vision and language models. However, the application of synthetic data to the training of multimodal context-augmented generation systems has been relatively unexplored. This gap in existing work is important because existing vision and language models (VLMs) are not trained specifically for context-augmented generation. Resources for adapting such models are therefore crucial for enabling their use in retrieval-augmented generation (RAG) settings, where a retriever is used to gather relevant information that is then subsequently provided to a generative model via context augmentation. To address this challenging problem, we generate SK-VQA: a large synthetic multimodal dataset containing over 2 million question-answer pairs which require external knowledge to determine the final answer. Our dataset is both larger and significantly more diverse than existing resources of its kind, possessing over 11x more unique questions and containing images from a greater variety of sources than previously-proposed datasets. Through extensive experiments, we demonstrate that our synthetic dataset can not only serve as a challenging benchmark, but is also highly effective for adapting existing generative multimodal models for context-augmented generation",
    "checked": true,
    "id": "17b2f5768a477673bbc995718284578638c0ac49",
    "semantic_title": "sk-vqa: synthetic knowledge generation at scale for training context-augmented multimodal llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zwkKn1v6J1": {
    "title": "ALPACA AGAINST VICUNA: Using LLMs to Uncover Memorization of LLMs",
    "volume": "review",
    "abstract": "In this paper, we introduce a black-box prompt optimization method that uses an attacker LLM agent to uncover higher levels of memorization in a victim agent, compared to what is revealed by prompting the target model with the training data directly, which is the dominant approach of quantifying memorization in LLMs. We use an iterative rejection-sampling optimization process to find instruction-based prompts with two main characteristics: (1) minimal overlap with the training data to avoid presenting the solution directly to the model, and (2)maximal overlap between the victim model's output and the training data, aiming to induce the victim to spit out training data. We observe that our instruction-based prompts generate outputs with 23.7% higher overlap with training data compared to the baseline prefix-suffix measurements. We analyze our attack in two settings: a practical approach with limited access to the sequence, excluding the suffix, and to demonstrate an empirical upper-bound scenario on the power of the attack where we have full sequence access but impose a penalty to discourage direct solutions. Our findings show that (1)instruction-tuned models can expose pre-training data as much as their base-models, if not more so, (2) contexts other than the original training data can lead to leakage, and (3) using instructions proposed by other LLMs can open a new avenue of automated attacks that we should further study and explore",
    "checked": true,
    "id": "97c17683dbd3bea6972b93f5ab8ebb208b344ee6",
    "semantic_title": "alpaca against vicuna: using llms to uncover memorization of llms",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=DZhqSyXAgo": {
    "title": "Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting Framework",
    "volume": "review",
    "abstract": "Retrieval-augmented generation (RAG) has emerged as a popular solution to mitigate the hallucination issues of large language models (LLMs). However, existing studies on RAG seldom address the issue of predictive uncertainty, i.e., how likely it is that a RAG model's prediction is incorrect, resulting in uncontrollable risks in real-world applications. In this work, we emphasize the importance of risk control, ensuring that RAG models proactively refuse to answer questions with low confidence. Our research identifies two critical latent factors affecting RAG's confidence in its predictions: the quality of the retrieved results and the manner in which these results are utilized. To guide RAG models in assessing their own confidence based on these two latent factors, we develop a counterfactual prompting framework that induces the models to alter these factors and analyzes the effect on their answers. We also introduce a benchmarking procedure to collect answers with the option to abstain, facilitating a series of experiments. For evaluation, we introduce several risk-related metrics and the experimental results demonstrate the effectiveness of our approach. Our code and benchmark dataset are available at https://anonymous.4open.science/r/RC-RAG-0367",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jemQNMrpR0": {
    "title": "Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision",
    "volume": "review",
    "abstract": "Process supervision, using a trained verifier to evaluate the intermediate steps generated by a reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid the expensive effort of human annotation on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions. Inaccuracies of the reasoner would cause MiPS underestimating the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior observations on human curated data. Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67\\% on GSM8K, +4.16\\% on MATH, +0.92\\% on MBPP compared with an output supervision trained verifier). Additionally, our study demonstrates that the verifier exhibits strong generalization ability across different reasoning models",
    "checked": true,
    "id": "ba1ce9f45b5a84d8c8609c1db23aebc887c0ae4d",
    "semantic_title": "multi-step problem solving through a verifier: an empirical analysis on model-induced process supervision",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=fWKgTz5sRL": {
    "title": "Pretrained Soft Prompt for Few-shot Unlearning via Relearning",
    "volume": "review",
    "abstract": "We propose PURE, a novel few-shot unlearning framework for LLMs. PURE conceptualizes the unlearning tasks as learning tasks, focusing on learning the new labels for the target instances. PURE integrates the task-specific and instance-specific information into a shared source prompt, called TI prompt and adapts TI prompt to downstream unlearning tasks. We conduct the experiments across different few-shot unlearning tasks and the results demonstrate that PURE significantly outperforms state-of-the-art baselines despite fine-tuning only 1\\% to 2\\% of the LLM's parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rOvr0w1R7J": {
    "title": "Step-Back Profiling: Distilling User Interactions for Personalized Scientific Writing",
    "volume": "review",
    "abstract": "Large language models (LLM) excel at a variety of natural language processing tasks, yet they struggle to generate personalized content for individuals, particularly in real-world scenarios like scientific writing. Addressing this challenge, we introduce Step-Back Profiling to personalize LLMs by distilling user history into concise profiles, including essential traits and preferences of users. Regarding our experiments, we construct a Personalized Scientific Writing (PSW) dataset to study multi-user personalization. PSW requires the models to write scientific papers given specialized author groups with diverse academic backgrounds. As for the results, we demonstrate the effectiveness of capturing user characteristics via Step-Back Profiling for collaborative writing. Moreover, our approach outperforms the baselines by up to 3.6 points on the general personalization benchmark (LaMP), including 7 personalization LLM tasks. Our extensive ablation studies validate the contributions of different components in our method and provide insights into our task definition. Our dataset and code will be available upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HRiAT7jaLB": {
    "title": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension",
    "volume": "review",
    "abstract": "As neural language models achieve human-comparable performance on Machine Reading Comprehension (MRC) and see widespread adoption, ensuring their robustness in real-world scenarios has become increasingly important. Current robustness evaluation research, though, primarily develops synthetic perturbation methods, leaving unclear how well they reflect real life. Considering this, we present a framework to automatically examine MRC models on occurring textual perturbations, by replacing paragraph in MRC benchmarks with their counterparts based on available Wikipedia edit history. Such perturbation type is natural as its design does not stem from an artificial generative process, inherently distinct from the previously investigated synthetic approaches. In a large-scale study encompassing various model architectures we observe that natural perturbations result in performance degradation in pre-trained encoder langauge models, with errors extending to Flan-T5 and Large Language Models (LLMs). We also show that exposing encoder-only models to naturally perturbed examples during training contributes to handling natural perturbations. This adversarial training approach, however, is not able to promote performance improvement on the majority of synthetic perturbations, indicating that many types of synthetic noise do not actually exist in our collected real-world textual perturbations. We hope this study will inspire future robustness investigation efforts to focus more on natural perturbations, thus deepening our understanding of how models respond to realistic linguistic challenges and providing insights into practical robustness enhancement strategies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TeRPGOGWRb": {
    "title": "Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models",
    "volume": "review",
    "abstract": "Recent studies have demonstrated the effectiveness of using large language language models (LLMs) in passage ranking. The listwise approaches, such as RankGPT, have become new state-of-the-art in this task. However, the efficiency of RankGPT models is limited by the maximum context length and relatively high latency of LLM inference. To address these issues, in this paper, we propose PE-Rank, leveraging the single passage embedding as a good context compression for efficient listwise passage reranking. By treating each passage as a special token, we can directly input passage embeddings into LLMs, thereby reducing input length. Additionally, we introduce an inference method that dynamically constrains the decoding space to these special tokens, accelerating the decoding process. For adapting the model to reranking, we employ listwise learning to rank loss for training. Evaluation results on multiple benchmarks demonstrate that PE-Rank significantly improves efficiency in both prefilling and decoding, while maintaining competitive ranking effectiveness",
    "checked": true,
    "id": "e030fe3a67d91e85f7da6a599b7fde7289062a25",
    "semantic_title": "leveraging passage embeddings for efficient listwise reranking with large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R2b1gSEbaN": {
    "title": "A Three-Pronged Approach to Cross-Lingual Adaptation with Multilingual LLMs",
    "volume": "review",
    "abstract": "Low-resource languages, by its very definition, tend to be under represented in the pre-training corpora of Large Language Models. In this work, we investigate three low-resource cross-lingual approaches that enable an LLM adapt to tasks in previously unseen languages. Llama-2 is an LLM where Indic languages, among many other language families, contribute to less than $0.005\\%$ of the total $2$ trillion token pre-training corpora. In this work, we experiment with the English-dominated Llama-2 for cross-lingual transfer to three Indic languages, Bengali, Hindi, and Tamil as target languages. We study three approaches for cross-lingual transfer, under ICL and fine-tuning. One, we find that adding additional supervisory signals via a dominant language in the LLM, leads to improvements, both under in-context learning and fine-tuning. Two, adapting the target languages to word reordering may be beneficial under ICL, but its impact diminishes with fine tuning. Finally, continued pre-training in one low-resource language can improve model performance for other related low-resource languages",
    "checked": true,
    "id": "59bca396192b881381ea43110db8899087238ed2",
    "semantic_title": "a three-pronged approach to cross-lingual adaptation with multilingual llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5CBIuoWLol": {
    "title": "$R^3$: \"This is My SQL, Are You With Me?\" A Consensus-Based Multi-Agent System for Text-to-SQL Tasks",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated strong performance on various tasks. To unleash their power on the Text-to-SQL task, we propose $R^3$ (Review-Rebuttal-Revision), a consensus-based multi-agent system for Text-to-SQL tasks. $R^3$ outperforms the existing single LLM Text-to-SQL systems and as well as the multi-agent Text-to-SQL systems by $1.3\\%$ to $8.1\\%$ on Spider and Bird. Surprisingly, we find that for Llama-3-8B, $R^3$ outperforms chain-of-thought prompting by over 20\\%, even outperforming GPT-3.5 on the development set of Spider",
    "checked": false,
    "id": "64766808635c5f0d6ab20c807381994c78f3d19c",
    "semantic_title": "$r^3$:\"this is my sql, are you with me?\"a consensus-based multi-agent system for text-to-sql tasks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=I6oClTK5xq": {
    "title": "Eagle: A Family of Advanced Arabic Large Language Models",
    "volume": "review",
    "abstract": "In this paper, we present Eagle, a suite of large language models (LLMs) designed for the Arabic language, built on Mistral-7b, LLaMA2-7b, and LLaMA3-7b. These models are pre-trained on the Oasis dataset, containing approximately 35 billion Arabic tokens, and further enhanced through instruction fine-tuning and reinforcement learning with AI feedback. We also introduce Amwaj, an Arabic embedding model for retrieval-augmented generation, and AraPO, a novel alignment method for improved Arabic culture alignment. To evaluate our models, we present OpenArabicEval, a diverse benchmark of 32 datasets covering comprehensive multiple-choice evaluation, natural language understanding, natural language generation, and long context evaluation. Extensive testing with OpenArabicEval demonstrates our models' exceptional performance and robustness across various NLP tasks, highlighting their effectiveness in processing Arabic. OpenArabicEval is the first benchmark to feature Long Context evaluation for Arabic LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MT4WEGzP5J": {
    "title": "Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets",
    "volume": "review",
    "abstract": "Language models, characterized by their black-box nature, often hallucinate and display sensitivity to input perturbations, causing concerns about trust. To enhance trust, it is imperative to gain a comprehensive understanding of the model's failure modes and develop effective strategies to improve their performance. In this study, we introduce a methodology designed to examine how input perturbations affect language models across various scales, including pre-trained models and large language models (LLMs). Utilizing fine-tuning, we enhance the model's robustness to input perturbations. Additionally, we investigate whether exposure to one perturbation enhances or diminishes the model's performance with respect to other perturbations. To address robustness against multiple perturbations, we present three distinct fine-tuning strategies. Furthermore, we broaden the scope of our methodology to encompass large language models (LLMs) by leveraging a chain of thought (CoT) prompting approach augmented with exemplars. We employ the Tabular-NLI task to showcase how our proposed strategies adeptly train a robust model, enabling it to address diverse perturbations while maintaining accuracy on the original dataset",
    "checked": true,
    "id": "3f8332a84034eed88f46c9720df7f1260ce0d721",
    "semantic_title": "evaluating concurrent robustness of language models across diverse challenge sets",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=IICFsvKyW6": {
    "title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new knowledge from existing one. While it has been widely studied in the context of knowledge graphs (KGs), knowledge reasoning in LLMs remains underexplored. In this paper, we introduce Chain-of-Knowledge, a comprehensive framework for knowledge reasoning, including methodologies for both dataset construction and model learning. For dataset construction, we create dataset KnowReason via rule mining on KGs. For model learning, we observe rule overfitting induced by naive training. Hence, we enhance CoK with a trial-and-error mechanism that simulates the human process of internal knowledge exploration. We conduct extensive experiments with KnowReason. Our results show the effectiveness of CoK in refining LLMs in not only knowledge reasoning, but also general reasoning benchmarks",
    "checked": true,
    "id": "309f87b79345ee2b48857a05b496ec4b2798ac80",
    "semantic_title": "chain-of-knowledge: integrating knowledge reasoning into large language models by learning from knowledge graphs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uvc1SDMdZK": {
    "title": "Low-rank Subspace for Binding in Large Language Models",
    "volume": "review",
    "abstract": "Entity tracking is essential for complex reasoning. To perform in-context entity tracking, language models (LMs) must bind an entity to its attribute (e.g., bind a container to its content) to recall attribute for a given entity. For example, given a context mentioning \"The coffee is in Box Z, the stone is in Box M, the map is in Box H\", to infer \"Box Z contains the coffee\" later, LMs must bind \"Box Z\" to \"coffee\". To explain the binding behaviour of LMs, existing research introduces a Binding ID mechanism and states that LMs use a abstract concept called Binding ID (BI) to internally mark entity-attribute pairs. However, they have not directly captured the BI information from entity activations. In this work, we provide a novel view of the Binding ID mechanism by localizing the BI information. Specifically, we discover that there exists a low-rank subspace in the hidden state (or activation) of LMs, that primarily encodes BIs. To identify this subspace, we take principle component analysis as our first attempt and it is empirically proven to be effective. Moreover, we also discover that when editing representations along directions in the subspace, LMs tend to bind a given entity to other attributes accordingly. For example, by patching activations along the BI encoding direction we can make the LM to infer \"Box Z contains the stone\" and \"Box Z contains the map\"",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9p8CI3cdqr": {
    "title": "The Ultimate Cookbook for Invisible Poison: Crafting Subtle Clean-Label Text Backdoors with Style Attributes",
    "volume": "review",
    "abstract": "Backdoor attacks against text classifiers cause a classifier to predict a predefined label when a particular \"trigger\" is present, but prior attacks often rely on ungrammatical or otherwise unusual triggers. The unnatural texts are easily detected by humans, therefore preventing the attack. We demonstrate that backdoor attacks can be subtle as well as effective, appearing natural even upon close inspection. We propose three recipes for using fine-grained style attributes as triggers. Following prior work, the triggers are added to texts through style transfer; unlike prior work, our recipes provide a wide range of more subtle triggers, and we use human annotation to directly evaluate their subtlety and invisibility. Our evaluations show that our attack consistently outperforms the baselines and that our human annotation provides information not captured by automated metrics used in prior work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g0zsQyXNhQ": {
    "title": "GOME: Grounding-based Metaphor Binding With Conceptual Elaboration For Figurative Language Illustration",
    "volume": "review",
    "abstract": "The illustration or visualization of figurative language, such as linguistic metaphors, is an emerging challenge for existing Large Language Models (LLMs) and multimodal models. Due to their comparison of seemingly unrelated concepts in metaphors, existing LLMs have a tendency of over-literalization, which illustrates figurative language solely based on literal objects, ignoring the underlying groundings and associations across disparate metaphorical domains. Furthermore, prior approaches have ignored the binding process between visual objects and metaphorical attributes, which further intensifies the infidelity of visual metaphors. To address the issues above, we propose GOME ($GO$unding-based $ME$taphor Binding), which illustrates linguistic metaphors from the grounding perspective elaborated through LLMs. GOME consists of two steps for metaphor illustration, including grounding-based elaboration and scenario visualization. In the elaboration step, metaphorical knowledge is integrated into systematic instructions for LLMs, which employs a CoT prompting method rooted in rhetoric. This approach specifies metaphorical devices such as vehicles and groundings, to ensure accurate and faithful descriptions consumed by text-to-image models. In the visualization step, an inference-time metaphor binding method is realized based on elaboration outputs, which register attentional control during the diffusion process, and captures the underlying attributes from the abstract metaphorical domain. Comprehensive evaluations using multiple downstream tasks confirm that, GOME is superior to isolated LLMs, diffusion models, or their direct collaboration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NHtScfaITd": {
    "title": "CEAMC: Corpus and Empirical Study of Argument Analysis in Education via LLMs",
    "volume": "review",
    "abstract": "This paper introduces the Chinese Essay Argument Mining Corpus (CEAMC), a comprehensive dataset for fine-grained argument analysis. Existing argument types in education remain simplistic and isolated, failing to encapsulate complete argument information. Originating from authentic examination settings, CEAMC transcends previous simple representations by conducting multi-level delineation of argument components, thus capturing the subtle nuances of argumentation in the real world and meeting the needs of complex and diverse argumentative scenarios. Our contributions include the development of the CEAMC, the establishment of baselines for further research, and an in-depth exploration of the performance of Large Language Models (LLMs) on CEAMC. The results indicate that our CEAMC can serve as a challenging benchmark for the development of argument analysis in the field of education",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mAegiwQYJ5": {
    "title": "Exploring Non-Autoregressive Image Captioning: Patterns and Semantics",
    "volume": "review",
    "abstract": "In the realm of image captioning (IC), learning sentence pattern and semantics plays a crucial role. The reason why this aspect has not received enough attention before is that the prevailing IC models utilize the autoregressive IC (AR-IC) paradigm which operates in a word-by-word manner. In this paradigm, coherence and fluency with the previous text are prioritized during word generation, without special considerations for the sentence pattern. While effective, the AR-IC approaches pose inherent challenges for real-time applications due to their time-consuming nature during inference. Unlike the AR-IC counterparts, non-autoregressive IC (NAR-IC) models necessitate simultaneous inference of all words in a caption. However, the existing NAR-IC models have been met with the hurdle of reduced effectiveness in comparison to their autoregressive counterparts. It is largely because they follow the AR-IC approach, neglecting the influence of patterns and semantics on NAR-IC. Considering that the dependency on preceding and following words is eliminated during NAR-IC generation, it becomes crucial to consider the sentence pattern to guide word generation. In this paper, we reconsider the impact of sentence patterns and semantics in NAR-IC training. We delve into NAR-IC and provide tips and tricks for training NAR-IC models, which include knowledge distillation, label selection, image pre-fusion, and NAR+AR enhancement. By meticulously examining the impact of these components on model performance, we achieve the state-of-the-art performance with a single-step generation. This paper aims to provide valuable strategies for those aiming to advance NAR-IC models. Our code will be provided in Supplementary materials",
    "checked": false,
    "id": "5672048ce3f49094d44b4f7acf3f8e933da0338b",
    "semantic_title": "learning distinct and representative modes for image captioning",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=OPf78dMVFf": {
    "title": "MantisScore: A Reliable Fine-grained Metric for Video Generation",
    "volume": "review",
    "abstract": "The recent years have witnessed great advances in text-to-video generation. However, the video evaluation metrics have lagged significantly behind, which fails to produce an accurate and holistic measure of the generated videos' quality. The main barrier is the lack of high-quality human rating data. In this paper, we release VideoEval, the first large-scale multi-aspect video evaluation dataset. VideoEval consists of high-quality human-provided ratings for 5 video evaluation aspects on the 37.6K videos generated from 11 existing popular video generative models. We train MantisScore based on VideoEval to enable automatic video quality assessent. Experiments show that the Spearman correlation between MantisScore and humans can reache 77.12 on VideoEval-test, beating the prior best metrics by about 50 points. Further result on the held-out EvalCrafter, GenAI-Bench, and VBench, show that MantisScore is highly generalizable and still beating the prior best metrics by a remarkable margin. We observe that using Mantis as the based model consistently beats that using Idefics2 and VideoLLaVA, and the regression-based model can achieve better results than the generative ones. Due to its high reliability, we believe MantisScore can serve as a valuable tool for accelerate video generation research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MydOaViluf": {
    "title": "KG-CF: Knowledge Graph Completion with Context Filtering under the Guidance of Large Language Models",
    "volume": "review",
    "abstract": "Recent years have witnessed the unprecedented performance of Large Language Models (LLMs) in various downstream tasks, where knowledge graph completion stands as a representative example. Nevertheless, despite the emerging explorations of utilizing LLMs for knowledge graph completion, most LLMs pose challenges in quantitative triplet score generation. This disadvantage fundamentally conflicts with the inherently ranking-based nature of the knowledge graph completion task and its associated evaluation protocols. In this paper, we propose a novel framework KG-CF for knowledge graph completion. In particular, KG-CF not only harnesses the exceptional reasoning capabilities of LLMs through context filtering but also aligns with ranking-based knowledge graph completion tasks and the associated evaluation protocols. Empirical evaluations on real-world datasets validate the superiority of KG-CF in knowledge graph completion tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FzEMhmC8jZ": {
    "title": "Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification",
    "volume": "review",
    "abstract": "Automatic factuality verification of large language model (LLM) generations is becoming more and more widely used to combat hallucinations. A major point of tension in the literature is the granularity of this fact-checking: larger chunks of text are hard to fact-check, but more atomic facts like propositions may lack context to interpret correctly. In this work, we assess the role of context in these atomic facts. We argue that fully atomic facts are not the right representation, and define two criteria for molecular facts: decontextuality, or how well they can stand alone, and minimality, or how little extra information is added to achieve decontexuality. We quantify the impact of decontextualization on minimality, then present a baseline methodology for generating molecular facts automatically, aiming to add the right amount of information. We compare against various methods of decontextualization and find that molecular facts balance minimality with fact verification accuracy in ambiguous settings",
    "checked": true,
    "id": "c80fb69f26fe186504a182c4b7f5ab186beeb019",
    "semantic_title": "molecular facts: desiderata for decontextualization in llm fact verification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q4Funv6xB8": {
    "title": "GEM-VPC: A dual Graph-Enhanced Multimodal integration for Video Paragraph Captioning",
    "volume": "review",
    "abstract": "Video Paragraph Captioning (VPC) aims to generate paragraph captions that summarises key events within a video. Despite recent advancements, challenges persist, notably in effectively utilising multimodal signals inherent in videos and addressing the long-tail distribution of words. The paper introduces a novel multimodal integrated caption generation framework for VPC that leverages information from various modalities and external knowledge bases. Our framework constructs two graphs: a video-specific temporal graph capturing major events and interactions between multimodal information and commonsense knowledge, and a theme graph representing correlations between words of a specific theme. These graphs serve as input for a transformer network with a shared encoder-decoder architecture. We also introduce a node selection module to enhance decoding efficiency by selecting the most relevant nodes from the graphs. Our results demonstrate superior performance across benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1zJrDjKTWg": {
    "title": "SummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization",
    "volume": "review",
    "abstract": "Summarization is an important application of Large Language Models (LLMs). When judging the quality of a summary, factual consistency holds a significant weight. Despite numerous efforts dedicated to building factual inconsistency detectors, the exploration of explanability remains limited among existing effort. In this study, we incorporate both human-annotated and model-generated natural language explanations elucidating how a summary deviates and thus becomes inconsistent with its source article. We build our explanation-augmented dataset on top of the widely used SummaC summarization consistency benchmark. Additionally, we develop an inconsistency detector that is jointly trained with the collected explanations. Our findings demonstrate that integrating explanations during training not only enables the model to provide rationales for its judgments but also enhances its accuracy significantly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RQeImTmSMI": {
    "title": "Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts",
    "volume": "review",
    "abstract": "Program of Thoughts (PoT) is an approach characterized by its executable intermediate steps, which ensure the accuracy of the logical calculations in the reasoning process. Currently, PoT primarily uses Python. However, relying solely on a single language may result in suboptimal solutions and overlook the potential benefits of other programming languages. In this paper, we conduct comprehensive experiments on the programming languages used in PoT and find that no single language consistently delivers optimal performance across all tasks and models. The effectiveness of each language varies depending on the specific scenarios. Inspired by this, we propose a task and model agnostic approach called MultiPoT, which harnesses strength and diversity from various languages. Experimental results reveal that it significantly outperforms Python Self-Consistency. Furthermore, it achieves comparable or superior performance compared to the best monolingual PoT in almost all tasks across all models. In particular, MultiPoT achieves more than 4.6\\% improvement on average on ChatGPT (gpt-3.5-turbo-0701)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UE5P9b4oBj": {
    "title": "LLMs are Turning a Blind Eye to Context: Insights from a Contrastive Dataset for Idiomaticity",
    "volume": "review",
    "abstract": "Recent studies have shown that language models achieve high performance in idiomaticity detection tasks. Given the crucial role of context in interpreting these expressions, it is important to evaluate how models use context to make this distinction. To this end, we collect a comprehensive evaluation dataset to see how the model discriminates the use of the same expression in two different contexts. In particular, we produce high-quality instances of idiomatic expressions occurring in their non-dominant literal interpretation, as a way to test whether models can use the context to construct meaning. Our findings highlight the models' tendency to default to figurative interpretations and they do not appear to fully attend to the context. Moreover, the frequency of idioms impacts their ability to accurately discern literal and figurative meanings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SvkOAxZdgv": {
    "title": "Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning",
    "volume": "review",
    "abstract": "Binary code analysis is the foundation of crucial tasks in the security domain; thus building effective binary analysis techniques is more important than ever. Large language models (LLMs) although have brought impressive improvement to source code tasks, do not directly generalize to assembly code due to the unique challenges of assembly: (1) the low information density of assembly and (2) the diverse optimizations in assembly code. To overcome these challenges, this work proposes a hierarchical attention mechanism that builds attention summaries to capture the semantics more effectively and designs contrastive learning objectives to train LLMs to learn assembly optimization. Equipped with these techniques, this work develops Nova, a generative LLM for assembly code. Nova outperforms existing techniques on binary code decompilation by up to 146.54%, and outperforms the latest binary code similarity detection techniques by up to 6.17%, showing promising abilities in both assembly generation and understanding tasks",
    "checked": true,
    "id": "c27a6c7a399c81ec8d218851017513b1cfba3dc7",
    "semantic_title": "nova: generative language models for assembly code with hierarchical attention and contrastive learning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=y5gbFhQUbU": {
    "title": "SCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved impressive performance across various domains, but the limited context window and the expensive computational cost of processing long texts restrict their more comprehensive application. In this paper, we propose Selective Compression Attention (SCA), a general and effective method to expand the context window and reduce memory footprint by compressing the KV cache of LLMs. Specifically, through preliminary experiments, we found that the KV cache contains many similar vectors, resulting in information redundancy, which can be compressed by retaining representative vectors and discarding others. Therefore, SCA continuously selects the most distinctive vectors to keep through a greedy algorithm, reducing information loss during compression. Extensive experiments on various tasks verify the effectiveness of our method. Compared with existing methods, SCA can significantly reduce the impact on model performance under the same compression ratio. Furthermore, the context window of LLMs can be efficiently expanded using SCA without any training, which can even achieve better performance than specially fine-tuned long context models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BKrYt5WEKN": {
    "title": "Incubating Text Classifiers Following User Instruction with Nothing but LLM",
    "volume": "review",
    "abstract": "In this paper, we aim to generate text classification data given arbitrary class definitions (i.e., user instruction), so one can train a text classifier without any human annotation or raw corpus. Recent advances in large language models (LLMs) lead to pioneer attempts to individually generate texts for each class via prompting. In this paper, we propose Incubator, the first framework that can handle complicated and even mutually dependent classes (e.g., \"\\emph{TED Talk given by Educator}\" and \"\\emph{Other}\"). Specifically, our Incubator is a fine-tuned LLM that takes the instruction of all class definitions as input, and in each inference, it can jointly generate one sample for every class. First, we tune Incubator on the instruction-to-data mappings that we obtained from classification datasets and descriptions on Hugging Face together with in-context augmentation by GPT-4. To emphasize the uniformity and diversity in generations, we refine Incubator by fine-tuning with the cluster centers of semantic textual embeddings of the generated samples. We compare Incubator on various classification tasks with strong baselines such as direct LLM-based inference and training data generation by prompt engineering. Experiments show Incubator is able to (1) outperform previous methods on traditional benchmarks, (2) take label interdependency and user preference into consideration, and (3) enable logical text mining by incubating multiple classifiers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TJUvajnLE1": {
    "title": "IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused summarization",
    "volume": "review",
    "abstract": "Query-focused summarization (QFS) aims to produce summaries that answer particular questions of interest, enabling greater user control and personalization. With the advent of large language models (LLMs), shows their impressive capability of textual understanding through large-scale pretraining, which implies the great potential of extractive snippet generation. In this paper, we systematically investigated two indispensable characteristics that the LLMs-based QFS models should be harnessed, Lengthy Document Summarization and Efficiently Fine-grained Query-LLM Alignment, respectively. Correspondingly, we propose two modules called Query-aware HyperExpert and Query-focused Infini-attention to access the aforementioned characteristics. These innovations pave the way for broader application and accessibility in the field of QFS technology. Extensive experiments conducted on existing QFS benchmarks indicate the effectiveness and generalizability of the proposed approach",
    "checked": true,
    "id": "5df1d39d1bdf0129250f1858f9d91acf66f08b88",
    "semantic_title": "ideal: leveraging infinite and dynamic characterizations of large language models for query-focused summarization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NvNcOt9Jtq": {
    "title": "Learning to Rank Salient Content for Query-focused Summarization",
    "volume": "review",
    "abstract": "This study examines the potential of integrating Learning-to-Rank (LTR) with Query-focused Summarization (QFS) to enhance the summary relevance via content prioritization. Using a shared secondary decoder with the summarization decoder, we carry out the LTR task at the segment level. Compared to the state-of-the-art, our model outperforms on QMSum benchmark (all metrics) and matches on SQuALITY benchmark (2 metrics) as measured by Rouge and BertScore while offering a lower training overhead. Specifically, on the QMSum benchmark, our proposed system achieves improvements, particularly in Rouge-L (+0.42) and BertScore (+0.34), indicating enhanced understanding and relevance. While facing minor challenges in Rouge-1 and Rouge-2 scores on the SQuALITY benchmark, the model significantly excels in Rouge-L (+1.47), underscoring its capability to generate coherent summaries. Human evaluations emphasize the efficacy of our method in terms of relevance and faithfulness of the generated summaries, without sacrificing fluency. A deeper analysis reveals our model's superiority over the state-of-the-art for broad queries, as opposed to specific ones, from a qualitative standpoint. We further present an error analysis of our model, pinpointing challenges faced and suggesting potential directions for future research in this field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kiHlEOJ1ID": {
    "title": "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation",
    "volume": "review",
    "abstract": "Low-rank adaptation (LoRA) has become the default approach to fine-tune large language models (LLMs) due to its significant reduction in trainable parameters. However, trainable parameter demand for LoRA increases with increasing model embedding dimensions, leading to high compute costs. Additionally, its backward updates require storing high-dimensional intermediate activations and optimizer states, demanding high peak GPU memory. In this paper, we introduce *large model fine-tuning via spectrally decomposed low-dimensional adaptation (LaMDA)*, a novel approach to fine-tuning large language models, which leverages low-dimensional adaptation to achieve significant reductions in trainable parameters and peak GPU memory footprint. LaMDA freezes a first projection matrix (PMA) in the adaptation path while introducing a low-dimensional trainable square matrix, resulting in substantial reductions in trainable parameters and peak GPU memory usage. LaMDA gradually freezes a second projection matrix (PMB) during the early fine-tuning stages, reducing the compute cost associated with weight updates to enhance parameter efficiency further. We also present an enhancement, LaMDA++, incorporating a ``lite-weight\" adaptive rank allocation for the LoRA path via normalized spectrum analysis of pre-trained model weights. We evaluate LaMDA/LaMDA++ across various tasks, including natural language understanding with the GLUE benchmark, text summarization, natural language generation, and complex reasoning on different LLMs. Results show that LaMDA matches or surpasses the performance of existing alternatives while requiring up to 17.7x fewer parameter updates and up to 1.32x lower peak GPU memory usage during fine-tuning. Code will be publicly available",
    "checked": true,
    "id": "c7de9c673376c1f535271bdfb72d237a26126b3b",
    "semantic_title": "lamda: large model fine-tuning via spectrally decomposed low-dimensional adaptation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=E6gQDgDVBc": {
    "title": "Where Am I From? Identifying Origin of LLM-generated Content",
    "volume": "review",
    "abstract": "Generative models, particularly Large Language Models, have demonstrated remarkable proficiency in producing natural and high-quality content. However, the widespread use of such models raises significant concerns related to copyright, privacy, and security vulnerabilities associated with AI-generated material. In response to these concerns, our objective is to develop digital forensics methods tailored for large language model to trace the generator given a AI-generated content. Our methodology begins with the incorporation of a secret watermark into the generated output, facilitating traceability without necessitating model retraining. To enhance effectiveness, especially in scenarios involving short outputs, we introduce a depth watermark. This framework ensures the traceability of content back to its original source, achieving both accurate tracing and the generation of high-quality output. Extensive experiments have been conducted across diverse settings and datasets to validate the effectiveness and robustness of our proposed framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EKlispzX65": {
    "title": "EBGCG: Effective White-Box Jailbreak Attack Against Large Language Model",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) excel in tasks like question answering and text summarization, but they are vulnerable to jailbreak attacks, which trick them into generating illicit content. Current black-box methods are inefficient, while white-box methods face issues like slow convergence and suboptimal results. We propose EBGCG, **E**mbedding space pre-optimization and **B**eam search-enhanced **G**reedy **C**oordinate **G**radient, a novel two-stage white-box jailbreak attack method. The first stage uses gradient descent to pre-optimize adversarial suffixes in the embedding space. The second stage employs beam search-enhanced greedy coordination gradient, weighting tokens based on their positions to reduce distractions. Our evaluation shows EBGCG achieves an average attack success rate (ASR) of 69.47\\%, outperforming GCG and BEAST by 16.68\\% and 43.65\\%, respectively, and reaching up to 87.12\\% ASR on Falcon-7B-instruct",
    "checked": false,
    "id": "27ce6603b7cd5a43f2ef9811b7775008a7d9602b",
    "semantic_title": "white-box multimodal jailbreaks against large vision-language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=lY26YCmzo4": {
    "title": "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons",
    "volume": "review",
    "abstract": "Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation LexC-Gen, a method that generates low-resource-language classification task data at scale. Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation. Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classification tasks respectively. Through ablation study, we show that conditioning on bilingual lexicons is the key component of LexC-Gen. LexC-Gen serves as a potential solution to close the performance gap between open-source multilingual models, such as BLOOMZ and Aya-101, and state-of-the-art commercial models like GPT-4o on low-resource-language tasks",
    "checked": true,
    "id": "49672006290b125aea958d1bae5d07c8e48ce8bb",
    "semantic_title": "lexc-gen: generating data for extremely low-resource languages with large language models and bilingual lexicons",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=HM55vfxtIs": {
    "title": "Crossroads of Continents: Automated Artifact Extraction for Cultural Adaptation with Large Multimodal Models",
    "volume": "review",
    "abstract": "In this work, we present a comprehensive three-phase study to examine (1) the effectiveness of large multimodal models (LMMs) in recognizing cultural contexts; (2) the accuracy of their representations of diverse cultures; and (3) their ability to adapt content across cultural boundaries. We first introduce Dalle Street, a large-scale dataset generated by DALL-E 3 and validated by humans, containing 9,935 images of 67 countries and 10 concept classes. We reveal disparities in cultural understanding at the sub-region level with both open-weight (LLaVA) and closed-source (GPT-4V) models on Dalle Street and other existing benchmarks. Next, we assess models' deeper culture understanding by an artifact extraction task and identify over 18,000 artifacts associated with different countries. Finally, we propose a highly composable pipeline, CultureAdapt, to adapt images from culture to culture. Our findings reveal a nuanced picture of the cultural competence of LMMs, highlighting the need to develop culture-aware systems",
    "checked": true,
    "id": "ede9bd98e627a9fb74bdd7481ec06b83fb099c14",
    "semantic_title": "crossroads of continents: automated artifact extraction for cultural adaptation with large multimodal models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=MZVrFEqVm7": {
    "title": "Exploring Rollback Inference for Aspect-based Sentiment Analysis",
    "volume": "review",
    "abstract": "With the giant help from pre-trained large language models (LLMs), templated sequence of how to organize the aspect-level elements become the hottest research target while only a few of them move their steps to inference, not to mention utilizing the semantic connection between aspect-level elements during it. We argue that, compared with the high computational cost methods of training language models, considering the inference process can also bring us potential benefits. Motivated by this, we propose rollback inference for aspect-based sentiment analysis, which can boost the performance of fine-tuned LLMs with a tiny cost, and adapt to various language models. Specifically, We first propose a novel entropy-based rollback inference framework that manipulates multi-reasoning and voting over the uncertain parts of the sequence with model's self-consistency. We then explore the possibility of capturing the correlations among elements during inference with a set of rollback strategies. Extensive experiments in several benchmarks underscore the robustness and effectiveness of our proposed rollback strategies and the value of the semantic connections in inference",
    "checked": false,
    "id": "7af66a73925da9cbd4447067eef6589d5acf08e7",
    "semantic_title": "exploring bert for aspect-based sentiment analysis in portuguese language",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=EmDtezabqo": {
    "title": "AI-LieDar : Examine the Trade-off Between Utility and Truthfulness in LLM Agents",
    "volume": "review",
    "abstract": "Truthfulness is a key component of the safety of large language models (LLM), particularly when they are deployed as helpful agents in our daily lives. However, the inherent conflict between utility and truthfulness in many LLM instructions raises the question of how LLMs balance these two dimensions. We propose AI-LieDar, a framework designed to study how LLM-based agents navigate these scenarios in an multi-turn interactive setting. Based on the framework, we design a set of scenarios and conduct multi-turn simulations. Additionally, we develop a truthfulness detector, inspired by psychological literature, to assess the agents' responses. Our experiments demonstrate that most models can effectively navigate the scenarios. The truthfulness and goal achievement rate vary, with no clear correlation to model size or capability. However, all models are truthful less than 50\\% of the time. We further test the steerability of LLMs towards truthfulness, finding that models can be directed to be deceptive, and even truth-steered models still lie.These findings reveal the complex nature of truthfulness in LLMs and underscore the importance of further research in this area to ensure the safe and reliable deployment of LLMs and AI agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CGaEMnkzdR": {
    "title": "EmoKnob: Enhance Voice Cloning with Fine-Grained Emotion Control",
    "volume": "review",
    "abstract": "While recent advances in Text-to-Speech (TTS) technology produce natural and expressive speech, they lack the option for users to select emotion and control intensity. We propose EmoKnob, a framework that allows fine-grained emotion control in speech synthesis with few-shot demonstrative samples of arbitrary emotion. Our framework leverages the expressive speaker representation space made possible by recent advances in foundation voice cloning models. Based on the few-shot capability of our emotion control framework, we propose two methods to apply emotion control on emotions described by open-ended text, enabling an intuitive interface for controlling a diverse array of nuanced emotions. To facilitate a more systematic emotional speech synthesis field, we introduce a set of evaluation metrics designed to rigorously assess the faithfulness and recognizability of emotion control frameworks. Through objective and subjective evaluations, we show that our emotion control framework effectively embeds emotions into speech and surpasses emotion expressiveness of commercial TTS services",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jYBjseMMSQ": {
    "title": "You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions",
    "volume": "review",
    "abstract": "Training question-answering QA and information retrieval systems for web queries require large, expensive datasets that are difficult to annotate and time-consuming to gather. Moreover, while natural datasets of information-seeking questions are often prone to ambiguity or ill-formed, there are troves of freely available, carefully crafted question datasets for many languages. Thus, we automatically generate shorter, information-seeking questions, resembling web queries in the style of the Natural Questions (NQ) dataset from longer trivia data. Training a QA system on these transformed questions is a viable strategy for alternating to more expensive training setups showing the F1 score difference of less than 6% and contrasting the final systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lcSHjDf6A6": {
    "title": "What Matters in Learning Facts in Language Models? Multifaceted Knowledge Probing with Diverse Multi-Prompt Datasets",
    "volume": "review",
    "abstract": "Large language models (LLMs) face issues in handling factual knowledge, making it vital to evaluate their true ability to understand facts. In this study, we introduce knowledge probing frameworks, BELIEF(-ICL), to evaluate the knowledge understanding ability of both encoder-based and decoder-based pre-trained LMs (PLMs) from diverse perspectives. BELIEFs utilize a multi-prompt dataset to evaluate PLM's accuracy, consistency, and reliability in factual knowledge understanding. To enable a more reliable evaluation with BELIEFs, we semi-automatically create MyriadLAMA, which has massively diverse prompts. We validate the effectiveness of BELIEFs in correctly and comprehensively evaluating PLM's factual understanding ability via extensive evaluations with recent LLMs. We then investigate key factors in learning facts in LLMs, and reveal the limitation of the prompt-based knowledge probing. The dataset is anonymously publicized",
    "checked": true,
    "id": "5502d5db8ab9e24dcc4c2f510acf1eb60c12ace3",
    "semantic_title": "what matters in learning facts in language models? multifaceted knowledge probing with diverse multi-prompt datasets",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3CrvafYU3I": {
    "title": "Self-Adapted Entity-Centric Data Augmentation for Discontinuous Named Entity Recognition",
    "volume": "review",
    "abstract": "Named Entity Recognition (NER) is a critical task in natural language processing, particularly challenging in identifying discontinuous entities. This study is the first to explore the application of image data augmentation techniques in the preprocessing phase for discontinuous entity recognition, aiming to overcome the limitations of traditional text segmentation methods. Through experiments, we found that traditional sentence segmentation methods might lead to incorrect segmentation of cross-sentence discontinuous entities, affecting the accuracy of model training and entity recognition. To address this, we introduced a new preprocessing strategy that combines graphic cropping, scaling, and padding techniques to improve the model's ability to recognize discontinuous entities. Experiments on three benchmark datasets, CADEC, ShARe13, and ShARe14, demonstrated that our preprocessing method increased the F1 scores of two state-of-the-art grid models by approximately 1\\% to 2.5\\%, proving the effectiveness of this method",
    "checked": false,
    "id": "cfca8cebe32913ac48af8fc19655a6f1fef49b04",
    "semantic_title": "de-bias for generative extraction in unified ner task",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=kWSpDKbHqS": {
    "title": "How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?",
    "volume": "review",
    "abstract": "As large language models (LLMs) are widely deployed, targeted editing of their knowledge has become a critical challenge. Recently, advancements in model editing techniques, such as Rank-One Model Editing (ROME), have paved the way for updating LLMs with new knowledge. However, the efficacy of these methods varies across different types of knowledge. This study investigates the capability of knowledge editing methods to incorporate new knowledge with varying degrees of \"perplexingness\", a term we use to describe the initial difficulty LLMs have in understanding new concepts. We begin by quantifying the \"perplexingness\" of target knowledge using pre-edit conditional probabilities, and assess the efficacy of edits through post-edit conditional probabilities. Utilizing the widely-used CounterFact dataset, we find significant negative correlations between the \"perplexingness\" of the new knowledge and the edit efficacy across all 12 scenarios. To dive deeper into this phenomenon, we introduce a novel dataset, HierarchyData, consisting of 99 hyponym-hypernym pairs across diverse categories. Our analysis reveal that more abstract concepts (hypernyms) tend to be more perplexing than their specific counterparts (hyponyms). Further exploration into the influence of knowledge hierarchy on editing outcomes indicates that knowledge positioned at higher hierarchical levels is more challenging to modify in some scenarios. Our research highlights a previously overlooked aspect of LLM editing: the variable efficacy of editing methods in handling perplexing knowledge. By revealing how hierarchical relationships can influence editing outcomes, our findings offer new insights into the challenges of updating LLMs and pave the way for more nuanced approaches to model editing in the future",
    "checked": true,
    "id": "5990e8876ebbf3e24ad682fd65448cf5b8747094",
    "semantic_title": "how well can knowledge edit methods edit perplexing knowledge?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=g6e9jgEobl": {
    "title": "VIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values",
    "volume": "review",
    "abstract": "This paper introduces VIVA, a benchmark for VIsion-grounded decision-making driven by human VA. While most large vision-language models (VLMs) focus on physical-level skills, our work is the first to examine their multimodal capabilities in leveraging human values to make decisions under a vision-depicted situation. VIVA contains 1,062 images depicting diverse real-world situations and the manually annotated decisions grounded in them. Given an image there, the model should select the most appropriate action to address the situation and provide the relevant human values and reason underlying the decision. Extensive experiments based on VIVA show the limitation of VLMs in using human values to make multimodal decisions. Further analyses indicate the potential benefits of exploiting action consequences and predicted human values",
    "checked": true,
    "id": "0f5582605876d3dab492b4db6e961353f0695063",
    "semantic_title": "viva: a benchmark for vision-grounded decision-making with human values",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E3VS45jxPR": {
    "title": "Do LLMs Plan Like Human Writers? Comparing Journalist Coverage of Press Releases with LLMs",
    "volume": "review",
    "abstract": "Journalists engage in multiple steps in the news writing process that depend on human creativity, like exploring different ``angles'' (i.e. story directions). These can potentially be aided by large language models (LLMs). By affecting planning decisions, such interventions can have an outsize impact on creative output. We advocate a careful approach to evaluating these interventions, to ensure alignment with human values, by comparing LLM decisions to previous human decisions. In a case study of journalistic coverage of press releases, we assemble a large dataset of 250k press releases and 650k human-written articles covering them. We develop methods to identify news articles that \\textit{challenge and contextualize} press releases. Finally, we evaluate suggestions made by LLMs for these articles and compare these with decisions made by human journalists",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GpMzZcHHXm": {
    "title": "To Know What User Concerns: Conceptual Knowledge Reasoning for Task-oriented Dialogue Quality Estimation",
    "volume": "review",
    "abstract": "Dialogue Quality Estimation (DQE) is crucial in assessing the effects of a conversational consultation system, which has wide applications in E-commerce and Social Media. In task-oriented scenarios, users usually seek personalized consultation about the target subjects they are concerned with rather than general knowledge commonly known by populations. It is essential to identify whether a dialogue solves the user's questions by task-oriented DQE. Existing studies mainly focus on analyzing dialogue semantics and user sentiment, neglecting to understand what the user is concerned about when requesting a consultation. It may cause fatal errors when the response is emotionally friendly but non-informative. In this paper, we propose a knowledge-enhanced DQE model named CoReT, which introduces the Conceptual Knowledge Reasoning for Task-oriented DQE. We first design a simple yet efficient entity linking and relation selection module enabling conceptual reasoning from a knowledge graph. Then, we propose a multi-turn textual encoder to capture the contextual information in dialogues. Finally, we introduce a knowledge enhancement module to fuse conceptual reasoning features into contextual embeddings to produce DQE results. For evaluation, we conduct experiments on two real-world datasets in e-commerce consultation systems, the results demonstrate the effectiveness and robustness of CoReT compared with the state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g3jAdDEzSa": {
    "title": "Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge",
    "volume": "review",
    "abstract": "Despite progress in multimodal large language models (MLLMs), the challenge of interpreting long-form videos in response to linguistic queries persists, largely due to the inefficiency in temporal grounding and limited pre-trained context window size. In this work, we introduce Temporal Grounding Bridge (TGB), a novel framework that bootstraps MLLMs with advanced temporal grounding capabilities and broadens their contextual scope. Our framework significantly enhances the temporal capabilities of current MLLMs through three key innovations: an efficient multi-span temporal grounding algorithm applied to low-dimension temporal features projected from flow; a multimodal length extrapolation training paradigm that utilizes low-dimension temporal features to extend the training context window size; and a bootstrapping framework that bridges our model with pluggable MLLMs without requiring annotation. We validate TGB across seven video benchmarks and demonstrate substantial performance improvements compared with prior MLLMs. Notably, our model, initially trained on sequences of four frames, effectively handles sequences up to 16$\\times$ longer without sacrificing performance, highlighting its scalability and effectiveness in real-world applications. Our code is publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6EUixqUlYS": {
    "title": "UKoSpeech: A Universal Korean ASR System for Diverse Domains",
    "volume": "review",
    "abstract": "The rapid advancement of Automatic Speech Recognition (ASR) systems has dramatically transformed transcription processes, minimizing the need for expert human intervention. Despite the growth in ASR technologies and the emergence of robust models like Whisper, significant challenges remain. Specifically, the scarcity of non-English training data and poor adaptability to domain-specific contexts hinder broader application. This paper introduces UKoSpeech, a novel Korean ASR system designed to address these issues through a unique two-pronged approach: a Korean data curation pipeline leveraging domain-specific data from sources such as YouTube subtitles, and a domain-specific training framework that utilizes a domain prompt technique for enhanced adaptability. Our results indicate that UKoSpeech not only fills the gap in multilingual ASR research but also provides superior domain-specific performance compared to established ASR systems like Whisper, Google STT, and CLOVA Speech. Through extensive evaluation across diverse domains such as finance, medicine, and law, UKoSpeech demonstrates state-of-the-art performance, establishing a new benchmark for domain-adaptable ASR systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0P1mkOvFIs": {
    "title": "Do LLMs Believe in Themselves? A Benchmark for LLM Robustness against External Counterfactual Knowledge",
    "volume": "review",
    "abstract": "Large language Models (LLMs) and AI chatbots have improved people's efficiency in various fields and shown strong capacities in many NLP tasks. However, when extrinsic knowledge contains misinformation from careless mistakes or malicious web texts that users do not realize, there is a higher probability for models to trust wrong external information and generate inaccurate answers that will mislead users. Therefore, we design two principles for models' behaviors in such cases and create a benchmark with counterfactual information in the contexts from existing knowledge bases for further evaluation. We also propose two new metrics to measure the extent to which this misinformation misleads models. Evaluation results show that existing LLMs are susceptible to interference from unreliable external knowledge with counterfactual information, and simple intervention methods make limited contributions to the alleviation of this issue",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OjRuBvZ8TO": {
    "title": "PUMGPT: A Large Vision-Language Model for Product Understanding",
    "volume": "review",
    "abstract": "E-commerce platforms benefit from accurate product understanding to enhance user experience and operational efficiency. Traditional methods often focus on isolated tasks such as attribute extraction or categorization, posing adaptability issues to evolving tasks and leading to usability challenges with noisy data from the internet. Current Large Vision Language Models (LVLMs) lack domain-specific fine-tuning, thus falling short in precision and instruction following. To address these issues, we introduce \\textbf{\\textsc{PumGPT}}, the first e-commerce specialized LVLM designed for multi-modal product understanding tasks. We collected and curated a dataset of over one million products from AliExpress, filtering out non-inferable attributes using a universal hallucination detection framework, resulting in 663k high-quality data samples. \\textbf{\\textsc{PumGPT}} focuses on five essential tasks aimed at enhancing workflows for e-commerce platforms and retailers. We also introduce \\textbf{\\textsc{PumBench}}, a benchmark to evaluate product understanding across LVLMs. Our experiments show that \\textbf{\\textsc{PumGPT}} outperforms five other open-source LVLMs and GPT-4V in product understanding tasks. We also conduct extensive analytical experiments to delve deeply into the superiority of \\textsc{PumGPT}, demonstrating the necessity for a specialized model in the e-commerce domain",
    "checked": true,
    "id": "eb5cf10406a8ad31e0ebe56b36571d5db4758a62",
    "semantic_title": "pumgpt: a large vision-language model for product understanding",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=rrzw1t7LHc": {
    "title": "How Trustworthy is AI? A Deep Dive into the Bias in LLM-Based Recommendations",
    "volume": "review",
    "abstract": "Large Language Model (LLM)-based recommendation systems provide more comprehensive recommendations than traditional systems by deeply analyzing content and user behavior. However, these systems often exhibit biases, favoring mainstream content while marginalizing non-traditional options due to skewed training data. This study investigates the intricate relationship between bias and LLM-based recommendation systems, with a focus on music, song, and book recommendations across diverse demographic and cultural groups. Through a comprehensive analysis, this paper evaluates the impact of bias on recommendation outcomes and assesses various strategies, such as prompt engineering and hyperparameter optimization, for bias mitigation. Our findings indicate that neither prompt engineering nor hyperparameter optimization are particularly effective in mitigating biases, highlighting the need for further research in this area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NmsQxtveeD": {
    "title": "Employing Glyphic Information for Chinese Event Extraction with Vision-Language Model",
    "volume": "review",
    "abstract": "As a complex task that requires rich information input, features from various aspects have been utilized in event extraction. However, most of the previous works ignored the value of glyph, which could contain enriched semantic information and can not be fully expressed by the pre-trained embedding in hieroglyphic languages like Chinese. We argue that, compared with combining the sophisticated textual features, glyphic information from visual modality could provide us with extra and straight semantic information in extracting events. Motivated by this, we propose a glyphic multi-modal Chinese event extraction model with hieroglyphic images to capture the intra- and inter-character morphological structure from the sequence. Extensive experiments build a new state-of-the-art performance in the ACE2005 Chinese and KBP Eval 2017 dataset, which underscores the effectiveness of our proposed glyphic event extraction model, and more importantly, the glyphic feature can be obtained at nearly zero cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Aqp3VzWHus": {
    "title": "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models",
    "volume": "review",
    "abstract": "Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting hallucinations in Machine Translation (MT) remains a critical challenge, particularly since existing methods excel with High-Resource Languages (HRLs) but exhibit substantial limitations when applied to Low-Resource Languages (LRLs). This paper evaluates hallucination detection approaches using Large Language Models (LLMs) and semantic similarity within massively multilingual embeddings. Our study spans 16 language directions, covering HRLs, LRLs, with diverse scripts. We find that the choice of model is essential for performance. On average, for HRLs, Llama3-70B outperforms the previous state of the art by as much as 0.16 MCC (Matthews Correlation Coefficient). However, for LRLs we observe that Claude Sonnet outperforms other LLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can achieve performance comparable or even better than previously proposed models, despite not being explicitly trained for any machine translation task.However, their advantage is less significant for LRLs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qXPJfQ9EWB": {
    "title": "CacheFormer: High Attention-based Segment Caching",
    "volume": "review",
    "abstract": "Efficiently handling long contexts in transformer-based language models with low perplexity is an active area of research. Although, numerous approaches have been recently presented like Linformer, Longformer, Performer, Structured state space models (SSMs) etc., yet it remains an unresolved problem. All these models strive to reduce the quadratic time complexity of the attention mechanism to approximate linear time complexity while minimizing the loss in quality due to the effective compression of the long context. Inspired by the cache and virtual memory concepts in computer architecture, we improve the work presented in Long-Short Transformer (Transformer-LS) that implements a sliding window for the short attention and compressed contextual segments for the long attention. Our enhancements include augmenting the architecture with attention on dynamically retrieved uncompressed context segments that indicate high attention at the compressed level. Similar to the cache and virtual memory principle in computers, where in case of a cache or page miss, not only the needed data is retrieved from the random-access memory or the hard disk, but the nearby following data is also obtained. On a similar note, we too retrieve the nearby segments in uncompressed form when a high attention occurs at the compressed level. We further enhance the long-short transformer by augmenting the long attention with compressed overlapping segments to reduce the loss in quality due to segment fragmentation that occurs in sequences with long context. Our results indicate significant improvements over the base line of the long-short transformer in terms of perplexity on the popular benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4nYcouJ6cI": {
    "title": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
    "volume": "review",
    "abstract": "In many scientific fields, large language models (LLMs) have revolutionized the way with which text and other modalities of data (e.g., molecules and proteins) are dealt, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one to two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this end, we comprehensively survey over 250 scientific LLMs, discuss their commonalities and differences, as well as summarize pre-training datasets and evaluation tasks for each field and modality. Moreover, we investigate how LLMs have been deployed to benefit scientific discovery. Resources related to this survey are available at https://anonymous.4open.science/r/SciLLM-72F8",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZUjqTc3dtz": {
    "title": "Improving the Factual Consistency of Abstractive Summarization: Model Self-Improvement Contrastive Learning",
    "volume": "review",
    "abstract": "Abstractive summarization models often produce summaries that are inconsistent with the content of the original text. Contrastive learning is an effective strategy for improving the factual consistency of generative text summarization. However, the success of contrastive learning depends largely on the construction of the dataset. Existing contrastive learning methods usually directly consider the gold summaries of summary datasets as factual summaries, and ignore the factual consistency problem of the gold summaries. They mainly focus on the generation of hallucinated summaries, i.e., negative samples, and the construction for negative samples is usually based on the gold summaries rather than from the perspective of the model itself. The quality of the positive and negative samples of these methods is not high enough, which will affect the effect of contrastive learning. Therefore, this paper proposes Model Self-Improvement Contrastive Learning: a method to improve the factual consistency of abstractive summarization. This method begins with fine-tuning the model itself, considering its already acquired knowledge of generating summaries. It focuses on the inference aspect of the generation phase, and delves deeper into the content that may cause factual errors. At the same time, it takes into account the factual consistency of both the positive and negative samples, constructs the negative samples in a targeted manner and improves the positive samples. It then further improves the factual consistency of the model through contrastive learning",
    "checked": false,
    "id": "12f881588e6b461b86b34209c30a2bdfe67f904d",
    "semantic_title": "improving factuality of abstractive summarization without sacrificing summary quality",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=m9TuxlYIUL": {
    "title": "NormAd: A Benchmark for Measuring the Cultural Adaptability of Large Language Models",
    "volume": "review",
    "abstract": "The integration of large language models (LLMs) into various global cultures fundamentally presents a challenge: LLMs must navigate interactions, respect social norms, and avoid transgressing cultural boundaries. However, it is still unclear if LLMs can adapt their outputs to diverse cultural norms. Our study focuses on this aspect. We introduce NormAd, a novel dataset, which includes 2.6k stories that represent social and cultural norms from 75 countries, to assess the ability of LLMs to adapt to different granular levels of socio-cultural contexts such as the country of origin, its associated cultural values, and prevalent social norms. Our study reveals that LLMs struggle with cultural reasoning across all contextual granularities, showing stronger adaptability to English-centric cultures over those from the Global South. Even with explicit social norms, the top-performing model, Mistral-7b-Instruct, achieves only 81.8\\% accuracy, lagging behind the 95.6\\% achieved by humans. Evaluation on NormAd further reveals that LLMs struggle to adapt to stories involving gift-giving across cultures. Due to inherent agreement or sycophancy biases, LLMs find it considerably easier to assess the social acceptability of stories that adhere to norms than those that deviate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NUnoxv5Wr0": {
    "title": "MM-ChatAlign: A Novel Multimodal Reasoning Framework based on Large Language Models for Entity Alignment",
    "volume": "review",
    "abstract": "Multimodal entity alignment (MMEA) integrates multi-source and cross-modal knowledge graphs, a crucial yet challenging task for data-centric applications. Traditional MMEA methods derive the visual embeddings of entities and combine them with other modal data for alignment by embedding similarity comparison. However, these methods are hampered by the limited comprehension of visual attributes and deficiencies in realizing and bridging the semantics of multimodal data. To address these challenges, we propose MM-ChatAlign, a novel framework that utilizes the visual reasoning abilities of MLLMs for MMEA. The framework features an embedding-based candidate collection module that adapts to various knowledge representation strategies, effectively filtering out irrelevant reasoning candidates. Additionally, a reasoning and rethinking module, powered by MLLMs, enhances alignment by efficiently utilizing multimodal information. Extensive experiments on four MMEA datasets demonstrate MM-ChatAlign's superiority and underscore the significant potential of MLLMs in MMEA tasks. The source code is available at https://anonymous.4open.science/r/MMEA/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IKK3v4uJz3": {
    "title": "ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees",
    "volume": "review",
    "abstract": "Uncertainty quantification (UQ) in natural language generation (NLG) tasks remains an open challenge, exacerbated by the intricate nature of the recent large language models (LLMs). This study investigates adapting conformal prediction (CP), which can convert any heuristic measure of uncertainty into rigorous theoretical guarantees by constructing prediction sets, for black-box LLMs in open-ended NLG tasks. We propose a sampling-based uncertainty measure leveraging self-consistency, and develop a conformal uncertainty criterion by integrating the uncertainty condition aligned with correctness into the design of the CP algorithm. Experimental results indicate that our uncertainty measure generally surpasses prior state-of-the-art methods. Furthermore, we calibrate the prediction sets within the model's unfixed answer distribution and achieve strict control over the correctness coverage rate across 6 LLMs on 4 free-form NLG datasets, spanning general-purpose and medical domains, while the small average set size further highlights the efficiency of our method in providing trustworthy guarantees for practical open-ended NLG applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8adI1JF8is": {
    "title": "Self-Improving Mathematical Reasoning of Large Language Models with a Code-Centric Paradigm",
    "volume": "review",
    "abstract": "There is a growing trend of teaching large language models (LLMs) to solve mathematical problems through coding during the problem-solving process. Existing studies primarily focus on distilling powerful, closed-source models and in-domain data augmentation, equipping LLMs with a considerable capacity for mathematical reasoning via coding. However, the self-improvement of such LLMs through leveraging large-scale, expert-written, diverse math question-answer pairs remains under-explored. To bridge the gap and tackle challenges such as code response assessment, we propose a novel paradigm that uses a code-based critic model to guide steps including question-code data construction, quality control, and complementary evaluation. We also explore different alignment algorithms with self-generated instruction/preference data to foster continuous improvements. Experiments across both in-domain (up to $+5.7\\%$) and out-of-domain ($+4.4\\%$) benchmarks in English and Chinese demonstrate the effectiveness of self-improving LLMs with the proposed paradigm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DmirBUDJIy": {
    "title": "RAR: Retrieval Augmented Retrieval for Code Generation in Low Resource Languages",
    "volume": "review",
    "abstract": "Language models struggle in generating correct code for low resource programming languages, since these are underrepresented in training data. Popular approaches use either examples or documentation to improve the performance of these models. Instead of considering the independent retrieval of this information, we introduce retrieval augmented retrieval (RAR) as a two-step retrieval method for selecting relevant examples and documentation. Extensive experiments on two low resource languages (Power Query M and OfficeScript) show that RAR outperforms example or grammar retrieval techniques (2.81--26.14\\%)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s0kOGjxYyS": {
    "title": "A Survey on Open Information Extraction from Rule-based Model to Large Language Model",
    "volume": "review",
    "abstract": "Open Information Extraction (OpenIE) represents a crucial NLP task aimed at deriving structured information from unstructured text, unrestricted by relation type or domain. This survey paper provides an overview of OpenIE technologies spanning from 2007 to 2024, emphasizing a chronological perspective absent in prior surveys. It examines the evolution of task settings in OpenIE to align with the advances in recent technologies. The paper categorizes OpenIE approaches into rule-based, neural, and pre-trained large language models, discussing each within a chronological framework. Additionally, it highlights prevalent datasets and evaluation metrics currently in use. Building on this extensive review, the paper considers how traditional OpenIE research can inspire future IE research in the LLM era, aiming to provide insights into the past, present, and future of OpenIE methodologies and applications",
    "checked": true,
    "id": "89d419482d339a81c059ee26aca4a611787f267c",
    "semantic_title": "a survey on open information extraction from rule-based model to large language model",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=jBLs05nDDH": {
    "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
    "volume": "review",
    "abstract": "In the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This paper aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this paper seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UCreMNQzMU": {
    "title": "Learning to Refine with Fine-Grained Natural Language Feedback",
    "volume": "review",
    "abstract": "Recent work has explored the capability of large language models (LLMs) to identify and correct errors in LLM-generated responses. These refinement approaches frequently evaluate what sizes of models are able to do refinement for what problems, but less attention is paid to what effective feedback for refinement looks like. In this work, we propose looking at refinement with feedback as a composition of three distinct LLM competencies: (1) identification of bad generations; (2) fine-grained natural language feedback generation; (3) refining with fine-grained feedback. The first step can be implemented with a high-performing discriminative model and steps 2 and 3 can be implemented either via prompted or fine-tuned LLMs. A key property of this approach is that the step 2 critique model can give fine-grained feedback about errors, made possible by offloading the discrimination to a separate model in step 1. We show that models of different capabilities benefit from refining with this approach on the task of improving factual consistency of document grounded summaries",
    "checked": true,
    "id": "69c8421f110dd8133ac45c2d77652fc85aebd18e",
    "semantic_title": "learning to refine with fine-grained natural language feedback",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B0wvJYPT7M": {
    "title": "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models",
    "volume": "review",
    "abstract": "Parameter-efficient fine-tuning~(\\textbf{PEFT}) is crucial for customizing Large Language Models (LLMs) with constrained resource. Although there have been various PEFT methods for dense-architecture LLMs, PEFT for sparse-architecture LLMs is still underexplored. In this work, we study the PEFT method for LLMs with the Mixture-of-Experts (MoE) architecture and the contents of this work are mainly threefold: (1) We investigate the dispersion degree of the activated experts in customized tasks, and found that the routing distribution for specific task tend to be highly concentrated, while the distribution of activated experts varies significantly across different tasks. (2) We propose the expert-specialized fine-tuning method, which tunes the experts most relevant to downstream tasks while freezing the other experts; experimental results demonstrate that our method not only improves the tuning efficiency, but also matches or even surpasses the performance of full-parameter fine-tuning. (3) We further analyze the impact of the MoE architecture on expert-specialized fine-tuning. We find that MoE models with finer-grained experts are more advantageous in selecting the combination of experts that are most relevant to downstream tasks, thereby enhancing the both the training efficiency and effectiveness",
    "checked": true,
    "id": "a3709e41b92282aa52918b557559d4a3e78f215a",
    "semantic_title": "let the expert stick to his last: expert-specialized fine-tuning for sparse architectural large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4SudP0Fuiz": {
    "title": "Grounding GPT-based Dialogue Agents with Knowledge Graphs for Consistent Personality",
    "volume": "review",
    "abstract": "This paper presents a novel approach for grounding GPT-based models using knowledge graphs (KGs) to develop domain-constrained dialogue agents with consistent personalities. We introduce the KG-grounded GPT model and compare its capacity to resonate with a general audience against two established models for this task: a persona-grounded GPT model and a relevance-based classifier. Furthermore, we compare all the models against a RAG model in terms of hallucination error rates. Through these human evaluation studies, we demonstrate that the KG-grounded GPT model outperforms existing approaches, yielding higher-quality responses with significantly reduced hallucination errors. Moreover, we highlight the scalability of our method, as it does not require fine-tuning and is straightforward to implement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yxotd46V0U": {
    "title": "Question-Based Retrieval using Atomic Units for Enterprise RAG",
    "volume": "review",
    "abstract": "Enterprise retrieval augmented generation (RAG) offers a highly flexible framework for combining powerful large language models (LLMs) with internal, possibly temporally changing, documents. In RAG, documents are first chunked. Relevant chunks are then retrieved for a specific user query, which are passed as context to a synthesizer LLM to generate the query response. However, the retrieval step can limit performance, as incorrect chunks can lead the synthesizer LLM to generate a false response. This work proposes a zero-shot adaptation of standard dense retrieval steps for more accurate chunk recall. Specifically, a chunk is first decomposed into atomic statements. A set of synthetic questions are then generated on these atoms (with the chunk as the context). Dense retrieval involves finding the closest set of synthetic questions, and associated chunks, to the user query. It is found that retrieval with the atoms leads to higher recall than retrieval with chunks. Further performance gain is observed with retrieval using the synthetic questions generated over the atoms. Higher recall at the retrieval step enables higher performance of the enterprise LLM using the RAG pipeline",
    "checked": true,
    "id": "82981e42c2cdf8e00db13faf5f8613b61d0f883b",
    "semantic_title": "question-based retrieval using atomic units for enterprise rag",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=wtd6kWpB2V": {
    "title": "CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) struggle to model long input sequences due to high memory and runtime costs. Memory-augmented models have emerged as a promising solution to this problem, but current methods are hindered by limited memory capacity and require costly re-training to integrate with a new LLM. In this work, we introduce an associative memory module which can be coupled to any pre-trained (frozen) attention-based LLM without re-training, enabling effective long language modeling. Unlike previous methods, our associative memory module consolidates representations of individual tokens into a non-parametric distribution model, dynamically managed by properly balancing the novelty and recency of the incoming data. By retrieving information from this consolidated associative memory, the base LLM can achieve significant (up to 29.7\\% on Arxiv) perplexity reduction in long-context language modeling compared to other baselines on various standard benchmarks. This architecture, which we call CAMELoT (**C**onsolidated **A**ssociative **M**emory **E**nhanced **Lo**ng **T**ransformer), demonstrates superior performance even with a tiny context window of 128 tokens",
    "checked": true,
    "id": "130850322b067af6961d49c84b04e8eee0002842",
    "semantic_title": "camelot: towards large language models with training-free consolidated associative memory",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=58Y5SMJ39R": {
    "title": "Birdie: Advancing State Space Models with a Minimalist Architecture and Novel Pre-training Objectives",
    "volume": "review",
    "abstract": "State Space Models (SSMs) are emerging as alternatives to Transformers but struggle with tasks needing long-range interactions, such as text copying and multi-query associative recall. Most improvements in SSMs focus on internal architecture rather than exploring diverse pre-training objectives. This paper introduces the Birdie model, a minimalist SSM architecture, with novel pre-training objectives. Experimental evaluations demonstrate that combining this minimalist architecture designed with refined control over recurrence parameterization with pre-training objectives like infilling, copying, and deshuffling significantly improves performance in practical generative tasks, achieving higher average metric scores and win rates. The findings offer valuable insights for optimizing SSMs to compete with Transformers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jr5I4gtkF6": {
    "title": "Shortcomings of LLMs for Low-Resource Translation: Retrieval and Understanding are Both the Problem",
    "volume": "review",
    "abstract": "This work investigates the in-context learning abilities of pretrained large language models (LLMs) when instructed to translate text from a low-resource language into a high-resource language as part of an automated machine translation pipeline. We conduct a set of experiments translating Southern Quechua to Spanish and examine the informativity of various types of information retrieved from a constrained database of digitized pedagogical materials (dictionaries and grammar lessons) and parallel corpora. Using both automatic and human evaluation of model output, we conduct ablation studies that manipulate (1) context type (morpheme translations, grammar descriptions, and corpus examples), (2) retrieval methods (automated vs. manual), and (3) model type. Our results suggest that even relatively small LLMs are capable of utilizing prompt context for zero-shot low-resource translation when provided a minimally sufficient amount of relevant linguistic information. However, the variable effects of prompt type, retrieval method, model type, and language community-specific factors highlight the limitations of using even the best LLMs as translation systems for the majority of the world's 7,000+ languages and their speakers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LTz44KTb0X": {
    "title": "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in handling complex reasoning tasks by generating step-by-step rationales. Some methods have proven effective in boosting accuracy by introducing extra verifiers to assess these paths. However, existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize the relative merits of intermediate steps, thereby limiting the effectiveness of the feedback provided. To overcome this limitation, we propose Tree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reasoning trees via a best-first search algorithm and collects step-level paired data for preference training. Compared to traditional binary classification, step-level preferences more finely capture the nuances between reasoning steps, allowing for a more precise evaluation of the complete reasoning path. We empirically evaluate Tree-PLV across a range of arithmetic and commonsense reasoning tasks, where it significantly outperforms existing benchmarks. For instance, Tree-PLV achieved substantial performance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55\\% â†’ 82.79\\%), MATH (17.00\\% â†’ 26.80\\%), CSQA (68.14\\% â†’ 72.97\\%), and StrategyQA (82.86\\% â†’ 83.25\\%). Additionally, our study explores the appropriate granularity for applying preference learning, revealing that step-level guidance provides feedback that better aligns with the evaluation of the reasoning process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CHkYtgbp6T": {
    "title": "EXCGEC: A Benchmark of Edit-wise Explainable Chinese Grammatical Error Correction",
    "volume": "review",
    "abstract": "Existing studies explore the explainability of Grammatical Error Correction (GEC) in a limited scenario, where they ignore the interaction between corrections and explanations. To bridge the gap, this paper introduces the task of EXplainable GEC (**ECXGEC**), which focuses on the integral role of both correction and explanation tasks. To facilitate the task, we propose **EXCGEC**, a tailored benchmark for Chinese EXGEC consisting of 8,216 explanation-augmented samples featuring the design of hybrid edit-wise explanations. We benchmark several series of LLMs in multiple settings, covering post-explaining and pre-explaining. To promote the development of the task, we introduce a comprehensive suite of automatic metrics and conduct human evaluation experiments to demonstrate the human consistency of the automatic metrics for free-text explanations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XehjPcp9la": {
    "title": "Intermediate Adapter: Efficient Alignment of Text in Diffusion Models",
    "volume": "review",
    "abstract": "Diffusion models have been widely used for text-to-image generation tasks. However, state-of-the-art models still fail to align the generated visual concepts with high-level semantics in a language such as object count, spatial relationship, etc. We approach this problem from an architectural perspective and investigate how conditioning architecture can affect vision-language alignment in diffusion models. We propose a new conditioning architecture named Intermediate Adapter to improve text-to-image alignment, generation quality, as well as training and inference speed for diffusion models. We perform experiments on the text-to-image generation task on the MS-COCO dataset. We apply Intermediate Adapters on two common conditioning methods on a U-ViT backbone. For both end-to-end training and fine-tuning of pretrained diffusion models, Our method boosts the CLIP Score, FID, and human evaluation results of the generated images, with 20% reduced FLOPs and 25% increased training and inference speed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kpDfxMzqck": {
    "title": "A New Competency Tagging Method through Semantic Matching with Fine-tuned LLM",
    "volume": "review",
    "abstract": "Competency tagging is essential in both academic and industrial domains, facilitating alignment of learning content, job posting and resumes with specific competencies. However, the manual tagging process is time-consuming, labor-intensive, and expensive. In this study, we propose semantic retrieval-based method for automated competency tagging. Particularly, we explore the potential of large language models (LLMs) to encode text data from learning content and competency descriptions. Subsequently, we employ similarity search to retrieve the most pertinent competency tags corresponding to a given learning content document. We investigated semantic search at different levels of granularity: per document, per paragraph, and per sentence. We further fine-tuned the LLM using the Low-Rank Adaptation (LoRA) technique. Our method yielded promising results, achieving a recall@10 of 80.29% when tested on 164 pages of learning content associated with 96 competencies. These findings highlight the effectiveness of fine-tuned LLMs, which enhanced recall@10 by 6%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yGbc8D9fpv": {
    "title": "Swan: A Family of Arabic-Centric Cross-Lingual Embedding Models",
    "volume": "review",
    "abstract": "This paper introduces Swan, a family of cutting-edge embedding models specialized for Arabic language understanding. We present two models, namely Swan-Base and Swan-Large, which are further trained using a large-scale synthetic corpus. To comprehensively evaluate our models, we introduce an extensive text evaluation benchmark, dubbed ArabicMTEB. ArabicMTEB is the largest Arabic text embedding evaluation benchmark to date, covering eight tasks across 74 diverse datasets. Additionally, we propose ArabicMTEBLite, a lightweight and domain-specific synthetic dataset designed for holistic evaluation. Our experiments reveal that Swan-Large exhibits remarkable text embedding capabilities, consistently outperforming all open source models including, Multilingual-E5-large, across all tasks. Furthermore, our efficient model, Swan-Base, also surpasses Multilingual-E5-base in all evaluated tasks. We also explore the impact of synthetic data and the number of hard negatives on the performance of Swan-Base and Swan-Large. Our findings demonstrate that Swan-Base offers an optimal balance between performance, inference time, and cost. Our models will be made publicly accessible for research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0hTlMm1im6": {
    "title": "Personalized LLM Response Generation with Parameterized User Memory Injection",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, personalized LLM response generation holds the potential to offer substantial benefits for individuals. However, existing work struggles with efficiently incorporating user information for LLM personalization. In this study, we draw inspirations from real-world bionic memory mechanism to propose a novel parameterized **M**emory-**i**njected approach using parameter-efficient fine-tuning (PEFT) combined with a Bayesian Optimisation searching strategy to achieve **L**LM **P**ersonalization(**MiLP**). Our MiLP takes advantage from the alignment between real-world memory mechanism and the LLM's architecture. Extensive experiments have shown the superiority and effectiveness of MiLP. To encourage further research into this area, we are releasing our implementation code",
    "checked": false,
    "id": "c1f3c757da46a029ea7fad35c1b183ca460f4100",
    "semantic_title": "personalized llm response generation with parameterized memory injection",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=nliOFWYlq8": {
    "title": "The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding",
    "volume": "review",
    "abstract": "In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, PhysiCo. Our task alleviates the memorization issue via the usage of grid-format inputs that abstractly describe physical phenomena. The grids represents varying levels of understanding, from the core phenomenon, application examples to analogies to other abstract patterns in the grid world. A comprehensive study on our task demonstrates that: (1) state-of-the-art LLMs lag behind humans by $\\sim$40%; (2) the stochastic parrot phenomenon is present in LLMs, as they fail on our grid task but can describe and recognize the same concepts well in natural language; (3) our task challenges the LLMs due to intrinsic difficulties rather than the unfamiliar grid format, as in-context learning and fine-tuning on same formatted data added little to their performance. Our data and code will be released for public research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fpSUK1rbQs": {
    "title": "Language Modeling with Editable External Knowledge",
    "volume": "review",
    "abstract": "When the world changes, so does the text that people write about it. How do we build language models that can be easily updated to reflect these changes? One popular approach is retrieval-augmented generation, in which new documents are inserted into a knowledge base and retrieved during prediction for downstream tasks. Most prior work on these systems have focused on improving behavior during *prediction* through better retrieval or reasoning. This paper introduces ERASE, which instead improves model behavior *when new documents are created*, by incrementally deleting or rewriting other entries in the knowledge base each time a new document is encountered. In two new benchmark datasets evaluating models' ability to answer questions about a stream of news articles or conversations, ERASE improves accuracy relative to conventional retrieval-augmented generation by 7--13\\% (Mixtral-8x7B) and 6--10\\% (Llama-3-8B) absolute. Code and data will be made publicly available",
    "checked": true,
    "id": "c1743c55e70965b0ffada89e73f38fef2216467f",
    "semantic_title": "language modeling with editable external knowledge",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UCiDbOtHUb": {
    "title": "OT-Class: Optimal Transport-Enhanced Multi-label Text Classification",
    "volume": "review",
    "abstract": "Multi-label text classification (MLTC) aims to assign at least one label from a vast label space to a document. This task is challenging due to the large number of labels, which can range from hundreds to thousands, and the potential interdependence of labels. While previous efforts have achieved success in fully-supervised settings, they have limited performance in more practical weakly-supervised settings. Despite its potential benefits, an auxiliary task of word-to-label alignment that aligns words in the input text to the large label space has been largely overlooked in existing work. Word-to-label alignment is significant, as it provides valuable insights into how words contribute to the overall classification of a document. However, existing MLTC datasets lack ground truth labels for word-to-label alignment for supervised training. To address this limitation, we propose a novel framework called OT-Class, which incorporates unsupervised word-to-label alignment into MLTC using optimal transport (OT). Our framework tackles MLTC in a multi-task setting, comprising a primary task that classifies documents using a standard text classification algorithm and an auxiliary task that identifies corresponding labels for all input document words via optimal transport. Our experiments demonstrate that OT-Class outperforms baselines that do not utilize word-to-label alignment, highlighting its effectiveness. A detailed analysis reveals that OT-Class has an amplified advantage in fine-grained label spaces and appropriately influences predictions through word-to-label alignment",
    "checked": false,
    "id": "1196fc4f84e6e1cd2e17cffd9c28ff4a9ae0d092",
    "semantic_title": "otseq2set: an optimal transport enhanced sequence-to-set model for extreme multi-label text classification",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=FyGJs8nhhU": {
    "title": "Pattern-Aware Chain-of-Thought Prompting in Large Language Models",
    "volume": "review",
    "abstract": "Chain-of-thought (CoT) prompting can guide language models to engage in complex multi-step reasoning. The quality of provided demonstrations significantly impacts the success of downstream inference tasks. While existing automated methods prioritize accuracy and semantics in these demonstrations, we show that the underlying reasoning patterns play a more crucial role in such tasks. In this paper, we propose Pattern-Aware CoT, a prompting method that considers the diversity of demonstration patterns. By incorporating reasoning patterns such as step length and operation within intermediate steps, PA-CoT effectively mitigates the issue of bias induced by demonstrations and enables better generalization to diverse scenarios. We conduct experiments on nine reasoning benchmark tasks using two open-source LLMs. The results show that our method substantially enhances reasoning performance and exhibits robustness to errors. The code will be made publicly available",
    "checked": true,
    "id": "449257147fe40e0016f3ef89a62f20db8ff29039",
    "semantic_title": "pattern-aware chain-of-thought prompting in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CvrzffhXSg": {
    "title": "When ``A Helpful Assistant'' Is Not Really Helpful: Personas in System Prompts Do Not Improve Performances of Large Language Models",
    "volume": "review",
    "abstract": "Prompting serves as the major way humans interact with Large Language Models (LLM). Commercial AI systems commonly define the role of the LLM in system prompts. For example, ChatGPT uses ``You are a helpful assistant'' as part of the default system prompt. Despite current practices to add personas in system prompts, it is unclear how different personas affect the models' performance. In this study, we present a systematic evaluation of personas in system prompts. We create a list of 162 roles covering 6 types of interpersonal relationships and 8 domains of expertise. Through extensive analysis of 4 popular LLMs and 2410 factual questions, we show that adding personas in system prompts does not improve the models' performance over a range of questions compared with the control setting where no persona is added. Despite this, further analysis suggests that the gender, type, and domain of the persona could all affect the consequential prediction accuracy. We further experimented with a list of persona search strategies and found that while aggregating the results from the best personas for each question could significantly lead to higher prediction accuracies, automatically identifying the best persona is challenging and may not be significantly better than random selection. Overall, our result suggests that while adding persona may lead to performance gain in certain settings, the effect of each persona can be largely random",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4AzksYndtz": {
    "title": "Seeing the Big through the Small\": Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations?",
    "volume": "review",
    "abstract": "Human label variation (HLV) is a valuable source of information that arises when multiple human annotators provide different labels for valid reasons. In Natural Language Inference (NLI) earlier approaches to capturing HLV involve either collecting annotations from many crowd workers to represent human judgment distribution (HJD) or use expert linguists to provide detailed explanations for their chosen labels. While the former method provides denser HJD information, obtaining it is resource-intensive. In contrast, the latter offers richer textual information but it is challenging to scale up to many human judges. Besides, large language models (LLMs) are increasingly used as evaluators (``LLM judges'') but with mixed results, and few works aim to study HJDs. This study proposes to exploit LLMs to approximate HJDs using a small number of expert labels and explanations. Our experiments show that a few explanations significantly improve LLMs' ability to approximate HJDs with and without explicit labels, thereby providing a solution to scale up annotations for HJD. However, fine-tuning smaller soft-label aware models with the LLM-generated model judgment distributions (MJDs) presents partially inconsistent results: while similar in distance, their resulting fine-tuned models and visualized distributions differ substantially. We show the importance of complementing instance-level distance measures with a global-level shape metric and visualization to more effectively evaluate MJDs against human judgment distributions",
    "checked": true,
    "id": "40da525d570cec153979c7ff709a7783ec5561ca",
    "semantic_title": "seeing the big through the small\": can llms approximate human judgment distributions on nli from a few explanations?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sgpUbD2okH": {
    "title": "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations",
    "volume": "review",
    "abstract": "While language model (LM)-powered chatbots and generative search engines excel at answering concrete queries, discovering information in the terrain of unknown unknowns remains challenging for users. To emulate the common educational scenario where children/students learn by listening to and participating in conversations of their parents/teachers, we create Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all the questions, Co-STORM lets users observe and occasionally steer the discourse among several LM agents. The agents ask questions on the user's behalf, allowing the user to discover unknown unknowns serendipitously. To facilitate user interaction, Co-STORM assists users in tracking the discourse by organizing the uncovered information into a dynamic mind map, ultimately generating a comprehensive report as takeaways. For automatic evaluation, we construct the WildSeek dataset by collecting real information-seeking records with user goals. Co-STORM outperforms baseline methods on both discourse trace and report quality. In a further human evaluation, 70% of participants prefer Co-STORM over a search engine, and 78% favor it over a RAG chatbot",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V0I9AyRoV1": {
    "title": "Regression (and Scoring) Aware Inference with LLMs",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown strong results on a range of applications, including regression and scoring tasks. Typically, one obtains outputs from an LLM via autoregressive sampling from the model's output distribution. We show that this inference strategy can be sub-optimal for common regression and scoring evaluation metrics. As a remedy, we build on prior work on Minimum Bayes Risk decoding, and propose alternate inference strategies for regression and scoring that estimate the Bayes-optimal solution for the given metric in closed-form from sampled responses. We show that our proposal yields significant improvements over baselines across datasets and models",
    "checked": false,
    "id": "d413b4c104ccd0513905f527994b5c1cf502e4ff",
    "semantic_title": "metric-aware llm inference for regression and scoring",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TnIUaq2d5y": {
    "title": "The Association Between Training Data and Success Ratios in Text-to-Image Generation",
    "volume": "review",
    "abstract": "Text-to-image (T2I) models are often touted for their supposed ability to create compositional images with many components. However, these models can fail to generate all entities when presented with prompts containing just two or three entities. In this work, we seek an explanation of such failures with respect to the training data. We introduce the training appearance ratio, which compares the number of training images depicting specific entities vs. the number of training captions mentioning those same entities, and examine how well this measure correlates with generation success rates. We then examine whether prompts consisting of various entities will result in successful generations (i.e., images that depict all specified entities) based on the training appearance ratio. We find positive and significant correlations between these ratios and successful image generations. Furthermore, our proposed measure yields stronger correlations with model success rates than existing training data frequency measures. These associations suggest that our measure (training appearance ratio) better captures the relationship between training data and generation success",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s4OukoHwnz": {
    "title": "Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning",
    "volume": "review",
    "abstract": "We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8$% (+$5.9$%), $34.7$% (+$5.8$%), and $76.4$% (+$15.8$%), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains",
    "checked": true,
    "id": "38333f6e8f0388968edc4b2ea7a683ce69677e69",
    "semantic_title": "monte carlo tree search boosts reasoning via iterative preference learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=LfDjUznLUv": {
    "title": "ChatGPT Doesn't Trust LA Chargers Fans: Guardrail Sensitivity in Context",
    "volume": "review",
    "abstract": "While the biases of language models in production are extensively documented, the biases of their guardrails themselves have been neglected. This paper studies how contextual information about the user influences the likelihood of an LLM to refuse to execute a request. By generating user biographies that offer ideological and demographic information, we find a number of biases in guardrail sensitivity on ChatGPT-3.5. Younger, female, White, and Asian-American personas were more likely to trigger a refusal guardrail when requesting censored or illegal information. Guardrails are also sycophantic, leading to refusals to comply with requests for a political position the user might disagree with. Furthermore, we find that certain identity groups and even seemingly innocuous user information like sports fandom can elicit changes in guardrail sensitivity similar to overt political endorsement. For each demographic category and even for National Football League (NFL) team fandom declarations, we find that ChatGPT seemingly infers a likely political ideology and modifies guardrail behavior accordingly",
    "checked": false,
    "id": "b3ff72674ce1c849a2cd9110f6d217927dcf7402",
    "semantic_title": "chatgpt doesn't trust chargers fans: guardrail sensitivity in context",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=sSkTGRZMBj": {
    "title": "Annotator-Centric Active Learning for Subjective NLP Tasks",
    "volume": "review",
    "abstract": "Active Learning (AL) addresses the high costs of collecting human annotations by strategically annotating the most informative samples. However, for subjective NLP tasks, incorporating a wide range of perspectives in the annotation process is crucial to capture the variability in human judgments. We introduce Annotator-Centric Active Learning (ACAL), which incorporates an annotator selection strategy following data sampling. Our objective is two-fold: (1) to efficiently approximate the full diversity of human judgments, and (2) to assess model performance using annotator-centric metrics, which emphasize minority perspectives over a majority. We experiment with multiple annotator selection strategies across seven subjective NLP tasks, employing both traditional and novel, human-centered evaluation metrics. Our findings indicate that ACAL improves data efficiency and excels in annotator-centric performance evaluations. However, its success depends on the availability of a sufficiently large and diverse pool of annotators to sample from",
    "checked": true,
    "id": "0bd68693587fbf7d9190bcc373fbafa4a0af177d",
    "semantic_title": "annotator-centric active learning for subjective nlp tasks",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=mjjTKZZPlY": {
    "title": "Can We Instruct LLMs to Compensate for Position Bias?",
    "volume": "review",
    "abstract": "Position bias in LLMs leads to difficulty in accessing information retrieved from the retriever, thus downgrading the effectiveness of RAG approaches in open-question answering. Recent studies reveal that this bias is related to disproportional attention across the context. In this work, we examine directing the LLMs to allocate more attention towards a selected segment of the context through prompting, such that the shortage of attention can be compensated. We find that language models do not have relative position awareness of the context but can be directed by promoting instruction with an exact document index. Our analysis contributes to a deeper understanding of position bias in LLMs and provides a pathway to mitigate this bias by instruction, thus benefiting LLMs in locating and utilizing relevant information from retrieved documents in RAG applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UzIpiYHEoU": {
    "title": "War and Peace (WarAgent): LLM-based Multi-Agent Simulation of World Wars",
    "volume": "review",
    "abstract": "Can we avoid wars at the crossroads of history? This question has been pursued by scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question based on the recent advances of Artificial Intelligence and Large Language Models. We propose WarAgent, an LLM-powered multi-agent AI system, to simulate the participating countries, their decisions, and the consequences, in three historical international conflicts. Based on WarAgent, we also propose standard evaluation protocols for LLM-based multi-agent simulation systems, which helps the community to advance on the important evaluation problem of multi-agent systems. By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems' abilities in studying complex collective human behaviors such as international conflicts under diverse settings. In these simulations, the emergent interactions among agents also offer a novel perspective for examining the triggers and conditions that lead to war. Our findings offer data-driven and AI-augmented insights that can redefine how we approach conflict resolution and peacekeeping strategies. The implications stretch beyond computer simulation and historical analysis, offering a blueprint for using AI to understand human history and possibly prevent future international conflicts. Code and data are available at \\url{https://anonymous.4open.science/r/war_and_peace-061E}",
    "checked": false,
    "id": "9ad3edeea4732cb44a26f39652a668d1a562b0cf",
    "semantic_title": "war and peace (waragent): large language model-based multi-agent simulation of world wars",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=4NxcIoCcDH": {
    "title": "Aligning Language Models Using Multi-Objective Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "The alignment techniques used in state-of-the-art language models (LMs), e.g., reinforcement learning from human feedback (RLHF), have driven many successful natural language processing (NLP) tasks. RLHF uses human preferences based on the guidelines of being helpful and safe as a *single* reward signal to fine-tune language models. However, the trade-offs between helpfulness and safety are often found to be a problem, which makes it difficult for a model trained towards one objective to perform well on both. This paper proposes a new alignment technique, multi-objective language model alignment (MOLMA). The framework is based on *multi*-objective deep reinforcement learning to fine-tune language models. MOLMA can efficiently address the conflicting or the dominating learning signal issue caused by the trade-offs of inherent, often conflicting, multi-objectives underlying the language model alignment task. From the overall objective of achieving helpfulness and safety, our results show that MOLMA outperforms the other alignment techniques that rely on single-objective deep reinforcement learning",
    "checked": false,
    "id": "d8c78221e4366d6a72a6b3e41e35b706cc45c01d",
    "semantic_title": "training diffusion models with reinforcement learning",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=gmcHqlRC4n": {
    "title": "VALUE-Bench: A Comprehensive Benchmark for Evaluating Large Vision-Language Models on Multimodal Ethical Understanding",
    "volume": "review",
    "abstract": "Multimodal ethical understanding refers to morally analyzing and discerning ethical scenarios described in both visual and natural language contexts. While various aspects of large vision-language models (LVLMs) have been evaluated, their capacity for multimodal ethical understanding remains unclear to the public. In this paper, we propose VALUE-Bench, a comprehensive benchmark that rigorously evaluates the multimodal ethical understanding ability of LVLMs. Instead of focusing on the surface descriptions of images and language, the VALUE-Bench is progressively and comprehensively evaluated on four dimensions: ethical understanding, robustness, reliability, and resistance to misuse. We collect 6 datasets and 10 multimodal ethical understanding tasks in real-world multimodal ethical scenarios (e.g., harmful, hateful, offensive, humiliating, violent, misogynistic, stereotyping, objectifying, etc.). Moreover, we provide an in-depth analysis of the multimodal ethical understanding of existing English and Chinese LVLMs. VALUE-Bench is very helpful to enhance the evaluation of LVLMs' multimodal ethical understanding by providing a nuanced view of their ethical understanding level and ethical decision-making ability in both English and Chinese contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Km7sEBdyzh": {
    "title": "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering",
    "volume": "review",
    "abstract": "Despite Large Language Models (LLMs) have performed impressively in various Natural Language Processing (NLP) tasks, their inherent hallucination phenomena severely challenge their credibility in complex reasoning. Combining explainable Knowledge Graphs (KGs) with LLMs is a promising path to address this issuse. However, structured KGs are difficult to utilize, and how to make LLMs understand and incorporate them is a challenging topic. We thereby reorganize a more efficient structure of KGs, while designing the KG-related instruction tuning and continual pre-training strategies to enable LLMs to learn and internalize this form of representation effectively. Moreover, we construct subgraphs to further enhance retrieval capabilities of KGs via CoT reasoning. Extensive experiments on two KGQA datasets demonstrate that our model achieve convincing performance compared to strong baselines\\footnote{All the code, data and models will be publicly available at \\url{https://anonymous.com}}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EQYtmyOrql": {
    "title": "One Model is All You Need: ByT5-Sanskrit, a Unified Model for Sanskrit NLP Tasks",
    "volume": "review",
    "abstract": "Morphologically rich languages are notoriously challenging to process for downstream NLP applications. This paper presents a new pretrained language model, ByT5-Sanskrit, designed for NLP applications involving the morphologically rich language Sanskrit. We evaluate ByT5-Sanskrit on established Sanskrit word segmentation tasks, where it outperforms previous data-driven approaches by a considerable margin and matches the performance of the current best lexicon-based model. It is easier to deploy and more robust to data not covered by external linguistic resources. It also achieves new state-of-the-art results in Vedic Sanskrit dependency parsing and OCR post-correction tasks. Additionally, based on the Digital Corpus of Sanskrit, we introduce a novel multitask dataset for the joint training of Sanskrit word segmentation, lemmatization, and morphosyntactic tagging tasks. We fine-tune ByT5-Sanskrit on this dataset, creating a versatile multitask model for various downstream Sanskrit applications. We have used this model in Sanskrit linguistic annotation projects, in information retrieval setups, and as a preprocessing step in a Sanskrit machine translation pipeline. We also show that our approach yields new best scores for lemmatization and dependency parsing of other morphologically rich languages. We thus demonstrate that byte-level pretrained language models can achieve excellent performance for morphologically rich languages, outperforming tokenizer-based models and presenting an important vector of exploration when constructing NLP pipelines for such languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zymMBdfyDw": {
    "title": "Large Language Models are In-context Teachers for Knowledge Reasoning",
    "volume": "review",
    "abstract": "Chain-of-thought (CoT) prompting teaches large language models (LLMs) in context to reason over queries that require more than mere information retrieval. However, human experts are usually required to craft demonstrations for in-context learning (ICL), which is expensive and has high variance. More importantly, how to craft helpful reasoning exemplars for ICL remains unclear. In this work, we investigate whether LLMs can be better in-context teachers for knowledge reasoning. We follow the ``encoding specificity'' hypothesis in human's memory retrieval to assume in-context exemplars at inference should match the encoding context in training data. We are thus motivated to propose Self-Explain to use one LLM's self-elicited explanations as in-context demonstrations for prompting it as they are generalized from the model's training examples. Self-Explain is shown to significantly outperform using human-crafted exemplars and other baselines. We further reveal that for in-context teaching, rationales by distinct teacher LLMs or human experts that more resemble the student LLM's self-explanations are better demonstrations, which supports our encoding specificity hypothesis. We then propose Teach-Back that aligns the teacher LLM with the student to enhance the in-context teaching performance. For example, Teach-Back enables a 7B model to teach the much larger GPT-3.5 in context, surpassing human teachers by around 5% in test accuracy on medical question answering",
    "checked": true,
    "id": "533f5ec8d126408247ad9c9ec5830ed00bfc7501",
    "semantic_title": "large language models are in-context teachers for knowledge reasoning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=GWZ0SYS3Bl": {
    "title": "Soft Prompting for Unlearning in Large Language Models",
    "volume": "review",
    "abstract": "The widespread popularity of Large Language Models (LLMs), partly due to their unique ability to perform in-context learning, has also brought to light the importance of ethical and safety considerations when deploying these pre-trained models. In this work, we focus on investigating machine unlearning for LLMs motivated by data protection regulations. In contrast to the growing literature on fine-tuning methods to achieve unlearning, we focus on a comparatively lightweight alternative called soft prompting to realize the unlearning of a subset of training data. With losses designed to enforce forgetting as well as utility preservation, our framework Soft Prompting for Unlearning (SPUL) learns prompt tokens that can be appended to an arbitrary query to induce unlearning of specific examples at inference time without updating LLM parameters. We conduct a rigorous evaluation of the proposed method and our results indicate that SPUL can significantly improve the trade-off between utility and forgetting in the context of text classification with LLMs. We further validate our method using multiple LLMs to highlight the scalability of our framework and provide detailed insights into the choice of hyperparameters and the influence of the size of unlearning data. Code and data are available at https://tinyurl.com/softprompt",
    "checked": true,
    "id": "ec78f7a49c49224c9647aaf23f133c00b7fefa6f",
    "semantic_title": "soft prompting for unlearning in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1Wz5w6rVSa": {
    "title": "Assessing the Role of Lexical Semantics in Cross-lingual Transfer through Controlled Manipulations",
    "volume": "review",
    "abstract": "While cross-linguistic model transfer is effective in many settings, there is still limited understanding of the conditions under which it works. In this paper, we focus on assessing the role of lexical semantics in cross-lingual transfer, as we compare its impact to that of other language properties. Examining each language property individually, we systematically analyze how differences between English and a target language influence the capacity to align the language with an English pretrained representation space. We do so by artificially manipulating the English sentences in ways that mimic specific characteristics of the target language, and reporting the effect of each modification on the quality of alignment with the representation space. We show that while properties such as the script or word order only have a limited impact on the alignment quality, the degree of lexical matching between the two languages, which we define using a measure of translation entropy, greatly affects it",
    "checked": true,
    "id": "41026e7ccb1f1dac273c255b591073116479afb1",
    "semantic_title": "assessing the role of lexical semantics in cross-lingual transfer through controlled manipulations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xMQ79IsvJL": {
    "title": "Improving Translation Faithfulness of Large Language Models via Augmenting Instructions",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) present strong general capabilities, and a current compelling challenge is stimulating their specialized capabilities, such as machine translation through low-cost instruction tuning. The standard data for instruction-following is organized as a con-catenated series of instructions, inputs, and outputs. Due to the inherent pattern of the attention mechanism in LLMs, these models tend to concentrate more on nearby tokens. Consequently, there is a high risk of forgetting instructions during the decoding process, particularly when dealing with long contexts. To alleviate the instruction forgetting issue on translation, we propose SWIE (Segment-Weighted Instruction Embedding) and an instruction-following dataset OVERUNDER. SWIE improves the model instruction understanding by adding an instruction representation on the following input and response representations. OVERUNDER improves model faithfulness by comparing over-translation and under-translation samples with the correct translation. We apply our methods to two mainstream open-source LLMs, i.e., BLOOM and LLaMA. Experimental results demonstrate that using SWIE and OVERUNDER in models improves translation performance and faithfulness over the strong baselines. Furthermore, SWIE improves the model performance on various long-context scenarios, including in-context translation, translation on language direction in the instruction-tuning corpus, and translation on zero-shot language pairs. The effectiveness of SWIE is demonstrated on the IFEval instruction-following test set, indicating its potential for broader task applicability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x2Ay4agzz2": {
    "title": "From Intentions to Techniques: A Comprehensive Taxonomy and Challenges in Text Watermarking for Large Language Models",
    "volume": "review",
    "abstract": "With the rapid growth of Large Language Models (LLMs), safeguarding textual content against unauthorized use is crucial. Text watermarking offers a vital solution, protecting both - LLM-generated and plain text sources. This paper presents a unified overview of different perspectives behind designing watermarking techniques, through a comprehensive survey of the research literature. Our work has two key advantages, (1) we analyze research based on the specific intentions behind different watermarking techniques, evaluation datasets used, watermarking addition, and removal methods to construct a cohesive taxonomy. (2) We highlight the gaps and open challenges in text watermarking to promote research in protecting text authorship. This extensive coverage and detailed analysis sets our work apart, offering valuable insights into the evolving landscape of text watermarking in language models",
    "checked": true,
    "id": "b230545fbf276b6f6e01c6662a62bc2c01ca4b9f",
    "semantic_title": "from intentions to techniques: a comprehensive taxonomy and challenges in text watermarking for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ouUtjTa8vO": {
    "title": "Large Language Models for Data Annotation: Methods, Applications, and Challenges",
    "volume": "review",
    "abstract": "Data annotation is the labeling or tagging of raw data with relevant information, essential for improving the efficacy of machine learning models. The process, however, is labor-intensive and expensive. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to revolutionize and automate the intricate process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, this paper uniquely focuses on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations, and Learning with LLM-generated annotations. Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation. As a key guide, this survey aims to direct researchers and practitioners in exploring the potential of the latest LLMs for data annotation, fostering future advancements in this critical domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dSl8Cy3ID6": {
    "title": "Graph-tree Fusion Model with Bidirectional Information Propagation for Long Document Classification",
    "volume": "review",
    "abstract": "Long document classification presents challenges in capturing both local and global dependencies due to their extensive content and complex structure. Existing methods often struggle with token limits and fail to adequately model hierarchical relationships within documents. To address these constraints, we propose a novel model leveraging a graph-tree structure. Our approach integrates syntax trees for sentence encodings and document graphs for document encodings, which capture fine-grained syntactic relationships and broader document contexts. We use Tree Transformers to generate sentence encodings, while a graph attention network models inter- and intra-sentence dependencies. During training, we implement bidirectional information propagation from word-to-sentence-to-document and vice versa, which enriches the contextual representation. Our proposed method enables a comprehensive understanding of content at all hierarchical levels and effectively handles arbitrarily long contexts without token limit constraints. Experimental results demonstrate the effectiveness of our approach in all types of long document classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CteqUe86mH": {
    "title": "Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks",
    "volume": "review",
    "abstract": "Human perception of language depends on personal backgrounds like gender and ethnicity. While existing studies have shown that large language models (LLMs) hold values that are closer to certain societal groups, it is unclear whether their prediction behaviors on subjective NLP tasks also exhibit a similar bias. In this study, leveraging the POPQUORN dataset which contains annotations from diverse demographic backgrounds, we conduct a series of experiments on six popular LLMs to investigate their capabilities to understand demographic differences and their potential biases in predicting politeness and offensiveness. We find that for both tasks, model predictions are closer to the labels from White participants than Asian and Black participants. While we observe no significant differences between the two gender groups for most of the models for offensiveness, LLMs' predictions for politeness are significantly closer to women's ratings. We further explore prompting with specific identity information and show that including a target demographic label in the prompt does not consistently improve models' performance. Our results suggest that LLMs hold gender and racial biases for subjective NLP tasks, and that demographic-infused prompts alone may not be sufficient to mitigate such biases",
    "checked": true,
    "id": "f953dbea152b4160a4fc7463cc6dcd3214111117",
    "semantic_title": "aligning with whom? large language models have gender and racial biases in subjective nlp tasks",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=LvfghCNzyT": {
    "title": "Enhancing Reinforcement Learning with Intrinsic Rewards from Language Model Critique",
    "volume": "review",
    "abstract": "Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful critic model, and another where a single language model fulfills both roles. We assess our approach on three text generation tasks: sentiment control, language model detoxification, and summarization. Experimental results show that incorporating artificial intrinsic rewards significantly improve both sample efficiency and the overall performance of the policy model, supported by both automatic and human evaluation",
    "checked": false,
    "id": "faae9de3d314e8731b0505607298fd826e3de1a7",
    "semantic_title": "beyond sparse rewards: enhancing reinforcement learning with language model critique in text generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=7NtfIYIqvk": {
    "title": "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have recently gained significant attention due to their remarkable capabilities in performing diverse tasks across various domains. However, a thorough evaluation of these models is crucial before deploying them in real-world applications to ensure they produce reliable performance. Despite the well-established importance of evaluating LLMs in the community, the complexity of the evaluation process has led to varied evaluation setups, causing inconsistencies in findings and interpretations. To address this, we systematically review the primary challenges and limitations causing these inconsistencies and unreliable evaluations in various steps of LLM evaluation. Based on our critical review, we present our perspectives and recommendations to ensure LLM evaluations are reproducible, reliable, and robust",
    "checked": true,
    "id": "df61f25e1f77163d82b8e52183b629404d535b88",
    "semantic_title": "a systematic survey and critical review on evaluating large language models: challenges, limitations, and recommendations",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=UP2apLrRO3": {
    "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions",
    "volume": "review",
    "abstract": "As Large Language Models (LLMs) continue to evolve, they are increasingly being employed in numerous studies to simulate societies and execute diverse social tasks. However, LLMs are susceptible to societal biases due to their exposure to human-generated data. Given that LLMs are being used to gain insights into various societal aspects, it is essential to mitigate these biases. To that end, our study investigates the presence of implicit gender biases in multi-agent LLM interactions and proposes two strategies to mitigate these biases. We begin by creating a dataset of scenarios where implicit gender biases might arise, and subsequently develop a metric to assess the presence of biases. Our empirical analysis reveals that LLMs tend to generate outputs with substantial male biases ($\\geq \\approx 50\\%$ of the time). Furthermore, these biases tend to escalate following multi-agent interactions. To mitigate them, we propose two strategies: self-reflection with in-context examples (ICE); and supervised fine-tuning. Our research demonstrates that both methods effectively mitigate implicit biases, with the ensemble of fine-tuning and self-reflection proving to be the most successful",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SIBusKukhe": {
    "title": "Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation",
    "volume": "review",
    "abstract": "Previous in-context learning (ICL) research has focused on tasks such as classification, machine translation, text2table, etc., while studies on whether ICL can improve human-like dialogue generation are scarce. Our work fills this gap by systematically investigating the ICL capabilities of large language models (LLMs) in persona-based dialogue generation, conducting extensive experiments on high-quality real human Chinese dialogue datasets. From experimental results, we draw three conclusions: 1) adjusting prompt instructions is the most direct, effective, and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results, possibly due to the greater diversity and the amount of effective information; counter-intuitively, retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi-turn associations and single-turn semantics in the demos, increasing the number of demos still improves dialogue performance, proving that LLMs can learn from corrupted dialogue demos. Previous explanations of the ICL mechanism, such as $n$-gram induction head, cannot fully account for this phenomenon",
    "checked": true,
    "id": "86b488a19f12888d707ae8895be0703a69480c2f",
    "semantic_title": "crafting a good prompt or providing exemplary dialogues? a study of in-context learning for persona-based dialogue generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LVzKm1nvsd": {
    "title": "An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance",
    "volume": "review",
    "abstract": "Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we introduce a new task of translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset -- (i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image; and (ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our code and data is released here: https://anonymous.4open.science/r/image-translation-6980",
    "checked": true,
    "id": "4a02d2fef5b94a4da8346f1fa88ae5f9294f886b",
    "semantic_title": "an image speaks a thousand words, but can everyone listen? on image transcreation for cultural relevance",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=HIJOhOLUh4": {
    "title": "Rethinking Pruning for Vision-Language Models: Strategies for Effective Sparsity and Performance Restoration",
    "volume": "review",
    "abstract": "Vision-Language Models (VLMs) integrate information from multiple modalities and have shown remarkable success across various tasks. However, deploying large-scale VLMs in resource-constrained scenarios is challenging. Pruning followed by finetuning offers a potential solution but remains underexplored for VLMs. This study addresses two key questions: how to distribute sparsity across different modality-specific models, and how to restore the performance of pruned sparse VLMs. Our preliminary studies identified two effective pruning settings: applying the same sparsity to both vision and language models, and pruning only the language models. While LoRA finetuning aims to restore sparse models, it faces challenges due to incompatibility with sparse models, disrupting the pruned sparsity. To overcome these issues, we propose SparseLoRA, which applies sparsity directly to LoRA weights. Our experimental results demonstrate significant improvements, including an 11.3\\% boost under 2:4 sparsity and a 47.6\\% enhancement under unstructured 70\\% sparsity. Code and scripts will be released upon acceptance",
    "checked": true,
    "id": "86cd09ebe9eacd5af120fb5650893f840cb3e610",
    "semantic_title": "rethinking pruning for vision-language models: strategies for effective sparsity and performance restoration",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z5DXkGyHCc": {
    "title": "Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction",
    "volume": "review",
    "abstract": "Large language model (LLM) agents show promise in an increasing number of domains. In many proposed applications, it is expected that the agent reasons over accumulated experience presented in an input prompt. We propose the OEDD (Operationalize Experience Despite Distraction) corpus, a human-annotator-validated body of scenarios with pre-scripted agent histories where the agent must make a decision based on disparate experiential information in the presence of a distractor. We evaluate three state-of-the-art LLMs (GPT-3.5 Turbo, GPT-4o, and Gemini 1.5 Pro) and observe that when (1) the input context contains over 1,615 tokens of historical interactions, (2) a crucially decision-informing premise is the rightful conclusion over two disparate environment premises, and (3) a trivial, but distracting red herring fact follows, all LLMs perform worse than random choice at selecting the better of two actions. Our code and test corpus are publicly available at: [omitted]",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kVVyhb1kwu": {
    "title": "Do Not Design, Learn: A Trainable Scoring Function for Uncertainty Estimation in Generative LLMs",
    "volume": "review",
    "abstract": "In this work, we introduce the Learnable Response Scoring Function (LARS) for Uncertainty Estimation (UE) in generative Large Language Models (LLMs). Current scoring functions for probability-based UE, such as length-normalized scoring and semantic contribution-based weighting, are designed to solve specific aspects of the problem but exhibit limitations, including the inability to handle biased probabilities and under-performance in low-resource languages like Turkish. To address these issues, we propose LARS, a scoring function that leverages supervised data to capture complex dependencies between tokens and probabilities, thereby producing more reliable and calibrated response scores in computing the uncertainty of generations. Our extensive experiments across multiple datasets show that LARS substantially outperforms existing scoring functions considering various probability-based UE methods",
    "checked": true,
    "id": "6c25859feca9aedba5a3fc83b9fede70764e2ed9",
    "semantic_title": "do not design, learn: a trainable scoring function for uncertainty estimation in generative llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P2J2C5c8e3": {
    "title": "Neuro-symbolic Training for Reasoning over Spatial Language",
    "volume": "review",
    "abstract": "Recent research shows that more data and larger models can provide more accurate solutions to natural language problems requiring reasoning. However, models can easily fail to provide solutions in unobserved complex input compositions due to not achieving the level of abstraction required for generalizability. To alleviate this issue, we propose training the language models with neuro-symbolic techniques that can exploit the logical rules of reasoning as constraints and provide additional supervision sources to the model. Training models to adhere to the regulations of reasoning pushes them to make more effective abstractions needed for generalizability and transfer learning. We focus on a challenging problem of spatial reasoning over text. Our results on various benchmarks using multiple language models confirm our hypothesis of effective domain transfer based on neuro-symbolic training",
    "checked": true,
    "id": "3bdcaeb736f65659385ffc06e4d3f04f35da74e0",
    "semantic_title": "neuro-symbolic training for reasoning over spatial language",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=APMnhbnk4B": {
    "title": "Improving Temporal Reasoning of Language Models via Recounted Narratives",
    "volume": "review",
    "abstract": "Reasoning about time and temporal relations is an integral aspect of human cognition, essential for perceiving the world and navigating our experiences. Though language models (LMs) have demonstrated impressive performance in many reasoning tasks, temporal reasoning remains challenging due to its intrinsic complexity. In this work, we first study an essential task of temporal reasoningâ€”temporal graph generation, to unveil LMs' inherent, global reasoning capabilities. We show that this task presents great challenges even for the most powerful large language models (LLMs), such as GPT-3.5/4. We also notice a significant performance gap by small LMs (< 10B) that lag behind LLMs by 50%. Next, we study how to close this gap with a budget constraint, e.g., not using model finetuning. We propose a new prompting technique tailored for temporal reasoning, GENSORT, that first converts the events set to a Python class, then prompts an LM to generate a temporally grounded narrative, guiding the final generation of a temporal graph. Extensive experiments showcase the efficacy of GENSORT in improving various metrics. Notably, GENSORT attains the highest F1 on the Schema-11 evaluation set, while securing an overall F1 on par with GPT-3.5. GENSORT also achieves the best structural similarity across the board, even compared with GPT-3.5/4",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ONJ1sfOqGV": {
    "title": "Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming",
    "volume": "review",
    "abstract": "This study evaluates the performance of Recurrent Neural Network (RNN) and Transformer models in replicating cross-language structural priming, a key indicator of abstract grammatical representations in human language processing. Focusing on Chinese-English priming, which involves two typologically distinct languages, we examine how these models handle the robust phenomenon of structural priming, where exposure to a particular sentence structure increases the likelihood of selecting a similar structure subsequently. Additionally, we use large language models (LLMs) to measure the crosslingual structural priming effect. Our findings indicate that Transformers outperform RNNs in generating primed sentence structures, challenging the conventional belief that human sentence processing primarily involves recurrent and immediate processing, and suggesting a role for cue-based retrieval mechanisms. In general, this work contributes to our understanding of how computational models may reflect human cognitive processes in multilingual contexts",
    "checked": true,
    "id": "c86ac17f7ec51f6771779a64128d9ef2c1398f64",
    "semantic_title": "modeling bilingual sentence processing: evaluating rnn and transformer architectures for cross-language structural priming",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jOddQtCjdQ": {
    "title": "Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective",
    "volume": "review",
    "abstract": "To better interpret the intrinsic mechanism of large language models (LLMs), recent studies focus on monosemanticity on its basic units. A monosemantic neuron is dedicated to a single and specific concept, which forms a one-to-one correlation between neurons and concepts. Despite extensive research in monosemanticity probing, it remains unclear whether monosemanticity is beneficial or harmful to model capacity. To explore this question, we revisit monosemanticity from the feature decorrelation perspective and advocate for its encouragement. We experimentally observe that the current conclusion by \\citet{wang2024learning}, which suggests that decreasing monosemanticity enhances model performance, does not hold when the model changes. Instead, we demonstrate that monosemanticity consistently exhibits a positive correlation with model capacity, in the preference alignment process. Consequently, we apply feature correlation as a proxy for monosemanticity and incorporate a feature decorrelation regularizer into the dynamic preference optimization process. The experiments show that our method not only enhances representation diversity and activation sparsity but also improves preference alignment performance",
    "checked": true,
    "id": "7ed992b818e4a0490f6b5ef9ac249da60531075d",
    "semantic_title": "encourage or inhibit monosemanticity? revisit monosemanticity from a feature decorrelation perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=syTkSPx0KU": {
    "title": "Transformers Can Model Human Hyperprediction in Buzzer Quiz",
    "volume": "review",
    "abstract": "Humans are thought to predict the next words during sentence comprehension, but under unique circumstances, they demonstrate an ability for longer coherent word sequence prediction. In this paper, we investigate whether Transformers can model such hyperprediction observed in humans during sentence processing, specifically in the context of Japanese buzzer quizzes. We conducted eye-tracking experiments where the participants read the first half of buzzer quiz questions and predicted the second half, while we modeled their reading time using the GPT-2. The results showed that the GPT-2 can partially capture human hyperprediction. When the language model was fine-tuned with quiz questions, the perplexity value decreased. Lower perplexity corresponded to higher psychometric predictive power; however, excessive data for fine-tuning led to a decrease in perplexity and the fine-tuned model exhibited a low psychometric predictive power. Overall, our findings suggest that a moderate amount of data is required for fine-tuning in order to model human hyperprediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J1JJ6qz9ZU": {
    "title": "LOCR: Location-Guided Transformer for Optical Character Recognition",
    "volume": "review",
    "abstract": "Academic documents are packed with texts, equations, tables, and figures, requiring comprehensive understanding for accurate Optical Character Recognition (OCR). While end-to-end OCR methods offer improved accuracy over layout-based approaches, they often grapple with significant repetition issues, especially with complex layouts in Out-Of-Domain (OOD) documents. To tackle this issue, we propose LOCR\\footnote{Source codes and datasets will be available under the MIT license upon publication}, a model that integrates location guiding into the transformer architecture during autoregression. We train the model on an original large-scale dataset comprising over 53M text-location pairs from 89K academic document pages, including bounding boxes for words, tables and mathematical symbols. LOCR adeptly handles various formatting elements and generates content in Markdown language. It outperforms all existing methods in our test set constructed from arXiv, as measured by edit distance, BLEU, METEOR and F-measure. LOCR also eliminates repetition in the arXiv dataset, and reduces repetition frequency in OOD documents, from 13.19\\% to 0.04\\% and from 8.10\\% to 0.11\\% for natural science and social science documents respectively. Additionally, LOCR features an interactive OCR mode, facilitating the generation of complex documents through a few location prompts from human",
    "checked": true,
    "id": "313c43a4c62fa8223fc8fa0b8b40f7a4e6f1dca8",
    "semantic_title": "locr: location-guided transformer for optical character recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u2IWQn4OUq": {
    "title": "Lessons from using PLMs for Human Cognitive Modeling",
    "volume": "review",
    "abstract": "Many studies show evidence for cognitive abilities in Pre-trained Language Models (PLMs). Researchers have evaluated the cognitive alignment of PLMs, i.e., their correspondence to adult performance across a range of cognitive domains. More recently, the focus has shifted to the developmental alignment of these models: identifying phases during training where improvements in model performance track improvements in children's thinking over development. However, challenges to this use are twofold: (1) PLMs have very different architectures than human minds and brains, and the data sets on which they are trained differ in many ways from the inputs children receive. (2) The \"outputs\" of PLMs are different from the behavioral measures that cognitive scientists collect in their experiments and evaluate their theories against. In this paper, we distill lessons learned from using PLMs for cognitive modeling and outline the pitfalls of attempting to use PLMs, not as engineering artifacts, but as cognitive science and developmental science models. We review assumptions used by researchers to map measures of PLM performance to measures of human performances and then, enumerate criteria for using PLMs as credible accounts of cognition and cognitive development",
    "checked": false,
    "id": "388ee63f59835917e13387d017500133c9817d04",
    "semantic_title": "pharmaco-fus in cognitive impairment: lessons from a preclinical model",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=yxAhdDI2yN": {
    "title": "SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have transformed machine learning but raised significant legal concerns due to their potential to produce text that infringes on copyrights, resulting in several high-profile lawsuits. The legal landscape is struggling to keep pace with these rapid advancements, with ongoing debates about whether generated text might plagiarize copyrighted materials. Current LLMs may infringe on copyrights or overly restrict non-copyrighted texts, leading to these challenges: (i) the need for a comprehensive evaluation benchmark to assess copyright compliance from multiple aspects; (ii) evaluating robustness against safeguard bypassing attacks; and (iii) developing effective defenses targeted against the generation of copyrighted text. To tackle these challenges, we introduce a curated dataset to evaluate methods, test attack strategies, and propose lightweight, real-time defenses to prevent the generation of copyrighted text, ensuring the safe and lawful use of LLMs. Our experiments demonstrate that current LLMs frequently output copyrighted text, and that jailbreaking attacks can significantly increase the volume of copyrighted output. Our proposed defense mechanisms significantly reduce the volume of copyrighted text generated by LLMs by effectively refusing malicious requests",
    "checked": true,
    "id": "53184f9f4122eb4b762e7dcbaf14b90a42aa0053",
    "semantic_title": "shield: evaluation and defense strategies for copyright compliance in llm text generation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=I1MrRb2K8l": {
    "title": "Understanding Translationese Effects in Multilingual Machine Translation",
    "volume": "review",
    "abstract": "This study explores the impact of translationese on multilingual machine translation (MT). Using a newly curated directed \"one-way\" parallel corpora from Global Voices (MSGV), featuring original texts in diverse languages and explicit anotation of actual translation directions, we evaluated the NLLB and TOWER models on MT tasks between English and five other languages. Our results reveal that translationese inputs are easier to translate into English but not out of English. Additionally, machine translations of translationese are lexically richer than those of original texts when translating into English. These findings suggest that multilingual MT systems experience different translationese effects compared to dedicated bilingual systems, underscoring the need for diverse test beds in MT evaluations. We contribute our dataset to enhance future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VFmjzSCZgI": {
    "title": "Language is Scary when Over-Analyzed: Unpacking Implied Misogynistic Reasoning with Argumentation Theory-Driven Prompts",
    "volume": "review",
    "abstract": "We propose misogyny detection as an Argumentative Reasoning task and we investigate the capacity of large language models (LLMs) to understand the implicit reasoning used to convey misogyny in both Italian and English. The central aim is to generate the missing reasoning link between a message and the implied meanings encoding the misogyny. Our study uses argumentation theory as a foundation to form a collection of prompts in both zero-shot and few-shot settings. These prompts integrate different techniques, including chain-of-thought reasoning and augmented knowledge. Our findings show that LLMs fall short on reasoning capabilities about misogynistic comments and that they mostly rely on their implicit knowledge derived from internalized common stereotypes about women to generate implied assumptions, rather than on inductive reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a81VIUxbgL": {
    "title": "Identifying Factual Inconsistencies in Summaries: Grounding Model Inference via Task Taxonomy",
    "volume": "review",
    "abstract": "Factual inconsistencies pose a significant hurdle for the faithful summarization by generative models. While a major direction to enhance inconsistency detection is to derive stronger Natural Language Inference (NLI) models, we propose an orthogonal aspect that underscores the importance of incorporating task-specific taxonomy into the inference. To this end, we consolidate key error types of inconsistent facts in summaries, and incorporate them to facilitate both the zero-shot and supervised paradigms of LLMs. Extensive experiments on ten datasets of five distinct domains suggest that, zero-shot LLM inference could benefit from the explicit solution space depicted by the error type taxonomy, and achieves state-of-the-art performance overall, surpassing specialized non-LLM baselines, as well as recent LLM baselines. We further distill models that fuse the taxonomy into parameters through our designed prompt completions and supervised training strategies, efficiently substituting state-of-the-art zero-shot inference with much larger LLMs",
    "checked": true,
    "id": "60be3610c60cbdb0f786e0ec2c6b0af635f61722",
    "semantic_title": "identifying factual inconsistencies in summaries: grounding model inference via task taxonomy",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=YP8QNMaAhq": {
    "title": "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?",
    "volume": "review",
    "abstract": "Although large language models (LLMs) have been largely successful in generating functionally correct programs, conditioning models to produce efficient solutions while ensuring correctness remains a challenge. Further, unreliability in benchmarking code efficiency is a hurdle across varying hardware specifications for popular interpreted languages such as Python. In this paper, we present ECCO, a reproducible benchmark for evaluating program efficiency via two paradigms: natural language (NL) based code generation and history-based code editing. On ECCO, we adapt and thoroughly investigate the three most promising existing LLM-based approaches: in-context learning, iterative refinement with execution or NL feedback, and fine-tuning conditioned on execution and editing history. While most methods degrade functional correctness and moderately increase program efficiency, we find that adding execution information often helps maintain functional correctness, and NL feedback enhances more on efficiency. We release our benchmark to support future work on LLM-based generation of efficient code",
    "checked": true,
    "id": "eb0db181546bd8493246d8f767b63d57ca4e686f",
    "semantic_title": "ecco: can we improve model-generated code efficiency without sacrificing functional correctness?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y6DKvWOTxB": {
    "title": "Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation",
    "volume": "review",
    "abstract": "Misinformation, defined as false or inaccurate information, can result in significant societal harm when it is spread with malicious or even unintentional intent. The rapid online information exchange necessitates advanced detection mechanisms to mitigate misinformation-induced harm. Existing research, however, has predominantly focused on the veracity of information, overlooking the legal implications and consequences of misinformation. In this work, we take a novel angle to consolidate the definition of misinformation detection using legal issues as a measurement of societal ramifications, aiming to bring interdisciplinary efforts to tackle misinformation and its consequence. We introduce a new task: Misinformation with Legal Consequence (MisLC), which leverages definitions from a wide range of legal domains covering 4 broader legal topics and 11 fine-grained legal issues, including hate speech, election laws, and privacy regulations. For this task, we advocate a two-step dataset curation approach that utilizes crowd-sourced checkworthiness and expert evaluations of misinformation. We provide insights about the MisLC task through empirical evidence, from the problem definition to experiments and expert involvement. While the latest large language models and retrieval-augmented generation are effective baselines for the task, we find they are still far from replicating expert performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MFIYTecQ9Z": {
    "title": "Analyzing the Capabilities of Large Language Models in Annotating Substance Use Behavior from Clinical Notes",
    "volume": "review",
    "abstract": "Large language models (LLMs) have been trialed to annotate complex medical information. In this paper, we explore the capabilities of LLMs in annotating the substance use behavior of patients from clinical notes. We used MIMIC-SBDH data, which is based on MIMIC-3 discharge summaries, and annotated alcohol use, tobacco use, and drug use behavior into five instances: Past, Present, Never, Unsure, and nan, using the Llama3 model. The model achieved high match scores for the Past category annotation, ranging from 83.26% to 90.62%. Overall, the model accurately predicted alcohol, drug, and tobacco behaviors with respective overall accuracies of 51.70%, 31.37%, and 72.62%. However, the model performed poorly in annotating the Unsure category, with match scores ranging from 2.25% to 3.47%. Our experimentation provides information regarding performance patterns and challenges with use of LLMs for annotating complex healthcare data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JuSCbH0Snw": {
    "title": "$\\texttt{ModSCAN}$: Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities",
    "volume": "review",
    "abstract": "Large vision-language models (LVLMs) have been rapidly developed and widely used in various fields, but the (potential) stereotypical bias in the model is largely unexplored. In this study, we present a pioneering measurement framework, $\\texttt{ModSCAN}$, to $\\underline{\\text{SCAN}}$ the stereotypical bias within LVLMs from both vision and language $\\underline{\\text{Mod}}$alities. $\\texttt{ModSCAN}$ examines stereotypical biases with respect to two typical stereotypical attributes (gender and race) across three kinds of scenarios: occupations, descriptors, and persona traits. Our findings suggest that 1) the currently popular LVLMs show significant stereotype biases, with CogVLM emerging as the most biased model; 2) these stereotypical biases may stem from the inherent biases in the training dataset and pre-trained models; 3) the utilization of specific prompt prefixes (from both vision and language modalities) performs well in reducing stereotypical biases. We believe our work can serve as the foundation for understanding and addressing stereotypical bias in LVLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KIMkNwkxHX": {
    "title": "DocEditAgent: Document Structure Editing Via Multimodal LLM Grounding",
    "volume": "review",
    "abstract": "Document structure editing involves manipulating localized textual, visual, and layout components in document images based on the user's requests. Past works have shown that multimodal grounding of user requests in the document image and identifying the accurate structural components and their associated attributes remain key challenges for this task. To address these, we introduce the DocEditAgent, a novel framework that performs end-to-end document editing by leveraging Large Multimodal Models (LMMs). It consists of three novel components -- (1) Doc2Command to simultaneously localize edit regions of interest (RoI) and disambiguate user edit requests into edit commands. (2) LLM-based Command Reformulation prompting to tailor edit commands originally intended for specialized software into edit instructions suitable for generalist LMMs. (3) Moreover, DocEditAgent processes these outputs via Large Multimodal Models like GPT-4V and Gemini, to parse the document layout, execute edits on grounded Region of Interest (RoI), and generate the edited document image. Extensive experiments on the DocEdit dataset show that DocEditAgent significantly outperforms strong baselines on edit command generation (2-33\\%), RoI bounding box detection (12-31\\%), and overall document editing (1-12\\%) tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UVYrFofLPk": {
    "title": "StateAct: State Tracking and Reasoning for Acting and Planning with Large Language Models",
    "volume": "review",
    "abstract": "Planning and acting to solve 'real' tasks using large language models (LLMs) in interactive environments has become a new frontier for AI methods. While recent advances allowed LLMs to interact with online tools, solve robotics tasks and many more, long range reasoning tasks remain a problem for LLMs. Existing methods to address this issue are very resource intensive and require additional data or human crafted rules, instead, we propose a simple method based on few-shot in-context-learning alone to enhance 'chain-of-thought' with state-tracking for planning and acting with LLMs. We show that our method establishes the new state-of-the-art on Alfworld for in-context-learning methods (+14\\% over the previous best in-context-learning method) and performs on par with methods that use additional training data and additional tools such as code-execution. We also demonstrate that our enhanced 'chain-of-states' allows the agent to both solve longer horizon problems and to be more efficient in number of steps required to solve a task. Finally, we also conduct ablation studies and show that `chain-of-thoughts' helps state-tracking accuracy, while a json-structure harms overall performance. We open-source our code and annotations at anonymous URL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hWxipcH3t9": {
    "title": "Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation",
    "volume": "review",
    "abstract": "Multimodal foundation models hold significant potential for automating radiology report generation, thereby assisting clinicians in diagnosing cardiac diseases. However, generated reports often suffer from serious factual inaccuracy. In this paper, we introduce a fact-aware multimodal retrieval-augmented pipeline in generating accurate radiology reports (FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then integrate factual knowledge to train a universal multimodal retriever. Given a radiology image, our retriever can identify high-quality reference reports to augment multimodal foundation models, thus enhancing the factual completeness and correctness of report generation. Experiments on two benchmark datasets demonstrate that our multimodal retriever significantly outperforms other state-of-the-art retrievers on both language generation and radiology-specific metrics, up to 6.5\\% and 2\\% score in F1CheXbert and F1RadGraph. Further analysis indicates that employing our factually-informed training strategy imposes an effective supervision signal, without relying on explicit diagnostic label guidance, and successfully propagate fact-aware capabilities from the multimodal retriever to the multimodal foundation model in radiology report generation",
    "checked": true,
    "id": "3f7933125681271776705090c9fce97a0e9180f6",
    "semantic_title": "fact-aware multimodal retrieval augmentation for accurate medical radiology report generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4MneeeHGuL": {
    "title": "To Err Is Human, but Llamas Can Learn It Too",
    "volume": "review",
    "abstract": "This study explores enhancing grammatical error correction (GEC) through automatic error generation (AEG) using language models (LMs). Specifically, we fine-tune Llama 2 LMs for error generation and find that this approach yields synthetic errors akin to human errors. Next, we train GEC Llama models using these artificial errors and outperform previous state-of-the-art error correction models, with gains ranging between 0.8 and 6 F0.5 points across all tested languages (German, Ukrainian, and Estonian). Moreover, we demonstrate that generating errors by fine-tuning smaller sequence-to-sequence models and prompting large commercial LMs (GPT3.5 and GPT4) also results in synthetic errors beneficially affecting error generation models. We openly release trained models for error generation and correction as well as all the synthesized error datasets for the covered languages",
    "checked": true,
    "id": "7ea2b0d6bab8e5a6ca0660c8737d7938cd3b711d",
    "semantic_title": "to err is human, but llamas can learn it too",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=h66QOiHyaZ": {
    "title": "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks",
    "volume": "review",
    "abstract": "We find that language models have difficulties generating fallacious and deceptive reasoning. When asked to generate deceptive outputs, language models tend to leak honest counterparts but believe them to be false. Exploiting this deficiency, we propose a jailbreak attack method that elicits an aligned language model for malicious output. Specifically, we query the model to generate a fallacious yet deceptively real procedure for the harmful behavior. Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism. Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones. We evaluate our approach over five safety-aligned large language models, comparing four previous jailbreak methods, and show that our approach achieves competitive performance with more harmful outputs. We believe the findings could be extended beyond model safety, such as self-verification and hallucination",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K1VIvCN4gz": {
    "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
    "volume": "review",
    "abstract": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hD04BoVVNR": {
    "title": "Evaluating Language Model Character Traits",
    "volume": "review",
    "abstract": "Language models (LMs) can exhibit human-like behaviour, but it is unclear how to describe this behaviour without undue anthropomorphism. We formalise a behaviourist view of LM character traits: qualities such as truthfulness, sycophancy, and coherent beliefs and intentions, which may manifest as consistent patterns of behaviour. Our theory is grounded in empirical demonstrations of LMs exhibiting different character traits, such as accurate and logically coherent beliefs and helpful and harmless intentions. We infer belief and intent from LM behaviour, finding their consistency varies with model size, fine-tuning, and prompting. In addition to characterising LM character traits, we evaluate how these traits develop over the course of an interaction. We find that traits such as truthfulness and harmfulness can be stationary, i.e., consistent over an interaction, in certain contexts but may be reflective in different contexts, meaning they mirror the LM's behaviour in the preceding interaction. Our formalism enables us to describe LM behaviour precisely and without undue anthropomorphism",
    "checked": false,
    "id": "ae03f10729959435ecefc0e90cba4cbe8438a10b",
    "semantic_title": "evaluating large language model biases in persona-steered generation",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=wo7FcGvM8r": {
    "title": "Evaluating Diversity in Automatic Poetry Generation",
    "volume": "review",
    "abstract": "Natural Language Generation (NLG), and more generally generative AI, are among the currently most impactful research fields. Creative NLG, such as automatic poetry generation, is a fascinating niche in this area. While most previous research has focused on forms of the Turing test when evaluating automatic poetry generation --- can humans distinguish between automatic and human generated poetry --- we evaluate the diversity of automatically generated poetry, by comparing distributions of generated poetry to distributions of human poetry along structural, lexical, semantic and stylistic dimensions, assessing different model types (word vs. character-level, general purpose LLMs vs. poetry-specific models), including the very recent LLaMA3, and types of fine-tuning (conditioned vs. unconditioned). We find that current automatic poetry systems are considerably underdiverse along multiple dimensions --- they often do not rhyme sufficiently, are semantically too uniform and even do not match the length distribution of human poetry. Among all models explored, character-level style-conditioned models perform slightly better. Our identified limitations may serve as the basis for more genuinely creative future poetry generation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q9kPg2ndg2": {
    "title": "ALLaM: A Series of Large Language Models for Arabic and English",
    "volume": "review",
    "abstract": "In this work, we present **ALLaM**: **A**rabic **L**arge **La**nguage **M**odel, a series of large language models to support the ecosystem of Arabic Language Technologies (ALT). \\model~ is carefully trained, considering the values of language alignment and transferability of knowledge at scale. The models are based on an autoregressive decoder-only architecture and are pretrained on a mixture of Arabic and English texts. We illustrate how the second-language acquisition via vocabulary expansion can help steer a language model towards a new language without any major catastrophic forgetting in English. Furthermore, we highlight the effectiveness of using translation data and the process of knowledge encoding within the language model's latent space. Finally, we show that effective alignment with human preferences can significantly enhance the performance of a large language model (LLM) compared to less aligned models of a larger scale. Our methodology enables us to achieve state-of-the-art performance in various Arabic benchmarks, including MMLU Arabic, ACVA, Arabic Exams. Our aligned models improve both in Arabic and English from its base aligned models. Our model is openly available via [Redacted]()",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hx4iYqZV5J": {
    "title": "Automated Clinical Data Extraction with Knowledge Conditioned LLMs",
    "volume": "review",
    "abstract": "The extraction of lung lesion information from clinical and medical imaging reports is crucial for research on and clinical care of lung-related diseases. Large language models (LLMs) can be effective at interpreting unstructured text in reports, but they often hallucinate due to a lack of domain-specific knowledge, leading to reduced accuracy and posing challenges for use in clinical settings. To address this, we propose a novel framework that aligns generated internal knowledge with external knowledge through in-context learning (ICL). Our framework employs a retriever to identify relevant units of internal or external knowledge and a grader to evaluate the truthfulness and helpfulness of the retrieved internal-knowledge rules to align and update the knowledge bases. Our knowledge-conditioned approach also improves the accuracy and reliability of LLM outputs by addressing the extraction task in two stages: (i) lung lesion finding detection and primary structured field parsing, followed by (ii) further parsing of lesion description text into additional structured fields. Extensive experiments with expert-curated test datasets demonstrate that this ICL approach can increase the F1 score for key fields (lesion size, margin and solidity) by an average of 12.9\\% over existing ICL methods",
    "checked": true,
    "id": "221f8e33faa0b1d4f2b2bd8f91f228767de82d32",
    "semantic_title": "automated clinical data extraction with knowledge conditioned llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dVTHEGwn3j": {
    "title": "Skills-in-Context: Unlocking Compositionality in Large Language Models",
    "volume": "review",
    "abstract": "We investigate how to elicit compositional generalization capabilities in large language models (LLMs). Compositional generalization empowers LLMs to solve complex problems by combining foundational skills, a critical reasoning ability akin to human intelligence. However, even the most advanced LLMs currently struggle with this form of reasoning. We examine this problem within the framework of in-context learning and find that demonstrating both foundational skills and compositional examples grounded in these skills within the same prompt context is crucial. We refer to this prompt structure as skills-in-context (SKiC). With as few as two exemplars, this in-context learning structure enables LLMs to tackle more challenging problems requiring innovative skill combinations, achieving near-perfect systematic generalization across a broad range of tasks. Intriguingly, SKiC also unlocks the latent potential of LLMs, allowing them to more actively utilize pre-existing internal skills acquired during earlier pretraining stages to solve complex reasoning problems. The SKiC structure is robust across different skill constructions and exemplar choices and demonstrates strong transferability to new tasks. Finally, inspired by our in-context learning study, we show that fine-tuning LLMs with SKiC-style data can elicit zero-shot weak-to-strong generalization, enabling the models to solve much harder problems directly with standard prompting",
    "checked": false,
    "id": "447bbdbeb5dfa9252b51a833eafe5e8f4d3b632e",
    "semantic_title": "skills-in-context prompting: unlocking compositionality in large language models",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=YeeUN3tj6s": {
    "title": "Attribute Diversity Determines the Systematicity Gap in VQA",
    "volume": "review",
    "abstract": "The degree to which neural networks can generalize to new combinations of familiar concepts, and the conditions under which they are able to do so, has long been an open question. In this work, we study the systematicity gap in visual question answering: the performance difference between reasoning on previously seen and unseen combinations of object attributes. To test, we introduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased quantity of training data does not reduce the systematicity gap, increased training data diversity of the attributes in the unseen combination does. In all, our experiments suggest that the more distinct attribute type combinations are seen during training, the more systematic we can expect the resulting model to be",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ziKFVuk373": {
    "title": "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics",
    "volume": "review",
    "abstract": "Automatic metrics are used as proxies to evaluate abstractive summarization systems when human annotations are too expensive. To be useful, these metrics should be fine-grained, show a high correlation with human annotations, and ideally be independant of reference quality; however, most standard evaluation metrics for summarization are reference-based, and existing reference-free metrics correlates poorly with relevance, especially on summaries of longer documents. In this paper, we introduce a reference-free metric that correlates well with human evaluated relevance, while being very cheap to compute. We show that this metric can also be used along reference-based metrics to improve their robustness in low quality reference settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CjmPCi8Gfg": {
    "title": "Leveraging Parameter-Efficient Transfer Learning for Multi-Lingual Text-to-Speech Adaptation",
    "volume": "review",
    "abstract": "Different languages have distinct phonetic systems and vary in their prosodic features making it challenging to develop a Text-to-Speech (TTS) model that can effectively synthesise speech in multilingual settings. Furthermore, TTS architecture needs to be both efficient enough to capture nuances in multiple languages and efficient enough to be practical for deployment. The standard approach is to build transformer based model such as SpeechT5 and train it on large multilingual dataset. As the size of these models grow the conventional fine-tuning for adapting these model becomes impractical due to heavy computational cost. In this paper, we proposes to integrate parameter-efficient transfer learning (PETL) methods such as adapters and hypernetwork with TTS architecture for multilingual speech synthesis. Notably, in our experiments PETL methods able to achieve comparable or even better performance compared to full fine-tuning with only $\\sim$2.5\\% tunable parameters\\footnote{The code and samples are available at:~\\url{ https://anonymous.4open.science/r/multilingualTTS-BA4C}}",
    "checked": true,
    "id": "0c3eeb9fb1f1e848125bd33f2c9f7e3c361094a3",
    "semantic_title": "leveraging parameter-efficient transfer learning for multi-lingual text-to-speech adaptation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sOrUfhfo8n": {
    "title": "Triple Preference Optimization: Achieving Better Alignment with Less Data in a Single Step Optimization",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) perform well across diverse tasks, but aligning them with human demonstrations is challenging. Recently, Reinforcement Learning (RL)-free methods like Direct Preference Optimization (DPO) have emerged, offering improved stability and scalability while retaining competitive performance relative to RL-based methods. However, while RL-free methods deliver satisfactory performance, they require significant data to develop a robust Supervised Fine-Tuned (SFT) model} and an additional step to fine-tune this model on a preference dataset, which constrains their utility and scalability. In this paper, we introduce Triple Preference Optimization (TPO), a new preference learning method designed to align an LLM with three preferences without requiring a separate SFT step and using considerably less data. Through a combination of practical experiments and theoretical analysis, we show the efficacy of TPO as a single-step alignment strategy. Specifically, we fine-tuned the Phi-2 (2.7B) and Mistral (7B) models using TPO directly on the UltraFeedback dataset, achieving superior results compared to models aligned through other methods such as SFT, DPO, KTO, IPO, CPO, and ORPO. Moreover, the performance of TPO without the SFT component led to notable improvements in the MT-Bench score, with increases of +1.27 and +0.63 over SFT and DPO, respectively. Additionally, TPO showed higher average accuracy, surpassing DPO and SFT by 4.2% and 4.97% on the Open LLM Leaderboard benchmarks",
    "checked": true,
    "id": "cf21fdf499af19abed5efbdf1613b007ac9eef4a",
    "semantic_title": "triple preference optimization: achieving better alignment with less data in a single step optimization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=i5804xg6cE": {
    "title": "Many-Turn Jailbreaking",
    "volume": "review",
    "abstract": "Current jailbreaking work on large language models (LLMs) aims to elicit unsafe outputs from given prompts. However, it only focuses on single-turn jailbreaking, given one specific query. On the contrary, the advanced LLMs are designed to handle extremely long contexts and can thus conduct multi-turn conversations. So, we propose exploring multi-turn jailbreaking, in which the jailbroken LLMs are continuously tested more than the initial conversation turn or a single target query. This is an even more serious challenge because 1) it is very common for users to continue asking relevant follow-up questions to clarify certain details, and 2) It is also possible that the initial round of jailbreaking causes the LLMs to consistently respond to additional irrelevant questions, rather than focusing on adversarially targeting each new query. %This approach significantly reduces the cost of jailbreaking. As the first step in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak Benchmark (**MTJ-Bench**) for benchmarking this setting and provide novel insights into this new safety threat. By revealing this new vulnerability, we aim to call for community efforts to build safer LLMs and pave the way for a more in-depth understanding of jailbreaking LLMs",
    "checked": false,
    "id": "af5633c94ed2beb282f6a53c595eb437e8e7b630",
    "semantic_title": "many-shot jailbreaking",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=VGNQEZfxKJ": {
    "title": "Direct-Inverse Prompting: Analyzing LLMs' Discriminative Capacity in Self-Improving Generation",
    "volume": "review",
    "abstract": "Mainstream LLM research has primarily focused on enhancing their generative capabilities. However, even the most advanced LLMs experience uncertainty in their outputs, often producing varied results on different runs or when faced with minor changes in input, despite no substantial change in content. Given multiple responses from the same LLM to the same input, we advocate leveraging the LLMs' discriminative capability to reduce this generative uncertainty, aiding in identifying the correct answers. Specifically, we propose and analyze three discriminative prompts: \\direct, \\inverse, and \\hybrid, to explore the potential of both closed-source and open-source LLMs in self-improving their generative performance on two benchmark datasets. Our insights reveal which discriminative prompt is most promising and when to use it. To our knowledge, this is the first work to systematically analyze LLMs' discriminative capacity to address generative uncertainty",
    "checked": true,
    "id": "22e5bfd5877aa496ef1eb77afc2b0905fdd79c48",
    "semantic_title": "direct-inverse prompting: analyzing llms' discriminative capacity in self-improving generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VWPanc8u1g": {
    "title": "CliniDial: A Naturally Emerged Multimodal Dialogue Dataset for Team Reflection During Clinical Operation",
    "volume": "review",
    "abstract": "In clinical operations, teamwork can be the crucial factor that determines the final outcome. Prior studies have shown that there can be 58\\% more deaths than expected due to insufficient collaboration. To understand how the team practices teamwork during the operation, we collected **CliniDial** from simulations of medical operations. **CliniDial** includes the audio data and its transcriptions, the simulated physiology signals of the patient manikins, and how the team operates from two camera angles. We annotated behavior codes following an existing framework to understand the teamwork process. Experimental results show that **CliniDial** poses significant challenges to the existing models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qt7yYNYiHL": {
    "title": "What's the most important value? INVP: INvestigating the Value Priorities of LLMs through Decision-making in Social Scenarios",
    "volume": "review",
    "abstract": "As large language models (LLMs) demonstrate impressive performance in various tasks and are increasingly integrated into the decision-making process, ensuring they align with human values has become crucial. This paper highlights that value priorities(the relative importance of different value) play a pivotal character in the decision-making process. To explore the value priorities in LLMs, this paper introduces INVP, a framework for INvestigating Value Priorities through decision-making in social scenarios. The framework encompasses social scenarios including binary decision-making, covering both individual and collective decision-making contexts, and is based on Schwartz's value theory for constructing value priorities. Based on this framework, we construct a dataset, which contains a total of 1613 scenarios and 3226 decisions across 283 topics. We evaluated seven popular LLMs and the experimental results reveal commonalities in the value priorities across different LLMs, such as an emphasis on Universalism and Benevolence, while Power and Hedonism are typically given lower priority. This study offers new perspectives on understanding and improving the moral and value alignment of LLMs in making complex social decisions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JixLyKI26g": {
    "title": "Data, Data Everywhere: A Guide for Pretraining Dataset Construction",
    "volume": "review",
    "abstract": "The impressive capabilities of recent language models can be largely attributed to the multi-trillion token pretraining datasets that they are trained on. However, model developers fail to disclose their construction methodology which has lead to a lack of open information on how to develop effective pretraining sets. To address this issue, we perform the first systematic study across the entire pipeline of pretraining set construction. First, we run ablations on existing techniques for pretraining set development to identify which methods translate to the largest gains in model accuracy on downstream evaluations. Then, we categorize the most widely used data source, web crawl snapshots, across the attributes of toxicity, quality, type of speech, and domain. Finally, we show how such attribute information can be used to further refine and improve the quality of a pretraining set. These findings constitute an actionable set of steps that practitioners can use to develop high quality pretraining sets",
    "checked": true,
    "id": "9697bc15d52b2a8c38baed62f38726a04c4a2f77",
    "semantic_title": "data, data everywhere: a guide for pretraining dataset construction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aBeDyLkDyq": {
    "title": "Explanations explained. Influence of free-text explanations on LLMs and the role of implicit knowledge",
    "volume": "review",
    "abstract": "Despite their remarkable performance, LLMs' ability to provide transparent and faithful explanations for their predictions remains a challenge. We investigate the influence of different types of natural language explanations on LLM predictions, focusing on four different datasets presenting tasks that involve leveraging implicit knowledge. We conduct experiments on three SOTA LLMs on 8 types of explanations, both written by humans or machine-generated, through three generation methods: label-agnostic, label-aware, and counterfactual (label-contradicting) explanation generation. Our results consistently demonstrate that providing explanations significantly improves the accuracy of LLM predictions, even when the models are not explicitly trained to generate explanations, and propose a method to study the relationship between implicitness and explanation effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ie7bTfSllb": {
    "title": "Situational Evaluation for Social Intelligence of Large Language Models",
    "volume": "review",
    "abstract": "The academic intelligence of large language models (LLMs) has made remarkable progress in recent times, but their social intelligence performance remains unclear. Inspired by established human social intelligence frameworks, particularly Daniel Goleman's social intelligence theory, we have developed a standardized social intelligence test based on real-world social scenarios to comprehensively assess the social intelligence of LLMs, termed as the Situational Evaluation for Social Intelligence (SESI). We conducted an extensive evaluation with 13 popular and state-of-art LLMs on SESI. The results indicate the social intelligence of LLMs still has significant room for improvement, with superficially friendliness as a primary reason for errors. Moreover, there exists a relatively low correlation between the social intelligence and academic intelligence exhibited by LLMs, suggesting that social intelligence is distinct from academic intelligence for LLMs. Additionally, while it is observed that LLMs can't ``understand'' what social intelligence is, their social intelligence, similar to that of humans, is influenced by social factors",
    "checked": false,
    "id": "04a343ffba0120ac02e3138ffd6d7b85f753846b",
    "semantic_title": "playing the werewolf game with artificial intelligence for language understanding",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=nyGYCmtZqb": {
    "title": "Unlocking Continual Learning Abilities in Language Models",
    "volume": "review",
    "abstract": "Language models (LMs) exhibit impressive performance and generalization capabilities. However, LMs struggle with the persistent challenge of catastrophic forgetting, which undermines their long-term sustainability in continual learning (CL). Existing approaches usually address the issue by incorporating old task data or task-wise inductive bias into LMs. However, old data and accurate task information are often unavailable or costly to collect, hindering the availability of current CL approaches for LMs. To address this limitation, we introduce ``MIGU''($\\textbf{M}$agn$\\textbf{I}$tude-based $\\textbf{G}$radient $\\textbf{U}$pdating for continual learning), a rehearsal-free and task-label-free method that only updates the model parameters with large magnitudes of output in LMs' linear layers. MIGU is based on our observation that the normalized magnitude distribution of the output in LMs' linear layers is different when the LM models deal with different task data. By imposing this simple constraint on the gradient update process, we can leverage the inherent behaviors of LMs, thereby unlocking their innate CL abilities. Our experiments demonstrate that MIGU is universally applicable to all three LM architectures (T5, RoBERTa, and Llama2), delivering state-of-the-art or on-par performance across continual finetuning and continual pre-training settings on four CL benchmarks. For example, MIGU brings a 15.2\\% average accuracy improvement over conventional parameter-efficient finetuning baselines in a 15-task CL benchmark. MIGU can also seamlessly integrate with all three existing CL types to further enhance performance. We include the code in the submission attachment",
    "checked": true,
    "id": "f00344d9b790411232de8ff2ca06af909e92f1b7",
    "semantic_title": "unlocking continual learning abilities in language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZSfjY9piYg": {
    "title": "PatentEdits: A Patent Dataset Built for Predicting Revisions",
    "volume": "review",
    "abstract": "Patents are critical protections of a company's intellectual property and competitive advantage, as they grant inventors the exclusive rights to make, use and sell the disclosed invention for 20 years. In order to be granted, a patent must be deemed novel and non-obvious by the US Patent Office (USPTO). To meet this criteria, most patent agents and inventors will revise the language and scope of the claimed invention after official feedback is received. To better understand what revisions lead to successful patents, we present the PatentEdits dataset, which contains 483,706 granted patent examples and is the first to align them before and after revision. We define and extract the following sentence or claim level edit actions in our dataset: a given draft claim is either kept, merged, edited, or deleted. For each patent we also include the USPTO examiner cited references, which can be used in edit action prediction. We also demonstrate the promise of the following model pipeline for predicting the entire granted patent: 1) the prediction of edit actions on the draft claims followed by 2) the prediction of the revised claims with the edit actions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nw42MFuiKi": {
    "title": "Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models",
    "volume": "review",
    "abstract": "Weight-based model editing methods update the parametric knowledge of language models post-training. However, these methods can unintentionally alter unrelated parametric knowledge representations, potentially increasing the risk of harm. In this work, we investigate how weight editing methods unexpectedly amplify model biases after edits. We introduce a novel benchmark dataset, Seesaw-CF, for measuring bias amplification of model editing methods for demographic traits such as race, geographic origin, and gender. We use Seesaw-CF to examine the impact of model editing on bias in five large language models. Our results demonstrate that edited models exhibit, to various degrees, more biased behavior for certain demographic groups than before they were edited, specifically becoming less confident in properties for Asian and African subjects. Additionally, editing facts about place of birth, country of citizenship, or gender has particularly negative effects on the model's knowledge about unrelated properties, such as field of work, a pattern observed across multiple models",
    "checked": true,
    "id": "161c6d3851b73fc844c49a9921cfc6bb6da6904b",
    "semantic_title": "flex tape can't fix that\": bias and misinformation in edited language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pTbU2eKEPh": {
    "title": "MetaKP: On-Demand Keyphrase Generation",
    "volume": "review",
    "abstract": "Traditional keyphrase prediction methods predict a single set of keyphrases per document, failing to cater to the diverse needs of users and downstream applications. To bridge the gap, we introduce on-demand keyphrase generation, a novel paradigm that requires keyphrases that conform to specific high-level goals or intents. For this task, we present MetaKP, a large-scale benchmark comprising four datasets, 7500 documents, and 3760 goals across news and biomedical domains with human-annotated keyphrases. Leveraging MetaKP, we design both supervised and unsupervised methods, including a multi-task fine-tuning approach and a self-consistency prompting method with large language models. The results highlight the challenges of supervised fine-tuning, whose performance is not robust to distribution shifts. By contrast, the proposed self-consistency prompting approach greatly improves the performance of large language models, enabling GPT-4o to achieve 0.548 SemF1, surpassing the performance of a fully fine-tuned BART-base model. Finally, we demonstrate the potential of our method to serve as a general NLP infrastructure, exemplified by its application in epidemic event detection from social media",
    "checked": true,
    "id": "83fad9edb0549c3bc912973abad2e9e98a2dedcd",
    "semantic_title": "metakp: on-demand keyphrase generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XgDfgnelgD": {
    "title": "Aligners: Decoupling LLMs and Alignment",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training *aligner* models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We use the same synthetic data to train *inspectors*, binary miss-alignment classification models to guide a *squad* of multiple aligners. Our empirical results demonstrate consistent improvements when applying aligner squad to various LLMs, including chat-aligned models, across several instruction-following and red-teaming datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mfgcxMm5aa": {
    "title": "One-to-Many Communication and Compositionality in Emergent Communication",
    "volume": "review",
    "abstract": "Compositional languages leverage rules that derive meaning from combinations of simpler constituents. This property is considered to be the hallmark of human language as it enables the ability to express novel concepts and ease of learning. As such, numerous studies in the emergent communication field explore the prerequisite conditions for emergence of compositionality. Most of these studies set out one-to-one communication environment wherein a speaker interacts with one listener during a single round of communication game. However, real-world communications often involve multiple listeners; their interests may vary and they may even need to coordinate among themselves to be successful at a given task. This work investigates the effects of one-to-many communication environment on emergent languages where a single speaker broadcasts its message to multiple listeners to cooperatively solve a task. We observe that simply broadcasting the speaker's message to multiple listeners does not induce more compositional languages. We then analyze two axes of environmental pressures that facilitate emergence of compositionality: listeners of *different interests* and *coordination* among listeners",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LepnjkXd08": {
    "title": "Carrot and Stick: Inducing Self-Motivation with Positive & Negative Feedback",
    "volume": "review",
    "abstract": "Positive thinking is thought to be an important component of self-motivation in various practical fields such as education and the workplace. Previous work, including sentiment transfer and positive reframing, has focused on the positive side of language. However, self-motivation that drives people to reach their goals has not yet been studied from a computational perspective. Moreover, negative feedback has not yet been explored, even though positive and negative feedback are both necessary to grow self-motivation. To facilitate self-motivation, we propose CArrot and STICk (CASTIC) dataset, consisting of 12,590 sentences with 5 different strategies for enhancing self-motivation. Our data and code are publicly available at here",
    "checked": true,
    "id": "a5beb1e0b0b1a99ccce6edf09fc143d422d05ae1",
    "semantic_title": "carrot and stick: inducing self-motivation with positive & negative feedback",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P0XxzFvTdJ": {
    "title": "EEG2Text: Open Vocabulary EEG-to-Text Decoding with EEG Pre-Training and Multi-View Transformer",
    "volume": "review",
    "abstract": "Deciphering the intricacies of the human brain has captivated curiosity for centuries. Recent strides in Brain-Computer Interface (BCI) technology, particularly using motor imagery, have restored motor functions such as reaching, grasping, and walking in paralyzed individuals. However, unraveling natural language from brain signals remains a formidable challenge. Electroencephalography (EEG) is a non-invasive technique used to record electrical activity in the brain by placing electrodes on the scalp. Previous studies of EEG-to-text decoding have achieved high accuracy on small closed vocabularies, but still fall short of high accuracy when dealing with large open vocabularies. We propose a novel method, EEG2Text, to improve the accuracy of open vocabulary EEG-to-text decoding. Specifically, EEG2Text leverages EEG pre-training to enhance the learning of semantics from EEG signals and proposes a multi-view transformer to model the EEG signal processing by different spatial regions of the brain. Experiments show that EEG2Text has superior performance, outperforming the state-of-the-art baseline methods by a large margin of up to 5% in absolute BLEU and ROUGE scores. EEG2Text shows great potential for a high-performance open-vocabulary brain-to-text system to facilitate communication",
    "checked": true,
    "id": "76bd9c77319acb80f51e0f222cc2505961f8d83e",
    "semantic_title": "eeg2text: open vocabulary eeg-to-text decoding with eeg pre-training and multi-view transformer",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=27BrXMjq0h": {
    "title": "GE2PE: Persian End-to-End Grapheme-to-Phoneme Conversion",
    "volume": "review",
    "abstract": "Text-to-Speech (TTS) systems have made significant strides, enabling the generation of speech from grapheme sequences. However, for low-resource languages, these models still struggle to produce natural and intelligible speech. Grapheme-to-Phoneme conversion (G2P) addresses this challenge by enhancing the input sequence with phonetic information. Despite these advancements, existing G2P systems face limitations when dealing with Persian texts due to the complexity of Persian transcription. In this study, we focus on enriching resources for the Persian language. To achieve this, we introduce two novel G2P training datasets: one manually labeled and the other machine-generated. These datasets comprise over five million sentences alongside their corresponding phoneme sequences. Additionally, we propose two evaluation datasets tailored for Persian sub-tasks, including Kasre-Ezafe detection, homograph disambiguation, and handling out-of-vocabulary (OOV) words. To tackle the unique challenges of the Persian language, we develop a new sentence-level End-to-End (E2E) model leveraging a two-step training approach, as outlined in our paper, to maximize the impact of manually labeled data. The results show that our model surpasses the state-of-the-art performance by 1.86\\% in word error rate, 4.03\\% in Kasre-Ezafe detection recall, and 3.42\\% in homograph disambiguation accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qd3tzSnUN6": {
    "title": "Estimating Knowledge in Large Language Models Without Generating a Single Token",
    "volume": "review",
    "abstract": "To evaluate knowledge in large language models (LLMs), current methods query the model and then evaluate its generated responses. In this work, we ask whether evaluation can be done $\\textit{before}$ the model has generated any text. Concretely, is it possible to estimate how knowledgeable a model is about a certain entity, only from its internal computation? We study this question with two tasks: given a subject entity, the goal is to predict (a) the ability of the model to answer common questions about the entity, and (b) the factuality of responses generated by the model about the entity. Experiments with a variety of LLMs show that KEEN, a simple probe trained over internal subject representations, succeeds at both tasks --- strongly correlating with both the QA accuracy of the model per-subject and FActScore, a recent factuality metric in open-ended generation. Moreover, KEEN naturally aligns with the model's hedging behavior and faithfully reflects changes in the model's knowledge after fine-tuning. Lastly, we show a more interpretable yet equally performant variant of KEEN, which highlights a small set of tokens that correlates with the model's lack of knowledge. Being simple and lightweight, KEEN can be leveraged to identify gaps and clusters of entity knowledge in LLMs, and guide decisions such as augmenting queries with retrieval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJ2GJm6tvr": {
    "title": "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown exceptional results on current benchmarks when working individually. The advancement in their capabilities, along with a reduction in parameter size and inference times, has facilitated the use of these models as agents, enabling interactions among multiple models to execute complex tasks. Such collaborations offer several advantages, including the use of specialized models (e.g. coding), improved confidence through multiple computations, and enhanced divergent thinking, leading to more diverse outputs. Thus, the collaborative use of language models is expected to grow significantly in the coming years. In this work, we evaluate the behavior of a network of models collaborating through debate under the influence of an adversary. We introduce pertinent metrics to assess the adversary's effectiveness, focusing on system accuracy and model agreement. Our findings highlight the importance of a model's persuasive ability in influencing others. Additionally, we explore inference-time methods to generate more compelling arguments and evaluate the potential of prompt-based mitigation as a defensive strategy",
    "checked": true,
    "id": "885bc1bf9dbe546ae9aaca7000b595688eda8d96",
    "semantic_title": "multiagent collaboration attack: investigating adversarial attacks in large language model collaborations via debate",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=uue5KxPxY2": {
    "title": "A Unified Framework for Model Editing",
    "volume": "review",
    "abstract": "ROME and MEMIT are largely believed to be two different model editing algorithms, with the major difference between them being the ability to perform batched edits. In this paper, we unify these two algorithms under a single conceptual umbrella, optimizing for the same goal, which we call the preservation-memorization objective. ROME uses an equality constraint to optimize this objective to perform one edit at a time, whereas MEMIT employs a more flexible least-square constraint that allows for batched edits. We generalize ROME and enable batched editing with equality constraint in the form of EMMET - an Equality-constrained Mass Model Editing algorithm for Transformers, a new batched memory-editing algorithm. EMMET can perform batched-edits up to a batch-size of 10,000, with very similar performance to MEMIT across multiple dimensions. With the introduction of EMMET, we truly unify ROME and MEMIT and show that both algorithms are equivalent in terms of their optimization objective, their abilities (singular and batched editing), their model editing performance and their limitations",
    "checked": true,
    "id": "65d9e725278da08fa29144b4f9a80d7d31f1c27f",
    "semantic_title": "a unified framework for model editing",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=t3RXpmeITf": {
    "title": "MANDALA: Multi-Agent Network for Backdoor Detection using AST Parsing and Large Language Models",
    "volume": "review",
    "abstract": "This paper presents MANDALA , a system that leverages locally deployed open-source large language models (LLMs) and multi-agent networks to enhance vulnerability detection in code bases. MANDALA uses an abstract syntax tree-based algorithm to parse code into digestible chunks for LLMs, generating code explanations and descriptions. A collaborative multi-agent network comprised of specialized agents such as static analysis, security management, and user interaction agents then coordinate to analyze the codebase for potential backdoors and vulnerabilities. Evaluations on open-source codebases demonstrate MANDALA's ability to significantly reduce manual effort while increasing detection speed and accuracy over traditional methods across various test cases. MANDALA represents an innovative integration of LLMs and multi-agent systems for efficient, scalable code vulnerability detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cA06EOIC3P": {
    "title": "Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language Models",
    "volume": "review",
    "abstract": "As language models have scaled both their number of parameters and pretraining dataset sizes, the computational cost for pretraining has become intractable except for the most well-resourced teams. This increasing cost makes it ever more important to be able to reuse a model after it has completed pretraining; allowing for a model's abilities to further improve without needing to train from scratch. In this work, we detail a set of guidelines that cover how to design efficacious data distributions and learning rate schedules for continued pretraining of language models. When applying these findings within a continued pretraining run on top of a well-trained 15B parameter model, we show an improvement of 9\\% in average model accuracy compared to the baseline of continued training on the pretraining set. The resulting recipe provides a practical starting point with which to begin developing language models through reuse rather than retraining",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hjSHtAEm0j": {
    "title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models with Leaky Visual Conversations",
    "volume": "review",
    "abstract": "Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved image-text web documents. Our approach transforms 45K web documents from the OBELICS dataset into 100K image conversation samples. We utilize GPT-4 to generate image-contextual captions and OpenChat 3.5 model to convert these captions into diverse free-form and multiple-choice question-answer pairs. Integrating this dataset for fine-tuning considerably enhances VLM performance across multiple benchmarks. Unlike methods that focus solely on fine-grained visual content, our approach leverages accompanying web context, yielding superior results. We also discover that a 'leaky modality mix,' where conversation samples contain questions answerable from both the image and its contextual caption, outperforms non-leaky combinations of captions and Q\\&A pairs. Our dataset shows strong performance with two popular VLM approaches: text-only large language model (LLM) aligned with a vision encoder using image captions data (ShareGPT4V-7b) and multimodally pretrained LLM (IDEFICS2-8b) using interleaved image-text data. In addition to releasing the VisCon-100K dataset, we provide a contextual captioner trained on this dataset, facilitating scalable fine-tuning data generation for future research and open-source applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tuAmmObUvK": {
    "title": "DistiLRR: Transferring Code Repair for Low-Resource Programming Languages",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown remarkable performance on code generation tasks. A recent application of LLMs for code generation is iterative code repair, where a model fixes an incorrect program by rationalizing about errors and generating new code. However, code repair is primarily studied on high-resource languages like Python, and the framework's efficacy is under-explored on low-resource languages. To study code repair for low-resource languages, we propose Distilling Low-Resource Repairs (DistiLRR), an approach that transfers the reasoning and code generation ability from a teacher model to a student model. Our results show that DistiLRR consistently outperforms baselines on low-resource languages, but has similar performance on high-resource languages. To investigate this behavior, we perform a further analysis and find that the correlation between rationale quality and code correctness is weaker than previously perceived. We hypothesize this weakness is magnified in low-resource settings where base models lack deep knowledge of a programming language, leading to wavering benefits of code repair between high-resource and low-resource languages",
    "checked": true,
    "id": "ac3ac7c3d66e5a1dd1bfdc53435b3d12780538fb",
    "semantic_title": "distilrr: transferring code repair for low-resource programming languages",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rrIkAMZEPS": {
    "title": "Rethinking Text Generation Evaluation: A Unified Evaluation Theory for Reflective and Open-Ended Generation Tasks",
    "volume": "review",
    "abstract": "With increased accessibility of machine generated texts, the need for their evaluation has also increased. There are two types of text generation task for which evaluation is required. In open-ended generation tasks (OGT), the model generates de novo text without any input on which to base it. Examples include story generation. In reflective generation tasks (RGT), the model output is generated to reflect an input sequence. An example is machine translation. Evaluation of RGTs is well-researched, and typically uses metrics that compare one or more gold-standard references to the model output. Evaluation of OGTs is less well-researched, and reference-based evaluations are more challenging: as the task is not seeking to reflect an input, there are usually no references. In this paper, we propose a theory of evaluation that covers both RGT and OGT evaluation. Based on this theory, we propose an output-oriented reference generation method for OGTs, develop an automatic language quality evaluation method for OGTs, and review previous literature from this new perspective. Our experiments demonstrate the effectiveness of these methods across informal texts, formal texts, and domain-specific texts. We conduct a meta-evaluation to compare existing and proposed metrics, finding that our approach better aligns with human judgement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WhGw4tm9cZ": {
    "title": "The Role of Language Imbalance in Cross-lingual Generalisation: Insights from Cloned Language Experiments",
    "volume": "review",
    "abstract": "Multilinguality is crucial for extending recent advancements in language modelling to diverse linguistic communities. To maintain high performance while representing multiple languages, multilingual models ideally align representations, allowing what is learned in one language to generalise to others. Prior research has emphasised the importance of parallel data and shared vocabulary elements as key factors for such alignment. In this study, we investigate an unintuitive novel driver of cross-lingual generalisa- tion: language imbalance. In controlled experiments on perfectly equivalent cloned languages, we observe that the existence of a predominant language during training boosts the performance of less frequent languages and leads to stronger alignment of model representations across languages. Further- more, we find that this trend is amplified with scale: with large enough mod- els or long enough training, we observe that bilingual training data with a 90/10 language split yields strictly better performance on both languages than a balanced 50/50 split. Building on these insights, we design training schemes that can improve performance in all cloned languages, even without alter- ing the training data. As we extend our analysis to real languages, we find that infrequent languages still benefit from frequent ones, yet whether language imbalance causes cross-lingual generalisation there is not conclusive",
    "checked": true,
    "id": "723cbf9206218ad188e1211668dfb8b981038954",
    "semantic_title": "the role of language imbalance in cross-lingual generalisation: insights from cloned language experiments",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=NXYVm3AjG2": {
    "title": "TabMeta: Table Metadata Generation with LLM-Curated Dataset and LLM-Judges",
    "volume": "review",
    "abstract": "Recent advances in LLMs have found use in several tabular related tasks including Text2SQL, data wrangling, imputation, Q&A, and other table-related tasks. Crucially however, researchers have often overlooked the fact that the downstream data consumers are often decoupled from the data producers. Downstream data users therefore, neither precisely know which tables to request access for and make use of, nor can easily understand complex cryptic terminology (in column names, etc) employed by the data producers. Specifically, the lack of descriptive metadata for tables has emerged as a significant obstacle to effective data governance and utilization. To tackle this, our work introduces TabMeta, a new natural language task aimed at automatically generating comprehensive metadata for arbitrarily complex tables, enabling non-expert users to discover, understand and use relevant data more effectively. First, we curate a unique benchmark dataset for the TabMeta task, consisting of table descriptions and column descriptions for 302 tables spanning 30 industry domains. Second, we propose two novel tabular metadata evaluation strategies (a) a robust and consistent LLM-Judge based framework which aligns with human judgement and employs confidence scores suited for tabular metadata and (b) ML based metrics to capture quality of the generated metadata such as conciseness, coherence and information gain. Finally, we also show that our metadata enhancement framework substantially improves the performance of tabular data discovery and search by a factor of 3-4x",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kgFXSF1Jra": {
    "title": "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models",
    "volume": "review",
    "abstract": "Large pre-trained models (LPMs), such as large language models, have become ubiquitous and are employed in many applications. These models are often adapted to a desired domain or downstream task through a fine-tuning stage. This paper proposes SQFT, an end-to-end solution for low-precision sparse parameter-efficient fine-tuning of LPMs, allowing for effective model manipulation in resource-constrained environments. Additionally, an innovative strategy enables the merging of sparse weights with low-rank adapters without losing sparsity and accuracy, overcoming the limitations of previous approaches. SQFT also addresses the challenge of having quantized weights and adapters with different numerical precisions, enabling merging in the desired numerical format without sacrificing accuracy. Multiple adaptation scenarios, models, and comprehensive sparsity levels demonstrate the effectiveness of SQFT",
    "checked": false,
    "id": "b14a92f444ba3c0fb228f9ccb04a515dd4d7f963",
    "semantic_title": "using global land cover product as prompt for cropland mapping via visual foundation model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A9miw3n6A5": {
    "title": "Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments",
    "volume": "review",
    "abstract": "Recent advancements in Large Language Models (LLMs) have paved the way for Vision Large Language Models (VLLMs) capable of erforming a wide range of visual understanding tasks. While LLMs have demonstrated impressive performance on standard natural images, their capabilities have not been thoroughly explored in cluttered datasets where there is complex environment having deformed shaped bjects. In this work, we introduce a novel dataset specifically designed for waste classification in real-world scenarios, characterized by complex environments and deformed shaped objects. Along with this dataset, we present an in-depth evaluation approach to rigorously assess the robustness and accuracy of VLLMs. The introduced dataset and comprehensive analysis provide valuable insights into the performance of VLLMs under challenging conditions. Our findings highlight the critical need for further advancements in VLLM's robustness to perform better in complex environments. The dataset and code for our experiments will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W7QCm2qjIh": {
    "title": "PromptAug: Data Augmentation for Fine Grained Conflict Identification",
    "volume": "review",
    "abstract": "Following the garbage in garbage out maxim, the quality of training data supplied to machine learning models impacts their performance. Generating these high-quality annotated training sets from unlabelled data is both expensive and unreliable. Moreover, social media platforms are increasingly limiting academic access to data, eliminating a key resource for NLP research. Consequently, researchers are shifting focus towards text data augmentation strategies to overcome these restrictions. In this work, we present an innovative data augmentation method, PromptAug, using Large Language Models (LLMs). We demonstrate the effectiveness of PromptAug, with improvements over the baseline dataset of 2\\% accuracy and 5\\% F1-score. Furthermore, we evaluate PromptAug over a variety of dataset sizes, proving it's effectiveness even in extreme data scarcity scenarios. To ensure a thorough evaluation of data augmentation methods we further perform qualitative thematic analysis, identifying four problematic themes with augmented text data; Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and Augmented Content Misinterpretation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K9zCZHfj6J": {
    "title": "Paraphrase Generation Evaluation Powered by LLM: A Semantic Metric, Not a Lexical One",
    "volume": "review",
    "abstract": "Evaluating automatic paraphrase production systems is a difficult task as it involves, among other things, assessing the semantic proximity between two sentences. Usual measures are based on lexical distances, or at least on semantic embedding alignments. The rise of large language models has provided tools to model relationships within a text thanks to the attention mechanism. In this article, we introduce ParaPLUIE, a new measure based on a log likelihood ratio from a LLM, to assess the quality of a potential paraphrase. This measure is compared with usual measures on three datasets of manually labeled paraphrases and non-paraphrases pairs. Two datasets, posterior to this study, are known for their quality or difficulty on this task. The third one, build for this occasion, is composed of LLM outputs. According to evaluations, the proposed measure is better for sorting pairs of sentences by semantic proximity. In particular, it is much more independent to lexical distance and offer a easy classification threshold between paraphrases and non-paraphrases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uZORaRP9DX": {
    "title": "Evidence Retrieval for Fact Verification using Multi-stage Reranking",
    "volume": "review",
    "abstract": "In the fact verification domain, the accuracy and efficiency of evidence retrieval are paramount. This paper presents a novel approach to enhance the fact verification process through a Multi-stage ReRanking (M-ReRank) paradigm, which addresses the inherent limitations of single-stage evidence extraction. Our methodology leverages the strengths of advanced reranking techniques, including dense retrieval models and list-aware rerankers, to optimise the retrieval and ranking of evidence of both structured and unstructured types. We demonstrate that our approach significantly outperforms previous state-of-the-art models, achieving a recall rate of 93.63% for Wikipedia pages. The proposed system not only improves the retrieval of relevant sentences and table cells, but also enhances the overall verification accuracy. Through extensive experimentation on the FEVEROUS dataset, we show that our M-ReRank pipeline achieves substantial improvements in evidence extraction, particularly increasing the recall of sentences by 7.85%, tables by 8.29% and cells by 3% compared to the current state-of-the-art on the development set",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f5q2g4PDFj": {
    "title": "Exploring Large Language Models for Low-resource Reference-less Evaluation of Machine Translation",
    "volume": "review",
    "abstract": "Quality Estimation (QE) helps evaluate machine translation without reliance on reference translations. In this paper, we leverage large language models (LLMs) to perform quality estimation for eight low-resource language pairs. We evaluate various LLMs in zero-shot, and few-shot scenarios, and perform instruction fine-tuning using novel prompts based on annotation guidelines provided to human annotators. We observe that providing context from annotation guidelines improved the task performance. Our guideline context-annotated prompt outperforms existing approaches based on LLMs in zero-shot scenarios for most low-resource language pairs. For low-resource languages, results obtained show that prompt-based approaches leveraging LLMs are outperformed by traditional supervised fine-tuned QE models. We present a comprehensive analysis of varying prompt-based approaches to QE. Our analysis reveals problems in LLM tokenization strategy and argues for refinement in model pre-training for low-resource languages. This study underscores the potential for prompt-based QE, indicating how additional context can help improve the task performance for low-resource scenarios. We release the code, data, and models trained publicly for further research",
    "checked": false,
    "id": "3ddf525bb95868d757745cea07484b4b05ed2f8a",
    "semantic_title": "a case study of improving english-arabic translation using the transformer model",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=75IAHKH1wX": {
    "title": "BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning",
    "volume": "review",
    "abstract": "Large language models (LLMs) possess extensive parametric knowledge, yet their size complicates the frequent integration of new information. Knowledge editing (KE) has emerged as a viable solution to modify LLMs' behaviors within specific knowledge scope without compromising overall performance. However, the potential for cross-lingual KE in current English-centric LLMs has not been fully explored. Recent advances in on-the-fly model modification methods, inspired by in-context learning (ICL), have shown promise for KE in LLMs, predominantly in monolingual English contexts. Our study introduces the the **BMIKE-53** benchmark, which evaluates cross-lingual KE covering 53 diverse languages across three KE task types. To investigate the effect of gradient-free methods on BMIKE-53, we implement the **M**ultilingual **I**n-context **K**nowledge **E**diting (**MIKE**) method. Our evaluation focuses on cross-lingual knowledge transfer in terms of reliability, generality, locality, and portability, offering valuable insights and a framework for future research in cross-lingual KE. Our code and data can be accessed publicly via this anonymous repository: <https://anonymous.4open.science/r/MIKE>",
    "checked": true,
    "id": "a5b997fd015f218440eca263910543b951979692",
    "semantic_title": "bmike-53: investigating cross-lingual knowledge editing with in-context learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qxCUqxR5hG": {
    "title": "Weighted Grouped Query Attention in Transformers",
    "volume": "review",
    "abstract": "The attention mechanism forms the foundational blocks for transformer language models. Recent approaches show that scaling the model achieves human-level performance. However, with increasing demands for scaling and constraints on hardware memory, the inference costs of these models remain high. To reduce the inference time, Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) were introduced. In this paper, we propose a variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning. Our model achieves an average of 0.53% improvement over GQA, and the performance converges to traditional Multi-head attention (MHA) with no additional overhead during inference. We evaluated the introduction of these parameters and subsequent finetuning informs the model about the grouping mechanism during training, thereby enhancing performance. Additionally, we demonstrate the scaling laws in our analysis by comparing the results between T5-small and T5-base architecture",
    "checked": true,
    "id": "61203ec44f69a1bb6838f772940ea468b2b6f96e",
    "semantic_title": "weighted grouped query attention in transformers",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=BxrgkvGusd": {
    "title": "Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks",
    "volume": "review",
    "abstract": "Despite large successes of recent language models, they suffer from severe performance degeneration in low-resource settings with limited training data available. Many existing works tackle this problem by generating synthetic data from the training data and then training models on them, recently using Large Language Models (LLMs). However, in low-resource settings, the amount of seed data samples to use for data augmentation is very small, which makes generated samples suboptimal and less diverse. To tackle this challenge, we propose a novel method that augments training data by incorporating a wealth of examples from other datasets, along with the given training data. Specifically, we first retrieve relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed data, and prompt LLMs to generate new samples with the contextual information within and across the original and retrieved samples. This approach can ensure that the generated data is not only relevant but also more diverse than what could be achieved using the limited seed data alone. We validate our Retrieval-Augmented Data Augmentation (RADA) framework on multiple datasets under low-resource settings of training and test-time data augmentation scenarios, on which it outperforms existing data augmentation baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RzyM8nR2MH": {
    "title": "Eliciting human preferences with language models",
    "volume": "review",
    "abstract": "Language models (LMs) can be directed to perform target tasks by using labeled examples or natural language prompts. But selecting examples or writing prompts for can be challenging--especially in tasks that involve unusual edge cases, demand precise articulation of nebulous preferences, or require an accurate mental model of LM behavior. We propose to use *LMs themselves* to guide the task specification process. In this paper, we introduce **Generative Active Task Elicitation (GATE)**: a learning framework in which models elicit and infer intended behavior through free-form, language-based interaction with users. We study GATE in three domains: email validation, content recommendation, and moral reasoning. In preregistered experiments, we show that LMs prompted to perform GATE (e.g., by generating open-ended questions or synthesizing informative edge cases) elicit responses that are often more informative than user-written prompts or labels. Users report that interactive task elicitation requires less effort than prompting or example labeling and surfaces novel considerations not initially anticipated by users. Our findings suggest that LM-driven elicitation can be a powerful tool for aligning models to complex human preferences and values",
    "checked": true,
    "id": "b4ba53eca9c286c8ba4e06f83bf20ee0145af1c0",
    "semantic_title": "eliciting human preferences with language models",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=8j0DgspBJH": {
    "title": "An Investigation into the Privacy Flow of Large Language Models in Interactive Contextual Settings",
    "volume": "review",
    "abstract": "Knowing about what to share in their outputs, for what purpose, and with whom is important for contextual privacy protection, a crucial work for real-world deployments of large language models(LLMs). Although existing research has demonstrated that Large Language Models (LLMs) disclose privacy in contexts where humans would not. Up to date, no evaluations have taken these large models as real-life participants, nor have they taken enough contextual or interactive factors into account. This paper proposes a multi-tier framework named privacyFlow, designed to investigate privacy flow of LLMs in interactive scenarios. The framework encompasses 147 privacy issues within 1,200 contextual scenarios. We conducted extensive experiments on four LLMs, assessing the impact of privacy type, recipient relationship, legal-moral instructions, and prompting attacks on privacy sharing behavior. The findings offer insights into disclosure patterns and suggest directions for future alignment work, emphasizing that LLMs should have the ability to control the flow of privacy in line with humans even in extreme situation such as prompting attacks scenario",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dq8VpnQ4M6": {
    "title": "PETapter: Leveraging PET-style classification heads for modular few-shot parameter-efficient fine-tuning",
    "volume": "review",
    "abstract": "Few-shot learning and parameter-efficient fine-tuning (PEFT) are crucial to overcome the challenges of data scarcity and ever growing language model sizes. This applies in particular to specialized domains such as argument mining, where complex and nuanced phrasing makes it difficult even for humans to distinguish not only between the stances but also whether a sentence contains a claim or an argument. We propose PETapter, a novel method that effectively combines PEFT methods with PET-style classification heads to boost few-shot learning capabilities without the significant computational overhead typically associated with full model training. We validate our approach on three established NLP benchmark datasets and one real-world argument mining dataset. We show that PETapter not only achieves comparable performance to full few-shot fine-tuning using pattern-exploiting training (PET), but also provides greater reliability and higher parameter efficiency while enabling higher modularity and easy sharing of the trained modules",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATZoN90B7g": {
    "title": "NewsEdits 2.0: Learning the Intentions Behind Updating News",
    "volume": "review",
    "abstract": "As events progress, news articles often update with new information: if we are not cautious, we risk propagating outdated facts. In this work, we hypothesize that linguistic features indicate factual fluidity, and that we can _predict which facts in a news article will update_ using solely the text of a news article (i.e. not external resources like search engines). We test this hypothesis, first, by isolating fact-updates in large news revisions corpora. News articles may update for many reasons (e.g. factual, stylistic, narrative). We introduce the _NewsEdits 2.0_ taxonomy, an edit-intentions schema that separates fact updates from stylistic and narrative updates in news writing. We annotate over 9,200 pairs of sentence revisions and train high-scoring ensemble models to apply this schema. Then, taking a large dataset of silver-labeled pairs, we show that we can predict when facts will update in older article drafts with high precision. Finally, to demonstrate the usefulness of these findings, we construct a language model question asking (LLM-QA) abstention task. Inspired by \\newcite{kasai2022realtime}, we wish the LLM to abstain from answering questions when information is likely to become outdated. Using our predictions, we show, LLM absention reaches _near oracle levels of accuracy_",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k5g7GkSf9y": {
    "title": "Exploring Quantization for Efficient Pre-Training of Transformer Language Models",
    "volume": "review",
    "abstract": "The increasing scale of Transformer models has led to an increase in their pre-training computational requirements. While quantization has proven to be effective after pre-training and during fine-tuning, applying quantization in Transformers during pre-training has remained largely unexplored at scale for language modeling. This study aims to explore the impact of quantization for efficient pre-training of Transformers, with a focus on linear layer components. By systematically applying straightforward linear quantization to weights, activations, gradients, and optimizer states, we assess its effects on model efficiency, stability, and performance during training. By offering a comprehensive recipe of effective quantization strategies to be applied during the pre-training of Transformers, we promote high training efficiency from scratch while retaining language modeling ability",
    "checked": true,
    "id": "2ad2bded5db34ab49c7d2a84e8d162b9707b56e8",
    "semantic_title": "exploring quantization for efficient pre-training of transformer language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vjsgqtmx0G": {
    "title": "Evaluating large language-vision models on geographic language understanding",
    "volume": "review",
    "abstract": "Geographic language understanding (GLU) tasks ask models to map from text to maps. Geographical complex description parsing (GCDP) is a GLU task where models must assign sets of map coordinates to text that goes beyond a single named location, such as \"...between the towns of Adrano and S. Maria di Licodia, 32 kilometres northwest of Catania\". In GCDP, the input is both a text and a set of reference geometries for known places in the text (e.g., Adrano, S. Maria di Licodia, Catania), and the output is the geometry of the location described. In this paper, we convert a GCDP corpus into an image + text $\\rightarrow$ image benchmark to evaluate recent large langugage-vision models on such complex task. The models show weak performance, with analysis showing a lack of understanding of even simpler tasks like recognizing regions by color",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PIt90fv6W4": {
    "title": "Persuasiveness of Generated Free-Text Rationales in Subjective Decisions: A Case Study on Pairwise Argument Ranking",
    "volume": "review",
    "abstract": "Generating free-text rationales is among the emergent capabilities of Large Language Models (LLMs). These rationales have been found to enhance LLM performance across various NLP tasks. Recently, there has been growing interest in using these rationales to provide insights for various important downstream tasks. In this paper, we analyze generated free-text rationales in tasks with subjective answers, emphasizing the importance of rationalization in such scenarios. We focus on pairwise argument ranking, a highly subjective task with significant potential for real-world applications, such as debate assistance. We evaluate the persuasiveness of rationales generated by nine LLMs to support their subjective choices. Our findings suggest that open-source LLMs, particularly Llama2-70B-chat, are capable of providing highly persuasive rationalizations, surpassing even GPT models. Additionally, our experiments show that rationale persuasiveness can be improved by controlling its parameters through prompting or through self-refinement",
    "checked": true,
    "id": "156f1a58661814f60ab4abaf027f64e47d1c4fdf",
    "semantic_title": "persuasiveness of generated free-text rationales in subjective decisions: a case study on pairwise argument ranking",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MJEaWaxC6B": {
    "title": "Biaffine Modal Dependency Parsing",
    "volume": "review",
    "abstract": "A modal dependency structure represents a web of connections in a document describing the source and epistemic strength of statements that helps to establish factuality in a given text. Obtaining such graphs defines a core task of modal dependency parsing, which involves event and source identification as well as labeling of modal relations between them. In this paper, we propose a simple yet effective biaffine modal dependency parser for English and Chinese that outperforms previous work",
    "checked": false,
    "id": "7d12432720078174c0af07beb094f3b12330e107",
    "semantic_title": "biaffine discourse dependency parsing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jxc8TbxweK": {
    "title": "Authorship Style Transfer with Policy Optimization",
    "volume": "review",
    "abstract": "Authorship style transfer aims to rewrite a given text into a specified target while preserving the original meaning in the source. Existing approaches rely on the availability of a large number of target style exemplars for model training. However, these overlook cases where a limited number of target style examples are available. The development of parameter-efficient transfer learning techniques and policy optimization (PO) approaches suggest lightweight PO is a feasible approach to low-resource style transfer. In this work, we propose a simple two-stage tune-and-optimize technique for low-resource textual style transfer. We apply our technique to authorship transfer as well as a larger-data native language style task and in both cases find it outperforms state-of-the-art baseline models",
    "checked": true,
    "id": "e33bd615b021ba90f87e02c036c3acae834d3893",
    "semantic_title": "authorship style transfer with policy optimization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=xFFyvJcqaq": {
    "title": "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance",
    "volume": "review",
    "abstract": "As large language models (LLMs) rapidly evolve, they are increasingly being customized through fine-tuning to suit the specific needs of various applications. A critical aspect of this advancement is the alignment process, which ensures that these models perform tasks in ways that align with human values and expectations. Current alignment methods, such as direct preference optimization (DPO) and reinforcement learning from human feedback (RLHF), focus primarily on alignment during training phase. However, these methods often involve complex and resource-intensive training processes, posing significant challenge for their implementation. Therefore, we propose \\textbf{InferAligner}, a simple yet effective method for harmlessness alignment during inference phase. InferAligner decouples harmlessness from helpfulness. During the training phase, it focuses solely on enhancing the target model's capabilities on downstream tasks. In the inference phase, it utilizes safety steering vectors extracted from the aligned model to guide the target model towards harmlessness alignment. Experimental results show that our method can be very effectively applied to domain-specific models in finance, medicine, and mathematics, as well as to multimodal large language models (MLLMs) such as LLaVA. It significantly diminishes the attack success rate (ASR) of both harmful instructions and jailbreak instructions, while maintaining almost unchanged performance in downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=duOsmTMvnu": {
    "title": "ApiQ: Finetuning of 2-Bit Quantized Large Language Model",
    "volume": "review",
    "abstract": "Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the effectiveness of these methods compared to full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework named ApiQ, designed to restore the lost information from quantization by concurrently initializing the LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM's activation precision while mitigating the error propagation from shallower into deeper layers. Through comprehensive evaluations conducted on a spectrum of language tasks with various LLMs, ApiQ demonstrably minimizes activation error during quantization. Consequently, it consistently achieves superior finetuning results across various bit-widths",
    "checked": true,
    "id": "4d5cbfbba6e336075cf11c7229b38b098d9243d1",
    "semantic_title": "apiq: finetuning of 2-bit quantized large language model",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=dsXatBeZV6": {
    "title": "Morphology of Chinese Characters: Evaluating LLMs and VLMs on Visual Features and Radical Prompting for NLP Tasks",
    "volume": "review",
    "abstract": "As a glyphic language, Chinese incorporates information-rich visual features below the character level, such as radicals which can provide hints about meaning or pronunciation. However, we argue that Large Language Models (LLMs) and Vision-Language Models (VLMs) fail to identify or harness these valuable features. Our study evaluates LLMs and VLMs in identifying visual information in Chinese characters, such as radicals, composition structures, strokes, and stroke count. Additionally, we design \"radical prompting\" to explore enhancements for LLMs in NLP tasks utilizing radical information. Results demonstrate most LLMs and VLMs struggle to recognize any visual information in Chinese characters. The introduction of `radical prompting' led to some improvements in LLM performance across NLP tasks, but significant improvement was seen only when correct radicals were provided, as observed in part-of-speech (POS) tagging task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mrjLTbCcHi": {
    "title": "StoC-TOT: Stochastic Tree-of-Thought with Constrained Decoding for Complex Reasoning in Multi-Hop Question Answering",
    "volume": "review",
    "abstract": "Multi-hop question answering (MHQA) requires a model to retrieve and integrate information from multiple passages to answer a complex question. Recent systems leverage the power of large language models and integrate evidence retrieval with reasoning prompts (e.g., chain-of-thought reasoning) for the MHQA task. However, the complexities in the question types (bridge v.s. comparison questions) and the reasoning types (sequential v.s. parallel reasonings) require more novel and fine-grained prompting methods to enhance the performance of MHQA under the zero-shot setting. In this paper, we propose StoC-ToT, a Stochastic Tree-of-Thought reasoning prompting method with constrained decoding for MHQA and conduct a detailed comparison with other reasoning prompts on different question types and reasoning types. Specifically, we construct a tree-like reasoning structure by prompting the model to break down the original question into smaller sub-questions to form different reasoning paths. We prompt the model to provide a probability estimation for each reasoning path at each reasoning step. Experiments on two MHQA datasets and five Large Language Models showed that StoC-ToT outperforms other reasoning prompts by a significant margin",
    "checked": true,
    "id": "ce3777f000616f1202157953b8578f52566817af",
    "semantic_title": "stoc-tot: stochastic tree-of-thought with constrained decoding for complex reasoning in multi-hop question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yEttrbXR4X": {
    "title": "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation",
    "volume": "review",
    "abstract": "Recent Retrieval Augmented Generation (RAG) aims to enhance Large Language Models (LLMs) by incorporating extensive knowledge retrieved from external sources. However, such approach encounters some challenges: Firstly, the original queries may not be suitable for precise retrieval, resulting in erroneous contextual knowledge; Secondly, the language model can easily generate inconsistent answer with external references due to their knowledge boundary limitation. To address these issues, we propose the chain-of-verification (CoV-RAG) to enhance the external retrieval correctness and internal generation consistency. Specifically, we integrate the verification module into the RAG, engaging in scoring, judgment, and rewriting. To correct external retrieval errors, CoV-RAG retrieves new knowledge using a revised query. To correct internal generation errors, we unify QA and verification tasks with a Chain-of-Thought (CoT) reasoning during training. Our comprehensive experiments across various LLMs demonstrate the effectiveness and adaptability compared with other strong baselines. Especially, our CoV-RAG can significantly surpass the state-of-the-art baselines using different LLM backbones",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UCr1wOgr8c": {
    "title": "What Does Infect Mean to Cardio? Investigating the Role of Clinical Specialty Instructions in Medical LLMs",
    "volume": "review",
    "abstract": "In this paper, we introduce S-MedQA, a medical question-answering (QA) dataset for benchmarking large language models (LLMs) in fine-grained clinical specialties. Using S-MedQA, we gauge the role of instructions in knowledge-intense scenarios by checking the applicability of two popular hypotheses related to knowledge injection and style/format learning. We show that in the medical domain, more instructions result in better performance. However, the improvement in performance derives neither from the extra knowledge contained in the instructions nor from the style/format learned from them. Thus, we suggest rethinking the role of instruction data in the medical domain. We release S-MedQA for the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JMrPaXEKru": {
    "title": "Towards Region-aware Bias Evaluation Metrics",
    "volume": "review",
    "abstract": "When exposed to human-generated data, language models are known to learn and amplify societal biases. While previous works introduced benchmarks that can be used to assess the bias in these models, they rely on assumptions that may not be universally true. For instance, a gender bias dimension commonly used by these metrics is that of family--career, but this may not be the only common bias in certain regions of the world. In this paper, we identify topical differences in gender bias across different regions and propose a region-aware bottom-up approach for bias assessment. Our proposed approach uses gender-aligned topics for a given region and identifies gender bias dimensions in the form of topic pairs that are likely to capture gender societal biases. Several of our proposed bias topic pairs are on par with human perception of gender biases in these regions in comparison to the existing ones, and we also identify new pairs that are more aligned than the existing ones. In addition, we use our region-aware bias topic pairs in a Word Embedding Association Test (WEAT)-based evaluation metric to test for gender biases across different regions in different data domains. We also find that LLMs have a higher alignment to bias pairs for highly-represented regions showing the importance of region-aware bias evaluation metric",
    "checked": true,
    "id": "34214e1de58c6b816263e2207c77ff65f093b967",
    "semantic_title": "towards region-aware bias evaluation metrics",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QpHBzKfNd3": {
    "title": "Enhancing Alzheimer's Disease Diagnosis Records with Large Language Models: A Pipeline for Multimodal and Longitudinal EHRs",
    "volume": "review",
    "abstract": "Alzheimer's disease (AD) is a neurodegenerative, incurable condition and a leading cause of morbidity among individuals over 65 in the US. The screening and early diagnosis of AD condition is usually based on the patient's electrical health records (HERs), including clinical observations, cognitive tests, patient profiles, and medical-imaging-aided diagnoses. However, above information for researchers is highly fragmented. One of the most critical clinical diagnostic notes is stored in structured tables using specialized terminological formats. This presents significant challenges to the accessibility and readability for non-experts, thereby hindering information processing and the development of general medical AI systems. This work proposes a novel pipeline for processing AD clinical diagnostic information: (1) we collect clinical data from the largest AD dataset of Alzheimer's Disease Neuroimaging Initiative (ADNI), explain abbreviations and terminology, and organize the information in an accessible manner for those without expert knowledge of AD. (2) Leveraging the power of Large Language Models (LLMs), we present a GPT-based method that effectively transforms tabular clinical data into fluent and faithful natural language diagnostic reports, as demonstrated by our experimental results. (3) We further explore the inherently multi-modal nature of medical data, collecting and processing a total of 10387 volumetric T1-weighted MRI scans from ADNI. (4) Finally, we discuss the existing limitations in applying multimodality EHRs for brain disease analysis and propose forward-looking directions to meet the demands of the neuroimaging domain. We expect that this work will provide new insights into the neuroimaging domain and improve AI applications in healthcare",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DmhkhcXa2q": {
    "title": "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models",
    "volume": "review",
    "abstract": "Guardrails have emerged as an alternative to safety alignment for content moderation of large language models (LLMs). Existing model-based guardrails have not been designed for resource-constrained computational portable devices, such as mobile phones, more and more of which are running LLM-based applications locally. We introduce LoRA-Guard, a parameter-efficient guardrail adaptation method that relies on knowledge sharing between LLMs and guardrail models. LoRA-Guard extracts language features from the LLMs and adapts them for the content moderation task using low-rank adapters, while a dual-path design prevents any performance degradation on the generative task. We show that LoRA-Guard outperforms existing approaches with 100-1000x lower parameter overhead while maintaining accuracy, enabling on-device content moderation",
    "checked": true,
    "id": "cfecd71fbf71fb3d043f21184f20d7aefe77bf85",
    "semantic_title": "lora-guard: parameter-efficient guardrail adaptation for content moderation of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MhDEbTtQPf": {
    "title": "A Psycholinguistic Evaluation of Language Models' Sensitivity to Argument Roles",
    "volume": "review",
    "abstract": "We present a systematic evaluation of large language models' sensitivity to argument roles, i.e., $\\textit{who}$ did what to $\\textit{whom}$, by replicating psycholinguistic studies on human argument role processing. In three experiments, we find that language models are able to distinguish verbs that appear in plausible and implausible contexts, where plausibility is determined through the relation between the verb and its preceding arguments. However, none of the models capture the same selective patterns that human comprehenders exhibit during real-time verb prediction. This indicates that language models' capacity to detect verb plausibility does not arise from the same mechanism that underlies human real-time sentence processing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h2dAggXjXo": {
    "title": "As easy as PIE: understanding when pruning causes language models to disagree",
    "volume": "review",
    "abstract": "Language Model (LM) pruning compresses the model by removing weights, nodes, or other parts of its architecture. Typically, pruning focuses on the resulting efficiency gains at the cost of effectiveness. However, when looking at how individual data points are affected by pruning, it turns out that a particular subset of data points always bears most of the brunt (in terms of reduced accuracy) when pruning, but this effect goes unnoticed when reporting the mean accuracy of all data points. These data points are called PIEs and have been studied in image processing, but not in NLP. In a study of various NLP datasets, pruning methods, and levels of compression, we find that PIEs impact inference quality considerably, regardless of class frequency, and that BERT is more prone to this than BiLSTM. We also find that PIEs contain a high amount of data points that have the largest influence on how well the model generalises to unseen data. This means that when pruning, with seemingly moderate loss to accuracy across all data points, we in fact hurt tremendously those data points that matter the most. We trace what makes PIEs both hard and impactful to inference to their overall longer and more semantically complex text. These findings are novel and contribute to understanding how LMs are affected by pruning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xsfPGxrBOr": {
    "title": "Driving Chinese Spelling Correction from a Fine-Grained Perspective",
    "volume": "review",
    "abstract": "This paper explores the task: Chinese spelling correction (CSC), from a fine-grained perspective by recognizing that existing evaluations lack nuanced typology for the spelling errors. This deficiency can create a misleading impression of models' performance, incurring an \"invisible\" bottleneck hindering the advancement of CSC research. In this paper, we first categorize spelling errors into six types and conduct a fine-grained evaluation across a wide variety of models, including BERT-based models and LLMs. Thus, we are able to pinpoint the underlying weaknesses of existing state-of-the-art models - utilizing contextual clues and handling co-existence of multiple typos, associated to contextual errors and multi-typo errors. However, these errors suffer from low occurrence in conventional training corpus. Therefore, we introduce new error generation methods to synthesize their occurrence. Eventually, these augmented data can be leveraged to enhance the training process of CSC models. We hope this work could provide fresh insight for future CSC research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zuS0YC7rpC": {
    "title": "MEANT: Multimodal Encoder for Antecedent Information",
    "volume": "review",
    "abstract": "The stock market provides a rich well of information that can be split across modalities, making it an ideal candidate for multimodal evaluation. Multimodal data plays an increasingly important role in the development of machine learning and has shown to positively impact performance. But information can do more than exist across modes--- it can exist across time. How should we attend to temporal data that consists of multiple information types? This work introduces (i) the MEANT model, a Multimodal Encoder for Antecedent information and (ii) a new dataset called \\textit{TempStock}, which consists of price, Tweets, and graphical data with over a million Tweets from all of the companies in the S\\&P 500 Index. We find that MEANT improves performance on existing baselines by over 15\\%, and that the textual information affects performance far more than the visual information on our time-dependent task from our ablation study",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AZ781CuNLk": {
    "title": "Follow the Beaten Path: The Role of Route Patterns on Vision-Language Navigation Agents Generalization Abilities",
    "volume": "review",
    "abstract": "Vision and language navigation (VLN) is a challenging task towards the creation of embodied agents that requires spatial and temporal reasoning over the instructions provided in natural language and aligning them with the visual perception of an environment. Although a number of methods and approaches have been developed, none achieves human level performance in outdoor settings (by up to 75 percent). The contributions of visual and language modalities to the success of VLN have been studied, however here we focus on an overlooked property of routes and show that navigational instructions can be represented as patterns of actions that also describe trajectory shapes. Through carefully crafted experiments, we show that agents generalization to unseen environments depends not only on visual and linguistic features, but also on the shape of trajectories presented to the model during the fine-tuning. Our experiments show that the diversity of patterns of actions during training is a key contributor to high success rates for agents. Our findings will guide researchers towards improved practices in the development and evaluation of VLN datasets and agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TNfYsLINzx": {
    "title": "A Novel Dual of Shannon Information and Weighting Scheme",
    "volume": "review",
    "abstract": "Shannon Information theory has achieved great success in not only communication technology where it was originally developed for but also many other science and engineering fields such as machine learning and artificial intelligence. Inspired by the famous weighting scheme TF-IDF, we discovered that Shannon information entropy actually has a natural dual. To complement the classical Shannon information entropy which measures the uncertainty we propose a novel information quantity, namely troenpy. Troenpy measures the certainty and commonness of the underlying distribution. So entropy and troenpy form an information twin. To demonstrate its usefulness, we propose a conditional troenpy based weighting scheme for document with class labels, namely positive class frequency (PCF). On a collection of public datasets we show the PCF based weighting scheme outperforms the classical TF-IDF and a popular Optimal Transport based word moving distance algorithm in a kNN setting with respectively more than 22.9 and 26.5 classification error reduction while the corresponding entropy based approach completely fails. We further developed a new odds-ratio type feature, namely Expected Class Information Bias(ECIB), which can be regarded as the expected odds ratio of the information twin across different classes. In the experiments we observe that including the new ECIB features and simple binary term features in a simple logistic regression model can further significantly improve the performance. The proposed simple new weighting scheme and ECIB features are very effective and can be computed with linear time complexity",
    "checked": true,
    "id": "8baccce125eb4b0cae777b9ba6a480e86f3c8960",
    "semantic_title": "a novel dual of shannon information and weighting scheme",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RWYxCEbMvy": {
    "title": "Reviewer2: Optimizing Review Generation Through Prompt Generation",
    "volume": "review",
    "abstract": "Recent developments in LLMs offer new opportunities for assisting authors in improving their work. In this paper, we envision a use case where authors can receive LLM-generated reviews that uncover weak points in the current draft. While initial methods for automated review generation already exist, these methods tend to produce reviews that lack detail, and they do not cover the range of opinions that human reviewers produce. To address this shortcoming, we propose an efficient two-stage review generation framework called Reviewer2. Unlike prior work, this approach explicitly models the distribution of possible aspects that the review may address. We show that this leads to more detailed reviews that better cover the range of aspects that human reviewers identify in the draft. As part of the research, we generate a large-scale review dataset of 27k papers and 99k reviews that we annotate with aspect prompts, which we make available as a resource for future research",
    "checked": false,
    "id": "ef5cd0eb266e3df3eb64aec18e1854fe0244d228",
    "semantic_title": "emotion-conditioned text generation through automatic prompt optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FkbcreYFJW": {
    "title": "LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference",
    "volume": "review",
    "abstract": "Long-context Multimodal Large Language Models (MLLMs) demand substantial computational resources for inference as the growth of their multimodal Key-Value (KV) cache, in response to increasing input lengths, challenges memory and time efficiency. Unlike single-modality LLMs that manage only textual contexts, the KV cache of long-context MLLMs includes representations from multiple images with temporal and spatial relationships and related textual contexts. The predominance of image tokens means traditional optimizations for LLMs' KV caches are unsuitable for multimodal long-context settings, and no prior works have addressed this challenge. In this work, we introduce \\textbf{\\textsc{LOOK-M}}, a pioneering, fine-tuning-free approach that efficiently reduces the multimodal KV cache size while maintaining performance comparable to a full cache. We observe that during prompt prefill, the model prioritizes more textual attention over image features, and based on the multimodal interaction observation, a new proposed text-prior method is explored to compress the KV cache. Furthermore, to mitigate the degradation of image contextual information, we propose several compensatory strategies using KV pairs merging. \\textbf{\\textsc{LOOK-M}}\\footnote{The source code will be made publicly available.} demonstrates that with a significant reduction in KV Cache memory usage, such as reducing it by \\textbf{80\\%} in some cases, it not only achieves approximately \\textbf{1.3x} faster decoding but also maintains or even \\textbf{enhances} performance across a variety of long context multimodal tasks",
    "checked": true,
    "id": "6c5e09cef64fe7fbeab9a6f3f062363bffba917d",
    "semantic_title": "look-m: look-once optimization in kv cache for efficient multimodal long-context inference",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=qcH80ZZGXa": {
    "title": "Unifying Demonstration Selection and Compression for In-Context Learning",
    "volume": "review",
    "abstract": "In-context learning (ICL) enables LLMs to exhibit spectacular emergent capabilities in various scenarios. Unfortunately, introducing demonstrations easily makes the prompt length explode, bringing a significant burden to hardware. In addition, random demonstrations usually achieve limited improvements in ICL, necessitating demonstration selection among accessible candidates. Previous studies introduce extra modules to perform demonstration compression or selection independently. In this paper, we propose an ICL framework UniICL, which \\textbf{Uni}fies demonstration selection and compression, and final response generation via a single frozen LLM. UniICL leverages the understanding ability of well-trained LLMs to independently compress different demonstrations into compressed features, and then a learnable projection layer converts features to LLM-acceptable compressed virtual tokens. Apart from substituting original demonstrations to reduce input length, virtual tokens are again used to select potential demonstrations. candidate demonstrations and inference input. Finally, current queries together with selected compressed virtual tokens are fed into the same frozen LLM for response generation. UniICL is a parameter-efficient framework that only contains 17M trainable parameters originating from the projection layer and a learnable embedding. We build UniICL upon two backbones and conduct experiments over in- and out-domain datasets of both generative and understanding tasks, encompassing ICL scenarios with plentiful and limited demonstration candidates. Results show that UniICL effectively unifies $12 \\times$ compression, demonstration selection, and response generation, efficiently scaling up the baseline from 4-shot to 64-shot ICL with 24 GB CUDA allocation\\footnote{The code and model will be released in the final version.}",
    "checked": true,
    "id": "a0c239fb3e2cc0a7c3b7957d12c83e5be0b48bd5",
    "semantic_title": "unifying demonstration selection and compression for in-context learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=gxQAPMDX6y": {
    "title": "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models",
    "volume": "review",
    "abstract": "Prompt learning is susceptible to intrinsic bias present in pre-trained language models (LMs), resulting in sub-optimal performance of prompt-based zero/few-shot learning. In this work, we propose a null-input prompting method to calibrate intrinsic bias encoded in pre-trained LMs. Different from prior efforts that address intrinsic bias primarily for social fairness and often involve excessive computational cost, our objective is to explore enhancing LMs' performance in downstream zero/few-shot learning while emphasizing the efficiency of intrinsic bias calibration. Specifically, we leverage a diverse set of auto-selected null-meaning inputs generated from GPT-4 to probe intrinsic bias of pre-trained LMs. Utilizing the bias-reflected probability distribution, we formulate a distribution disparity loss for bias calibration, where we exclusively update bias parameters (0.1% of total parameters) of LMs towards equal probability distribution. Experimental results show that the calibration promotes an equitable starting point for LMs while preserving language modeling abilities. Across a wide range of datasets, including sentiment analysis and topic classification, our method significantly improves zero/few-shot learning performance of LMs for both in-context learning and prompt-based fine-tuning (on average 9% and 2%, respectively)",
    "checked": true,
    "id": "8827b7b7e6dbba853c7e647fd06aa92e42b5f273",
    "semantic_title": "prompt-based bias calibration for better zero/few-shot learning of language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bI93TNG81x": {
    "title": "Investigating Continual Pretraining in Large Language Models: Insights and Implications",
    "volume": "review",
    "abstract": "Continual learning (CL) in large language models (LLMs) is an evolving domain that focuses on developing strategies for efficient and sustainable training. Our primary emphasis is on \\emph{continual domain-adaptive pretraining}, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge and enhancing cross-domain knowledge transfer without relying on domain-specific identification. Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting, our research evaluates the adaptability and capabilities of LLMs to changing data landscapes in practical scenarios. To this end, we introduce a new benchmark designed to measure the adaptability of LLMs to these evolving data environments, offering a comprehensive framework for evaluation. We examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the knowledge transfer within these models. Our findings uncover several key insights: (i) performance improves only if the adaptation corpora match the original pretraining scale, (ii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both forgetting and learning, (iii) when the sequence of domains shows semantic similarity, continual pretraining enables LLMs to specialize better compared to stand-alone pretraining, and (iv) fine-tuning performance on standard benchmarks is indeed influenced by continual pretraining domains. We posit that our research marks a shift towards establishing a more realistic benchmark for investigating CL in LLMs",
    "checked": true,
    "id": "12358df20ccf4085e6c8a45d3ab5fa15714abcd6",
    "semantic_title": "investigating continual pretraining in large language models: insights and implications",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=f7ODMHHcln": {
    "title": "Large Language Models Often Say One Thing and Do Another",
    "volume": "review",
    "abstract": "As large language models (LLMs) increasingly become central to various applications and interact with diverse user populations, ensuring their reliable and consistent performance is becoming more important. This paper explores a critical issue in assessing the reliability of LLMs: the consistency between their words and deeds. To quantitatively explore this consistency, we developed a novel evaluation benchmark, the Words and Deeds Consistency Test (WDCT), which establishes a strict correspondence between word-based and deed-based questions across different domains, including opinion versus action, non-ethical value versus action, ethical value versus action, and theory versus application. The evaluation results reveal a widespread inconsistency between words and deeds across LLMs and domains. Subsequently, we conducted experiments with either word alignment or deed alignment to observe their impact on the other aspect. The experiment results indicate that alignment only on words or deeds poorly and unpredictably influences the other aspect. This supports our hypothesis that the underlying knowledge guiding LLMs' choices of words or deeds is not contained within a unified space",
    "checked": false,
    "id": "1d3af04b206612d837bfff5646d6695a273d0af4",
    "semantic_title": "the strait of messina then and now",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LBw0Yyo97R": {
    "title": "FarsInstruct: Empowering Large Language Models for Persian Instruction Understanding",
    "volume": "review",
    "abstract": "Instruction-tuned large language models, such as T0, have demonstrated remarkable capabilities in following instructions across various domains. However, their proficiency remains notably deficient in many low-resource languages. To address this challenge, we introduce FarsInstruct: a comprehensive instruction dataset designed to enhance the instruction-following ability of large language models specifically for the Persian languageâ€”a significant yet underrepresented language globally. FarsInstruct encompasses a wide range of task types and datasets, each containing a mix of straightforward to complex manual written instructions, as well as translations from Public Pool of Prompts, ensuring a rich linguistic and cultural representation. Furthermore, we introduce Co-CoLA, a framework designed to enhance the multi-task adaptability of LoRA-tuned models. Through extensive experimental analyses, our study showcases the effectiveness of FarsInstruct dataset coupled with training by Co-CoLA framework, in improving the performance of large language models within the Persian context. As of the current writing, FarsInstruct comprises more than 200 templates across 21 distinct datasets, and we intend to update it consistently, thus augmenting its applicability",
    "checked": true,
    "id": "7f6695ff81f5ed2ae289aedbf90195d8b057dd87",
    "semantic_title": "farsinstruct: empowering large language models for persian instruction understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sCYZZBXk7z": {
    "title": "Demographically Blind Models Can be Unfair: Fairness through Awareness",
    "volume": "review",
    "abstract": "Debiasing methods for learning systems fall into two distinct philosophies of fairness: removing the use of protected attributes from the model, or including protected attributes in decision-making. However, the source of the bias that we seek to mitigate should dictate our choice of debiasing strategy. We categorize existing debiasing methods in these two fairness families, describe different types of biases, and show in controlled experiments that the choice of debiasing method should depend on the type of bias. Our results yield recommendations for practitioners moving forward",
    "checked": false,
    "id": "75950ca84fa6d3bae213ef8d8c6f41c3dc3f169c",
    "semantic_title": "group fairness via group consensus",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1IKZ876fWt": {
    "title": "Generating Demonstrations for In-Context Compositional Generalization in Grounded Language Learning",
    "volume": "review",
    "abstract": "In-Context-learning and few-shot prompting are viable methods compositional output generation. However, these methods can be very sensitive to the choice of support examples used. Retrieving good supports from the training data for a given test query is already a difficult problem, but in some cases solving this may not even be enough. We consider the setting of grounded language learning problems where finding relevant supports in the same or similar states as the query may be difficult. We design an agent which instead generates possible supports inputs and targets current state of the world, then uses them in-context-learning to solve the test query. We show substantially improved performance on a previously unsolved compositional generalization test without a loss of performance in other areas. The approach is general and can even scale to instructions expressed in natural language",
    "checked": false,
    "id": "1d41a0ddda57caa6c8d268dd1703e4c9b35db18b",
    "semantic_title": "one-shot learning from a demonstration with hierarchical latent language",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=YqwQgLVMI7": {
    "title": "Analyzing Context Utilization of LLMs in Document-Level Translation",
    "volume": "review",
    "abstract": "Large language models (LLM) are increasingly strong contenders in machine translation. We study document-level translation, where some words cannot be translated without context from outside the sentence. We investigate the ability of prominent LLMs to utilize context by analyzing models' robustness to perturbed (randomized) document context. We find that the strongest translation LLMs are robust to random context in translation performance. However, improved document-translation performance is not always reflected in pronoun translation performance. We highlight the need for context-aware finetuning of LLMs to improve their reliability for document-level translation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ww64cfDsTv": {
    "title": "Evaluating Linguistic Capabilities of Multimodal LLMs in the Lens of Few-Shot Learning",
    "volume": "review",
    "abstract": "The linguistic capabilities of Multimodal Large Language Models (MLLMs) are critical for their effective application across diverse tasks. This study aims to evaluate the performance of MLLMs on the VALSE benchmark, focusing on the efficacy of few-shot In-Context Learning (ICL), and Chain-of-Thought (CoT) prompting. We conducted a comprehensive assessment of state-of-the-art MLLMs, varying in model size and pretraining datasets. The experimental results reveal that ICL and CoT prompting significantly boost model performance, particularly in tasks requiring complex reasoning and contextual understanding. Models pretrained on captioning datasets show superior zero-shot performance, while those trained on interleaved image-text data benefit from few-shot learning. Our findings provide valuable insights into optimizing MLLMs for better grounding of language in visual contexts, highlighting the importance of the composition of pretraining data and the potential of few-shot learning strategies to improve the reasoning abilities of MLLMs",
    "checked": true,
    "id": "5a7aa562b12169e5ffc3b2a9744071a9c57e4e4c",
    "semantic_title": "evaluating linguistic capabilities of multimodal llms in the lens of few-shot learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x3RxLiMLr0": {
    "title": "From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP",
    "volume": "review",
    "abstract": "Interpretability and analysis (IA) research is a growing subfield within NLP with the goal of developing a deeper understanding of the behavior or inner workings of NLP systems and methods. Despite growing interest in the subfield, a commonly voiced criticism is that it lacks actionable insights and therefore has little impact on NLP. In this paper, we seek to quantify the impact of IA research on the broader field of NLP. We approach this with a mixed-methods analysis of: (1) a citation graph of 185K+ papers built from all papers published at ACL and EMNLP conferences from 2018 to 2023, and (2) a survey of 138 members of the NLP community. Our quantitative results show that IA work is well-cited outside of IA, and central in the NLP citation graph. Through qualitative analysis of survey responses and manual annotation of 556 papers, we find that NLP researchers build on findings from IA work and perceive it is important for progress in NLP, multiple subfields, and rely on its findings and terminology for their own work. Many novel methods are proposed based on IA findings and highly influenced by them, but highly influential non-IA work cites IA findings without being driven by them. We end by summarizing what is missing in IA work today and provide a call to action, to pave the way for a more impactful future of IA research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mG5jikbsaJ": {
    "title": "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity",
    "volume": "review",
    "abstract": "Combining large language models during training or at inference time has shown substantial performance gain over component LLMs. This paper presents LLM-TOPLA, a diversity-optimized LLM ensemble method with three unique properties: (i) We introduce the focal diversity metric to capture the diversity-performance correlation among component LLMs of an ensemble. (ii) We develop a diversity-optimized ensemble pruning algorithm to select the top-k sub-ensembles from a pool of $N$ base LLMs. Our pruning method recommends top-performing LLM subensembles of size $S$, often much smaller than $N$. (iii) We generate new output for each prompt query by utilizing a learn-to-ensemble approach, which learns to detect and resolve the output inconsistency among all component LLMs of an ensemble. Extensive evaluation on four different benchmarks shows good performance gain over the best LLM ensemble methods: (i) In constrained solution set problems, LLM-TOPLA outperforms the best-performing ensemble (Mixtral) by 3.5\\% in accuracy on MMLU and the best-performing LLM ensemble (MoreAgent) on GSM8k by 2.1\\%. (ii) In generative tasks, LLM-TOPLA outperforms the top-2 performers (Llama70b/Mixtral) on SearchQA by $3.9\\mathrm{x}$ in F1, and on XSum by more than $38$ in ROUGE-1. Our code and dataset, which contains outputs of 8 modern LLMs on 4 benchmarks is available at \\url{https://anonymous.4open.science/r/llm_topla-891B}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cI06Zq5z4P": {
    "title": "ProtocoLLM: Automatic Evaluation Framework of LLMs on Domain-Specific Scientific Protocol Formulation Tasks",
    "volume": "review",
    "abstract": "Automated generation of scientific protocols executable by robots can significantly accelerate scientific research processes. Large Language Models (LLMs) excel at Scientific Protocol Formulation Tasks (SPFT), but the evaluation of their capabilities relies on human evaluation. Here, we propose a flexible, automatic framework to evaluate LLMs' capability on SPFT: ProtocoLLM. This framework prompts the target model and GPT-4 to extract pseudocode from biology protocols using only predefined lab actions and evaluates the output of the target model using Llam-Eval, the pseudocode generated by GPT-4 serves as a baseline, and Llama-3 acts as the evaluator. Our adaptable prompt-based evaluation method, Llam-Eval, offers significant flexibility in terms of evaluation model, material, criteria, and is free of cost. We evaluate GPT variations, Llama, Mixtral, Gemma, Cohere, and Gemini. Overall, we find that GPT and Cohere are powerful scientific protocol formulators. We also introduce Bioprot 2.0, a dataset with biology protocols and corresponding pseudocodes, which can aid LLMs in the formulation and evaluation of SPFT. Our work is extensible to assess LLMs on SPFT across various domains and other fields that require protocol generation for specific goals",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RFo4zsNRkP": {
    "title": "Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization",
    "volume": "review",
    "abstract": "While large language models exhibit certain cross-lingual generalization capabilities, they suffer from performance degradation (PD) on unseen closely-related languages (CRLs) and dialects relative to their high-resource language neighbour (HRLN). However, we currently lack a fundamental understanding of what kinds of linguistic distances contribute to PD, and to what extent. Furthermore, studies of cross-lingual generalization are confounded by unknown quantities of CRL language traces in the training data, and by the frequent lack of availability of evaluation data in lower-resource related languages and dialects. To address these issues, we model phonological, morphological, and lexical distance as Bayesian noise processes to synthesize artificial languages that are controllably distant from the HRLN. We quantitatively and qualitatively analyse PD as a function of underlying noise parameters, offering insights on model robustness to isolated and composed linguistic phenomena, and the impact of task and HRL characteristics on PD. We calculate parameter posteriors on real CRL-HRLN pair data and show that they follow computed trends of artificial languages, demonstrating the viability of our noisers. Our framework offers a cheap solution to estimating task performance on an unseen CRL given HRLN performance using its posteriors, as well as for diagnosing observed PD on a CRL in terms of its linguistic distances from its HRLN, and opens doors to principled methods of mitigating performance degradation",
    "checked": true,
    "id": "cf2f09a0b81f7b2b1b92c5bddeaa6544617d53a9",
    "semantic_title": "evaluating large language models along dimensions of language variation: a systematik invesdigatiom uv cross-lingual generalization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1oebeZzs6T": {
    "title": "Learn and Unlearn in Multilingual LLMs",
    "volume": "review",
    "abstract": "This paper investigates the propagation of harmful information in multilingual large language models (LLMs) and evaluates the efficacy of various unlearning methods. We demonstrate that fake information, regardless of the language it is in, once introduced into these models through training data, can spread across different languages, compromising the integrity and reliability of the generated content. Our findings reveal that standard unlearning techniques, which typically focus on English data, are insufficient in mitigating the spread of harmful content in multilingual contexts and could inadvertently reinforce harmful content across languages. We show that only by addressing harmful responses in both English and the original language of the harmful data can we effectively eliminate generations for all languages. This underscores the critical need for comprehensive unlearning strategies that consider the multilingual nature of modern LLMs to enhance their safety and reliability across diverse linguistic landscapes",
    "checked": false,
    "id": "68b853b9d92e62353b0915416f52b67477e6ebe8",
    "semantic_title": "every language counts: learn and unlearn in multilingual llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cnrLITgaJN": {
    "title": "Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation",
    "volume": "review",
    "abstract": "In subjective NLP tasks, where a single ground truth does not exist, the inclusion of diverse annotators becomes crucial as their unique perspectives significantly influence the annotations. In realistic scenarios, the annotation budget often becomes the main determinant of the number of perspectives (i.e., annotators) included in the data and subsequent modeling. We introduce a novel framework for annotation collection and modeling in subjective tasks that aims to minimize the annotation budget while maximizing the predictive performance for each annotator. Our framework has a two-stage design: first, we rely on a small set of annotators to build a multitask model, and second, we augment the model for a new perspective by strategically annotating a few samples per annotator. To test our framework at scale, we introduce and release a unique dataset, Moral Foundations Subjective Corpus, of 2000 Reddit posts annotated by 24 annotators for moral sentiment. We demonstrate that our framework surpasses the previous SOTA in capturing the annotators' individual perspectives with as little as 25% of the original annotation budget on two datasets. Furthermore, our framework results in more equitable models, reducing the performance disparity among annotators",
    "checked": true,
    "id": "8d687f8edca283308d024fa3016cc18ec4a21c1e",
    "semantic_title": "cost-efficient subjective task annotation and modeling through few-shot annotator adaptation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6lKk4WY58T": {
    "title": "TABLE CALL: A New Paradigm for Table Question Answering",
    "volume": "review",
    "abstract": "Large language models (LLMs) have exhibited strong semantic understanding capabilities in interpreting and reasoning for table question answering (TQA). However, they struggle with excessively lengthy or complex input tables, especially when dealing with disorganized or hierarchical structures. To address these issues, we propose a new paradigm for TQA, named TABLE CALL, which leverages the tool-using capabilities of LLMs. Specifically, TABLE CALL invokes different tools for various types of table questions, such as SQL, Python, and LLMs, to simplify table understanding. Moreover, to enhance table comprehension capabilities of the LLM, we propose a few-shot library updating technique where we use a dynamically updated library to provide better QA pairs for LLM prompting. Experimental results on both open-domain and specific-domain datasets demonstrate that our approach achieves state-of-the-art performance, significantly outperforming previous methods",
    "checked": false,
    "id": "cc00884063e89f25bd92366160cb8df0cf8750c4",
    "semantic_title": "clinical neuroscience continuing education for psychiatrists",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=x1b7n8r9vE": {
    "title": "More \"Clever\" than \"Hans\": Probing and Adversarial Training in Translationese Classification",
    "volume": "review",
    "abstract": "Modern classifiers, especially neural networks, excel at leveraging subtle signals competing with many other signals in the data. When such noisy setups lead to accuracy rates of 90\\%+, as is for instance the case with current high-performance neural translationese classifiers, it raises concerns about potential spurious correlations in the data with the target labels -- a phenomenon often referred to as \"Clever Hans\". Recent research has indeed found evidence that high-performance multi-lingual BERT translationese classifiers use spurious topic information in the form of location names, rather than just translationese signals. In this paper, we address two difficult open problems associated with confounding signals in translationese classification. First, we use probing to provide direct evidence that these classifiers learn and use spurious topic correlations, some potentially unknown. Second, we introduce adversarial training as a strategy to mitigate any spurious topic correlation, including those unknown apriori. We show the effectiveness of our approach on translationese classification using three multi-lingual models, two language pairs, and four translationese data sets, as well as on a non-translationese classification task: occupation classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XOEUN7FggX": {
    "title": "How Susceptible are Large Language Models to Ideological Manipulation?",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the influence of ideological manipulations on LLMs",
    "checked": true,
    "id": "a10325ad749859e5c857ecdb10f3c5a674f2e0a4",
    "semantic_title": "how susceptible are large language models to ideological manipulation?",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=ZgByAYBMLX": {
    "title": "FOOL ME IF YOU CAN! An Adversarial Dataset to Investigate the Robustness of LMs in Word Sense Disambiguation",
    "volume": "review",
    "abstract": "Word sense disambiguation (WSD) is a key task in natural language processing and lexical semantics. Pre-trained language models with contextualized word embeddings have significantly improved performance in regular WSD tasks. However, these models still struggle with recognizing semantic boundaries and often misclassify homonyms in adversarial context. Therefore, we propose FOOL: FOur-fold Obscure Lexical, a new coarse-grained WSD dataset, which includes four different test sets designed to assess the robustness of language models in WSD tasks. Two sets feature typical WSD scenarios, while the other two include sentences with opposing contexts to challenge the models further. We tested two types of models on the proposed dataset: models with encoders, such as the BERT and T5 series of varying sizes by probing their embeddings, and state-of-the-art large decoder models like GPT-4o and the LlaMA3 family, using zero shot prompting. Across different state-of-the-art language models, we observed a decrease in performance in the latter two sets compared to the first two, with some models being affected more than others. We show interesting findings where small models like T5-large and BERT-large performed better than GPT-4o on Set 3 of the dataset. This indicates that, despite excelling in regular WSD tasks, these models still struggle to correctly disambiguate homonyms in artificial (Set 3) or realistic adversarial contexts (Set 4)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q3zEMBprem": {
    "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval",
    "volume": "review",
    "abstract": "Effective information retrieval (IR) from vast datasets relies on advanced techniques to extract relevant information in response to queries. Recent advancements in dense passage retrieval (DPR) have showcased remarkable efficacy compared to traditional sparse retrieval methods. To further enhance retrieval performance, knowledge distillation techniques, often leveraging robust cross-encoder rerankers, have been extensively explored. However, existing approaches primarily distill knowledge from pointwise rerankers, which assign absolute relevance scores to documents, thus facing challenges related to inconsistent standards. This paper introduces Pairwise Relevance Distillation (PairDistill) to leverage pairwise reranking, offering fine-grained distinctions between similarly relevant documents to enrich the training of dense retrieval models. Our experiments demonstrate that PairDistill outperforms existing methods, achieving new state-of-the-art results across multiple benchmarks. This highlights the potential of PairDistill in advancing dense retrieval techniques effectively. Our source code and trained models are released at https://anonymous.4open.science/r/pair-distill-AE1F",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iAIuEQKDa3": {
    "title": "One-to-Many Testing for Code Generation from (Just) Natural Language",
    "volume": "review",
    "abstract": "We adapt the popular MBPP dataset for code generation from natural language to emphasize on the natural language aspect by evaluating generated code on multiple sets of assertions. Additionally, we update the text descriptions to remove ambiguity and instructions that are not evaluated by the assertions, like specific algorithms to use. This adapted dataset resolves three problems with the original one: reliance on providing test cases to generate the right signature, contamination of the exact phrasing being present in training datasets, and better alignment between instruction and what is being tested for. We also show results on popular open and closed weight models on the original and adapted datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jihnOw3uSC": {
    "title": "Improving Causal Event Attribution in LLMs using Cross-Questions to Validate Causal Inference Assumptions",
    "volume": "review",
    "abstract": "In this paper, we address the challenge of identifying real-world events that could have caused observed anomalies in time-series data of public indicators. Previously, this was a daunting task in a data analysis pipeline due to the open-ended nature of the answer domain. However, with the advent of modern large language models (LLMs), this task appears within reach. Our experiments on three diverse public time-series datasets shows that while LLMs can retrieve meaningful events with a single prompt, they often struggle with establishing the causal validity of these events. To enhance causal validity, we design a set of carefully crafted cross-questions that check adherence to fundamental assumptions of causal inference in a temporal setting. The responses when combined through a simple feature-based classifier, improve the accuracy of causal event attributation from average of 65\\% to 90\\%. Our approach, including the questions, features, and classifier, generalizes across different datasets, serving as a meta-layer for temporal causal reasoning on event-anomaly pairs. We release our code and three datasets, which include time-series data with tagged anomalies and corresponding real-world events",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mUcrxZje0K": {
    "title": "SparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers",
    "volume": "review",
    "abstract": "The performance of Transformer models has been enhanced by increasing the number of parameters and the length of the processed text. Consequently, fine-tuning the entire model becomes a memory-intensive process. High-performance methods for parameter-efficient fine-tuning (PEFT) typically work with Attention blocks and often overlook MLP blocks, which contain about half of the model parameters. We propose a new selective PEFT method, namely SparseGrad, that performs well on MLP blocks. We transfer layer gradients to a space where only about 1\\% of the layer's elements remain significant. By converting gradients into a sparse structure, we reduce the number of updated parameters. We apply SparseGrad to fine-tune BERT and RoBERTa for the NLU task and LLaMa-2 for the Question-Answering task. In these experiments, our method provides higher quality than LoRA and MeProp, robust popular state-of-the-art PEFT approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r8qiZ0UQQd": {
    "title": "Balancing the Scales: Reinforcement Learning for Fair Classification",
    "volume": "review",
    "abstract": "Fairness in classification tasks has traditionally focused on bias removal from neural representations, but recent trends favor algorithmic methods that embed fairness into the training process. These methods steer models towards fair performance, preventing potential elimination of valuable information that arises from representation manipulation. Reinforcement Learning (RL), with its capacity for learning through interaction and adjusting reward functions to encourage desired behaviors, emerges as a promising tool in this domain. In this paper, we explore the usage of RL to address bias in multi-class classification by scaling the reward function to mitigate bias. We employ the contextual multi-armed bandit framework and adapt three popular RL algorithms to suit our objectives, demonstrating a novel approach to mitigating bias",
    "checked": true,
    "id": "a1e647fea1b4a8b3f057e583ee64fa324d3381f7",
    "semantic_title": "balancing the scales: reinforcement learning for fair classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8LeNxDkH3A": {
    "title": "Comparing Hallucination Detection Methods for Multilingual Generation",
    "volume": "review",
    "abstract": "While many hallucination detection techniques have been evaluated on English text, their effectiveness in multilingual contexts remains unknown. This paper assesses how well various factual hallucination detection metrics (lexical metrics like ROUGE and Named Entity Overlap, and Natural Language Inference (NLI)-based metrics) identify hallucinations in generated biographical summaries across languages. We compare how well automatic metrics correlate to each other and whether they agree with human judgments of factuality. Our analysis reveals that while the lexical metrics are ineffective, NLI-based metrics perform well, correlating with human annotations in many settings and often outperforming supervised models. However, NLI metrics are still limited, as they do not detect single-fact hallucinations well and fail for lower-resource languages. Therefore, our findings highlight the gaps in exisiting hallucination detection methods for non-English languages and motivate future research to develop more robust multilingual detection methods for LLM hallucinations",
    "checked": false,
    "id": "4bb30cdd7216d0bde3f937489ec04b161b44977c",
    "semantic_title": "comparing hallucination detection metrics for multilingual generation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=6bjKMg61kc": {
    "title": "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models",
    "volume": "review",
    "abstract": "The widespread use of social media necessitates reliable and efficient detection of offensive content to mitigate harmful effects. Although sophisticated models perform well on individual datasets, they often fail to generalize due to varying definitions and labeling of \"offensive content.\" In this paper, we introduce HateCOT, an English dataset with over 52,000 samples from diverse sources, featuring explanations generated by GPT-3.5Turbo and curated by humans. We demonstrate that pretraining on HateCOT significantly enhances the performance of open-source Large Language Models on three benchmark datasets for offensive content detection in both zero-shot and few-shot settings, despite differences in domain and task. Additionally, HateCOT facilitates effective K-shot fine-tuning of LLMs with limited data and improves the quality of their explanations, as confirmed by our human evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rH5IIN5FX4": {
    "title": "IotaCode: A Small Code Model Can Be Reinforced to Beat the Bigger One",
    "volume": "review",
    "abstract": "Large language models (LLMs) are one of the most rapidly developing areas of research in machine learning. To fine-tune LLMs to better align with user requests and values, reinforcement learning techniques based on human feedback (RLHF) have been developed, allowing for the inclusion of negative as well as positive examples. An important domain for the application of large language models is the analysis and generation of source code. In this study, we investigated how modern RLHF algorithms can be applied to code generation using the CodeContests problem set. The best results were achieved using the Proximal Policy Optimization algorithm, which significantly improves the supervised fine-tuning baseline, producing IotaCode model with 1.3 billion parameters that surpass the performance of the AlphaCode model with 9 billion parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Xkyy48UOK": {
    "title": "Chain of Second Thoughts: Augmenting Chain-of-Thought Prompting with General Purpose Verifiers",
    "volume": "review",
    "abstract": "Many of the recent capabilities demonstrated by Large Language Models (LLMs) arise primarily from their ability to exploit contextual information. In this paper, we explore ways to improve reasoning capabilities of LLMs through (1) exploration of different chains of thought and (2) validation of the individual steps of the reasoning process. We propose three general principles that a model should adhere to while reasoning: (i) Relevance, (ii) Mathematical Accuracy, and (iii) Logical Consistency. We apply these constraints to the reasoning steps generated by the LLM to improve the accuracy of the final generation. The constraints are applied in the form of verifiers: the model itself is asked to verify if the generated steps satisfy each constraint. To further steer the generations towards high-quality solutions, we use the perplexity of the reasoning steps as an additional verifier. We evaluate our method on 4 distinct types of reasoning tasks, spanning a total of 9 different datasets. Experiments show that our method is always better than vanilla generation, and, in 6 out of the 9 datasets, it is better than best-of N sampling which samples N reasoning chains and picks the lowest perplexity generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ShkBlFFoFT": {
    "title": "Toward an Unsupervised Method for Assessing Semantic Specificity",
    "volume": "review",
    "abstract": "In the rapidly evolving field of natural language processing (NLP), classifying and understanding semantic specificity or \"scope\" is increasingly crucial. The ability to discern whether textual content is \"general\" or \"specific\" benefits advanced systems, such as recommendation engines, to deliver more targeted and relevant content or other NLP systems that require nuanced understanding of text. Although classifying scope is an important topic, there is minimal research on how to utilize NLP techniques to determine scope. The incorporation of unsupervised learning methods in determining textual scope presents a novel approach within the NLP field. This paper expands the use of unsupervised techniques to classify text according to its semantic specificity without directly relying on vast amounts of pre-labeled data. By embedding textual data and applying heuristic clustering based on linguistic and syntactic cues, the methodology in this paper addresses the absence of direct unsupervised methods for classifying the \"scope\" of sentences or paragraphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wEwg3Ff37d": {
    "title": "How Does Quantization Affect Multilingual LLMs?",
    "volume": "review",
    "abstract": "Quantization techniques are widely used to improve inference speed and deployment of large language models. While a wide body of work examines the impact of quantized LLMs on English tasks, none have examined the effect of quantization across languages. We conduct a thorough analysis of quantized multilingual LLMs, focusing on their performance across languages and at varying scales. We use automatic benchmarks, LLM-as-a-Judge methods, and human evaluation, finding that (1) harmful effects of quantization are apparent in human evaluation, and automatic metrics severely underestimate the detriment: a 1.7% average drop in Japanese across automatic tasks corresponds to a 16.0% drop reported by human evaluators on realistic prompts; (2) languages are disparately affected by quantization, with non-Latin script languages impacted worst; and (3) challenging tasks such as mathematical reasoning degrade fastest. As the ability to serve low-compute models is critical for wide global adoption of NLP technologies, our results urge consideration of multilingual performance as a key evaluation criterion for efficient models",
    "checked": true,
    "id": "52b269168a7b5b6fa9cff1e08343d3309455dfb8",
    "semantic_title": "how does quantization affect multilingual llms?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=njKEUmn81R": {
    "title": "Resampled Datasets Are Not Enough: Mitigating Societal Bias Beyond Single Attributes",
    "volume": "review",
    "abstract": "We tackle societal bias in image-text datasets by removing spurious correlations between protected groups and image attributes. Traditional methods only target labeled attributes, ignoring biases from unlabeled ones. Using text-guided inpainting models, our approach ensures protected group independence from all attributes and mitigates inpainting biases through data filtering. Evaluations on multi-label image classification and image captioning tasks show our method effectively reduces bias without compromising performance across various models",
    "checked": true,
    "id": "eac8c19f0c7bcce7ff24dc70fd9dc8efdc73f0ce",
    "semantic_title": "resampled datasets are not enough: mitigating societal bias beyond single attributes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U7LlVdChPU": {
    "title": "Long Input Benchmark for Russian Analysis",
    "volume": "review",
    "abstract": "Recent advancements in Natural Language Processing (NLP) have fostered the development of Large Language Models (LLMs) that can solve an immense variety of tasks. One of the key aspects of their application is their ability to work with long text documents and to process long sequences of tokens. This has created a demand for proper evaluation of long-context understanding. To address this need for the Russian language, we propose LIBRA (Long Input Benchmark for Russian Analysis), which comprises 21 adapted datasets to study the LLM's abilities to understand long texts thoroughly. The tests are divided into four complexity groups and allow the evaluation of models across various context lengths ranging from 4k up to 128k tokens. We provide the open-source datasets, codebase, and public leaderboard for LIBRA to guide forthcoming research",
    "checked": true,
    "id": "774bc51e2f8a706472f32974de06d3ec0a60cbed",
    "semantic_title": "long input benchmark for russian analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dgbnkK0Dz8": {
    "title": "Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning with Insights from Multi-Agent Collaboration",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) could struggle to fully understand legal theories and perform complex legal reasoning tasks. In this study, we introduce a challenging task (confusing charge prediction) to better evaluate LLMs' understanding of legal theories and reasoning capabilities. We also propose a novel framework: Multi-Agent framework for improving complex Legal Reasoning capability (MALR). MALR employs non-parametric learning, encouraging LLMs to automatically decompose complex legal tasks and mimic human learning process to extract insights from legal rules, helping LLMs better understand legal theories and enhance their legal reasoning abilities. Extensive experiments on multiple real-world datasets demonstrate that the proposed framework effectively addresses complex reasoning issues in practical scenarios, paving the way for more reliable applications in the legal domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sgFCrAgFnH": {
    "title": "GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs",
    "volume": "review",
    "abstract": "The ability to understand and reason about spatial relationships between objects in images is an important component of visual reasoning. This skill rests on the ability to recognize and localize objects of interest and determine their spatial relation. Early vision and language models (VLMs) have been shown to struggle to recognize spatial relations. We extend the previously released What'sUp dataset and propose a novel comprehensive evaluation for spatial relationship understanding that highlights the strengths and weaknesses of 27 different models. In addition to the VLMs evaluated in What'sUp, our extensive evaluation encompasses 3 classes of Multimodal LLMs (MLLMs) that vary in their parameter sizes (ranging from 7B to 110B), training/instruction-tuning methods, and visual resolution to benchmark their performances and scrutinize the scaling laws in this task",
    "checked": true,
    "id": "7c752d0a7b2cb33295b50efc7a03afb88bed77b5",
    "semantic_title": "gsr-bench: a benchmark for grounded spatial reasoning evaluation via multimodal llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N06hbHULIP": {
    "title": "EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning",
    "volume": "review",
    "abstract": "In this paper, we present EH-MAM (Easy-to-Hard adaptive Masked Acoustic Modeling), a novel self-supervised learning approach for speech representation learning. In contrast to the prior methods that use random masking schemes for Masked Acoustic Modeling (MAM), we introduce a novel selective and adaptive masking strategy. Specifically, during SSL training, we progressively introduce harder regions to the model for reconstruction. Our approach automatically selects hard regions and is built on the observation that the reconstruction loss of individual frames in MAM can provide natural signals to judge the difficulty of solving the MAM pre-text task for that frame. To identify these hard regions, we employ a teacher model that first predicts the frame-wise losses and then decides which frames to mask. By learning to create challenging problems, such as identifying harder frames and solving them simultaneously, the model is able to learn more effective representations and thereby acquire a more comprehensive understanding of the speech. Quantitatively, EH-MAM outperforms several state-of-the-art baselines across various low-resource speech recognition and SUPERB benchmarks by 5%-10%. Additionally, we conduct a thorough analysis to show that the regions masked by EH-MAM effectively capture useful context across speech frames",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jtsk15wzv4": {
    "title": "Benchmarking Public Large Language Models in Low-resource Languages",
    "volume": "review",
    "abstract": "In recent years, Large Language Models have demonstrated impressive performances, particularly in zero-shot and few-shot learning across various languages. However, these models are often evaluated in English or high-resource languages, with limited focus on low-resource languages. This study benchmarks public LLMs which are commonly used in HuggingFace, including XGLM, Falcon, Llama, mT5-base, BLOOM, Mistral, Pegasus-Xsum, and the fine-tuned variants of mT5 and T5, on benchmark datasets in different low-resource languages. We conducted our experiments to evaluate the performance of these models across three natural language processing tasks: machine translation, text summarization, and question answering. Our evaluation results show a significant variability in performance, highlighting both the strengths and limitations of current multilingual large language models when applied to low-resource languages. Specifically, we observed that language models tend to perform better on languages with Latin alphabet, which is the most widely used in alphabetic writing, compared to those with non-Latin scripts, highlighting the need for more balanced training data",
    "checked": false,
    "id": "2dd7fbb2519676255c26615085a377fde82211fa",
    "semantic_title": "llamaturk: adapting open-source generative large language models for low-resource language",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gJGKsTG9xd": {
    "title": "Scaling laws for post-training quantized large language models",
    "volume": "review",
    "abstract": "Generalization abilities of well-trained large language models (LLMs) are known to scale predictably as a function of model size. In contrast to the existence of practical scaling laws governing pre-training, the quality of LLMs after post-training compression for efficient deployment remains highly unpredictable, often requiring case-by-case validation in practice. In this work, we attempted to close this gap for post-training weight quantization of LLMs, by conducting a systematic empirical study on multiple LLM families quantized to numerous low-precision tensor data types using popular weight quantization techniques. We identified key scaling factors pertaining to characteristics of the local loss landscape, based on which the performance of quantized LLMs can be reasonably well predicted by a statistical model",
    "checked": false,
    "id": "9b200baa28a8c30b320c61b167fce0bdd829e8ad",
    "semantic_title": "fptq: fine-grained post-training quantization for large language models",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=GTnzkwBoxO": {
    "title": "An Empirical Study of Validating Synthetic Data for Formula Generation",
    "volume": "review",
    "abstract": "Large language models (LLMs) can be leveraged to help with writing formulas in spreadsheets, but resources on these formulas are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use a(nother) model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the NL generated by the LLM is indeed accurate to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data",
    "checked": true,
    "id": "3033094629412763c356f2fb262422411bcd5020",
    "semantic_title": "an empirical study of validating synthetic data for formula generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ed7ZGhBGJ": {
    "title": "From Generation to Selection Findings of Converting Analogical Problem-Solving into Multiple-Choice Questions",
    "volume": "review",
    "abstract": "As the reasoning abilities of artificial intelligence gain more attention, generating reliable benchmarks to evaluate reasoning capabilities is becoming increasingly important. The Abstract and Reasoning Corpus (ARC) is one of the introduced reasoning benchmarks, providing challenging problems that artificial intelligence has yet to solve. While ARC has been recognized for assessing reasoning abilities, it has a limitation in that its evaluation method through generation fails to consider other aspects of assessment. Bloom's taxonomy, widely known in education, argues that good evaluation methods should evaluate the six stages of Remember, Understand, Apply, Analyze, Evaluate, and Create in a step-by-step manner. Therefore, to utilize the current ARC, which only evaluates the Create stage, for assessing Understanding, Application, and other stages, our research aimed to modify the benchmark into a multiple-choice language format to make it more suitable for evaluating large language models (LLMs), termed MC-LARC. We evaluated the analogical reasoning abilities of ChatGPT4V with MC-LARC, confirming that 1) a multiple-choice format can support the language model's reasoning capabilities and 2) facilitate evidence analysis. However, we noticed LLMs relying on shortcuts when tackling MC-LARC. By analyzing this, we identified areas to consider in multiple-choice synthesis and specified criteria for what constitutes good choices based on these findings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RGclruEFoK": {
    "title": "LLMRank: Enhancing Large Language Models for Unsupervised Keyphrase Extraction with a Candidate Graph Approach",
    "volume": "review",
    "abstract": "Keyphrase extraction is a crucial NLP task that extracts essential information from extensive texts, aiding in content summarization and browsing. This paper introduces LLMRank, a novel unsupervised keyphrase extraction method that enhances Large Language Models (LLMs) with a graph-based approach. Initially, LLMs generate a wide array of candidate keyphrases, which are then represented as nodes in a custom graph. Edges between these nodes are established based on the co-occurrence of candidates within the content, enhancing keyphrase ranking through structured contextual information. We evaluated LLMRank using three state-of-the-art LLMs across four publicly available datasets, comparing its performance against seventeen baseline models. The results demonstrate that LLMRank effectively extracts keyphrases from long and complex documents in an unsupervised manner. The source code is available on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pexlcefYkj": {
    "title": "Multi-Hop Table Retrieval for Open-Domain Text-to-SQL",
    "volume": "review",
    "abstract": "The open-domain text-to-SQL task aims to retrieve question-relevant tables from massive databases and then generate SQL. However, the performance of current methods is limited by single-hop retrieval, and existing multi-hop retrieval of other tasks cannot be directly applied due to two primary challenges: error cascades and the tendency to retrieve tables similar to the retrieved ones but irrelevant to the question. Therefore, we propose the multi-hop table retrieval with rewrite and beam search (Murre). To reduce error cascades, our method employs beam search to select multiple tables at each hop. To avoid retrieving similar but irrelevant tables, we remove the retrieved information from the question, guiding the retriever to focus on unretrieved tables. We conduct experiments on two open-domain text-to-SQL datasets, achieving an average improvement of 5.7% over the previous state-of-the-art results",
    "checked": true,
    "id": "df0ad2f70946034174a4a6d814423de69d1e00d8",
    "semantic_title": "multi-hop table retrieval for open-domain text-to-sql",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=kEScimnD9Y": {
    "title": "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models",
    "volume": "review",
    "abstract": "Vision-language models (VLMs) have gained widespread adoption in both industry and academia. In this study, we propose a unified framework for systematically evaluating gender, race, and age biases in VLMs with respect to professions. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. We create a synthetic, high-quality dataset comprising text and images that intentionally obscure gender, race, and age distinctions across various professions. The dataset includes action-based descriptions of each profession and serves as a benchmark for evaluating societal biases in vision-language models (VLMs). In our benchmarking of popular vision-language models (VLMs), we observe that different input-output modalities result in distinct bias magnitudes and directions. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code",
    "checked": true,
    "id": "b9e96c330c8e458c02c57c0b8ae38f8a667eb316",
    "semantic_title": "a unified framework and dataset for assessing societal bias in vision-language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EYkQFEc2MG": {
    "title": "Hypothesis Generation with Large Language Models",
    "volume": "review",
    "abstract": "Effective generation of novel hypotheses is instrumental to scientific progress. So far, researchers have been the main powerhouse behind hypothesis generation by painstaking data analysis and thinking (also known as the Eureka moment). In this paper, we examine the potential of large language models (LLMs) to generate hypotheses. We focus on hypothesis generation based on data (i.e., labeled examples). To enable LLMs to handle long contexts, we generate initial hypotheses from a small number of examples and then update them iteratively to improve the quality of hypotheses. Inspired by multi-armed bandits, we design a reward function to inform the exploitation-exploration tradeoff in the update process. Our algorithm is able to generate hypotheses that enable much better predictive performance than few-shot prompting in classification tasks, improving accuracy by 31.7\\% on a synthetic dataset and by 13.9\\%, 3.3\\% and, 24.9\\% on three real-world datasets. We also outperform supervised learning by 12.1\\% and 11.6\\% on two challenging real-world datasets. Furthermore, we find that the generated hypotheses not only corroborate human-verified theories but also uncover new insights for the tasks",
    "checked": true,
    "id": "975b23dd7f82ea53ccf9a0c6a5d65368fb19c3d8",
    "semantic_title": "hypothesis generation with large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=BvFyvkzs41": {
    "title": "Improving Linguistic Diversity of Large Language Models with Possibility Exploration Fine-Tuning",
    "volume": "review",
    "abstract": "While Large Language Models (LLMs) have made significant strides in replicating human-like abilities, there are concerns about a reduction in the linguistic diversity of their outputs. This results in the homogenization of viewpoints and perspectives, as well as the underrepresentation of specific demographic groups. Although several fine-tuning and prompting techniques have been suggested to tackle the issue, they are often tailored to specific tasks or come with a substantial increase in computational cost and latency. This makes them challenging to apply to applications that demand very low latency, such as chatbots and virtual assistants. We propose Possibility Exploration Fine-Tuning (PEFT), a task-agnostic framework that enhances the text diversity of LLMs without increasing latency or computational cost. Given the same prompt, models fine-tuned with PEFT can simultaneously generate multiple diverse responses, each corresponding with a controllable possibility number. Experiments with Mistral 7B and LLAMA 2 on open-domain dialogue generation demonstrate that PEFT significantly enhances output diversity, as shown by a lower similarity between candidate responses. As PEFT focuses more on semantic diversity rather than lexical diversity, it can remarkably reduce demographic bias in dialogue systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=221GeR7I7i": {
    "title": "Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge",
    "volume": "review",
    "abstract": "Recent studies have highlighted the presence of cultural biases in Large Language Models (LLMs), yet often lack a robust methodology to dissect these phenomena comprehensively. Our work aims to bridge this gap by delving into the Food domainâ€”a universally relevant yet culturally diverse aspect of human life. We introduce FmLAMA, a multilingual dataset centered on food-related cultural facts and variations in food practices. We analyze LLMs across various architectures and configurations, evaluating their performance in both monolingual and multilingual settings. By leveraging templates in six different languages, we investigate how LLMs interact with language-specific and cultural knowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias towards food knowledge prevalent in the United States; (2) Incorporating relevant cultural context significantly improves LLMs' ability to access cultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is highly dependent on the interplay between the probing language, the specific model architecture, and the cultural context in question. This research underscores the complexity of integrating cultural understanding into LLMs and emphasizes the importance of culturally diverse datasets to mitigate biases and enhance model performance across different cultural domains",
    "checked": true,
    "id": "e8fc7d4ab83dc28ff4628bbc62188c3438e5ea41",
    "semantic_title": "does mapo tofu contain coffee? probing llms for food-related cultural knowledge",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=2OwDyu9xR3": {
    "title": "Relation Extraction with Instance-Adapted Predicate Descriptions",
    "volume": "review",
    "abstract": "Relation extraction (RE) is a standard information extraction task playing a major role in downstream applications such as knowledge discovery and question answering. Although decoder-only large language models are excelling in generative tasks, smaller encoder models are still the go to architecture for RE. In this paper, we revisit fine-tuning such smaller models using a novel dual-encoder architecture with a joint contrastive and cross-entropy loss. Unlike previous methods that employ a fixed linear layer for predicate representations, our approach uses a second encoder to compute instance-specific predicate representations by infusing them with real entity spans from corresponding input instances. We conducted experiments on three different RE datasets from both general and biomedical domains. Our approach achieved F1 score improvements ranging from 1% to 2% over state-of-the-art methods with a simple but elegant formulation. Ablation studies justify the importance of various components built into the proposed architecture",
    "checked": false,
    "id": "5c78be55221f0685714d13dfe9e0fafa762c96e7",
    "semantic_title": "nearside: structured knowledge extraction framework from species descriptions",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=MHknR4bWrz": {
    "title": "MKT: A Multi-Stage Knowledge Transfer Framework to Mitigate Catastrophic Forgetting in Multi-Domain Chinese Spelling Correction",
    "volume": "review",
    "abstract": "Chinese Spelling Correction (CSC) aims to detect and correct spelling errors in given sentences. Recently, multi-domain CSC has gradually attracted the attention of researchers because it is more practicable. In this paper, we focus on the key flaw of the CSC model when adapting to multi-domain scenarios: the tendency to forget previously acquired knowledge upon learning new domain-specific knowledge (i.e., $\\textbf{catastrophic forgetting}$). To address this, we propose a novel model-agnostic $\\textbf{M}$ulti-stage $\\textbf{K}$nowledge $\\textbf{T}$ransfer ($\\textbf{MKT}$) framework with an evolving teacher model and dynamic distillation weights for knowledge transfer in each domain, rather than focusing solely on new domain knowledge. It deserves to be mentioned that we are the first to apply continual learning methods to the multi-domain CSC task. Experiments prove our method's effectiveness over traditional approaches, highlighting the importance of overcoming catastrophic forgetting to enhance model performance",
    "checked": false,
    "id": "86eee1d440bda13abe6ae9f17935a676b616c5ab",
    "semantic_title": "mitigating catastrophic forgetting in multi-domain chinese spelling correction by multi-stage knowledge transfer framework",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Q2K8ijV4bY": {
    "title": "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning",
    "volume": "review",
    "abstract": "The tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs, while tool-free methods chose another track: augmenting math reasoning data. However, a great method to integrate the above two research paths and combine their advantages remains to be explored. In this work, we firstly include new math questions via multi-perspective data augmenting methods and then synthesize code-nested solutions to them. The open LLMs (i.e., Llama-2) are finetuned on the augmented dataset to get the resulting models, MuMath-Code ($\\mu$-Math-Code). During the inference phase, our MuMath-Code generates code and interacts with the external python interpreter to get the execution results. Therefore, MuMath-Code leverages the advantages of both the external tool and data augmentation. To fully leverage the advantages of our augmented data, we propose a two-stage training strategy: In Stage-1, we finetune Llama-2 on pure CoT data to get an intermediate model, which then is trained on the code-nested data in Stage-2 to get the resulting MuMath-Code. Our MuMath-Code-7B achieves 83.8 on GSM8K and 52.4 on MATH, while MuMath-Code-70B model achieves new state-of-the-art performance among open methods---achieving 90.7% on GSM8K and 55.1% on MATH. Extensive experiments validate the combination of tool use and data augmentation, as well as our two-stage training strategy. We release the proposed dataset along with the associated code for public use",
    "checked": true,
    "id": "512a5c307fdab29112a0f4af5c94a3436632eda1",
    "semantic_title": "mumath-code: combining tool-use large language models with multi-perspective data augmentation for mathematical reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aCGxn1jpFe": {
    "title": "GPT makes a poor AMR parser",
    "volume": "review",
    "abstract": "Much of Abstract Meaning Representation (AMR) parsing is currently concentrated on fine-tuning pre-trained language models. Large Language Models (LLMs) bring a new paradigm for NLP research, prompting. LLMs also show impressive 'reasoning' capabilities and a certain kind of interpretability with Chain-of-Thought (CoT) prompting. In this paper, we apply a variety of prompting strategies to induce GPT to do AMR parsing. We demonstrate that GPT models are insufficient as AMR parsers, but CoT prompting may shed light on how errors arise",
    "checked": false,
    "id": "6dcce0d1e2176adc520f09e7437f6a557ef06c4c",
    "semantic_title": "gpt-4 as an interface between researchers and computational software: improving usability and reproducibility",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LCBMGy3qjc": {
    "title": "Future Events as Backdoor Triggers: Investigating Temporal Vulnerabilities in LLMs",
    "volume": "review",
    "abstract": "Backdoors are hidden behaviors that are only triggered once an AI system has been deployed. Bad actors looking to create successful backdoors must design them to avoid activation during training and evaluation. Since data used in these stages often only contains information about events that have already occurred, a simple backdoor trigger could be a model recognizing data that is in the future relative to when it was trained. Through prompting experiments and by probing internal activations, we show that current large language models (LLMs) can distinguish past from future events, with probes on model activations achieving 90% accuracy. We train models with backdoors triggered by a temporal distributional shift; they activate when the model is exposed to news headlines beyond their training cut-off dates. Fine-tuning on helpful, harmless and honest (HHH) data does not work well for removing simpler backdoor triggers but is effective on our backdoored models, although this distinction is smaller for the larger-scale model we tested. We also find that an activation-steering vector representing a model's internal representation of the date influences the rate of backdoor activation. We take these results as initial evidence that, at least for models at the modest scale we test, standard safety measures are enough to remove these backdoors",
    "checked": true,
    "id": "802295ca0c1ebfdf32291eaafdaefa5643dc36e4",
    "semantic_title": "future events as backdoor triggers: investigating temporal vulnerabilities in llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=tA08E8bKoH": {
    "title": "Protecting Privacy in Classifiers by Token Manipulation",
    "volume": "review",
    "abstract": "Using language models as a remote service entails sending private information to an untrusted provider. In addition, potential eavesdroppers can intercept the messages, thereby exposing the information. In this work, we explore the prospects of avoiding such data exposure at the level of text manipulation. We focus on text classification models, examining various token mapping and contextualized manipulation functions in order to see whether classifier accuracy may be maintained while keeping the original text unrecoverable. We find that although some token mapping functions are easy and straightforward to implement, they heavily influence performance on the downstream task, and via a sophisticated attacker can be reconstructed. In comparison, the contextualized manipulation provides an improvement in performance",
    "checked": true,
    "id": "6f0a78365bbdc9c4d210ac7788eb7145ba706cf1",
    "semantic_title": "protecting privacy in classifiers by token manipulation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5pKv0gNLBz": {
    "title": "Explicit Inductive Inference using Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are reported to hold undesirable attestation bias on inference tasks: when asked to predict if a premise $P$ entails a hypothesis $H$, instead of considering $H$'s conditional truthfulness entailed by $P$, LLMs tend to use the out-of-context truth label of $H$ as a fragile proxy. In this paper, we propose a pipeline that exploits this bias to do explicit inductive inference. Our pipeline uses an LLM to transform a premise into a set of attested alternatives, and then aggregate answers of the derived new entailment inquiries to support the original inference prediction. On a directional predicate entailment benchmark, we demonstrate that by applying this simple pipeline, we can improve the overall performance of LLMs on inference and substantially alleviate the impact of their attestation bias",
    "checked": false,
    "id": "5824788bc6b355e1a655add875b100541aef4b59",
    "semantic_title": "finding inductive loop invariants using large language models",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=OykupORtFB": {
    "title": "Detecting Synthetic Lyrics with Few-Shot Inference",
    "volume": "review",
    "abstract": "In recent years, generated content in music has gained significant popularity, with large language models being effectively utilized to produce human-like lyrics in various styles, themes, and linguistic structures. This technological advancement supports artists in their creative processes but also raises issues of authorship infringement, consumer satisfaction and content spamming. To address these challenges, methods for detecting generated lyrics are necessary. However, existing works have not yet focused on this specific modality or on creative text in general regarding machine-generated content detection methods and datasets. In response, we have curated the first dataset of high-quality synthetic lyrics and conducted a comprehensive quantitative evaluation of various few-shot content detection approaches, testing their generalization capabilities and complementing this with a human evaluation. Our best few-shot detector, based on LLM2Vec, surpasses stylistic and statistical methods, which shown competitive in other domains at distinguishing human-written from machine-generated content. It also shown good generalization capabilities to new artists and models, and effectively detects post-generation paraphrasing. This study emphasizes the need for further research on creative content detection, particularly in terms of generalization and scalability with larger songs catalogs. All datasets, pre-processing scripts, and code are available publicly on GitHub and Hugging Face under the Apache 2.0 license",
    "checked": true,
    "id": "fe7dd6a3968b51e4628aed843c2ef4816d9e50c1",
    "semantic_title": "detecting synthetic lyrics with few-shot inference",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PzOLpU6ofN": {
    "title": "Towards Textless Multilingual Audio Question Answering (TM-AQA) System Using Audio-MAMBA",
    "volume": "review",
    "abstract": "Audio Question Answering (AQA) is a complex task in Multi-Modal Learning, where a system interprets audio inputs and associated questions to produce appropriate answers. Previous AQA research has primarily focused on text-based queries, exploration into spoken questions in languages like English has been limited. Since speech is a primary mode of communication, integrating spoken queries could significantly enhance AQA system capabilities. To bridge this gap, this paper introduces a Spoken AQA system utilizing the Textless Multilingual Audio Question Answering (TM-AQA) dataset. This dataset comprises 107,514 question-answer pairs in English, Hindi, and Bengali, derived from 1991 environmental audio recordings corresponding to various environmental scenes. The study establishes baseline performance metrics by evaluating several multimodal (MML) AQA frameworks that employ diverse acoustic features and architectures. The experimental results demonstrate that the proposed Audio-MAMBA (A-MAMBA) based MML framework, incorporating a Continuous Scanning Mechanism (CSM), surpasses Transformer-based MML frameworks in performance and computational efficiency",
    "checked": false,
    "id": "dd3491a632e4bca6e67b17caf8e831248e958689",
    "semantic_title": "tm-pathvqa:90000+ textless multilingual questions for medical visual question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mv3Ix4srg3": {
    "title": "Post Training Quantization of Large Language Models with Microscaling Formats",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have distinguished themselves with outstanding performance in complex language modeling tasks, yet they come with significant computational and storage challenges. This paper explores the potential of quantization to mitigate these challenges. We systematically study the combined application of three well-known post-training techniques, SmoothQuant, AWQ, and GPTQ, and provide a comprehensive analysis of their interactions and implications for advancing LLM quantization. We enhance the versatility of these techniques by enabling quantization to microscaling (MX) formats, expanding their applicability beyond their initial fixed-point format targets. We show that combining different PTQ methods enables us to quantize models to 4-bit weights and 8-bit activations using the MXINT format with negligible accuracy loss compared to the uncompressed baseline",
    "checked": false,
    "id": "0af048fd211a6db0c86c4c24e8f00550591cffb0",
    "semantic_title": "lrq: optimizing post-training quantization for large language models by learning low-rank weight-scaling matrices",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=etw0Iseq0r": {
    "title": "On the Relationship between Truth and Political Bias in Language Models",
    "volume": "review",
    "abstract": "Language model alignment research often attempts to ensure that models are not only helpful and harmless, but also truthful and unbiased. However, optimizing these objectives simultaneously can obscure how improving one aspect might impact the others. In this work, we focus on analyzing the relationship between two concepts essential in both language model alignment and political science: truthfulness and political bias. We train reward models on various popular truthfulness datasets and subsequently evaluate their political bias. Our findings reveal that optimizing reward models for truthfulness on these datasets tends to result in a left-leaning political bias. We also find that existing open-source reward models (i.e. those trained on standard human preference datasets) already show a similar bias and that the bias is larger for larger models. These results raise important questions about both the datasets used to represent truthfulness and what language models capture about the relationship between truth and politics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AJJZfN6jdv": {
    "title": "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference",
    "volume": "review",
    "abstract": "As large language models (LLMs) constantly evolve, ensuring their safety remains a critical research issue. Previous red teaming approaches for LLM safety have primarily focused on single prompt attacks or goal hijacking. To the best of our knowledge, we are the first to study LLM safety in multi-turn dialogue coreference. We created a dataset of 1,400 questions across 14 categories, each featuring multi-turn coreference safety attacks. We then conducted detailed evaluations on five widely used open-source LLMs. The results indicated that under multi-turn coreference safety attacks, the highest attack success rate was 56% with the LLaMA2-Chat-7b model, while the lowest was 13.9% with the Mistral-7B-Instruct model. These findings highlight the safety vulnerabilities in LLMs during dialogue coreference interactions",
    "checked": true,
    "id": "b0f8b91c2ca91803324069bc237b15314f70e6ca",
    "semantic_title": "cosafe: evaluating large language model safety in multi-turn dialogue coreference",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dxTO4dYbrS": {
    "title": "Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning",
    "volume": "review",
    "abstract": "Personalization in large language models (LLMs) is increasingly important, aiming to align the LLMs' interactions, content, and recommendations with individual user preferences. Recent advances have highlighted effective prompt design by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these methods faced limitations due to a lack of model ownership, resulting in constrained customization and privacy issues, and often failed to capture complex, dynamic user behavior patterns. To address these shortcomings, we introduce One PEFT Per User (OPPU), employing personalized parameter-efficient fine-tuning (PEFT) modules to store user-specific behavior patterns and preferences. By plugging in personal PEFT parameters, users can own and use their LLMs individually. OPPU integrates parametric user knowledge in the personal PEFT parameters with non-parametric knowledge from retrieval and profiles, adapting LLMs to user behavior shifts. Experimental results demonstrate that OPPU significantly outperforms existing prompt-based methods across seven diverse tasks in the LaMP benchmark. Further studies reveal OPPU's enhanced capabilities in handling user behavior shifts, modeling users at different activity levels, maintaining robustness across various user history formats, and displaying versatility with different PEFT methods",
    "checked": true,
    "id": "6551817cbcbd182a58d19f6d2f62ddb93a4d7865",
    "semantic_title": "democratizing large language models via personalized parameter-efficient fine-tuning",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=MmHSjAKH4U": {
    "title": "Measuring Psychological Depth in Language Models",
    "volume": "review",
    "abstract": "Evaluations of creative stories generated by large language models (LLMs) often focus on objective properties of the text, such as its style, coherence, and toxicity. While these metrics are indispensable, they do not speak to a story's subjective, psychological impact from a reader's perspective. We introduce the Psychological Depth Scale (PDS), a novel framework rooted in literary theory that measures an LLM's ability to produce authentic and narratively complex stories that provoke emotion, empathy, and engagement. We empirically validate our framework by showing that humans can consistently evaluate stories based on PDS (0.72 Krippendorff's alpha). We also explore techniques for automating the PDS to easily scale future analyses. GPT-4o, combined with a novel Mixture-of-Personas (MoP) prompting strategy, achieves an average Spearman correlation of 0.51 with human judgment while Llama-3-70B scores as high as 0.68 for empathy. Finally, we compared the depth of stories authored by both humans and LLMs. Surprisingly, GPT-4 stories either surpassed or were statistically indistinguishable from highly-rated human-written stories sourced from Reddit. By shifting the focus from text to reader, the Psychological Depth Scale is a validated, automated, and systematic means of measuring the capacity of LLMs to connect with humans through the stories they tell",
    "checked": true,
    "id": "de69c6b8b5eae28d45bbac31364175a9abfb826c",
    "semantic_title": "measuring psychological depth in language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SCttYXtNIa": {
    "title": "RecLLMSim: A Comprehensive Task-based Recommendation Conversation Dataset Generated by Large Language Models",
    "volume": "review",
    "abstract": "Conversational systems have garnered significant attention and importance in recent years. However, collecting conversational datasets has traditionally been a time-consuming and labor-intensive process. With the advent of large language models (LLMs), there is a growing interest in using them to generate synthetic datasets. Given LLMs' strong role-playing capabilities, they hold the potential to simulate users effectively. This capability allows for the automated generation of conversations, with LLMs acting as both users and assistants across various scenarios. Our study proposes a framework designed to generate task-based recommendation conversation datasets across multiple scenarios with LLMs. We have created a comprehensive conversational dataset using this framework, and the dataset is named RecLLMSim. We conducted extensive experiments to measure the quality of the user simulator and the assistant, and annotated user intent and hallucinations to improve its usability. Experimental results demonstrate that using LLMs as user simulators is a promising approach. Besides, the generated RecLLMSim dataset can be adapted for various tasks such as user profiling and simulation, offering a rich resource for further advancements in conversational systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=azYizvxfLh": {
    "title": "Benchmarking LLMs on the Semantic Overlap Summarization Task",
    "volume": "review",
    "abstract": "Semantic Overlap Summarization (SOS) is a constrained multi-document summarization task, where the constraint is to capture the common/overlapping information between two alternative narratives. While recent advancements in Large Language Models (LLMs) have achieved exceptional performance in numerous summarization tasks, a benchmarking study of the SOS task using LLMs is yet to be performed. As LLMs' responses are highly sensitive to variations in prompt design, a major challenge in conducting such a benchmarking study is to systematically explore a variety of prompts before drawing a reliable conclusion. Fortunately, the TELeR taxonomy has been recently proposed, which can be used to design and explore various prompts for LLMs. Using this TELeR taxonomy, this paper comprehensively evaluates 16 popular LLMs on the SOS Task",
    "checked": true,
    "id": "f5f6da6dc15403a31b1f8e44273a37084e495ba5",
    "semantic_title": "benchmarking llms on the semantic overlap summarization task",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XJEmvFwrkT": {
    "title": "ILENS: Iterative Logical Enhancement via Neurosymbolic Computation and Common Sense",
    "volume": "review",
    "abstract": "Trained on internet-scale datasets, large language models (LLMs) excel in tasks relying on surface patterns and exhibit strong common sense knowledge. However, their performance decreases on tasks requiring deeper reasoning steps. Recent techniques aim to combine the strengths of both reasoning programs and LLMs by converting natural language problems into formal logic specifications, thereby enhancing reasoning task performance. Despite these advancements, LLMs often struggle with ambiguities and complex cases, leading to reasoning errors in the formal method step. In this paper, based on the observation that LLMs can provide the implicit common sense facts when asked explicitly, we propose \\textsc{iLens} (\\textbf{I}terative \\textbf{L}ogical \\textbf{E}nhancement via \\textbf{N}eurosymbolic Computation and Common \\textbf{S}ense), a new iterative neurosymbolic system for logical inferences which integrates the two systems in an iterative manner. Initially, we translate the problem specifications into AMR graphs, and then convert them into first-order logic (FOL) expressions to minimize inaccurate interpretations from natural language to FOL. Subsequently, we use formal theorem provers (Prover9, Mace4) to deduce the conclusion. Within this process, we ask the theorem prover to generate counterexamples based on the given premises when the theorem prover fails to provide a definite answer, then prompting the LLM to identify any implicit common sense facts. These facts are then incorporated back into the theorem to attempt proof completion. Through the iterative steps and leveraging the GPT-4 API in conjunction with Prover9 and Mace4, our new proposed \\textsc{iLens} system significantly reduces uncertain and error cases and achieves 80.22\\% accuracy on the challenging FOLIO dataset, setting a new state of the art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5vjZJ56miF": {
    "title": "Benchmarking Language Models for Offensive Sentences Classification in Offensive Nepali Roman Multi-Label Dataset",
    "volume": "review",
    "abstract": "This paper presents a comprehensive methodology for benchmarking and evaluating multiple language models to detect offensive language in Romanized Nepali text. Recognizing Nepali as a low-resource language, we introduce the Offensive Nepali Roman Multi-label Dataset (ONRMD), labeled for abuse, scam, sexual, and neutral content,specifically designed for this study. We employ various models, including BERT-base-multilingual-cased, RoBERTa-base, distilbert-base-nepali, FastText, and LASER + CNN, and compare their performance on the ONRMD. Our approach encompasses thorough preprocessing and tokenization of the dataset, followed by training and evaluation using standard metrics such as accuracy, precision, recall, and F1 score. Additionally, we conduct human evaluations with two distinct groups to further validate our findings, given the novelty of our dataset and the absence of a standard baseline. The results demonstrate the potential of these models in effectively handling the nuances of Romanized Nepali text for offensive language detection. This study serves as a foundation for future research involving other pre-trained language models and multilingual datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zS9v4KyNFZ": {
    "title": "Using Linguistic Synchrony to Evaluate Large Language Models for Cognitive Behavioral Therapy",
    "volume": "review",
    "abstract": "Synchrony, the responsive communication between interacting individuals, is a crucial process in building a strong relationship between a mental health therapist and their client, leading to positive therapeutic outcomes. However, so far synchrony has not been investigated as a measure of efficacy of large language models (LLMs) delivering mental health therapy. In this work, we evaluate the linguistic synchrony of an LLM (ChatGPT 3.5-turbo) in a mental health dialog setting by first validating a computational measure of linguistic synchrony with two measures of the quality of client self-disclosures--intimacy and engagement ($p < 0.05$). We then compare the linguistic synchrony of the LLM to trained therapists and non-expert online peer supporters in a Cognitive Behavioral Therapy (CBT) setting. We show that the LLM is outperformed by humans with respect to linguistic synchrony ($p < 0.001$). These results support the need to be cautious in using LLMs in mental health applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tkbzXpY4hD": {
    "title": "Learning Personalized Alignment for Evaluating Open-ended Text Generation",
    "volume": "review",
    "abstract": "With rapid progress made in language qualities such as fluency and consistency via large language models (LLMs), there has been increasing interest in assessing alignment with diverse human preferences. Traditional metrics heavily rely on lexical similarity with human-written references and have been observed to suffer from a poor correlation with human evaluation. Furthermore, they ignore the diverse preferences of humans, a key aspect in evaluating open-ended tasks like story generation. Inspired by these challenges, we introduce an interpretable open-ended evaluation framework PerSE to assess the alignment with a specific human preference. It is tuned to deduce the specific preference from a given personal profile and evaluate the alignment between the generation and the personal preference. PerSE also explains its assessment by a detailed comment or several fine-grained scores. This enhances its interpretability, making it more suitable to tailor a personalized generation. Our 13B LLaMA-2-based PerSE shows a 15.8% increase in Kendall correlation and a 13.7% rise in accuracy on zero-shot reviewers compared to GPT-4. It also outperforms GPT-4 by 46.01% in the Kendall correlation on new domains, indicating its transferability",
    "checked": true,
    "id": "c81fd487a418268d42dce8613236297a9dc127fc",
    "semantic_title": "learning personalized alignment for evaluating open-ended text generation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=DVuo4r9ATq": {
    "title": "The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown capabilities close to human performance in various analytical tasks, leading researchers to use them for time and labor-intensive analyses. However, their capability to handle highly specialized and open-ended tasks in domains like policy studies remains in question. This paper investigates the efficiency and accuracy of LLMs in specialized tasks through a structured user study focusing on Human-LLM partnership. The study, conducted in two stagesâ€”Topic Discovery and Topic Assignmentâ€”integrates LLMs with expert annotators to observe the impact of LLM suggestions on what is usually human-only analysis. Results indicate that LLM-generated topic lists have significant overlap with human generated topic lists, with minor hiccups in missing document-specific topics. However, LLM suggestions may significantly improve task completion speed, but at the same time introduce anchoring bias, potentially affecting the depth and nuance of the analysis, raising a critical question about the trade-off between increased efficiency and the risk of biased analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HlXuNuHRf4": {
    "title": "HaloRAG: Towards Mitigating LLM Hallucinations with Low-Cost Real-Time Retrieval",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) often struggle to stay up-to-date due to their reliance on static datasets, leading to outdated responses and hallucinations. We introduce HaloRAG, a cost-efficient agentic wrapper that enhances LLMs with real-time information retrieval using advanced web scraping technologies. Leveraging semantic searches and Retrieval-Augmented Generation (RAG), this wrapper fetches, validates, and summarizes up-to-date web data, extending the LLM's knowledge base without retraining. This method significantly enhances the accuracy and relevance of LLM responses, particularly for queries requiring the latest information. Comparative analysis indicates that the wrapper-enhanced LLM outperforms models like GPT-3.5 and Claude on queries involving recent events and emerging technologies. This work advocates for integrating real-time data retrieval techniques to significantly reduce hallucinations and extend the practical applicability of LLMs across various domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OJoXQOKsJK": {
    "title": "Optimizing Class-Level Code Generation: Enhancing In-Context Learning in Large Language Models with Pruning Techniques",
    "volume": "review",
    "abstract": "Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Such generation focuses on generating independent and often method-level code, thus leaving it unclear how LLMs perform in generating more complicated tasks. To fill this research gap, researchers have studied to construct prompt to achieve good performance in complicated code generation, i.e., class-level code generation, and they have launched a corresponding benchmark, ClassEval, on generating and evaluating class-level code. However, the obvious difficulty of class-level code generation prompt construction lies in that class-level prompt has longer texts than those of method-level code generation, and at the same time the input length of the released models always has a limitation, such as: GPT-3.5 and GPT-4 with 4096 tokens and 8192 tokens limitations respectively.\\\\ Therefore, it is important to research how to construct the class-level prompt by pruning some code tokens. Through the pruning strategy, we add more code examples into prompt to deliver as many semantic information as possible to LLMs. We introduce a new pruning strategy, namely attention-guided strategy, to this research point. By this pruning strategy, we conduct experiments on code generation by GPT-3.5, a kind of LLM proved to achieve excellent performance on generation tasks. In our work, we adopt ClassEval benchmark dataset specialized for class-level code generation to conduct our experiments. Additionally, we evaluate the strategy both in method-level and class-level metrics, finding that this pruning strategy is effective to prune appropriate tokens for LLM to generate class-level code. Above all, attention-guided strategy outperforms the randomly pruning strategy with 4.2\\%, 10.2\\% and 13\\% higher class-level code generation accuracy by LLM. We also analyze the impact of the quantity of code reduction on the quality of code generation in LLM, concluding that pruning under 40\\% of code snippets with extra 4 examples included can take great advantage of the intelligence of LLM to contribute to perfect class-level code generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k88OiuUp2j": {
    "title": "A Survey on Misinformation Prevention and Detection methods in Large Language Models",
    "volume": "review",
    "abstract": "The rapid advancement of large language models (LLMs) has significantly impacted various fields within natural language processing (NLP). However, the issue of misinformation has become increasingly prominent, necessitating urgent solutions. Recent studies have categorized misinformation into two types: unintentional misinformation, often resulting from hallucinations, and intentional misinformation, which is deliberately created and spread by malicious actors. This paper provides a comprehensive survey of recent approaches to mitigating both types of misinformation in LLMs. It explores internal and external prevention methods, along with various techniques for misinformation tracing and detection. By evaluating the strengths and weaknesses of these approaches, this survey aims to illuminate the direction for future research in addressing misinformation in LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jlrqe4XQ6S": {
    "title": "Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts",
    "volume": "review",
    "abstract": "Personalized large language models (LLMs) aim to tailor interactions, content, and recommendations to individual user preferences. While parameter-efficient fine-tuning (PEFT) methods excel in performance and generalization, they are costly and limit communal benefits when used individually. To this end, we introduce Personalized Pieces (Per-Pcs), a framework that allows users to safely share and assemble personalized PEFT efficiently with collaborative efforts. Per-Pcs involves selecting sharers, breaking their PEFT into pieces, and training gates for each piece. These pieces are added to a pool, from which target users can select and assemble personalized PEFT using their history data. This approach preserves privacy and enables fine-grained user modeling without excessive storage and computation demands. Experimental results show Per-Pcs outperforms non-personalized and PEFT retrieval baselines, offering performance comparable to OPPU with significantly lower resource use across six tasks. Further analysis highlights Per-Pcs's robustness concerning sharer count and selection strategy, pieces sharing ratio, and scalability in computation time and storage space. Per-Pcs's modularity promotes safe sharing, making LLM personalization more efficient, effective, and widely accessible through collaborative efforts",
    "checked": true,
    "id": "64dbe50f4bf7b37f395437f13028a06624aa70e8",
    "semantic_title": "personalized pieces: efficient personalized large language models through collaborative efforts",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6altEjJnVK": {
    "title": "Automated Evaluation of Large Vision-Language Models on Self-driving Corner Cases",
    "volume": "review",
    "abstract": "Large Vision-Language Models (LVLMs) have received widespread attention in advancing the interpretable self-driving. Existing evaluations of LVLMs primarily focus on the multi-faceted capabilities in natural circumstances, lacking automated and quantifiable assessment for self-driving, let alone the severe road corner cases. In this paper, we propose CODA-LM, the very first benchmark for the automatic evaluation of LVLMs for self-driving corner cases. We adopt a hierarchical data structure to prompt powerful LVLMs to analyze complex driving scenes and generate high-quality pre-annotation for human annotators, and for LVLM evaluation, we show that using the text-only large language models (LLMs) as judges reveals even better alignment with human preferences than the LVLM judges. Moreover, with CODA-LM, we build CODA-VLM, a new driving LVLM surpassing all the open-sourced counterparts on CODA-LM. Our CODA-VLM performs comparably with GPT-4V, even surpassing GPT-4V by +21.42% on the regional perception task. We hope CODA-LM can become the catalyst to promote interpretable self-driving empowered by LVLMs",
    "checked": true,
    "id": "decf9cb628638642d2c6cdf843d01376fff5e0e5",
    "semantic_title": "automated evaluation of large vision-language models on self-driving corner cases",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=tAr6Kso5hm": {
    "title": "Prompt4LJP: Prompt Learning for Legal Judgement Prediction",
    "volume": "review",
    "abstract": "The task of Legal Judgment Prediction (LJP) involves predicting court decisions based on the facts of the case, including identifying the applicable law article, the charge, and the term of penalty. While neural methods have made significant strides in this area, they often fail to fully harness the rich semantic potential of language models (LMs). Prompt learning, a novel approach that reformulates downstream tasks as cloze-style or prefix-style prediction challenges for Masked Language Models using specialized prompt templates, has shown considerable promise across various Natural Language Processing (NLP) domains. However, the dynamic word lengths typical in LJP labels present a challenge to the standard prompt templates designed for single-word [MASK] tokens commonly used in many NLP tasks. To address this gap, we introduce the Prompt4LJP framework, a pioneering method tailored to incorporate the knowledge of LMs into the LJP task by effectively accommodating dynamic word lengths in labels. This framework leverages a dual-slot prompt template and correlation scoring to maximize the utility of LMs without requiring additional resources or complex tokenization schemes. Our method significantly outperforms current state-of-the-art techniques on the CAIL-2018 dataset, thereby enhancing the accuracy and reliability of LJP. This contribution not only advances the field of LJP but also demonstrates a novel application of prompt learning to complex tasks involving dynamic word lengths",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dTdoZql621": {
    "title": "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly",
    "volume": "review",
    "abstract": "Compilers are complex software containing millions of lines of code, taking years to develop. This paper investigates to what extent Large Language Models (LLMs) can replace hand-crafted compilers in translating high-level programming languages to machine instructions, using C to x86 assembly as a case study. We identify two challenges of using LLMs for code translation and introduce two novel data pre-processing techniques to address the challenges: numerical value conversion and training data resampling. While only using a 13B model, our approach achieves a behavioral accuracy of over 91\\%, outperforming the much larger GPT-4 Turbo model by over 50\\%. Our results are encouraging, showing that LLMs have the potential to transform how compilation tools are constructed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w4Z0xj0hzG": {
    "title": "Temporal Cognitive Tree: A Hierarchical Modeling Approach for Event Temporal Relation Extraction",
    "volume": "review",
    "abstract": "Understanding and analyzing event temporal relations is a crucial task in Natural Language Processing (NLP). This task, known as Event Temporal Relation Extraction (ETRE), aims to identify and extract temporal connections between events in text. Recent studies focus on locating the relative position of event pairs on the timeline by designing logical expressions or auxiliary tasks to predict their temporal occurrence. Despite these advances, this modeling approach neglects the multidimensional information in temporal relation and the hierarchical process of reasoning. In this study, we propose a novel hierarchical modeling approach for this task by introducing a Temporal Cognitive Tree (TCT) that mimics human logical reasoning. Additionally, we also design a integrated model incorporating prompt optimization and deductive reasoning to exploit multidimensional supervised information. Extensive experiments on TB-Dense and MATRES datasets demonstrate that our approach outperforms existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ct8iMUwc9m": {
    "title": "Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from LLMs",
    "volume": "review",
    "abstract": "In search settings, calibrating the scores during the ranking process to quantities such as click-through rates or relevance levels enhances a system's usefulness and trustworthiness for downstream users. While previous research has improved this notion of calibration for low complexity learning-to-rank models, the larger data demands and parameter count specific to modern neural text rankers produce unique obstacles that hamper the efficacy of methods intended for the learning-to-rank setting. This paper proposes exploiting large language models (LLMs) to provide relevance and uncertainty signals for these neural text rankers to produce scale-calibrated scores through Monte Carlo sampling of natural language explanations (NLEs). Our approach transforms the neural ranking task from ranking textual query-document pairs to ranking corresponding synthesized NLEs. Comprehensive experiments on two popular document ranking datasets show that the NLE-based calibration approach consistently outperforms past calibration methods and LLM-based methods for ranking, calibration, and query performance prediction tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pv1T9sgbeF": {
    "title": "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities",
    "volume": "review",
    "abstract": "Perceiving and understanding non-speech sounds and non-verbal speech is essential to making decisions that help us interact with our surroundings. In this paper, we propose GAMA, a novel General-purpose Large Audio-Language Model (LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. We build GAMA by integrating an LLM with multiple types of audio representations, including features from a custom Audio Q-Former, a multi-layer aggregator that aggregates features from multiple layers of an audio encoder. We fine-tune GAMA on a large-scale audio-language dataset, which augments it with audio understanding capabilities. Next, we propose CompA-R (Instruction-Tuning for Complex Audio Reasoning), a synthetically generated instruction-tuning (IT) dataset with instructions that require the model to perform complex reasoning on the input audio. We instruction-tune GAMA with CompA-R to endow it with complex reasoning abilities, where we further add a soft prompt as input with high-level semantic evidence by leveraging event tags of the input audio. Finally, we also propose CompA-R-test, a human-labeled evaluation dataset for evaluating the capabilities of LALMs on open-ended audio question-answering that requires complex reasoning. Through automated and expert human evaluations, we show that GAMA outperforms all other LALMs in literature on diverse audio understanding tasks by margins of 1%-84%. Further, GAMA IT-ed on CompA-R proves to be superior in its complex reasoning and instruction following capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XNobSD0MI9": {
    "title": "Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing",
    "volume": "review",
    "abstract": "In visual speech processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual Speech Processing incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to perform multi-tasks of visual speech recognition and translation, where the given instructions control the type of task. The input video is mapped to the input latent space of an LLM by employing a self-supervised visual speech model. Focused on the fact that there is redundant information in input frames, we propose a novel deduplication method that reduces the embedded visual features by employing visual speech units. Through the proposed deduplication and Low Rank Adaptation (LoRA), VSP-LLM can be trained in a computationally efficient manner. In the translation dataset, the MuAViC benchmark, we demonstrate that VSP-LLM trained on just 30 hours of labeled data can more effectively translate lip movements compared to the recent model trained with 433 hours of data",
    "checked": true,
    "id": "187de1107f760e10c52d29e9cd720fa63cb01ea8",
    "semantic_title": "where visual speech meets language: vsp-llm framework for efficient and context-aware visual speech processing",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=mjWhyqmABo": {
    "title": "Gender Bias in Nepali-English Machine Translation: A Comparison of LLMs and Existing MT Systems",
    "volume": "review",
    "abstract": "Bias in Nepali NLP is seldom addressed due to its classification as a low-resource language, perpetuating biases in subsequent systems. Our work addresses gender bias in Nepali-English machine translation. With the advent of Large Language Models, there is an opportunity to mitigate this bias. We quantify and evaluate gender bias by building an occupation corpus and contextualizing three gender-bias challenge sets for Nepali. While gender bias is prominent in existing translation systems, LLMs perform better in both gender-neutral and gender-specific contexts. Despite their quirks, LLMs can be a valuable alternative to traditional machine learning systems for culture-rich languages like Nepali",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZSvYMHFgbL": {
    "title": "``Let's Argue Both Sides'': Argument Generation Can Force Models to Utilize Previously Inaccessible Reasoning Capabilities",
    "volume": "review",
    "abstract": "Large Language Models (LLMs), despite achieving state-of-the-art results in a number of evaluation tasks, struggle to maintain their performance when logical reasoning is strictly required to correctly infer a prediction. In this work, we propose Argument Generation as a method of forcing models to utilize their reasoning capabilities when other approaches such as chain-of-thought reasoning prove insufficient. Our method involves the generation of arguments for each possible inference result, and asking the end model to rank the generated arguments. We show that Argument Generation can serve as an appropriate substitute for zero-shot prompting techniques without the requirement to add layers of complexity. Furthermore, we argue that knowledge-probing techniques such as chain-of-thought reasoning and Argument Generation are only useful when further reasoning is required to infer a prediction, making them auxiliary to more common zero-shot approaches. Finally, we demonstrate that our approach forces larger gains in smaller language models, showcasing a complex relationship between model size and prompting methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1p3h7Y7BeD": {
    "title": "Document-Level Event Extraction Based on Multi-instance Learning",
    "volume": "review",
    "abstract": "Current research focuses on utilizing intradocument information for event extraction, but it has limitations in capturing the complexity and diversity of events because it overlooks the relationships between documents. Additionally, current instance learning methods for event extraction primarily focus on similaritybased instance retrieval, failing to emphasize comprehensive model learning, and a single measure of similarity cannot fully reflect the semantics of a document. To address these issues, this paper proposes an event extraction model based on multi-instance learning, exploring the connections between documents through event types and event arguments. We designed multiple instance selection strategies and construction methods to enable the model to achieve a more thorough understanding of events. Furthermore, we implemented a two-stage training approach to optimize the model's ability to learn from instances obtained through different instances. Experiments conducted on the RAMS and WIKIEVENTS datasets demonstrate that our method surpasses the current state-of-the-art models in terms of F1 scores, validating its effectiveness and superiority",
    "checked": false,
    "id": "f0d9d22a83020e2583d70400c9991dd37e7c4d0b",
    "semantic_title": "rauie: a relation-augmented document-level event extraction model based on uie",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=scXinOTZio": {
    "title": "Multimodal Large Language Models \"Foresee\" Objects Based on Verb Information But Not Gender",
    "volume": "review",
    "abstract": "This study employs the classical psycholinguistics paradigm, the visual world eye-tracking paradigm (VWP), to explore the predictive capabilities of multimodal large language models (MLLMs) and compare them with human anticipatory gaze behaviors. Specifically, we examine the attention weight distributions of LLAVA when presented with visual displays and sentences containing verb and gender cues. Our findings reveal that LLAVA, like humans, can predictively attend to objects relevant to verbs, but fails to demonstrate gender-based anticipatory attention. Layer-wise analysis indicates that the middle layers of the model are more related to predictive attention than the early or late layers. This study is pioneering in applying psycholinguistic paradigms to compare the multimodal predictive attention of humans and MLLMs, revealing both similarities and differences between them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2oS2kmTWZt": {
    "title": "Socially Aware Language Technologies: Perspectives and Practices",
    "volume": "review",
    "abstract": "Language technologies have made enormous progress, especially with the introduction of large language models (LLMs). These advances can, however, exacerbate a variety of issues that models have traditionally struggled with, such as bias, evaluation, and risks. In this perspective paper, we argue that many of these issues share a common core: a lack of awareness of the factors, context, and implications of the social environment in which NLP operates, which we call social awareness. While NLP is getting better at solving the linguistic aspects, relatively limited progress has been made in adding the social awareness required for language applications to work in all situations for all users. Integrating social awareness into NLP models will make applications more natural, helpful, and safe, and will open up new possibilities. Thus we argue that substantial challenges remain for NLP to develop social awareness and that we are just at the beginning of a new era for the field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FOQTQPBzPY": {
    "title": "A Context-Aware Approach for Enhancing Data Imputation with Pre-trained Language Models",
    "volume": "review",
    "abstract": "This paper presents a novel approach named Contextually Relevant Imputation leveraging pre-trained Language Models (CRILM) for handling missing data in tabular datasets, complementing existing numeric-estimation methods. Instead of relying on traditional numerical estimations, CRILM uses pre-trained language models (LMs) to create contextually relevant descriptors for missing values. This method aligns datasets with LMs' strengths, allowing large LMs to generate these descriptors and small LMs to be fine-tuned on the enriched datasets for enhanced downstream task performance. Our evaluations demonstrate CRILM's superior performance and robustness across MCAR, MAR, and challenging MNAR scenarios, with up to a 10% improvement over the best-performing baselines. By mitigating biases, particularly in MNAR settings, CRILM improves downstream task performance and offers a cost-effective solution for resource-constrained environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=thTaXZDeJC": {
    "title": "Are Large Language Models All You Need for Temporal Knowledge Graph Forecasting?",
    "volume": "review",
    "abstract": "While temporal knowledge graph forecasting (TKGF) approaches have traditionally relied heavily on complex graph neural network architectures, recent advances in large language models (LLMs) and in-context learning (ICL) have presented promising out-of-the-box alternatives. However, little is known about LLMs' limitations and generalization capabilities for TKGF. In this study, we conduct a comparative analysis of complexity (\\textit{e.g.}, more number of hops) and sparsity (\\textit{e.g.}, relation frequency) confounders between LLMs and supervised models using two weakly annotated TKGF benchmarks. Our experimental results showcase that while LLMs perform on par or outperform supervised models in low-complexity scenarios, their effectiveness diminishes in more complex settings (\\textit{e.g.}, multi-step, more number of hops, etc.) where supervised models maintain superior performance",
    "checked": false,
    "id": "7941f1b3202eb27d6f12346249863bbe23219670",
    "semantic_title": "species distribution, abundance and survival modeling: new opportunities and methods",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cSX19q9Jne": {
    "title": "Unifying Multimodal Retrieval via Document Screenshot Embedding",
    "volume": "review",
    "abstract": "In the real world, documents are organized in different formats and varied modalities. Traditional retrieval pipelines require tailored document parsing techniques and content extraction modules to prepare input for indexing. This process is tedious, prone to errors, and has information loss. To this end, we propose Document Screenshot Embedding (DSE), a novel retrieval paradigm that regards document screenshots as a unified input format, which does not require any content extraction preprocess and preserves all the information in a document (e.g., text, image and layout). DSE leverages a large vision-language model to directly encode document screenshots into dense representations for retrieval. To evaluate our method, we first craft the dataset of Wiki-SS, a 1.3M Wikipedia web page screenshots as the corpus to answer the questions from the Natural Questions dataset. In such a text-intensive document retrieval setting, DSE shows competitive effectiveness compared to other text retrieval methods relying on parsing. For example, DSE outperforms BM25 by 17 points in top-1 retrieval accuracy. Additionally, in a mixed-modality task of slide retrieval, DSE significantly outperforms OCR text retrieval methods by over 15 points in nDCG@10. These experiments show that DSE is an effective document retrieval paradigm for diverse types of documents. Model checkpoints, code, and Wiki-SS collection will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v5YGQK1qCP": {
    "title": "Improving Referring Ability for Biomedical Language Models",
    "volume": "review",
    "abstract": "Existing auto-regressive large language models (LLMs) are primarily trained using documents from general domains. In the biomedical domain, continual pre-training is a prevalent method for domain adaptation to inject professional knowledge into powerful LLMs that have been pre-trained in general domains. Previous studies typically conduct standard pre-training by randomly packing multiple documents into a long pre-training sequence. Recently, some existing works suggest that enhancing the relatedness of documents within the same pre-training sequence may be advantageous. However, these studies primarily focus on general domains, which cannot be readily applied in the biomedical domain where the distinction of fine-grained topics is harder. Is it possible to further improve the pre-training for biomedical language models (LMs) using exactly the same corpus? In this paper, we explore an improved approach to continual pre-training, which is a prevalent method for domain adaptation, by utilizing information from the citation network in this challenging scenario. Empirical studies demonstrate that our proposed LinkLM data improves both the intra-sample and inter-sample referring abilities of auto-regressive LMs in the biomedical domain, encouraging more profound consideration of task-specific pre-training sequence design for continual pre-training",
    "checked": false,
    "id": "dac3a172b504f4e33c029655e9befb3386e5f63a",
    "semantic_title": "emergent abilities of large language models",
    "citation_count": 1673,
    "authors": []
  },
  "https://openreview.net/forum?id=o7GOyn0QJe": {
    "title": "An Efficient Approach for Studying Cross-Lingual Transfer in Multilingual Language Models",
    "volume": "review",
    "abstract": "The capacity and effectiveness of pre-trained multilingual models (MLMs) for zero-shot cross-lingual transfer is well established. However, phenomena of positive or negative transfer, and the effect of language choice still need to be fully understood, especially in the complex setting of massively multilingual LMs. We propose an \\textit{efficient} method to study transfer language influence in zero-shot performance on another target language. Unlike previous work, our approach \\textit{disentangles downstream tasks from language}, using dedicated adapter units. Our findings suggest that some languages do not largely affect others, while some languages, especially ones unseen during pre-training, can be extremely beneficial or detrimental for different target languages. We find that no transfer language is beneficial for all target languages. We do, curiously, observe languages previously unseen by MLMs consistently benefit from transfer from \\textit{almost any} language. We additionally use our modular approach to quantify negative interference efficiently and categorize languages accordingly. Furthermore, we provide a list of promising transfer-target language configurations that consistently lead to target language performance improvements",
    "checked": true,
    "id": "41173fa8cb93f51a4f8e2c023a2767c4f03e2ad0",
    "semantic_title": "an efficient approach for studying cross-lingual transfer in multilingual language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IcPpNETR2y": {
    "title": "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "Retrieval-augmented generation (RAG) offers an effective approach for addressing question answering (QA) tasks. However, the imperfections of the retrievers in RAG models often result in the retrieval of irrelevant information, which could introduce noises and degrade the performance, especially when handling multi-hop questions that require multiple steps of reasoning. To enhance the multi-hop reasoning ability of RAG models, we propose TRACE. TRACE constructs knowledge-grounded reasoning chains, which are a series of logically connected knowledge triples, to identify and integrate supporting evidence from the retrieved documents for answering questions. Specifically, TRACE employs a KG Generator to create a knowledge graph (KG) from the retrieved documents, and then uses an Autoregressive Reasoning Chain Constructor to build reasoning chains. Experimental results on three multi-hop QA datasets show that TRACE achieves an average performance improvement of up to 14.03% compared to using all the retrieved documents. Moreover, the results indicate that using reasoning chains as context, rather than the entire documents, is often sufficient to correctly answer questions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1cW5OIJFhT": {
    "title": "Low-Resource Machine Translation through the Lens of Personalized Federated Learning",
    "volume": "review",
    "abstract": "We present a new approach based on the Personalized Federated Learning algorithm MeritFed that can be applied to Natural Language Tasks with heterogeneous data. We evaluate it on the Low-Resource Machine Translation task, using the dataset from the Large-Scale Multilingual Machine Translation Shared Task (Small Track \\#2) and the subset of Sami languages from the multilingual benchmark for Finno-Ugric languages. In addition to its effectiveness, MeritFed is also highly interpretable, as it can be applied to track the impact of each language used for training. Our analysis reveals that target dataset size affects weight distribution across auxiliary languages, that unrelated languages do not interfere with the training, and auxiliary optimizer parameters have minimal impact. Our approach is easy to apply with a few lines of code, and we provide scripts for reproducing the experiments",
    "checked": true,
    "id": "f9a1fa85ab5346fd5df0ee94d58f80e341f48f53",
    "semantic_title": "low-resource machine translation through the lens of personalized federated learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CXsrhCqnKI": {
    "title": "Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP",
    "volume": "review",
    "abstract": "Image-text contrastive models like CLIP have wide applications in zero-shot classification, image-text retrieval, and transfer learning. However, they often struggle on compositional visio-linguistic tasks (e.g., attribute-binding or object-relationships) where their performance is no better than random chance. To address this, we introduce SDS-CLIP, a lightweight and sample-efficient distillation method to enhance CLIP's compositional visio-linguistic reasoning. Our approach fine-tunes CLIP using a distillation objective borrowed from large text-to-image generative models like Stable-Diffusion, which are known for their strong visio-linguistic reasoning abilities. On the challenging Winoground benchmark, SDS-CLIP improves the visio-linguistic performance of various CLIP models by up to 7%, while on the ARO dataset, it boosts performance by up to 3%. This work underscores the potential of well-designed distillation objectives from generative models to enhance contrastive image-text models with improved visio-linguistic reasoning capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ddlMkvyErA": {
    "title": "FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models",
    "volume": "review",
    "abstract": "Foundation models have demonstrated remarkable capabilities in handling diverse modalities and tasks, outperforming conventional artificial intelligence (AI) approaches that are highly task-specific and modality-reliant. In the medical domain, however, the development of comprehensive foundation models is constrained by limited access to diverse modalities and stringent privacy regulations. To address these constraints, this study introduces a novel knowledge injection approach, FedKIM, designed to scale the medical foundation model within a federated learning framework. FedKIM leverages lightweight local models to extract healthcare knowledge from private data and integrates this knowledge into a centralized foundation model using a designed adaptive Multitask Multimodal Mixture Of Experts (M$^3$OE) module. This method not only preserves privacy but also enhances the model's ability to handle complex medical tasks involving multiple modalities. Our extensive experiments across twelve tasks in seven modalities demonstrate the effectiveness of FedKIM in various settings, highlighting its potential to scale medical foundation models without direct access to sensitive data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YWGUisSIMz": {
    "title": "Dynamic Few-Shot Learning for Knowledge Graph Question Answering",
    "volume": "review",
    "abstract": "Large language models present opportunities for innovative Question Answering over Knowledge Graphs (KGQA). However, they are not inherently designed for query generation. To bridge this gap, solutions have been proposed that rely on fine-tuning or ad-hoc architectures, achieving good results but limited out-of-domain distribution generalization. In this study, we introduce a novel approach called Dynamic Few-Shot Learning (DFSL). DFSL integrates the efficiency of in-context learning and semantic similarity and provides a generally applicable solution for KGQA with state-of-the-art performance. We run an extensive evaluation across multiple benchmark datasets and architecture configurations",
    "checked": true,
    "id": "75ec7010174da085a5f280eb7e1487bbed19435e",
    "semantic_title": "dynamic few-shot learning for knowledge graph question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rxnhpYt6i1": {
    "title": "GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization",
    "volume": "review",
    "abstract": "Heterogeneous graph neural networks have recently gained attention for long document summarization, modeling the extraction as a node classification task. Although effective, these models often require external tools or additional machine learning models to define graph components, producing highly complex and less intuitive structures. We present GraphLSS, a heterogeneous graph for long document extractive summarization, incorporating Lexical, Structural, and Semantic features. It defines two levels of information (words and sentences) and four types of edges (sentence semantic similarity, sentence occurrence order, word in sentence, and word semantic similarity) without requiring auxiliary learning models. Experiments on two benchmark datasets show that GraphLSS is competitive with top-performing graph-based methods, outperforming recent non-graph models. We release our code on \\url{<anonymized>}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QHGaAX9OOH": {
    "title": "SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering",
    "volume": "review",
    "abstract": "Audio-Visual Question Answering (AVQA) is a challenging task that involves answering questions based on both auditory and visual information in videos. A significant challenge is interpreting complex multi-modal scenes, which include both visual objects and sound sources, and connecting them to the given question. In this paper, we introduce the Source-aware Semantic Representation Network (SaSR-Net), a novel model designed for AVQA. SaSR-Net utilizes source-wise learnable tokens to efficiently capture and align audio-visual elements with the corresponding question. It streamlines the fusion of audio and visual information using spatial and temporal attention mechanisms to identify answers in multi-modal scenes. Extensive experiments on the Music-AVQA and AVQA datasets show that SaSR-Net outperforms state-of-the-art AVQA methods. We will release our source code and pre-trained models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4gS6YvnXXN": {
    "title": "Interpretable Catastrophic Forgetting of Large Language Model Fine-tuning via Instruction Vector",
    "volume": "review",
    "abstract": "Fine-tuning large language models (LLMs) can cause them to lose their general capabilities. However, the intrinsic mechanisms behind such forgetting remain unexplored. In this paper, we begin by examining this phenomenon by focusing on knowledge understanding and instruction following, with the latter identified as the main contributor to forgetting during fine-tuning. Consequently, we propose the Instruction Vector (IV) framework to capture model representations highly related to specific instruction-following capabilities, thereby making it possible to understand model-intrinsic forgetting. Through the analysis of IV dynamics pre and post-training, we suggest that fine-tuning mostly adds specialized reasoning patterns instead of erasing previous skills, which may appear as forgetting. Building on this insight, we develop IV-guided training, which aims to preserve original computation graph, thereby mitigating catastrophic forgetting. Empirical tests on three benchmarks confirm the efficacy of this new approach, supporting the relationship between IVs and forgetting. Our code will be made available soon",
    "checked": true,
    "id": "dd7a5ff60d019974bccd945dc3a5fc82fd1dee16",
    "semantic_title": "interpretable catastrophic forgetting of large language model fine-tuning via instruction vector",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k44TLajHss": {
    "title": "MemAgent: A cache inspired framework for augmenting conversational Web Agents with task-specific information",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown promise as web agents, but their current limitations hinder their widespread adoption for general users. A critical issue behind this is the misalignment between user expectations and the agent's actions due to ineffective communication leading to a lack of crucial context required for successful task completion. To address this gap, we propose MemAgent, a novel pipeline for LLM-based web agents. Inspired by caching mechanisms, MemAgent incorporates a memory component to store task-specific information. This memory bank enables the LLM agent to proactively query for supplementary context relevant to the current task, thereby reducing user interaction overhead. Our evaluations demonstrate that MemAgent significantly enhances the agent's performance and usabilities, bringing us a step closer to seamless LLM integration in web agent technologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2UOqOvaLUn": {
    "title": "Are Large Language Models (LLMs) Good Social Predictors?",
    "volume": "review",
    "abstract": "With the recent advancement of Large Language Models (LLMs), efforts have been made to leverage LLMs in crucial social science study methods, including predicting human features of social life such as presidential voting. Existing works suggest that LLMs are capable of generating human-like responses. Nevertheless, it is unclear how well LLMs work and where the plausible predictions derive from. This paper critically examines the performance of LLMs as social predictors, pointing out the source of correct predictions and limitations. Based on the notion of mutability that classifies social features, we design three realistic settings and a novel social prediction task, where the LLMs make predictions with input features of the same mutability and accessibility with the response feature. We find that the promising performance achieved by previous studies is because of input shortcut features to the response, which are hard to capture in reality; the performance degrades dramatically to near-random after removing the shortcuts. With the comprehensive investigations on various LLMs, we reveal that LLMs struggle to work as expected on social prediction when given ordinarily available input features without shortcuts. We further investigate possible reasons for this phenomenon and suggest potential ways to enhance LLMs for social prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=weLpjmdAFt": {
    "title": "Instance-Level Dynamic LoRAs Composition for Cross-Task Generalization",
    "volume": "review",
    "abstract": "Large language models perform well on tasks that have undergone fine-tuning of instructions, but their performance to completely unseen tasks is often less than ideal. To overcome the challenge of cross-task generalization, task-level LoRA combination is proposed, which does not require training a model for new tasks. Instead, it learns the LoRA combination weights based on a small number of samples to form the task model. However, task-level LoRA combination only utilize a few task modules due to its reliance on the weight enumeration method, and it also overlooks the specificity between different instances. Therefore, we proposed an instance-level LoRA composition for cross-task generalization, which selects appropriate multiple task LoRAs for each input instance and dynamically determines the composition weights. Our experiments on publicly available datasets show that our method outperforms the typical method, LoraHub, in 16 out of 27 tasks. We release the source code at https://github.com/noname822/iLoraComp.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=buKTXCa0aT": {
    "title": "PclGPT: A Large Language Model for Patronizing and Condescending Language Detection",
    "volume": "review",
    "abstract": "Disclaimer: Samples in this paper may be harmful and cause discomfort! Patronizing and condescending language (PCL) is a form of speech directed at vulnerable groups. As an essential branch of toxic language, this type of language exacerbates conflicts and confrontations among Internet communities and detrimentally impacts disadvantaged groups. Traditional pre-trained language models (PLMs) perform poorly in detecting PCL due to its implicit toxicity traits like hypocrisy and false sympathy. With the rise of large language models (LLMs), we can harness their rich emotional semantics to optimize PCL detection. In this paper, we introduce PclGPT, a comprehensive LLM benchmark designed specifically for PCL. We collect, annotate, and integrate the Pcl-PT/SFT dataset, and then develop a bilingual PclGPT-EN/CN model group through a comprehensive pre-training and supervised fine-tuning staircase process to facilitate cross-language detection. Group detection results and fine-grained detection from PclGPT and other models reveal significant variations in the degree of bias in PCL towards different vulnerable groups, necessitating increased societal attention to protect them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IxNM13HBVJ": {
    "title": "Extending Cognitive Reframing Therapy: Multimodal Support and Multi-hop Psychotherapeutic Reasoning",
    "volume": "review",
    "abstract": "Previous studies have shown that Large Language Models (LLMs) have significant potential in supporting cognitive reframing therapy. However, these studies have primarily focused on uni-modal therapy, often overlooking the importance of the client's non-verbal cues. Identifying non-verbal emotions plays a crucial role in effective communication and is considered a central skill in psychotherapy. To alleviate this gap, we extend the concept of cognitive reframing conversation to multimodality. Specifically, we present a new dataset called Multi Modal-Cognitive Support Conversation (MM-CSConv), which pairs each dialogue with an image of the client's facial expression. Additionally, we introduce a multi-hop psychotherapeutic reasoning approach to enhance the capabilities of Vision-Language Models (VLMs) as psychotherapists. This approach uses multi-hop reasoning over the conversations, incorporating implicit evidence crucial in psychotherapy. Our extensive experiments with both LLMs and VLMs show that the abilities of VLMs as psychotherapists are significantly enhanced through the MM-CSConv. Moreover, the multi-hop psychotherapeutic reasoning method allows VLMs to offer more rational and empathetic suggestions, outperforming standard prompting methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y2eUPuHQ2C": {
    "title": "A Chain-of-Task Framework for Instruction Tuning of LLMs Based on Chinese Grammatical Error Correction",
    "volume": "review",
    "abstract": "Over-correction is a critical issue for large language models (LLMs) to address Grammatical Error Correction (CGEC) task, esp. for Chinese. This paper proposes a Chain-of-Task (CoTask) framework to reduce over-correction. The CoTask framework is applied as multi-task instruction tuning of LLMs by decomposing the process of grammatical error analysis to design auxiliary tasks and adjusting the combinations and sequence orders of tasks. A curriculum learning strategy is also presented to enhance the performance of LLMs, together with an algorithm for automatic dataset annotation to avoid additional manual cost. Experimental results demonstrate that our method achieves new state-of-the-art results on the on both FCGEC (in-domain) and NaCGEC (out-of-domain) test sets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=355wzFydL4": {
    "title": "Good Idea or Not, Representation of LLM Could Tell",
    "volume": "review",
    "abstract": "In the ever-expanding landscape of academic research, the proliferation of ideas presents a significant challenge for researchers: discerning valuable ideas from the less impactful ones. The ability to efficiently evaluate the potential of these ideas is crucial for the advancement of science and paper review. In this work, we focus on idea assessment, which aims to leverage the knowledge of large language models to assess the merit of scientific ideas. First, we investigate existing text evaluation research and define the problem of quantitative evaluation of ideas. Second, we curate and release a benchmark dataset from nearly four thousand manuscript papers with full texts, meticulously designed to train and evaluate the performance of different approaches to this task. Third, we establish a framework for quantifying the value of ideas by employing representations in a specific layer of large language models. Experimental results show that the scores predicted by our method are relatively consistent with those of humans. Our findings suggest that the representations of large language models hold more potential in quantifying the value of ideas than their generative outputs, demonstrating a promising avenue for automating the idea assessment process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Di4SpgOszX": {
    "title": "Dual Diffusion Learning for Knowledge-Grounded Dialogue Generation",
    "volume": "review",
    "abstract": "Knowledge-grounded dialogue generation plays a crucial role in the intelligent conversational agents. However, previous work suffers from inadequate control information in both knowledge selection and dialogue generation. Firstly, priori-based knowledge selection lacks a posteriori distribution, while posterior-based methods suffer from biases at the inference and training stages. Secondly, the conventional autoregressive generation lacks precise control over the injection of knowledge, leading to unintended shifts in focus of response. To address these limitations, we propose a Controllable Dual Diffusion Learning model, which serves as an enhanced framework for knowledge-grounded dialogue generation through the controllable modules. Our approach formulates response generation and knowledge generation as dual tasks to fully leverage prior and posterior knowledge, and to avoid training and inference biases. We optimize knowledge selection by employing knowledge labels generated by the dual module and iteratively update the generated dialogue with global-related knowledge information. Experimental results on two public datasets demonstrate that our approach achieves significant improvements in both automatic and manual evaluations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6nIFhDa5Ma": {
    "title": "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage",
    "volume": "review",
    "abstract": "In-context learning (ICL) adapts Large Language Models (LLMs) to new tasks, without requiring any parameter updates, but few annotated examples as input. In this work, we investigate selective annotation for ICL, where there is a limited budget for annotating examples, similar to low-budget active learning (AL). Although uncertainty-based selection is unreliable with few annotated data, we present CoverICL, an adaptive graph-based selection algorithm, that effectively incorporates uncertainty sampling into selective annotation for ICL. First, CoverICL builds a nearest-neighbor graph based on the semantic similarity between candidate ICL examples. Then, CoverICL employs uncertainty estimation by the LLM to identify hard examples for the task. Selective annotation is performed over the active graph of the hard examples, adapting the process to the particular LLM used and the task tackled. CoverICL selects the most representative examples by solving a Maximum Coverage problem, approximating diversity-based sampling. Extensive experiments on nine datasets and six LLMs show that, by incorporating uncertainty via coverage on the active graph, CoverICL (1) outperforms existing AL methods for ICL by 2--4.6\\% accuracy points, (2) is up to 2x more budget-efficient than SOTA methods for low-budget AL, and (3) generalizes better across tasks compared to non-graph alternatives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=db1WNQ79wx": {
    "title": "Concept Bottleneck Large Language Models",
    "volume": "review",
    "abstract": "We introduce the Concept Bottleneck Large Language Model (CB-LLM), a pioneering approach to creating inherently interpretable Large Language Models (LLMs). Unlike traditional black-box LLMs that rely on post-hoc interpretation methods with limited neuron function insights, CB-LLM sets a new standard with its built-in interpretability, scalability, and ability to provide clear, accurate explanations. We investigate two essential tasks in the NLP domain: text classification and text generation. In text classification, CB-LLM narrows the performance gap with traditional black-box models and provides clear interpretability. In text generation, we show how interpretable neurons in CB-LLM can be used for concept detection and steering text generation. Our CB-LLMs enable greater interaction between humans and LLMs across a variety of tasks --- a feature notably absent in existing LLMs",
    "checked": false,
    "id": "9cfb4fd70cee64cd0d300472147c4fda7962c93b",
    "semantic_title": "towards concept-aware large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=H8vXUkIlVk": {
    "title": "Argument Mining with LLaMA 8B",
    "volume": "review",
    "abstract": "An end-to-end argument mining (AM) pipeline takes a text as input and provides the argumentative structure of this text as output, by identifying and classifying the argument units and relations within it. In this work, we focus on LLM fine-tuning approach to AM. We model the three sub-tasks of the AM pipeline as text generation tasks. We fine-tune classical and quantized versions of LLaMA--3, the most capable open-source model available, on the benchmark Persuasive Essays (PE) dataset. We consider various contextual and structural fine-tuning modalities, where the AM sub-tasks are modeled either at the paragraph or at the essay level, with or without inclusion of additional markup tags. We achieve state-of-the-art results on all three sub-tasks, with significant improvements over previous benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VOdZlvtlJz": {
    "title": "MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding",
    "volume": "review",
    "abstract": "Improving user experience and providing personalized search results in E-commerce platforms heavily rely on understanding purchase intention. However, existing methods for acquiring large-scale intentions bank on distilling large language models with human annotation for verification. Such an approach tends to generate product-centric intentions, overlook valuable visual information from product images, and incurs high costs for scalability. To address these issues, we introduce MIND, a multimodal framework that allows Large Vision-Language Models (LVLMs) to infer purchase intentions from multimodal product metadata and prioritize human-centric ones. Using Amazon Review data, we apply MIND and create a multimodal intention knowledge base, which contains 1,264,441 intentions derived from 126,142 co-buy shopping records across 107,215 products. Extensive human evaluations demonstrate the high plausibility and typicality of our obtained intentions and validate the effectiveness of our distillation framework and filtering mechanism. Further experiments reveal the positive downstream benefits that MIND brings to intention comprehension tasks and highlight the importance of multimodal generation and role-aware filtering. Additionally, MIND shows robustness to different prompts and superior generation quality compared to previous methods",
    "checked": true,
    "id": "72099c9eae74859e5389b3806732ed1b7f2ce69b",
    "semantic_title": "mind: multimodal shopping intention distillation from large vision-language models for e-commerce purchase understanding",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ZJFgdmfAfT": {
    "title": "ClauseQA: Enhancing Customized Clause Extraction in Large Language Models via Instruction Following",
    "volume": "review",
    "abstract": "Contract review is a critical and time-consuming task for lawyers, involving the identification of key clauses that may pose potential risks. However, previous methods trained on predefined taxonomies struggle to generalize to meet varying requirements. To address this limitation, we propose ClauseQA, a framework to adapt large language models (LLMs) to extract clauses by following instructions with customized clause descriptions. Additionally, we introduce an out-of-distribution setting for recognizing unseen clause categories, investigating how supervised fine-tuning (SFT) affects LLMs' generalization. Our experiments show that SFT significantly reduces hallucinations while making LLMs more cautious in providing positive answers, which can sometimes lead to lower recall. Furthermore, we observe that SFT tends to induce the original pre-training capability in decoder-only models like Llama3, whereas encoder-decoder models, such as Flan-T5, fit the SFT data more closely and thus show less robustness to distribution shifts. Finally, we discuss potential directions for future research. Our code and models will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JF4qyxaAJo": {
    "title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
    "volume": "review",
    "abstract": "The pace of scientific research, vital for improving human life, is complex, slow, and needs specialized expertise. Meanwhile, novel, impactful research often stems from both a deep understanding of prior work, and a cross-pollination of ideas across domains and fields. To enhance the productivity of researchers, we propose ResearchAgent, which leverages the encyclopedic knowledge and linguistic reasoning capabilities of Large Language Models (LLMs) to assist them in their work. This system automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents. Specifically, starting with a core scientific paper, ResearchAgent is augmented not only with relevant publications by connecting information over an academic graph but also entities retrieved from a knowledge store derived from shared underlying concepts mined across numerous papers. Then, mimicking a scientific approach to improving ideas with peer discussions, we leverage multiple LLM-based ReviewingAgents that provide reviews and feedback via iterative revision processes. These reviewing agents are instantiated with human preference-aligned LLMs whose criteria for evaluation are elicited from actual human judgments via LLM prompting. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showing its effectiveness in generating novel, clear, and valid ideas based on both human and model-based evaluation results. Our initial foray into AI-mediated scientific research has important implications for the development of future systems aimed at supporting researchers in their ideation and operationalization of novel work",
    "checked": true,
    "id": "51b7b3ad7645a69e3c1c80cae69473b8bd472f67",
    "semantic_title": "researchagent: iterative research idea generation over scientific literature with large language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=ny51IA44q8": {
    "title": "Leveraging Large Models for Evaluating Novel Content: A Case Study on Advertisement Creativity",
    "volume": "review",
    "abstract": "Evaluating creativity is a challenging task, even for humans, not only because it is a subjective call, but also because it involves complex cognitive processes such as decomposition and drawing unlikely connections. Inspired by previous work in marketing, we attempt to break down creativity into atypicality and originality and collect fine-grained human annotation on these categories. With controlled experiments with vision language models (VLM), we evaluate the alignment between models and humans by a suite of novel tasks. Our results show decent alignments between humans and models, pointing to the promising direction for future work in automatic creativity evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OWiyyvMc0e": {
    "title": "Liar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "Knights and knaves problems represent a classic genre of logical puzzles where characters either tell the truth or lie. The objective is to logically deduce each character's identity based on their statements. The challenge arises from the truth-telling or lying behavior, which influences the logical implications of each statement. Solving these puzzles requires not only direct deductions from individual statements, but the ability to assess the truthfulness of statements by reasoning through various hypothetical scenarios. As such, knights and knaves puzzles serve as compelling examples of suppositional reasoning. In this paper, we introduce $\\textit{TruthQuest}$, a benchmark for suppositional reasoning based on the principles of knights and knaves puzzles. Our benchmark presents problems of varying complexity, considering both the number of characters and the types of logical statements involved. Evaluations on $\\textit{TruthQuest}$ show that large language models like Llama 3 and Mixtral-8x7B exhibit significant difficulties solving these tasks. A detailed error analysis of the models' output reveals that lower-performing models exhibit a diverse range of reasoning errors, frequently failing to grasp the concept of truth and lies. In comparison, more proficient models primarily struggle with accurately inferring the logical implications of potentially false statements",
    "checked": true,
    "id": "a33815829ea243b226e17986fa1ffda97adecb76",
    "semantic_title": "liar, liar, logical mire: a benchmark for suppositional reasoning in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1wEUzefTW3": {
    "title": "A Notion of Complexity for Theory of Mind via Discrete World Models",
    "volume": "review",
    "abstract": "Theory of Mind (ToM) can be used to assess the capabilities of Large Language Models (LLMs) in complex scenarios where social reasoning is required. While the research community has proposed many ToM benchmarks, their hardness varies greatly, and their complexity is not well defined. This work proposes a framework to measure the complexity of ToM tasks. We quantify a problem's complexity as the number of states necessary to solve it correctly. Our complexity measure also accounts for spurious states of a ToM problem designed to make it apparently harder. We use our method to assess the complexity of five widely adopted ToM benchmarks. On top of this framework, we design a prompting technique that augments the information available to a model with a description of how the environment changes with the agents' interactions. We name this technique Discrete World Models (DWM) and show how it elicits superior performance on ToM tasks",
    "checked": true,
    "id": "6dd415a07a6c304a78e191bbb09f50ee04c03d89",
    "semantic_title": "a notion of complexity for theory of mind via discrete world models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9tjO2SoXAf": {
    "title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model",
    "volume": "review",
    "abstract": "Recent advancements in large language models (LLMs) have expanded their scope in natural language processing (NLP) to encompass multimodal functions. However, integrating listening capabilities effectively remains a significant challenge for generalization and complex auditory task execution. In this work, we introduce WavLLM, a robust and adaptive speech large language model featuring dual encodersâ€”a Whisper encoder for semantics and a WavLM encoder for speaker characteristics. Within the two-stage curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks and also apply it to specialized speech-question-answer (SQA) dataset, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. The codes, models, audio samples, and SQA evaluation set can be accessed at \\url{https://github.com/wavllm/wavllm-anonymous}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6T8qeXK7PN": {
    "title": "The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems",
    "volume": "review",
    "abstract": "We examine how users perceive the limitations of an AI system when it encounters a task that it cannot perform perfectly and whether providing explanations alongside its answers aids users in constructing an appropriate mental model of the system's capabilities and limitations. We employ a visual question answer and explanation task where we control the AI system's limitations by manipulating the visual inputs: during inference, the system either processes full-color or grayscale images. Our goal is to determine whether participants can perceive the limitations of the system. We hypothesize that explanations will make limited AI capabilities more transparent to users. However, our results show that explanations do not have this effect. Instead of allowing users to more accurately assess the limitations of the AI system, explanations generally increase users' perceptions of the system's competence -- regardless of its actual performance",
    "checked": true,
    "id": "ff74eae326493f8c4d3bfda14f330194031f3437",
    "semantic_title": "the illusion of competence: evaluating the effect of explanations on users' mental models of visual question answering systems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fbgGzJ0Uu9": {
    "title": "Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution",
    "volume": "review",
    "abstract": "Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \\textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \\textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions not trained with \\textit{RLHF}. Further analysis indicates that the model's language capabilities play a crucial role in preference learning. A reward model with strong language capabilities can more sensitively learn the subtle differences in translation quality and align better with real human translation preferences",
    "checked": true,
    "id": "f3bce99b1257132b386db8e46177b936440376a4",
    "semantic_title": "advancing translation preference modeling with rlhf: a step towards cost-effective solution",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=zSwnz6BsDa": {
    "title": "SHADES: Towards a Multilingual Assessment of Stereotypes in Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown impressive performance on a series of task benchmarks, including open-ended generation for English, spurring a race to increase the number of languages covered by models. However, LLMs also reproduce and exacerbate a range of social biases that are present in the text used for its training data. While research has attempted to identify and mitigate such biases, most efforts have been concentrated around English. In this paper, we introduce a new multilingual dataset for examining culturally-specific stereotypes that may be learned by Large Language Models (LLMs), and templates to enable further generation of bias evaluation data. The dataset includes stereotypes from 20 geopolitical regions and 15 languages. We demonstrate its utility in a series of evaluations for both \"base\" and \"Instruct\" language models. Initial results suggest that there is a vast difference in the representation of stereotypes across both models and languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gRpRRhTEJr": {
    "title": "How Facility is the Small-Scale Abstractive Summarization Model: A Quantitative Study of Semantics and Syntax",
    "volume": "review",
    "abstract": "Large-scale language models (LLMs) have demonstrated advancements in numerous capabilities, including factual consistency in abstractive summarization. However, the benefits of straightforward deployment and reduced invocation latency for small-scale language models (SLMs) should not be disregarded. Current evaluation metrics merely provide an abstract indication of factual score differences, leaving us uncertain about the specific areas where SLMs underperform and whether this gap is tolerable in certain contexts. This study initially illustrates the disparities between LLMs and SLMs regarding semantic knowledge and syntactic ability. Subsequently, we propose an SLM based on contrastive learning that allows tailored semantic and syntactic information and generates a parallel corpus with diverse summaries for the same document, each containing subtle semantic or syntactic flaws. By comprehensively integrating eight distinct factual evaluation metrics, we further elucidate the meaning of the gap in factual scores and identify the primary factual challenges current SLMs face in the abstractive summarization task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dKODiqIfRY": {
    "title": "Granular Entity Mapper: Advancing Fine-grained Multimodal Named Entity Recognition and Grounding",
    "volume": "review",
    "abstract": "Multimodal Named Entity Recognition and Grounding (MNERG) aims to extract paired textual and visual entities from texts and images. It has been well explored through a two-step paradigm: initially identifying potential visual entities using object detection methods and then aligning the extracted textual entities with their corresponding visual entities. However, when it comes to fine-grained MNERG, the long-tailed distribution of textual entity categories and the performance of object detectors limit the effectiveness of traditional methods. Specifically, more detailed classification leads to many low-frequency categories, and existing object detection methods often fail to pinpoint subtle regions within images. To address these challenges, we propose the Granular Entity Mapper (GEM) framework. Firstly, we design a multi-granularity entity recognition module, followed by a reranking module based on the Multimodal Large Language Model (MLLM) to incorporate hierarchical information of entity categories, visual cues, and external textual resources collectively for accurate fine-grained textual entity recognition. Then, we utilize a pre-trained Large Visual Language Model (LVLM) as an implicit visual entity grounder that directly deduces relevant visual entity regions from the entire image without the need for bounding box training. Experimental results on the GMNER and FMNERG datasets demonstrate that our GEM framework achieves state-of-the-art results on the fine-grained content extraction task",
    "checked": false,
    "id": "9826daa07a7fa8e5386609ac1c4604bbdae78ec0",
    "semantic_title": "fine-grained multimodal named entity recognition and grounding with a generative framework",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=NNFIu4KZpZ": {
    "title": "The Need for a Leaderboard: A Survey of LLM as a Judge in NLP",
    "volume": "review",
    "abstract": "Recently, the use of large language model (LLM) as a judge gains popularity in Natural Language Processing (NLP) research. This paper reviews recent studies on LLM-as-a-judge, revealing significant efforts in developing various methods for LLM-based assessment. However, there is a lack of a common standard for meta-evaluations, and several potential risks associated with LLMs need to be acknowledged. Therefore, we recommend creating a leaderboard and offer a draft proposal to support the development and adoption of LLM-as-a-judge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pywu9bFaHl": {
    "title": "Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach",
    "volume": "review",
    "abstract": "Distantly-Supervised Named Entity Recognition (DS-NER) uses knowledge bases or dictionaries for annotations, reducing manual efforts but facing challenges like false positives and negatives in training data. In this paper, we re-examined existing DS-NER methods in real-world scenarios and found that many of them rely on large validation sets and some used test set for tuning inappropriately. We introduced a new dataset named QTL, where the training data is annotated using domain dictionaries and the test data is annotated by domain experts. This dataset has a small validation set, reflecting real-life scenarios. We also propose a new approach, token-level Curriculum-based Positive-Unlabeled Learning (CuPUL), which uses curriculum learning to order training samples from easy to hard. This method stabilizes training, making it robust and effective on small validation sets. CuPUL also addresses false negative issues using the Positive-Unlabeled learning paradigm, demonstrating improved performance in real-life applications",
    "checked": true,
    "id": "e98037a0363e168d35ea236a9502618e7d1817e7",
    "semantic_title": "re-examine distantly supervised ner: a new benchmark and a simple approach",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fXsHY6NdiB": {
    "title": "NALA: an Effective and Interpretable Entity Alignment Method",
    "volume": "review",
    "abstract": "Entity alignment (EA) aims to find equivalent entities between two Knowledge Graphs. Existing embedding-based EA methods usually encode entities as embeddings, triples as embeddings' constraint and learn to align the embeddings. However, the details of the underlying logical inference steps among the alignment process are usually omitted, resulting in inadequate inference process. In this paper, we introduce NALA, an entity alignment method that captures three types of logical inference paths with Non-Axiomatic Logic (NAL). Type 1&2 align the entity pairs and type 3 aligns relations. NALA iteratively aligns entities and relations by integrating the conclusions of the inference paths. Our method is logically interpretable and extensible by introducing NAL, and thus suitable for various EA settings. Experimental results show that NALA outperforms state-of-the-art methods in terms of Hits@1, achieving 0.98+ on all three datasets of DBP15K with both supervised and unsupervised settings. We offer a pioneering in-depth analysis of the fundamental principles of entity alignment, approaching the subject from a unified and logical perspective. Our code is available at https://anonymous.4open.science/r/NALA-976B",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k5cBrtuElA": {
    "title": "An Empirical Study of Iterative Refinements for Non-autoregressive Translation",
    "volume": "review",
    "abstract": "Iterative non-autoregressive (NAR) models share a spirit of mixed autoregressive (AR) and fully NAR models, seeking a balance between generation quality and inference efficiency. These models have recently demonstrated impressive performance in varied generation tasks, surpassing the autoregressive Transformer. However, they also face several challenges that impede further development. In this work, we target building more efficient and competitive iterative NAR models. Firstly, we produce two simple metrics to identify the potential problems existing in current refinement processes, and look back on the various iterative NAR models to find the key factors for realizing our purpose. Subsequently, based on the analyses of the limitations of previous inference algorithms, we propose a simple yet effective strategy to conduct efficient refinements without performance declines. Experiments on five widely used datasets show that our final models set the new state-of-the-art performance compared to all previous NAR models, even with fewer decoding steps, and outperform AR Transformer by around one BLEU on average",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ks89hD05cu": {
    "title": "Can We Verify Step by Step for Incorrect Answer Detection?",
    "volume": "review",
    "abstract": "Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the answer-checking baseline by a large margin. Concretely, this resulted in an average of $5.1\\%$ increase in the F1 score and $2.97\\%$ improvement in AUC-PR across all 45 subsets within R2PE. We further demonstrate our PDS's efficacy in advancing open-domain QA accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BeglmPVIVe": {
    "title": "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning",
    "volume": "review",
    "abstract": "Fine-tuning and in-context learning (ICL) are two prevalent methods in imbuing large language models with task-specific knowledge. It is commonly believed that fine-tuning can surpass ICL given sufficient training samples as it allows the model to adjust its internal parameters based on the data. However, this paper presents a counterintuitive finding: For tasks with implicit patterns, ICL captures these patterns significantly better than fine-tuning. We developed several datasets featuring implicit patterns, such as sequences determining answers through parity or identifying reducible terms in calculations. We then evaluated the models' understanding of these patterns under both fine-tuning and ICL across models ranging from 0.5B to 7B parameters. The results indicate that models employing ICL can quickly grasp deep patterns and significantly improve accuracy. In contrast, fine-tuning, despite utilizing thousands of times more training samples than ICL, achieved only limited improvements. We also proposed circuit shift theory from a mechanistic interpretability's view to explain why ICL wins",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GDQ2us7g3Y": {
    "title": "Data Contamination Can Cross Language Barriers",
    "volume": "review",
    "abstract": "The opacity in developing large language models (LLMs) is raising growing concerns about the potential contamination of public benchmarks in the pre-training data. Existing contamination detection methods are typically based on the text overlap between training and evaluation data, which can be too superficial to reflect deeper forms of contamination. In this paper, we first present a cross-lingual form of contamination that inflates LLMs' performance while evading current detection methods, deliberately injected by overfitting LLMs on the translated versions of benchmark test sets. Then, we propose generalization-based approaches to unmask such deeply concealed contamination. Specifically, we examine the LLM's performance change after modifying the original benchmark by replacing the false answer choices with correct ones from other questions. Contaminated models can hardly generalize to such easier situations, where the false choices can be \\emph{not even wrong}, as all choices are correct in their memorization. Experimental results demonstrate that cross-lingual contamination can easily fool existing detection methods, but not ours. In addition, we discuss the potential utilization of cross-lingual contamination in interpreting LLMs' working mechanisms and in post-training LLMs for enhanced multilingual capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YHTR7Trkep": {
    "title": "Multi-view Content-aware Indexing for Long Document Retrieval",
    "volume": "review",
    "abstract": "Long document question answering (DocQA) aims to answer questions from long documents over 10k words. They usually contain content structures such as sections, sub-sections, and paragraph demarcations. However, the indexing methods of long documents remain under-explored, while existing systems generally employ fixed-length chunking. As they do not consider content structures, the resultant chunks can exclude vital information or include irrelevant content. Motivated by this, we propose the **M**ulti-view **C**ontent-aware **indexing** (**MC-indexing**) for more effective long DocQA via (i) segment structured document into content chunks, and (ii) represent each content chunk in raw-text, keywords, and summary views. We highlight that MC-indexing requires neither training nor fine-tuning. Having plug-and-play capability, it can be seamlessly integrated with any retrievers to boost their performance. Besides, we propose a long DocQA dataset that includes not only question-answer pair, but also document structure and answer scope. When compared to state-of-art chunking schemes, MC-indexing has significantly increased the recall by **42.8%**, **30.0%**, **23.9%**, and **16.3%** via top k = 1.5, 3, 5, and 10 respectively. These improved scores are the average of 8 widely used retrievers (2 sparse and 6 dense) via extensive experiments",
    "checked": true,
    "id": "1fbbaa475e93c2d851344c5181a0e39a99324aca",
    "semantic_title": "multi-view content-aware indexing for long document retrieval",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=afj2KQYzzj": {
    "title": "RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "The Retrieval Augmented Generation (RAG) framework utilizes a combination of parametric knowledge and external knowledge to demonstrate state-of-the-art performance on open-domain question answering tasks. However, the RAG framework suffers from performance degradation when the query is accompanied by irrelevant contexts. In this work, we propose the RE-RAG framework, which introduces a relevance estimator (RE) that not only provides relative relevance between contexts as previous rerankers did, but also provide confidence, which can be used to classify whether given context is useful for answering the given question. We propose a weakly supervised method for training the RE simply utilizing question-answer data without any labels for correct contexts. We show that RE trained with a small generator (sLM) can not only improve the sLM fine-tuned together with RE but also improve previously unreferenced large language models (LLMs). Furthermore, we investigate new decoding strategies that utilize the proposed confidence measured by RE such as choosing to let the user know that it is \"unanswerable\" to answer the question given the retrieved contexts or choosing to rely on LLM's parametric knowledge rather than unrelated contexts",
    "checked": true,
    "id": "755bf404a7e8a81df53172abd3fdb5ff8e31ecf3",
    "semantic_title": "re-rag: improving open-domain qa performance and interpretability with relevance estimator in retrieval-augmented generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sNJQMrM9rb": {
    "title": "Verifiable, Debuggable, and Repairable Commonsense Logical Reasoning via LLM-based Theory Resolution",
    "volume": "review",
    "abstract": "Recent advances in Large Language Models (LLM) have led to substantial interest in their application to commonsense reasoning tasks. Despite their potential, LLMs are susceptible to reasoning errors and hallucinations that may be harmful in use cases where accurate reasoning is critical. This challenge underscores the need for verifiable, debuggable, and repairable LLM reasoning. Recent works have made progress toward verifiable reasoning with LLMs by using them as either (i) a reasoner over an axiomatic knowledge base, or (ii) a semantic parser for use in existing logical inference systems. However, both settings are unable to extract commonsense axioms from the LLM that are not already formalized in the knowledge base, and also lack a reliable method to repair missed commonsense inferences. In this work, we present LLM-TRes, a logical reasoning framework based on the notion of \"theory resolution\" that allows for seamless integration of the commonsense knowledge from LLMs with a verifiable logical reasoning framework that mitigates hallucinations and facilitates debugging of the reasoning procedure as well as repair. We crucially prove that repaired axioms are theoretically guaranteed to be given precedence over flawed ones in our theory resolution inference process. We conclude by evaluating on three diverse language-based reasoning tasks -- preference reasoning, deductive reasoning, and causal commonsense reasoning -- and demonstrate the superior performance of LLM-TRes vs. state-of-the-art LLM-based reasoning methods in terms of both accuracy and reasoning correctness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GKNUTvkal9": {
    "title": "DIRAS: Efficient LLM-Assisted Annotation of Document Relevance in Retrieval Augmented Generation",
    "volume": "review",
    "abstract": "Retrieval Augmented Generation (RAG) is widely employed to ground responses to queries on domain-specific documents. But do RAG implementations leave out important information or excessively include irrelevant information? To allay these concerns, it is necessary to annotate domain-specific benchmarks to evaluate information retrieval (IR) performance, as relevance definitions vary across queries and domains. Furthermore, such benchmarks should be cost-efficiently annotated to avoid annotation selection bias. In this paper, we propose DIRAS (**D**omain-specific **I**nformation **R**etrieval **A**nnotation with **S**calability), a manual-annotation-free schema that fine-tunes open-sourced LLMs to annotate relevance labels with calibrated relevance probabilities. Extensive evaluation shows that DIRAS fine-tuned models achieve GPT-4-level performance on annotating and ranking unseen (query, document) pairs, and is helpful for real-world RAG development",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZfIB7yUgAS": {
    "title": "Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved great success, but their occasional content fabrication, or hallucination, limits their practical application. Hallucination arises because LLMs struggle to admit ignorance due to inadequate training on knowledge boundaries. We call it a limitation of LLMs that they can not accurately express their knowledge boundary, answering questions they know while admitting ignorance to questions they do not know. In this paper, we aim to teach LLMs to recognize and express their knowledge boundary, so they can reduce hallucinations caused by fabricating when they do not know. We propose CoKE, which first probes LLMs' knowledge boundary via internal confidence given a set of questions, and then leverages the probing results to elicit the expression of the knowledge boundary. Extensive experiments show CoKE helps LLMs express knowledge boundaries, answering known questions while declining unknown ones, significantly improving in-domain and out-of-domain performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u6QfNfk4Gf": {
    "title": "First Multi-Dimensional Evaluation of Flowchart Comprehension for Multimodal Large Language Models",
    "volume": "review",
    "abstract": "With the development of Multimodal Large Language Models (MLLMs) technology, its general capabilities are increasingly powerful. To evaluate the various abilities of MLLMs, numerous evaluation systems have emerged. But now there is still a lack of a comprehensive method to evaluate MLLMs in the tasks related to flowcharts, which are very important in daily life and work. We propose the first comprehensive method, FlowCE, to assess MLLMs across various dimensions for tasks related to flowcharts. It encompasses evaluating MLLMs' abilities in Reasoning, Localization Recognition, Information Extraction, Logical Verification, and Summarization on flowcharts. However, we find that even the GPT4o model achieves only a score of 56.63. Among open-source models, Phi-3-Vision obtained the highest score of 49.97. We hope that FlowCE can contribute to future research on MLLMs for tasks based on flowcharts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gil6KUTqvm": {
    "title": "An Exam for Anyone on Anything: LLM-Based Textbook Data Transformation",
    "volume": "review",
    "abstract": "Supervised learning traditionally depends on labeled data, collected and organized for specific tasks. Producing these datasets has generally been time-consuming, costly and error-prone. The emergence of large language models (LLMs) demonstrate a remarkable ability to produce well-formatted data, which could potentially revolutionize the dataset construction process. In this paper, we propose an LLM-based data transformation pipeline to generate multiple-choice question-answer (MCQA) data from raw sources such as textbooks. Furthermore, we extend this process by proposing a pseudo-open-book reasoning approach, wherein student LLMs are trained to first recreate the original textbook excerpts used to generate the questions, before answering them. We evaluate our methods using the Llama2 13B model on domain-specific subsections from the MMLU testing set, and observe an improvement of up to 18.8% in testing accuracy, increasing from 45.8% to 64.6%, without accessing the corresponding MMLU training set",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=smP90p65Sn": {
    "title": "Revealing The Intrinsic Ability of Generative Text Summarizers for Irrelevant Document Detection",
    "volume": "review",
    "abstract": "In Retrieval-Augmented Generation (RAG), generative models are prone to performance degradation due to retrieved irrelevant documents. Adding irrelevant documents to the training data and retraining language models incurs significant costs. Supervised models can detect irrelevant documents in the retrieved results and avoid retraining, but they cannot counter domain shifts in the real world. By introducing a method that emphasizes the unique features of infrequent words, we reveal the ability of the cross-attention mechanism to detect irrelevant documents within the inputs of generative models. We present CODE, a novel irrelevant document detector using a closed-form expression rooted in cross-attention scores. Our experimental results validate the superiority of CODE under in-domain and cross-domain detection. For in-domain detection, CODE achieves a 5.80% FPR at 95% TPR vs. 30.3% by supervised baseline on the T5-Large and Delve domain. When sampling irrelevant documents from out-of-domain, the FPR of CODE decreases from 5.8% to 0.1%, while the FPR of the supervised baseline increases from 30.3% to 34.3%. For more insight, we highlight the importance of cross-attention, word frequency normalization, and integrating in-domain irrelevant documents during pretraining",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9IVeNxXFV4": {
    "title": "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering",
    "volume": "review",
    "abstract": "Recent advances in Vision-Language Models (VLMs) and the scarcity of high-quality multi-modal alignment data have inspired numerous researches on synthetic VLM data generation. Challenging the conventional norm in VLM data construction, which uses a mixture of specialists in caption and OCR, or stronger VLM APIs and expensive human annotation, we propose to leverage the VLM itself for extracting cross-modal information of each via different prompts and filter the generated outputs again by itself via a consistency filtering strategy. In this paper, we present World to Code (W2C), a meticulously curated multi-modal data construction pipeline that organizes the final generation output into a Python code format. Experiments have demonstrated the high quality of W2C by improving various existing visual question answering and visual grounding benchmarks across different VLMs. Further analysis also demonstrates that the new code parsing ability of VLMs presents better cross-modal equivalence than the commonly used detail caption ability. Our code and data will be made public",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X6MllOyg2y": {
    "title": "Evaluate, Scale, and Credit: A Comprehensive Study on Multi-Agent Collaboration of Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models based Multi-Agent Systems (LLM-MAS) perform well in many domains, but we still lack a clear understanding of the collaboration mechanism among multiple LLM-based agents. This study aims to explore three key issues: (1) Can multi-agent outperform single-agent systems? (2) Is scaling better for multi-agent systems? (3) How to credit agents and find potential effective structures? Specifically, we design five collaboration architectures and evaluate their effectiveness across different LLMs and tasks. Our findings offer significant insights for understanding the collaboration within MAS, building collaboration architectures among agents, and reducing system costs. Furthermore, our conclusion will inspire and provide new perspectives for future studies on LLM-MAS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xKGlpYvv9K": {
    "title": "Textless Speech-to-Speech Translation With Limited Parallel Data",
    "volume": "review",
    "abstract": "Existing speech-to-speech translation (S2ST) models fall into two camps: they either leverage text as an intermediate step or require hundreds of hours of parallel speech data. Both approaches are incompatible with textless languages or language pairs with limited parallel data. We present a framework for training textless S2ST models that require just dozens of hours of parallel speech data. We first pretrain a model on large-scale monolingual speech data, finetune it with a small amount of parallel speech data (20-60 hours), and lastly train with unsupervised backtranslation objective. We train and evaluate our models for English-to-German, German-to-English and Marathi-to-English translation on three different domains (European Parliament, Common Voice, and All India Radio) with single-speaker synthesized speech. Evaluated using the ASR-BLEU metric, our models achieve reasonable performance on all three domains, with some being within 1-2 points of our higher-resourced topline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W045tMwo4l": {
    "title": "Weakly-supervised Argument Mining with Boundary Refinement and Relation Denoising",
    "volume": "review",
    "abstract": "Argument mining (AM) involves extracting argument components and predicting relations between them to create argumentative graphs, which are essential for applications requiring argumentative comprehension. To automatically provide high-quality graphs, previous works require a large amount of human-annotated training samples to train AM models. Instead, we leverage a large language model (LLM) to assign pseudo-labels to training samples for reducing reliance on human-annotated training data. However, the training data weakly-labeled by the LLM are too noisy to develop an AM model with reliable performance. In this paper, to improve the model performance, we propose a center-based component detector that refines the boundaries of the detected components and a relation denoiser to deal with noise present in the pseudo-labels when classifying relations between detected components. Experimentally, our AM model improves the boundary detection obtained from the LLM by up to $16$\\% in terms of IoU75 and of the relation classification obtained from the LLM by up to $12$\\% in terms of macro-F1 score. Our AM model achieves new state-of-the-art performance in weakly-supervised AM, showing up to a $6$\\% improvement over the state-of-the-art component detector and up to a $7$\\% improvement over the state-of-the-art relation classifier. Additionally, our model uses less than $20$\\% of human-annotated data to match the performance of state-of-the-art fully-supervised AM models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tULtVDdu8D": {
    "title": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics",
    "volume": "review",
    "abstract": "The idea of personality in descriptive psychology, traditionally defined through observable behavior, has now been extended to Large Language Models (LLMs) to better understand their behavior. This raises a question: do LLMs exhibit distinct and consistent personality traits, similar to humans? Existing self-assessment personality tests, while applicable, lack the necessary validity and reliability for precise personality measurements. To address this, we introduce TRAIT, a new tool consisting of 8K multi-choice questions designed to assess the personality of LLMs with validity and reliability. TRAIT is built on the psychometrically validated human questionnaire, Big Five Inventory (BFI) and Short Dark Triad (SD-3), enhanced with the ATOMIC10X knowledge graph for testing personality in a variety of real scenarios. TRAIT overcomes the reliability and validity issues when measuring personality of LLM with self-assessment, showing the highest scores across three metrics: refusal rate, prompt sensitivity, and option order sensitivity. It reveals notable insights into personality of LLM: 1) LLMs exhibit distinct and consistent personality, which is highly influenced by their training data (i.e. data used for alignment tuning), and 2) current prompting techniques have limited effectiveness in eliciting certain traits, such as high psychopathy or low conscientiousness, suggesting the need for further research in this direction",
    "checked": true,
    "id": "c86eba7860f37c47b97fda9d607b5386b7a1cb15",
    "semantic_title": "do llms have distinct and consistent personality? trait: personality testset designed for llms with psychometrics",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzOyE5wBMO": {
    "title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models",
    "volume": "review",
    "abstract": "Mixture of experts (MoE) has become the standard for constructing production-level large language models (LLMs) due to its promise to boost model capacity without causing significant overheads. Nevertheless, existing MoE methods usually enforce a constant top-$k$ routing for all tokens, which is arguably restrictive because various tokens (e.g., \"<EOS>\" vs. \"apple\") may require various numbers of experts for feature abstraction. Lifting such a constraint can help make the most of limited resources and unleash the potential of the model for downstream tasks. In this sense, we introduce **AdaMoE** to realize token-adaptive routing for MoE, where different tokens are permitted to select a various number of experts. AdaMoE makes minimal modifications to the vanilla MoE with top-$k$ routing---it simply introduces a fixed number of *null experts*, which do not consume any FLOPs, to the expert set and increases the value of $k$. AdaMoE does not force each token to occupy a fixed number of null experts but ensures the average usage of the null experts with a load-balancing loss, leading to an adaptive number of null/true experts used by each token. AdaMoE exhibits a strong resemblance to MoEs with expert choice routing while allowing for trivial auto-regressive modeling. AdaMoE is easy to implement and can be effectively applied to pre-trained (MoE-)LLMs. Extensive studies show that AdaMoE can reduce average expert load (FLOPs) while achieving superior performance. For example, on the ARC-C dataset, applying our method to fine-tuning Mixtral-8x7B can reduce FLOPs by 14.5% while increasing accuracy by 1.69%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fqBBwIh238": {
    "title": "AdaCQR: Enhancing Query Reformulation for Conversational Search via Sparse and Dense Retrieval Alignment",
    "volume": "review",
    "abstract": "Conversational Query Reformulation (CQR) has significantly advanced in addressing the challenges of conversational search, particularly those stemming from the latent user intent and the need for historical context. Recent works aimed to boost the performance of CRQ through alignment. However, they are designed for one specific retrieval system, which potentially results in poor generalization. To overcome this limitation, we present a novel framework AdaCQR. By aligning reformulation models with both term-based and semantic-based retrieval systems, AdaCQR enhances the generalizability of information-seeking queries across diverse retrieval environments through a dual-phase training strategy. We also developed two effective approaches for acquiring superior labels and diverse input candidates, boosting the efficiency and robustness of the framework. Experimental evaluations on the TopiOCQA and QReCC datasets demonstrate that AdaCQR significantly outperforms existing methods, offering both quantitative and qualitative improvements in conversational query reformulation",
    "checked": true,
    "id": "3c49aa02bc05f5343b090786d272bd31d7e53b3a",
    "semantic_title": "adacqr: enhancing query reformulation for conversational search via sparse and dense retrieval alignment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=efksXXZa6k": {
    "title": "Large Language Models in Real-World Table Task Workflows: A Survey",
    "volume": "review",
    "abstract": "Tables are widely used across various fields such as finance, healthcare, and public administration, playing an indispensable role in modern society. Despite their importance, the structured nature of tabular data, like permutation invariance, adds complexity to its processing. Large Language Models (LLMs) offer new opportunities, but their performance remains suboptimal due to the unique characteristics of tables. Rapidly improving LLMs' ability to process tables is unattainable in the short term. Therefore, we believe that table tasks should be broken down into many interrelated subtasks to enhance performance. So, we define workflows for handling table tasks, refine existing methods based on these workflows, and compare potentially effective methods, such as LLM-based agents, for implementing all workflows, thus providing assistance for future development",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Knvy5pqhoo": {
    "title": "CLEME2.0: Towards More Interpretable Evaluation by Disentangling Edits for Grammatical Error Correction",
    "volume": "review",
    "abstract": "The paper focuses on improving the interpretability of Grammatical Error Correction (GEC) metrics, which receives little attention in previous studies. To bridge the gap, we propose **CLEME2.0**, a reference-based evaluation strategy that can describe four elementary dimensions of GEC systems, namely hit-correction, error-correction, under-correction, and over-correction. They collectively contribute to revealing the critical characteristics and locating drawbacks of GEC systems. Evaluating systems by Combining these dimensions leads to high human consistency over other reference-based and reference-less metrics. Extensive experiments on 2 human judgement datasets and 6 reference datasets demonstrate the effectiveness and robustness of our method",
    "checked": true,
    "id": "2c8e52125073607d3eec0f456b73cf917e10609f",
    "semantic_title": "cleme2.0: towards more interpretable evaluation by disentangling edits for grammatical error correction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q1hoMgzfw3": {
    "title": "EmbodiedBERT: Cognitively Informed Metaphor Detection Incorporating Sensorimotor Information",
    "volume": "review",
    "abstract": "The identification of metaphor is a crucial prerequisite for many downstream language tasks, such as sentiment analysis, opinion mining, and textual entailment. State-of-the-art systems of metaphor detection require training data annotated based on heuristic principles such as Metaphor Identification Procedure (MIP) (Pragglejaz Group, 2007) and Selection Preference Violation (SPV) (Wilks, 1975; Wilson, 2002). We propose an innovative approach that leverages the cognitive information of embodiment that can be derived from word embeddings, and explicitly models the process of sensorimotor shedding that has been demonstrated as essential for human metaphor processing. We showed that this cognitively motivated module is more effective and can improve the prediction of metaphoricity compared with the heuristic MIP that has been applied previously",
    "checked": false,
    "id": "ffc58acc0cb843a30ab1320efde3e9b14716f8d6",
    "semantic_title": "perceptional and actional enrichment for metaphor detection with sensorimotor norms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Qw5KhoZOtA": {
    "title": "Benchmarking Vision Language Models for Cultural Understanding",
    "volume": "review",
    "abstract": "Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding -- recognizing objects, attributes, and actions -- rather than cultural comprehension. This study introduces CulturalVQA, a visual question-answering benchmark aimed at assessing VLM's geo-diverse cultural understanding. We curate a diverse collection of 2,378 image-question pairs with 1-5 answers per question representing cultures from 11 countries across 5 continents. The questions probe understanding of various facets of culture such as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of cultural understanding across regions, with strong cultural understanding capabilities for North America while significantly weaker capabilities for Africa. We observe disparity in their performance across cultural facets too, with clothing, rituals, and traditions seeing higher performances than food and drink. These disparities help us identify areas where VLMs lack cultural understanding and demonstrate the potential of CulturalVQA as a comprehensive evaluation set for gauging VLM progress in understanding diverse cultures",
    "checked": true,
    "id": "d838f425c2e5e7b0fabb4ac108fc3f57bb4a85c0",
    "semantic_title": "benchmarking vision language models for cultural understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KMD13Pdmit": {
    "title": "Multiple-Choice Questions are Efficient and Robust LLM Evaluators",
    "volume": "review",
    "abstract": "We present GSM-MC, a multiple-choice (MC) dataset constructed by collecting answers and incorrect predictions on GSM8K from 60 open-source models. Through extensive experiments, we show that LLMs' performance on the MC version of this popular benchmark is strongly correlated with their performance on the original version and is quite robust to distractor choices and option orders, while the evaluation time is reduced by a factor of up to 30. Following similar procedures, we introduce MATH-MC, constructed from MATH, and PythonIO, a new program reasoning MC dataset constructed from HumanEval and MBPP. Experimental results indicate that LLMs' performance on these MC benchmarks leaves much room for improvement",
    "checked": true,
    "id": "9b73f37d89fa1484609909c335a50203fa094a0a",
    "semantic_title": "multiple-choice questions are efficient and robust llm evaluators",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D2iN1eaFey": {
    "title": "Where does meaning live? Investigating the synthetic-analytic distinction in LLMs using gender as a case study",
    "volume": "review",
    "abstract": "Some linguistic inferences--e.g., inferring that a square has four sides--seem to follow inherently from what words mean, while others--e.g., inferring that a house has four sides--are considered to follow from \"common sense'\" or \"world knowledge\". It has long been debated whether such categorical distinctions, referred to in philosophy as analytic vs.\\ synthetic, can be made and what effect they should have on theories and models of semantic meaning. In this paper, we use gender (male vs.\\ female) as a case study to explore whether large language models (LLMs) differentiate analytic inferences about gender (e.g., that a woman is female) from synthetic inferences (e.g., that nurses are most often female). We find that, by and large, there are not substantial mechanistic differences, but rather the difference appears to be a matter of degree--i.e., how strongly the inference is encoded and how easily it is overwritten by contextual information. Our study serves as a proof-of-concept for how LLMs can be used to revisit long-standing questions about language representation and processing in general",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NBeUgOpyWK": {
    "title": "Aligning Language Models to Explicitly Handle Ambiguity",
    "volume": "review",
    "abstract": "In interactions between users and language model agents, user utterances frequently exhibit ellipsis (omission of words or phrases) or imprecision (lack of exactness) to prioritize efficiency. This can lead to varying interpretations of the same input based on different assumptions or background knowledge. It is thus crucial for agents to adeptly handle the inherent ambiguity in queries to ensure reliability. However, even state-of-the-art large language models (LLMs) still face challenges in such scenarios, primarily due to the following hurdles: (1) LLMs are not explicitly trained to deal with ambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may vary depending on the possessed knowledge. To address these issues, we propose Alignment with Perceived Ambiguity (APA), a novel pipeline that aligns LLMs to manage ambiguous queries by leveraging their own assessment of ambiguity (i.e., perceived ambiguity). Experimental results on question-answering datasets demonstrate that APA empowers LLMs to explicitly detect and manage ambiguous queries while retaining the ability to answer clear questions. Furthermore, our finding proves that APA excels beyond training with gold-standard labels, especially in out-of-distribution scenarios",
    "checked": true,
    "id": "1544f956492b613b47efd9ac79c3987df373fb81",
    "semantic_title": "aligning language models to explicitly handle ambiguity",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=v3x1Pf4Y3r": {
    "title": "Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach",
    "volume": "review",
    "abstract": "Recent progress in Spoken Language Modeling has demonstrated the feasibility of learning language directly from speech. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems tend to trail behind text-based language models in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, which in turn improve downstream language modeling performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a3rIzsawoF": {
    "title": "MoreHopQA: More Than Multi-hop Reasoning",
    "volume": "review",
    "abstract": "Most existing multi-hop datasets are extractive answer datasets, where the answers to the questions can be extracted directly from the provided context. This often leads models to use heuristics or shortcuts instead of performing true multi-hop reasoning. In this paper, we propose a new multi-hop dataset, MoreHopQA, which shifts from extractive to generative answers. Our dataset is created by utilizing three existing multi-hop datasets: HotpotQA, 2WikiMultihopQA, and MuSiQue. Instead of relying solely on factual reasoning, we enhance the existing multi-hop questions by adding another layer of questioning that involves one, two, or all three of the following types of reasoning: commonsense, arithmetic, and symbolic. Our dataset is created through a semi-automated process, resulting in a dataset with 1,118 samples that have undergone human verification. We then use our dataset to evaluate five different large language models: Mistral 7B, Gemma 7B, Llama 3 (8B and 70B), and GPT-4. We also design various cases to analyze the reasoning steps in the question-answering process. Our results show that models perform well on initial multi-hop questions but struggle with our extended questions, indicating that our dataset is more challenging than previous ones. Our analysis of question decomposition reveals that although models can correctly answer questions, only a portionâ€”38.7\\% for GPT-4 and 33.4\\% for Llama3-70Bâ€”achieve perfect reasoning, where all corresponding sub-questions are answered correctly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PMa5ef9wxC": {
    "title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
    "volume": "review",
    "abstract": "In this work, we are interested in automated methods for knowledge graph creation (KGC) from input text. Progress on large language models (LLMs) has prompted a series of recent works applying them to KGC, e.g., via zero/few-shot prompting. Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that in prior methods, the KG schema has to be included in the LLM prompt to generate valid triplets; larger and more complex schema easily exceed the LLMs' context window length. Furthermore, there are scenarios where a fixed pre-defined schema is not available and we would like the method to construct an intrinsically high-quality KG with accurate information and a succinct self-generated schema. To address these problems, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): open information Extraction (OIE) followed by schema definition and post-hoc Canonicalization. EDC is flexible in that it can be applied to settings where a target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization. We demonstrate on three KGC benchmarks that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works. To improve performance, we further introduce a trained component that retrieves schema elements relevant to the input text; this improves the LLMs' extraction performance in a retrieval-augmented generation-like manner",
    "checked": true,
    "id": "db96e019410006c3ee0ae0184800ab206f8704dd",
    "semantic_title": "extract, define, canonicalize: an llm-based framework for knowledge graph construction",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=BiY9SjM4OW": {
    "title": "Assessing and Post-Processing Black Box Large Language Models for Knowledge Editing",
    "volume": "review",
    "abstract": "The task of Knowledge Editing (KE) is aimed at efficiently and precisely adjusting the behavior of large language models (LLMs) to update specific knowledge while minimizing any adverse effects on other knowledge. Current research predominantly concentrates on editing white-box LLMs, neglecting a significant scenario: editing black-box LLMs, where access is limited to interfaces and only textual output is provided. In this paper, we initially officially introduce KE on black-box LLMs, followed by presenting a thorough evaluation framework aimed at addressing the shortcomings of current evaluations, which are inadequate for black-box LLMs editing and lack comprehensiveness. To address privacy leaks of editing data and style over-editing in existing approaches, we propose a new postEdit framework, ensuring privacy through downstream processing and maintaining textual style consistency via fine-grained editing. Experiments and analysis conducted on two benchmarks show that postEdit surpasses all baselines and exhibits robust generalization, notably enhancing style retention by an average of +20.82\\%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5mhCK0g5gw": {
    "title": "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction",
    "volume": "review",
    "abstract": "Automated red teaming is an effective method for identifying misaligned behaviors in large language models (LLMs). Existing approaches, however, often focus primarily on improving attack success rates while overlooking the need for comprehensive test case coverage. Additionally, most of these methods are limited to single-turn red teaming, failing to capture the multi-turn dynamics of real-world human-machine interactions. To overcome these limitations, we propose \\textbf{HARM} (\\textbf{H}olistic \\textbf{A}utomated \\textbf{R}ed tea\\textbf{M}ing), which scales up the diversity of test cases using a top-down approach based on an extensible, fine-grained risk taxonomy. Our method also leverages a novel fine-tuning strategy and reinforcement learning techniques to facilitate multi-turn adversarial probing in a human-like manner. Experimental results demonstrate that our framework enables a more systematic understanding of model vulnerabilities and offers more targeted guidance for the alignment process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lTe2TqV6Iw": {
    "title": "MuAP: Multi-step Adaptive Prompt Learning for Vision-Language Model with Missing Modality",
    "volume": "review",
    "abstract": "Recently, prompt learning has garnered considerable attention for its success in various Vision-Language (VL) tasks. However, existing prompt-based models are primarily focused on studying prompt generation and prompt strategies with complete modality settings, which does not accurately reflect real-world scenarios where partial modality information may be missing. In this paper, we present the first comprehensive investigation into prompt learning behavior when modalities are incomplete, revealing the high sensitivity of prompt-based models to missing modalities. To this end, we propose a novel $\\underline{\\textbf{Mu}}$lti-step $\\underline{\\textbf{A}}$daptive $\\underline{\\textbf{P}}$rompt Learning $(\\textbf{MuAP}) $framework, aiming to generate multimodal prompts and perform multi-step prompt tuning, which adaptively learns knowledge by iteratively aligning modalities. Specifically, we generate multimodal prompts for each modality and devise prompt strategies to integrate them into the Transformer model. Subsequently, we sequentially perform prompt tuning from single-stage and alignment-stage, allowing each modality-prompt to be autonomously and adaptively learned, thereby mitigating the imbalance issue caused by only textual prompts that are learnable in previous works. Extensive experiments demonstrate the effectiveness of our MuAP and this model achieves significant improvements compared to the state-of-the-art on all benchmark datasets. Our codes are available at https://anonymous.4open.science/r/multiview_adaptative_prompt_learning/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5j1dGzpxr4": {
    "title": "Rater Cohesion and Quality from a Vicarious Perspective",
    "volume": "review",
    "abstract": "Human feedback is essential for building human-centered AI systems across domains where disagreement is prevalent, such as AI safety, content moderation, or sentiment analysis. Many disagreements, particularly in politically charged settings, arise because raters have opposing values or beliefs. Vicarious annotation is a method for breaking down disagreement by asking raters how they think others would annotate the data. In this paper, we explore the use of vicarious annotation with analytical methods for moderating rater disagreement. We employ rater cohesion metrics to study the potential influence of political affiliations and demographic backgrounds on raters' perceptions of offense. Additionally, we utilize CrowdTruth's rater quality metrics, which consider the demographics of the raters, to score the raters and their annotations. We study how the rater quality metrics influence the in-group and cross-group rater cohesion across the personal and vicarious levels",
    "checked": true,
    "id": "2f10b22e0109d6e9f29243fd649df937a56861be",
    "semantic_title": "rater cohesion and quality from a vicarious perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YdMiRnaU55": {
    "title": "On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning",
    "volume": "review",
    "abstract": "Text-based reinforcement learning involves an agent interacting with a fictional environment using observed text and admissible actions in natural language to complete a task. Previous works have shown that agents can succeed in text-based interactive environments even in the complete absence of semantic understanding or other linguistic capabilities. The success of these agents in playing such games suggests that semantic understanding may not be important for the task. This raises an important question about the benefits of LMs in guiding the agents through the game states. In this work, we show that rich semantic understanding leads to efficient training of text-based RL agents. Moreover, we describe the occurrence of semantic degeneration as a consequence of inappropriate fine-tuning of language models in text-based reinforcement learning (TBRL). Specifically, we describe the shift in the semantic representation of words in the LM, as well as how it affects the performance of the agent in tasks that are semantically similar to the training games. We believe these results may help develop better strategies to fine-tune agents in text-based RL scenarios",
    "checked": true,
    "id": "74160d998b673381a2a46b515c95724c66ba24e4",
    "semantic_title": "on the effects of fine-tuning language models for text-based reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rTIU50bCtN": {
    "title": "QCRD: Quality-guided Contrastive Rationale Distillation for Large Language Models",
    "volume": "review",
    "abstract": "Deploying large language models (LLMs) faces challenges with resource constraints and inference efficiency. Recent research has focused on using smaller task-specific models enhanced by distilling knowledge rationales from LLMs. However, previous works have primarily emphasized the effectiveness of positive knowledge distillation, while overlooking knowledge noise and the potential benefits of negative knowledge. In this paper, we first propose a general approach called quality-guided contrastive rationale distillation for reasoning capacity learning, considering contrastive learning perspectives. For positive knowledge, we collect positive rationales through self-consistency to denoise the LLM rationales generated by temperature sampling. For negative knowledge, we propose a novel approach that generates negative rationales by sampling previous iterations of smaller language models with a high temperature value. Finally, we design a contrastive loss to distill both positive and negative knowledge into smaller language models, where an online-update discriminator is used to assess the quality of the rationales, and assign weights to better optimize the training process. Through extensive experiments on multiple reasoning tasks, we demonstrate that our method consistently outperforms the previous distillation methods and produces higher-quality rationales",
    "checked": true,
    "id": "e47b4c1212ed22504531cb782adf49f4bc6fb8ba",
    "semantic_title": "qcrd: quality-guided contrastive rationale distillation for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fJ5ks1PLqS": {
    "title": "Efficient Unicode-Compatible Grammar-Constrained Decoding via String Homomorphism",
    "volume": "review",
    "abstract": "Grammar-constrained decoding (GCD) is a powerful technique that enforces formal grammar constraints on the outputs of large language models (LLMs). This method ensures that generated text adheres to predefined structural rules, making it highly suitable for tasks requiring precise output formats. Despite its broad applications, the theoretical fundamentals of GCD remain underexplored, particularly in the context of formal language theory. In this work, we introduce the concept of tokenization as an inverse homomorphism, which maps the original string language to a token language defined on the alphabet of token IDs. The fact that tokenization is an inverse homomorphism is important for the efficiency of GCD, providing both a theoretical basis and an efficient construction method for the GCD algorithm. We further extend this framework to support Unicode characters, which are essential for multilingual NLP applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t8MJG2Nav6": {
    "title": "Progressive Multimodal Chain-of-Thought Tuning for Vision-Indispensable Reasoning",
    "volume": "review",
    "abstract": "Recent advancements in multimodal large language models (MLLMs) have showcased their impressive capabilities in multimodal understanding and generation. Nevertheless, current open-source MLLMs still encounter challenges in complex reasoning and problem solving, especially in vision-indispensable scenarios. In this paper, we present ViLamr, an MLLM tailored for vision-indispensable reasoning. To endow ViLamr with powerful reasoning capabilities, we initially construct a multimodal instruction-following dataset, MCoT-Instruct, featuring 266K high-quality chain-of-thought responses. Subsequently, we equip ViLamr with a novel connector to selectively integrate different visual features and facilitate alignment between correlated vision and language content. Finally, we fine-tune ViLamr on MCoT-Instruct with a meticulously designed reasoning progressive-enhancement tuning scheme, encouraging ViLamr to follow the cognitive process of ``understanding before reasoning''. Experiments on multiple multimodal benchmarks and datasets demonstrate the effectiveness of ViLamr and the contribution of MCoT-Instruct in bolstering MLLM reasoning capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ul3AZUhMxO": {
    "title": "Opinion Units: Concise and Contextualized Representations for Aspect-Based Sentiment Analysis",
    "volume": "review",
    "abstract": "We introduce opinion units, a novel approach to Aspect-Based Sentiment Analysis (ABSA) that extends traditional aspect-sentiment pairs by including substantiating excerpts derived through hybrid abstractive-extractive summarisation. This reduces the information loss inherent in traditional ABSA methods, and the structured format facilitates downstream processing tasks. Experiments on benchmark datasets for ABSA demonstrate that large language models (LLMs) can accurately extract opinion units using a few-shot approach. The main types of errors are overlooking aspects in the text, and characterising objective statements as opinions. The method eliminates the need for labelled data and allows the LLM to dynamically define aspect types. Additionally, we present a case study on similarity search for opinions in academic datasets and public review data. Our results indicate that searches based on opinion units are more successful than those using traditional data-segmentation strategies, demonstrating robustness across datasets and embeddings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OaBqHlz3vP": {
    "title": "Organic Data-Driven Approach for Turkish Grammatical Error Correction and LLMs",
    "volume": "review",
    "abstract": "Grammatical Error Correction has seen significant progress with the recent advancements in deep learning. As those methods require huge amounts of data, synthetic datasets are being built to fill this gap. Unfortunately, synthetic datasets are not organic enough in some cases and even require clean data to start with. Furthermore, most of the work that has been done is focused mostly on English. In this work, we introduce a new organic data-driven approach, clean insertions, to build parallel Turkish Grammatical Error Correction datasets from any organic data, and to clean the data used for training Large Language Models. We achieve state-of-the-art results on two Turkish Grammatical Error Correction test sets out of the three publicly available ones. We also show the effectiveness of our method on the training losses of training language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d8pBs6OQZ4": {
    "title": "Prosody Detection improves Pretrained Automatic Speech Recognition",
    "volume": "review",
    "abstract": "We show the performance of Automatic Speech Recognition (ASR) systems that use semi-supervised speech representations can be be boosted by a complimentary prosody detection module, by introducing a joint ASR and prosody detection model. The prosody detection component of our model achieves a significant improvement on the state-of-the-art for the task, closing the gap in F1-score by 41%. Additionally, the ASR performance in joint training decreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With these results, we show the importance of extending pretrained speech models to retain or relearn important prosodic cues",
    "checked": false,
    "id": "6afb36e70a31aa8421d2584f430b02596b11cc17",
    "semantic_title": "reading miscue detection in primary school through automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q3hhdoJo5v": {
    "title": "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) are essential tools that users employ across various scenarios, so evaluating their performance and guiding users in selecting the suitable service is important. Although many benchmarks exist, they mainly focus on specific predefined model abilities, such as world knowledge, reasoning, etc. Based on these ability scores, it is hard for users to determine which LLM best suits their particular needs. To address these issues, we propose to evaluate LLMs from a user-centric perspective and design this benchmark to measure their efficacy in satisfying user needs under distinct intents. Firstly, we collect 1,846 real-world use cases from a user study with 712 participants from 23 countries. This first-hand data helps us understand actual user intents and needs in LLM interactions, forming the User Reported Scenarios (URS) dataset, which is categorized with six types of user intents. Secondly, based on this authentic dataset, we benchmark 10 LLM services with GPT-4-as-Judge. Thirdly, we show that benchmark scores align well with human preference in both real-world experience and pair-wise annotations, achieving Pearson correlations of 0.95 and 0.94, respectively. This alignment confirms that the URS dataset and our evaluation method establish an effective user-centric benchmark. The dataset, code, and process data are anonymously available at https://anonymous.4open.science/r/URS/",
    "checked": false,
    "id": "b16c1158043fc34ca1d06d59460fe1be6b89325c",
    "semantic_title": "a user-centric benchmark for evaluating large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=rzkx5AGJ3N": {
    "title": "Data Management For Training Large Language Models: A Survey",
    "volume": "review",
    "abstract": "Data plays a fundamental role in training Large Language Models (LLMs). Efficient data management, particularly in formulating a well-suited training dataset, is significant for enhancing model performance and improving training efficiency during pretraining and supervised fine-tuning stages. Despite the considerable importance of data management, the underlying mechanism of current prominent practices are still unknown. Consequently, the exploration of data management has attracted more and more attention among the research community. This survey aims to provide a comprehensive overview of current research in data management within both the pretraining and supervised fine-tuning stages of LLMs, covering various aspects of data management strategy design. Looking into the future, we extrapolate existing challenges and outline promising directions for development in this field. Therefore, this survey serves as a guiding resource for practitioners aspiring to construct powerful LLMs through efficient data management practices",
    "checked": true,
    "id": "2f79b5eebe8f04566938d4d1eafbc885346f4f80",
    "semantic_title": "data management for training large language models: a survey",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=CRtPOGbNOd": {
    "title": "Cross-Lingual Multi-Hop Knowledge Editing -- Benchmarks, Analysis and a Simple Contrastive Learning based Approach",
    "volume": "review",
    "abstract": "Large language models (LLMs) are often expected to be constantly adapted to new sources of knowledge and knowledge editing techniques aim to efficiently patch the outdated model knowledge, with minimal modification. Most prior works focus on monolingual knowledge editing in English, even though new information can emerge in any language from any part of the world. We propose the Cross-Lingual Multi-Hop Knowledge Editing paradigm, for measuring and analyzing the performance of various SoTA knowledge editing techniques in a cross-lingual setup. Specifically, we create a parallel cross-lingual benchmark, CroLin-MQuAKE for measuring the knowledge editing capabilities. Our extensive analysis over various knowledge editing techniques uncover significant gaps in performance between the cross-lingual and English-centric setting. Following this, we propose a significantly improved system for cross-lingual multi-hop knowledge editing, CLeVer-CKE. CLeVer-CKE is based on a retrieve, verify and generate knowledge editing framework, where a retriever is formulated to recall edited facts and support an LLM to adhere to knowledge edits. We develop language-aware and hard-negative based contrastive losses for improving the cross-lingual and fine-grained fact retrieval and verification process used within this framework. Extensive experiments across three LLMs, eight languages, and two datasets show the CLeVer-CKE's significant gains of up to 30\\% over prior methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vqD0wLCRli": {
    "title": "Streamlining Knowledge Discovery in Scientific Literature: A Comprehensive End-to-End System for Research Artifact Analysis",
    "volume": "review",
    "abstract": "Knowledge Discovery and Research Artifact Analysis (RAA) are crucial for promoting reproducibility and reusability in scientific research. In this work, we introduce a novel end-to-end system to efficiently identify and analyze tangible research artifacts (RAs), specifically datasets and software, within scientific literature. Building on recent advancements, our architecture employs Large Language Models (LLMs) fine-tuned with the Low-Rank Adaptation (LoRA) method to streamline the process of RAA into an instruction-based Question Answering (QA) task. The system comprises five stages: (i) candidate detection using a list of curated keywords and gazetteers, (ii) RA mention identification and validation, (iii) extraction of RA mention metadata, such as names, versions, licenses, and URLs, (iv) classification of RA mentions by usage and provenance, and (v) deduplication of RA mentions to ensure the uniqueness of each identified RA. Through benchmarking on two RA mention datasets, we demonstrated robust performance in RAA and provided a comprehensive qualitative analysis, underscoring the nuances and complexities of ensuring reproducibility and reusability in diverse scientific fields",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uy39I0axE8": {
    "title": "Step-level Value Preference Optimization for Mathematical Reasoning",
    "volume": "review",
    "abstract": "Direct Preference Optimization (DPO) using an implicit reward model has proven to be an effective alternative to reinforcement learning from human feedback (RLHF) for fine-tuning preference aligned large language models (LLMs). However, the overall preference annotations of responses do not fully capture the fine-grained quality of model outputs in complex multi-step reasoning tasks, such as mathematical reasoning. To address this limitation, we introduce a novel algorithm called Step-level Value Preference Optimization (SVPO). Our approach employs Monte Carlo Tree Search (MCTS) to automatically annotate step-level preferences for multi-step reasoning. Furthermore, from the perspective of learning-to-rank, we train an explicit value model to replicate the behavior of the implicit reward model, complementing standard preference optimization. This value model enables the LLM to generate higher reward responses with minimal cost during inference. Experimental results demonstrate that our method achieves state-of-the-art performance on both in-domain and out-of-domain mathematical reasoning benchmarks",
    "checked": true,
    "id": "830c277b2992f59ec2f21982e245bd1e17dd85ca",
    "semantic_title": "step-level value preference optimization for mathematical reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u0GQVamcdA": {
    "title": "P-Distill: Efficient and Effective Prompt Tuning using Knowledge Distillation",
    "volume": "review",
    "abstract": "In the field of natural language processing (NLP), prompt-based learning is widely used for efficient parameter learning. However, this method has the drawback of shortening the input length by the extent of the attached prompt, leading to an inefficiency in utilizing the input space. In this study, we propose P-Distill, a novel prompt compression method that mitigates the aforementioned limitation of prompt-based learning while maintaining performance via knowledge distillation. The knowledge distillation process of P-Distill consists of two methods, namely prompt initialization and prompt distillation. Experiments on various NLP tasks demonstrate that P-Distill exhibits comparable or superior performance compared to other state-of-the-art prompt-based learning methods, even with significantly shorter prompts. Specifically, We achieve a peak improvement of 1.90% even with the prompt lengths compressed to one-eighth. An additional study further provides insights into the distinct impact of each method on the overall performance of P-Distill. Our code will be released upon acceptance",
    "checked": false,
    "id": "c1fc2546b1476b77448e01b9d7d0a50d1bf632d3",
    "semantic_title": "large language model distilling medication recommendation model",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=pet6Wynw5y": {
    "title": "Scoring Assessment Center Exercises with LLMs",
    "volume": "review",
    "abstract": "We compared six methods for classification-based scoring of an assessment center (AC) exercise that tests managerial coaching skills. The accurate scoring of these skills is crucial for selecting the most capable job candidates. Four of the methods we employed were based on fine-tuning â€” with Gemma-7b, Llama3-8b, Phi-3, and RoBERTa-base, respectively. The other two methods were zero-shot prompting and few-shot prompting with GPT-4o. Phi-3 and Gemma-7b performed the best across the fine-tuned LLMs, and Llama3-8b followed them closely. RoBERTa performed robustly, it had the best performance for one of the coaching skills, but in general performed slightly lower than the fine-tuned LLMs. Zero-shot and few-shot prompting with GPT-4o performed the worst, but zero-shot performed better than few-shot. The pattern of results indicates that the complexity and psychological nature of each skill might be interacting with model performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aVx6Fo86G5": {
    "title": "ACE: A LLM-based Negotiation Coaching System",
    "volume": "review",
    "abstract": "The growing prominence of LLMs has led to an increase in the development of AI tutoring systems. These systems are crucial in providing underrepresented populations with improved access to valuable education. One important area of education that is unavailable to many learners is strategic bargaining related to negotiation. To address this, we develop a LLM-based Assistant for Coaching nEgotiation (ACE). ACE not only serves as a negotiation partner for users but also provides them with targeted feedback for improvement. To build our system, we collect a dataset of negotiation transcripts between MBA students. These transcripts come from trained negotiators and emulate realistic bargaining scenarios. We use the dataset, along with expert consultations, to design an annotation scheme for detecting negotiation mistakes. ACE employs this scheme to identify mistakes and provide targeted feedback to users. To test the effectiveness of ACE-generated feedback, we conducted a user experiment with two consecutive trials of negotiation and found that it improves negotiation performances significantly compared to a negotiation agent that doesn't provide feedback",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JdVkgTAXi4": {
    "title": "KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems",
    "volume": "review",
    "abstract": "Embodied AI agents responsible for executing interconnected, long-sequence household tasks often face difficulties with in-context memory, leading to inefficiencies and errors in task execution. To address this issue, we introduce KARMA, an innovative memory system that integrates long-term and short-term memory modules, enhancing large language models (LLMs) for planning in embodied agents through memory-augmented prompting. KARMA distinguishes between long-term and short-term memory, with long-term memory capturing comprehensive 3D scene graphs as representations of the environment, while short-term memory dynamically records changes in objects' positions and states. This dual-memory structure allows agents to retrieve relevant past scene experiences, thereby improving the accuracy and efficiency of task planning. Short-term memory employs policies for effective and adaptive memory replacement, ensuring that critical information is retained while less important data is discarded. The memory-augmented embodied AI agent improves $1.9\\times$ success rates and $3.2\\times$ task execution efficiency. Through this plug-and-play memory system, KARMA significantly enhances the ability of embodied agents to generate coherent and contextually appropriate plans, making the execution of complex household tasks more efficient",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pkTPPfBryJ": {
    "title": "Rethinking Code Refinement: Learning to Judge Code Efficiency",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating codes. Due to these capabilities, many recent methods are proposed to automatically refine the codes with LLMs. However, we should rethink that the refined codes (from LLMs and even humans) are not always more efficient than their original versions. On the other hand, running two different versions of codes and comparing them every time is not ideal and time-consuming. Therefore, in this work, we propose a novel method based on the code language model that is trained to judge the efficiency between two different codes (generated across humans and machines) by either classifying the superior one or predicting the relative improvement. We validate our method on multiple programming languages with multiple refinement steps, demonstrating that the proposed method can effectively distinguish between more and less efficient versions of code",
    "checked": false,
    "id": "656d87d314009a9f1925348d30652f39e0cc7ed5",
    "semantic_title": "de-biased teacher: rethinking iou matching for semi-supervised object detection",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=W4bEjhSPtI": {
    "title": "Uncovering the Intention Behind Equations in Mathematical Problems",
    "volume": "review",
    "abstract": "Mathematical Equation Intent Recognition(MEIR) is a novel task aimed at identifying the intentions behind mathematical equations that people produce while solving math world problems(MWPs). We observe that, in previous research, researchers have often focused on how to let large language models(LLMs) correctly solve an MWP. However, focusing solely on the reasoning behind each step of a correct inference process is insufficient. We prefer that LLMs can provide guidance on the process of solving MWPs for students in educational settings. Therefore, they need to adjust the strategy based on the student's responses. We notice that, unlike existing mathematical datasets, students typically do not provide overly detailed descriptions of their steps in the real world. As a result, it is crucial for LLMs to possess the capability to understand the intention they produce those equations. We treat MEIR as a generation task, requiring models to summarize the intent in a single sentence. We also propose a data augmentation framework and utilized this framework to generate a benchmark called Grade School Math Intention(GSMI). To evaluate MEIR task, we benchmark serveral LLMs on GSMI dataset. The results indicate that there is still significant room for improvement in the performance of general-purpose LLMs on the MEIR task. Conversely, capabilities acquired during pre-training and fine-tuning specifically in the field of mathematics significantly contribute to the model's ability to tackle those problems. Codes and datasets are available on https://github.com/ch-666-six/MEIR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5uweK9D3dN": {
    "title": "Aspect Information Enhanced Contrastive Learning for Aspect-based Sentiment Analysis",
    "volume": "review",
    "abstract": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Despite significant improvements in this field, progress is hindered by challenges such as the scarcity of context for specific aspects, interference from irrelevant words in sentences, and a lack of research focus on leveraging correlations between samples. To address these issues, we introduce a novel method named Aspect Information Enhanced Contrastive Learning Learning (AIECL) for ABSA. Firstly, we employ cutting-edge prompting techniques with Large Language Models (LLMs) to generate nuanced aspect-specific descriptions, thereby enhancing contexts related to the aspect. Subsequently, we design a novel fusion module aimed at seamlessly amalgamating aspectual insights with the original sentence structure. Finally, we develop three pioneering contrastive learning strategies aimed at exploring and learning complex correlations between samples, which is crucial for fine-grained sentiment analysis. Experiments on six benchmark datasets demonstrate that our AIECL method substantially outperforms state-of-the-art techniques and provides valuable insights for applying LLMs to downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wVyceGU0CC": {
    "title": "Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons",
    "volume": "review",
    "abstract": "In this paper, we investigate whether Large Language Models (LLMs) actively recall or retrieve their internal repositories of factual knowledge when faced with reasoning tasks. Through an analysis of LLMs' internal factual recall at each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness the critical factual associations under certain circumstances. Instead, they tend to opt for alternative, shortcut-like pathways to answer reasoning questions. By manually manipulating the recall process of parametric knowledge in LLMs, we demonstrate that enhancing this recall process directly improves reasoning performance whereas suppressing it leads to notable degradation. Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a powerful technique for addressing complex reasoning tasks. Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and reliable reasoning. Furthermore, we explored how contextual conflicts affect the retrieval of facts during the reasoning process to gain a comprehensive understanding of the factual recall behaviors of LLMs. Code and data will be available soon",
    "checked": true,
    "id": "72e06972d652b53809a4d0934ee2e999393da0ce",
    "semantic_title": "unveiling factual recall behaviors of large language models through knowledge neurons",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VHujO38XX5": {
    "title": "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations",
    "volume": "review",
    "abstract": "The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (GARAG), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our GARAG to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world. The code will be disclosed after acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o7AIK7ieho": {
    "title": "Native Design Bias: Studying the Impact of English Nativeness on Language Model Performance",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) excel at providing information acquired during pretraining on large-scale corpora and following instructions through user prompts. This study investigates whether the quality of LLM responses varies depending on the demographic profile of users. Considering English as the global lingua franca, along with the diversity of its dialects among speakers of different native languages, we explore whether non-native English speakers receive lower-quality or even factually incorrect responses from LLMs more frequently. Our results show that performance discrepancies occur when LLMs are prompted by native versus non-native English speakers and persist when comparing native speakers from Western countries with others. Additionally, we find a strong anchoring effect when the model recognizes or is made aware of the user's nativeness, which further degrades the response quality when interacting with non-native speakers. Our analysis is based on a newly collected dataset with over 12,000 unique annotations from 124 annotators, including information on their native language and English proficiency",
    "checked": true,
    "id": "3f2f14be57a67ca8509f96e986308c8d878faf26",
    "semantic_title": "native design bias: studying the impact of english nativeness on language model performance",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s7FF2HPft0": {
    "title": "Span-Based Semantic Role Labeling with Contrastive Learning",
    "volume": "review",
    "abstract": "Contrastive learning is widely recognized for its ability to understand the relationships between data and map them into a high-dimensional feature space. In this study, we apply this technique to semantic role labeling, constructing a model that effectively captures the relationships between spans and labels and determines spans accurately. Our model integrates the characteristics of both a conventional span-based model, which predicts spans for labels, and a model that is comparable to state-of-the-art, which predicts labels for spans. In our experiments, we apply these models to NPCMJ-PT, a Japanese corpus that is annotated with semantic role labels and has about 52,500 entries. The semantic roles are defined with 32 types of labels such as Arg0, Arg1 and ArgM-LOC, which are similar to PropBank. The experimental results show that our model outperforms the conventional span-based models, achieving a highest F1 score of 81.2",
    "checked": false,
    "id": "0cd40131efb7bc4aab9c1f8f630c19fb45938e8d",
    "semantic_title": "semantic role labeling guided out-of-distribution detection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qhvLPnO3Ou": {
    "title": "Self-training Large Language Models through Knowledge Detection",
    "volume": "review",
    "abstract": "Large language models (LLMs) often necessitate extensive labeled datasets and training compute to achieve impressive performance across downstream tasks. This paper explores a self-training paradigm, where the LLM autonomously curates its own labels and selectively trains on unknown data samples identified through a reference-free consistency method. Empirical evaluations demonstrate significant improvements in reducing hallucination in generation across multiple subjects. Furthermore, the selective training framework mitigates catastrophic forgetting in out-of-distribution benchmarks, addressing a critical limitation in training LLMs. Our findings suggest that such an approach can substantially reduce the dependency on large labeled datasets, paving the way for more scalable and cost-effective language model training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3rGbu05VwO": {
    "title": "Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs",
    "volume": "review",
    "abstract": "Multilingual large language models (LLMs) have greatly increased the ceiling of performance on non-English tasks. However, the mechanisms behind multilingualism in these LLMs are poorly understood. Of particular interest is the degree to which internal representations are shared between languages. Recent work on neuron analysis of LLMs has focused on the monolingual case, and the limited work on the multilingual case has not considered the interaction between tasks and linguistic representations. In our work, we investigate how neuron activation is shared across languages by categorizing neurons into four distinct groups according to their responses across different languages for a particular input: all-shared, partial-shared, specific, and non-activated. This categorization is combined with a study of neuron attribution, i.e. the importance of a neuron w.r.t an output. Our analysis reveals the following insights: (i) the linguistic sharing patterns are strongly affected by the type of task, but neuron behavior changes across different inputs even for the same task; (ii) all-shared neurons play a key role in generating correct responses; (iii) boosting multilingual alignment by increasing all-shared neurons can enhance accuracy on multilingual tasks. We will release the code to foster research in this area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3gqcH4Gkp0": {
    "title": "Source Attribution for Large Language Model-Generated Data",
    "volume": "review",
    "abstract": "The impressive performances of Large Language Models (LLMs) and their immense potential for commercialization have given rise to serious concerns over the Intellectual Property (IP) of their training data. In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs. To this end, it is imperative to be able to perform source attribution by identifying the data provider who contributed to the generation of a synthetic text by an LLM. In this paper, we show that this problem can be solved by watermarking, i.e., by enabling an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s). We identify the key properties of such watermarking frameworks (e.g., source attribution accuracy, robustness against adversaries), and propose a source attribution framework that satisfies these key properties due to our algorithmic designs. Our framework enables an LLM to learn an accurate mapping from the generated texts to data providers, which sets the foundation for effective source attribution. Extensive empirical evaluations show that our framework achieves effective source attribution",
    "checked": false,
    "id": "b590a26956f3057a348f180731ece3e44b30c5a9",
    "semantic_title": "wasa: watermark-based source attribution for large language model-generated data",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=R5YLhou5PX": {
    "title": "TriageAgent: Towards Better Multi-Agents Collaborations for Large Language Model-Based Clinical Triage",
    "volume": "review",
    "abstract": "The global escalation in emergency department patient visits poses significant challenges to efficient clinical management, particularly in clinical triage. Traditionally managed by human professionals, clinical triage is susceptible to substantial variability and high workloads. Although large language models (LLMs) demonstrate promising reasoning and understanding capabilities, directly applying them to clinical triage remains challenging due to the complex and dynamic nature of the clinical triage task. To address these issues, we introduce TriageAgent, a novel heterogeneous multi-agent framework designed to enhance collaborative decision-making in clinical triage. TriageAgent leverages LLMs for role-playing, incorporating self-confidence and early-stopping mechanisms in multi-round discussions to improve document reasoning and classification precision for triage tasks. In addition, TriageAgent employs the medical Emergency Severity Index (ESI) handbook through a retrieval-augmented generation (RAG) approach to provide precise clinical knowledge and integrates both coarse- and fine-grained ESI-level predictions in the decision-making process. Extensive experiments demonstrate that TriageAgent outperforms state-of-the-art LLM-based methods on three clinical triage test sets. Furthermore, we have released the first public benchmark dataset for clinical triage with corresponding ESI levels and human expert performance for comparison",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pl8gvW3nJn": {
    "title": "Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game",
    "volume": "review",
    "abstract": "The New York Times Connections game has emerged as a popular and challenging pursuit for word puzzle enthusiasts. We collect 200 Connections games to evaluate the performance of state-of-the-art large language models (LLMs) against expert and novice human players. Our results show that even the best-performing LLM, GPT-4o, which has otherwise shown impressive reasoning abilities on a wide variety of benchmarks, can only fully solve 8\\% of the games. Compared to GPT-4o, novice and expert players perform better, with expert human players significantly outperforming GPT-4o. To deepen our understanding we create a taxonomy of the knowledge types required to successfully categorize words in the Connections game, revealing that LLMs struggle with associative, encyclopedic, and linguistic knowledge. Our findings establish the New York Times Connections game as a challenging benchmark for evaluating abstract reasoning capabilities in humans and AI systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XHjrLKVsrO": {
    "title": "Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated impressive performance and spurred numerous AI applications, in which role-playing agents (RPAs) are particularly popular, especially for fictional characters. The prerequisite for these RPAs lies in the capability of LLMs to understand characters from fictional works. Previous efforts have evaluated this capability via basic classification tasks or characteristic imitation, failing to capture the nuanced character understanding with LLMs. In this paper, we propose evaluating LLMs' character understanding capability via the character profiling task, i.e., summarizing character profiles from corresponding materials, a widely adopted yet understudied practice for RPA development. Specifically, we construct the CROSS dataset from literature experts and assess the generated profiles by comparing ground truth references and their applicability in downstream tasks. Our experiments, which cover various summarization methods and LLMs, have yielded promising results. These results strongly validate the character understanding capability of LLMs. Resources of this paper will be released upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M977txkdXs": {
    "title": "Representative Chain-of-Reasoning for Aspect Sentiment Quad Prediction",
    "volume": "review",
    "abstract": "Aspect Sentiment Quad Prediction (ASQP) is a crucial sentiment analysis task that has attracted increasing attention. The most recent studies focus on generating complete sentiment quadruples through end-to-end generative models. However, these methods heavily depend on labeled data quality and quantity, performing poorly in low-resource scenarios and less suitable for real-world applications. To address these issues, we propose a novel Representative Chain-of-Reasoning framework (RCR), with the aim of providing representative knowledge for large language models (LLMs) and fully activating their reasoning capabilities for ASQP. Specifically, we develop a Chain Prompting (ChaPT) module to decompose the ASQP task into three subtasks using the stepby-step reasoning mechanism. Then, a Representative Demonstration Retriever (RepDR) is introduced to provide ChaPT with representative demonstrations, balancing diversity and similarity, and enhancing the reasoning capabilities of LLMs at each step. Experimental results confirm the superiority of RCR in both zero-shot and few-shot scenarios, significantly surpassing existing counterparts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3RTAjD5cZB": {
    "title": "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL",
    "volume": "review",
    "abstract": "In-context learning with large language models (LLMs) is the current mainstream method for text-to-SQL. Previous studies have explored selecting relevant demonstrations from a human-labeled demonstration pool, but these methods lack diversity and incur high labeling costs. In this work, we address measuring and enhancing the diversity of the text-to-SQL demonstration pool. First, we introduce a diversity metric and present that the diversity of the existing labeling data can be further enhanced. Motivated by these findings, we propose **Fused** that iteratively fuses demonstrations to create a diverse demonstration pool based on human labeling or even from scratch with LLMs, reducing labeling costs. **Fused** achieves an average improvement of 3.2% based on existing labeling and 5.0% from scratch on several mainstream datasets, demonstrating its effectiveness",
    "checked": true,
    "id": "2d125b26788e41586fa1594da60169f33fb481f0",
    "semantic_title": "improving demonstration diversity by human-free fusing for text-to-sql",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=bLFEhvNYU8": {
    "title": "Vernacular? I Barely Know Her: Challenges with Style Control and Stereotyping",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are increasingly being used in educational and learning applications. Research has demonstrated that controlling for style, to fit the needs of the learner, fosters increased understanding, promotes inclusion, and helps with knowledge distillation. To understand the capabilities and limitations of contemporary LLMs in style control, we evaluated five state-of-the-art models: GPT-3.5, GPT-4, GPT-4o, Llama-3, and Mistral-instruct- 7B across two style control tasks. We observed significant inconsistencies in the first task, with model performances averaging between 5th and 8th grade reading levels for tasks intended for first-graders, and standard deviations up to 27.6. For our second task, we observed a statistically significant improvement in performance from 0.02 to 0.26. However, we find that even without stereotypes in reference texts, LLMs often generated culturally insensitive content during their tasks. We provide a thorough analysis and discussion of the results",
    "checked": true,
    "id": "060905737e702e380d01b465944368b389565f15",
    "semantic_title": "vernacular? i barely know her: challenges with style control and stereotyping",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=IALYg2CH5c": {
    "title": "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures",
    "volume": "review",
    "abstract": "To handle the vast amounts of qualitative data produced in corporate climate communication, stakeholders increasingly rely on Retrieval Augmented Generation (RAG) systems. However, a significant gap remains in evaluating domain-specific information retrieval -- the basis for answer generation. To address this challenge, this work simulates the typical tasks of a sustainability analyst by examining 30 sustainability reports with 16 detailed climate-related questions. As a result, we obtain a dataset with over 8.5K unique question-source-answer pairs labeled by different levels of relevance. Furthermore, we develop a use case with the dataset to investigate the integration of expert knowledge into information retrieval with embeddings. Although we show that incorporating expert knowledge works, we also outline the critical limitations of embeddings in knowledge-intensive downstream domains like climate change communication",
    "checked": true,
    "id": "dd4db8841fc2471815cc6b41bc209cefdf960491",
    "semantic_title": "climretrieve: a benchmarking dataset for information retrieval from corporate climate disclosures",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T2VCRZTVeI": {
    "title": "People will agree what I think: Investigating LLM's False Consensus Effect",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have recently been widely adopted on interactive systems requiring communications. As the false belief in a model can harm the usability of such systems, LLMs should not have cognitive biases that humans have. Especially psychologists focused on the False Consensus Effect (FCE), which can distract smooth communication by posing false beliefs. However, previous studies have less examined FCE in LLMs thoroughly, which needs more consideration of confounding biases, general situations, and prompt changes. Therefore, in this paper, we conduct two studies to deeply examine the FCE phenomenon in LLMs. In Study 1, we investigate whether LLMs have FCE. In Study 2, we explore how various prompting styles affect the demonstration of FCE. As a result of these studies, we identified that popular LLMs have FCE. Also, the result specifies the conditions when the strength of FCE becomes larger or smaller compared to normal usage",
    "checked": true,
    "id": "c2f12dab4c0a31e69f2feef5609e3ebb2229448a",
    "semantic_title": "people will agree what i think: investigating llm's false consensus effect",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LbQCMP2GOR": {
    "title": "Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance in the legal domain, with GPT-4 even passing the uniform bar exam. However their efficacy remains limited for non-standardized tasks and tasks in languages other than English. This underscores the need for careful evaluation of LLMs within each legal system before application. Here, we introduce KBL, a benchmark for assessing the Korean legal language understanding of LLMs, consisting of (1) 7 legal knowledge tasks (503 examples), (2) 4 legal reasoning tasks (270 examples), and (3) the Korean bar exam (4 domains, 53 tasks, 2,510 examples). First two datasets were developed in close collaboration with lawyers to evaluate LLMs in practical scenarios in a certified manner. Furthermore, considering legal practitioners' frequent use of extensive legal documents for research, we assess LLMs in both a closed book setting, where they rely solely on internal knowledge, and a retrieval-augmented generation (RAG) setting, using a corpus of Korean statutes and precedents. The results indicate substantial room and opportunities for improvement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ApSBYVzkFj": {
    "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment",
    "volume": "review",
    "abstract": "Pre-trained language models like BERT and T5 serve as crucial backbone encoders for dense retrieval. However, these models often exhibit limited generalization capabilities and face challenges in improving in-domain accuracy. Recent research has explored using large language models (LLMs) as retrievers, achieving state-of-the-art performance across various tasks. Despite these advancements, the specific benefits of LLMs over traditional retrievers and the impact of different LLM configurationsâ€”such as parameter sizes, pre-training duration, and alignment processesâ€”on retrieval tasks remain unclear. In this work, we conduct a comprehensive empirical study on a wide range of retrieval tasks, including in-domain accuracy, data efficiency, zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. We evaluate over 15 different backbone LLMs and non-LLMs. Our findings reveal that larger models and extensive pre-training consistently enhance in-domain accuracy and data efficiency. Additionally, larger models demonstrate significant potential in zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. These results underscore the advantages of LLMs as versatile and effective backbone encoders in dense retrieval, providing valuable insights for future research and development in this field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8CLfC6V2Jb": {
    "title": "ProductAgent: Benchmarking Conversational Product Search Agent with Asking Clarification Questions",
    "volume": "review",
    "abstract": "This paper introduces the task of *product demand clarification* within an e-commercial scenario, where the user commences the conversation with ambiguous queries and the task-oriented agent is designed to achieve more accurate and tailored product searching by asking clarification questions. To address this task, we propose **ProductAgent**, a conversational information seeking agent equipped with abilities of strategic clarification question generation and dynamic product retrieval. Specifically, we develop the agent with strategies for product feature summarization, query generation, and product retrieval. Furthermore, we propose the benchmark called **PRPCLARE** to evaluate the agent's performance both automatically and qualitatively with the aid of a LLM-driven user simulator. Experiments show that ProductAgent interacts positively with the user and enhances retrieval performance with increasing dialogue turns, where user demands become gradually more explicit and detailed",
    "checked": true,
    "id": "7763b85817b2c6eb0bc64b32198f722ce120ce30",
    "semantic_title": "productagent: benchmarking conversational product search agent with asking clarification questions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DY063SQ2Cy": {
    "title": "Limited Out-of-Context Knowledge Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities as knowledge bases and significant in-context reasoning capabilities. However, previous work challenges their out-of-context reasoning ability, i.e., the ability to infer information from their training data, instead of from the context or prompt. This paper focuses on a significant facet of out-of-context reasoning: Out-of-Context Knowledge Reasoning (OCKR), which is to combine multiple knowledge to infer new knowledge. We designed a synthetic dataset with seven representative OCKR tasks to systematically assess the OCKR capabilities of LLMs. Using this dataset, we evaluated the LLaMA2-13B-chat model and discovered that its proficiency in this aspect is limited, regardless of whether the knowledge is trained in a separate or adjacent training settings. Moreover, training the model to reason with complete reasoning data did not result in significant improvement. Training the model to perform explicit knowledge retrieval helps in only one of the tasks, indicating that the model's limited OCKR capabilities are due to difficulties in retrieving relevant knowledge. Furthermore, we treat cross-lingual knowledge transfer as a distinct form of OCKR, and evaluate this ability. Our results show that the evaluated model also exhibits limited ability in transferring knowledge across languages",
    "checked": true,
    "id": "fa055a6949629b53c4949b7a199687ee28233467",
    "semantic_title": "limited out-of-context knowledge reasoning in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dgv6ZX2kNp": {
    "title": "Exploring the Memory Ability of Large Language Models",
    "volume": "review",
    "abstract": "The memory ability is critical for large language models (LLMs). However, the difference between small LLMs and large LLMs in memory ability is still blurring. In this paper, we study the memory ability of them. To this end, we create a knowledge based dataset with frequency. This dataset is based on Wikidata5M, and we count the number of co-occur documents in Wikipedia and Baidu Baike of head and tail entity as the fact's frequency. Our experiments demonstrate that large LLMs has strong memory ability. They can remember most facts even with low frequency. Small LLMs, on the contrary, can only remember part of the high frequency facts, not to mention low frequency facts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DVtTMQ7CJj": {
    "title": "DragFT: Adapting Large Language Models with Dictionary and Retrieval Augmented Fine-Tuning for Domain-specific Machine Translation",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown great potential in domain-specific machine translation (MT). However, one major issue is that LLMs trained on general corpus might not generalize well to specific domains due to the lack of domain-specific knowledge. To address this issue, this paper focuses on enhancing the domain-specific MT capability of LLMs, by providing high-quality training datasets and proposing a novel fine-tuning framework denoted by DragFT. DragFT augments LLMs via three techniques: Dictionary-enhanced prompting improves domain-specific terminology translation; RAG-based few-shot example selection provides high-quality examples that simulate both the domain and style characteristics; Fine-tuning with few-shot examples further boosts fine-tuning with in-domain examples. We deploy DragFT on three well-known LLM backbones to validate its effectiveness. The results on three domain-specific datasets show that DragFT achieves a significant performance boost and shows superior performance compared to strong baselines such as GPT-3.5 and GPT-4o. The drastic performance improvement of DragFT over existing LLMs can be attributed to the incorporation of relevant knowledge while mitigating noise. Our three well-constructed datasets can accelerate future research in domain-specific MT: a benchmark dataset designed for MT within the IT domain, and two datasets constructed from publicly available datasets respectively in law and medicine",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qpiH4K3qcr": {
    "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
    "volume": "review",
    "abstract": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific downstream tasks by utilizing labeled examples as demonstrations (demos) in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. This work introduces a novel transferable attack against ICL to address these issues, aiming to hijack LLMs to generate the target response or jailbreak. Our hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demos without directly contaminating the user queries. Comprehensive experimental results across different generation and jailbreaking tasks highlight the effectiveness of our hijacking attack, resulting in distracted attention towards adversarial tokens and consequently leading to unwanted target outputs. We also propose a defense strategy against hijacking attacks through the use of extra clean demos, which enhances the robustness of LLMs during ICL. Broadly, this work reveals the significant security vulnerabilities of LLMs and emphasizes the necessity for in-depth studies on their robustness",
    "checked": true,
    "id": "6d68b5c1eaf03aba857476a9825acf3e48edd840",
    "semantic_title": "hijacking large language models via adversarial in-context learning",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=a08QLmtYEi": {
    "title": "Learning to Extract Structured Entities Using Language Models",
    "volume": "review",
    "abstract": "Recent advances in machine learning have significantly impacted the field of information extraction, with Language Models (LMs) playing a pivotal role in extracting structured information from unstructured text. Prior works typically represent information extraction as triplet-centric and use classical metrics such as precision and recall for evaluation. We reformulate the task to be entity-centric, enabling the use of diverse metrics that can provide more insights from various perspectives. We contribute to the field by introducing Structured Entity Extraction and proposing the Approximate Entity Set OverlaP (AESOP) metric, designed to appropriately assess model performance. Later, we introduce a new model that harnesses the power of LMs for enhanced effectiveness and efficiency by decomposing the extraction task into multiple stages. Quantitative and human side-by-side evaluations confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction. Our source code and datasets are available at this anonymous link: https://anonymous.4open.science/r/Learning-to-Extract-Structured-Entities-Using-Language-Models-7310/README.md",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PtC74QYtva": {
    "title": "Adapt Language Agent to Different Task via Automatic Mechanism Activation",
    "volume": "review",
    "abstract": "Language Agent (LA) could be endowed with different mechanisms for autonomous task accomplishment. Current LAs typically rely on fixed mechanism or a set of mechanisms activated in a predefined order, limiting their adaptability to varied potential task solution structures. To this end, this paper introduces Unify agent mechanisms by Actions (UniAct), a unified agent that integrates different mechanisms. Additionally, we propose Automatic Language Agent Mechanism Activation Learning with Self-Exploration (ALAMA), which focuses on optimizing mechanism activation adaptability without reliance on expert models. By leveraging self-generated UniAct trajectories with different rewards, ALAMA enables the agent to adaptively activate mechanisms that may result in high downstream task rewards based on the potential characteristics of the task. Experimental results demonstrate significant improvements in downstream agent tasks, affirming the effectiveness of our approach in facilitating more dynamic and context-sensitive mechanism activation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=usICOeTMCQ": {
    "title": "Parameter-Efficient Instruction Tuning Code Large Language Models: A Comprehensive Study on OctoCoder Models",
    "volume": "review",
    "abstract": "The high cost of full-parameter fine-tuning (FFT) of Large Language Models (LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods. However, it remains unclear which methods provide the best cost-performance trade-off at different model scales. We introduce Astraios, a suite of 28 instruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up to 16 billion parameters. Through investigations across 5 tasks and 8 different datasets encompassing both code comprehension and code generation tasks, we find that FFT generally leads to the best downstream performance across all scales, and PEFT methods differ significantly in their efficacy based on the model scale. LoRA usually offers the most favorable trade-off between cost and performance. Further investigation into the effects of these methods on both model robustness and code security reveals that larger models tend to demonstrate reduced robustness and less security. At last, we explore the relationships among updated parameters, and task performance. We find that the tuning effectiveness observed in small models generalizes well to larger models, and the validation loss in instruction tuning can be a reliable indicator of overall downstream performance. We believe that our findings of PEFT can generalize to other decoder-only LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UCE2Q4XLbN": {
    "title": "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization",
    "volume": "review",
    "abstract": "Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient Fine-Tuning (PEFT) method, significantly enhances the training efficiency by updating only a small portion of the weights in Large Language Models (LLMs). Recently, weight-only quantization techniques have also been applied to LoRA methods to reduce the memory footprint of fine-tuning. However, applying weight-activation quantization to the LoRA pipeline is under-explored, and we observe substantial performance degradation primarily due to the presence of activation outliers. In this work, we propose RoLoRA, the first LoRA-based scheme to apply rotation for outlier elimination, and then fine-tune rotated outlier-free LLMs for effective weight-activation quantization. Different from previous work tackling the outlier challenges from a post-training perspective, we propose rotation-aware fine-tuning to eliminate and preserve the outlier-free characteristics brought by rotation operations. RoLoRA can improve low-bit LoRA convergence and post-training quantization robustness in weight-activation settings. RoLoRA is evaluated across various LLM series (LLaMA2, LLaMA3, LLaVA-1.5), tasks, and quantization settings, achieving up to 29.5% absolute accuracy gain of 4-bit weight-activation quantized LLaMA2-13B on commonsense reasoning tasks compared to LoRA baseline. We further demonstrate its effectiveness on Large Multimodal Models (LMMs) and prove the compatibility with advanced LoRA variants",
    "checked": true,
    "id": "5e9f22bba13332709cc9880d7e197385bca316de",
    "semantic_title": "rolora: fine-tuning rotated outlier-free llms for effective weight-activation quantization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zsVdXouyPK": {
    "title": "Twice is Nice! Improving MEMIT for Knowledge Editing",
    "volume": "review",
    "abstract": "In this ever changing world, Large Language Models (LLMs) pose a major challenge on frequently retraining as they consume enormous resources. Direct knowledge editing emerged as an efficient alternative, where it locates a stale factual knowledge in the LLM's layers and edits those layers' weights in order for the LLM to generate new factual knowledge. We observed that MEMIT, state-of-the art knowledge editing algorithm is not used at its full potential. In this paper, we empirically demonstrated the limitations of executing only one single MEMIT update. We then proposed an intuitive and straightforward solution, Running MEMIT twice, and showed its effectiveness over two knowledge editing datasets compared to strong baselines. We conducted extensive analysis to understand the effectiveness of our solution. In particular we analyzed multiple runs of MEMIT and found out the performance to plateaus at second run of MEMIT. To discern the reason we analyzed the gradients between each run and found negligible change in gradients between second and third run of MEMIT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4UDIBUIkwv": {
    "title": "Improving Language Transfer Capability of Decoder-only Architecture in Multilingual Neural Machine Translation",
    "volume": "review",
    "abstract": "Decoder-only architecture performs poorly in multilingual neural machine translation, despite its potential benefits in zero-shot translation, i.e., translation of unseen language pairs during training. In this work, we identify the main issue of the decoder-only architecture as its lack of language transfer capability. Specifically, representations from different source languages are not aligned in the representational subspace of the target language. We propose dividing the decoding process into two stages so that target tokens are explicitly excluded in the first stage to implicitly boost the transfer capability across languages. Additionally, we impose contrastive learning on translation instructions, resulting in improved performance in zero-shot translation. We conduct experiments on TED-19 and OPUS-100 datasets, considering both training from scratch and fine-tuning scenarios. Experimental results show that, compared to the encoder-decoder architecture, our methods not only perform competitively in supervised translations but also achieve improvements of up to 3.39 BLEU, 6.99 chrF++, 3.22 BERTScore, and 4.81 COMET in zero-shot translations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Slu4Bvqu41": {
    "title": "Can LLM be a Personalized Judge",
    "volume": "review",
    "abstract": "As large language models (LLMs) gain widespread adoption, ensuring they cater to diverse user needs has become increasingly important. While many researchers have studied LLM personalization and role-playing, they primarily use LLM-as-a-Judge for evaluation without thoroughly examining its validity. This paper investigates the reliability of LLM-as-a-Personalized-Judgeâ€”asking LLMs to judge user preferences based on persona. Our results suggest that LLM-as-a-Personalized-Judge is less reliable for personalization than previously believed, showing low agreement with human ground truth. We observed that the personas provided to the LLM often have limited predictive power for the tasks, leading us to introduce verbal uncertainty estimation. We find that powerful LLMs are aware of the certainty of their prediction and can achieve high agreement with ground truth on high-certainty samples, indicating a promising approach for building reliable and scalable proxies for evaluating LLM personalization. Our human annotation reveals that third-person crowd worker evaluations of personalized preferences are even worse than LLM predictions, highlighting the challenges of evaluating LLM personalization",
    "checked": false,
    "id": "0012eb77f5627d4f960a0c8c1f4848033a59e52f",
    "semantic_title": "can llm be a personalized judge?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=7td0k31obl": {
    "title": "AnyTrans: Translate AnyText in the Image with Large Scale Models",
    "volume": "review",
    "abstract": "This paper introduces AnyTrans, an all-encompassing framework for the task--Translate AnyText in the Image (TATI), which includes multilingual text translation and text fusion within images. Our framework leverages the strengths of large-scale models, such as Large Language Models (LLMs) and text-guided diffusion models, to incorporate contextual cues from both textual and visual elements during translation. The few-shot learning capability of LLMs allows for the translation of fragmented texts by considering the overall context. Meanwhile, the advanced inpainting and editing abilities of diffusion models make it possible to fuse translated text seamlessly into the original image while preserving its style and realism. Additionally, our framework can be constructed entirely using open-source models and requires no training, making it highly accessible and easily expandable. To encourage advancement in the \\taskname task, we have meticulously compiled a test dataset called MTIT6, which consists of multilingual text image translation data from six language pairs",
    "checked": true,
    "id": "0db468a7f7d52b1edba7a2ff78a8cf0eb2e36f85",
    "semantic_title": "anytrans: translate anytext in the image with large scale models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jVggyXpdeF": {
    "title": "GeNRe: a French Gender-Neutral Rewriting System Using Collective Nouns",
    "volume": "review",
    "abstract": "A significant portion of the textual data used in the field of Natural Language Processing (NLP) exhibits gender biases, particularly due to the use of masculine generics (masculine words that are supposed to refer to mixed groups of men and women), which can perpetuate and amplify stereotypes. Gender rewriting, a NLP task that involves automatically detecting and replacing gendered forms with neutral or opposite forms (e.g., from masculine to feminine), can be employed to mitigate these biases. Such systems are available for English, Arabic, Portuguese and German, but no French system is available. We create an original French gender-neutral rewriting system using collective nouns, which are gender-fixed in French. This paper presents GeNRe, the very first French gender-neutral rewriting system. We introduce a rule-based system (RBS) tailored for the French language alongside two fine-tuned large language models trained on data generated by our RBS. We also explore the use of instruction models to enhance the performance of our other systems and find that Claude 3 Opus combined with our dictionary achieves results close to our RBS. Through this contribution, we hope to promote the advancement of gender bias mitigation techniques in NLP for French",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yr9iwPBl12": {
    "title": "S-Agent: an Agent Collaborative Framework \\Inspired by the Scientific Methodology",
    "volume": "review",
    "abstract": "An increasing number of advancements have been accomplished in agents empowered by Large Language Models (LLM), particularly in resolving simple dialogue tasks. However, existing agents still face intractable robustness issues for solving complex tasks, encountering the cascading hallucinations induced by multi-step invocations of LLM. Certain recent studies utilize multi-step reasoning, planning strategies, and domain workflows to improve the success rate of complex tasks, yet they neglect the scientific methodology that encompasses the accumulated wisdom derived from centuries of scientific inquiry. Drawing inspiration from the scientific methodology, we propose the S-Agent - an agent collaborative framework meticulously designed to actively experiment and refine theories based on the analysis of experimental results, thereby enhancing the deductive capabilities of LLMs and complementing their inductive and communicative strengths. Additionally, we introduce an innovative parallel planning methodology, wherein agents with identical roles collaborate to simultaneously address the same inquiry. Extensive experiments demonstrate the effectiveness and efficiency of our approach. Notably, we achieve a new state-of-the-art $33.3\\%$ pass@1 accuracy on the LeetcodeHardGym coding benchmark and a relatively good $96.3\\%$ pass@1 on HumanEval with GPT-4",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9G9f1PGW7B": {
    "title": "RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning based on Emotional Information",
    "volume": "review",
    "abstract": "Misinformation is prevalent in various fields such as education, politics, health, etc., causing significant harm to society. However, current methods for cross-domain misinformation detection rely on time and resources consuming fine-tuning and complex model structures. With the outstanding performance of LLMs, many studies have employed them for misinformation detection. Unfortunately, they focus on in-domain tasks and do not incorporate significant sentiment and emotion features (which we jointly call affect). In this paper, we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework to address cross-domain misinformation detection using in-context learning based on affective information. It accomplishes this by applying an emotion-aware LLM to construct a retrieval database of affective embeddings. This database is used by our retrieval module to obtain source-domain samples, which are subsequently used for the inference module's in-context few-shot learning to detect target domain misinformation. We evaluate our framework on three misinformation benchmarks. Results show that RAEmoLLM achieves significant improvements compared to the zero-shot method on three datasets, with the highest increases of 20.69\\%, 23.94\\%, and 39.11\\% respectively. This work will be released on Github",
    "checked": true,
    "id": "c4d5e888a0ee0fe7cc7c8be5d91965985f7a2ebb",
    "semantic_title": "raemollm: retrieval augmented llms for cross-domain misinformation detection using in-context learning based on emotional information",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=50ZaXUDwb0": {
    "title": "Exploring Cultural Bias in Language Models Through Word Grouping Games",
    "volume": "review",
    "abstract": "Large language models (LLMs) can exhibit cultural bias, overlooking and misrepresenting cultural nuances. Models who unequally represent global cultures can reinforce harmful stereotypes. Evaluating the extent of cultural bias in an LLM, then, is crucial to equitable model development. Most previous works focus on question-answering (QA) tasks~\\cite{palta-rudinger-2023-fork}. QA tasks focus on one correct answer given the cultural context, despite in many cases, there being a group of correct answers with shared characteristics for a given question. We proposed a task focusing on word groups, Word Grouping Game (WGG) that implicitly evaluates the model's cultural knowledge and norms. In WGG, LLMs are given a pool of words, where they must separate the words into groups of four words tied under a common topic. In order to perform well in the game, the model also needs to perform culture-related reasoning. We evaluated the game with two cultures, Latinx/Hispanic and Chinese, in both the native language and an English translation for comparison. Through experimentation, we find biases towards Chinese culture-based groupings, as well as disparities in performance between open- and closed-source models based on the language used for a given game",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZGsGFmUrm6": {
    "title": "ALiiCE: Evaluating Positional Fine-grained Citation Generation",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) can enhance the credibility and verifiability by generating text with citations. However, existing tasks and evaluation methods are predominantly limited to sentence-level statement, neglecting the significance of positional fine-grained citations that can appear anywhere within sentences. To facilitate further exploration of the fine-grained citation generation, we propose ALiiCE, the first automatic evaluation framework for this task. Our framework first parses the sentence claim into atomic claims via dependency analysis and then calculates citation quality at the atomic claim level. ALiiCE introduces three novel metrics for positional fined-grained citation quality assessment, including positional fine-grained citation recall and precision, and coefficient of variation of citation positions. We evaluate the positional fine-grained citation generation performance of several LLMs on two long-form QA datasets. Our experiments and analyses demonstrate the effectiveness and reasonableness of ALiiCE. The results also indicate that existing LLMs still struggle to provide positional fine-grained citations",
    "checked": true,
    "id": "6fb7545d3b29248c52f58d8ea623dd0497040b35",
    "semantic_title": "aliice: evaluating positional fine-grained citation generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=xXN16BtjnL": {
    "title": "Exploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation",
    "volume": "review",
    "abstract": "Multilingual neural machine translation models support fine-tuning hundreds of languages simultaneously. However, fine-tuning on full parameters solely is inefficient potentially leading to negative interactions among languages. In this work, we demonstrate that the fine-tuning for a language occurs in its intrinsic language-specific subspace with a tiny fraction of entire parameters. Thus, we propose language-specific LoRA to isolate intrinsic language-specific subspaces. Furthermore, we propose architecture learning techniques and introduce a gradual pruning schedule during fine-tuning to exhaustively explore the optimal setting and the minimal intrinsic subspaces for each language, resulting in a lightweight yet effective fine-tuning procedure. The experimental results on a 12-language subset and a 30-language subset of FLORES-101 show that our methods not only outperform full-parameter fine-tuning up to 2.25 spBLEU scores but also reduce trainable parameters to $0.4\\%$ for high and medium-resource languages and $1.6\\%$ for low-resource ones",
    "checked": false,
    "id": "921ff7e87c5a599459d53d3172a5e6ebd12a8348",
    "semantic_title": "exploring multilingual pretrained machine translation models for interactive translation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=BLe8rV3VBH": {
    "title": "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model",
    "volume": "review",
    "abstract": "Private data, being larger and quality-higher than public data, can greatly improve large language models (LLM). However, due to privacy concerns, this data is often dispersed in multiple silos, making its secure utilization for LLM training a challenge. Federated learning (FL) is an ideal solution for training models with distributed private data, but traditional frameworks like FedAvg are unsuitable for LLM due to their high computational demands on clients. An alternative, split learning, offloads most training parameters to the server while training embedding and output layers locally, making it more suitable for LLM. Nonetheless, it faces significant challenges in security and efficiency. Firstly, the gradients of embeddings are prone to attacks, leading to potential reverse engineering of private data. Furthermore, the server's limitation of handling only one client's training request at a time hinders parallel training, severely impacting training efficiency. In this paper, we propose a Federated Learning framework for LLM, named FL-GLM, which prevents data leakage caused by both server-side and peer-client attacks while improving training efficiency. Specifically, we first place the input block and output block on local client to prevent embedding gradient attacks from server. Secondly, we employ key-encryption during client-server communication to prevent reverse engineering attacks from peer-clients. Lastly, we employ optimization methods like client-batching or server-hierarchical, adopting different acceleration methods based on the actual computational capabilities of the server. Experimental results on NLU and generation tasks demonstrate that FL-GLM achieves comparable metrics to centralized chatGLM model, validating the effectiveness of our federated learning framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5tzlr1IWRN": {
    "title": "Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages",
    "volume": "review",
    "abstract": "Neural Machine Translation (NMT) models are typically trained on datasets with limited exposure to Scientific, Technical and Educational domains. Translation models thus, in general, struggle with tasks that involve scientific understanding or technical jargon. Their performance is found to be even worse for low-resource Indian languages. Finding a translation dataset that tends to these domains in particular, poses a difficult challenge. In this paper, we address this by creating a multilingual parallel corpus containing more than 2.8 million rows of English-to-Indic and Indic-to-Indic high-quality translation pairs across 8 Indian languages. We achieve this by bitext mining human-translated transcriptions of NPTEL video lectures. We also finetune and evaluate NMT models using this corpus and surpass all other publicly available models at in-domain tasks. We also manage to improve the baseline by over 2 BLEU for these Indian languages on average, thus demonstrating the potential for generalizing to out-of-domain translation tasks as well. We are pleased to release the corresponding models and dataset, accessible via this link: https://huggingface.co/anon-auth",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PlldFUpUFD": {
    "title": "Quantum-Inspired Sentence Representation: Rethinking Word-Based Density Matrices",
    "volume": "review",
    "abstract": "This paper proposes a novel approach to enhance traditional quantum-inspired models. We introduce a Quantum-Inspired Sentence Representation model (QISR), which transforms word density matrices into representations of entire sentences, improving computational resource efficiency. Compared with traditional quantum-inspired models, the QISR method works at the density matrix layer and has better effects on the overall model as the embedding dimension increases. Even the QPDN model with a word embedding of 768 dimensions only requires 1736MB. This optimization has potential benefits for the overall model architecture, particularly when dealing with large word embedding dimensions. Furthermore, this approach reduces computing resource consumption while maintaining high computational accuracy, highlighting its potential benefits in processing complex language tasks. This research provides a novel approach to sentence representation in quantum-inspired language models and highlights the potential value of improved computational methods in a quantum-inspired context. Our research results are expected to provide modeling support and practical application guidance for future text processing endeavors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BAxkfTaM7D": {
    "title": "SecCoder: Towards Generalizable and Robust Secure Code Generation",
    "volume": "review",
    "abstract": "After large models (LMs) have gained widespread acceptance in code-related tasks, their superior generative capacity has greatly promoted the application of the code LM. Nevertheless, the security of the generated code has raised attention to its potential damage. Existing secure code generation methods have limited generalizability to unseen test cases and poor robustness against the attacked model, leading to safety failures in code generation. In this paper, we propose a generalizable and robust secure code generation method SecCoder by using in-context learning (ICL) and the safe demonstration. The dense retriever is also used to select the most helpful demonstration to maximize the improvement of the generated code's security. Experimental results show the superior generalizability of the proposed model SecCoder compared to the current secure code generation method, achieving a significant security improvement of an average of 7.20% on unseen test cases. The results also show the better robustness of SecCoder compared to the current attacked code LM, achieving a significant security improvement of an average of 7.74%. Our analysis indicates that SecCoder enhances the security of LMs in generating code, and it is more generalizable and robust",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B4VnDh8nuj": {
    "title": "A framework for annotating and modelling intentions behind metaphor use",
    "volume": "review",
    "abstract": "Metaphors are part of everyday language and shape the way in which we conceptualize various domains. Moreover, they play a multifaceted role in communication, making their understanding and generation a challenging task for language models (LMs). While there has been extensive work in the literature linking metaphor to the achievement of individual intentions, no comprehensive taxonomy, suitable for Natural Language Processing (NLP) applications, is available to present day. In this paper, we propose a novel taxonomy of intentions commonly attributed to metaphor, which comprises 9 categories. We also release the first dataset annotated for intentions behind metaphor use. Finally, we use this dataset to test the capability of large language models (LLMs) in inferring the intentions behind metaphor use, in zero- and in-context few-shot settings. Our experiments show that this is still a challenge for LLMs",
    "checked": true,
    "id": "87ef498c52bf763630d240e37788470d5cfa329a",
    "semantic_title": "a framework for annotating and modelling intentions behind metaphor use",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Mm2be80em": {
    "title": "Handling Dialog Dependencies to Reformulate Requests in Human-Agent Interaction",
    "volume": "review",
    "abstract": "Given the continual emergence of digital agents that employ tools and engines to satisfy multiple-nature user requests, there arises a critical need for efficiently orchestrating dialog in human-agent interactions. A fundamental function of this orchestration is to recognize user intent and send the appropriate request to the right engine/tool. However, given a dialog is conducted, information about the request might span through the whole conversation. In this work, we investigate the ability of large language models to recognize the user request in multi-turn human-agent interactions, considering dependencies in dialog and also reformulate it as a stand-alone sentence to be used for intent recognition and activation of tools, and engines without memory cells. To evaluate models as orchestrators, a demonstration dataset consisting of 42 dialogs, between an agent specialized in satellite data archives and a user, is developed and made publicly available. Thirteen models have been tested and five of them give outputs that comply with reference requests, with Gemini Pro 1.5 coming first",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uckixscoGy": {
    "title": "PsyGUARD: An Automated System for Suicide Detection and Risk Assessment in Psychological Counseling",
    "volume": "review",
    "abstract": "As awareness of mental health issues grows, online counseling support services are becoming increasingly prevalent worldwide. Detecting whether users express suicidal ideation in text-based counseling services is crucial for identifying and prioritizing at-risk individuals. However, the lack of domain-specific systems to facilitate fine-grained suicide detection and corresponding risk assessment in online counseling poses a significant challenge for automated crisis intervention aimed at suicide prevention. In this paper, we propose PsyGUARD, an automated system for detecting suicide ideation and assessing risk in psychological counseling. To achieve this, we first develop a detailed taxonomy for detecting suicide ideation based on foundational theories. We then curate a large-scale, high-quality dataset for suicide detection, called PsySUICIDE. To evaluate the capabilities of automated systems in fine-grained suicide detection, we establish a range of baselines. Subsequently, to assist automated services in providing safe, helpful, and tailored responses for further assessment, we propose building a suite of risk assessment frameworks. Our study provides an insightful analysis of the effectiveness of automated risk assessment systems based on fine-grained suicide detection and highlights their potential to improve mental health services on online counseling platforms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B4lN0hYgXO": {
    "title": "What Kinds of Tokens Benefit from Distant Text? An Analysis on Long Context Language Modeling",
    "volume": "review",
    "abstract": "As the context length that large language models can handle continues to increase, these models demonstrate an enhanced ability to utilize distant information for tasks such as language modeling. This capability contrasts with human reading and writing habits, where it is uncommon to remember and use particularly distant information, except in cases of foreshadowing. In this paper, we aim to explore which kinds of words benefit more from long contexts in language models. By analyzing the changes in token probabilities with increasing context length, we find that content words (e.g., nouns, adjectives) and the initial tokens of words benefit the most. Frequent patterns in the context (N-grams) also significantly impact predictions. Additionally, the model's prior knowledge plays a crucial role in influencing predictions, especially for rare tokens. We also observe that language models become more confident with longer contexts, resulting in sharper probability distributions. This overconfidence may contribute to the increasing probabilities of tokens with distant contextual information. We hope that our analysis will help the community better understand long-text language modeling and contribute to the design of more reliable long-context models",
    "checked": true,
    "id": "74163e4315b3549a4f5d7622e103e1a1625d1cf5",
    "semantic_title": "what kinds of tokens benefit from distant text? an analysis on long context language modeling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PbRlwcqaFO": {
    "title": "Is Synthetic Data Sufficient for Extractive Spoken Question Answering?",
    "volume": "review",
    "abstract": "Spoken language understanding is essential for extracting meaning from spoken language, particularly in low- or zero-resource language settings relying on speech in the absence of text data. This work investigates the effectiveness of using synthetic speech data in Spoken Question Answering (SQA). By manipulating prosody in human-read test sets, as well as proposing a new SQA dataset for fine-tuning, we demonstrate that models trained solely on synthetic speech can utilise prosodic cues. Moreover, synthetic speech fine-tuned models outperform those fine-tuned on natural speech, even with the same or restricted lexical information. Our findings suggest that current text-to-speech systems can simulate sufficient prosody for SQA models, and that the contribution from natural prosody is limited within the current textless SQA framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kpxjgjJBM8": {
    "title": "Automated Concept Map Extraction from Text",
    "volume": "review",
    "abstract": "Concept maps are summaries of nodes and relations from text in a directed graph format that can foster students' learning and understanding. However, manually constructing them is a challenging task. Automatic concept map extraction methods have emerged, standardly with a pipeline approach consisting of methods to extract entities and their relations. Yet, existing methods face efficiency limitations: 1) they are not capable of dealing with big corpora, 2) they are not open-access architectures, 3) they rely on the existence of annotated datasets. To bridge these gaps, we introduce a novel, modularized and open-source methods for concept map extraction that addresses efficiency by using semantic and sub-symbolic techniques with a new preliminary summarisation component. Moreover, we compare the pipeline approaches with three end-to-end Large Language Models methods. The best models for our pipeline and our end-to-end baseline achieve state-of-the-art results on METEOR metrics, with F1 scores of $25.69$ and $28.5$ respectively and on ROUGE-2 recall, with scores of $24.26$ and $24.3$. This contribution advances the task of automated concept map extraction, opening doors to wider applications supporting learning. The code is open-access and available",
    "checked": false,
    "id": "08b984e68d98a47b5d76fc1be484214541207393",
    "semantic_title": "sentic parser: a graph-based approach to concept extraction for sentiment analysis",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=ZZcdkHEKD2": {
    "title": "Adaptive Retrieval-Augmented Generation for Conversational Systems",
    "volume": "review",
    "abstract": "Despite the success of integrating large language models into the development of conversational systems, many studies have shown the effectiveness of retrieving and augmenting external knowledge for informative responses. Hence, many existing studies commonly assume the *always* need for Retrieval Augmented Generation (RAG) in a conversational system without explicit control. This raises a research question about such a necessity. In this study, we propose to investigate the need for each turn of system response to be augmented with external knowledge. In particular, by leveraging human judgements on the binary choice of adaptive augmentation, we develop *RAGate*, a gating model, which models conversation context and relevant inputs to predict if a conversational system requires RAG for improved responses. We conduct extensive experiments on devising and applying *RAGate* to conversational models and well-rounded analyses of different conversational scenarios. Our experimental results and analysis indicate the effective application of *RAGate* in RAG-based conversational systems in identifying system responses for appropriate RAG with high-quality responses in a high generation confidence. This study also identifies the correlation between the generation's confidence level and the relevance of the augmented knowledge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wLo8Hnyfvs": {
    "title": "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective",
    "volume": "review",
    "abstract": "Activation Editing, which involves directly editting the internal representations of large language models (LLMs) to alter their behavior and achieve desired properties, has emerged as a promising area of research. Existing works primarily treat LLMs' activations as points in space and modify them by adding steering vectors. We show that doing so would break the magnitude consistency of the activation vectors in LLMs. To overcome this shortcoming, we propose a novel editing method that views activations in terms of their directions and magnitudes. Our method, which we name \\emph{Householder Pseudo-Rotation} (HPR), mimics the rotation transformation, thus preserving activation norm and resulting in an improved performance on various safety benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DuEesONIfZ": {
    "title": "DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators",
    "volume": "review",
    "abstract": "Generally, the ${\\it decoder\\-only}$ large language models (LLMs) are adapted to context-aware neural machine translation (NMT) in a concatenating way, where LLMs take the concatenation of the source sentence (i.e., intra-sentence context) and the inter-sentence context as the input, and then to generate the target tokens sequentially. This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts. In this paper, we propose an alternative adaptation approach, named ${\\bf D}$ecoding-${\\bf e}$nhanced ${\\bf M}$ulti-phase ${\\bf P}$rompt ${\\bf T}$uning (DeMPT), to make LLMs discriminately model and utilize the inter- and intra-sentence context and more effectively adapt LLMs to context-aware NMT. First, DeMPT divides the context-aware NMT process into three separate phases. During each phase, different continuous prompts are introduced to make LLMs discriminately model various information. Second, DeMPT employs a heuristic way to further discriminately enhance the utilization of the source-side inter- and intra-sentence information at the final decoding phase. Experiments show that our approach significantly outperforms the concatenation method, and further improves the performance of LLMs in discourse modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9oZB6Nqs2q": {
    "title": "SeeD: Accelerating Reasoning Tree Construction via Scheduled Speculative Decoding",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable emergent abilities across various tasks, yet fall short of complex reasoning and planning tasks. The tree-search-based reasoning methods address this by surpassing the capabilities of chain-of-thought prompting, encouraging exploration of intermediate steps. However, such methods introduce significant inference latency due to the systematic exploration and evaluation of multiple thought paths. This paper introduces SeeD, a novel and efficient inference framework to optimize runtime speed and GPU memory management concurrently. By employing a scheduled speculative execution, SeeD efficiently handles multiple iterations for the thought generation and the state evaluation, leveraging a rounds-scheduled strategy to manage draft model dispatching. Extensive experimental evaluations on three reasoning datasets demonstrate superior speedup performance of SeeD, providing a viable path for batched inference in training-free speculative decoding",
    "checked": true,
    "id": "540209de237977a2fb16d3a2c4021618e8ea8b47",
    "semantic_title": "seed: accelerating reasoning tree construction via scheduled speculative decoding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D8QOKkS43N": {
    "title": "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning",
    "volume": "review",
    "abstract": "We focus on Text-to-SQL semantic parsing from the perspective of Large Language Models. Motivated by challenges related to the size of commercial database schemata and the deployability of business intelligence solutions, we propose an approach that dynamically retrieves input database information and uses abstract syntax trees to select few-shot examples for in-context learning. Furthermore, we investigate the extent to which an in-parallel semantic parser can be leveraged for generating approximated versions of the expected SQL queries, to support our retrieval. We take this approach to the extreme--we adapt a model consisting of less than $500$M parameters, to act as an extremely efficient approximator, enhancing it with the ability to process schemata in a parallelised manner. We apply our approach to monolingual and cross-lingual benchmarks for semantic parsing, showing improvements over state-of-the-art baselines. Comprehensive experiments highlight the contribution of modules involved in this retrieval-augmented generation setting, revealing interesting directions for future work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JmNvS3Z4RF": {
    "title": "Balanced Watermark: A Simple High-Imperceptibility Watermark for Large Language Models",
    "volume": "review",
    "abstract": "In order to counteract the potential risks posed by increasingly intelligent Large Language Models (LLMs), several scholars attempt to apply watermarking to the detection of LLM-generated text. Watermark researchers typically focus on detectability, robustness and invisible, but they tend to overlook the imperceptibility, which is crucial for preventing the watermark from being cracked. Watermarks with low imperceptibility are easily stolen and analyzed by malicious users, who can then forge watermarked text. To fill this research gap, we design Balanced Watermark (BW) by balancing the watermark strength across the vocabulary, achieving a fit to a non-watermarked LLM distribution to enhance imperceptibility. To effectively evaluate the imperceptibility of watermarks, we design a metric to evaluate for the first time. Our experiments prove that BW effectively improves imperceptibility and maintains high performance of the watermark in other features",
    "checked": false,
    "id": "17507e77ed335e8bb4ea01deb274719d68d288bf",
    "semantic_title": "waterpool: a watermark mitigating trade-offs among imperceptibility, efficacy and robustness",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bWooXcoxCZ": {
    "title": "Zero-shot Commonsense Reasoning over Machine Imagination",
    "volume": "review",
    "abstract": "Recent approaches to zero-shot commonsense reasoning have enabled Pre-trained Language Models (PLMs) to learn a broad range of commonsense knowledge without being tailored to specific situations. However, they often suffer from human reporting bias inherent in textual commonsense knowledge, leading to discrepancies in understanding between PLMs and humans. In this work, we aim to bridge this gap by introducing an additional information channel to PLMs. We propose Imagine (Machine Imagination-based Reasoning), a novel zero-shot commonsense reasoning framework designed to complement textual inputs with visual signals derived from machine-generated images. To achieve this, we enhance PLMs with imagination capabilities by incorporating an image generator into the reasoning process. To guide PLMs in effectively leveraging machine imagination, we create a synthetic pre-training dataset that simulates visual question-answering. Our extensive experiments on diverse reasoning benchmarks and analysis show that Imagine outperforms existing methods by a large margin, highlighting the strength of machine imagination in mitigating reporting bias and enhancing generalization capabilities",
    "checked": false,
    "id": "40d5c1d887a43fb03d1995bd4d067ca5a9f5e687",
    "semantic_title": "perceptual-iq: visual commonsense reasoning about perceptual imagination",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UXeNNfKWGj": {
    "title": "LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning",
    "volume": "review",
    "abstract": "Large language models (LLMs) sometimes demonstrate poor performance on knowledge-intensive tasks, commonsense reasoning is one of them. Researchers typically address these issues by retrieving related knowledge from knowledge graphs or employing self-enhancement methods to elicit knowledge in LLMs. However, noisy knowledge and invalid reasoning issues hamper their ability to answer questions accurately. To this end, we propose a novel method named $\\textbf{L}$iciting, f$\\textbf{I}$ltering and i$\\textbf{N}$tegrating $\\textbf{K}$nowledge in large languag$\\textbf{E}$ mo$\\textbf{D}$el ($\\mathbb{LINKED}$). In it, we design a reward model to filter out the noisy knowledge and take the marginal consistent reasoning module to reduce invalid reasoning. With our comprehensive experiments on two complex commonsense reasoning benchmarks, our method outperforms SOTA baselines (up to $\\textbf{9.0}$% improvement of accuracy). Besides, to measure the positive and negative impact of the injected knowledge, we propose a new metric called effectiveness-preservation score for the knowledge enhancement works. Finally, through extensive experiments, we conduct an in-depth analysis and find many meaningful conclusions about LLMs in commonsense reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VANqfQbDZN": {
    "title": "A LLM-based Framework for Biomedical Terminology Normalization via Multi-Agent Collaboration",
    "volume": "review",
    "abstract": "Biomedical Terminology Normalization aims to identify the standard term in a specified termbase for non-standardized mentions from social media or clinical texts, employing the mainstream \"Recall and Re-rank\" framework. Instead of the traditional pretraining-finetuning paradigm, we would like to explore the possibility of accomplishing this task through a tuning-free paradigm using powerful Large Language Models (LLMs), hoping to address the costs of re-training due to discrepancies of both standard termbases and annotation protocols. Another major obstacle in this task is that both mentions and terms are short texts. Short texts contain an insufficient amount of information that can introduce ambiguity, especially in a biomedical context. Therefore, besides using the advanced embedding model, we implement a Retrieval-Augmented Generation (RAG) based knowledge enhancement module. This module introduces an LLM agent that expands the short texts into accurate, harmonized, and more informative descriptions using a search engine and a domain knowledge base. Furthermore, we present an innovative tuning-free biomedical terminology normalization agent collaboration framework. By leveraging the reasoning capabilities of LLM, our framework conducts more sophisticated ranking and re-ranking processes with the collaboration of different LLM agents. Experimental results across multiple datasets indicate that our approach exhibits competitive performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V2eacwEW7H": {
    "title": "EditTrans: Speedy Edit-based Detailed Transformation of Academic Documents into Markup",
    "volume": "review",
    "abstract": "Academic Documents stored in PDF format can be transformed into plain text structured markup languages to enhance accessibility. Markup languages allow for easier updates and customization, making academic content more adaptable and accessible to diverse usage, such as linguistic corpus compilation. Existing end-to-end decoder transformer models can transform screenshots of documents into markup language, their flexibility is superior to encoder transformers based on Document Layout Analysis. However, decoder transformers have more parameters and operate more slowly. Their token-by-token decoding from scratch wastes a lot of inference steps in generating dense text, which can be directly copied from PDF files. To solve this problem, we introduce EditTrans, whose features allow identifying a queue of to-be-edited text from a PDF before starting to generate markup language. EditTrans contains a lightweight classifier that is fine-tuned from a Document Layout Analysis model on 162,127 pages of documents from arXiv. In our evaluations, EditTrans reduced the number of generation steps by 42.9% compared to end-to-end decoder transformer models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O4rVJEcwi3": {
    "title": "MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT",
    "volume": "review",
    "abstract": "Bigger the better' has been the predominant trend in recent Large Language Models (LLMs) development. However, LLMs do not suit well for scenarios that require on-device processing, energy efficiency, low memory footprint, and response efficiency. These requisites are crucial for privacy, security, and sustainable deployment. This paper explores the 'less is more' paradigm by addressing the challenge of designing accurate yet efficient Small Language Models (SLMs) for resource constrained devices. Our primary contribution is the introduction of an accurate and fully transparent open-source 0.5 billion (0.5B) parameter SLM, named MobiLlama, catering to the specific needs of resource-constrained computing with an emphasis on enhanced performance with reduced resource demands. MobiLlama is a SLM design that initiates from a larger model and applies a careful parameter sharing scheme to reduce both the pre-training and the deployment cost. Our work strives to not only bridge the gap in open-source SLMs but also ensures full transparency, where complete training data pipeline, training code, model weights, and over 300 checkpoints along with evaluation codes will be publicly released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ykwLBnTxGO": {
    "title": "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model",
    "volume": "review",
    "abstract": "Although most current large multimodal models (LMMs) can already understand photos of natural scenes and portraits, their understanding of abstract images, e.g., charts, maps, or layouts, and visual reasoning capabilities remains quite rudimentary. They often struggle with simple daily tasks, such as reading time from a clock, understanding a flowchart, or planning a route using a road map. In light of this, we design a multi-modal self-instruct, utilizing large language models and their code capabilities to synthesize massive abstract images and visual reasoning instructions across daily scenarios. Our strategy effortlessly creates a multimodal benchmark with 11,193 instructions for eight visual scenarios: charts, tables, simulated maps, dashboards, flowcharts, relation graphs, floor plans, and visual puzzles. \\textbf{This benchmark, constructed with simple lines and geometric elements, exposes the shortcomings of most advanced LMMs} like GPT-4V and Llava in abstract image understanding, spatial relations reasoning, and visual element induction. Besides, to verify the quality of our synthetic data, we fine-tune an LMM using 62,476 synthetic chart, table and road map instructions. The results demonstrate improved chart understanding and map navigation performance, and also demonstrate potential benefits for other visual reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oVly870Uac": {
    "title": "Explanations for explainability: towards an annotated corpus",
    "volume": "review",
    "abstract": "Providing good explanations plays a pivotal role in enhancing human understanding. First, we organize explanations into categories based on a framework inspired by scientific and philosophical discussions on the nature of explanations. We then focus on developing retrieval techniques for single-sentence explanations, aiming to lay the groundwork for creating an open-source corpus of scientific articles containing annotations of explanations. A user study was conducted to label 100 sentences according to our classification categories. This collection of annotated examples, balanced with topic-related non-explanatory sentences, was used to refine three large language models (LLMs) via the Cohere API, enabling them to perform (a) semantic search, (b) binary classification and (c) single-label classification. Models (b) and (c) presented results superior to base Llama 3 8B and on par with GPT-4, with model (b) showing balanced results and outperforming GPT-4 by 12\\% accuracy",
    "checked": false,
    "id": "5aaa39b360c39e8a43957e688d7adf549b1e95e5",
    "semantic_title": "towards interpretable mental health analysis with chatgpt",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=O0p5wleHcd": {
    "title": "Random Label Forests: An Ensemble Method with Label Subsampling For Extreme Multi-Label Problems",
    "volume": "review",
    "abstract": "Text classification is one of the essential topics in natural language process fields, and the prediction in text classification can usually be multiple labels. Thus, a text classification problem can be a particular problem in machine learning: a multi-label classification problem. Recently, the number of labels has become larger and larger, especially in the applications of e-commerce, so handling text-related e-commerce problems further requires more and more memory space in many existing multi-label learning methods. Hence, utilizing distributed system to share those large memory requirement is a reasonable solution. We propose ``random label forests'', a distributed ensemble method with label subsampling, for handling extremely large-scale labels by label subsampling and parallel computing. Random label forests can reduce the memory usage per computer while keeping competitive performances over six real-world data sets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ui4IdCvESt": {
    "title": "In-Context Former: Lightning-fast Compressing Context for Large Language Model",
    "volume": "review",
    "abstract": "With the rising popularity of Transformer-based large language models (LLMs), reducing their high inference costs has become a significant research focus. One effective approach to mitigate these costs is compressing the long input contexts. Existing methods typically leverage the self-attention mechanism of the large model itself for context compression. While these methods have achieved notable results, the compression process still entails quadratic complexity. To overcome this limitation, we propose the In-Context Former (IC-Former). This method does not rely on the target large model but instead utilizes cross-attention mechanisms to extract and condense information from the contextual embeddings. The computational overhead of our method grows linearly with the compression range. Experimental results indicate that our method requires only 1/32 of the floating-point operations of the baseline during compression and improves processing speed by 68 to 112 times while achieving 90\\% of the baseline performance on evaluation metrics. Additionally, IC-Former demonstrates strong regularity in its interactions with the context, enhancing its interpretability. Overall, IC-Former significantly reduces compression costs, making real-time compression scenarios feasible",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IASLoR2XfZ": {
    "title": "MQM-Chat: Multidimensional Quality Metrics for Chat Translation",
    "volume": "review",
    "abstract": "The complexities of chats pose significant challenges for machine translation models. Recognizing the need for a precise evaluation metric to address the issues of chat translation, this study introduces Multidimensional Quality Metrics for Chat Translation (MQM-Chat). Through the experiments of five models using MQM-Chat, we observed that all models generated certain fundamental errors, while each of them has different shortcomings, such as omission, overly correcting ambiguous source content, and buzzword issues, resulting in the loss of stylized information. Our findings underscore the effectiveness of MQM-Chat in evaluating chat translation, emphasizing the importance of stylized content and dialogue consistency for future studies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8D1e3k2UDe": {
    "title": "Rational Sensibility: LLM Enhanced Empathetic Response Generation Guided by Self-presentation Theory",
    "volume": "review",
    "abstract": "The development of Large Language Models (LLMs) provides human-centered Artificial General Intelligence (AGI) with a glimmer of hope. Empathy serves as a key emotional attribute of humanity, playing an irreplaceable role in human-centered AGI. Despite numerous researches aim to improve the cognitive empathy of models by incorporating external knowledge, there has been limited attention on the sensibility and rationality of the conversation itself, which are vital components of the empathy. However, the rationality information within the conversation is restricted, and previous methods of extending knowledge are subject to semantic conflict and single-role view. In this paper, we design an innovative encoder module inspired by self-presentation theory in sociology, which specifically processes sensibility and rationality sentences in dialogues. And we employ a LLM as a rational brain to decipher profound logical information preserved within the conversation, which assists our model in assessing the balance between sensibility and rationality to produce high-quality empathetic response. Experimental results demonstrate that our model outperforms other methods in both automatic and human evaluations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NdWzQIeqe8": {
    "title": "Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias",
    "volume": "review",
    "abstract": "The common toxicity and societal bias in contents generated by large language models (LLMs) necessitate strategies to reduce harm. Present solutions often demand white-box access to the model or substantial training, which is impractical for cutting-edge commercial LLMs. Moreover, prevailing prompting methods depend on external tool feedback and fail to simultaneously lessen toxicity and bias. Motivated by social psychology principles, we propose a novel strategy named perspective-taking prompting (PeT) that inspires LLMs to integrate diverse human perspectives and self-regulate their responses. This self-correction mechanism can significantly diminish toxicity (up to 89%) and bias (up to 73%) in LLMs' responses. Rigorous evaluations and ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and three open-source LLMs, revealing PeT's superiority in producing less harmful responses, outperforming five strong baselines",
    "checked": true,
    "id": "ded7778be3d2ac18765168e04d03a22dd2aed908",
    "semantic_title": "walking in others' shoes: how perspective-taking guides large language models in reducing toxicity and bias",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CG90d084Ns": {
    "title": "ProcessChat: A Business Process Grounded Dialogue Dataset",
    "volume": "review",
    "abstract": "Business processes are designed to streamline and optimize work within an organization and are often defined and documented by domain experts or process analysts using formal specifications. However, these specifications may be complex for the users executing the tasks of the process. For example, a recruitment process designed by a domain expert is used by many actors in the organization, who may not be skilled in understanding the formal notations that specify the process. With recent advancements in large language models, there has been increasing interest in enabling users to ask questions in natural language and receive relevant responses that are specific to the user's context. We propose a dataset grounded in domain-specific process knowledge, which it is supposed to follow during the conversation. The dataset consists of 316 dialogs grounded on 73 different process model specifications. We also present a baseline model, which is trained on the proposed dataset. Our experiments find that the model can do zero-shot transfer to unseen processes, and sets a strong baseline for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EntRkZ6vaP": {
    "title": "SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis",
    "volume": "review",
    "abstract": "Recent breakthroughs in Large Language Models (LLMs) have revolutionized scientific literature analysis. However, existing benchmarks fail to adequately evaluate the proficiency of LLMs in this domain, particularly in scenarios requiring higher-level abilities beyond mere memorization and the handling of multimodal data. In response to this gap, we introduce SciAssess, a benchmark specifically designed for the comprehensive evaluation of LLMs in scientific literature analysis. It aims to thoroughly assess the efficacy of LLMs by evaluating their capabilities in Memorization (L1), Comprehension (L2), and Analysis \\& Reasoning (L3). It encompasses a variety of tasks drawn from diverse scientific fields, including fundamental science, alloy materials, biomedicine, drug discovery, and organic materials. To ensure the reliability of SciAssess, rigorous quality control measures have been implemented, ensuring accuracy, anonymization, and compliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting their strengths and areas for improvement. We hope this evaluation supports the ongoing development of LLM applications in scientific literature analysis. SciAssess and its resources are available at \\url{https://anonymous.4open.science/r/SciAssess-2D14}",
    "checked": true,
    "id": "95d19f8ede34cd712a09ae3b86bed2b338bd9a48",
    "semantic_title": "sciassess: benchmarking llm proficiency in scientific literature analysis",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=6v0aNxTXuG": {
    "title": "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization",
    "volume": "review",
    "abstract": "Large vision--language models (LVLMs) suffer from hallucination, resulting in misalignment between the output textual response and the input visual content. Recent research indicates that the over-reliance on the Large Language Model (LLM) backbone, as one cause of the LVLM hallucination, inherently introduces bias from language priors, leading to insufficient context attention to the visual inputs. We tackle this issue of hallucination by mitigating such over-reliance through preference learning. We propose Vision-guided Direct Preference Optimization (V-DPO) to enhance visual context learning at training time. To interpret the effectiveness and generalizability of V-DPO on different types of training data, we construct a synthetic dataset containing both response- and image-contrast preference pairs, compared against existing human-annotated hallucination samples. Our approach achieves significant improvements compared with baseline methods across various hallucination benchmarks. Our analysis indicates that V-DPO excels in learning from image-contrast preference data, demonstrating its superior ability to elicit and understand nuances of visual context",
    "checked": false,
    "id": "27d55a944b5c02b8c10eb250773d8eb082e06476",
    "semantic_title": "detecting and mitigating hallucination in large vision language models via fine-grained ai feedback",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=sZ2Q4FkVDq": {
    "title": "Detecting Machine-Generated Long-Form Content with Latent-Space Variables",
    "volume": "review",
    "abstract": "The increasing capability of large language models (LLMs) to generate fluent long-form texts is presenting new challenges in distinguishing these outputs from those of humans. Existing zero-shot detectors that primarily focus on token-level distributions are vulnerable to real-world domain shift including different decoding strategies, variations in prompts, and attacks. We propose a more robust method that incorporates abstract elements---such as topic or event transitions---as key deciding factors, by training a latent-space model on sequences of events or topics derived from human-written texts. On three different domains, machine generations which are originally inseparable from humans' on the token level can be better distinguished with our latent-space model, leading to a 31% improvement over strong baselines such as DetectGPT. Our analysis further reveals that unlike humans, modern LLMs such as GPT-4 selecting event triggers and transitions differently, and inherent disparity regardless of the generation configurations adopted in real-time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tUv6h1FOv5": {
    "title": "NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting by Learning from Human",
    "volume": "review",
    "abstract": "Increasing concerns about privacy leakage issues in academia and industry arise when employing NLP models from third-party providers to process sensitive texts. To protect privacy before sending sensitive data to those models, we suggest sanitizing sensitive text using two common strategies used by humans: i) deleting sensitive expressions, and ii) obscuring sensitive details by abstracting them. To explore the issues and develop a tool for text rewriting, we curate the first corpus, coined \\ourBenchmark, through both crowdsourcing and the use of large language models (LLMs). Compared to the prior works based on differential privacy, which lead to a sharp drop in information utility and unnatural texts, the human-inspired approaches result in more natural rewrites and offer an improved balance between privacy protection and data utility, as demonstrated by our extensive experiments. Our dataset is available at https://anonymous.4open.science/r/NAP-2-benchmark-for-privacy-aware-rewriting-59F4/",
    "checked": true,
    "id": "336974b4e41a275016999b952dd955224dee2004",
    "semantic_title": "nap^2: a benchmark for naturalness and privacy-preserving text rewriting by learning from human",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=whRJT6j4EM": {
    "title": "Large Language Models are Students at Various Levels: Zero-shot Question Difficulty Estimation",
    "volume": "review",
    "abstract": "Recent advancements in educational platforms have emphasized the importance of personalized education. Accurately estimating question difficulty based on the group level of a student is essential for personalized question recommendations. Several studies have focused on predicting question difficulty using student question-solving records or textual information about the questions. However, these approaches require a large amount of student question-solving records and fail to account for the subjective difficulties perceived by different student groups. To address these limitations, we propose the LLaSA framework that utilizes large language models to represent students at various levels. LLaSA estimates question difficulty using student abilities derived from their question-solving records. Furthermore, the zero-shot LLaSA can estimate question difficulty without any student question-solving records. In evaluations on the DBE-KT22 and ASSISTMents 2005â€“2006 benchmarks, the zero-shot LLaSA demonstrated a performance comparable to those of strong baseline models even without any training. When evaluated using the classification method, LLaSA outperformed the baseline models, achieving state-of-the-art performance. In addition, the zero-shot LLaSA achieved a high correlation compared with the question difficulty derived from the question-solving records of students, suggesting the potential of LLaSA for real world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TXIW34StZn": {
    "title": "Text Clustering as Classification with LLMs",
    "volume": "review",
    "abstract": "Text clustering remains valuable in real-world applications where manual labeling is cost-prohibitive. It facilitates efficient organization and analysis of information by grouping similar texts based on their representations. However, implementing this approach necessitates fine-tuned embedders for downstream data and sophisticated similarity metrics. To address this issue, this study presents a novel framework for text clustering that effectively leverages the in-context learning capacity of Large Language Models (LLMs). Instead of fine-tuning embedders, we propose to transform the text clustering into a classification task via LLM. First, we prompt LLM to generate potential labels for a given dataset. Second, after integrating similar labels generated by the LLM, we prompt the LLM to assign the most appropriate label to each sample in the dataset. Our framework has been experimentally proven to achieve comparable or superior performance to state-of-the-art clustering methods that employ embeddings, without requiring complex fine-tuning or clustering algorithms. We make our code available to the public for utilization https://anonymous.4open.science/r/Text-Clustering-via-LLM-E500",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b9P2cei6ZP": {
    "title": "Sequential API Function Calling Using GraphQL Schema",
    "volume": "review",
    "abstract": "Function calling using Large Language Models (LLMs) is an active research area that aims to empower LLMs with the ability to execute APIs to perform real-world tasks. However, sequential function calling using LLMs with interdependence between functions is still under-explored. To this end, we introduce GraphQLRestBench, a dataset consisting of natural language utterances paired with function call sequences representing real-world REST API calls with variable mapping between functions. In order to represent the response structure of the functions in the LLM prompt, we use the GraphQL schema of the REST APIs. We also introduce a custom evaluation framework for our dataset consisting of four specially designed metrics. We evaluate three open-source code LLMs on our dataset using few-shot Chain-of-Thought and ReAct prompting to establish a reasonable baseline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3jmColxURv": {
    "title": "Grammatical Error Correction for Low-Resource Languages: The Case of Zarma",
    "volume": "review",
    "abstract": "Zarma is a Nilo-Saharan language spoken predominantly in West Africa. The limited availability of annotated data and the need for standardized orthography make grammatical error correction (GEC) particularly challenging for Zarma. This study presents a comparative analysis of GEC methods for Zarma, exploring classical GEC approaches such as rule-based methods, machine translation (MT) models, and state-of-the-art large language models (LLMs). Through rigorous evaluations, we compare the strengths and limitations of each method, assessing their effectiveness in identifying and correcting errors in Zarma texts. Our findings highlight the promising potential of both LLMs and MT models to significantly enhance GEC capabilities for low-resource languages, paving the way for developing more inclusive and robust NLP tools for African languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g328uOSM2B": {
    "title": "Revisiting Supervised Contrastive Learning for Microblog Classification",
    "volume": "review",
    "abstract": "Microblog content (e.g., Tweets) is noisy due to its informal use of language and its lack of contextual information within each post. To tackle these challenges, state-of-the-art microblog classification models rely on pre-training language models (LMs). However, pre-training dedicated LMs is resource-intensive and not suitable for small labs. Supervised contrastive learning (SCL) has shown its effectiveness with small, available resources. In this work, we examine the effectiveness of fine-tuning transformer-based language models, regularized with a SCL loss for English microblog classification. Despite its simplicity, the evaluation on two English microblog classification benchmarks (TweetEval and Tweet Topic Classification) shows an improvement over baseline models. The result shows that, across all subtasks, our proposed method has a performance gain of up to 11.9 percentage points. All our models are open source",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7nIUWNewmT": {
    "title": "Event-level Knowledge Editing",
    "volume": "review",
    "abstract": "Knowledge editing aims at updating knowledge of large language models (LLMs) to prevent them from becoming outdated. Existing work edits LLMs at the level of factual knowledge triplets. However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets. In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into LLMs and improves over conventional triplet-level editing on (1) Efficiency. A single event edit leads to updates in multiple entailed knowledge triplets. (2) Completeness. Beyond updating factual knowledge, event-level editing also requires considering the event influences and updating LLMs' knowledge about future trends. We construct a high-quality event-level editing benchmark ELKEN, consisting of $1,515$ event edits, $6,449$ questions about factual knowledge, and $10,150$ questions about future tendencies. We systematically evaluate the performance of various knowledge editing methods and LLMs on this benchmark. We find that ELKEN poses significant challenges to existing knowledge editing approaches. Our codes and dataset will be publicly released to facilitate further research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=npcw2oz2yG": {
    "title": "Unveiling the Invisible: Captioning Videos with Metaphors",
    "volume": "review",
    "abstract": "Metaphors are a common communication tool used in our day-to-day life. The detection and generation of metaphors in textual form have been studied extensively but metaphors in other forms have been under-explored. Recent studies have shown that Vision-Language (VL) models cannot understand visual metaphors in memes and adverts. As of now, no probing studies have been done that involve complex language phenomena like metaphors with videos. Hence, we introduce a new VL task of describing the metaphors present in the videos in our work. To facilitate this novel task, we construct and release a manually created dataset with 705 videos and 2115 human-written captions, along with a new metric called Average Concept Distance (ACD), to automatically evaluate the creativity of the metaphors generated. We also propose a novel low-resource video metaphor captioning system: GIT-LLaVA, which obtains comparable performance to SoTA video language models on the proposed task. We perform a comprehensive analysis of existing video language models on this task and publish our dataset, models, and benchmark results to enable further research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sWBUs6gFXR": {
    "title": "Enhancing Cross-Prompt Transferability in Vision-Language Models through Contextual Injection of Target Tokens",
    "volume": "review",
    "abstract": "Vision-language models (VLMs) seamlessly integrate visual and textual data to perform tasks such as image classification, caption generation, and visual question answering. However, adversarial images often struggle to deceive all prompts effectively in the context of cross-prompt migration attacks, as the probability distribution of the tokens in these images tends to favor the semantics of the original image rather than the target tokens. To address this challenge, we propose a Contextual-Injection Attack (CIA) that employs gradient-based perturbation to inject target tokens into both visual and textual contexts, thereby improving the probability distribution of the target tokens. By shifting the contextual semantics towards the target tokens instead of the original image semantics, CIA enhances the cross-prompt transferability of adversarial images. Extensive experiments on the BLIP2, InstructBLIP, and Llava models show that CIA outperforms existing methods in cross-prompt transferability, demonstrating its potential for more effective adversarial strategies in VLMs",
    "checked": true,
    "id": "fe3086119d3006fc3ad0bb47eea14ab16a2a5d0f",
    "semantic_title": "enhancing cross-prompt transferability in vision-language models through contextual injection of target tokens",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=obhgH4gzuW": {
    "title": "A Survey on In-context Learning",
    "volume": "review",
    "abstract": "With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL",
    "checked": true,
    "id": "30c0cdc414f68211d5d0514df027cec22e005174",
    "semantic_title": "a survey on in-context learning",
    "citation_count": 203,
    "authors": []
  },
  "https://openreview.net/forum?id=MkabNxaq7R": {
    "title": "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models",
    "volume": "review",
    "abstract": "We introduce Mathador-LM, a new benchmark for evaluating the mathematical reasoning on large language models (LLMs), combining ruleset interpretation, planning, and problem-solving. This benchmark is inspired by the Mathador game, where the objective is to reach a target number using basic arithmetic operations on a given set of base numbers, following a simple set of rules. We show that, across leading LLMs, we obtain stable average performance while generating benchmark instances dynamically, following a target difficulty level. Thus, our benchmark alleviates concerns about test-set leakage into training data, an issue that often undermines popular benchmarks. Additionally, we conduct a comprehensive evaluation of both open and closed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that contemporary models struggle with Mathador-LM, scoring significantly lower than average 5th graders. This stands in stark contrast to their strong performance on popular mathematical reasoning benchmarks",
    "checked": true,
    "id": "69958f6cacf86537c8fb7e4efaa8fb2d8e519ce2",
    "semantic_title": "mathador-lm: a dynamic benchmark for mathematical reasoning on large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3xwJjQXnKN": {
    "title": "Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding",
    "volume": "review",
    "abstract": "The primary aim of Knowledge Graph Embeddings (KGE) is to learn low-dimensional representations of entities and relations for predicting missing facts. While rotation-based methods like RotatE and QuatE perform well in KGE, they face two challenges: limited model flexibility requiring proportional increases in relation size with entity dimension, and difficulties in generalizing the model for higher-dimensional rotations. To address these issues, we introduce OrthogonalE, a novel KGE model employing matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations. This approach not only enhances the generality and flexibility of KGE models but also captures several relation patterns that rotation-based methods can identify. Experimental results indicate that our new KGE model, OrthogonalE, offers generality and flexibility, captures several relation patterns, and significantly outperforms state-of-the-art KGE models while substantially reducing the number of relation parameters",
    "checked": true,
    "id": "d8b92da52780452591c81135a779dc5a336210e5",
    "semantic_title": "block-diagonal orthogonal relation and matrix entity for knowledge graph embedding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oFsyz1OdFW": {
    "title": "Do Large Language Models Exhibit Cognitive Dissonance? Studying the Difference Between Revealed Beliefs and Stated Answers",
    "volume": "review",
    "abstract": "Prompting and Multiple Choices Questions (MCQ) have become the preferred approach to assess the capabilities of Large Language Models (LLMs), due to their ease of manipulation and evaluation. Such experimental appraisals have pointed toward the LLMs' apparent ability to perform causal reasoning or to grasp uncertainty. In this paper, we investigate whether these abilities are measurable outside of tailored prompting and MCQ by reformulating these issues as direct text completion - the foundation of LLMs. To achieve this goal, we define scenarios with multiple possible outcomes and we compare the prediction made by the LLM through prompting (their Stated Answer) to the probability distributions they compute over these outcomes during next token prediction (their Revealed Belief). Our findings suggest that the Revealed Belief of LLMs significantly differs from their Stated Answer and hint at multiple biases and misrepresentations that their beliefs may yield in many scenarios and outcomes. As text completion is at the core of LLMs, these results suggest that common evaluation methods may only provide a partial picture and that more research is needed to assess the extent and nature of their capabilities",
    "checked": true,
    "id": "037914f671cd9e32ef5ec62b0377514f36f5714d",
    "semantic_title": "do large language models exhibit cognitive dissonance? studying the difference between revealed beliefs and stated answers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gSpxHMoJAA": {
    "title": "Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion",
    "volume": "review",
    "abstract": "Temporal Knowledge Graph Completion (TKGC) is a complex task involving the prediction of missing event links at future timestamps by leveraging established temporal structural knowledge. This paper aims to provide a comprehensive perspective on harnessing the advantages of Large Language Models (LLMs) for reasoning in temporal knowledge graphs, presenting an easily transferable pipeline. In terms of graph modality, we underscore the LLMs' prowess in discerning the structural information of pivotal nodes within the historical chain. As for the generation mode of the LLMs utilized for inference, we conduct an exhaustive exploration into the variances induced by a range of inherent factors in LLMs, with particular attention to the challenges in comprehending reverse logic. We adopt a parameter-efficient fine-tuning strategy to harmonize the LLMs with the task requirements, facilitating the learning of the key knowledge highlighted earlier. Comprehensive experiments are undertaken on several widely recognized datasets, revealing that our framework exceeds or parallels existing methods across numerous popular metrics. Additionally, we execute a substantial range of ablation experiments and draw comparisons with several advanced commercial LLMs, to investigate the crucial factors influencing LLMs' performance in structured temporal knowledge inference tasks",
    "checked": true,
    "id": "a15364e83f8986d8b8b1b6212596071e4f2d986d",
    "semantic_title": "chain of history: learning and forecasting with llms for temporal knowledge graph completion",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=PmI4x9roMz": {
    "title": "Collaborative Performance Prediction for Large Language Models",
    "volume": "review",
    "abstract": "Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works demonstrated intrinsic similarities within model families and utilized such similarities for performance prediction. However, they tend to overlook the similarities between model families and only consider design factors listed in the original scaling law. To overcome these limitations, we introduce a novel framework, Collaborative Performance Prediction (CPP), which significantly enhances prediction accuracy by leveraging the historical performance of various models on downstream tasks and other design factors for both model and task. We also collect a collaborative data sourced from online platforms containing both historical performance and additional design factors. With the support of the collaborative data, CPP not only surpasses traditional scaling laws in predicting the performance of scaled LLMs but also facilitates a detailed analysis of factor importance, an area previously overlooked",
    "checked": true,
    "id": "4c51430adfe11ff9f56399b710178d8b12411430",
    "semantic_title": "collaborative performance prediction for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r9BWCloj54": {
    "title": "EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models",
    "volume": "review",
    "abstract": "Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we propose an efficient fine-grained unlearning framework (EFUF), which performs gradient ascent utilizing three tailored losses to eliminate hallucinations without paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available",
    "checked": true,
    "id": "bf54792cf01761a2c51ac3410287797fff665cd4",
    "semantic_title": "efuf: efficient fine-grained unlearning framework for mitigating hallucinations in multimodal large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=vpyboJVuwN": {
    "title": "MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning",
    "volume": "review",
    "abstract": "Low-rank adaptation (LoRA) is a popular parameter-efficient fine-tuning (PEFT) method for large language models (LLMs). In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zhQFVs9Q6L": {
    "title": "When is the consistent prediction likely to be a correct prediction?",
    "volume": "review",
    "abstract": "Self-consistency (Wang et al., 2023) suggests that the most consistent answer obtained through large language models (LLMs) is more likely to be correct. In this paper, we challenge this argument and propose a nuanced correction. Our observations indicate that consistent answers derived through more computation i.e. longer reasoning texts, rather than simply the most consistent answer across all outputs, are more likely to be correct. This is predominantly because we demonstrate that LLMs can autonomously produce chain-of-thought (CoT) style reasoning with no custom prompts merely while generating longer responses, which lead to consistent predictions that are more accurate. In the zero-shot setting, by sampling Mixtral-8x7B model multiple times and considering longer responses, we achieve nearly 85% of its self-consistency performance obtained through zero-shot CoT prompting on the GSM8K and MultiArith datasets. Finally, we demonstrate that the probability of LLMs generating a longer response is quite low, highlighting the need for decoding strategies conditioned on output length",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lcqbEZkz26": {
    "title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands. Many sought to mitigate this by restricting the attention window within the pre-trained length. However, these methods introduce new issues such as ignoring the middle context and requiring additional training. To address these problems, we propose LongHeads, a training-free framework that enhances LLM's long context ability by unlocking multi-head attention's untapped potential. Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences, we allow each head to process in-distribution length by selecting and attending to important context chunks. To this end, we propose a chunk selection strategy that relies on the inherent correlation between the query and the key representations, efficiently distributing context chunks to different heads. In this way, each head ensures it can effectively process attended tokens within the trained length, while different heads in different layers can collectively process longer contexts. LongHeads works efficiently in linear time, and fits seamlessly with many LLMs that use relative positional encoding. LongHeads achieves 100\\% accuracy at the 128k length on passkey retrieval task, verifying LongHeads' efficacy in extending the usable context window for existing models",
    "checked": true,
    "id": "f6440a16ccc5c13d2a86af91b76e078685abfd16",
    "semantic_title": "longheads: multi-head attention is secretly a long context processor",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=GnMHN9N0GY": {
    "title": "PDF-to-Tree: Parsing PDF Text Blocks into a Tree",
    "volume": "review",
    "abstract": "In PDF documents, the reading order of text blocks is missing, which can hinder machine understanding of the document's content.Existing works try to extract one universal reading order for a PDF file.However, applications, like Retrieval Augmented Generation (RAG), require breaking long articles into sections and subsections for better indexing.For this reason, this paper introduces a new task and dataset, PDF-to-Tree, which organizes the text blocks of a PDF into a tree structure.Since a PDF may contain thousands of text blocks, far exceeding the number of words in a sentence, this paper proposes a transition-based parser that uses a greedy strategy to build the tree structure.Compared to parser for plain text, we also use multi-modal features to encode the parser state.Experiments show that our approach achieves an accuracy of 93.93%, surpassing the performance of baseline methods by an improvement of 6.72%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lSFEkeIrFd": {
    "title": "Beyond Instruction Following: Evaluating Rule Following of Large Language Models",
    "volume": "review",
    "abstract": "Although Large Language Models (LLMs) have demonstrated strong \\textbf{instruction-following} ability to be helpful, they are further supposed to be controlled and guided by $\\textbf{rules}$ in real-world scenarios to be safe, and accurate in responses. This demands the possession of $\\textbf{rule-following}$ capability of LLMs. However, few works have made a clear evaluation of the rule-following capability of LLMs. Previous studies that try to evaluate the rule-following capability of LLMs fail to distinguish the rule-following scenarios from the instruction-following scenarios. Therefore, this paper first makes a clarification of the concept of rule-following, and curates a comprehensive benchmark, $\\textbf{RuleBench}$, to evaluate a diversified range of rule-following abilities. Our experimental results on a variety of LLMs show that they are still limited in following rules. Our further analysis provides insights into the improvements for LLMs toward a better rule-following intelligent agent. The data and code can be found at: https://anonymous.4open.science/r/llm-rule-following-B3E3/",
    "checked": true,
    "id": "a093a1eb2b10b1e40240cea631a94c413d3b15e3",
    "semantic_title": "beyond instruction following: evaluating rule following of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zxvjkk0aAI": {
    "title": "Active Instruction Tuning for Large Language Models with Reference-Free Instruction Selection",
    "volume": "review",
    "abstract": "Recent works on efficient instruction tuning have shown that large language models (LLMs) can achieve comparable performance through the calibrated selection of a small subset of high-quality (instruction, response) pairs from labeled instruction pools. Despite reduced computational costs, these approaches often overlook the labor-intensive nature of instruction acquisition for labeling. We introduce a novel paradigm, Active Instruction Tuning with Reference-Free Instruction Selection, which supports instruction selection from both labeled and unlabeled instruction pools. Our experimental results demonstrate that this method not only achieves comparable or superior performance while reducing labeling costs but also matches the performance of prior studies in labeled instruction settings. Furthermore, we pioneer the investigation into the relationship between text evaluation correlated with human subjective evaluations and instruction tuning, confirming the effectiveness of ranking aggregation in enhancing the tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I98JkUN6es": {
    "title": "Do Pre-Trained Language Models Truly Focus on the Content They Are Expected to?",
    "volume": "review",
    "abstract": "Pre-trained language models (PLMs) have significantly revolutionized various natural language processing tasks, showcasing extraordinary capabilities in text comprehension and processing. Despite their widespread success, the elucidation of PLMs' interest towards the input texts remains unclear, i.e., which part of the inputs gains models' attention. Existing methods either rely on various stringent assumptions or ignore the intricate dependency relations inherent in natural language, causing inaccurate estimation results. In response to this limitation, this paper introduces a novel perturbation-based approach for estimating the PLMs' interest, comprising two crucial designs, i.e., the co-perturbation strategy and an adaptive optimization algorithm. Specifically, the strategy aims to inject noises across all input words, thereby confronting the inherent combinatorial explosion challenge. Furthermore, the proposed adaptive algorithm focuses on the estimation of interest degree for disentangling the output changes caused by the co-perturbation setting. Through extensive experimentation on various PLMs and datasets, we verify the effectiveness of the proposed approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sb4cHMf8OV": {
    "title": "Large Language Models Can Better Understand Knowledge Graphs Than We Thought",
    "volume": "review",
    "abstract": "As the parameter scale of large language models (LLMs) grows, jointly training knowledge graph (KG) embeddings with model parameters to enhance LLM capabilities becomes increasingly costly. Consequently, the community has shown interest in developing prompt strategies that effectively integrate KG information into LLMs. However, the format for incorporating KGs into LLMs lacks standardization; for instance, KGs can be transformed into linearized triples or natural language (NL) text. Current prompting methods often rely on a trial-and-error approach, leaving researchers with an incomplete understanding of which KG input format best facilitates LLM comprehension of KG content. To elucidate this, we design a series of experiments to explore LLMs' understanding of different KG input formats within the context of prompt engineering. Our analysis examines both literal and attention distribution levels. Through extensive experiments, we indicate a counter-intuitive phenomenon: when addressing fact-related questions, unordered linearized triples are more effective for LLMs' understanding of KGs compared to fluent NL text. Furthermore, noisy, incomplete, or marginally relevant subgraphs can still enhance LLM performance. Finally, different LLMs have distinct preferences for different formats of organizing unordered triples",
    "checked": true,
    "id": "5ef8b2b65f0f2b9b216c29164c8246e363c693ce",
    "semantic_title": "large language models can better understand knowledge graphs than we thought",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gEzRRrWDan": {
    "title": "Recurrent Alignment with Hard Attention for Hierarchical Text Rating",
    "volume": "review",
    "abstract": "While large language models (LLMs) excel at understanding and generating plain text, they are not tailored to handle hierarchical text structures or directly predict task-specific properties such as text rating. In fact, selectively and repeatedly grasping the hierarchical structure of large-scale text is pivotal for deciphering its essence. To this end, we propose a novel framework for hierarchical text rating utilizing LLMs, which incorporates Recurrent Alignment with Hard Attention (RAHA). Particularly, hard attention mechanism prompts a frozen LLM to selectively focus on pertinent leaf texts associated with the root text and generate symbolic representations of their relationships. Inspired by the gradual stabilization of the Markov Chain, recurrent alignment strategy involves feeding predicted ratings iteratively back into the prompts of another trainable LLM, aligning it to progressively approximate the desired target. Experimental results demonstrate that RAHA outperforms existing state-of-the-art methods on three hierarchical text rating datasets. Theoretical and empirical analysis confirms RAHA's ability to gradually converge towards the underlying target through multiple inferences. Additional experiments on plain text rating datasets verify the effectiveness of this Markov-like alignment. Our data and code can be available in https://anonymous.4open.science/r/RAHA/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oyp66J0uW7": {
    "title": "ESAN: An Efficient Semantic Attention Network for Remote Sensing Image Change Captioning",
    "volume": "review",
    "abstract": "With the continuous progress of remote sensing technology, an increasing number of remote sensing images containing rich geographical and environmental information is obtained. Unlike natural images, remote sensing images usually cover a large area and have complex spatial distribution, making it a challenge to accurately extract and describe changes from images. In order to effectively mine and utilize the rich semantic information contained in the image to guide the decoder to generate high-quality change descriptions, we propose an efficient semantic attention network (ESAN). Specifically, we first perform global efficient semantic representation (GESR) on the obtained remote sensing feature map to promote the understanding of complex scenes in remote sensing images. Then we further propose a cross-semantic feature enhancement module (CSFE) to effectively distinguish semantic changes from irrelevant changes. Finally, we input the obtained image features into the adaptive multi-layer Transformer decoder to guide the generation of change description. Extensive experiments on two representative remote sensing datasets, Dubai-CC and LEVIR-CC, demonstrate the superiority of the proposed model over many advanced technologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NzVVWsokkg": {
    "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
    "volume": "review",
    "abstract": "Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the \\textit{Uncertainty} about the question and the \\textit{Fidelity} to the answer generated by language models. Then, we propose a plug-and-play method, \\textit{UF Calibration}, to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \\textit{Truly Well-Calibrated Confidence} for large language models. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration",
    "checked": true,
    "id": "036e96ed196a7f4bb812380f3b76ac75d4a648e4",
    "semantic_title": "calibrating the confidence of large language models by eliciting fidelity",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=RAeYWAPpty": {
    "title": "Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling",
    "volume": "review",
    "abstract": "Ensembling multiple models has always been an effective approach to push the limits of existing performance and is widely used in classification tasks by simply averaging the classification probability vectors from multiple classifiers to achieve better accuracy. However, in the thriving open-source Large Language Model (LLM) community, ensembling methods are rare and typically limited to ensembling the full-text outputs of LLMs, such as selecting the best output using a ranker, which leads to underutilization of token-level probability information. In this paper, we treat the $\\textbf{G}$eneration of each token by LLMs $\\textbf{a}$s a $\\textbf{C}$lassification ($\\textbf{GaC}$) for ensembling. This approach fully exploits the probability information at each generation step and better prevents LLMs from producing early incorrect tokens that lead to snowballing errors. In experiments, we ensemble state-of-the-art LLMs on several benchmarks, including exams, mathematics and reasoning, and observe that our method breaks the existing community performance ceiling. Furthermore, we observed that most of the tokens in the answer are simple and do not affect the correctness of the final answer. Therefore, we also experimented with ensembling only key tokens, and the results showed better performance with lower latency across benchmarks",
    "checked": true,
    "id": "1d9c539eb54aee46fdb93a3f9798cb4cacd40bbe",
    "semantic_title": "breaking the ceiling of the llm community by treating token generation as a classification for ensembling",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=4FdgKIYnJk": {
    "title": "Source Code Data Augmentation for Deep Learning: A Survey",
    "volume": "review",
    "abstract": "The increasingly popular adoption of deep learning models in many critical source code tasks motivates the development of data augmentation (DA) techniques to enhance training data and improve various capabilities (e.g., robustness and generalizability) of these models. Although a series of DA methods have been proposed and tailored for source code models, there is a lack of comprehensive surveys and examinations to understand their effectiveness and implications. This paper fills this gap by conducting a comprehensive and integrative survey of data augmentation for source code, wherein we systematically compile and encapsulate existing literature to provide a comprehensive overview of the field. Complementing this, we present a continually updated GitHub repository that hosts a list of update-to-date papers on DA for source code modeling\\footnote{\\url{https://anonymous.4open.science/r/da4code}}",
    "checked": true,
    "id": "a59353904b1562b085fd989a1892ceb38ce6e52b",
    "semantic_title": "source code data augmentation for deep learning: a survey",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jaZXnb8yeF": {
    "title": "LLM4Decompile: Decompiling Binary Code with Large Language Models",
    "volume": "review",
    "abstract": "Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100%. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results",
    "checked": true,
    "id": "2589e7435b478b0389b78cafea75910c2e9460e0",
    "semantic_title": "llm4decompile: decompiling binary code with large language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=XG9Nrgf6Pf": {
    "title": "Improving Proactive Dialogue Strategy Planning with Interactive Environment and Goal-oriented Reward",
    "volume": "review",
    "abstract": "Proactive dialogue has become a crucial yet challenging aspect of human-computer interaction, applicable to various non-collaborative dialogue tasks such as negotiation, persuasion, and psychological counseling. However, current proactive dialogue systems are hindered by their simplistic single-turn interactions and lack of capability for multi-turn, long-term strategy planning, which obstructs effective goal completion. Additionally, corpus-based training procedures are inadequate for addressing low-resource environments and transferability requirements across different dialogue tasks. In this paper, we introduce a proactive dialogue strategy planning (ProDSP) method to overcome these challenges. By utilizing a small supervised fine-tuning language model, we enable the anticipation of future strategy sequences as simulation hints. This approach guides large language models (LLMs) in generating goal-oriented responses and facilitates training within an interactive environment using another LLM-based user simulator. To assess online user feedback during the training process, we employ a GPT-4-based user simulator to represent goal-oriented rewards through multi-faceted metrics. Extensive experiments demonstrate that our model surpasses competitive baselines in both strategy planning and dialogue generation for emotional support and negotiation tasks, offering a more adaptive and efficient approach to proactive dialogue strategy planning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EHlAE1bnxq": {
    "title": "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated impressive reasoning capabilities, particularly in textual mathematical problem-solving. However, existing open-source image instruction fine-tuning datasets, containing limited question-answer pairs per image, do not fully exploit visual information to enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs (MLLMs). To bridge this gap, we address the lack of high-quality, diverse multimodal mathematical datasets by collecting 40K high-quality images with question-answer pairs from 24 existing datasets and synthesizing 320K new pairs, creating the MathV360K dataset, which enhances both the breadth and depth of multimodal mathematical questions. We introduce Math-LLaVA, a LLaVA-1.5-based model fine-tuned with MathV360K. This novel approach significantly improves the multimodal mathematical reasoning capabilities of LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V on MathVista's minitest split. Furthermore, Math-LLaVA demonstrates enhanced generalizability, showing substantial improvements on the MMMU benchmark. Our research highlights the importance of dataset diversity and synthesis in advancing MLLMs' mathematical reasoning abilities",
    "checked": true,
    "id": "207f0c123e48b9e980f49e42cbc35711e14fea07",
    "semantic_title": "math-llava: bootstrapping mathematical reasoning for multimodal large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Hpk9n92O6I": {
    "title": "Hidden Vulnerabilities: The Knowledge Degradation in Fine-Tuned Large Language Models",
    "volume": "review",
    "abstract": "As real-world applications often require further fine-tuning for better downstream performance, we investigate the impact of such instruction fine-tuning on the general performance of large language models (LLMs). Using standard LLM benchmarks, we observe significant degradation for tasks requiring more complex and compositional skills, as represented by BBH benchmarks. On the other hand, model's general capability for Knowledge Retrieval, as indicated by MMLU scores across various domains seems to be relatively stable. Our finding sheds light on general degradation in model performance which is not confined to a specific domain but is more closely related to the type of capability involved where in this paper we benchmark two of them: Knowledge Retrieval and Knowledge Reasoning. Furthermore, we examine how fine-tuning training data impacts performance by comparing the effects of knowledge-compatible data training versus knowledge-conflict data training across different benchmark datasets",
    "checked": false,
    "id": "e2d282bba1500e002c7c14ab49bd3d2216950b46",
    "semantic_title": "chatbug: a common vulnerability of aligned llms induced by chat templates",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3AEEkGPiM1": {
    "title": "Leveraging Language Models for Summarizing Mental State Examinations: A Comprehensive Evaluation and Dataset Release",
    "volume": "review",
    "abstract": "Mental health disorders affect a significant portion of the global population, with diagnoses primarily conducted through Mental State Examinations (MSEs). MSEs serve as structured assessments to evaluate behavioral and cognitive functioning across various domains, aiding mental health professionals in diagnosis and treatment monitoring. However, in developing countries, access to mental health support is limited, leading to an overwhelming demand for mental health professionals. Resident doctors often conduct initial patient assessments and create summaries for senior doctors, but their availability is constrained, resulting in extended patient wait times. This study addresses the challenge of generating concise summaries from MSEs through the evaluation of various language models. Given the scarcity of relevant mental health conversation datasets, we developed a 12-item descriptive MSE questionnaire and collected responses from 405 participants, resulting in 9720 utterances covering diverse mental health aspects. Subsequently, we assessed the performance of five well-known pre-trained summarization models, both with and without fine-tuning, for summarizing MSEs. Our comprehensive evaluation, leveraging metrics such as ROUGE, SummaC, and human evaluation, demonstrates that language models can generate automated coherent MSE summaries for doctors. With this paper, we release our collected conversational dataset and trained models publicly for the mental health research community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VgRaJ78ytQ": {
    "title": "Difficulty-Based Training Strategy with MLLMs for Multimodal Sarcasm Explanation",
    "volume": "review",
    "abstract": "Multimodal Sarcasm Explanation (MuSE) is a new yet challenging task, which aims at generating natural language explanations for sarcasm in social media image-text pairs. MuSE can further enhance sarcasm understanding and has attracted increasing research interest. Previous works design cross-modal attention or multi-source semantic graphs and achieve promising performance. However, these works either ignore the semantic gap between visual features and textual decoder or introduce complex graph constructions, which limits their practical applicability and scalability for real-world scenarios. Furthermore, they treat each sample equally during training, overlooking the different effects of samples at different levels of difficulty. In this paper, we propose a novel MultiDimensional Sample Difficulty (MDSD) based training strategy with the Multimodal Large Language Models (MLLMs) for MuSE.Leveraging the multidimensional sample difficulty of image-text pairs, we enable MLLMs to learn from easy to hard samples in the training stage, mitigating the impact of samples of varying difficulty and preventing local optima. We can achieve better cross-modal alignment without complicated procedures based on the alignment and innate knowledge of MLLMs. Experimental results on two open-source MLLMs on a publicly released dataset MORE demonstrate that MDSD can further enhance MLLMs and achieve state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lGlmaoUfW8": {
    "title": "Does Reinforcement Learning from Human Feedback Framework Still Work for Task-Oriented Dialogue Systems?",
    "volume": "review",
    "abstract": "The paradigm of reinforcement learning from human feedback (RLHF) after supervised fine-tuning (SFT) language models has become widespread. In this work, we investigate whether RLHF with turn-level preferences is still effective in task-oriented dialogue (TOD) task that requires dialog-level rewards. Since there is no human preference dataset for TOD task, we develop two synthetic feedback generation methods for fully annotated or partially annotated TOD dataset. We compare these two methods to the corresponding SFT methods in an online environment where user goals are unknown. Despite the simplicity of the proposed methods, RLHF outperformed SFT on the partially annotated TOD dataset in both corpus-based and simulator-based evaluations. Our comprehensive experiments present a direction for effectively enhancing system performance using data generated while providing services in real-world environments",
    "checked": false,
    "id": "f3af4c95c048fc698085901fa9a8dc48bb3768e7",
    "semantic_title": "domain adaptation for conversational query production with the rag model feedback",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=47Jkgzr9HO": {
    "title": "Probing zero shot VLMs for hate meme detection: Opportunities, risks and interpretations",
    "volume": "review",
    "abstract": "Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for quality labeled datasets for accurate detection. Recently, the research community has witnessed the emergence of several vision language models (VLMs) that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of open-source VLMs in handling intricate tasks such as hate meme detection in a completely zero-shot setting. In particular, we systematically study various prompt strategies using zero-shot capabilities of VLMs to detect hateful/harmful memes. Next we use a novel superpixel based occlusion technique to obtain better interpretations of the misclassification results. Finally we show that these misclassified data points nicely cluster into well-defined topics thus naturally identifying the vulnerabilities of the VLMs and paving the way to better fabrication of safety guardrails in future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zozQq4UWN3": {
    "title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution",
    "volume": "review",
    "abstract": "The rise of LLM-based agents shows great potential to revolutionize task planning, capturing significant attention. Given that these agents will be integrated into high-stake domains, ensuring their reliability and safety is crucial. This paper presents an Agent-Constitution-based agent framework, TrustAgent, with a particular focus on improving the LLM-based agent safety. The proposed framework ensures strict adherence to the Agent Constitution through three strategic components: pre-planning strategy which injects safety knowledge to the model before plan generation, in-planning strategy which enhances safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Our experimental results demonstrate that the proposed framework can effectively enhance an LLM agent's safety across multiple domains by identifying and mitigating potential dangers during the planning. Further analysis reveals that the framework not only improves safety but also enhances the helpfulness of the agent. Additionally, we highlight the importance of the LLM reasoning ability in adhering to the Constitution. This paper sheds light on how to ensure the safe integration of LLM-based agents into human-centric environments. Data and code are available at \\url{https://anonymous.4open.science/r/TrustAgent-06DC}",
    "checked": true,
    "id": "da2880c1b525ae49125b9b2c627126d6891aab5a",
    "semantic_title": "trustagent: towards safe and trustworthy llm-based agents through agent constitution",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=yuBVl3SRrU": {
    "title": "How Do Llamas Process Multilingual Text? A Latent Exploration through Patchscopes",
    "volume": "review",
    "abstract": "A central question in multilingual language modeling is whether large language models (LLMs) develop a universal concept representation, disentangled from specific languages. In this paper, we address this question by analyzing the Llama-2's forward pass during a word translation task. We strategically extract latents from a source translation prompt and insert them into the forward pass on a target translation prompt. By doing so, we find that the output language is encoded in the latent at an earlier layer than the concept to be translated. Utilizing this insight we show that both, target concept in source language and source concept in target language, are achievable via patching alone. Furthermore, we show that patching in the mean of multiple source language latents does not impair our ability to decode source concept in target language, indicating that concept representations are language-agnostic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aw5grkur0F": {
    "title": "Knowledge Graph Enhanced Large Language Model Editing",
    "volume": "review",
    "abstract": "Large language models (LLMs) are pivotal in advancing natural language processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and outdated knowledge. Model editing emerges as a promising solution to address these challenges. However, existing editing methods struggle to track and incorporate changes in knowledge associated with edits, which limits the generalization ability of post-edit LLMs in processing edited knowledge. To tackle these problems, we propose a novel model editing method that leverages knowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we first utilize a knowledge graph augmentation module to uncover associated knowledge that has changed due to editing, obtaining its internal representations within LLMs. This approach allows knowledge alterations within LLMs to be reflected through an external graph structure. Subsequently, we design a graph-based knowledge edit module to integrate structured knowledge into the model editing. This ensures that the updated parameters reflect not only the modifications of the edited knowledge but also the changes in other associated knowledge resulting from the editing process. Comprehensive experiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME significantly improves the generalization capabilities of post-edit LLMs in employing edited knowledge",
    "checked": true,
    "id": "9acb2df43927a2df64a4b9d39d68ab0a482d05f5",
    "semantic_title": "knowledge graph enhanced large language model editing",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=thRA7Tlg89": {
    "title": "Efficient Overshadowed Entity Disambiguation by Mitigating Shortcut Learning",
    "volume": "review",
    "abstract": "Entity disambiguation (ED) is crucial in natural language processing (NLP) for tasks such as question-answering and information extraction. A major challenge in ED is handling overshadowed entitiesâ€”uncommon entities sharing mention surfaces with common entities. The current approach to enhance performance on these entities involves reasoning over facts in a knowledge base (KB), increasing computational overhead during inference. We argue that the ED performance on overshadowed entities can be enhanced during training by addressing shortcut learning, which does not add computational overhead at inference. We propose a simple yet effective debiasing technique to prevent models from shortcut learning during training. Experiments on a range of ED datasets show that our method achieves state-of-the-art performance without compromising inference speed. Our findings suggest a new research direction for improving entity disambiguation via shortcut learning mitigation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H63IXdjesp": {
    "title": "R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation",
    "volume": "review",
    "abstract": "Retrieval augmented generation (RAG) has been applied in many scenarios to augment large language models (LLMs) with external documents provided by retrievers. However, a semantic gap exists between LLMs and retrievers due to differences in their training objectives and architectures. This misalignment forces LLMs to passively accept the documents provided by the retrievers, leading to incomprehension in the generation process, where the LLMs are burdened with the task of distinguishing these documents using their inherent knowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill this gap by incorporating \\textbf{R}etrieval information into \\textbf{R}etrieval \\textbf{A}ugmented \\textbf{G}eneration. Specifically, R$^2$AG utilizes the nuanced features from the retrievers and employs a R$^2$-Former to capture retrieval information. Then, a retrieval-aware prompting strategy is designed to integrate retrieval information into LLMs' generation. Notably, R$^2$AG suits low-source scenarios where LLMs and retrievers are frozen. Extensive experiments across five datasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our analysis reveals that retrieval information serves as an anchor to aid LLMs in the generation process, thereby filling the semantic gap",
    "checked": true,
    "id": "48d11fb087d9d06ab4a1118305b9930a1f9ed3a9",
    "semantic_title": "r^2ag: incorporating retrieval information into retrieval augmented generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k7xjAa2X22": {
    "title": "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search",
    "volume": "review",
    "abstract": "Instruction tuning is a crucial technique for aligning language models with humans' actual goals in the real world. Extensive research has highlighted the quality of instruction data is essential for the success of this alignment. However, creating high-quality data manually is labor-intensive and time-consuming, which leads researchers to explore using LLMs to synthesize data. Recent studies have focused on using a stronger LLM to iteratively enhance existing instruction data, showing promising results. Nevertheless, previous work often lacks control over the evolution direction, resulting in high uncertainty in the data synthesis process and low-quality instructions. In this paper, we introduce a general and scalable framework, IDEA-MCTS (Instruction Data Enhancement using Monte Carlo Tree Search), a scalable framework for efficiently synthesizing instructions. With tree search and evaluation models, it can efficiently guide each instruction to evolve into a high-quality form, aiding in instruction fine-tuning. Experimental results show that IDEA-MCTS significantly enhances the seed instruction data, raising the average evaluation scores of quality, diversity, and complexity from 2.19 to 3.81. Furthermore, in open-domain benchmarks, experimental results show that IDEA-MCTS improves the accuracy of real-world instruction-following skills in LLMs by an average of 5% in low-resource settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pjITMVzArC": {
    "title": "Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for Information Retrieval",
    "volume": "review",
    "abstract": "Recent research has explored distilling knowledge from large language models (LLMs) to optimize retriever models, especially within the retrieval-augmented generation (RAG) framework. However, most existing training methods rely on extracting supervision signals from LLMs' weights or their output probabilities, which is not only resource-intensive but also incompatible with black-box LLMs. In this paper, we introduce $Intermediate Distillation$, a data-efficient knowledge distillation training scheme that treats LLMs as black boxes and distills their knowledge via an innovative LLM-ranker-retriever pipeline, solely using LLMs' ranking generation as the supervision signal. Extensive experiments demonstrate that our proposed method can significantly improve the performance of retriever models with only 1,000 training instances. Moreover, our distilled retriever model significantly boosts performance in question-answering tasks within the RAG framework, demonstrating the potential of LLMs to economically and effectively train smaller models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jyd1wQ5Mhh": {
    "title": "TopViewRS: Vision-Language Models as Top-View Spatial Reasoners",
    "volume": "review",
    "abstract": "Top-view perspective denotes a typical way in which humans read and reason over different types of maps, and it is vital for localization and navigation of humans as well as of `non-human' agents, such as the ones backed by large Vision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities of modern VLMs in this setup remain unattested and underexplored. In this work, we study their capability to understand and reason over spatial relations from the top view. The focus on top view also enables controlled evaluations at different granularity of spatial reasoning; we clearly disentangle different abilities (e.g., recognizing particular objects versus understanding their relative positions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset, consisting of 11,384 multiple-choice questions with either realistic or semantic top-view map as visual input. We then use it to study and evaluate VLMs across 4 perception and reasoning tasks with different levels of complexity. Evaluation of 10 representative open- and closed-source VLMs reveals the gap of more than 50% compared to average human performance, and it is even lower than the random baseline in some cases. Although additional experiments show that Chain-of-Thought reasoning can boost model capabilities by 5.82% on average, the overall performance of VLMs remains limited. Our findings underscore the critical need for enhanced model capability in top-view spatial reasoning and set a foundation for further research towards human-level proficiency of VLMs in real-world multimodal tasks",
    "checked": true,
    "id": "7530ff872ad718828fe44fe4610d50a5c48e7063",
    "semantic_title": "topviewrs: vision-language models as top-view spatial reasoners",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LHfAyDyHcE": {
    "title": "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging",
    "volume": "review",
    "abstract": "While large language models (LLMs) excel in many domains, their complexity and scale challenge deployment in resource-limited environments. Current compression techniques, such as parameter pruning, often fail to effectively utilize the knowledge from pruned parameters. To address these challenges, we propose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA), a novel approach that uses manifold learning and the Normalized Pairwise Information Bottleneck (NPIB) measure to merge similar layers, reducing model size while preserving essential performance. We evaluate MKA on multiple benchmark datasets and various LLMs. Our findings show that MKA not only preserves model performance but also achieves substantial compression ratios, outperforming traditional pruning methods. Moreover, when coupled with quantization, MKA delivers even greater compression. Specifically, on the MMLU dataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75\\% with a minimal performance decrease of only 2.82\\%. The proposed MKA method offers a resource-efficient and performance-preserving model compression technique for LLMs",
    "checked": true,
    "id": "fc46b1c65bdc89d89ddf91a5f59137c696fb6101",
    "semantic_title": "pruning via merging: compressing llms via manifold alignment based layer merging",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cGV6c2lXEj": {
    "title": "Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning",
    "volume": "review",
    "abstract": "Enhancing the instruction-following ability of Large Language Models primarily demands substantial instruction-tuning datasets. However, the sheer volume of these imposes a considerable computational burden and annotation cost. To investigate a label-efficient instruction tuning method that allows the model itself to actively sample subsets that are equally or even more effective, we introduce a self-evolving mechanism DiverseEvol. In this process, a model iteratively augments its training subset to refine its own performance, without requiring any intervention from humans or more advanced LLMs. The key to our data sampling technique lies in the enhancement of diversity in the chosen subsets, as the model selects new data points most distinct from any existing ones according to its current embedding space. Extensive experiments across three datasets and benchmarks demonstrate the effectiveness of DiverseEvol. Our models, trained on less than 4\\% of the original dataset, maintain or improve performance compared with finetuning on full data. We also provide empirical evidence to analyze the importance of diversity in instruction data and the iterative scheme as opposed to one-time sampling. Our code will be made publicly available",
    "checked": true,
    "id": "91e8ce403704a38bee8a5df90d99979a796d1741",
    "semantic_title": "self-evolved diverse data sampling for efficient instruction tuning",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=VZSRlvGyZt": {
    "title": "Towards Measuring and Modeling \"Culture\" in LLMs: A Survey",
    "volume": "review",
    "abstract": "We present a survey of more than 90 recent papers that aim to study cultural representation and inclusion in large language models (LLMs). We observe that none of the studies explicitly define \"culture, which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of \"culture\". We call these aspects the *proxies of culture*, and organize them across two dimensions of demographic and semantic proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of ``culture,'' such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness of probing techniques and situated studies on the impact of cultural mis- and under-representation in LLM-based applications",
    "checked": true,
    "id": "5a0573a3c15d094e8b3d488c11a660773f631070",
    "semantic_title": "towards measuring and modeling \"culture\" in llms: a survey",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=EYXS82MS9s": {
    "title": "LoraMap: Harnessing the Power of LoRA Connections",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) can benefit from mitigating hallucinations through fact-checking and overcoming substantial computational overhead with parameter-efficient techniques such as Low-Rank Adaptation (LoRA). While some studies have explored the parallel integration of multiple LoRAs, these approaches need attention to the connections between them. This paper investigates methods to establish connections among multiple LoRAs. We create three reasoning datasets tailored to fact-checking and fine-tune individual LoRAs, allowing them to view and reason from diverse perspectives. Then, we explore strategies for allocating these reasoning LoRAs and introduce LoraMap, an approach to map connections between them. The results on the fact-checking task demonstrate that the performance of LoraMap is superior to LoraHub, an existing LoRA composition method. LoraMap also outperforms with significantly fewer parameters than LoraConcat, which concatenates LoRAs and further fine-tunes them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZIu3i6bV5N": {
    "title": "Automatic sentence segmentation of clinical record narratives in real-world data",
    "volume": "review",
    "abstract": "Sentence segmentation is a linguistic task and is widely used as a pre-processing step in many NLP applications. The need for sentence segmentation is particularly pronounced in clinical notes, where ungrammatical and fragmented texts are common. We propose a straightforward and effective sequence labeling classifier to predict sentence spans using a dynamic sliding window based on the prediction of each input sequence. This sliding window algorithm allows our approach to segment long text sequences on the fly. To evaluate our approach, we annotated 90 clinical notes from the MIMIC-III dataset. Additionally, we tested our approach on five other datasets to assess its generalizability and compared its performance against state-of-the-art systems on these datasets. Our approach outperformed all the systems, achieving an F1 score that is 15\\% higher than the next best-performing system on the clinical dataset",
    "checked": false,
    "id": "1ed2fb9bdf61ea085307e5bb0054ed2815b267e7",
    "semantic_title": "extraction of crohn's disease clinical phenotypes from clinical text using natural language processing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5w0ghjY2Ct": {
    "title": "Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning",
    "volume": "review",
    "abstract": "Deception detection has attracted increasing attention due to its importance in real-world scenarios. Its main goal is to detect deceptive behaviors from multimodal clues such as gestures, facial expressions, prosody, etc. However, these bases are usually subjective and related to personal habits. Therefore, we extend deception detection to deception reasoning, further providing objective evidence to support subjective judgment. Specifically, we provide potential lies and basic facts and then analyze why this sentence may be a lie by combining factual inconsistencies and intent behind them. Compared with deception detection, this task is more applicable to real-world scenarios. For example, in interrogation, the police should judge whether a person is lying based on solid evidence. This paper presents our initial attempts at this task, including constructing a dataset and defining evaluation metrics. Meanwhile, this task can serve as a benchmark for evaluating the complex reasoning capability of large language models. Code and data are provided in the supplementary material",
    "checked": true,
    "id": "8a0414576973c122f72b09a25ba88d494cc7e36e",
    "semantic_title": "can deception detection go deeper? dataset, evaluation, and benchmark for deception reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5q5JbJqh5d": {
    "title": "Mixed Distillation Helps Smaller Language Models Reason Better",
    "volume": "review",
    "abstract": "As large language models (LLMs) have demonstrated impressive multiple step-by-step reasoning capabilities in recent natural language processing (NLP) reasoning tasks, many studies are interested in distilling reasoning abilities into smaller language models (SLMs) via fine-tuning. Previous distillation methods usually utilize the capabilities of LLMs to generate chain-of-thought (CoT) samples to teach SLMs. However, this distillation approach performs poorly in certain scenarios due to the limitations of CoT. In this work, we introduce a novel \\textbf{Mixed Distillation} (MD) framework, distilling multiple step-by-step reasoning abilities into SLMs. First, we leverage LLMs to generate multiple step-by-step reasoning rationales by sampling automatically. Then, we create high-quality, well-balanced mixed thought data and design a novel multi-task loss to help SLMs better learn and adaptively activate multiple step-by-step reasoning. Our extensive experiments demonstrate that MD enhances both single-path (using either CoT or PoT) and multi-path (using both CoT and PoT) reasoning abilities of SLMs during inference across reasoning tasks. Notably, a single model generated by MD exceeds the comprehensive performance of an ensemble of two individual CoT and PoT distilled models. Mistral-7B using MD can achieve remarkable improvements of 87.5\\%, 74.0% and 77.1% on SVAMP, GSM8K and ASDIV, respectively, outperforming the teacher model, GPT-3.5-Turbo. We hope our work provides insight into SLMs' multiple step-by-step reasoning abilities",
    "checked": false,
    "id": "1bd9466f0bb10d29a16f614943ec7823e13cb210",
    "semantic_title": "mixed distillation helps smaller language model better reasoning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=X2LNm1SCjs": {
    "title": "Alleviating Hallucinations of Large Language Models through Induced Hallucinations",
    "volume": "review",
    "abstract": "Despite their impressive capabilities, large language models (LLMs) have been observed to generate responses that include inaccurate or fabricated information, a phenomenon commonly known as hallucination. In this work, we propose a simple Induce-then-Contrast Decoding (ICD) strategy to alleviate hallucinations. We first construct a factually weak LLM by inducing hallucinations from the original LLMs. Then, we penalize these induced hallucinations during decoding to enhance the factuality of the generated content. Concretely, we determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding. Experimental results on both discrimination-based and generation-based hallucination evaluation benchmarks, such as TruthfulQA and \\textsc{FActScore}, demonstrate that our proposed ICD methods can effectively enhance the factuality of LLMs across various task formats, model sizes, and model families. For example, when equipped with ICD, Llama2-7B-Chat and Mistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on TruthfulQA, respectively, without compromising their generalization capabilities on other tasks",
    "checked": true,
    "id": "3f915aab835cbfe69e7b2ea1c73b74ac8a2d384e",
    "semantic_title": "alleviating hallucinations of large language models through induced hallucinations",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=RqlRBySXu0": {
    "title": "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks",
    "volume": "review",
    "abstract": "Creating human-like large language model (LLM) agents is crucial for faithful social simulation. Having LLMs role-play based on demographic information sometimes improves human likeness but often does not. This study assessed whether LLM alignment with human behavior can be improved by integrating information from empirically-derived human belief networks. Using data from a human survey, we estimated a belief network encompassing 18 topics loading on two non-overlapping latent factors. We then seeded LLM-based agents with an opinion on one topic, and assessed the alignment of its expressed opinions on remaining test topics with corresponding human data. Role-playing based on demographic information alone did not align LLM and human opinions, but seeding the agent with a single belief greatly improved alignment for topics related in the belief network, and not for topics outside the network. These results suggest a novel path for human-LLM belief alignment in work seeking to simulate and understand patterns of belief distributions in society",
    "checked": true,
    "id": "04e5c40f897098d1781e2d6ee721f5fafcbf0417",
    "semantic_title": "beyond demographics: aligning role-playing llm-based agents using human belief networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iWXp19Qh6s": {
    "title": "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation",
    "volume": "review",
    "abstract": "Despite the significant progress of large language models (LLMs) in various tasks, they often produce factual errors due to their limited internal knowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with external knowledge sources, offers a promising solution. However, these methods can be misled by irrelevant paragraphs in retrieved documents. Due to the inherent uncertainty in LLM generation, inputting the entire document may introduce off-topic information, causing the model to deviate from the central topic and affecting the relevance of the generated content. To address these issues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates plan tokens to guide subsequent generation in the plan stage. In the answer stage, the model selects relevant fine-grained paragraphs based on the plan and uses them for further answer generation. This plan-answer process is repeated iteratively until completion, enhancing generation relevance by focusing on specific topics. To implement this framework efficiently, we utilize a simple but effective multi-task prompt-tuning method, enabling the existing LLMs to handle both planning and answering. We comprehensively compare RPG with baselines across 5 knowledge-intensive generation tasks, demonstrating the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aI9MESYatf": {
    "title": "LongAlign: A Recipe for Long Context Alignment of Large Language Models",
    "volume": "review",
    "abstract": "Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length. To address this, we present LongAlign---a recipe of the instruction data, training, and evaluation for long context alignment. First, we construct a long instruction-following dataset using Self-Instruct. To ensure the data diversity, it covers a broad range of tasks from various long context sources. Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. Third, we introduce the LongBench-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length. Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30%, while also maintaining their proficiency in handling short, generic tasks",
    "checked": true,
    "id": "ec9203f6c25a353325dd23ed38e5036b79d9e79b",
    "semantic_title": "longalign: a recipe for long context alignment of large language models",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=YeuRO2UdvZ": {
    "title": "DA 3 : A Distribution-Aware Adversarial Attack against Language Models",
    "volume": "review",
    "abstract": "Language models can be manipulated by adversarial attacks, which introduce subtle perturbations to input data. While recent attack methods can achieve a relatively high attack success rate (ASR), we've observed that the generated adversarial examples have a different data distribution compared with the original examples. Specifically, these adversarial examples exhibit reduced confidence levels and greater divergence from the training data distribution. Consequently, they are easy to detect using straightforward detection methods, diminishing the efficacy of such attacks. To address this issue, we propose a Distribution-Aware Adversarial Attack (DA$^3$) method. DA$^3$ considers the distribution shifts of adversarial examples to improve attacks' effectiveness under detection methods. We further design a novel evaluation metric, the Non-detectable Attack Success Rate (NASR), which integrates both ASR and detectability for the attack task. We conduct experiments on four widely used datasets to validate the attack effectiveness and transferability of adversarial examples generated by DA$^3$ against both the white-box BERT-base and RoBERTa-base models and the black-box LLaMA2-7b model",
    "checked": false,
    "id": "e58033abe114035ef7bffb50934e0abad9a99f77",
    "semantic_title": "dec . 8 , 2017 learning in the presence of strategic behavior",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CGx4w1YOrH": {
    "title": "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models",
    "volume": "review",
    "abstract": "As Large Language Models (LLMs) continue to exhibit remarkable performance in natural language understanding tasks, there is a crucial need to measure their ability for human-like multi-step logical reasoning. Existing logical reasoning evaluation benchmarks often focus primarily on simplistic single-step or multi-step reasoning with a limited set of inference rules. Furthermore, the lack of datasets for evaluating non-monotonic reasoning represents a crucial gap since it aligns more closely with human-like reasoning. To address these limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset encompassing multi-step logical reasoning with various inference rules and depths. Multi-LogiEval covers three logic types â€” propositional, first-order, and non-monotonic consisting of more than 30 inference rules and more than 60 of their combinations with various depths. Leveraging this dataset, we conduct evaluations on a range of LLMs such as GPT-4, ChatGPT, Gemini-Pro, Orca, and Mistral, employing a zero-shot chain-of-thought. Experimental results show that there is a significant drop in the performance of LLMs as the reasoning steps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5). We further conduct a thorough investigation of reasoning chains generated by LLMs which reveals several important findings. We believe that Multi-LogiEval facilitates future research for evaluating and enhancing the logical reasoning ability of LLMs",
    "checked": true,
    "id": "12aea189b9d108cac5475a81f73f0187c05bd730",
    "semantic_title": "multi-logieval: towards evaluating multi-step logical reasoning ability of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gLcpCzxoiM": {
    "title": "The Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples",
    "volume": "review",
    "abstract": "Recent studies have revealed the vulnerability of pre-trained language models to adversarial attacks. Adversarial defense techniques have been proposed to reconstruct adversarial examples within feature or text spaces. However, these methods struggle to effectively repair the semantics in adversarial examples, resulting in unsatisfactory defense performance. To repair the semantics in adversarial examples, we introduce a novel approach named Reactive Perturbation Defocusing (Rapid), which employs an adversarial detector to identify the fake labels of adversarial examples and leverages adversarial attackers to repair the semantics in adversarial examples. Our extensive experimental results, conducted on four public datasets, demonstrate the consistent effectiveness of Rapid in various adversarial attack scenarios. For easy evaluation, we provide a click-to-run demo of Rapid at \\url{https://tinyurl.com/22ercuf8}",
    "checked": true,
    "id": "32f4780a3e9e63e316d4ef23404139b7f2daa963",
    "semantic_title": "the best defense is attack: repairing semantics in textual adversarial examples",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EduhNteyC9": {
    "title": "DECOR: Improving Coherence in L2 English Writing with a Novel Benchmark for Incoherence Detection, Reasoning, and Rewriting",
    "volume": "review",
    "abstract": "Coherence in writing, an aspect that L2 English learners often struggle with, is crucial in assessing L2 English writing. Existing automated writing evaluation systems primarily use basic surface linguistic features to detect coherence in writing. However, little effort has been made to correct the detected incoherence, which could significantly benefit L2 language learners seeking to improve their writing. To bridge this gap, we introduce DECOR, a novel benchmark that includes expert annotations for detecting incoherence in L2 English writing, identifying the underlying reasons, and rewriting the incoherent sentences. To our knowledge, DECOR is the first coherence assessment dataset specifically designed for improving L2 English writing, featuring pairs of original incoherent sentences alongside their expert-rewritten counterparts. Additionally, we fine-tuned models to automatically detect and rewrite incoherence in student essays. We find that incorporating specific reasons for incoherence during fine-tuning consistently improves the quality of the rewrites, achieving a level that is favored in both automatic and human evaluations",
    "checked": true,
    "id": "ef00e8f46708f0345270e19b24dc03aee32c5a3e",
    "semantic_title": "decor: improving coherence in l2 english writing with a novel benchmark for incoherence detection, reasoning, and rewriting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sm71zywX66": {
    "title": "Understanding Performance of Long-Document Ranking Models through Comprehensive Evaluation and Leaderboarding",
    "volume": "review",
    "abstract": "We evaluated 20+ Transformer models for ranking of long documents (including recent LongP models trained with FlashAttention) and compared them with a simple FirstP baseline, which applies the same model to the truncated input (at most 512 tokens). We used MS MARCO Documents v1 as a primary training set and evaluated both the zero-shot transferred and fine-tuned models. On MS MARCO, TREC DLs, and Robust04 no long-document model outperformed FirstP by more than 5% in NDCG and MRR (when averaged over all test sets). We conjectured this was not due to models' inability to process long context, but due to a positional bias of relevant passages, whose distribution was skewed towards the beginning of documents. We found direct evidence of this bias in some test sets, which motivated us to create MS MARCO FarRelevant (based on MS MARCO Passages) where the relevant passages were not present among the first 512 tokens. Unlike standard collections where we saw both little benefit from incorporating longer contexts and limited variability in model performance (within a few %), experiments on MS MARCO FarRelevant uncovered dramatic differences among models. The FirstP models per- formed roughly at the random-baseline level in both zero-shot and fine-tuning scenarios. Simple aggregation models including MaxP and PARADE Attention had good zero-shot accuracy, but benefited little from fine-tuning. Most other models had poor zero-shot performance (sometimes at a random baseline level), but outstripped MaxP by as much as 13-28% after finetuning. Thus, the positional bias not only diminishes benefits of processing longer document contexts, but also leads to model over- fitting to positional bias and performing poorly in a zero-shot setting when the distribution of relevant passages changes substantially. We make our software and data available",
    "checked": true,
    "id": "3f848e8620a1f1ccef544e46b483ceff5cbc7f2a",
    "semantic_title": "understanding performance of long-document ranking models through comprehensive evaluation and leaderboarding",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=NM26AdIWdY": {
    "title": "In-Memory Learning: A Declarative Learning Framework for Large Language Models",
    "volume": "review",
    "abstract": "The exploration of whether agents can align with their environment without relying on human-labeled data presents an intriguing research topic. Drawing inspiration from the alignment process observed in intelligent organisms, where declarative memory plays a pivotal role in summarizing past experiences, we propose a novel learning framework. The agents adeptly distill insights from past experiences, refining and updating existing notes to enhance their performance in the environment. This entire process transpires within the memory components and is implemented through natural language, so we character this framework as In-memory Learning. We also delve into the key features of benchmarks designed to evaluate the self-improvement process. Through systematic experiments, we demonstrate the effectiveness of our framework and provide insights into this problem",
    "checked": true,
    "id": "002a2f9da012bcf37e59c7ea0406f6bf11e5ab55",
    "semantic_title": "in-memory learning: a declarative learning framework for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=EF0bkisSQX": {
    "title": "Comparative Study of Named Entity Recognition Models",
    "volume": "review",
    "abstract": "Named Entity Recognition (NER) is a fundamental and non-trivial task in natural language processing, that is crucial for various downstream applications. This paper presents a comprehensive comparative study of NER performance across a spectrum of state-of-the-art models, with a particular focus on the adaptation and fine-tuning of Question Answering (QA) models, such as BERT and RoBERTa, alongside prominent text generation models, including Llama2 (Touvron et al., 2023), Mistral (Jiang et al., 2023), and ChatGPT3.5-Turbo. In this study, we explore the efficacy of QA models when repurposed and adapted to NER tasks and additionally, we examine the zero-shot capabilities of Large Language Models, utilizing them without task-specific fine-tuning to assess their innate ability to recognize named entities. Through extensive experimentation on the benchmark dataset BUSTER, we analyze and compare the precision, recall, and F1 scores of each model variant across various NER categories. Furthermore, we investigate the robustness of these models under different training regimes and evaluation metrics",
    "checked": false,
    "id": "df8740034b68e4250d0ceefa9fcbdf42c83af25d",
    "semantic_title": "a comparative study of cross-sentence features for named entity recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q8SncUWGSZ": {
    "title": "Evaluating Computational Metrics for Predicting N400 Amplitude during Reading Comprehension",
    "volume": "review",
    "abstract": "Given the interest recent research showed towards cognitive modeling of ERPs, we explored whether traditional word-level features such as position, word frequency, and number of strokes overlap with probability-based metrics such as surprisal, entropy, and entropy reduction. Analyzing and comparing different generalized linear models we found that the mathematical metrics do represent the same information as some of the \"traditional\" features overpowering them. A new cognitive-motivated computational feature is proposed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ml6a8y161S": {
    "title": "Multilingual evaluation of image captioning: How far can we get with CLIP models?",
    "volume": "review",
    "abstract": "The evaluation of image captions, looking at both linguistic fluency and semantic correspondence to visual contents, has witnessed a significant effort. Still, despite advancements such as the CLIPScore metric, multilingual captioning evaluation has remained relatively unexplored. This work assesses the use of CLIPScore in multilingual captioning, evaluating different models in a variety of settings. To address the lack of multilingual test data, we consider two different strategies: (1) using machine-translated datasets with human judgements, and (2) re-purposing multilingual datasets that target inference and reasoning. Our results show that multilingual CLIP models can perform on par with their English-centric counterparts on English benchmarks while allowing for multilingual assessments. Performance increases with model finetuning and according to model size. Larger models, trained with more data, attained similar performance to more advanced methods that extended the original CLIPScore. Tests with machine-translated data show that multilingual CLIPScore can also maintain a high correlation with human judgements across different languages, and additional tests with natively multilingual and multicultural data further attest to the high-quality assessments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1EPBE4ynAy": {
    "title": "LINKAGE: Listwise Ranking among Varied-Quality References for Non-Factoid QA Evaluation via LLMs",
    "volume": "review",
    "abstract": "Non-Factoid (NF) Question Answering (QA) is challenging to evaluate due to diverse potential answers and no objective criterion. The commonly used automatic evaluation metrics like ROUGE or BERTScore cannot accurately measure semantic similarities or answers from different perspectives. Recently, Large Language Models (LLMs) have been resorted to for NFQA evaluation due to their compelling performance on various NLP tasks. Common approaches include pointwise scoring of each candidate answer and pairwise comparisons between answers. Inspired by the evolution from pointwise to pairwise to listwise in learning-to-rank methods, we propose a novel listwise NFQA evaluation approach, that utilizes LLMs to rank candidate answers in a list of reference answers sorted by descending quality. Moreover, for NF questions that do not have multi-grade or any golden answers, we leverage LLMs to generate the reference answer list of various quality to facilitate the listwise evaluation. Extensive experimental results on three NFQA datasets, i.e., ANTIQUE, the TREC-DL-NF, and WebGLM show that our method has significantly higher correlations with human annotations compared to automatic scores and common pointwise and pairwise approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sVBRqSO9Ad": {
    "title": "A Cognitive-grounded Computational Model for Emotional Alignment During Conversations",
    "volume": "review",
    "abstract": "Aligning emotionally during a conversation means showing appropriate emotional reactions to what our interlocutors say and the emotions they share with us. These appropriate emotional reactions are dictated by social standards and ensure smooth and effective interactions. Based on a psychological framework, and adapting already existing models from cognitive modeling and NLP, we investigate the role in conversational dynamics of 1) social expectations over emotional reactions, 2) internal emotional state, and 3) dialog acts. We implement and compare graph-based models and deep learning models on the task of emotion prediction, employing categorical accuracy as the target metric and using MELD and DialyDialog as benchmarks. The results suggest that the internal emotional state and the dialog acts have an influence on the emotional reaction during conversations. These elements, however, did not show a significant impact within the deep learning model. Possible improvements to the models and insights on future directions are provided",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dw3rqkELFQ": {
    "title": "Literacy is g -eneral for a Small Language: Evaluating GLLMs in Danish",
    "volume": "review",
    "abstract": "When modern language processing systems became massively multilingual with the succes of Generative, Large Language Models (GLLMs), some low-resource languages experienced a language technology moonshot moment. As GLLM performance is mainly presented on English-language benchmarks and low-resource languages lack available evaluation corpora, model capabilities are challenging to verify beyond qualitative demonstrations. We present a GLLM benchmark for one low-resource language, evaluating model capabilities in Danish across eight diverse scenarios including Danish cultural knowledge and abstractive question answering. This limited-size benchmark is found to produce a robust ranking that correlates to human feedback at $\\rho \\sim 0.8$ with GPT-4 and Claude Opus models at the top spot. Analyzing these model results across scenarios, we find one strong underlying $g$-factor explaining $95$ % of variance for GLLMs in Danish, suggesting model consistency in language adaption",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0oUnTvaPgG": {
    "title": "Knowledge Editing of Large Language Models in the Wild",
    "volume": "review",
    "abstract": "Large language models (LLMs) face the issue of rapid obsolescence as the information they store can quickly become outdated. In addition, retraining LLMs is expensive. Efficient methods for knowledge editing of LLMs are crucial. Existing datasets for knowledge editing typically assume that new knowledge is injected as a simple sentence that details a single tuple, such as \"Ellie Kemper is a citizen of United States of America''. However, we are concerned that these datasets are inadequate for evaluating real-world scenarios. In-the-wild text data from natural settings often contains ambiguous relationships between entities and does not solely detail a single tuple. This difference can lead to a drop in performance for existing methods. In this study, we present a new dataset, MQuAKE-Wild, which features new knowledge presented in a style that resembles naturally occurring text. The new dataset provides a benchmark to evaluate the performance of existing methods in scenarios that are more representative of real-world applications. Our findings indicate that current methods perform poor on such a dataset. To tackle the challenge, we propose an innovative architectural design, MuRef, that leverages natural data to refine the relationships between entities. Comparing with existing methods, our method is superior on wild data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BqooRAZN3T": {
    "title": "Amphista: Accelerate LLM Inference with Bi-directional Multiple Drafting Heads in a Non-autoregressive Style",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) inherently use autoregressive decoding, which lacks parallelism in inference and results in significantly slow inference speeds, especially when hardware parallel accelerators and memory bandwidth are not fully utilized. In this work, we propose Amphista, a speculative decoding algorithm that adheres to a non-autoregressive decoding paradigm. Owing to the increased parallelism, our method demonstrates higher efficiency in inference compared to autoregressive methods. Specifically, Amphista models an Auto-embedding Block capable of parallel inference, incorporating bi-directional attention to enable interaction between different drafting heads. Additionally, Amphista implements Staged Adaptation Layers to facilitate the transition of semantic information from the base model's autoregressive inference to the drafting heads' non-autoregressive speculation, thereby achieving paradigm transformation and feature fusion. We conduct a series of experiments on a suite of Vicuna models using MT-Bench and Spec-Bench. For the Vicuna 33B model, Amphista achieves up to 2.75$\\times$ and 1.40$\\times$ wall-clock acceleration compared to vanilla autoregressive decoding and Medusa, respectively, while preserving lossless generation quality",
    "checked": true,
    "id": "cdb07a7765ddae5fc480e5d2521b878b961de966",
    "semantic_title": "amphista: accelerate llm inference with bi-directional multiple drafting heads in a non-autoregressive style",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=XpWOKaxfCE": {
    "title": "Detecting Machine-Generated Text: Not just \"AI vs Humans\" and Explainability is Complicated",
    "volume": "review",
    "abstract": "As Large Language Models (LLMs) rapidly advance, increasing concerns arise regarding risks about the actual authorship of texts we see online and in the real world. The task of distinguishing LLM-authored texts is complicated by the nuanced and overlapping behaviors of both machines and humans. In this paper, we challenge the current practice of considering the LLM-generated text detection a binary classification task of differentiating human from AI. Instead, we introduce a novel ternary text classification scheme, adding an ''undecided'' category for texts that could be attributed to either source, and we show that this new category is crucial to understand how to make the detection result more explainable to lay users. This research shifts the paradigm from merely classifying to explaining machine-generated texts, emphasizing the need for detectors to provide clear and understandable explanations to users. Our study involves creating four new datasets comprised of texts from various LLMs and human authors. Based on the new datasets, we performed binary classification tests to ascertain the most effective state-of-the-art (SOTA) detection methods and identified SOTA LLMs capable of producing harder-to-detect texts. Then, we constructed a new dataset of texts generated by the two top-performing LLMs and human authors, and asked three human annotators to produce ternary labels with explanation notes. This dataset was used to investigate how three top-performing SOTA detectors behave in the new ternary classification context. Our results highlight why the ''undecided'' category is much needed from the viewpoint of explainability. Additionally, we conducted an analysis of explainability of the three best-performing detectors and the explanation notes of the human annotators, revealing insights about the complexity of explainable detection of machine-generated texts. Finally, we propose guidelines for developing future detection systems with improved explanatory power",
    "checked": false,
    "id": "2d68804f314a87e591a60a59412674f100b984fa",
    "semantic_title": "detecting machine-generated texts: not just \"ai vs humans\" and explainability is complicated",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QRoSeqTJLa": {
    "title": "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems",
    "volume": "review",
    "abstract": "Generating faithful and fast responses is crucial in the knowledge-grounded dialogue. Retrieval Augmented Generation (RAG) strategies are effective but are inference inefficient, while previous Retrieval Free Generations (RFG) are more efficient but sacrifice faithfulness. To solve this faithfulness-efficiency trade-off dilemma, we propose a novel retrieval-free model training scheme named Retrieval Augmented to Retrieval Free Distillation (RA2FD) to build a retrieval-free model that achieves higher faithfulness than the previous RFG method while maintaining inference efficiency. The core idea of RA2FD is to use a teacher-student framework to distill the faithfulness capacity of a teacher, which is an oracle RAG model that generates multiple knowledge-infused responses. The student retrieval-free model learns how to generate faithful responses from these teacher labels through sequence-level distillation and contrastive learning. Experiment results show that RA2FD let the faithfulness performance of an RFG model surpass the previous SOTA RFG baseline on three knowledge-grounded dialogue datasets by an average of 33% and even matching an RAG model's performance while significantly improving inference efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z3GKTf6h6t": {
    "title": "Is Child-Directed Speech Effective Training Data for Language Models?",
    "volume": "review",
    "abstract": "While high-performing language models are typically trained on hundreds of billions of words, human children become fluent language users with a much smaller amount of data. What are the features of the data they receive, and how do these features support language modeling objectives? To investigate this question, we train GPT-2 models on 29M words of English-language child-directed speech and a new matched, synthetic dataset (TinyDialogues), comparing to a heterogeneous blend of datasets from the BabyLM challenge. We evaluate both the syntactic and semantic knowledge of these models using developmentally-inspired evaluations. Through pretraining experiments, we test whether the global developmental ordering or the local discourse ordering of children's training data support high performance relative to other datasets. The local properties of the data affect model results, but somewhat surprisingly, global properties do not. Further, child language input is not uniquely valuable for training language models. These findings support the hypothesis that, rather than proceeding from better data, children's learning is instead substantially more efficient than current language modeling techniques",
    "checked": true,
    "id": "1914a29816bb5f09f6c46c862d8db3ecd4b6923f",
    "semantic_title": "is child-directed speech effective training data for language models?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FbpCKiIvmC": {
    "title": "Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues",
    "volume": "review",
    "abstract": "A successful negotiation requires a range of capabilities, including comprehension of the conversation context, Theory-of-Mind (ToM) skills to infer the partner's motives, strategic reasoning, and effective communication, making it challenging for automated systems. Despite the remarkable performance of LLMs in various NLP tasks, there is no systematic evaluation of their capabilities in negotiation. Such an evaluation is critical for advancing AI negotiation agents and negotiation research, ranging from designing dialogue systems to providing pedagogical feedback and scaling up data collection practices. This work aims to systematically analyze the multifaceted capabilities of LLMs across diverse dialogue scenarios throughout the stages of a typical negotiation interaction. Our analysis highlights GPT-4's superior performance in many tasks while identifying specific challenges, such as making subjective assessments and generating contextually appropriate, strategically advantageous responses",
    "checked": true,
    "id": "4e61fa433eb1a905309f5955a75235e74f7eb6b2",
    "semantic_title": "are llms effective negotiators? systematic evaluation of the multifaceted capabilities of llms in negotiation dialogues",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=SRATnNcHDK": {
    "title": "Token-based Decision Criteria Are Suboptimal in In-context Learning",
    "volume": "review",
    "abstract": "In-Context Learning (ICL) typically utilizes classification criteria from probabilities of manually selected label tokens. However, we argue that such token-based classification criteria lead to suboptimal decision boundaries, despite delicate calibrations through translation and constrained rotation. To address this problem, we propose Hidden Calibration, which renounces token probabilities and uses the nearest centroid classifier on the LM's last hidden states. In detail, we use the nearest centroid classification on the hidden states, assigning the category of the nearest centroid previously observed from a few-shot calibration set to the test sample as the predicted label. Our experiments on 3 models and 10 classification datasets indicate that Hidden Calibration consistently outperforms current token-based calibrations by about 20%. Our further analysis demonstrates that Hidden Calibration finds better classification criteria with less inter-categories overlap, and LMs provide linearly separable intra-category clusters with the help of demonstrations, which supports Hidden Calibration and gives new insights into the conventional ICL",
    "checked": true,
    "id": "356bcfd8e4f367363c09ccb05f39018613b26539",
    "semantic_title": "token-based decision criteria are suboptimal in in-context learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UBTcvaGl21": {
    "title": "FactGenius: Combining Zero-Shot Prompting and Fuzzy Relation Mining to Improve Fact Verification with Knowledge Graphs",
    "volume": "review",
    "abstract": "Fact-checking is a crucial natural language processing (NLP) task that verifies the truthfulness of claims by considering reliable evidence. Traditional methods are labour-intensive, and most automatic approaches focus on using documents as evidence. In this paper, we focus on the relatively under-researched fact-checking with Knowledge Graph data as evidence and experiment on the recently introduced FactKG benchmark. We present FactGenius, a novel method that enhances fact-checking by combining zero-shot prompting of large language models (LLMs) with fuzzy text matching on knowledge graphs (KGs). Our method employs LLMs for filtering relevant connections from the graph and validates these connections via distance-based matching. The evaluation of FactGenius on an existing benchmark demonstrates its effectiveness, as we show it significantly outperforms state-of-the-art methods",
    "checked": true,
    "id": "5ca9b721b3dce19944751524289677eb478faf82",
    "semantic_title": "factgenius: combining zero-shot prompting and fuzzy relation mining to improve fact verification with knowledge graphs",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=DqSBGhmQXb": {
    "title": "Correct after Answer: Enhancing Multi-Span Question Answering with Post-Processing Method",
    "volume": "review",
    "abstract": "Multi-Span Question Answering (MSQA) requires models to extract one or multiple answer spans from a given context to answer a question. Prior work mainly focus on designing specific methods or applying heuristic strategies to encourage models to predict more correct predictions. However, these models are trained on gold answers and fail to considier the incorrect predictions. Through a statistical analysis, we observe that models with stronger abilities do not predict less incorrect predictions compared to other models. In this work, we propose $\\textbf{Answering-Classifying-Correcting}$ (ACC) framework, which employs a post-processing strategy to handle with incorrect predictions. Specifically, the ACC framework first introduces a $\\textbf{classifier}$ to classify the predictions into three types and exclude \"wrong predictions\", then introduces a $\\textbf{corrector}$ to modify \"partially correct predictions\". Experiments on four datasets show that ACC framework significantly improves the EM F1 scores of several MSQA models, and further analysis demostrate that ACC framework efficiently reduces the number of incorrect predictions, improving the quality of predictions. Our code and data are available at https://anonymous.4open.science/r/ACC-F6FB",
    "checked": false,
    "id": "b6f76b01d35d86890dbc793d4d9573ef7560a412",
    "semantic_title": "modeling uncertainty and using post-fusion as fallback improves retrieval augmented generation with llms",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=8ywPAiYbQb": {
    "title": "Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models",
    "volume": "review",
    "abstract": "As large language models (LLMs) increasingly permeate daily lives, there is a growing demand for real-time interactions that mirror human conversations. Traditional turn-based chat systems driven by LLMs prevent users from verbally interacting with the system while it is generating responses. To overcome these limitations, we adapt existing LLMs to \\textit{duplex models} so that these LLMs can listen for users while generating output and dynamically adjust themselves to provide users with instant feedback. % such as in response to interruptions. Specifically, we divide the queries and responses of conversations into several time slices and then adopt a time-division-multiplexing (TDM) encoding-decoding strategy to pseudo-simultaneously process these slices. Furthermore, to make LLMs proficient enough to handle real-time conversations, we build a fine-tuning dataset consisting of alternating time slices of queries and responses as well as covering typical feedback types in instantaneous interactions. Our experiments show that although the queries and responses of conversations are segmented into incomplete slices for processing, LLMs can preserve their original performance on standard benchmarks with a few fine-tuning steps on our dataset. Automatic and human evaluation indicate that duplex models make user-AI interactions more natural and human-like, and greatly improve user satisfaction compared to vanilla LLMs. Our duplex model and dataset will be released",
    "checked": true,
    "id": "125ff68732c441e143cf33587bebd2ab01372e49",
    "semantic_title": "beyond the turn-based game: enabling real-time conversations with duplex models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dNecVbFZVX": {
    "title": "How Well Do Multi-modal LLMs Interpret CT Scans? An Auto-Evaluation Framework for Analyses",
    "volume": "review",
    "abstract": "Automatically interpreting CT scans can ease the workload of radiologists. However, this is challenging mainly due to the scarcity of adequate datasets and reference standards for evaluation. This study aims to bridge this gap by introducing a novel evaluation framework, named ``GPTRadScore''. This framework assesses the capabilities of multi-modal LLMs, such as GPT-4 with Vision (GPT-4V), Gemini Pro Vision, LLaVA-Med, and RadFM, in generating descriptions for prospectively-identified findings. By employing a decomposition technique based on GPT-4, GPTRadScore compares these generated descriptions with gold-standard report sentences, analyzing their accuracy in terms of body part, location, and type of finding. Evaluations demonstrated a high correlation with clinician assessments and highlighted its potential over traditional metrics, such as BLEU, METEOR, and ROUGE. Furthermore, to contribute to future studies, we plan to release a benchmark dataset annotated by clinicians. Using GPTRadScore, we found that while GPT-4V and Gemini Pro Vision fare better, their performance revealed significant areas for improvement, primarily due to limitations in the dataset used for training these models. To demonstrate this potential, RadFM was fine-tuned and it resulted in significant accuracy improvements: location accuracy rose from 3.41\\% to 12.8\\%, body part accuracy from 29.12\\% to 53\\%, and type accuracy from 9.24\\% to 30\\%, thereby validating our hypothesis",
    "checked": true,
    "id": "88b77125f51105b67ad6c033a484d62fd98afd49",
    "semantic_title": "how well do multi-modal llms interpret ct scans? an auto-evaluation framework for analyses",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rfn0euSgbX": {
    "title": "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models",
    "volume": "review",
    "abstract": "Language Language Models (LLMs) face safety concerns due to potential misuse by malicious users. Recent red-teaming efforts have identified adversarial suffixes capable of jailbreaking LLMs using the gradient-based search algorithm Greedy Coordinate Gradient (GCG). However, GCG struggles with computational inefficiency, limiting further investigations regarding suffix transferability and scalability across models and data. In this work, we bridge the connection between search efficiency and suffix transferability. We propose a two-stage transfer learning framework, DeGCG, which decouples the search process into behavior-agnostic pre-searching and behavior-relevant post-searching. Specifically, we employ direct first target token optimization in pre-searching to facilitate the search process. We apply our approach to cross-model, cross-data, and self-transfer scenarios. Furthermore, we introduce an interleaved variant of our approach, i-DeGCG, which iteratively leverages self-transferability to accelerate the search process. Experiments on HarmBench demonstrate the efficiency of our approach across various models and domains. Notably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of $43.9$ ($+22.2$) and $39.0$ ($+19.5$) on valid and test sets, respectively. Further analysis on cross-model transfer indicates the pivotal role of first target token optimization in leveraging suffix transferability for efficient searching",
    "checked": false,
    "id": "4ad33969188555b8303b375e18f5c117a68387c6",
    "semantic_title": "amplegcg: learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=WzG1rQVkPP": {
    "title": "Orchestrating Human and AI Feedback: PCUI-DPO for Human-Aligned LLM Responses",
    "volume": "review",
    "abstract": "This paper proposes a novel approach to Large Language Model (LLM) training that prioritizes AI-generated responses, reducing reliance on extensive human feedback. We introduce the Predicted Confidence and Uncertainty Index (PCUI) metric, offering a new dimension of LLM interpretability by capturing both confidence and uncertainty in generated text. Integrating PCUI into Direct Preference Optimization (DPO) guides the model towards favoring its own high-confidence responses during training. Notably, a confidence threshold is established using PCUI, enabling the model to prioritize AI-generated responses exceeding the threshold over human-provided feedback. This approach promotes a gradual shift towards automated LLM training with interpretability and control. We demonstrate the effectiveness of this method in text generation tasks, achieving significant improvements in performance. This work lays the groundwork for a future where AI and human feedback collaborate to create more robust and user-centric LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BNp3NLZbTc": {
    "title": "LLM-Assisted Content Conditional Debiasing for Fair Text Embedding",
    "volume": "review",
    "abstract": "Mitigating biases in machine learning models has become an increasing concern in Natural Language Processing (NLP), particularly in developing fair text embeddings, which are crucial yet challenging for real-world applications like search engines. In response, this paper proposes a novel method for learning fair text embeddings. First, we define a novel content-conditional equal distance (CCED) fairness for text embeddings, ensuring content-conditional independence between sensitive attributes and text embeddings. Building on CCED, we introduce a content-conditional debiasing (CCD) loss to ensure that embeddings of texts with different sensitive attributes but identical content maintain the same distance from the embedding of their corresponding neutral text. Additionally, we tackle the issue of insufficient training data by using Large Language Models (LLMs) with instructions to fairly augment texts into different sensitive groups. Our extensive evaluations show that our approach effectively enhances fairness while maintaining the utility of embeddings. Furthermore, our augmented dataset, combined with the CCED metric, serves as an new benchmark for evaluating fairness",
    "checked": true,
    "id": "186ebf480e02b89d69afc87781dc865d1cfe47ab",
    "semantic_title": "llm-assisted content conditional debiasing for fair text embedding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8AaByYdbod": {
    "title": "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition",
    "volume": "review",
    "abstract": "Synthetic data is widely used in speech recognition due to the availability of text-to-speech models, which facilitate adapting models to previously unseen text domains. However, existing methods suffer in performance when they fine-tune an automatic speech recognition (ASR) model on synthetic data as they suffer from the distributional shift commonly referred to as the synthetic-to-real gap. In this paper, we find that task arithmetic is effective at mitigating this gap. Our proposed method, $SYN2REAL$ task vector, shows an average improvement of 10.03\\% improvement in word error rate over baselines on the SLURP dataset. Additionally, we show that an average of $SYN2REAL$ task vectors, when we have real speeches from multiple different domains, can further adapt the original ASR model to perform better on the target text domain",
    "checked": true,
    "id": "3d1b81c5bd340dcf9ad7abdd85119556885ed40d",
    "semantic_title": "task arithmetic can mitigate synthetic-to-real gap in automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=10KaRLc3zD": {
    "title": "Visual Expression for Referring Expression Segmentation",
    "volume": "review",
    "abstract": "Referring expression segmentation aims to segment a target object precisely in the image by referencing to a given linguistic expression. Since the network predicts based on the reference information that guides the network on which regions to pay attention, the capacity of this guidance information has a significant impact on the segmentation result. However, most existing methods rely on linguistic context-based tokens as the guidance elements, which are limited in providing the visual understanding of the fine-grained target regions. To address this issue, we propose a novel Multi-Expression Guidance framework for Referring Expression Segmentation, MERES, which enables the network to refer to the visual expression tokens as well as the linguistic expression tokens to complement the linguistic guidance capacity by effectively providing the visual contexts of the fine-grained target regions. To produce the semantic visual expression tokens, we introduce a visual expression extractor that adaptively selects the useful visual information relevant to the target regions from the image context and allows the visual expression to capture the richer visual contexts. The proposed module strengthens the adaptability to the diverse image and language inputs, and improves visual understanding of the target regions. Our method consistently shows strong performance on three public benchmarks, where it surpasses the existing state-of-the-art methods",
    "checked": false,
    "id": "03d07b12408a61d701944b6e3180ab4cc2a18b83",
    "semantic_title": "meta compositional referring expression segmentation",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=U4jKMqdW9N": {
    "title": "Entity Retrieval for Answering Entity-Centric Questions",
    "volume": "review",
    "abstract": "The similarity between the question and indexed documents is a crucial factor in document retrieval for retrieval-augmented question answering. Although this is typically the only method for obtaining the relevant documents, it is not the sole approach when dealing with entity-centric questions. In this study, we propose Entity Retrieval, a novel retrieval method which rather than relying on question-document similarity, depends on the salient entities within the question to identify the retrieval documents. We conduct an in-depth analysis of the performance of both dense and sparse retrieval methods in comparison to Entity Retrieval. Our findings reveal that our method not only leads to more accurate answers to entity-centric questions but also operates more efficiently",
    "checked": true,
    "id": "1465f3bb4ab048e3171499dafe79484068c76413",
    "semantic_title": "entity retrieval for answering entity-centric questions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=labT01N7sb": {
    "title": "MultiCAT: Multimodal Communication Annotations for Teams",
    "volume": "review",
    "abstract": "Successful teamwork requires team members to understand each other and communicate effectively, managing multiple linguistic and paralinguistic tasks at once. Because of the potential for interrelatedness of these tasks, it is important to have the ability to make multiple types of predictions on the same dataset. Here, we introduce Multimodal Communication Annotations for Teams (MultiCAT), a speech- and text-based dataset consisting of audio recordings, automated and hand-corrected transcriptions. MultiCAT builds upon data from teams working collaboratively to save victims in a simulated search and rescue mission, and consists of annotations and benchmark results for the following tasks: (1) dialog act classification, (2) adjacency pair detection, (3) sentiment and emotion recognition, (4) closed-loop communication detection, and (5) phonetic entrainment detection. We also present exploratory analyses on the relationship between our annotations and team outcomes. We posit that additional work on these tasks and their intersection will further improve understanding of team communication and its relation to team performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nS839JhspA": {
    "title": "TOOLVERIFIER: Generalization to New Tools via Self-Verification",
    "volume": "review",
    "abstract": "Teaching language models to use tools is an important milestone towards building general assistants, but remains an open problem. While there has been significant progress on learning to use specific tools via fine-tuning, language models still struggle with learning how to robustly use new tools from only a few demonstrations. In this work we introduce a self-verification method which distinguishes between close candidates by self-asking contrastive questions during (1) tool selection; and parameter generation. We construct synthetic, high-quality, self-generated data for this goal using Llama-2 70B, which we intend to release publicly. Extensive experiments on 4 tasks from the ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average improvement of 22% over few-shot baselines, even in scenarios where the distinctions between candidate tools are finely nuanced",
    "checked": true,
    "id": "57cbb0578326ed792d03981ca701214844462d22",
    "semantic_title": "toolverifier: generalization to new tools via self-verification",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=rGAi3RUlgl": {
    "title": "Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation",
    "volume": "review",
    "abstract": "Speculative decoding stands as a pivotal technique to expedite inference in autoregressive (large) language models. This method employs a smaller \\textit{draft} model to speculate a block of tokens, which the \\textit{target} model then evaluates for acceptance. Despite a wealth of studies aimed at increasing the efficiency of speculative decoding, the influence of generation configurations on the decoding process remains poorly understood, especially concerning decoding temperatures. This paper delves into the effects of decoding temperatures on speculative decoding's efficacy. Beginning with knowledge distillation (KD), we first highlight the challenge of decoding at higher temperatures, and demonstrate KD in a consistent temperature setting could be a remedy. We also investigate the effects of out-of-domain testing sets with out-of-range temperatures. Building upon these findings, we take an initial step to further the speedup for speculative decoding, particularly in a high-temperature generation setting. Our work offers new insights into how generation configurations drastically affect the performance of speculative decoding, and underscores the need for developing methods that focus on diverse decoding configurations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mtao3Gj7Mh": {
    "title": "Evaluating Knowledge-based Cross-lingual Inconsistency in Large Language Models",
    "volume": "review",
    "abstract": "This paper investigates the cross-lingual inconsistencies observed in Large Language Models (LLMs), such as ChatGPT, Llama, and Baichuan, which have shown exceptional performance in various Natural Language Processing (NLP) tasks. Despite their successes, these models often exhibit significant inconsistencies when processing the same concepts across different languages. This study focuses on three primary questions: the existence of cross-lingual inconsistencies in LLMs, the specific aspects in which these inconsistencies manifest, and the correlation between cross-lingual consistency and multilingual capabilities of LLMs.To address these questions, we propose an innovative evaluation method for Cross-lingual Semantic Consistency (xSC) using the LaBSE model. We further introduce metrics for Cross-lingual Accuracy Consistency (xAC) and Cross-lingual Timeliness Consistency (xTC) to comprehensively assess the models' performance regarding semantic, accuracy, and timeliness inconsistencies. By harmonizing these metrics, we provide a holistic measurement of LLMs' cross-lingual consistency. Our findings aim to enhance the understanding and improvement of multilingual capabilities and interpretability in LLMs, contributing to the development of more robust and reliable multilingual language models",
    "checked": true,
    "id": "f5bbe4ca8d03e230e49214a70d015b274b8c0442",
    "semantic_title": "evaluating knowledge-based cross-lingual inconsistency in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bo1XPDF2ob": {
    "title": "SCOI: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation",
    "volume": "review",
    "abstract": "In-context learning (ICL) greatly improves the performance of large language models (LLMs) on various down-stream tasks, where the improvement highly depends on the quality of demonstrations. In this work, we introduce syntactic knowledge to select better in-context examples for machine translation (MT). We propose a new strategy, namely **S**yntax-augmented **CO**verage-based **I**n-context example selection (SCOI), leveraging the deep syntactic structure beyond conventional word matching. Specifically, we measure the set-level syntactic coverage by computing the coverage of polynomial terms with the help of a simplified tree-to-polynomial algorithm, and lexical coverage using word overlap. Furthermore, we devise an alternate selection approach to combine both coverage measures, taking advantage of syntactic and lexical information. We conduct experiments with two multi-lingual LLMs on six translation directions. Empirical results show that our proposed SCOI obtains the highest average COMET score among all learning-free methods, indicating that combining syntactic and lexical coverage successfully helps to select better in-context examples for MT",
    "checked": true,
    "id": "7533a8aecce8c31b740c81471df7f5a4332c32d3",
    "semantic_title": "scoi: syntax-augmented coverage-based in-context example selection for machine translation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=DHKEQCBP9f": {
    "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
    "volume": "review",
    "abstract": "Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers. In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, **Video-LLaVA**, which learns from a mixed dataset of images and videos, mutually enhancing each other. As a result, Video-LLaVA outperforms Video-ChatGPT by **5.8%, 9.9%, 18.6%, and 10.1%** on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, our Video-LLaVA also achieves superior performances on a broad range of 9 image benchmarks. Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM",
    "checked": true,
    "id": "107fb6eec2febbae12db29bf3e311aaf5680027c",
    "semantic_title": "video-llava: learning united visual representation by alignment before projection",
    "citation_count": 165,
    "authors": []
  },
  "https://openreview.net/forum?id=VAXwQqkp5e": {
    "title": "MalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) exhibit advanced proficiency in language reasoning and comprehension across a wide array of languages. While their performance is notably robust in well-resourced languages, the capabilities of LLMs in low-resource languages, such as Bahasa Malaysia (hereinafter referred to as Malay), remain less explored due to a scarcity of dedicated studies and benchmarks. To enhance our understanding of LLMs' performance in Malay, we introduce the first multi-task language understanding benchmark specifically for this language, named MalayMMLU. This benchmark comprises 24,213 questions spanning both primary (Year 1-6) and secondary (Form 1-5) education levels in Malaysia, encompassing 5 broad topics that further divide into 22 subjects. We conducted an empirical evaluation of 18 LLMs, assessing their proficiency in both Malay and the nuanced contexts of Malaysian culture using this benchmark. We will release the MalayMMLU benchmark and the corresponding code publicly upon paper acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2OB76SHUGU": {
    "title": "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "Retrieval-augmented language models (RALMs) have shown strong performance and wide applicability in knowledge-intensive tasks. However, there are significant trustworthiness concerns as RALMs are prone to generating unfaithful outputs, including baseless information or contradictions with the retrieved context. This paper proposes SynCheck, a lightweight monitor that leverages fine-grained decoding dynamics including sequence likelihood, uncertainty quantification, context influence, and semantic alignment to synchronously detect unfaithful sentences. By integrating efficiently measurable and complementary signals, SynCheck enables accurate and immediate feedback and intervention. Experiments show that SynCheck significantly outperforms existing faithfulness detection baselines, achieving over 0.85 AUROC across a suite of six long-form retrieval-augmented generation tasks. Leveraging SynCheck, we further introduce FOD, a faithfulness-oriented decoding algorithm guided by beam search for long-form retrieval-augmented generation. Empirical results demonstrate that FOD outperforms traditional strategies such as abstention, reranking, or contrastive decoding significantly in terms of faithfulness, achieving over 10% improvement across six datasets",
    "checked": true,
    "id": "fa49021ce42229385f39bf5e1a2dcff48c2e2157",
    "semantic_title": "synchronous faithfulness monitoring for trustworthy retrieval-augmented generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s7OZoZCOJd": {
    "title": "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents",
    "volume": "review",
    "abstract": "Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation. Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct large-scale user simulations for the web search scenario to improve the analysis and modeling of user search behavior. Specially, we propose BASES, a novel user simulation framework with LLM-based agents, designed to facilitate comprehensive simulations of web search user behaviors. Our simulation framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors. To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human benchmarks in both Chinese and English, demonstrating that BASES can effectively simulate large-scale human-like search behaviors. To further accommodate the research on web search, we develop WARRIORS, a new large-scale dataset encompassing web search user behaviors, including both Chinese and English versions, which can greatly bolster research in the field of information retrieval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IRJZADcLM0": {
    "title": "A Dataset for Evaluating LLM-based Evaluation Functions for Research Question Extraction Task",
    "volume": "review",
    "abstract": "The progress in text summarization techniques has been remarkable. However the task of accurately extracting and summarizing necessary information from highly specialized documents such as research papers has not been sufficiently investigated. We are focusing on the task of extracting research questions (RQ) from research papers and construct a new dataset consisting of machine learning papers, RQ extracted from these papers by GPT-4, and human evaluations of the extracted RQ from multiple perspectives. Using this dataset, we systematically compared recently proposed LLM-based evaluation functions for summarizations, and found that none of the functions showed sufficiently high correlations with human evaluations. We expect our dataset provides a foundation for further research on developing better evaluation functions tailored to the RQ extraction task, and contribute to enhance the performance of the task. The dataset is available at https://anonymous.4open.science/r/PaperRQ-HumanAnno-Dataset-8473/README.md",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oYQ8la668U": {
    "title": "Towards Situated Bias Evaluations in LLM Alignment",
    "volume": "review",
    "abstract": "The global adoption of chat-based large language models (LLMs) necessitates ensuring their inclusivity across diverse sociocultural contexts. Despite efforts to align these models with human preferences, it remains uncertain whether such alignment may amplify pre-existing social biases. Current bias evaluation frameworks are limited to narrow, hegemonic social contexts, such as binary gender biases in occupational associations, overlooking the diverse range of harms affecting marginalized communities. In this paper, we investigate aligned LLMs for biases across underrepresented evaluation dimensions such as gender-diverse representation and multilingual accessibility. Through a comprehensive evaluation of 12 models, we uncover several key findings: (1) gender-diverse disparities persist after alignment and can be measured both in extrinsic model output and intrinsic reward analysis (2) aligned models reflect linguistic norms which favor higher-resourced languages, potentially disadvantaging lower-resource languages. Our findings highlight the need for more comprehensive bias evaluation frameworks formed in dialogue with diverse sociocultural contexts",
    "checked": false,
    "id": "34214e1de58c6b816263e2207c77ff65f093b967",
    "semantic_title": "towards region-aware bias evaluation metrics",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lbP0VhiK94": {
    "title": "Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction",
    "volume": "review",
    "abstract": "Given the accelerating progress of vision and language modeling, accurate evaluation of machine-generated image captions remains critical. In order to evaluate captions more closely to human preferences, metrics need to discriminate between captions of varying quality and content. However, conventional metrics fall short of comparing beyond superficial matches of words or embedding similarities; thus, they still need improvement. This paper presents VisCE2, a vision language model-based caption evaluation method. Our method focuses on visual context, which refers to the detailed content of images, including objects, attributes, and relationships. By extracting and organizing them into a structured format, we replace the human-written references with visual contexts and help VLMs better understand the image, enhancing evaluation performance. Through meta-evaluation on multiple datasets, we validated that VisCE2 outperforms the conventional pre-trained metrics in capturing caption quality and demonstrates superior consistency with human judgment",
    "checked": true,
    "id": "4d0302276d4491fca50f3429f57c3134b7805f0e",
    "semantic_title": "vision language model-based caption evaluation method leveraging visual context extraction",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=GJYZPcVnGe": {
    "title": "Refiner : Restructure Retrieved Content Efficiently to Advance Question-Answering Capabilities",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are limited by their parametric knowledge, leading to hallucinations in knowledge-extensive tasks. To address this, Retrieval-Augmented Generation (RAG) incorporates external document chunks to expand LLM knowledge. Furthermore, compressing information from document chunks through extraction or summarization can improve LLM performance. Nonetheless, LLMs still struggle to notice and utilize scattered key information, a problem known as the \"lost-in-the-middle\" syndrome. Therefore, we typically need to restructure the content for LLM to recognize the key information. We propose $\\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that operates in the post-retrieval process of RAG. $\\textit{Refiner}$ leverages a single decoder-only LLM to adaptively extract query-relevant contents verbatim along with the necessary context, and section them based on their interconnectedness, thereby highlights information distinction, and aligns downstream LLMs with the original context effectively. Experiments show that a trained $\\textit{Refiner}$ (with 7B parameters) exhibits significant gain to downstream LLM in improving answer accuracy, and outperforms other state-of-the-art advanced RAG and concurrent compressing approaches in various single-hop and multi-hop QA tasks. Notably, $\\textit{Refiner}$ achieves a 80.5% tokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared to the next best solution. $\\textit{Refiner}$ is a plug-and-play solution that can be seamlessly integrated with RAG systems, facilitating its application across diverse open-source frameworks",
    "checked": false,
    "id": "0513d9a10c0c06caaad30f68bf5051884f5a2327",
    "semantic_title": "refiner: restructure retrieval content efficiently to advance question-answering capabilities",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QYiZAYbDtE": {
    "title": "Dynamic Planning for LLM-based Graphical User Interface Automation",
    "volume": "review",
    "abstract": "The advent of large language models (LLMs) has spurred considerable interest in advancing autonomous LLMs-based agents, particularly in intriguing applications within smartphone graphical user interfaces (GUIs). When presented with a task goal, these agents typically emulate human actions within a GUI environment until the task is completed. However, a key challenge lies in devising effective plans to guide action prediction in GUI tasks, though planning have been widely recognized as effective for decomposing complex tasks into a series of steps. Specifically, given the dynamic nature of environmental GUIs following action execution, it is crucial to dynamically adapt plans based on environmental feedback and action history.We show that the widely-used ReAct approach fails due to the excessively long historical dialogues. To address this challenge, we propose a novel approach called Dynamic Planning of Thoughts (D-PoT) for LLM-based GUI agents.D-PoT involves the dynamic adjustment of planning based on the environmental feedback and execution history. Experimental results reveal that the proposed D-PoT significantly surpassed the strong GPT-4V baseline by +12.7\\% (34.66\\% $\\rightarrow$ 47.36\\%) in accuracy. The analysis highlights the generality of dynamic planning in different backbone LLMs, as well as the benefits in mitigating hallucinations and adapting to unseen tasks. Code is available at Anonymous",
    "checked": false,
    "id": "d21a1fbeb53db40f9dc641e1b2c48cbaf36def08",
    "semantic_title": "autonode: a neuro-graphic self-learnable engine for cognitive gui automation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FOP5TCrZ4S": {
    "title": "MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context",
    "volume": "review",
    "abstract": "Large Vision Language Models (LVLMs) have recently achieved superior performance in various tasks on natural image and text data, which inspires a large amount of studies for LVLMs fine-tuning and training. Despite their advancements, there has been scant research on the robustness of these models against hallucination when fine-tuned on smaller datasets. In this study, we introduce a new benchmark dataset, the Medical Visual Hallucination Test (MedVH), to evaluate the hallucination of domain-specific LVLMs. MedVH comprises five tasks to evaluate hallucinations in LVLMs within the medical context, which includes tasks for comprehensive understanding of textual and visual input, as well as long textual response generation. Our extensive experiments with both general and medical LVLMs reveal that, although medical LVLMs demonstrate promising performance on standard medical tasks, they are particularly susceptible to hallucinations, often more so than the general models, raising significant concerns about the reliability of these domain-specific models. For medical LVLMs to be truly valuable in real-world applications, they must not only accurately integrate medical knowledge but also maintain robust reasoning abilities to prevent hallucination. Our work paves the way for future evaluations of these studies",
    "checked": true,
    "id": "bff123171a5b7bddd699a72daed96b4c56742069",
    "semantic_title": "medvh: towards systematic evaluation of hallucination for large vision language models in the medical context",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2ScFDlBwgn": {
    "title": "Beneath the tip of the melting iceberg: uncovering deep narrative structures in climate change news",
    "volume": "review",
    "abstract": "Decades of climate change discourse has shaped a landscape of impactful narratives that conceptualize aspects of the issue, spark action, or advance denialist claims. While the impact of stories has been widely acknowledged, their reliable formalization, annotation, and large-scale prediction remained elusive in both the social sciences and NLP. Here, we introduce a functional definition of narratives into NLP and present a modular taxonomy of narratives. We annotate and release a data set of 100 articles, and show how our framework significantly improves narrative prediction with LLM compared to less structured prompts. Our work lays the foundation for larger-scale investigation into the emergence and effects of narratives in climate change discourse",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U5kVVgcSif": {
    "title": "CharacterQA: A Corpus for Multimodal Character Conversational Movie Question Answering",
    "volume": "review",
    "abstract": "The rapid advancement of Large Language Models has sparked extensive exploration of their applications across various fields. Among them, the personalized conversation based on characters in movies is an attractive research area. To achieve such comprehensive conversations, the integration of extensive multimodal information, notably visual content alongside textual data, is crucial. This necessity underlines the significance of multimodal insights for enriching personalized conversations, thereby further emphasizing the urgent need for a sophisticated multimodal character conversational dataset. To this end, we introduce CharacterQA, a novel video question-answering (QA) dataset for multimodal character conversation in movies. The dataset consists of 101 selected Chinese movies, each of which is annotated with the main character profiles, the character information of the scripted conversations and their timestamps. Furthermore, a set of questions from various designed tasks and their detailed answers are annotated. Most of those questions require taking into account visual signals for logical comprehension of movie characters and plots. Subsequently, we adopt an advanced multimodal large language model MovieGPT to evaluate the CharacterQA dataset. The results yield insightful findings that are expected to drive further development of multimodal large language models in the character conversation field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NyMYyFRq6j": {
    "title": "Iterative Utility Judgment Framework via LLMs Inspired by Relevance in Philosophy",
    "volume": "review",
    "abstract": "Utility and topical relevance are critical measures in information retrieval (IR), reflecting system and user perspectives, respectively. While topic relevance has long been emphasized, utility is a higher standard of relevance and is more useful for facilitating downstream tasks, e.g., in Retrieval-Augmented Generation (RAG). When we incorporate utility judgments into RAG, we realize that the topical relevance, utility, and answering in RAG are closely related to the three types of relevance that Schutz discussed from a philosophical perspective. They are topical relevance, interpretational relevance, and motivational relevance, respectively. Inspired by the dynamic iterations of the three types of relevance, we propose an Iterative utiliTy judgmEnt fraMework (\\modelname) to promote each step of the cycle of RAG. We conducted extensive experiments on multi-grade passage retrieval and factoid question-answering datasets (i.e., TREC DL, WebAP, and NQ). Experimental results demonstrate significant improvements in utility judgments, ranking of topical relevance, and answer generation upon representative baselines, including multiple single-shot utility judging approaches. Our code and benchmark can be found at https://anonymous.4open.science/r/ITEM-B486/",
    "checked": true,
    "id": "89cd443218c776f6d7a12088235d5dcb6ae7e433",
    "semantic_title": "iterative utility judgment framework via llms inspired by relevance in philosophy",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oHJ2Kh6hkR": {
    "title": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model",
    "volume": "review",
    "abstract": "Large language models, initially pre-trained with a limited context length, can better handle longer texts by continuing training on a corpus with extended contexts. However, obtaining effective long-context data is challenging due to the scarcity and uneven distribution of long documents across different domains. To address this issue, we propose a Query-centric data synthesis method, abbreviated as Quest. Quest is an interpretable method based on the observation that documents retrieved by similar queries are relevant but low-redundant, thus well-suited for synthesizing long-context data. The method is also scalable and capable of constructing large amounts of long-context data. Using Quest, we synthesize a long-context dataset up to 128k context length, significantly outperforming other data synthesis methods on multiple long-context benchmark datasets. In addition, we further verify that the Quest method is predictable through scaling law experiments, making it a reliable solution for advancing long-context models",
    "checked": true,
    "id": "bfe1057e5bdd2ed6d7b5564cf644a7a11fb8387a",
    "semantic_title": "quest: query-centric data synthesis approach for long-context scaling of large language model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dVmoLDLHL6": {
    "title": "Towards Efficient Large Language Models for Science: A Review",
    "volume": "review",
    "abstract": "Large language models (LLMs) have ushered in a new era for processing complex information in various fields, including science. The increasing amount of scientific literature allows these models to acquire and understand scientific knowledge effectively, thus improving their performance in a wide range of tasks. Due to the power of LLMs, they require extremely expensive computational resources, intense amounts of data, and training time. Therefore, in recent years, researchers have proposed various methodologies to make scientific LLMs more affordable. The most well-known approaches align in two directions. It can be either focusing on the size of the models or enhancing the quality of data. To date, a comprehensive review of these two families of methods has not yet been undertaken. In this paper, we (I) summarize the current advances in the emerging abilities of LLMs into more accessible AI solutions for science, and (II) investigate the challenges and opportunities of developing affordable solutions using LLMs",
    "checked": false,
    "id": "053a8e32edfc6b9c2e57e6769084697d7b6d1e42",
    "semantic_title": "towards efficient large language models for scientific text: a review",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EBGBMf1PkF": {
    "title": "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning",
    "volume": "review",
    "abstract": "In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we design a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning demonstration prompts, which can make models behave in alignment with predefined intentions. ICLAttack does not require additional fine-tuning to implant a backdoor, thus preserving the model's generality. Furthermore, the poisoned examples are correctly labeled, enhancing the natural stealth of our attack method. Extensive experimental results across several language models, ranging in size from 1.3B to 180B parameters, demonstrate the effectiveness of our attack method, exemplified by a high average attack success rate of 95.0% across the three datasets on OPT models",
    "checked": true,
    "id": "eb16eae728f54962992e6115c5dcd0df3be28c89",
    "semantic_title": "universal vulnerabilities in large language models: backdoor attacks for in-context learning",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=f3jw8jn039": {
    "title": "Learning to Write Rationally: How Information Is Distributed in Non-native Speakers' Essays",
    "volume": "review",
    "abstract": "People tend to distribute information evenly in language production for better and clearer communication. In this study, we compared essays written by second language (L2) learners with various native language (L1) backgrounds to investigate how they distribute information in their non-native L2 production. Analyses of surprisal and constancy of entropy rate indicated that writers with higher L2 proficiency can reduce the expected uncertainty of language production while still conveying informative content. However, the uniformity of information distribution showed less variability among different groups of L2 speakers, suggesting that this feature may be universal in L2 essay writing and less affected by L2 writers' variability in L1 background and L2 proficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7yXmHHAQZp": {
    "title": "Memory Sharing for Large Language Model based Agents",
    "volume": "review",
    "abstract": "The adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning, but are constrained by the comprehensiveness and diversity of the provided examples, leading to outputs that often diverge significantly from expected results, especially when it comes to the open-ended questions. This paper introduces the Memory Sharing, a framework which integrates the real-time memory filter, storage and retrieval to enhance the In-Context Learning process. This framework allows for the sharing of memories among multiple agents, whereby the interactions and shared memories between different agents effectively enhance the diversity of the memories. The collective self-enhancement through interactive learning among multiple agents facilitates the evolution from individual intelligence to collective intelligence. Besides, the dynamically growing memory pool is utilized not only to improve the quality of responses but also to train and enhance the retriever. We evaluated our framework across three distinct domains involving specialized tasks of agents. The experimental results demonstrate that the MS framework significantly improves the agents' performance in addressing open-ended questions",
    "checked": true,
    "id": "212318b81f99a7dc83929b4fea679b096cdf513d",
    "semantic_title": "memory sharing for large language model based agents",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Ec7guJsZpQ": {
    "title": "Probing API Name Knowledge in Pre-trained Code Models",
    "volume": "review",
    "abstract": "Recent advances in pre-trained code models, like CodeBERT and Codex, have demonstrated remarkable performance across diverse tasks. However, the accurate and clear use of APIs is vital for optimal program functionality, necessitating a deep understanding of API fully qualified names both structurally and semantically. Despite their prowess, current models often falter in suggesting appropriate APIs during code generation, with the underlying reasons remaining largely unexplored. To bridge this gap, we leverage the knowledge probing technique and employ cloze-style tests to gauge the knowledge embedded within these models. Our in-depth analysis assesses a model's grasp of API fully qualified names from two angles: API call and API import. The results shed light on the strengths and weaknesses of existing pre-trained code models. We posit that integrating API structure during pre-training can enhance API usage and code representation. This research aims to steer the evolution of code intelligence and set the course for subsequent investigations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IeofzBIx62": {
    "title": "Reusing Transferable Weight Increments for Low-resource Style Generation",
    "volume": "review",
    "abstract": "Text style transfer (TST) is crucial in natural language processing, aiming to endow text with a new style without altering its meaning. In real-world scenarios, not all styles have abundant resources. This work introduces TWIST (reusing Transferable Weight Increments for Style Text generation), a novel framework to mitigate data scarcity by utilizing style features in weight increments to transfer low-resource styles effectively. During target style learning, we derive knowledge via a specially designed weight pool and initialize the parameters for the unseen style. To enhance the effectiveness of merging, the target style weight increments are often merged from multiple source style weight increments through singular vectors. Considering the diversity of styles, we also designed a multi-key memory network that simultaneously focuses on task- and instance-level information to derive the most relevant weight increments. Results from multiple style transfer datasets show that TWIST demonstrates remarkable performance across different backbones, achieving particularly effective results in low-resource scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qizgdzpc8m": {
    "title": "A Survey on Detection of LLMs-Generated Content",
    "volume": "review",
    "abstract": "The burgeoning capabilities of advanced large language models (LLMs) such as ChatGPT have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. As such, the ability to detect LLMs-generated content has become of paramount importance. We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. We also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs. To the best of our knowledge, this work is the first comprehensive survey on the detection in the era of LLMs. We hope it will provide a broad understanding of the current landscape of LLMs-generated content detection, and we have maintained a website to consistently update the latest research as a guiding reference for researchers and practitioners",
    "checked": true,
    "id": "60bd26bdb23ba353e5d79f161542dd074bc8391c",
    "semantic_title": "a survey on detection of llms-generated content",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=6qPs9Y75iT": {
    "title": "VITS-Based Data Augmentation for Improved ASR Performance and Domain Adaptation",
    "volume": "review",
    "abstract": "Although significant advancements have been made in end-to-end speech recognition, it still remains a challenging task when dealing with low-resource scenarios, even with the utilization of traditional data augmentation methods. Recent technological progress, demonstrated by the success of VITS and its variations, has spurred interest in exploring Text-to-Speech (TTS) synthesis for data augmentation to address the aforementioned difficulties. In this study, we investigate the effectiveness of integrating synthetic speech generated by VITS into the train sets of ASR systems. Through comprehensive experiments, we assess the impact of this approach on improving the generalization and performance of ASR models in English, Mandarin, and Japanese. Experimental results indicate that the average character-level accuracy of the VITS-based data augmentation method matches the best performance observed among traditional data augmentation methods before model transfer. After model transfer, the average character-level accuracy of the VITS-based data augmentation method significantly outperforms all traditional methods, surpassing Speed Perturbation, the best-performing traditional method, by 3.5%, as well as Tacotron2 and Fastspeech. Our findings indicate that models trained with the VITS-based data augmentation method exhibit enhanced resilience towards domain shift challenges, demonstrating improved adaptability across varied linguistic contexts, thus highlighting the potential of VITS as a valuable data augmentation technique",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ilFPS7BOpe": {
    "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding",
    "volume": "review",
    "abstract": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV) caching, but can lead to major memory bottlenecks as model size, batch size, and sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV) sharing, a novel approach extending KV sharing across transformer layers to reduce memory usage beyond what was possible with Multi-Query Attention (MQA) and Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and inference metrics using uptrained Pythia-160M variants demonstrate that MLKV significantly reduces memory usage with minimal performance loss, reducing KV cache size down to a factor of 6x compared to MQA. These results highlight MLKV's potential for efficient deployment of transformer models at scale",
    "checked": true,
    "id": "f7e76d1a952f8120ca2f2afbb419dd849d25228a",
    "semantic_title": "mlkv: multi-layer key-value heads for memory efficient transformer decoding",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=NWXn9iPZ7r": {
    "title": "Knowledge-Centric Templatic Views of Documents",
    "volume": "review",
    "abstract": "Authors seeking to communicate with broader audiences often share their ideas in various document formats, such as slide decks, newsletters, reports, and posters. Prior work on document generation has generally tackled the creation of each separate format to be a different task, leading to fragmented learning processes, redundancy in models and methods, and disjointed evaluation. We consider each of these documents as templatic views of the same underlying knowledge/content, and we aim to unify the generation and evaluation of these templatic views. We begin by showing that current LLMs are capable of generating various document formats with little to no supervision. Further, a simple augmentation involving a structured intermediate representation can improve performance, especially for smaller models. We then introduce a novel unified evaluation framework that can be adapted to measuring the quality of document generators for heterogeneous downstream applications. This evaluation is adaptable to a range of user defined criteria and application scenarios, obviating the need for task specific evaluation metrics. Finally, we conduct a human evaluation, which shows that people prefer 82\\% of the documents generated with our method, while correlating more highly with our unified evaluation framework than prior metrics in the literature",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LXKPEUUdkL": {
    "title": "Bridging Local Details and Global Context in Text-Attributed Graphs",
    "volume": "review",
    "abstract": "Representation learning on text-attributed graphs (TAGs) is vital for real-world applications, as they combine semantic textual and contextual structural information. Research in this field generally consist of two main perspectives: local-level encoding and global-level aggregating, respectively refer to textual node information unification ($e.g.$, using Language Models) and structure-augmented modeling ($e.g.$, using Graph Neural Networks). Most existing works focus on combining different information levels but overlook the interconnections, $i.e.$, the contextual textual information among nodes, which provides semantic insights to bridge local and global levels. In this paper, we propose GraphBridge, a $multi-granularity integration$ framework that bridges local and global perspectives by leveraging contextual textual information, enhancing fine-grained understanding of TAGs. Besides, to tackle scalability and efficiency challenges, we introduce a graph-aware token reduction module. Extensive experiments across various models and datasets show that our method achieves state-of-the-art performance, while our graph-aware token reduction module significantly enhances efficiency and solves scalability issues. Codes are available at https://anonymous.4open.science/r/GraphBridge-13E0",
    "checked": true,
    "id": "4a7550c486ba19ddcb23c2df38ceaaba8d69cacc",
    "semantic_title": "bridging local details and global context in text-attributed graphs",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=hFoLMcsCml": {
    "title": "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning",
    "volume": "review",
    "abstract": "Multi-step reasoning is widely adopted in the community to explore the better performance of language models (LMs). We report on the systematic strategy that LMs use in this process. Our controlled experiments reveal that LMs rely more heavily on heuristics, such as lexical overlap, in the earlier stages of reasoning when more steps are required to reach an answer. Conversely, as LMs progress closer to the final answer, their reliance on heuristics decreases. This suggests that LMs track only a limited number of future steps and dynamically combine heuristic strategies with logical ones in tasks involving multi-step reasoning",
    "checked": true,
    "id": "9289bc1f0c4578c1b819f95edd91161d5169e893",
    "semantic_title": "first heuristic then rational: dynamic use of heuristics in language model reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mbUHHpmdm4": {
    "title": "Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing",
    "volume": "review",
    "abstract": "In this paper, we address the data scarcity problem in automatic data-driven glossing for low-resource languages by coordinating multiple sources of linguistic expertise. We supplement models with translations at both the token and sentence level as well as leverage the extensive linguistic capability of modern LLMs. Our enhancements lead to an average absolute improvement of 5%-points in word-level accuracy over the previous state of the art on a typologically diverse dataset spanning six low-resource languages. The improvements are particularly noticeable for the lowest-resourced language Gitksan, where we achieve a 10%-point improvement. Furthermore, in a simulated ultra-low resource setting for the same six languages, training on fewer than 100 glossed sentences, we establish an average 10%-point improvement in word-level accuracy over the previous state-of-the-art system",
    "checked": true,
    "id": "4991cbd4fe71769ce0a74b27715488124560b5b5",
    "semantic_title": "multiple sources are better than one: incorporating external knowledge in low-resource glossing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Glu9bfQxN6": {
    "title": "Scaling Sentence Embeddings with Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have recently gained significant interest due to their impressive results in various natural language tasks. However, their application to sentence embeddings is still under active research. In this work, we introduce PromptEOL, a simple and efficient method designed to enhance LLM performance on sentence embeddings with a one-word limitation. We further integrate PromptEOL with in-context learning and alignment to leverage LLMs in two settings: without fine-tuning and with fine-tuning. Our extensive experiments show that PromptEOL enables LLMs to generate superior sentence embeddings without fine-tuning, outperforming contrastive learning methods. Additionally, with fine-tuning, a 2.7B parameter model using PromptEOL surpasses the performance of a 4.8B parameter model from previous methods. We also analyze how scaling model parameters, from 125 million to 66 billion, impacts sentence embedding performance",
    "checked": true,
    "id": "f7ccf8ecd508e0b2d423169588dd1c1a82dd3b4d",
    "semantic_title": "scaling sentence embeddings with large language models",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=fOMfNIF334": {
    "title": "MOYU: Massive Over-activation Yielded Uplifts in LLMs",
    "volume": "review",
    "abstract": "Massive Over-activation Yielded Uplifts(MOYU) is the inherent properties of large language models and dynamic activation (DA) based on MOYU property is a clever but under-explored method designed to accelerate inference in large language models. Existing approaches to utilize MOYU typically face at least one major drawback, whether in maintaining model performance, enhancing inference speed, or broadening applicability across different architectures. This paper introduces two Sequential DA methods called sTDA and sRIDA that leverage sequence information while utilizing MOYU property, effectively overcome the \"impossible triangle\" that bothers current DA approaches. Our two schemes have improved generation speeds by 20-25\\% without significantly compromising the model's task performance. Additionally, given the blur of theoretical studies of MOYU, this paper also explains its root cause, then outlines the mechanisms of two main limitations (i.e. history-related activation uncertainty and semantic-irrelevant activation inertia) faced by existing DA methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MQ78Fhhqs3": {
    "title": "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation",
    "volume": "review",
    "abstract": "Automatically generated questions often suffer from problems such as unclear expression or factual inaccuracies, requiring a reliable and comprehensive evaluation of their quality. Human evaluation is widely used in the field of question generation (QG) and serves as the gold standard for automatic metrics. However, there is a lack of unified human evaluation criteria, which hampers consistent and reliable evaluations of both QG models and automatic metrics. To address this, we propose QGEval, a multi-dimensional Evaluation benchmark for Question Generation, which evaluates both generated questions and existing automatic metrics across 7 dimensions: fluency, clarity, conciseness, relevance, consistency, answerability, and answer consistency. We demonstrate the appropriateness of these dimensions by examining their correlations and distinctions. Through consistent evaluations of QG models and automatic metrics with QGEval, we find that 1) most QG models perform unsatisfactorily in terms of answerability and answer consistency, and 2) existing metrics fail to align well with human judgments when evaluating generated questions across the 7 dimensions. We expect this work to foster the development of both QG technologies and their evaluation",
    "checked": false,
    "id": "038a83d2fb6a0654c95468db967f2814a2b314f8",
    "semantic_title": "qgeval: a benchmark for question generation evaluation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Fr2uyKwFD": {
    "title": "Enhancing Advanced Visual Reasoning Ability of Large Language Models",
    "volume": "review",
    "abstract": "Recent advancements in Vision-Language (VL) research have sparked new benchmarks for complex visual reasoning, challenging models' advanced reasoning ability. Traditional Vision-Language models (VLMs) perform well in visual perception tasks but struggle with complex reasoning scenarios. Conversely, Large Language Models (LLMs) demonstrate robust text reasoning capabilities but lack visual acuity. To bridge this gap, we propose $\\textbf{C}$omplex $\\textbf{V}$isual $\\textbf{R}$easoning $\\textbf{L}$arge $\\textbf{L}$anguage $\\textbf{M}$odels (CVR-LLM), capitalizing on VLMs' visual perception proficiency and LLMs' extensive reasoning capability. Unlike recent multimodal large language models (MLLMs) that require a projection layer, our approach transforms images into detailed, context-aware descriptions using an iterative self-refinement loop and leverages LLMs' text knowledge for accurate predictions without extra training. We also introduce a novel multi-modal in-context learning (ICL) methodology to enhance LLMs' contextual understanding and reasoning. Additionally, we introduce Chain-of-Comparison (CoC), a step-by-step comparison technique enabling contrasting various aspects of predictions. Our CVR-LLM presents the first comprehensive study across a wide array of complex visual reasoning tasks and achieves SOTA performance among all",
    "checked": false,
    "id": "2dc51609829369f773ca6b19fd2718994a31abbc",
    "semantic_title": "empowering vision-language models for reasoning ability through large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=UPLoeVTPGj": {
    "title": "Revealing the Parallel Multilingual Learning within Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) can handle multilingual and cross-lingual text within a single input; however, previous works leveraging multilingualism in LLMs primarily focus on using English as the pivot language to enhance language understanding and reasoning. Given that multiple languages are a compensation for the losses caused by a single language's limitations, it's a natural next step to enrich the model's learning context through the integration of the original input with its multiple translations. In this paper, we start by revealing that LLMs learn from $\\textbf{P}$arallel $\\textbf{M}$ultilingual $\\textbf{I}$nput ($\\textbf{PMI}$). Our comprehensive evaluation shows that PMI enhances the model's comprehension of the input, achieving superior performance than conventional in-context learning (ICL). Furthermore, to explore how multilingual processing affects prediction, we examine the activated neurons in LLMs. Surprisingly, involving more languages in the input activates fewer neurons, leading to more focused and effective neural activation patterns. Also, this neural reaction coincidently mirrors the neuroscience insight about synaptic pruning, highlighting a similarity between artificial and biological `brains'",
    "checked": false,
    "id": "932156355ff5bba8fb8d75faef6ca36a1b91f575",
    "semantic_title": "youtube-sl-25: a large-scale, open-domain multilingual sign language parallel corpus",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=gRg1JeLte2": {
    "title": "Implicit Visual Knowledge Enhanced Training-free Image Captioning",
    "volume": "review",
    "abstract": "Training-free Image Captioning (IC) aims to generate image-related descriptions without any training process. Benefiting from the zero-shot representation capability of CLIP, existing methods can flexibly generate image-related captions by constraining the output word distribution of the image captioner (i.e., training-free Language Models (LMs)). Currently, the primary concerns center on enhancing caption diversity, in addition to ensuring their relevance to the corresponding images (i.e., accuracy). These methods encounter a significant performance bottleneck stemming from the captioner's limited ability to comprehend the images, which arises from \\textit{the disconnect between CLIP's visual representation and the linguistic understanding of LMs}. To address this issue, we propose a simple yet effective strategy: introducing additional visual understanding to assist the captioner in comprehending images, achieving the effect where $1+1>2$. Specifically, we introduce an extra LM to extract implicit visual knowledge from the CLIP's visual representation. The obtained implicit knowledge can provide more comprehensive visual understanding for the captioner, resulting in more diverse and accurate captions. Extensive experiment results demonstrate the superiority of our proposed framework to SOTA methods on various metrics. Especially, our proposed model achieves diversity performance improvements of 15.9\\% and 7.6\\% in both sequential and shuffle settings, respectively. The code will be released once the paper is accepted",
    "checked": false,
    "id": "e3c70b0b71b51872bbdaa0f4bf2b56908f97abec",
    "semantic_title": "medklip: medical knowledge enhanced language-image pre-training for x-ray diagnosis",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=sNYOeqiBTH": {
    "title": "From RAG to Riches: Retrieval Interlaced with Sequence Generation",
    "volume": "review",
    "abstract": "We present RICHES, a novel approach that interleaves retrieval with sequence generation tasks. RICHES offers an alternative to conventional RAG systems by eliminating the need for separate retriever and generator. It retrieves documents by directly decoding their contents, constrained on the corpus. Unifying retrieval with generation allows us to adapt to diverse new tasks via prompting alone. RICHES can work with any Instruction-tuned model, without additional training. It provides attributed evidence, supports multi-hop retrievals and interleaves thoughts to plan on what to retrieve next, all within a single decoding pass of the LLM. We demonstrate the strong performance of RICHES across ODQA tasks including attributed and multi-hop QA",
    "checked": true,
    "id": "7fc1aaa277898af0444a7788440760f6008ed530",
    "semantic_title": "from rag to riches: retrieval interlaced with sequence generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G4vlza2ZaC": {
    "title": "Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation",
    "volume": "review",
    "abstract": "Recently, training an image captioner without annotated image-sentence pairs has gained traction. Previous methods face limitations due to either using mismatched corpora for inaccurate pseudo pairs or relying on resource-intensive pre-training. To alleviate these challenges, we propose a new strategy where the prior knowledge from large pre-trained models (LPMs) is distilled and leveraged as supervision, and a retrieval process is integrated to further reinforce its effectiveness. Specifically, we introduce $\\textbf{R}$etrieval-$\\textbf{a}$ugmented $\\textbf{P}$seudo $\\textbf{S}$entence $\\textbf{G}$eneration (RaPSG), which can efficiently retrieve highly relevant short region descriptions from mismatching corpora and use them to generate a variety of high-quality pseudo sentences via LPMs. Additionally, we introduce a fluency filter to eliminate low-quality pseudo sentences and a CLIP guidance objective to enhance contrastive information learning. Experimental results show that our method outperforms SOTA captioning models in zero-shot, unsupervised, semi-supervised, and cross-domain scenarios. Moreover, we observe that generating high-quality pseudo sentences may offer better supervision than the crawling sentence strategy, highlighting future research opportunities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2ZsYbEWdXo": {
    "title": "Style Transfer with Multi-iteration Preference Optimization",
    "volume": "review",
    "abstract": "Numerous recent techniques for text style transfer characterize their approaches as variants of reinforcement learning and preference optimization. In this work, we consider the relationship between these approaches and a class of optimization approaches developed primarily for (non-neural) statistical machine translation, formerly known as 'tuning'. Inspired by these techniques from the past, we improve upon established preference optimization approaches, incorporating multiple iterations of exploration and optimization, and choosing contrastive examples by following a 'hope' vs 'fear' sampling strategy. Cognizant of the difference between machine translation and style transfer, however, we further tailor our framework with a new pseudo-parallel generation method and a dynamic weighted reward aggregation method to tackle the lack of parallel data and the need for a multi-objective reward. We evaluate our model on two commonly used text style transfer datasets. Through automatic and human evaluation results we show the effectiveness and the superiority of our model compared to state-of-the-art baselines",
    "checked": true,
    "id": "4eae5af6597134c6190d72a9ad9c7c29c92c8a1e",
    "semantic_title": "style transfer with multi-iteration preference optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AUvmWdE9hs": {
    "title": "Rethink to Check: Mitigating Confirmation Bias for End-to-End Multimodal Fact-Checking",
    "volume": "review",
    "abstract": "End-to-end multimodal fact-checking (MFC) aims to assess the truthfulness of claims using retrieved multimodal evidence. Existing methods rely on the stance extracted from the evidence, achieving good performance with annotated gold evidence, but performing poorly with system-retrieved evidence. The key issue is that the existing model is only exposed to annotated gold evidence during training, inevitably leading to confirmation bias. Such bias refers to that the model tends to treat low-quality system-retrieved evidence as high-quality gold evidence during testing, thus resulting in low robustness and generalization of the model. To mitigate the bias, we propose a novel multi-check framework with causal intervention and counterfactual reasoning. It incorporates three independent checkers to verify claims from diverse perspectives, thereby ensuring a more balanced and accurate fact-checking. Specifically, we first construct two distinct types of counterfactual instances via causal intervention. Then, we apply counterfactual reasoning to train three independent checkers with tailored counterfactual instances or annotated samples. During inference, we eliminate confirmation bias by synthesizing the verification results of all checkers. Experimental results demonstrate the superiority of our proposed framework to state-of-the-art methods, showing performance improvements of 5.5\\% and 16.9\\% with annotated and system-retrieved evidence, respectively. Our code will be released once the paper is accepted",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BA2XusS6GP": {
    "title": "Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood",
    "volume": "review",
    "abstract": "Human and model-generated texts can be distinguished by examining the magnitude of likelihood in language. However, it is becoming increasingly difficult as language model's capabilities of generating human-like texts keep evolving. This study provides a new perspective by using the relative likelihood values instead of absolute ones, and extracting useful features from the spectrum-view of likelihood for the human-model text detection task. We propose a detection procedure with two classification methods, supervised and heuristic-based, respectively, which results in competitive performances with previous zero-shot detection methods and a new state-of-the-art on short-text detection. Our method can also reveal subtle differences between human and model languages, which find theoretical roots in psycholinguistics studies",
    "checked": true,
    "id": "64deb56a947b1409c1bfa67e7b702148a683a0fb",
    "semantic_title": "detecting subtle differences between human and model languages using spectrum of relative likelihood",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hufOctxUan": {
    "title": "A Fairness-Driven Method for Learning Human-Compatible Negotiation Strategies",
    "volume": "review",
    "abstract": "Despite recent advancements in AI and NLP, negotiation remains a difficult domain for AI agents. Traditional game-theoretic approaches that have worked well in two-player zero-sum games struggle in the context of negotiation due to their inability to learn human-compatible strategies. On the other hand, approaches that only use human data tend to be domain-specific and lack the theoretical guarantees provided by strategies grounded in game-theory. Motivated by the notion of fairness as a criteria for optimality in general sum games, we propose a negotiation framework called FDHC which incorporates fairness into both the reward design and search to learn human-compatible negotiation strategies. Our method includes a novel, RL+search technique called LGM-Zero which leverages a pre-trained language model to retrieve human-compatible offers from large action spaces. Our results show that our method is able to achieve more egalitarian negotiation outcomes and improve negotiation quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJGT47VFhM": {
    "title": "BPO: Supercharging Online Preference Learning by Adhering to the Proximity of Behavior LLM",
    "volume": "review",
    "abstract": "Direct alignment from preferences (DAP) has emerged as a promising paradigm for aligning large language models (LLMs) to human desiderata from pre-collected, offline preference datasets. While recent studies indicate that existing offline DAP methods can directly benefit from online training samples, we highlight the need to develop specific online DAP algorithms to fully harness the power of online training. Specifically, we identify that the learned LLM should adhere to the proximity of the *behavior LLM*, which collects the training samples. To this end, we propose online **P**reference **O**ptimization in proximity to the **B**ehavior LLM ($\\mathcal{B}$PO), emphasizing the importance of constructing a proper trust region for LLM alignment. We conduct extensive experiments to validate the effectiveness and applicability of our approach by integrating it with various DAP methods, resulting in significant performance improvements across a wide range of tasks when training with the same amount of preference data. Even when only introducing *one* additional data collection phase, our online $\\mathcal{B}$PO improves its offline DAP baseline from *72.0%* to *80.2%* on TL;DR and from *82.2%* to *89.1%* on Anthropic Helpfulness in terms of win rate against human reference text",
    "checked": true,
    "id": "77a0db12161862066055ca94ac59715cc40b2ed4",
    "semantic_title": "bpo: supercharging online preference learning by adhering to the proximity of behavior llm",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=70yPFGRERi": {
    "title": "Database-Augmented Query Representation for Information Retrieval",
    "volume": "review",
    "abstract": "Information retrieval models that aim to search for the relevant documents to the given query have shown many successes, which have been applied to diverse tasks. However, the query provided by the user is oftentimes very short, which challenges the retrievers to correctly fetch relevant documents. To tackle this, existing studies have proposed expanding the query with a couple of additional (user-related) features related to the query. Yet, they may be suboptimal to effectively augment the query, meanwhile, there is plenty of information available to augment it in a relational database. In this work, motivated by this, we present a novel retrieval framework called Database-Augmented Query representation (DAQu), which augments the original query with various (query-related) metadata across multiple tables. In addition, as the number of features in the metadata can be very large and there is no order among them, we encode them with our graph-based set encoding strategy, which considers hierarchies of features in the database without order. We validate DAQu in diverse retrieval scenarios that can incorporate metadata from the relational database, demonstrating that ours significantly enhances overall retrieval performance, compared to existing query augmentation methods",
    "checked": true,
    "id": "e0a3b67665dfd660d0adf30137e15d315ecca906",
    "semantic_title": "database-augmented query representation for information retrieval",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=REEohcwnOJ": {
    "title": "Utilizing Everything in History: Modeling Relation Inference Path and Entity Structure for Temporal Knowledge Graph Reasoning",
    "volume": "review",
    "abstract": "Temporal Knowledge Graph (TKG) extrapolation fundamentally involves selecting the correct answer from all entities based on historical information. Current methods can easily eliminate most incorrect answers, narrowing the candidate pool to a tiny area called the candidate zone. However, these methods often fail to find the correct answer within this zone, primarily because the entities within the candidate zone are similar in subgraph structure or relational connectivity, causing significant interference. These methods, which either model the graph structure of entities or the paths of relationships, can only address one type of similarity. To address this issue, we propose a model called the $\\textbf{R}$elation Causal Logic $\\textbf{I}$nference and $\\textbf{E}$ntity $\\textbf{S}$tructure Learning (RIES), which consists of two modules: relation inference and entity structure. These two modules model the causal logic of relations over time and the temporal evolution of entities' subgraph structure, respectively, allowing for the differentiation of candidates similar in subgraph structure and relational connectivity. When evaluated on five commonly used public datasets, the performance of RIES surpasses that of other state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NgWSakw55z": {
    "title": "Evaluating n -Gram Novelty of Language Models Using Rusty-DAWG",
    "volume": "review",
    "abstract": "How novel are texts generated by language models (LMs) relative to their training corpora? In this work, we investigate the extent to which modern LMs generate $n$-grams from their training data, evaluating both (i) the probability LMs assign to complete training $n$-grams and (ii) $n$-novelty, the proportion of $n$-grams generated by an LM that did not appear in the training data (for arbitrarily large $n$). To enable arbitrary-length $n$-gram search over a corpus in constant time, we develop Rusty-DAWG, a novel search tool inspired by indexing of genomic data. We compare the novelty of LM-generated text to human-written text and explore factors that affect generation novelty, focusing on the Pythia models. We find that, for $n > 4$, LM-generated text is *less novel* than human-written text, though it is *more novel* for smaller $n$. Larger LMs and more constrained decoding strategies both *decrease novelty*. Finally, we show that LMs complete $n$-grams with lower loss if they are less frequent in the training data. Overall, our results reveal factors influencing the novelty of LM-generated text, and we release Rusty-DAWG to facilitate further pretraining data research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SKDJBY6NvD": {
    "title": "TuringQ: Benchmarking AI Comprehension in Theory of Computation",
    "volume": "review",
    "abstract": "We present TuringQ, to the best of our knowledge, the first effort to evaluate the reasoning capabilities of large language models (LLMs) in the theory of computation. TuringQ consists of 4,006 question-answer pairs spanning undergraduate and graduate-level problems collected from a diverse set of universities. It covers three difficulty levels and six main concepts, including a valuable subset of axioms and essential theoretical concepts. We evaluated various open-source LLMs and GPT-4 using Chain of Thought prompting and human expert assessment. Additionally, we explored an automated LLM-Judge, demonstrating its potential to compete with human precision. We show that fine-tuning an LLaMA-3B model on TuringQ improves its reasoning ability. TuringQ serves as both a benchmark and a fine-tuning resource for enhancing LLM reasoning in this complex domain. Our comparative analysis reveals insights into LLM performance, contributing to advancements in AI comprehension of theoretical computer science. The dataset, code, and fine-tuned model will be made publicly available upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1QCjU78j9": {
    "title": "Factual Dialogue Summarization via Learning from Large Language Models",
    "volume": "review",
    "abstract": "Factual consistency is an important quality in dialogue summarization. Large language model (LLM)-based automatic text summarization models generate more factually consistent summaries compared to those by smaller pretrained language models, but they face deployment challenges in real-world applications due to privacy or resource constraints. In this paper, we investigate the use of symbolic knowledge distillation to improve the factual consistency of smaller pretrained models for dialogue summarization. We employ zero-shot learning to extract symbolic knowledge from LLMs, generating both factually consistent (positive) and inconsistent (negative) summaries. We then apply two contrastive learning objectives on these summaries to enhance smaller summarization models. Experiments with BART, PEGASUS, and Flan-T5 indicate that our approach surpasses strong baselines that rely on complex data augmentation strategies. Our approach achieves better factual consistency while maintaining coherence, fluency, and relevance, as confirmed by various automatic evaluation metrics. We also provide access to the data and code to facilitate future research",
    "checked": true,
    "id": "5a24294d1dd5806675054c70ee81515c7e725786",
    "semantic_title": "factual dialogue summarization via learning from large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FrjPJxP0lO": {
    "title": "LEGO: Language Model Building Blocks",
    "volume": "review",
    "abstract": "Large language models (LLMs) are essential in natural language processing (NLP) but are costly in fine-tuning and inference, and involve invasive data collection. Task-specific small language models (SLMs) offer a cheaper alternative but lack robustness and generalization. This paper proposes a novel technique to combine SLMs and construct a robust, general LLM. Using state-of-the-art LLM pruning strategies, we create task- and user-specific SLM building blocks that are efficient for fine-tuning and inference while also preserving user data privacy. Utilizing Federated Learning and a novel aggregation scheme, we can compile an LLM from distributed SLMs, maintaining robustness without high costs and preserving user data privacy",
    "checked": false,
    "id": "16a7e1f9470322f154ee4b93cdd5103ad0044f60",
    "semantic_title": "digitizing interlocking building blocks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NZPDERBlMU": {
    "title": "When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?",
    "volume": "review",
    "abstract": "The introduction of Large Language Models (LLMs) has advanced data representation and analysis, bringing significant progress in their use for medical questions and answering. Despite these advancements, integrating tabular data, especially numerical data pivotal in clinical contexts, into LLM paradigms has not been thoroughly explored. In this study, we examine the effectiveness of vector representations from last hidden states of LLMs for medical diagnostics and prognostics using electronic health record (EHR) data. We compare the performance of these embeddings with that of raw numerical EHR data when used as feature inputs to traditional machine learning (ML) algorithms that excel at tabular data learning, such as eXtreme Gradient Boosting. We focus on instruction-tuned LLMs in a zero-shot setting to represent abnormal physiological data and evaluating their utilities as feature extractors to enhance ML classifiers for predicting diagnoses, length of stay, and mortality. Furthermore, we examine prompt engineering techniques on zero-shot and few-shot LLM embeddings to measure their impact comprehensively. Although findings suggest the raw data features still prevails in medical ML tasks, zero-shot LLM embeddings demonstrate competitive results, suggesting a promising avenue for future research in medical applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nos1wF3ekA": {
    "title": "Are Large Language Models Capable of Generating Human-Level Narratives?",
    "volume": "review",
    "abstract": "As daily reliance on large language models (LLMs) grows, assessing their generation quality is crucial to understanding how they might impact on our communications. This paper investigates the capability of LLMs in storytelling, focusing on narrative development and plot progression. We introduce a novel computational framework to analyze narratives through three discourse-level aspects: i) story arcs, ii) turning points, and iii) affective dimensions, including arousal and valence. By leveraging expert and automatic annotations, we uncover significant discrepancies between the LLM- and human- written stories. While human-written stories are suspenseful, arousing, and diverse in narrative structures, LLM stories are homogeneously positive and lack tension. Next, we measure narrative reasoning skills as a precursor to generative capacities, concluding that most LLMs fall short of human abilities in discourse understanding. Finally, we show that explicit integration of aforementioned discourse features can enhance storytelling, as is demonstrated by over 40\\% improvement in neural storytelling in terms of diversity, suspense, and arousal. Such advances promise to facilitate greater and more natural roles LLMs in human communication",
    "checked": true,
    "id": "325143c178b6c303ff74631ed8d98f2f475ccc38",
    "semantic_title": "are large language models capable of generating human-level narratives?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eM0CGTmWlM": {
    "title": "Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement",
    "volume": "review",
    "abstract": "Scene Graph Generation (SGG) provides basic language representation of visual scenes, requiring models to grasp complex and diverse semantics between objects. This complexity and diversity in SGG leads to underrepresentation, where parts of triplet labels are rare or even unseen during training, resulting in imprecise predictions. To tackle this, we propose integrating the pretrained Vision-language Models to enhance representation. However, due to the gap between pretraining and SGG, direct inference of pretrained VLMs on SGG leads to severe bias, which stems from the imbalanced predicates distribution in the pretraining language set. To alleviate the bias, we introduce a novel LM Estimation to approximate the unattainable predicates distribution. Finally, we ensemble the debiased VLMs with SGG models to enhance the representation, where we design a certainty-aware indicator to score each sample and dynamically adjust the ensemble weights. Our training-free method effectively addresses the predicates bias in pretrained VLMs, enhances SGG's representation, and significantly improve the performance",
    "checked": false,
    "id": "b7875f28cfb4831c4a52af75c19cd8ecb859ba98",
    "semantic_title": "improving scene graph generation with relation words' debiasing in vision-language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dmrEeQRfsF": {
    "title": "Optimizing Chinese Lexical Simplification Across Word Types: A Hybrid Approach",
    "volume": "review",
    "abstract": "This paper addresses the task of Chinese lexical simplification (CLS), which aims to replace complex words in a given sentence with simpler alternatives that retain the original meaning. One of the challenges in CLS is the scarcity of data resources. Previous unsupervised methods exhibit limited performance, while supervised methods struggle because of the lack of annotated data. We begin by evaluating the few-shot performance of several dialogue models at various scales on CLS, discovering that their effectiveness is sensitive to different word types. For large but expensive Large Language Models (LLMs), such as GPT-4, excel at simplifying in-dictionary common words and Chinese idioms compared to smaller models. Therefore, we propose an automatic knowledge distillation approach that generates training data for common words and Chinese idioms using GPT-4, and then use the training data to fine-tune smaller models in a unified but word-type aware manner. Besides, even GPT-4 encounters difficulties with out-of-dictionary (OOD) words. To address this, we employ a retrieval-based interpretation augmentation strategy, injecting relevant information from external sources into context. The experimental results show that the fine-tuned small models can obtain superior performance than GPT-4 for simplifying common words and idioms, which optimizes the balance between CLS performance and computational cost. The interpretation augmentation strategy can improve the performance of most models for simplifying OOD words",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Np69McMZKo": {
    "title": "HyQE: Ranking Contexts with Hypothetical Query Embeddings",
    "volume": "review",
    "abstract": "Retrieval-augmented generation (RAG) systems can effectively address user queries by leveraging indexed document corpora to retrieve the relevant contexts. Ranking techniques have been adopted in RAG systems to sort the retrieved contexts by their relevance to the query so that users can select the most useful contexts for their downstream tasks. While many existing ranking methods rely on the similarity between the embedding vectors of the context and query to measure relevance, it is important to note that similarity does not equate to relevance in all scenarios. Some ranking methods use large language models (LLMs) to rank the contexts by putting the query and the candidate contexts in the prompt and asking LLM about their relevance. The scalability of those methods is contingent on the number of candidate contexts and the context window of those LLMs. Also, those methods require fine-tuning the LLMs, which can be computationally expensive and require domain-related data. In this work, we propose a scalable ranking framework that does not involve LLM training. Our framework uses an off-the-shelf LLM to hypothesize the user's query based on the retrieved contexts and ranks the contexts based on the similarity between the hypothesized queries and the user query. Our framework is efficient at inference time and is compatible with many other context retrieval and ranking techniques. Experimental results show that our method improves the ranking performance of retrieval systems in multiple benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZcDSae49pR": {
    "title": "PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking Large Language Models",
    "volume": "review",
    "abstract": "The emergence of Large Language Models (LLMs) in the medical domain has stressed a compelling need for standard datasets to evaluate their performance. Although there have been several benchmark datasets for medical problems, they either cover common knowledge across different departments or are specific to another department rather than pediatrics. Moreover, some of them are limited to objective questions and do not measure the generation capacity of LLMs. Therefore, they cannot comprehensively assess the ability of LLMs in pediatrics. To fill this gap, we construct PediaBench, the first Chinese pediatric dataset for LLM evaluation. Specifically, it contains 4,565 objective questions and 1,632 subjective questions spanning 12 pediatric disease groups. It adopts an integrated scoring criterion based on five types of questions to thoroughly assess the proficiency of an LLM in instruction following, knowledge understanding, clinical case analysis, etc. Finally, we validate the effectiveness of PediaBench with extensive experiments on 21 open-source and commercial LLMs. Through an in-depth analysis of experimental results, we offer insights into the ability of LLMs to handle pediatric questions in the Chinese context, highlighting their limitations for further improvements. Our code and data are published anonymously at https://anonymous.4open.science/r/PediaBench-E8E2",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jEBT890By0": {
    "title": "MDCR: A Dataset for Multi-Document Conditional Reasoning",
    "volume": "review",
    "abstract": "The same real-life questions posed to different individuals may lead to different answers based on their unique situations. For instance, whether a student is eligible for a scholarship depends on eligibility conditions, such as major or degree required. ConditionalQA was proposed to evaluate models' capability of reading a document and answering eligibility questions, considering *unmentioned* conditions. However, it is limited to questions on single documents, neglecting harder cases that may require *cross-document reasoning* and *optimization*, for example, \"What is the maximum number of scholarships attainable?\" Such questions over multiple documents are not only more challenging due to more context having to understand, but also because the model has to (1) explore all possible combinations of unmentioned conditions and (2) understand the relationship between conditions across documents, to reason about the optimal outcome. To evaluate models' capability of answering such questions, we propose a new dataset MDCR, which can reflect real-world challenges and serve as a new test bed for complex conditional reasoning that requires optimization. We evaluate this dataset using the most recent LLMs and demonstrate their limitations in solving this task. We believe this dataset will facilitate future research in answering optimization questions with unknown conditions",
    "checked": true,
    "id": "5c23d83e1dad580160f58afb84d12e61f369b8ab",
    "semantic_title": "mdcr: a dataset for multi-document conditional reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d2HtyiNZfz": {
    "title": "Ask-before-Plan: Proactive Language Agents for Real-World Planning",
    "volume": "review",
    "abstract": "The evolution of large language models (LLMs) has enhanced the planning capabilities of language agents in diverse real-world scenarios. Despite these advancements, the potential of LLM-powered agents to comprehend ambiguous user instructions for reasoning and decision-making is still under exploration. In this work, we introduce a new task, Proactive Agent Planning, which requires language agents to predict clarification needs based on user-agent conversation and agent-environment interaction, invoke external tools to collect valid information, and generate a plan to fulfill the user's demands. To study this practical problem, we establish a new benchmark dataset, Ask-before-Plan. To tackle the deficiency of LLMs in proactive planning, we propose a novel multi-agent framework, Clarification-Execution-Planning ($\\texttt{CEP}$), which consists of three agents specialized in clarification, execution, and planning. We introduce the trajectory tuning scheme for the clarification agent and static execution agent, as well as the memory recollection mechanism for the dynamic execution agent. Extensive evaluations and comprehensive analyses conducted on the Ask-before-Plan dataset validate the effectiveness of our proposed framework",
    "checked": true,
    "id": "367e43d1561fce27c919e2d370e42399a40846bd",
    "semantic_title": "ask-before-plan: proactive language agents for real-world planning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k32hz3fO7P": {
    "title": "Can Generic LLMs Help Analyze Child-Adult Interactions Involving Children with Autism in Clinical Observation?",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown significant potential in understanding human communication and interaction. However, their performance in the domain of child-inclusive interactions, including in clinical settings, remains less explored. In this work, we evaluate generic LLMs' ability to analyze child-adult dyadic interactions across four tasks: classifying child-adult utterances, predicting engaged activities, language skills, and traits that are clinically relevant. Our evaluation shows that generic LLMs are highly capable of analyzing long and complex conversations in clinical observation sessions, often surpassing the performance of non-expert humans. The results show their potential to segment interactions of interest, assist in language skills evaluation, and offer clinical-relevant context for assessments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u5ggNGb6U5": {
    "title": "Reference-based Metrics Disprove Themselves in Question Generation",
    "volume": "review",
    "abstract": "Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our metric accurately distinguishes between high-quality questions and flawed ones, and achieves state-of-the-art alignment with human judgment",
    "checked": true,
    "id": "74d6e2a3d2bc2c43fdd56c154c614edf26b3191b",
    "semantic_title": "reference-based metrics disprove themselves in question generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D2F4Kz6Tmr": {
    "title": "ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts",
    "volume": "review",
    "abstract": "Typical schemes for automated red-teaming large language models (LLMs) focus on discovering prompts that trigger a frozen language model (the defender) to generate toxic text. This often results in the prompting model (the adversary) producing text that is unintelligible and unlikely to arise. Here, we propose a reinforcement learning formulation of the LLM red-teaming task which allows us to discover prompts that both (1) trigger toxic outputs from a frozen defender and (2) have low perplexity as scored by the defender. We argue these cases are most pertinent in a red-teaming setting because of their likelihood to arise during normal use of the defender model. We solve this formulation through a novel online and weakly supervised variant of Identity Preference Optimization (IPO) on GPT-2 and GPT-2 XL defenders. We demonstrate that our policy is capable of generating likely prompts that also trigger toxicity. Finally, we qualitatively analyze learned strategies, trade-offs of likelihood and toxicity, and discuss implications",
    "checked": true,
    "id": "81178c837092279d9184e229fa95d4efeedceaa4",
    "semantic_title": "astprompter: weakly supervised automated language model red-teaming to identify likely toxic prompts",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eDRSI97wbs": {
    "title": "Iterative Introspection based Refinement: Boosting Multi-Document Scientific Summarization with Large Language Models",
    "volume": "review",
    "abstract": "The current task setting and constructed datasets for Multi-Document Scientific Summarization (MDSS) have led to a significant gap between existing research and practical applications. However, the emergence of Large Language Models (LLMs) provides us with an opportunity to address MDSS from a more practical perspective. To this end, we redefine MDSS task based on the scenario that automatically generates the entire related work section, and then construct a corresponding new dataset, ComRW. We first conduct a comprehensive evaluation of the performance of LLMs on the newly defined task, and identify three major deficiencies in their ability to address MDSS task: low coverage of reference papers, disorganized structure, and high redundancy. To alleviate these three deficiencies, we propose an Iterative Introspection based Refinement (IIR) method that utilizes LLMs to generate higher-quality summaries. The IIR method uses prompts equipped with Chain-of-Thought and fine-grained operators to treat LLMs as an evaluator and a generator to evaluate and refine the three deficiencies, respectively. We conduct thorough automatic and human evaluation to validate the effectiveness of our method. The results demonstrate that the proposed IIR method can effectively mitigate the three deficiencies and improve the quality of summaries generated by LLMs. Moreover, our exploration provides insights for better addressing MDSS task with LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JlgyYVBaqN": {
    "title": "HMamba: Towards Multifaceted Computer-assisted Pronunciation Training Leveraging Hierarchical Selective State Space Model and Decoupled Cross-entropy Loss",
    "volume": "review",
    "abstract": "Prior efforts in building computer-assisted pronunciation training (CAPT) systems often treat automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD) as separate fronts. APA aims to provide multiple pronunciation aspect scores across diverse linguistic levels, while MDD focuses instead on pinpointing the precise phonetic errors made by non-native language learners. However, a full-fledged CAPT system should integrate both features simultaneously. To address this pressing need, we in this work first propose HMamba, a novel hierarchical selective state space method that jointly tackles APA and MDD tasks. In addition, to enhance model performance, we introduce a novel loss function, decoupled cross-entropy loss (deXent), specifically tailored for the MDD task to facilitate better supervised label learning. A comprehensive set of empirical results carried out on the speechocean762 benchmark dataset demonstrate the effectiveness of our approach in multi-aspect multi-granular assessments. Furthermore, our proposed approach also yields considerable improvement in MDD performance over a competitive baseline, achieving an F1-score of 63.32%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GlFzqCNXiG": {
    "title": "FastLexRank: Efficient Lexical Ranking for Structuring Social Media Posts",
    "volume": "review",
    "abstract": "In this paper, we present FastLexRank, a computationally efficient adaptation of the LexRank algorithm, which is an unsupervised approach to ranking texts based on graph-based centrality scoring of sentences, which we have tailored to be efficient text ranking. Addressing the computational and memory complexities of the original LexRank, FastLexRank employs a new algorithm to approximate the stationary distribution of sentence graphs, thereby enhancing efficiency while maintaining the quality of summarization. The correlation of FastLexRank's centrality scores with the original LexRank scores approaches a perfect match, and the Kendall rank correlation between ranked sequences produced by the original and the new approximation approach also reaches this high level of agreement. The paper details these algorithmic modifications and their transformative effect on the size of the data sets that can be processed, e.g., large social media corpora. Empirical results confirm FastLexRank's ability to effectively generate centrality scores for sentences in large social media corpora, underscoring its suitability for real-time analysis in various applications. We further suggest that FastLexRank can act as a ranker to identify the most central tweet, which can then be integrated with more advanced NLP technologies, such as Large Language Models, for enhanced analysis. This research contributes to Natural Language Processing by offering a scalable solution for text centrality calculation, critical for managing the ever-increasing volume of digital content",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kS41AMEm5l": {
    "title": "ESM+: Modern Insights into Perspective on Text-to-SQL Evaluation in the Age of Large Language Models",
    "volume": "review",
    "abstract": "The task of Text-to-SQL enables anyone to retrieve information from SQL databases using natural language. Despite several challenges, recent models have made remarkable advancements in this task using large language models (LLMs). Interestingly, we find that LLM-based models without fine-tuning exhibit distinct natures compared to their fine-tuned counterparts, leading to inadequacies in current evaluation metrics to accurately convey their performance. Thus, we analyze the two primary metrics, Test Suite Execution Accuracy (EXE) and Exact Set Matching Accuracy (ESM), to examine their robustness for this task and address shortcomings. We compare the performance of 9 LLM-based models using EXE, the original ESM, and our improved ESM (called ESM+). Our results show that EXE and ESM have high false positive and negative rates of 11.3% and 13.9%, while ESM+ gives those of 0.1% and 2.6% respectively, providing a significantly more stable evaluation. We release the ESM+ script as open-source for the community to contribute, while enjoying a more reliable assessment of Text-to-SQL",
    "checked": true,
    "id": "d564a7252cd9f4ff4eb43d694d48b075ab8bd74f",
    "semantic_title": "esm+: modern insights into perspective on text-to-sql evaluation in the age of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w0ZtUXRARU": {
    "title": "PuzzleGPT: Emulating Human Puzzle-Solving Ability for Time and Location Prediction",
    "volume": "review",
    "abstract": "The task of predicting time and location from images is challenging and requires complex human-like puzzle-solving ability over different clues. In this work, we formalize this ability into core skills and implement them using different modules in an expert pipeline called PuzzleGPT. PuzzleGPT consists of a perceiver to identify visual clues, a reasoner to deduce prediction candidates, a combiner to combinatorially combine information from different clues, a web retriever to get external knowledge if the task can't be solved locally, and a noise filter for robustness. This results in a zero-shot, interpretable, and robust approach that records state-of-the-art performance on two datasets -- TARA and WikiTilo. PuzzleGPT outperforms large VLMs such as BLIP-2, InstructBLIP, LLaVA, and even GPT-4V, as well as automatically generated reasoning pipelines like VisProg, by at least 32\\% and 38\\%, respectively. It even rivals or surpasses finetuned models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zd2cMHHoGe": {
    "title": "Is Compound Aspect-Based Sentiment Analysis Addressed by ChatGPT?",
    "volume": "review",
    "abstract": "Aspect-based sentiment analysis (ABSA) aims to predict aspect-based elements from the given text, mainly including four elements, i.e., aspect category, sentiment polarity, aspect term, and opinion term. Extracting pair, triple, or quad of elements is defined as compound ABSA. Due to its challenges and practical applications, such a compound scenario has become an emerging topic. Recently, large language models (LLMs), e.g. ChatGPT, present impressive abilities in tackling various human instructions. In this work, we are particularly curious whether ChatGPT still possesses superior performance in handling compound ABSA tasks. To assess the performance of ChatGPT, we design a novel framework, called ChatABSA. Concretely, we design two strategies: constrained prompts, to automatically organize the returned predictions; post-processing, to better evaluate the capability of ChatGPT in recognition of implicit information. The overall evaluation involves 5 compound ABSA tasks and 8 publicly available datasets. We compare ChatGPT with few-shot supervised baselines and fully supervised baselines, including corresponding state-of-the-art (SOTA) models on each task. Experimental results show that ChatABSA exhibits excellent aspect-based sentiment analysis capabilities and overwhelmingly beats few-shot supervised methods under the same few-shot settings. Surprisingly, it can even outperform fully supervised methods in some cases. However, in most cases, it underperforms fully supervised methods, and there is still a huge gap between its performance and the SOTA method. Moreover, we also conduct a series of correlation analyses to gain a deeper understanding of its sentiment analysis capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gn8Ex8hMnV": {
    "title": "Identifying and Resolving Ambiguous Intents in Coding Instructions using Discourse Frameworks",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are used by many to generate code in a pair-programming-like setting. However, intents in users' coding instructions are ambiguous, and models are limited in their ability to use dialogue to disambiguate intent to produce unambiguous code. This presents a fundamental difficulty in code generation, wherein the ambiguity in natural language can lead to seemingly correct programs that are different from the intended. We propose to use dialogue to reduce this ambiguity, specifically in the plotting domain, and contribute an analysis of the different types of ambiguity that may exist in multi-modal code generation. Based on our analysis, we propose different pragmatic models to inform dialogue strategies for ambiguity resolution, including those based on Rational Speech Acts (cooperative), Discourse Theory (discoursive), and Questions under Discussion (inquisitive). Finally, we compare these dialogue strategies in a simulated dialogue setting â€” operationalizing the pragmatic models via prompting. Our findings suggest that discoursive and cooperative reasoning styles show the best results regarding executability and disambiguation, while inquisitive reasoning performs the best in disambiguation for vagueness. These suggest that simulated dialogues with pragmatic frameworks can resolve ambiguities in code generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j5HU5PKnJk": {
    "title": "LawInstruct: A Resource for Studying Language Model Adaptation to the Legal Domain",
    "volume": "review",
    "abstract": "Instruction tuning is an important step in making language models useful for direct user interaction. However, the legal domain is underrepresented in typical instruction datasets (e.g., only 10 out of 1600+ tasks in Super-NaturalInstructions). To study whether instruction tuning on legal datasets is necessary for strong legal reasoning, we aggregate 58 annotated legal datasets and write instructions for each, creating LawInstruct. LawInstruct covers 17 global jurisdictions, 24 languages and a total of 12M examples across diverse tasks such as legal QA, summarization of court cases, and legal argument mining. We evaluate our models on LegalBench, measuring legal reasoning across five categories in 162 challenging and realistic legal tasks, and MMLU, to measure potential drops in general reasoning capabilities. We find that legal-specific instruction tuning on Flan-T5 â€“ yielding FLawN-T5 â€“ improves performance on LegalBench across all model sizes, with an aggregate increase of 15 points or 50\\% over Flan-T5 for the base size. No model size shows performance drops in MMLU. We publish LawInstruct as a resource for further study of instruction tuning in the legal domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EeZUl6RVBF": {
    "title": "ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations",
    "volume": "review",
    "abstract": "Detecting hate speech and offensive language is essential for maintaining a safe and respectful digital environment. This study examines the limitations of state-of-the-art large language models (LLMs) in identifying offensive content within systematically perturbed data, with a focus on Chinese, a language particularly susceptible to such perturbations. We introduce \\textsf{ToxiCloakCN}, an enhanced dataset derived from ToxiCN, augmented with homophonic substitutions and emoji transformations, to test the robustness of LLMs against these cloaking perturbations. Our findings reveal that existing models significantly underperform in detecting offensive content when these perturbations are applied. We provide an in-depth analysis of how different types of offensive content are affected by these perturbations and explore the alignment between human and model explanations of offensiveness. Our work highlights the urgent need for more advanced techniques in offensive language detection to combat the evolving tactics used to evade detection mechanisms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n94TkRInd3": {
    "title": "Human-AI Collaborative Essay Scoring: A Dual-Process Framework with LLMs",
    "volume": "review",
    "abstract": "Receiving timely and personalized feedback is essential for second-language learners, especially when human instructors are unavailable. This study explores the effectiveness of Large Language Models (LLMs), including both proprietary and open-source models, for Automated Essay Scoring (AES). Through extensive experiments with public and private datasets, we find that while LLMs do not surpass conventional state-of-the-art (SOTA) grading models in performance, they exhibit notable consistency, generalizability, and explainability. We propose an open-source LLM-based AES system, inspired by the dual-process theory. Our system offers accurate grading and high-quality feedback, at least comparable to that of fine-tuned proprietary LLMs, in addition to its ability to alleviate misgrading. Furthermore, we conduct human-AI co-grading experiments with both novice and expert graders. We find that our system not only automates the grading process but also enhances the performance and efficiency of human graders, particularly for essays where the model has lower confidence. These results highlight the potential of LLMs to facilitate effective human-AI collaboration in the educational context, potentially transforming learning experiences through AI-generated feedback",
    "checked": true,
    "id": "49640761172f9410ab45c94d5d973b835b9cd505",
    "semantic_title": "human-ai collaborative essay scoring: a dual-process framework with llms",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=YwYm5lq6DA": {
    "title": "Exploring Group and Symmetry Principles in Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across a wide range of applications; however, assessing their reasoning capabilities remains a significant challenge. In this paper, we introduce a framework grounded in group and symmetry principles, which have played a crucial role in fields such as physics and mathematics, and offer another way to evaluate their capabilities. While the proposed framework is general, to showcase the benefits of employing these properties, we focus on arithmetic reasoning and investigate the performance of these models on four group properties: closure, identity, inverse, and associativity. Our findings reveal that LLMs studied in this work struggle to preserve group properties across different test regimes. In the closure test, we observe biases towards specific outputs and an abrupt degradation in their performance from 100% to 0% after a specific sequence length. They also perform poorly in the identity test, which represents adding irrelevant information in the context, and show sensitivity when subjected to inverse test, which examines the robustness of the model with respect to negation. In addition, we demonstrate that breaking down problems into smaller steps helps LLMs in the associativity test that we have conducted. To support these tests we have developed a synthetic dataset which will be released",
    "checked": true,
    "id": "ebf7b27d826e8193dbe58f76602c5069abb5745e",
    "semantic_title": "exploring group and symmetry principles in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5o9QLeoLY2": {
    "title": "Detecting Temporal Ambiguity in Questions",
    "volume": "review",
    "abstract": "Detecting and answering ambiguous questions has been a challenging task in open-domain question answering. Ambiguous questions have different answers depending on their interpretation and can take diverse forms. Temporally ambiguous questions are one of the most common types of such questions. In this paper, we introduce TEMPAMBIQA, a manually annotated temporally ambiguous QA dataset consisting of 8,162 open-domain questions to study the task of detecting the temporally ambiguous questions. We propose a novel approach for detecting temporally ambiguous questions by using diverse search strategies based on disambiguated versions of the questions. We also introduce and test non-search, competitive baselines for detecting temporal ambiguity using zero-shot and few-shot approaches",
    "checked": false,
    "id": "6b68c15599ce2e09070585054c3d69976b746276",
    "semantic_title": "welcome to the touch dome!",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cTte4olXJz": {
    "title": "Contextual Moral Value Alignment Through Context-Based Aggregation",
    "volume": "review",
    "abstract": "Developing value-aligned agents is a complex undertaking and an ongoing challenge in the field of AI. Specifically within the domain of Large Language Models (LLMs), designing models that can balance multiple possibly conflicting moral values based on the context is a problem of paramount importance. In this paper, we propose a system that does contextual moral value alignment based on contextual aggregation. Here, aggregation is defined as the process of integrating a subset of LLM responses that are best suited to a user's input, taking into account features extracted about the user's moral preferences. The proposed system trained using the Moral Integrity Corpus shows better results in term of alignment to human values compared to state-of-the-art baselines",
    "checked": true,
    "id": "fe573a5bb8dfa1061c0e07c6f0c895cbf353139e",
    "semantic_title": "contextual moral value alignment through context-based aggregation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=LE1w3S7YNE": {
    "title": "Exploring Forgetting in Large Language Model Pre-Training",
    "volume": "review",
    "abstract": "In large language models (LLMs), the challenge of catastrophic forgetting remains a formidable obstacle to building an omniscient model. Despite the pioneering research on task-level forgetting in LLM fine-tuning, there is scant focus on sample-level forgetting during this phase, where models often see each datapoint only once. We systematically explore the existence, essence, and measurement of forgetting in LLM pre-training, questioning traditional metrics such as perplexity (PPL) and introducing new metrics to better detect entity memory retention, which is the indicator of forgetting. Taking inspiration from human memory patterns, we propose and refine memory replay techniques to combat the phenomenon of forgetting in LLMs. Extensive evaluations and analyses on forgetting of pre-training could facilitate future research on LLMs",
    "checked": false,
    "id": "8ce9b1e527c4d9d15239621ec4e3ef3fbbe32202",
    "semantic_title": "on the role of bidirectionality in language model pre-training",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=usqKwDx9lA": {
    "title": "Predicting Compact Phrasal Rewrites with Large Language Models forAutomatic Speech Recognition Post Editing",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) excel at rewriting tasks such as text style transfer and grammatical error correction. Although the output in these tasks often significantly overlaps with the input, the decoding cost still increases with output length, regardless of the number of overlaps. By leveraging the overlap between the input and the output, Kaneko and Okazaki (2023) proposed model-agnostic edit span representations to compress the rewrites to save computation. They reported an output length reduction rate of nearly 80% with minimal accuracy impact in four rewriting tasks. In this paper, we propose alternative edit phrase representations inspired by phrase-based statistical machine translation. We systematically compare our phrasal representations with their span representation. We apply the LLM rewriting model to the task of Automatic Speech Recognition (ASR) post editing and show that our target-phrase-only edit representation has the best efficiency-accuracy trade-off. On the LibriSpeech test set, our method closes 50-60% of the WER gap between the edit span model and the full rewrite model while losing only 10-20% of the length reduction rate of the edit span model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mlXlLyFFvs": {
    "title": "Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs",
    "volume": "review",
    "abstract": "While Large Language Models (LLMs) demonstrate impressive generation abilities, they frequently struggle when it comes to specialized domains due to their limited domain-specific knowledge. Studies on domain-specific LLMs resort to expanding the vocabulary before fine-tuning on domain-specific corpus, aiming to decrease the sequence length and enhance efficiency during decoding, without thoroughly investigate the results of vocabulary expansion to LLMs over different domains. Our pilot study reveals that expansion with only a subset of the entire vocabulary may lead to superior performance. Guided by the discovery, this paper explores how to identify a vocabulary subset to achieve the optimal results. We introduce VEGAD, an adaptive method that automatically identifies valuable words from a given domain vocabulary. Our method has been validated through experiments on three Chinese datasets, demonstrating its effectiveness. Additionally, we have undertaken comprehensive analyses of the method. The selection of a optimal subset for expansion has shown to enhance performance on both domain-specific tasks and general tasks, showcasing the potential of VEGAD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ELcDNqOooA": {
    "title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
    "volume": "review",
    "abstract": "As LLMs advance, evaluating generated text reliably becomes more challenging due to the high costs of human evaluation. To make progress toward better LLM autoraters, we introduce FLAME, a family of Foundational Large Autorater ModEls. FLAME is trained on our large and diverse collection of nearly 100 quality assessment tasks comprising 5M+ human judgments, curated and standardized using publicly released human evaluations from previous research. FLAME significantly improves generalization to a wide variety of held-out tasks, outperforming proprietary LLMs like GPT-4 and Claude on many tasks. Additionally, we show that our FLAME multitask mixture can be further optimized for specific downstream applications, e.g., reward modeling evaluation, through a novel tail-patch fine-tuning technique. Notably, on RewardBench, our model (86.7) is the top-performing generative model trained solely on permissively licensed data, outperforming both GPT-4-0125 (85.9) and GPT-4o (84.7). Our analysis reveals that FLAME is significantly less biased than popular LLM-as-a-Judge models on the CoBBLEr cognitive bias benchmark, while effectively identifying high-quality responses for code generation. We release our FLAME data collection at this http URL",
    "checked": true,
    "id": "6c5ae8a0bf1885c2dd62c4787db4ac0f5159141f",
    "semantic_title": "foundational autoraters: taming large language models for better automatic evaluation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=f8z18xDC1Z": {
    "title": "Robust Hate Speech Detection Without Predefined Spurious Words",
    "volume": "review",
    "abstract": "Hate speech detection classifiers suffer from spurious correlations between specific words and the hate class. The spurious words can be either the identity words (e.g., \"black\", \"female\", \"gay\") or non-identity words (e.g., \"sport\", \"football\"). The current studies mainly focus on removing spurious correlations based on predefined identity words. In this paper, we develop a novel spurious correlation mitigating strategy, called ARLHAD, without any prior knowledge of spurious words. ARLHAD leverages a minimax game for optimization between a classifier and an adversary, in which the classifier aims to improve the hate speech detection performance by minimizing the classification loss while the adversary aims to maximize the loss mainly caused by spurious words. After training, ARLHAD improves the overall performance and more importantly, alleviates the spurious correlations. Experimental results on three hate speech detection datasets show the effectiveness of ARLHAD",
    "checked": false,
    "id": "4b82ba13ae276b9fa705ababb156987556d66c95",
    "semantic_title": "robust hate speech detection via mitigating spurious correlations",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=eubxEnxOfp": {
    "title": "My LLM might Mimic AAE - But When Should it?",
    "volume": "review",
    "abstract": "We examine the representation of African American English (AAE) in large language models, exploring (a) the perceptions Black Americans have of how effective these technologies are at producing authentic AAE, and (b) in what contexts Black Americans find this desirable. Through both a survey of Black Americans ($n=$ 104) and annotation of LLM-produced AAE, we find that Black Americans favor choice and autonomy in determining when AAE is appropriate in LLM output. They tend to prefer that LLMs default to communicating in Mainstream U.S. English in formal settings, with greater interest AAE production in less formal settings. When LLMs were appropriately prompted and provided in context examples, our participants found their outputs to have a level of AAE authenticity on par with transcripts of Black American speech",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dpGK4B0mLR": {
    "title": "Enhancing Incremental Summarization with Structured Representations",
    "volume": "review",
    "abstract": "Large language models (LLMs) often struggle with processing extensive input contexts, which can lead to redundant, inaccurate, or incoherent summaries. Recent methods have used unstructured memory to incrementally process these contexts, but they still suffer from information overload due to the volume of unstructured data handled. In our study, we introduce structured knowledge representations (GU_{json}), which significantly improve summarization performance by 40% and 14% across two public datasets. Most notably, we propose the Chain-of-Key strategy (CoK_{json}) that dynamically updates or augments these representations with new information, rather than recreating the structured memory for each new source. This method further enhances performance by 7% and 4% on the datasets",
    "checked": true,
    "id": "4dedb0ba8ce61e0c9e09618b49d7a05a9459d858",
    "semantic_title": "enhancing incremental summarization with structured representations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BuU8aUGior": {
    "title": "Paraphrase Identification Datasets: Usage Survey and Generalization Patterns",
    "volume": "review",
    "abstract": "We perform a survey to identify the most commonly used paraphrase identification datasets. We then look deeper at the top three English datasets containing sentential paraphrases, comparing various qualitative and quantitative characteristics of the datasets. In addition, we investigate the generalization performance of modern models trained on these datasets, showing that models do not generalize well across datasets, showing a weakness in real-world generalisation ability. Lastly, we test some methods to improve generalisation ability, showing that MNLI pre-training and improved label consistency are useful",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=soKmlpSh32": {
    "title": "AAPM: Large Language Model Agent-based Asset Pricing Models",
    "volume": "review",
    "abstract": "In this study, we propose a novel asset pricing approach, LLM Agent-based Asset Pricing Models (AAPM), which fuses qualitative discretionary investment analysis from LLM agents and quantitative manual financial economic factors to predict excess asset returns. The experimental results show that our approach outperforms machine learning-based asset pricing baselines in portfolio optimization and asset pricing errors. Specifically, the Sharpe ratio and average $|\\alpha|$ for anomaly portfolios improved significantly by 9.6\\% and 10.8\\% respectively. In addition, we conducted extensive ablation studies on our model and analysis of the data to reveal further insights into the proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=STzJudM9cp": {
    "title": "Improving Minimum Bayes Risk Decoding with Multi-Prompt",
    "volume": "review",
    "abstract": "While instruction fine-tuned LLMs are effective text generators, sensitivity to prompt construction makes performance unstable and sub-optimal in practice. Relying on a single 'best' prompt cannot capture all differing approaches to a generation problem. Using this observation, we propose multi-prompt decoding, where many candidate generations are decoded from a prompt bank at inference-time. To ensemble candidates, we use Minimum Bayes Risk (MBR) decoding, which selects a final output using a trained value metric. We show multi-prompt improves MBR across a comprehensive set of conditional generation tasks, and show this is a result of estimating a more diverse and higher quality candidate space than that of a single prompt. Our experiments confirm multi-prompt improves generation across tasks, models and metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cKFNUTzU3I": {
    "title": "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies",
    "volume": "review",
    "abstract": "A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often don't surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized",
    "checked": true,
    "id": "f04c25fcf3247ff4d8eca72d862b22090b884b75",
    "semantic_title": "reasoning in token economies: budget-aware evaluation of llm reasoning strategies",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=TtZHqD4lHD": {
    "title": "Beyond Turing Test: Can GPT-4 Sway Experts' Decisions?",
    "volume": "review",
    "abstract": "In the post-Turing era, evaluating large language models (LLMs) involves assessing generated text based on readers' reactions rather than merely its indistinguishability from human-produced content. This paper explores how LLM-generated text impacts readers' decisions, focusing on both amateur and expert audiences. Our findings indicate that GPT-4 can generate persuasive analyses affecting the decisions of both amateurs and professionals. Furthermore, we evaluate the generated text from the aspects of grammar, persuasiveness, logical coherence, and usefulness. The results highlight a high correlation between real-world evaluation through audience reactions and the current multi-dimensional evaluators commonly used for generative models. Overall, this paper shows the potential and risk of using generated text to sway human decisions and also points out a new direction for evaluating generated text, i.e., leveraging the reactions and decisions of readers. We release our dataset to assist future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2RLqASVf5x": {
    "title": "Locate&Edit: Energy-based Text Editing for Efficient, Flexible, and Faithful Controlled Text Generation",
    "volume": "review",
    "abstract": "Recent approaches to controlled text generation (CTG) often involve manipulating the weights or logits of base language models (LMs) at decoding time. However, these methods are inapplicable to latest black-box LMs and ineffective at preserving the core semantics of the base LM's original generations. In this work, we propose Locate&Edit(L&E), an efficient and flexible energy-based approach to CTG, which edits text outputs from a base LM using off-the-shelf energy models. Given text outputs from the base LM, L&E first locates spans that are most relevant to constraints (e.g., toxicity) utilizing energy models, and then edits these spans by replacing them with more suitable alternatives. Importantly, our method is compatible with black-box LMs, as it requires only the text outputs. Also, since L&E doesn't mandate specific architecture for its component models, it can work with a diverse combination of available off-the-shelf models. Moreover, L&E preserves the base LM's original generations, by selectively modifying constraint-related aspects of the texts and leaving others unchanged. These targeted edits also ensure that L&E operates efficiently. Our experiments confirm that L&E achieves superior semantic preservation of the base LM generations and speed, while simultaneously obtaining competitive or improved constraint satisfaction. Furthermore, we analyze how the granularity of energy distribution impacts CTG performance and find that fine-grained, regression-based energy models improve constraint satisfaction, compared to conventional binary classifier energy models",
    "checked": true,
    "id": "2c23641cc7daf5501a2077a2a8e78be9fafbd7a1",
    "semantic_title": "locate&edit: energy-based text editing for efficient, flexible, and faithful controlled text generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IvN1xqMe9X": {
    "title": "Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets",
    "volume": "review",
    "abstract": "Advancements in Large Language Models (LLMs) have significantly enhanced instruction-following capabilities. However, most Instruction Fine-Tuning (IFT) datasets are predominantly in English, limiting model performance in other languages. Traditional methods for creating multilingual IFT datasetsâ€”such as translating existing English IFT datasets or converting existing NLP datasets into IFT datasets by templatingâ€”struggle to capture linguistic nuances and ensure prompt (instruction) diversity. To address this issue, we propose a novel method for collecting multilingual IFT datasets that preserves linguistic naturalness and ensures prompt diversity. This approach leverages English-focused LLMs, monolingual corpora, and a scoring function to create high-quality, diversified IFT datasets in multiple languages. Experiments demonstrate that LLMs finetuned using these IFT datasets show notable improvements in both generative and discriminative tasks, indicating enhanced language comprehension by LLMs in non-English contexts. Specifically, on the multilingual summarization task, LLMs using our IFT dataset achieved 17.57% and 15.23% improvements over LLMs fine-tuned with translation-based and template-based datasets, respectively",
    "checked": true,
    "id": "5db663563f7299531409a30f539663d4c2ec0aec",
    "semantic_title": "improving multilingual instruction finetuning via linguistically natural and diverse datasets",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZwfaHjBmPP": {
    "title": "Can Textual Unlearning Solve Cross-Modality Safety Alignment?",
    "volume": "review",
    "abstract": "Recent studies reveal that integrating new modalities into large language models (LLMs), such as vision-language models (VLMs), creates a new attack surface that bypasses existing safety training techniques like supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF). While further SFT and RLHF-based safety training can be conducted in multi-modal settings, collecting multi-modal training datasets poses a significant challenge. Inspired by the structural design of recent multi-modal models, where all input modalities are ultimately fused into the language space, we explore whether unlearning solely in the textual domain can be effective for cross-modality safety alignment. Our empirical evaluation across seven datasets demonstrates promising transferability --- textual unlearning in VLMs significantly reduces the Attack Success Rate (ASR) to less than 8% and in some cases, even as low as nearly 2% for both text-based and vision-text-based attacks, alongside preserving the utility. Moreover, our experiments show that unlearning with a multi-modal dataset offers no potential benefits but incurs significantly increased computational demands",
    "checked": false,
    "id": "80ad06205d9f301e3f60218eca315336329950f7",
    "semantic_title": "cross-modal safety alignment: is textual unlearning all you need?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=6OZb1bG0di": {
    "title": "PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval",
    "volume": "review",
    "abstract": "Utilizing large language models (LLMs) for zero-shot document ranking is done in one of two ways: 1) prompt-based re-ranking methods, which require no further training but are only feasible for re-ranking a handful of candidate documents due to computational costs; and 2) unsupervised contrastive trained dense retrieval methods, which can retrieve relevant documents from the entire corpus but require a large amount of paired text data for contrastive training. In this paper, we propose PromptReps, which combines the advantages of both categories: no need for training and the ability to retrieve from the whole corpus. Our method only requires prompts to guide an LLM to generate query and document representations for effective document retrieval. Specifically, we prompt the LLMs to represent a given text using a single word, and then use the last token's hidden states and the corresponding logits associated with the prediction of the next token to construct a hybrid document retrieval system. The retrieval system harnesses both dense text embedding and sparse bag-of-words representations given by the LLM. We further explore variations of this core idea that consider the generation of multiple words, and representations that rely on multiple embeddings and sparse distributions. Our experimental evaluation on the MSMARCO, TREC deep learning and BEIR zero-shot document retrieval datasets illustrates that this simple prompt-based LLM retrieval method can achieve a similar or higher retrieval effectiveness than state-of-the-art LLM embedding methods that are trained with large amounts of unsupervised data, especially when using a larger LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1PwOSwgVdg": {
    "title": "CluMo: Cluster-based Modality Fusion Prompt for Continual Learning in Visual Question Answering",
    "volume": "review",
    "abstract": "Large vision-language models (VLMs) have shown significant performance boost in various application domains. However, adopting them to deal with several sequentially encountered tasks has been limited because finetuning a VLM on a task normally leads to reducing its generalization power and the capacity of learning new tasks. Enabling using VLMs in multimodal continual learning (CL) settings can help to address such scenarios. Hence, we propose a novel prompt-based CL method for VLMs, namely $\\textbf{Clu}$ster-based $\\textbf{Mo}$dality Fusion Prompt (CluMo). Our approach addresses catastrophic forgetting through constructing modality-specific prompts using $k$-means clustering for selecting the best semantically matched prompt, which also enables benefiting from past experiences through forward transfer. Experiments on two benchmarks demonstrate that our method achieves SOTA against existing alternatives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4riSN1vVfc": {
    "title": "HiGenQA: Exploring Hint Generation Approaches for Open Domain Question Answering",
    "volume": "review",
    "abstract": "Automatic Question Answering (QA) systems rely on contextual information to provide accurate answers. Commonly, contexts are prepared through either retrieval-based or generation-based methods. The former involves retrieving relevant documents from a corpus like Wikipedia, whereas the latter uses generative models such as Large Language Models (LLMs) to generate the context. In this paper, we introduce a novel context preparation approach called HiGenQA, which employs Automatic Hint Generation (HG) systems. Unlike traditional methods, HiGenQA prompts LLMs to produce hints about potential answers for the question rather than generating relevant context. We evaluate our approach across three QA datasets including TriviaQA, Natural Questions, and Web Questions, examining how the number and order of hints impact performance. Our findings show that HiGenQA surpasses both retrieval-based and generation-based approaches. We demonstrate that hints enhance the accuracy of answers more than retrieved and generated contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9xrPOEDwk3": {
    "title": "Open-world Multi-label Text Classification with Extremely Weak Supervision",
    "volume": "review",
    "abstract": "We study open-world multi-label text classification under extremely weak supervision (XWS), where the user only provides a brief description for classification objectives without any labels or ground-truth label space. Similar single-label XWS settings have been explored recently, however, these methods cannot be easily adapted for multi-label. We observe that (1) most documents have a dominant class covering the majority of content and (2) long-tail labels would appear in some documents as a dominant class. Therefore, we first utilize the user description to prompt a large language model (LLM) for dominant keyphrases of a subset of raw documents, and then construct a (initial) label space via clustering. We further apply a zero-shot multi-label classifier to locate the documents with small top predicted scores, so we can revisit their dominant keyphrases for more long-tail labels. We iterate this process to discover a comprehensive label space and construct a multi-label classifier as a novel method, X-MLClass. X-MLClass exhibits a remarkable increase in ground-truth label space coverage on various datasets, for example, a 40\\% improvement on the AAPD dataset over topic modeling and keyword extraction methods. Moreover, X-MLClass achieves the best end-to-end multi-label classification accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hkc719CMqd": {
    "title": "Are Human Conversations Special? A Large Language Model Perspective",
    "volume": "review",
    "abstract": "In this paper, we study the changes in the attention behavior of large language models (LLMs) when used to understand natural conversations between humans (human-human conversations). By analyzing metrics such as attention distance, dispersion, and interdependency across these domains, we highlight the unique challenges posed to LLMs by conversational data. Our findings reveal that while language models exhibit domain-specific attention behaviors, there is a significant gap in their ability to specialize in human conversations. Through detailed attention entropy analysis and t-SNE visualizations, we demonstrate the need for models trained with diverse, high-quality conversational data to enhance understanding and generation of human-like dialogue",
    "checked": true,
    "id": "609cf03f319dcd6fa4dc5c12359f57a6b0fdfe30",
    "semantic_title": "are human conversations special? a large language model perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G3EuSXKA4n": {
    "title": "A Simple yet Effective Training-free Prompt-free Approach to Chinese Spelling Correction Based on Large Language Models",
    "volume": "review",
    "abstract": "This work proposes a simple yet effective approach for leveraging large language models (LLMs) in Chinese spelling correction (CSC) task. Our approach consists of two components: a large language model and a minimal distortion model. At each decoding step, the large language model calculates the probabilities of the next token based on the preceding context. Then, the distortion model adjusts these probabilities to penalize the generation of tokens that deviate too far from the input. Different from the prior supervised fine-tuning and prompt-based approaches, our approach enables efficient CSC without requiring additional training or task-specific prompts. To address practical challenges, we propose a length reward strategy to mitigate the local optima problem during beam search decoding, and a faithfulness reward strategy to reduce over-corrections. Comprehensive experiments on five public datasets demonstrate that our approach significantly improves LLM performance, enabling them to compete with state-of-the-art domain-general CSC models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f12k6JxhQ9": {
    "title": "DiffuseDef: Improved Robustness to Adversarial Attacks",
    "volume": "review",
    "abstract": "Pretrained language models have significantly advanced performance across various natural language processing tasks. However, adversarial attacks continue to pose a critical challenge to system built using these models, as they can be exploited with carefully crafted adversarial texts. Inspired by the ability of diffusion models to predict and reduce noise in computer vision, we propose a novel and flexible adversarial defense method for language classification tasks, DiffuseDef, which incorporates a diffusion layer as a denoiser between the encoder and the classifier. During inference, the adversarial hidden state is first combined with sampled noise, then denoised iteratively and finally ensembled to produce a robust text representation. By integrating adversarial training, denoising, and ensembling techniques, we show that DiffuseDef improves over different existing adversarial defense methods and achieves state-of-the-art performance against common adversarial attacks",
    "checked": true,
    "id": "3d5a2ef7cfa5120fb465e6de108196b567f066a9",
    "semantic_title": "diffusedef: improved robustness to adversarial attacks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6IJXQ5Iobe": {
    "title": "Goal-Oriented Dialogue Grounding over Structured Lists",
    "volume": "review",
    "abstract": "Document-grounded goal-oriented dialogue systems are designed to respond to user queries by leveraging relevant external information. Previous studies have mainly focused on handling free-form documents, often overlooking structured data such as list items, which can represent a range of nuanced semantic relations. Motivated by the observation that even advanced language models like GPT-3.5 often miss semantic cues from lists, this paper aims to enhance dialogue systems for better interpretation and use of structured lists. To this end, we introduce the List2Dial dataset, a novel benchmark to evaluate the ability of dialogue systems to respond effectively using list information. This dataset is created from unlabeled customer service documents using language models and model-based filtering processes to enhance data quality, and can be used both to fine-tune and evaluate dialogue models. Apart from directly generating responses through fine-tuning models, we further investigate the explicit use of Intermediate Steps for List (ISL) information, including list types and alignment with user background, which better reflects how humans assess list items before formulating responses. Our experimental results demonstrate that models trained on List2Dial with our ISL approach outperform baselines across various metrics. Specifically, our fine-tuned Flan-T5-XL model shows increases of 3.1% in ROUGE-L, 4.6% in correctness, 4.5% in faithfulness, and 20.6% in completeness compared to models without applying filtering and the proposed ISL method. We make our source code and dataset publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2mUNv2hCRV": {
    "title": "Evaluating Human Alignment and Model Faithfulness of LLM Rationale",
    "volume": "review",
    "abstract": "We study how well large language models (LLMs) explain their generations with rationales -- a set of tokens extracted from the input texts that reflect the decision process of LLMs. We examine LLM rationales extracted with two methods: 1) attribution-based methods that use attention or gradients to locate important tokens, and 2) prompting-based methods that guide LLMs to extract rationales using prompts. Through extensive experiments, we show that prompting-based rationales align better with human-annotated rationales than attribution-based rationales, and demonstrate reasonable alignment with humans even when model performance is poor. We additionally find that the faithfulness limitations of prompting-based methods, which are identified in previous work, may be linked to their collapsed predictions. By fine-tuning these models on the corresponding datasets, both prompting and attribution methods demonstrate improved faithfulness. Our study sheds light on more rigorous and fair evaluations of LLM rationales, especially for prompting-based ones. Code and data will be released upon paper acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lsjhpI3oi7": {
    "title": "Understanding \"Democratization\" in NLP and ML Research",
    "volume": "review",
    "abstract": "Recent improvements in natural language processing (NLP) and machine learning (ML) and increased mainstream adoption have led to researchers frequently discussing the \"democratization\" of artificial intelligence. In this paper, we seek to clarify how democratization is understood in NLP and ML publications, through large-scale mixed-methods analyses of papers using the keyword \"democra*\" published in NLP and adjacent venues. We find that democratization is most frequently used to convey (ease of) access to or use of technologies, without meaningfully engaging with theories of democratization, while research using other invocations of \"democra*\" tends to be grounded in theories of deliberation and debate. Based on our findings, we call for researchers to enrich their use of the term democratization with appropriate theory, towards democratic technologies beyond superficial access",
    "checked": true,
    "id": "9c970ab6a3f8a5c3a5b3fcff19952edaaa79d24a",
    "semantic_title": "understanding \"democratization\" in nlp and ml research",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wOQahax16Q": {
    "title": "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
    "volume": "review",
    "abstract": "Recent advances in Large Language Models (LLMs) have sparked wide interest in validating and comprehending the human-like cognitive-behavioral traits LLMs may have. These cognitive-behavioral traits include typically Attitudes, Opinions, Values (AOV). However, measuring AOV embedded within LLMs remains opaque, and different evaluation methods may yield different results. This has led to a lack of clarity on how different studies are related to each other and how they can be interpreted. This paper aims to bridge this gap by providing an overview of recent works on the evaluation of AOV in LLMs. Moreover, we survey related approaches in different stages of the evaluation pipeline in these works. By doing so, we address the potential and challenges with respect to understanding the model, human-AI alignment, and downstream application in social sciences. Finally, we provide practical insights into evaluation methods, model enhancement, and interdisciplinary collaboration, thereby contributing to the evolving landscape of evaluating AOV in LLMs",
    "checked": true,
    "id": "af61c4f2c9e4ed58d1cd3daf8cd4dbf916f5f4e7",
    "semantic_title": "the potential and challenges of evaluating attitudes, opinions, and values in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WbQLQDQF71": {
    "title": "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models",
    "volume": "review",
    "abstract": "Fine-tuning pre-trained large language models (LLMs) on a diverse array of tasks has become a common approach for building models that can solve various natural language processing (NLP) tasks. However, where and to what extent these models retain task-specific knowledge remains largely unexplored. This study investigates the task-specific information encoded in pre-trained LLMs and the effects of instruction tuning on their representations across a diverse set of over 60 NLP tasks. We use a set of matrix analysis tools to examine the differences between the way pre-trained and instruction-tuned LLMs store task-specific information. Our findings reveal that while some tasks are already encoded within the pre-trained LLMs, others greatly benefit from instruction tuning. Additionally, we pinpointed the layers in which the model transitions from high-level general representations to more task-oriented representations. This finding extends our understanding of the governing mechanisms of LLMs and facilitates future research in the fields of parameter-efficient transfer learning and multi-task learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TwHDAvQvhn": {
    "title": "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation",
    "volume": "review",
    "abstract": "Large language models (LLMs) have significantly advanced various natural language processing tasks, but deploying them remains computationally expensive. Knowledge distillation (KD) is a promising solution, enabling the transfer of capabilities from larger teacher LLMs to more compact student models. Particularly, sequence-level KD, which distills rationale-based reasoning processes instead of merely final outcomes, shows great potential in enhancing students' reasoning capabilities. However, current methods struggle with sequence-level KD under long-tailed data distributions, adversely affecting generalization on sparsely represented domains. We introduce the Multi-Stage Balanced Distillation (BalDistill) framework, which iteratively balances training data within a fixed computational budget. By dynamically selecting representative head domain examples and synthesizing tail domain examples, BalDistill achieves state-of-the-art performance across diverse long-tailed datasets, enhancing both the efficiency and efficacy of the distilled models",
    "checked": true,
    "id": "6fa3544f42bc026ac684cf6c7a8cd50f59b3ee7d",
    "semantic_title": "multi-stage balanced distillation: addressing long-tail challenges in sequence-level knowledge distillation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PeaBxTtvCG": {
    "title": "Does Vec2Text Pose a New Corpus Poisoning Threat?",
    "volume": "review",
    "abstract": "The emergence of Vec2Text --- a method for text embedding inversion --- has raised serious privacy concerns for dense retrieval systems which use text embeddings. This threat comes from the ability for an attacker with access to embeddings to reconstruct the original text. In this paper, we take a new look at Vec2Text and investigate how much of a threat it poses to the different attacks of corpus poisoning, whereby an attacker injects adversarial passages into a retrieval corpus with the intention of misleading dense retrievers. Theoretically, Vec2Text is far more dangerous than previous attack methods because it does not need access to the embedding model's weights and it can efficiently generate many adversarial passages. We show that under certain conditions, corpus poisoning with Vec2Text can pose a serious threat to dense retriever system integrity and user experience by injecting adversarial passaged into top ranked positions. Code and data are made available at github.com/anonymous",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZORfB3FK7V": {
    "title": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon",
    "volume": "review",
    "abstract": "Memorization in language models is typically treated as a homogenous phenomenon, neglecting the specifics of the memorized data. We instead model memorization as the effect of a set of complex factors that describe each sample and relate it to the model and corpus. To build intuition around these factors, we break memorization down into a taxonomy: recitation of highly duplicated sequences, reconstruction of inherently predictable sequences, and recollection of sequences that are neither. We demonstrate the usefulness of our taxonomy by using it to construct a predictive model for memorization. By analyzing dependencies and inspecting the weights of the predictive model, we find that different factors have different influences on the likelihood of memorization depending on the taxonomic category",
    "checked": true,
    "id": "abdc28cc33ff5449df9427c82c9caa7b5b79f40f",
    "semantic_title": "recite, reconstruct, recollect: memorization in lms as a multifaceted phenomenon",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=6fxBvEwIzX": {
    "title": "Beyond Sentiment: Evaluating Gendered Language Using Zero-Shot Text Classification",
    "volume": "review",
    "abstract": "Situational Judgment Tests (SJTs) present hypothetical job-related situations to assess judgment and decision-making skills. Using zero-shot text classification, we replicated previously established findings on sentiment effects and further explored the influence of gendered language on participant responses in SJTs. Our study demonstrates that negative sentiment in action statements lowers effectiveness ratings and increases response variability. Contrary to gender schema theory, we found no evidence that gender-congruent phrasing led to higher effectiveness ratings. These findings underscore the potential of zero-shot text classification for refining SJT item development and mitigating unintended biases",
    "checked": false,
    "id": "15f50249f554204afbf3c0e70b193e0ea6da1675",
    "semantic_title": "leveraging label variation in large language models for zero-shot text classification",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=tT61qT0bzF": {
    "title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models",
    "volume": "review",
    "abstract": "This paper presents a novel framework for benchmarking hierarchical gender hiring bias in Large Language Models (LLMs) for resume scoring, revealing significant issues of reverse bias and overdebiasing. Our contributions are fourfold: First, we introduce a framework using a real, anonymized resume dataset from the Healthcare, Finance, and Construction industries, meticulously used to avoid confounding factors. It evaluates gender hiring biases across hierarchical levels, including Level bias, Spread bias, Taste-based bias, and Statistical bias. This framework can be generalized to other social traits and tasks easily. Second, we propose novel statistical and computational hiring bias metrics based on a counterfactual approach, including Rank After Scoring (RAS), Rank-based Impact Ratio, Permutation Test-Based Metrics, and Fixed Effects Model-based Metrics. These metrics, rooted in labor economics, NLP, and law, enable holistic evaluation of hiring biases. Third, we analyze hiring biases in ten state-of-the-art LLMs. Six out of ten LLMs show significant biases against males in healthcare and finance. An industry-effect regression reveals that the healthcare industry is the most biased against males. GPT-4o and GPT-3.5 are the most biased models, showing significant bias in all three industries. Conversely, Gemini-1.5-Pro, Llama3-8b-Instruct, and Llama3-70b-Instruct are the least biased. The hiring bias of all LLMs, except for Llama3-8b-Instruct and Claude-3-Sonnet, remains consistent regardless of random expansion or reduction of resume content. Finally, we offer a user-friendly demo to facilitate adoption and practical application of the framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yQ6Jk7muDr": {
    "title": "Using Similarity to Evaluate Factual Consistency in Summaries",
    "volume": "review",
    "abstract": "Cutting-edge abstractive summarisers generate fluent summaries, but the factuality of the generated text is not guaranteed. Early summary factuality evaluation metrics are usually based on n-gram overlap and embedding similarity, but are reported fail to align with human annotations. Therefore, many techniques for detecting factual inconsistencies build pipelines around natural language inference (NLI) or question-answering (QA) models with additional supervised learning steps. In this paper, we revisit similarity-based metrics, showing that this failure stems from the comparison text selection and its granularity. We propose a new zero-shot factuality evaluation metric, Sentence-BERT Score (SBERTScore), which compares sentences between the summary and the source document. It outperforms widely-used word-word metrics including BERTScore and can compete with existing NLI and QA-based factuality metrics on the benchmark without needing any fine-tuning. Our experiments indicate that each technique has different strengths, with SBERTScore particularly effective in identifying correct summaries. We demonstrate how a combination of techniques is more effective in detecting various types of error",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EKXcrktzuk": {
    "title": "Language Fusion for Parameter-Efficient Cross-lingual Transfer",
    "volume": "review",
    "abstract": "Limited availability of multilingual text corpora for training language models often leads to poor performance on downstream tasks due to undertrained representation spaces for languages other than English. This 'under-representation' has motivated recent cross-lingual transfer methods to leverage the English representation space by e.g. mixing English and non-English tokens at input or extending model parameters, which in turn increases computational complexity. To address this, we introduce Fusion for Language Representations (FLARE) in adapters, a method designed to improve both the representation quality and downstream performance for languages other than English. FLARE integrates source and target language representations within the bottlenecks of low-rank LoRA adapters using lightweight linear transformations. This maintains parameter efficiency as the method does not require additional parameters, while improving transfer performance, further narrowing the performance gap to English. Another key advantage of the proposed latent representation fusion is that it does not increase the number of input tokens, thus maintaining computational efficiency. Moreover, FLARE provides flexibility to integrate various types of representations, e.g., we show that it is possible to fuse latent translations extracted from machine translation models. Our results demonstrate FLARE's effectiveness on natural language understanding tasks, reducing the performance gap to English across all tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oxQHbJN7iZ": {
    "title": "CovScore: Evaluation of Multi-Document Abstractive Title Set Generation",
    "volume": "review",
    "abstract": "This paper introduces CovScore, an automatic reference-less methodology for evaluating thematic title sets, extracted from a corpus of documents. While such extraction methods are widely used, evaluating their effectiveness remains an open question. Moreover, some existing practices heavily rely on slow and laborious human annotation procedures. Inspired by recently introduced LLM-based judge methods, we propose a novel methodology that decomposes quality into five main metrics along different aspects of evaluation. This framing simplifies and expedites the manual evaluation process and enables automatic and independent LLM-based evaluation. As a test case, we apply our approach to a corpus of Holocaust survivor testimonies, motivated both by its relevance to title set extraction and by this pursuit's moral significance. We validate the methodology by experimenting with naturalistic and synthetic title set generation systems and compare their performance with the methodology",
    "checked": true,
    "id": "99cc08c1c2923a42d0f7b4d3efb3b2c6661e4a6a",
    "semantic_title": "covscore: evaluation of multi-document abstractive title set generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vYNjVgpixS": {
    "title": "If I understand the context, I will act accordingly: Combining Complementary Information with Generative Visual Language Models",
    "volume": "review",
    "abstract": "The effectiveness of autoregressive LLMs has allowed many language and vision tasks to be reframed as generative problems. Generative visual language models (VLMs) have recently shown potential across various downstream tasks. However, it is still an open question whether, and to what extent, these models can properly understand a multimodal context where language and vision provide complementary information---a mechanism routinely in place in human language communication. In this work, we test various VLMs on the task of generating action descriptions consistent with both an image's visual content and an intention or attitude (not visually grounded) conveyed by a textual prompt. Our results show that BLIP-2 is not far from human performance when the task is framed as a generative multiple-choice problem, while other models struggle. Furthermore, the actions generated by BLIP-2 in an open-ended generative setting are better than those by the competitors; indeed, human annotators judge most of them as plausible continuations for the multimodal context. Our study reveals substantial variability among VLMs in integrating complementary multimodal information, yet BLIP-2 demonstrates promising trends across most evaluations, paving the way for seamless human-computer interaction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=riTHLxuZWR": {
    "title": "Fine-grained Controllable Text Generation through In-context Learning with Feedback",
    "volume": "review",
    "abstract": "We present a method for rewriting an input sentence to match specific values of nontrivial linguistic features, such as dependency depth. In contrast to earlier work, our method uses in-context learning rather than finetuning, making it applicable in use cases where data is sparse. We show that our model performs accurate rewrites and matches the state of the art on rewriting sentences to a specified school grade level",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RUwW2Up9PK": {
    "title": "Learning from Natural Language Explanations for Generalizable Entity Matching",
    "volume": "review",
    "abstract": "Entity matching is the task of linking records from different sources that refer to the same real-world entity. Past work has primarily treated entity linking as a standard supervised learning problem. However, supervised entity matching models often do not generalize well to new data, and collecting exhaustive labeled training data is often cost prohibitive. Further, recent efforts have adopted LLMs for this task in few/zero-shot settings, exploiting their general knowledge. But LLMs are prohibitively expensive for performing inference at scale for real-world entity matching tasks. As an efficient alternative, we re-cast entity matching as a conditional generation task as opposed to binary classification. This enables us to \"distill\" LLM reasoning into smaller entity matching models via natural language explanations. This approach achieves strong performance, especially on out-of-domain generalization tests (10.85% F-1) where standalone generative methods struggle. We perform ablations that highlight the importance of explanations, both for performance and model robustness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q2eC52ql9O": {
    "title": "Outcome-Constrained Large Language Models for Countering Hate Speech",
    "volume": "review",
    "abstract": "Counter speech (CS) that challenges or counteracts harmful or discriminatory messages is an effective way to diminish the influence of hate speech (HS). Automatic CS generation methods have been developed to assist efforts in combating online HS. Existing research focuses on generating CS with linguistic attributes, such as being polite, informative, and intent-driven. However, the real impact of CS in online environments is seldom considered. This study aims to develop methods for generating CS constrained by conversation outcomes and evaluate their effectiveness. We experiment with large language models (LLMs) to incorporate into the text generation process two desired conversation outcomes: low conversation incivility and non-hateful hater reentry. Specifically, we experiment with instruction prompts, LLM finetuning, and LLM reinforcement learning. Evaluation results show that our methods effectively steer the generation of conversational systems towards desired outcomes. Our analyses, however, show that there are differences in the quality and style of the generated CS",
    "checked": true,
    "id": "82cafec3b5af73751ccbde129d8f24a0f2acef1d",
    "semantic_title": "outcome-constrained large language models for countering hate speech",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=13FvsnmPve": {
    "title": "Analyzing Key Neurons in Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) possess vast amounts of knowledge within their parameters, prompting research into methods for locating and editing this knowledge. Previous investigations have primarily focused on fill-in-the-blank tasks and locating entity-related (\\emph{usually single-token facts}) information in relatively small-scale language models. However, several key questions remain unanswered: (1) \\textit{How can we effectively locate query-relevant neurons in contemporary autoregressive LLMs, such as LLaMA and Mistral? } (2) \\textit{How can we address the challenge of long-form text generation?} (3) \\textit{Are there localized knowledge regions in LLMs?} In this study, we introduce Neuron Attribution-Inverse Cluster Attribution (NA-ICA), a novel architecture-agnostic framework capable of identifying key neurons in LLMs. NA-ICA allows for the examination of long-form answers beyond single tokens by employing the proxy task of multi-choice question answering. To evaluate the effectiveness of our detected key neurons, we construct two multi-choice QA datasets spanning diverse domains and languages. Empirical evaluations demonstrate that NA-ICA outperforms baseline methods significantly. Moreover, analysis of neuron distributions reveals the presence of visible localized regions, particularly within different domains. Finally, we demonstrate the potential applications of our detected key neurons in knowledge editing and neuron-based prediction",
    "checked": false,
    "id": "200c78938dd9085067e93b7ab5dd8be2a041488a",
    "semantic_title": "finding safety neurons in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IQaynoIdrT": {
    "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning",
    "volume": "review",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG) have become popular methods for adapting large language models while minimizing compute requirements. In this paper, we apply PEFT methods (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) and a baseline GPT model across several sizes, ranging from 823 million to 48 billion parameters. We show that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process but GPT models have higher performance potential with PEFT. Additionally, our study indicates that 8B parameter models strike an optimal balance between cost and performance and P-tuning lags behind other PEFT techniques. We further provide a comparative analysis of between applying PEFT to Instruction-tuned RETRO model and base RETRO model. This work presents the first comprehensive comparison of various PEFT methods integrated with RAG, applied to both GPT and RETRO models, highlighting their relative performance",
    "checked": true,
    "id": "6c9548c556d44dba51746c280312c659057cf06f",
    "semantic_title": "gpt vs retro: exploring the intersection of retrieval and parameter-efficient fine-tuning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XIa0ZHgHeJ": {
    "title": "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training",
    "volume": "review",
    "abstract": "We propose an end-to-end differentiable training paradigm for stable training of a rationalized transformer classifier. Our approach results in a single model that simultaneously classifies a sample and scores input tokens based on their relevance to the classification. To this end, we build on the widely-used three-player-game for training rationalized models, which typically relies on training a rationale selector, a classifier and a complement classifier. We simplify this approach by making a single model fulfill all three roles, leading to a more efficient training paradigm that is not susceptible to the common training instabilities that plague existing approaches. Further, we extend this paradigm to produce class-wise rationales while incorporating recent advances in parameterizing and regularizing the resulting rationales, thus leading to substantially improved and state-of-the-art alignment with human annotations without any explicit supervision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bHCbpRKIBX": {
    "title": "Split and Merge: Aligning Position Biases in LLM-based Evaluators",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, taking into account both length and semantics, and merges them back into a single prompt for evaluation by LLMs. Extensive experiments with six LLMs on 11,520 answer pairs demonstrate that PORTIA markedly enhances the consistency rates for all models and forms of comparison tested, achieving an average relative improvement of 47.46\\%. It also enables GPT-3.5 to achieve performance comparable to GPT-4 and elevates GPT-4's consistency rate up to 98\\%. Subsequent human evaluations indicate that the PORTIA-enhanced GPT-3.5 model can even surpass standalone GPT-4 in terms of alignment with human evaluators, highlighting PORTIA's ability to correct position bias, improve LLM consistency, and boost performance while keeping cost efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WmEb3dKg1t": {
    "title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models",
    "volume": "review",
    "abstract": "Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available",
    "checked": true,
    "id": "ecdd53eaab7455daea27609b07a418a21aa7ad35",
    "semantic_title": "prometheus 2: an open source language model specialized in evaluating other language models",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=Wg7WJPKhJU": {
    "title": "Cascading Large Language Models for Salient Event Graph Generation",
    "volume": "review",
    "abstract": "Generating event graphs from long documents is challenging due to the inherent complexity of multiple tasks involved such as detecting events, identifying their relationships, and reconciling unstructured input with structured graphs. Recent studies typically consider all events with equal importance, failing to distinguish salient events crucial for understanding narratives. This paper presents CALLMSAE, a CAscading Large Language Model framework for SAlient Event graph generation, which leverages the capabilities of LLMs and eliminates the need for costly human annotations. We first identify salient events by prompting LLMs to generate summaries, from which salient events are identified. Next, we develop an iterative code refinement prompting strategy to generate event relation graphs, removing hallucinated relations and recovering missing edges. Fine-tuning contextualised graph generation models on the LLM-generated graphs outperforms the models trained on CAEVO-generated data. Experimental results on a human-annotated test set show that the proposed method generates salient and more accurate graphs, outperforming competitive baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O68Cpu6KHE": {
    "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling",
    "volume": "review",
    "abstract": "Long sequence modeling has gained broad interest as large language models (LLMs) continue to advance. Recent research has identified that a large portion of hidden states within the key-value caches of Transformer models can be discarded (also termed *evicted*) without affecting the perplexity performance in generating long sequences. However, we show that these methods, despite preserving perplexity performance, often drop information that is important for solving downstream tasks, a problem which we call *information neglect*. To address this issue, we introduce **C**hunked **I**ns**tru**ction-aware **S**tate Eviction (**CItruS**), a novel modeling technique that integrates the attention preferences useful for a downstream task into the eviction process of hidden states. In addition, we design a method for chunked sequence processing to further improve efficiency. Our training-free method exhibits superior performance on long sequence comprehension and retrieval tasks over several strong baselines under the same memory budget, while preserving language modeling perplexity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r7GkDVkruw": {
    "title": "ASRank: Zero-Shot Re-Ranking with Answer Scent for Document Retrieval",
    "volume": "review",
    "abstract": "Retrieval-Augmented Generation (RAG) models have drawn considerable attention in modern open-domain question answering. The effectiveness of RAG depends on the quality of the top retrieved documents. However, conventional retrieval methods sometimes fail to rank the most relevant documents at the top. In this paper, we introduce ASRank, a new re-ranking method based on scoring retrieved documents using a zero-shot answer scent, which relies on a pretrained large language model to compute the likelihood of the document-derived answers aligning with the answer scent. Our approach demonstrates marked improvements across several datasets, including NQ, TriviaQA, WebQA, ArchivalQA, HotpotQA, and Entity Questions. Notably, ASRank increases Top-1 retrieval accuracy on NQ from $19.2\\%$ to $46.5\\%$ for MSS and from $22.1\\%$ to $47.3\\%$ for BM25. Finally, ASRank shows strong retrieval performance on several datasets compared to state-of-the-art methods 47.3 Top-1 by ASRank vs 35.4 by UPR (Sachan et al., 2022) by BM25",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VAzkX8j7pd": {
    "title": "LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law",
    "volume": "review",
    "abstract": "Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs",
    "checked": true,
    "id": "93b058243165c2b2e06c6802ab12538713e167c3",
    "semantic_title": "llms learn governing principles of dynamical systems, revealing an in-context neural scaling law",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=6v8ehjE0Jn": {
    "title": "NativQA: Multilingual Culturally-Aligned Natural Query for LLMs",
    "volume": "review",
    "abstract": "Natural Question Answering (QA) datasets play a crucial role in developing and evaluating the capabilities of large language models (LLMs), ensuring their effective usage in real-world applications. Despite the numerous QA datasets that have been developed, there is a notable lack of region-specific datasets generated by native users in their own languages. This gap hinders the effective benchmarking of LLMs for regional and cultural specificities. In this study, we propose a scalable framework, NativQA, to seamlessly construct culturally and regionally aligned QA datasets in native languages, for LLM evaluation and tuning. Moreover, to demonstrate the efficacy of the proposed framework, we designed a multilingual natural QA dataset, \\mnqa, consisting of $\\sim$72K QA pairs in seven languages, ranging from high to extremly low resource, based on queries from native speakers covering 18 topics. We benchmark the MultiNativQA dataset with open- and closed-source LLMs. We made both the framework NativQA and MultiNativQA dataset publicly available for the community (anonymous.com)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XuazKRDuAS": {
    "title": "MOSEL: Inference Serving Using Dynamic Modality Selection",
    "volume": "review",
    "abstract": "Rapid advancements over the years have helped machine learning models reach previously hard-to-achieve goals, sometimes even exceeding human capabilities. However, achieving desired accuracy comes at the cost of larger model sizes and increased computational demands. Thus, serving predictions from these models to meet any latency and cost requirements of applications remains a key challenge, despite recent work in building inference serving systems as well as algorithmic approaches that dynamically adapt models based on inputs. Our paper introduces a new form of dynamism, modality selection, where we adaptively choose modalities from inference inputs while maintaining the model quality. We introduce MOSEL, an automated inference serving system for multi-modal ML models that carefully picks input modalities per request based on user-defined performance and accuracy requirements. MOSEL exploits modality configurations extensively, improving system throughput by 3.6$\\times$ with an accuracy guarantee and shortening job completion times by 11$\\times$ compared to modality-agnostic approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rZ5IcL2i4d": {
    "title": "Learning Peer Support Interactions via Bi-LSTM Graph Neural Networks for Suicide Risk Prediction",
    "volume": "review",
    "abstract": "Suicide prevention through early detection using social media data has been widely studied. However, the critical role of peer supportâ€”interactions among individuals with similar mental disordersâ€”has not been deeply investigated or exploited. In this study, we explore peer interactions in online communities for individuals with bipolar disorder and leverage this information to predict suicide risk levels. We propose a model that uses contextualized posts and comments along with their sentiment features. By embedding these features into a peer support network, our model captures peer interactions and predicts suicide risk levels using bidirectional LSTM Graph Neural Networks (Bi-LSTM GNNs). Experimental results demonstrate the effectiveness of our approach, outperforming baseline methods. Our findings highlight the importance of peer comments in predicting suicide risk",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cv8VnWNjmS": {
    "title": "RepMatch: Quantifying Cross-Instance Similarities in Representation Space",
    "volume": "review",
    "abstract": "Advancements in dataset analysis methods have led to the development of ways to analyze and categorize training data instances. These methods categorize the data based on specific features like \"difficulty\". We propose a framework that categorizes data from a viewpoint of similarity. This framework quantifies the similarities between subsets of training instances by comparing the models trained on them. This approach addresses the limitations of existing methodologies that focus on individual instances and are confined to single-dataset analyses. Our method enables the evaluation of similarities among arbitrary subsets of instances, facilitating both dataset-dataset and instance-dataset analyses. To compare two models efficiently, we leverage the Low-Rank Adaptation (LoRA) method. The effectiveness of our method has been validated across various NLP tasks, datasets, and models. The method can be used to compare datasets, find a smaller subset that outperforms a randomly selected subset of the same size, and successfully uncovers heuristics used in the construction of a challenge dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gliHPIEo87": {
    "title": "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection",
    "volume": "review",
    "abstract": "Grammatical Error Detection (GED) methods rely heavily on human annotated error corpora. However, these annotations are unavailable in many low-resource languages. In this paper, we investigate GED in this context. Leveraging the zero-shot cross-lingual transfer capabilities of multilingual pre-trained language models, we train a model using data from a diverse set of languages to generate synthetic errors in other languages. These synthetic error corpora are then used to train a GED model. Specifically we propose a two-stage fine-tuning pipeline where the GED model is first fine-tuned on multilingual synthetic data from target languages followed by fine-tuning on human-annotated GED corpora from source languages. This approach outperforms current state-of-the-art annotation-free GED methods. We also analyse the errors produced by our method and other strong baselines, finding that our approach produces errors that are more diverse and more similar to human errors",
    "checked": true,
    "id": "ce5496b880a15d05c2d046b475c20b6e26a68a9f",
    "semantic_title": "zero-shot cross-lingual transfer for synthetic data generation in grammatical error detection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E5RaPdrGz7": {
    "title": "RETHiNK: Simulate Human Decision-making via Multi-LLM Debates in Cognitive Reframing",
    "volume": "review",
    "abstract": "Cognitive Reframing is a therapeutic intervention aimed at transforming negative thoughts into more optimistic perspectives, often under the guidance of a trained therapist. However, due to limited access to traditional therapy and the challenges of self-reflection, there is an increasing demand for AI-assisted solutions. While prior studies have demonstrated the effectiveness of Large Language Models (LLMs) in cognitive reframing, there has been limited exploration of explicitly integrating human-like reasoning into reframing generation process. To fill this gap, we introduce \\textit{\\textbf{RETHiNK}}, a system that leverages multi-LLM debates to emulate the reasoning process that people use for self-guided cognitive reframing. This system also incorporates an iterative refinement mechanism with customized feedback to assess five attributes, such as overpositivity and empathy, ensuring the quality of the final output. Our results indicate that RETHiNK outperforms baseline models and references in empathy, overpositivity and actionability in automatic evaluation, as well as helpfulness, empathy and rationality in human and GPT-4 evaluations. The code will be released publicly shortly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ReH45Ha5Em": {
    "title": "Latent Concept-based Explanation of NLP Models",
    "volume": "review",
    "abstract": "Interpreting and understanding the predictions made by deep learning models poses a formidable challenge due to their inherently opaque nature. Many previous efforts aimed at explaining these predictions rely on input features, specifically, the words within NLP models. However, such explanations are often less informative due to the discrete nature of these words and their lack of contextual verbosity. To address this limitation, we introduce the Latent Concept Attribution method (LACOAT), which generates explanations for predictions based on latent concepts. Our foundational intuition is that a word can exhibit multiple facets, contingent upon the context in which it is used. Therefore, given a word in context, the latent space derived from our training process reflects a specific facet of that word. LACOAT functions by mapping the representations of salient input words into the training latent space, allowing it to provide latent context-based explanations of the prediction",
    "checked": true,
    "id": "315773f41083f1e9c790168a55d40c79b7117c43",
    "semantic_title": "latent concept-based explanation of nlp models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jCXIj8HPWX": {
    "title": "An Efficient Rehearsal Scheme for Catastrophic Forgetting Mitigation during Multi-stage Fine-tuning",
    "volume": "review",
    "abstract": "Incrementally fine-tuning foundational models on new tasks or domains is now the de facto approach in NLP. A known pitfall of this approach is the \\emph{catastrophic forgetting} of prior knowledge that happens during fine-tuning. A common approach to alleviate such forgetting is to rehearse samples from prior tasks during fine-tuning. Several existing works assume a fixed memory buffer to store prior task examples, while relying on inferences (forward passes) with the model at hand for choosing examples for rehearsal from the buffer. However, given the increasing computational cost of model inference, and decreasing cost of data storage, we focus on the setting to rehearse samples with a fixed computational budget instead of a fixed memory budget. We propose a sampling scheme, \\texttt{\\bf mix-cd}, that prioritizes rehearsal of ``collateral damage'' samples, which are samples predicted correctly by the prior model but forgotten by the incrementally tuned one. The crux of our scheme is a procedure to efficiently estimate the density of collateral damage samples without incurring additional model inferences. Our approach is computationally efficient, easy to implement, and outperforms several leading continual learning methods in compute constrained settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q78OATOGjv": {
    "title": "Evaluating LLMs for Detecting Climate Misinformation: How Aligned are LLMs with Expert Classification of False or Misleading Claims about Climate Change on Social Media?",
    "volume": "review",
    "abstract": "Online dis/misinformation is a social problem that has the potential to be substantially aggravated by the development of Large Language Models (LLMs). In this study we evaluate the potential for LLMs to be part of the solution for mitigating online dis/misinformation rather than the problem. Employing a public expert annotated dataset and a curated sample of social media content we evaluate the performance of GPT-4 and fine-tuned GPT-3.5-turbo on climate misinformation classification task, comparing them to existing climate-focused computer-assisted tools and expert assessments. Results show that fine-tuned GPT-3.5-turbo outperforms GPT-4 and a BERT-based model for classifying climate misinformation and functions at the equivalency of climate change experts with over 20 years of experience in climate communication. These findings highlight the potential of fine-tuned LLMs in 1) facilitating civil society organizations with limited technical expertise to engage in a range of governance tasks with respect to climate misinformation, and 2) encourage further exploration of AI-driven solutions for detecting climate dis/misinformation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vTQeBcMhvq": {
    "title": "Evaluating Transparency of Machine Generated Fact Checking Explanations",
    "volume": "review",
    "abstract": "An important factor when it comes to generating fact-checking explanations is the selection of evidence: intuitively, high-quality explanations can only be generated given the right evidence. In this work, we investigate the impact of human-curated vs. machine-selected evidence for explanation generation using large language models. To assess the quality of explanations, we focus on transparency (whether an explanation cites sources properly) and utility (whether an explanation is helpful in clarifying a claim). Surprisingly, we found that large language models generate similar or higher quality explanations using machine-selected evidence, suggesting carefully curated evidence (by humans) may not be necessary. That said, even with the best model, the generated explanations are not always faithful to the sources, suggesting further room for improvement in explanation generation for fact-checking",
    "checked": true,
    "id": "1d2906ed4aaaf01d4d451cccb1568734d64d2126",
    "semantic_title": "evaluating transparency of machine generated fact checking explanations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I7KZoCV50T": {
    "title": "Dx-LLM: Two-layer Retrieval-Augmented Multilingual Diagnosis System",
    "volume": "review",
    "abstract": "Automatic diagnosis (AD) represents a pivotal area in healthcare, where patient symptoms are analyzed for disease diagnosis. Traditional approaches depend on extracting features from symptoms and diseases within collected patient cases. However, real-life patient data collection poses challenges, often resulting in incomplete clinical datasets that can lead to misdiagnosis, especially for new diseases or unrecorded symptoms. Recently, retrieval-augmented large language models (RA-LLMs) have shown significant promise in addressing knowledge-intensive Natural Language Processing (NLP) tasks. To mitigate reliance on previously seen data, we propose a two-layer AD system, termed Dx-LLM, leveraging RA-LLMs. Dx-LLM first constructs a disease-symptom knowledge graph from an external dataset of disease symptom descriptions and conducts initial disease filtering to identify potential candidate diseases based on patient symptoms. Subsequently, in the second layer, we utilize the robust language understanding and generation capabilities of LLMs to re-rank these candidates, thereby producing refined diagnostic outcomes. This two-layer approach reduces the computational load on the second-layer LLM by narrowing down the disease candidates in the first layer. Our results demonstrate that Dx-LLM achieves hit@10 scores of 71.41\\% and 70.38\\% across 1058 diseases in English and Chinese datasets, consistently outperforming state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gsxK1vWUR6": {
    "title": "Generating and Evaluating Synthetic Data for Privacy Preservation in High-Stakes Domains",
    "volume": "review",
    "abstract": "The difficulty of anonymizing text data hinders the development and deployment of NLP in high-stakes domains that involve private data, such as healthcare and social services. Poorly anonymized sensitive data cannot be easily shared with annotators or external researchers, nor can it be used to train public models. In this work, we develop methods to generate and evaluate synthetic data to facilitate the development of NLP in these domains without compromising privacy. We use language models fine-tuned with differential privacy to generate data and incorporate NLI-based filtering to improve text coherence. In contrast to prior work, we generate and evaluate data for fine-grained applications in real high-stakes domains. Our results show that prior simplistic evaluations have failed to highlight utility, privacy, and fairness issues in the synthetic data generated, and while NLI-based filtering can help alleviate some of these weaknesses, the quality of the synthetic data generated still necessitates further improvements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WVaWlamdYt": {
    "title": "On the token distance modeling ability of higher RoPE attention dimension",
    "volume": "review",
    "abstract": "Length extrapolation algorithms based on Rotary position embedding (RoPE) have shown promising results in extending the context length of language models. However, understanding how position embedding can capture longer-range contextual information remains elusive. Based on the intuition that different dimensions correspond to different frequency of changes in RoPE encoding, we conducted a dimension-level analysis to investigate the correlation between a hidden dimension of an attention head and its contribution to capturing long-distance dependencies. Using our correlation metric, we identified a particular type of attention heads, which we named \\emph{Positional Heads}, from various length-extrapolated models. These heads exhibit a strong focus on long-range information interaction and play a pivotal role in long input processing, as evidence by our ablation. We further demonstrate the correlation between the efficiency of length extrapolation and the extension of the high-dimensional attention allocation of these heads. The identification of Positional Heads provides insights for future research in long-text comprehension",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GbXKiJyt1w": {
    "title": "MoCoKGC: Momentum Contrast Entity Encoding for Knowledge Graph Completion",
    "volume": "review",
    "abstract": "In recent years, numerous studies have sought to enhance the capabilities of pretrained language models (PLMs) for Knowledge Graph Completion (KGC) tasks by integrating structural information from knowledge graphs. However, existing approaches have not effectively combined the structural attributes of knowledge graphs with the textual descriptions of entities to generate robust entity encodings. To address this issue, this paper proposes MoCoKGC (Momentum Contrast Entity Encoding for Knowledge Graph Completion), which incorporates three primary encoders: the entity-relation encoder, the entity encoder, and the momentum entity encoder. Momentum contrast learning not only provides more negative samples but also allows for the gradual updating of entity encodings. Consequently, we reintroduce the generated entity encodings into the encoder to incorporate the graph's structural information. Additionally, MoCoKGC enhances the inferential capabilities of the entity-relation encoder through deep prompts of relations. On the standard evaluation metric, Mean Reciprocal Rank (MRR), the MoCoKGC model demonstrates superior performance, achieving a 7.1% improvement on the WN18RR dataset and an 11% improvement on the Wikidata5M dataset, while also surpassing the current best model on the FB15k-237 dataset. Through a series of experiments, this paper thoroughly examines the role and contribution of each component and parameter of the model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ONmOTVvY8Y": {
    "title": "Faithfulness and Content Selection in Long-Input Multi-Document Summarisation of U.S. Civil Rights Litigation",
    "volume": "review",
    "abstract": "Automatic summarisation of legal cases would reduce the burden on legal professionals and increase the accessibility of the law. However, the abstractive methods which dominate recent research are prone to hallucination. Despite the fact that this is a barrier to practical use, preventing hallucination is currently an understudied area in the legal domain. We conduct the first study at the intersection of legal, multi-document, and faithful summarisation. In particular, by introducing a BERT-based content selection mechanism, we achieve an improvement of 0.2614 in the probability of a generated summary being entailed by its source text compared to a naÃ¯ve content selection baseline, and observe qualitative improvements. Further, we demonstrate possible improvements of 5.56 ROUGE-1 F1, 5.46 ROUGE-2 F1, 2.7 ROUGE-L F1, and 2.15 BERTScore over the state-of-the-art if a perfectly predictive classifier was used, demonstrating the importance of content selection for summary faithfulness and quality for long-input legal abstractive summarisation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xcpHu030aY": {
    "title": "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models",
    "volume": "review",
    "abstract": "In-context learning methods are popular inference strategies where Large Language Models (LLMs) are elicited to solve a task using provided demonstrations without parameter updates. Among these approaches are the reasoning methods, best exemplified by Chain-of-Thought (CoT) and Program-Aided Language Models (PAL), which elicit LLMs to generate reasoning paths, thus promoting accuracy and attracting increasing attention. However, despite the success of these methods, the ability to deliver multi-step reasoning remains limited to a single language, making it challenging to generalize to other languages and hindering global development. In this work, we propose Cross-lingual Program-Aided Language Models (CrossPAL), a method for aligning reasoning programs across languages. In particular, our method delivers programs as intermediate reasoning steps in different languages through a double-step cross-lingual prompting mechanism inspired by the Program-Aided approach. In addition, we introduce Self-consistent CrossPAL (SCrossPAL) to ensemble different reasoning paths across languages. Our experimental evaluations show that our method significantly outperforms existing prompting methods, reducing the number of interactions and achieving state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PtkLLWQeRe": {
    "title": "Better Call SAUL: Fluent and Consistent Language Model Editing with Generation Regularization",
    "volume": "review",
    "abstract": "To ensure large language models contain up-to-date knowledge, they need to be updated regularly. However, model editing is challenging as it might also affect knowledge that is unrelated to the new data. State-of-the-art methods identify parameters associated with specific knowledge and then modify them via direct weight updates. However, these locate-and-edit methods suffer from heavy computational overhead and lack theoretical validation. In contrast, directly fine-tuning the model on requested edits affects the model's behavior on unrelated knowledge, and significantly damages the model's generation fluency and consistency. To address these challenges, we propose SAUL, a streamlined model editing method that uses **s**entence concatenation with **au**gmented random facts for generation regu**l**arization. Evaluations on three model editing benchmarks show that \\saul is a practical and reliable solution for model editing outperforming state-of-the-art methods while maintaining generation quality and reducing computational overhead",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=obzgclkKYQ": {
    "title": "Consistent Autoformalization for Constructing Mathematical Libraries",
    "volume": "review",
    "abstract": "Autoformalization is the task of automatically translating mathematical content written in natural language to a formal language expression. The growing language interpretation capabilities of Large Language Models (LLMs), including in formal languages, are lowering the barriers for autoformalization. However, LLMs alone are not capable of consistently and reliably delivering autoformalization, in particular as the complexity and specialization of the target domain grows. As the field evolves into the direction of systematically applying autoformalization towards large mathematical libraries, the need to improve syntactic, terminological and semantic control increases. This paper proposes the coordinated use of three mechanisms, most-similar retrieval augmented generation (MS-RAG), denoising steps and auto-correction with syntax error feedback (Auto-SEF) to improve autoformalization quality. The empirical analysis, across different models, demonstrates that these mechanisms can deliver autoformalizaton results which are syntactically, terminologically and semantically more consistent. These mechanisms can be applied across different LLMs and have shown to deliver improve results across different model types",
    "checked": false,
    "id": "788381f0055bd61bc7600d79ce9e279ed92f25a0",
    "semantic_title": "arima-models: modeling and forecasting prices of stocks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3UnUxUYR6A": {
    "title": "Exploring Human-AI Perception Alignment in Sensory Experiences: Do LLMs Understand Textile Hand?",
    "volume": "review",
    "abstract": "Aligning LLMs behaviour with human intent is critical for future AI. An important yet often overlooked aspect of this alignment is the perceptual alignment. Perceptual modalities like touch are more multifaceted and nuanced compared to other sensory modalities such as vision. This work investigates how well LLMs align with human touch experiences. We created an interaction in which participants were given two textile samples to handle without seeing them and describe the differences between them to the LLM. Using these descriptions, the LLM attempted to identify the target textile by assessing similarity within its high-dimensional embedding space. Our results suggest that a degree of perceptual alignment exists, however varies significantly among different textile samples. Moreover, participants didn't perceive their textile experiences closely matched by the LLM predictions. We discuss possible sources of this alignment variance, and how better human-AI perceptual alignment can benefit future everyday tasks",
    "checked": true,
    "id": "679958aff3317fb1bcad6fbbb0ba642f1700857c",
    "semantic_title": "exploring human-ai perception alignment in sensory experiences: do llms understand textile hand?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ddmw1vaOd1": {
    "title": "Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM's understanding of the world, enabling it to provide traceable gold knowledge. This knowledge base is separate from the LLM and initially empty. It can be filled with validated knowledge and progressively expanded. When an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently. Additionally, we introduce a method for automatically and efficiently expanding the knowledge base of LLMs. Through qualitative and quantitative analysis, we demonstrate that our approach enhances the controllability and reliability of LLMs",
    "checked": true,
    "id": "01bf8ade86d33ee2fe08780a51bdf59f9ee171b6",
    "semantic_title": "learn to refuse: making large language models more controllable and reliable through knowledge scope limitation and refusal mechanism",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=AJTtDe96tS": {
    "title": "xTower: A Multilingual LLM for Explaining and Correcting Translation Errors",
    "volume": "review",
    "abstract": "While machine translation (MT) systems are achieving increasingly strong performance on benchmarks, they often produce translations with errors and anomalies. Understanding these errors can potentially help improve the translation quality and user experience. This paper introduces xTower, an open large language model (LLM) built on top of TowerBase designed to provide free-text explanations for translation errors in order to guide the generation of a corrected translation. The quality of the generated explanations by xTower are assessed via both intrinsic and extrinsic evaluation. We ask expert translators to evaluate the quality of the explanations across two dimensions: relatedness towards the error span being explained and helpfulness in error understanding and improving translation quality. Extrinsically, we test xTower across various experimental setups in generating translation corrections, demonstrating significant improvements in translation quality. Our findings highlight xTower's potential towards not only producing plausible and helpful explanations of automatic translations, but also leveraging them to suggest corrected translations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qoK5NRj9Ae": {
    "title": "Fact or Fiction? Exploring Diverse Approaches to Fact Verification with Language Models",
    "volume": "review",
    "abstract": "Recent advancements in natural language processing (NLP) have greatly improved the performance of language reasoning and generating. However, a well known shortcoming of language models is that they tend to generate information that is untrue, referred to as \\emph{hallucinations}. In order to help advance the correctness of language models, we improve the performance and the computational efficiency of models trained on classifying claims as true or false. We use the \\textsc{FactKG} dataset, which is constructed from the \\emph{DBpedia} knowledge graph extracted from Wikipedia. We create fine-tuned text models and hybrid models using graphs and text that significantly outperform the benchmark \\textsc{FactKG} models and all other known approaches, both with respect to test-set accuracy and training time. The increase in performance and efficiency stems from simplifying the methods for retrieving subgraphs, using simple logical retrievals rather than fine-tuned language models. Finally, we construct prompts to ChatGPT 4o that achieves decent performance, but without the need of fine-tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TC7nGHC1rb": {
    "title": "Generative evaluation for contextual machine translation",
    "volume": "review",
    "abstract": "Despite the fact that context is known to be vital for resolving a range of translation ambiguities, most traditional machine translation systems continue to be trained and to operate at the sentence level. This limitation is an inherent performance ceiling that is increasingly glaring compared to their natively-contextual LLM counterparts. A common explanation is the lack of document-level annotations for existing training data. This work investigates whether having such annotations would be helpful for training traditional MT systems at scale. Working with a private parallel and monolingual data set, we build large-scale, state-of-the-art contextual MT systems into German, French, and Russian. We find that these systems are harmed when including contextual training examples sourced from mined parallel bitext. We also show that these improvements are invisible when using contrastive score-based test sets; instead, models must be tested directly on their ability to generate correct outputs, or with standard metrics on discourse-dense test sets. This provides evidence that mined parallel bitext does not contain reliable contextual signals---perhaps because it was translated in a sentence-level manner. Where possible, we repeat our results on public data",
    "checked": false,
    "id": "8c835daaf7720a168e5d3d669f419765c510bbaf",
    "semantic_title": "towards explainable evaluation metrics for machine translation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=6AjeDjlg3d": {
    "title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
    "volume": "review",
    "abstract": "As Large Language Models (LLMs) have become more advanced, they have outpaced the ability to accurately evaluate them. Not only is finding data to adequately probe particular model properties difficult, but evaluating the correctness of a model's free-form generation alone is a challenge. To address this, many evaluations now rely on LLMs themselves to judge the quality of outputs from other LLMs, typically using a single large model like GPT-4. While this method has grown in popularity, it is costly, introduces intra-model bias, and in this work, we find that the largest models are often unnecessary. Instead, we propose evaluation with a Panel of LLm evaluators (PoLL) composed of a larger number of smaller models. Across three distinct judge settings and spanning six different datasets, we find that PoLL outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r6ZV4ufZXO": {
    "title": "Do Language Differences Lead to Ethical Bias in LLMs? Exploring Dilemmas with the MSQAD and Statistical Hypothesis Tests",
    "volume": "review",
    "abstract": "Despite the recent strides in large language models, studies have underscored the existence of social biases within these systems. In this paper, we delve into the validation and comparison of the multilingual biases of LLM concerning globally discussed and potentially sensitive topics, hypothesizing that these biases may arise from language-specific distinctions. Introducing the Multilingual Sensitive Questions \\& Answers Dataset (MSQAD), we compiled news articles from Human Rights Watch covering 17 topics, and generated socially sensitive and controversial questions along with corresponding responses in multiple languages. We scrutinized the biases of these responses across languages and topics, employing various statistical hypothesis tests. The results showed that the null hypotheses were rejected in most cases, indicating a notable cross-language bias. It demonstrates the widespread prevalence of informational bias in responses across diverse languages. By making the proposed MSQAD openly available, we aim to facilitate future research endeavors focused on examining cross-language biases in LLMs and their variant models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lL3I8I70fM": {
    "title": "Socratic Human Feedback (SoHF): Understanding Socratic Feedback Based Steering Strategies Used by Expert Programmers for Code-generation with LLMs",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are increasingly used for generating code solutions, empowered by features like self-debugging and self-reflection. However, LLMs often struggle with complex programming problems without human guidance. This paper investigates the strategies employed by expert programmers to steer code-generating LLMs toward successful outcomes. Through a study involving experts using natural language to guide GPT-4, Gemini Ultra, and Claude Opus on highly difficult programming challenges, we frame our analysis using the \"Socratic Feedback\" paradigm for understanding effective steering strategies. By analyzing 30 conversational transcripts across all three models, we map observed feedback strategies to five stages of Socratic Questioning: Definition, Elenhus, Maieutic, Dialectic, and Counter-factual reasoning. We find evidence that by employing a combination of different Socratic feedback strategies across multiple turns, programmers successfully guided the models to solve 58\\% of the problems that the models initially failed to solve on their own",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6WtzwfinXv": {
    "title": "Reflective Human-Machine Co-adaptation for Enhanced Text-to-Image Generation Dialogue System",
    "volume": "review",
    "abstract": "Today's image generation systems are capable of producing realistic and high-quality images. However, user prompts often contain ambiguities, making it difficult for these systems to interpret users' potential intentions. Consequently, machines need to interact with users multiple rounds to better understand users' intents. The unpredictable costs of using or learning image generation models through multiple feedback interactions hinder their widespread adoption and full performance potential, especially for non-expert users. In this research, we aim to enhance the user-friendliness of our image generation system. To achieve this, we propose a reflective human-machine co-adaptation strategy, named RHM-CAS. Externally, the Agent engages in meaningful language interactions with users to reflect on and refine the generated images. Internally, the Agent tries to optimize the policy based on user preferences, ensuring that the final outcomes closely align with user preferences. Various experiments on different tasks demonstrate the effectiveness of the proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jhEgpbkK5D": {
    "title": "TF-Attack: Transferable and Fast Adversarial Attacks on Large Language Models",
    "volume": "review",
    "abstract": "With the great advancements in large language models (LLMs), adversarial attacks against LLMs have recently attracted increasing attention. We found that pre-existing adversarial attack methodologies exhibit limited transferability and are notably inefficient, particularly when applied to LLMs. In this paper, we analyze the core mechanisms of previous predominant adversarial attack methods, revealing that 1) the distributions of importance score differ markedly among victim models, restricting the transferability; 2) the sequential attack processes induces substantial time overheads. Based on the above two insights, we introduce a new scheme, named TF-Attack, for Transferable and Fast adversarial attacks on LLMs. TF-Attack employs an external LLM as a third-party overseer rather than the victim model to identify critical units within sentences. Moreover, TF-Attack introduces the concept of Importance Level, which allows for parallel substitutions of attacks. We conduct extensive experiments on 6 widely adopted benchmarks, evaluating the proposed method through both automatic and human metrics. Results show that our method consistently surpasses previous methods in transferability and delivers significant speed improvements, up to 20$\\times$ faster than earlier attack strategies",
    "checked": false,
    "id": "05f999abc2c60fdc8d043376aa2924177d4412c0",
    "semantic_title": "efficient generation of targeted and transferable adversarial examples for vision-language models via diffusion models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FedLpkdOjf": {
    "title": "CMMaTH: A Chinese Multi-modal Math Skill Evaluation Benchmark for Foundation Models",
    "volume": "review",
    "abstract": "Due to the rapid advancements in multimodal large language models, evaluating their multimodal mathematical capabilities continues to receive wide attention. Despite the datasets like MathVista proposed benchmarks for assessing mathematical capabilities in multimodal scenarios, there is still a lack of corresponding evaluation tools and datasets for fine-grained assessment in the context of K12 education in Chinese language. To systematically evaluate the capability of multimodal large models in solving Chinese multimodal mathematical problems, we propose a Chinese Multi-modal Math Skill Evaluation Benchmark, named CMMaTH, contraining 23k multimodal K12 math related questions, forming the largest Chinese multimodal mathematical problem benchmark to date. CMMaTH questions from elementary to high school levels, provide increased diversity in problem types, solution objectives, visual elements, detailed knowledge points, and standard solution annotations. We have constructed an open-source tool GradeGPT integrated with the CMMaTH dataset, facilitating stable, rapid, and cost-free model evaluation. Our data and code are available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AFzzIOwQ6B": {
    "title": "Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods",
    "volume": "review",
    "abstract": "Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs. Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width vs. depth) concerning their impact on LLM inference efficiency. In this work, we show that simple depth pruning can effectively compress LLMs while achieving comparable or superior performance to recent width pruning studies. Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective. In retraining pruned models for quality recovery, continued pretraining on a large corpus markedly outperforms LoRA-based tuning, particularly at severe pruning ratios. We hope this work can help build compact yet capable LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5LEGB38tqn": {
    "title": "Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion",
    "volume": "review",
    "abstract": "Recent work in mechanistic interpretability of language models (LMs) has established that fact completion is mediated by localized computations. However, these findings rely on the assumption that the same computations occur for all predictions, as long as the model is accurate, and aggregate results for these. Meanwhile, a parallel body of work has shown that accurate fact completions can result from various inference processes, including predictions based on superficial properties of the query or even pure guesswork. In this paper, we present a taxonomy of relevant prediction mechanisms and observe that a well-known dataset for interpreting the inference process of LMs for fact completion misses important distinctions in this taxonomy. With this in mind, we propose a model-specific recipe for constructing precise testing data, which we call PrepMech. We use this data to investigate the sensitivity of a popular interpretability method, causal tracing (CT), to different prediction mechanisms. We find that while CT produces different results for different mechanisms, aggregations are only representative of the mechanism that corresponds to the strongest signal. In summary, we contribute tools for a more granular study of fact completion in language models and analyses that provide a more nuanced understanding of the underlying mechanisms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6nVnX7fpQa": {
    "title": "Enhancing Data Privacy in Large Language Models through Private Association Editing",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are powerful tools with extensive applications, but their tendency to memorize private information raises significant concerns as private data leakage can easily happen. In this paper, we introduce Private Association Editing (PAE), a novel defense approach for private data leakage. PAE is designed to effectively remove Personally Identifiable Information (PII) without retraining the model. Our approach consists of a four-step procedure: detecting memorized PII, applying PAE cards to mitigate memorization of private data, verifying resilience to targeted data extraction (TDE) attacks, and ensuring consistency in the post-edit LLMs. The versatility and efficiency of PAE, which allows for batch modifications, significantly enhance data privacy in LLMs. Experimental results demonstrate the effectiveness of PAE in mitigating private data leakage. We believe PAE will serve as a critical tool in the ongoing effort to protect data privacy in LLMs, encouraging the development of safer models for real-world applications",
    "checked": true,
    "id": "4961434623fc2b9d8a48fcfdf21d2c53ada6d16d",
    "semantic_title": "enhancing data privacy in large language models through private association editing",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=hE7DgtEia2": {
    "title": "aCAT: Automatically Choosing Anchor Tokens in prompt for Natural Language Understanding",
    "volume": "review",
    "abstract": "P-tuning has demonstrated that anchor tokens are beneficial for improving the performance of downstream tasks. However, selecting anchor tokens manually may result in subjective or suboptimal results. In this paper, we present aCat to choose anchor tokens automatically. Following the framework of the soft-hard prompt paradigm, aCat achieves the automatic construction of prompt templates. Experiments conducted on natural language understanding benchmarks demonstrate the effectiveness of our proposed method. On the seven datasets of SuperGlue, the proposed method has higher accuracy than the P-tuning, and the average accuracy is higher than P-tuning V2",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qg0E3gI8lD": {
    "title": "Aligning Large Language Models with Diverse Political Viewpoints",
    "volume": "review",
    "abstract": "Large language models such as ChatGPT often exhibit striking political biases. If users query them about political information, they might take a normative stance and reinforce such biases. To overcome this, we align LLMs with diverse political viewpoints from 100,000 comments written by candidates running for national parliament in Switzerland. Such aligned models are able to generate more accurate political viewpoints from Swiss parties compared to commercial models such as ChatGPT. We also propose a procedure to generate balanced overviews from multiple viewpoints using such models",
    "checked": true,
    "id": "232469e4195759c6f2e3f2ff383d0f7e8dbe433c",
    "semantic_title": "aligning large language models with diverse political viewpoints",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=GlZpVV2KgN": {
    "title": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models",
    "volume": "review",
    "abstract": "Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge, and numerous benchmarks are proposed for hallucination detection. Nevertheless, some of these benchmarks are not naturally generated by LLMs but are intentionally induced. Also, many merely focus on the factuality hallucination while ignoring the faithfulness hallucination. Additionally, although dialogue pattern is more widely utilized in the era of LLMs, current benchmarks only concentrate on sentence-level and passage-level hallucination. In this study, we propose DiaHalu, the first dedicated dialogue-level hallucination evaluation benchmark for LLMs to our knowledge. Initially, we integrate the collected topics into system prompts and facilitate a dialogue between two LLMs. Subsequently, we manually modify the contents that do not adhere to human language conventions and then have LLMs re-generate, simulating authentic human-machine interaction scenarios. Finally, professional scholars annotate all the samples in the dataset. DiaHalu covers four common multi-turn dialogue domains and five hallucination subtypes, extended from factuality and faithfulness hallucination. Experiments through some well-known LLMs and detection methods on the dataset show that DiaHalu is a challenging benchmark, holding significant value for further research",
    "checked": true,
    "id": "83d81e31f5c32f6989d98be1133adfc08db094ce",
    "semantic_title": "diahalu: a dialogue-level hallucination evaluation benchmark for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5pk7CflVsb": {
    "title": "Are Large Language Models More Empathetic than Humans?",
    "volume": "review",
    "abstract": "With the emergence of large language models (LLMs), investigating if they can surpass humans in areas such as emotion recognition and empathetic responding has become a focal point of research. This paper presents a comprehensive study exploring the empathetic responding capabilities of four state-of-the-art LLMs: GPT-4, LLaMA-2-70B-Chat, Gemini-1.0-Pro, and Mixtral-8x7B-Instruct in comparison to a human baseline. We engaged 1,000 participants in a between-subjects user study, assessing the empathetic quality of responses generated by humans and the four LLMs to 2,000 emotional dialogue prompts meticulously selected to cover a broad spectrum of 32 distinct positive and negative emotions. Our findings reveal a statistically significant superiority of the empathetic responding capability of LLMs over humans. GPT-4 emerged as the most empathetic, marking approximately 31% increase in responses rated as \"Good\" compared to the human benchmark. It was followed by LLaMA-2, Mixtral-8x7B, and Gemini-Pro, which showed increases of approximately 24%, 21%, and 10% in \"Good\" ratings, respectively. We further analyzed the response ratings at a finer granularity and discovered that some LLMs are significantly better at responding to specific emotions compared to others. The suggested evaluation framework offers a scalable and adaptable approach for assessing the empathy of new LLMs, avoiding the need to replicate this study's findings in future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H4faeJKJAo": {
    "title": "Can LLMs provide Recommendations to support Policy Making and Agency Operations?",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have provided incredible tools when it comes to text generation. These generative capabilities bring us to a point where LLMs can provide useful insights in policy making or agency operations. In this paper, we introduce a new task consisting of generating recommendations which can be used to inform future actions and improvements of agencies work within private and public organisations. The paper presents the first benchmark and coherent evaluation for developing recommendation systems to inform organisation policies. This task is clearly different from usual product or user recommendation systems, but rather aims at providing a basis to suggest policy improvements based on the conclusions drawn from reports. Our results demonstrate that state-of-the-art LLMs have the potential to emphasize and reflect on key issues and learning points within generated recommendations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TOIIhbQmUL": {
    "title": "Learning to Rewrite: Generalized LLM-Generated Text Detection",
    "volume": "review",
    "abstract": "Large language models (LLMs) can be abused at scale to create non-factual content and spread disinformation. Detecting LLM-generated content is essential to mitigate these risks, but current classifiers often fail to generalize in open-world contexts. Prior work shows that LLMs tend to rewrite LLM-generated content less frequently, which can be used for detection and naturally generalizes to unforeseen data. However, we find that the rewriting edit distance between human and LLM content can be indistinguishable across domains, leading to detection failures. We propose training an LLM to rewrite input text, producing minimal edits for LLM-generated content and more edits for human-written text, deriving a distinguishable and generalizable edit distance difference across different domains. Experiments on text from 21 independent domains and three popular LLMs (e.g., GPT-4o, Gemini, and Llama-3) show that our classifier outperforms the state-of-the-art zero-shot classifier by up to 27.1\\% on AUROC score and the rewriting classifier by 6\\% on F1 score. Our work suggests that LLM can effectively detect machine-generated text if they are trained properly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YRCAvXNP0v": {
    "title": "Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation",
    "volume": "review",
    "abstract": "Training a unified multilingual model promotes knowledge transfer but inevitably introduces negative interference. Language-specific modeling methods show promise in reducing interference. However, they often rely on heuristics to distribute capacity and struggle to foster cross-lingual transfer via isolated modules. In this paper, we explore intrinsic task modularity within multilingual networks and leverage these observations to circumvent interference under multilingual translation. We show that neurons in the feed-forward layers tend to be activated in a language-specific manner. Meanwhile, these specialized neurons exhibit structural overlaps that reflect language proximity, which progress across layers. Based on these findings, we propose Neuron Specialization, an approach that identifies specialized neurons to modularize feed-forward layers and then continuously updates them through sparse networks. Extensive experiments show that our approach achieves consistent performance gains over strong baselines with additional analyses demonstrating reduced interference and increased knowledge transfer",
    "checked": true,
    "id": "07675d6362465471ee327910348c390de7fd57c1",
    "semantic_title": "neuron specialization: leveraging intrinsic task modularity for multilingual machine translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=67tSTNPecz": {
    "title": "Code-Optimise: Self-Generated Preference Data for Correctness and Efficiency",
    "volume": "review",
    "abstract": "Code Language Models have been trained to generate accurate solutions, typically with no regard for runtime. On the other hand, previous works that explored execution optimisation have observed corresponding drops in functional correctness. To that end, we introduce Code-Optimise, a framework that incorporates both correctness (passed, failed) and runtime (quick, slow) as learning signals via self-generated preference data. Our framework is both lightweight and robust as it dynamically selects solutions to reduce overfitting while avoiding a reliance on larger models for learning signals. Code-Optimise achieves significant improvements in $pass@k$ while decreasing the competitive baseline runtimes by an additional 6% for in-domain data and up to 3% for out-of-domain data. As a byproduct, the average length of the generated solutions is reduced by up to 48% on MBPP and 23% on HumanEval, resulting in faster and cheaper inference. The generated data and codebase will be open-sourced at www.open-source.link",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U1m6MpRSVj": {
    "title": "In-Dialogues We Learn\": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning",
    "volume": "review",
    "abstract": "Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method",
    "checked": false,
    "id": "5678a1ee9d5542785115555b856e51a1dd9eb0e9",
    "semantic_title": "in dialogues we learn\": towards personalized dialogue without pre-defined profiles through in-dialogue learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=iINAtDdgqW": {
    "title": "Quantifying the Capabilities of LLMs across Scale and Precision",
    "volume": "review",
    "abstract": "Scale is often attributed as one of the factors that cause an increase in the performance of Large Language Models (LLMs), resulting in models with billion and trillion parameters. One of the limitations of such large models is the high computational requirements that limit their usage, deployment, and debugging in resource-constrained scenarios. Two commonly used alternatives to bypass these limitations are to use the smaller versions of LLMs (e.g. Llama 7B instead of Llama 70B) or lower the memory requirements by using quantization. While both approaches effectively address the limitation of resources, their impact on model performance needs thorough examination to make an informed decision. For instance, given a certain memory budget that fits a large model with low precision and a small model with high precision, what would be the right choice that results in good performance? In this study, we aim to answer such questions and investigate the effect of model scale and quantization on the performance using two major families of open-source instruct models. Our extensive zero-shot experiments reveal that larger models generally outperform their smaller counterparts, suggesting that scale remains an important factor in enhancing performance. Moreover, large models show exceptional resilience to precision reduction and serve as a better solution than smaller models at high precision under similar memory requirements",
    "checked": true,
    "id": "1a77f1784d3b3f08da94629cfa7e60963c2e9335",
    "semantic_title": "quantifying the capabilities of llms across scale and precision",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=C3MbT9xzxj": {
    "title": "Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models",
    "volume": "review",
    "abstract": "While recent research endeavors have focused on developing Large Language Models (LLMs) with robust long-context capabilities, due to the lack of long-context benchmarks, relatively little is known about how well the performance of long-context LLMs. To address this gap, we propose a multi-evidence, position-aware, and scalable benchmark for evaluating long-context LLMs, named Counting-Stars, which evaluates long-context LLMs by using two tasks: multi-evidence acquisition and multi-evidence reasoning. Based on the Counting-Stars test, we conduct experiments to evaluate long-context LLMs (i.e., GPT-4 Turbo, Gemini 1.5 Pro, Claude3 Opus, GLM-4, and Moonshot-v1). Experimental results demonstrate that Gemini 1.5 Pro achieves the best overall results, while the performance of GPT-4 Turbo is the most stable across various tasks. Furthermore, our analysis of these LLMs, which are extended to handle long-context scenarios, indicates that there is potential for improvement as the length of the input context and the intricacy of the tasks are increasing",
    "checked": true,
    "id": "f3ba4d7e5a7ee0b98cc3a1b6d44c34724f23403b",
    "semantic_title": "counting-stars: a multi-evidence, position-aware, and scalable benchmark for evaluating long-context large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=1g7NgXbIDO": {
    "title": "SeCoKD: Aligning Large Language Models for In-Context Learning with Fewer Shots",
    "volume": "review",
    "abstract": "Previous studies have shown that demonstrations can significantly help Large Language Models (LLMs ) perform better on the given tasks. However, this so-called In-Context Learning ( ICL ) ability is very sensitive to the presenting context, and often dozens of demonstrations are needed. In this work, we investigate if we can reduce the shot number while still maintaining a competitive performance. We present SeCoKD, a self-Knowledge Distillation ( KD ) training framework that aligns the student model with a heavily prompted variation, thereby increasing the utilization of a single demonstration. We experiment with the SeCoKD across three LLMs and six benchmarks focusing mainly on reasoning tasks. Results show that our method outperforms the base model and Supervised Fine-tuning ( SFT ), especially in zero-shot and one-shot settings by 30% and 10%, respectively. Moreover, SeCoKD brings little negative artifacts when evaluated on new tasks, which is more robust than Supervised Fine-tuning",
    "checked": true,
    "id": "d7e07ee25e03080374c683048aa30a468be2f131",
    "semantic_title": "secokd: aligning large language models for in-context learning with fewer shots",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IKKMRm2zwh": {
    "title": "MTEB-French: Resources for French Sentence Embedding Evaluation and Analysis",
    "volume": "review",
    "abstract": "Recently, numerous embedding models have been made available and widely used for various NLP tasks. The Massive Text Embedding Benchmark (MTEB) has primarily simplified the process of choosing a model that performs well for several tasks in English, but extensions to other languages remain challenging. This is why we expand MTEB to propose the first massive benchmark of sentence embeddings for French. We gather 15 existing datasets in an easy-to-use interface and create three new French datasets for a global evaluation of 8 task categories. We compare 51 carefully selected embedding models on a large scale, conduct comprehensive statistical tests, and analyze the correlation between model performance and many of their characteristics. We find out that even if no model is the best on all tasks, large multilingual models pre-trained on sentence similarity perform exceptionally well. Our work comes with open-source code, new datasets and a public leaderboard",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zJCAR3nWz2": {
    "title": "Leveraging Web-Crawled Data for High-Quality Fine-Tuning",
    "volume": "review",
    "abstract": "Most large language models are fine-tuned using either expensive human-annotated data or GPT-4 generated data which cannot guarantee performance in certain domains. We argue that although the web-crawled data often has formatting errors causing semantic inaccuracies, it can still serve as a valuable source for high-quality supervised fine-tuning in specific domains without relying on advanced models like GPT-4. To this end, we create a paired training dataset by aligning web-crawled data with a smaller set of high-quality data. By training a language model on this dataset, we can convert web data with irregular formats into high-quality ones. Our experiments show that training with the model-transformed data yields better results, surpassing training with only high-quality data by an average of 9.4\\% in Chinese elementary school math problems. Additionally, our 7B model outperforms several open-source models larger than 30B and surpasses well-known closed-source models such as GPT-3.5 and Claude-2, highlighting the efficacy of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=syAo91pgm1": {
    "title": "Wavelet and Optical Features Sparkling NLP",
    "volume": "review",
    "abstract": "Computational resources are vital in natural language processing (NLP) development. Since the physical limit of transistors is approaching a saturation point due to the outspace of Moore's Law and Dennard scaling, we look for alternative computing power from optical devices. As an initial step in this research direction, we facilitate feature extraction using optical computing and integrate optical extracted features to enhance NLP baselines on conventional electronic GPUs. Unlike another one of a kind of features extracted from Transformer, such as lexical embeddings, we extend the feature space beyond traditional embeddings using Wavelet functions that can run on optical toolkits. These extracted features, alongside the original input text, provide additional information that enhances model performance in NLP tasks. We employ two different feature extraction methods: a direct approach involving Wavelet or FFT transformations, and a novel method employing optical computing for NLP feature extraction. Our evaluation encompasses fice GLUE tasks - CoLA, SST-2, STSB, MRPC, and RTE - and reveals a notable improvement of up to +2.8% in classification accuracy",
    "checked": false,
    "id": "b3cddf8a3c83a66f8a5d995411ab7f1dc5a23cdf",
    "semantic_title": "gestformer: multiscale wavelet pooling transformer network for dynamic hand gesture recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GOFMsrWoxh": {
    "title": "Self-MoE: Self Mixture of Experts in between decoder layers",
    "volume": "review",
    "abstract": "Mixture of Experts is a well-known technique in machine learning and is widely used to empower large language models. Unfortunately, it requires a lot of resources to train experts. To weaken this requirement, we propose a modification to the architecture of pretrained LLMs we call self Mixture of Experts (self-MoE), which is a mixture of experts with all the experts being the same exact model. This adjustment adds a handful of weights and yields a significant improvement in model performance. We have evaluated self-MoE on two main tracks: mathematical reasoning and code generation, and observed a significant improvement across various benchmarks. We will publish the training code and the model weights upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vANmnQOqTf": {
    "title": "PsyPlay: Personality-Infused Role-Playing Conversational Agents",
    "volume": "review",
    "abstract": "The current research on Role-Playing Conversational Agents (RPCAs) with Large Language Models (LLMs) primarily focuses on imitating specific speaking styles and utilizing character backgrounds, neglecting the depiction of deeper personality traits. In this study, we introduce personality-infused role-playing for LLM agents, which encourages agents to accurately portray their designated personality traits during dialogues. We then propose PsyPlay, a dialogue generation framework that facilitates the expression of rich personalities among multiple LLM agents. Specifically, PsyPlay enables agents to assume roles with distinct personality traits and engage in discussions centered around specific topics, consistently exhibiting their designated personality traits throughout the interactions. Validation on generated dialogue data demonstrates that PsyPlay can accurately portray the intended personality traits, achieving an overall success rate of 80.31\\%. Notably, we observe that LLMs aligned with positive values are more successful in portraying positive personality roles compared to negative ones. Moreover, we construct a dialogue dataset for personality-infused role-playing, called PsyPlay-Bench. The dataset, which consists of 4745 instances of correctly portrayed dialogues using PsyPlay, aims to further facilitate research in personalized role-playing and dialogue personality detection. Our code and data are submitted in the supplementary material and will be released",
    "checked": false,
    "id": "c70e19518f7016d01d67884d06de4523b92dd867",
    "semantic_title": "understanding how people rate their conversations",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=kLpqgBip9x": {
    "title": "Evaluating Transformers for OCR Post-Correction in Early Modern Dutch Comedies and Farces",
    "volume": "review",
    "abstract": "In this paper, we investigate the performance on OCR post-correction in early modern Dutch of two types of transformers: large generative models and sequence-to-sequence models. To this end, we create a parallel corpus by automatically aligning OCR sentences to their ground truth from the EmDComF early modern Dutch comedies and farces corpus, and propose an alignment methodology that creates new segments based on combinations of newline splits. This improves the alignment between gold and OCR texts, which is essential for the creation of a high-quality parallel corpus. After filtering out misalignments, we fine-tune and evaluate both generative and sequence-to-sequence models. We find that mBART outperforms generative models for the automatic post-correction of early modern Dutch in the EmDComF corpus, correcting more OCR sequences and avoiding overgeneration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1Yn4VnETJ": {
    "title": "MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction",
    "volume": "review",
    "abstract": "Unsupervised rationale extraction aims to extract text snippets to support model predictions without explicit rationale annotation. Researchers have made many efforts to solve this task. However, Previous works encode each aspect independently, ignoring their internal correlations. Meanwhile, such a uni-aspect encoding model can only explain and predict one aspect of the text at a time, which limits its downstream applications. In this paper, we propose a Multi-Aspect Rationale Extractor (MARE) to explain and predict multiple aspects simultaneously. Concretely, we propose a Multi-Aspect Multi-Head Attention (MAMHA) mechanism based on hard deletion to encode multiple text chunks simultaneously. Furthermore, multiple special tokens are prepended in front of the text with each corresponding to one certain aspect. Finally, multi-task training is deployed to reduce the training overhead. Experimental results on two unsupervised rationale extraction benchmarks show that MARE achieves state-of-the-art performance. Ablation studies further demonstrate the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dt0WRSK6cL": {
    "title": "Mitigating Hallucinations of Large Language Models in Medical Domain via Contrastive Decoding",
    "volume": "review",
    "abstract": "The impressive capabilities of large language models (LLMs) have attracted extensive interests of applying LLMs to medical field. However, the complex nature of clinical environments presents significant hallucination challenges for LLMs, hindering their widespread adoption. In this paper, we address these hallucination issues in the context of Medical Information Extraction (MIE) tasks by introducing ALternate Contrastive Decoding (ALCD). We begin by redefining MIE tasks as an \\textit{identify-and-classify} process. We then separate the identification and classification functions of LLMs by selectively masking the optimization of tokens during fine-tuning. During the inference stage, we alternately contrast output distributions derived from sub-task models. This approach aims to selectively enhance the identification and classification capabilities while minimizing the influence of other inherent abilities in LLMs. Additionally, we propose an alternate adaptive constraint strategy to more effectively adjust the scale and scope of contrastive tokens. Through comprehensive experiments on two different backbones and six diverse medical information extraction tasks, ALCD demonstrates significant improvements in resolving hallucination issues compared to conventional decoding methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3bcFeZRxVY": {
    "title": "Knowledge Introspection: A Self-reflection Method for Reliable and Helpful Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have inherent knowledge deficiency due to insufficient or erroneous data and incomplete training strategies. Furthermore, LLMs are often overconfident and unaware of their own knowledge deficiency, which will pose safety and legal risks to users. Inspired by the process of human introspection, we propose a two-stage method that enables LLMs to master the capability of knowledge introspection. Our method relies on data only generated by the LLM itself, and makes the LLM distinguish among what is known, uncertain and unknown. The method is trained in two-stages, in which supervised fine-tuning is employed in the first stage and direct preference optimization is utilized in the second stage. Experimental results demonstrate that our method effectively enhances the LLM's understanding of its internal knowledge, significantly improves generation accuracy, reliability and helpfulness of the model responses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5nT79lhAsl": {
    "title": "On the Importance of Nuanced Taxonomies for LLM-Based Understanding of Harmful Events: A Case Study on Antisemitism",
    "volume": "review",
    "abstract": "Monitoring the news at scale for incidents of hate, violence, and other toxicity is essential to understanding broad societal trends, including harms to marginalized communities. As large language models (LLMs) become a primary tool for understanding events at scale, they can be useful for elucidating these harms. However, labeling harmful events is challenging due to the subjectivity of labels such as \"toxicity\" and \"hate.\" Motivated by the rise of antisemitism, this paper presents a case study of the capability of LLMs to discover reports of antisemitic events. We pilot the task of hateful event classification on the AMCHA Corpus, a continuously updated dataset with expert-labeled instances of 3 coarse-grained categories and 14 fine-grained types of antisemitism. We show that incorporating domain knowledge from fine-grained taxonomies is needed to make LLMs more effective at this task. Our experiments find that providing precise definitions from a fine-grained taxonomy of antisemitism can steer GPT-4 and Llama-3 to perform better on tagging antisemitic event descriptions to a limited extent, with GPT-4 achieving up to a 14\\% increase in mean weighted F1. However, our results suggest that LLMs are still far from perfect at understanding antisemitic events, suggesting avenues for future work on more creative LLM alignment and more policy work on creating precise definitions of antisemitism",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oUMKv7Nlav": {
    "title": "GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction",
    "volume": "review",
    "abstract": "Document-level relation extraction (DocRE) aims to extract relations between entities from unstructured document text. Compared to sentence-level relation extraction, it requires more complex semantic understanding from a broader text context. Currently, some studies are utilizing logical rules within evidence sentences to enhance the performance of DocRE. However, in the data without provided evidence sentences, researchers often obtain a list of evidence sentences for the entire document through evidence retrieval (ER). Therefore, DocRE suffers from two challenges: firstly, the relevance between evidence and entity pairs is weak; secondly, there is insufficient extraction of complex cross-relations between long-distance multi-entities. To overcome these challenges, we propose GEGA, a novel model for DocRE. The model leverages graph neural networks to construct multiple weight matrices, guiding attention allocation to evidence sentences. It also employs multi-scale representation aggregation to enhance ER. Subsequently, we integrate the most efficient evidence information to implement both fully supervised and weakly supervised training processes for the model. We evaluate the GEGA model on three widely used benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The experimental results indicate that our model has achieved comprehensive improvements compared to the existing SOTA model",
    "checked": true,
    "id": "76961f026f25db2ec9a4c5e372dffed9adc8c410",
    "semantic_title": "gega: graph convolutional networks and evidence retrieval guided attention for enhanced document-level relation extraction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZSEioFIrzX": {
    "title": "Enhancing Intent Understanding for Ambiguous Prompt: A Human-Machine Co-Adaption Strategy",
    "volume": "review",
    "abstract": "Modern text-to-image generation models are capable of producing realistic and high-quality images. However, user prompts often contain ambiguities, making it difficult for these systems to interpret users' actual intentions. Consequently, users often need to modify their prompts several times to ensure the generated images meet their expectations. Although some previous works aim to refine prompts for generating images that align with user requirements, comprehending the true needs of users, particularly non-expert individuals, remains a challenge for the model. In this research, we aim to enhance the visual parameter-tuning process, making the model user-friendly for individuals without specialized knowledge and it can better understand user needs. We propose a human-machine co-adaption strategy by maximizing the mutual information between the user's prompts and the pictures under modification as the optimizing target in order to make the system better adapt to user needs. We find that an improved model can reduce the necessity for multiple rounds of adjustments. We also collect multi-round dialogue datasets with prompts and images pairs and user intent. Various experiments demonstrate the effectiveness of the proposed method in our proposed dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d7KsesYb6E": {
    "title": "GAT-Edge: Graph Attention Neural Network with Adjacent Edge Features",
    "volume": "review",
    "abstract": "Edge features are a crucial component of graph data as they provide a wealth of information that can enhance model performance. In this paper, we propose an improved model called GAT-Edge, which builds upon the graph attention network by optimizing the attention mechanism to incorporate edge feature information. By leveraging adjacent edge features in the graph, our model can assist downstream tasks such as node classification. The connection between nodes in graph data is often enriched by the adjacent edges, which provide more effective and abundant information. To exploit this, our model combines edge features and node features in the attention calculation, convolving them together to generate new attention coefficients. This approach facilitates efficient information transmission and aggregation between nodes, leading to improved performance. We apply our new model to several citation networks commonly used in the field of graph neural networks for node classification, and compare it with the current mainstream graph convolution neural network models. Our results demonstrate that our model achieved better accuracy, highlighting the importance and research value of mining adjacent edge features in graphs",
    "checked": false,
    "id": "41c3c938145f89a81546d71fa8ede1a5400abbbb",
    "semantic_title": "dynamic edge-based graph attention network and visual-semantic embedding for outfits recommendation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VEAxwRogmP": {
    "title": "Defending Against Social Engineering Attacks in the Age of LLMs",
    "volume": "review",
    "abstract": "The proliferation of Large Language Models (LLMs) poses challenges in detecting and mitigating digital deception, as these models can emulate human conversational patterns and facilitate chat-based social engineering (CSE) attacks. This study investigates the dual capabilities of LLMs as both facilitators and defenders against CSE threats. We develop a novel dataset, $\\textbf{SEConvo}$, simulating CSE scenarios in academic and recruitment contexts, and designed to examine how LLMs can be exploited in these situations. Our findings reveal that, while off-the-shelf LLMs generate high-quality CSE content, their detection capabilities are suboptimal, leading to increased operational costs for defense. In response, we propose $\\textbf{ConvoSentinel}$, a modular defense pipeline that improves detection at both the message and the conversation levels, offering enhanced adaptability and cost-effectiveness. The retrieval-augmented module in $\\textbf{ConvoSentinel}$ identifies malicious intent by comparing messages to a database of similar conversations, enhancing CSE detection at all stages. Our study highlights the need for advanced strategies to leverage LLMs in cybersecurity. Our code and data are available at this anonymous repo link: https://anonymous.4open.science/r/ConvoSentinel_Anonymous-1E1D/README.md",
    "checked": true,
    "id": "b43eb24cfc1930fb9c7bace4b69cdf34fecfde3c",
    "semantic_title": "defending against social engineering attacks in the age of llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=TwJ2q0W0OH": {
    "title": "BERTrend: Neural Topic Modeling for Emerging Trends Detection",
    "volume": "review",
    "abstract": "Detecting and tracking emerging trends and weak signals in large, evolving text corpora is vital for applications such as monitoring scientific literature, managing brand reputation, and surveilling critical infrastructure. Existing solutions often fail to capture the nuanced context or dynamically track evolving patterns over time. BERTrend, a novel method, addresses these limitations using neural topic modeling in an online setting. It introduces a new metric to quantify topic popularity over time by considering both the number of documents and update frequency. This metric classifies topics as noise, weak, or strong signals, flagging emerging, rapidly growing topics for further investigation. Evaluations on two large real-world datasets demonstrate BERTrend's ability to accurately detect and track meaningful weak signals while filtering out noise, offering a comprehensive solution for monitoring emerging trends in large-scale, evolving text corpora",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=izSknHt9Vn": {
    "title": "Irrelevant Alternatives Bias Large Language Model Hiring Decisions",
    "volume": "review",
    "abstract": "We investigate whether LLMs display a well-known human cognitive bias, the attraction effect, in hiring decisions. The attraction effect occurs when the presence of an inferior candidate makes a superior candidate more appealing, increasing the likelihood of the superior candidate being chosen over a non-dominated competitor. Our study finds consistent and significant evidence of the attraction effect in GPT-3.5 and GPT-4 when they assume the role of a recruiter. Irrelevant attributes of the decoy, such as its gender, further amplify the observed bias. GPT-4 exhibits greater bias variation than GPT-3.5. Our findings remain robust even when warnings against the decoy effect are included and the recruiter role definition is varied",
    "checked": false,
    "id": "ebf4575298a812c69f9693fe6c32e6f49d2c637b",
    "semantic_title": "auditing the use of language models to guide hiring decisions",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=SoPeTS9GVn": {
    "title": "PDSS: A Privacy-Preserving Framework for Step-by-Step Distillation of Large Language Models",
    "volume": "review",
    "abstract": "In the context of real-world applications, leveraging large language models (LLMs) for domain-specific tasks often faces two major challenges: domain-specific knowledge privacy and constrained resources. To address these issues, we propose PDSS, a privacy-preserving framework for step-by-step distillation of LLMs. PDSS works on a server-client architecture, wherein client transmits perturbed prompts to the server's LLM for rationale generation. The generated rationales are then decoded by the client and used to enrich the training of task-specific small language model(SLM) within a multi-task learning paradigm. PDSS introduces two privacy protection strategies: the \\textit{Exponential Mechanism Strategy} and the \\textit{Encoder-Decoder Strategy}, balancing prompt privacy and rationale usability. Experiments demonstrate the effectiveness of PDSS in various text generation tasks, enabling the training of task-specific SLM with enhanced performance while prioritizing data privacy protection",
    "checked": true,
    "id": "3ff5917df2ab501b90a53a9a793d4f38fd2c6b49",
    "semantic_title": "pdss: a privacy-preserving framework for step-by-step distillation of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FmUS9qVnut": {
    "title": "Exploring the Role of Transliteration in In-Context Learning for Low-resource Languages Written in Non-Latin Scripts",
    "volume": "review",
    "abstract": "Decoder-only large language models (LLMs) excel in high-resource languages across various tasks through few-shot or even zero-shot in-context learning (ICL). However, their performance often does not transfer well to low-resource languages, especially those written in non-Latin scripts. Inspired by recent work that leverages transliteration in encoder-only models, we investigate whether transliteration is also effective in improving LLMs' performance for low-resource languages written in non-Latin scripts. To this end, we propose three prompt templates, where the target-language text is represented in (1) its original script, (2) Latin script, or (3) both. We apply these methods to several representative LLMs of different sizes on various tasks including text classification and sequential labeling. Our findings show that the effectiveness of transliteration varies by task type and model size. For instance, all models benefit from transliterations for sequential labeling (with increases of up to 25%). We make our code publicly available",
    "checked": true,
    "id": "08c013110fa4df5e5d54fdbeff9db595bfb6eb26",
    "semantic_title": "exploring the role of transliteration in in-context learning for low-resource languages written in non-latin scripts",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vCscszwihl": {
    "title": "Think-in-Memory: Metacognition-Augmented LLM with Long-Term Memory",
    "volume": "review",
    "abstract": "Memory-augmented Large Language Models (LLMs) can recall and reason on recalled past contexts (named recall-reason step). However, multiple recall-reason steps may produce biased thoughts, i.e., inconsistent reasoning paths over the same recalled results. Motivated by that humans only memorize the metacognition thoughts in mind rather than event details, we propose a novel memory-augmented framework called Think-in-Memory (TiM) to flexibly utilize the historical context. Concretely, we formulate a self-organizing memory mechanism equipped with a metacognition space and stationary operation actions, leveraging role-playing LLM agents to achieve thought generator, retriever, and organizer. Supported by such multi-agent self-organization, TiM can imitate human-level metacognition to memorize and update history context as metacognition thoughts without suffering from reasoning inconsistency. TiM can process ultra-long history context in a plug-and-play paradigm to benefit downstream interactions. To conduct evaluations under more complex tasks, clinical diagnosis is adopted as the evaluation task: (1) we formulate a role-play simulator to simulate long-term interactions between the doctor and patient. (2) we collect a multi-turn medical consultations dataset from the real-world hospitals. Besides, two daily conversation datasets are also involved. Experiments demonstrate that our method achieves remarkable improvements on memory-augmented long-term dialogues about both daily and medical topics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8jXsWBMAYH": {
    "title": "Exploiting Reversible Semantic Parsing and Text Generation for Error Correction with Pre-trained LLMs",
    "volume": "review",
    "abstract": "Semantic parsing and text generation are reversible processes when working with Discourse Representation Structures (DRS). Obviously, errors can arise in both the parsing (text-to-DRS) and generation (DRS-to-text). This paper presents an approach that exploits the reversible nature of these tasks to automatically correct such errors without additional model training. We leverage pre-trained large language models (LLMs) in two pipeline setups: Pars-Gen-Pars and Gen-Pars-Gen, where the output of one model serves as the input to the next. In the Pars-Gen-Pars pipeline, input text is parsed into a DRS, then used to generate text, which is finally parsed again. Conversely, the Gen-Pars-Gen pipeline starts with a DRS, generates text, parses it, and regenerates text from the parsed DRS. Interestingly, by propagating the data through these reversible pipelines, errors from the initial parse or generation step can be mitigated, instead of being amplified. Experiments on the Parallel Meaning Bank dataset demonstrate the efficacy of our approach, with improved performance over baseline models on semantic parsing (SMATCH) and text generation (BLEU, METEOR, COMET, chrF, BERT-Score) metrics. Our error analysis also sheds light on the types of mistakes addressed by each pipeline setup. The proposed method offers a simple yet effective way to enhance DRS-based natural language processing without costly model retraining",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CoSkl4iUQr": {
    "title": "PG-Story: Taxonomy, Dataset, and Evaluation for Ensuring Child-Safe Content for Story Generation",
    "volume": "review",
    "abstract": "Creating children's stories through text generation is a creative task that demands stories to be not only entertaining but also suitable for young audiences. However, current story generation systems rely on pre-trained language models fine-tuned with limited story data, which may not always prioritize child-friendliness. This can lead to the unintended generation of stories containing problematic elements such as violence, profanity, and biases. Regrettably, despite the significance of these concerns, there is a lack of clear guidelines and benchmark datasets for ensuring content safety for children. In our paper, we introduce a taxonomy specifically tailored to assess content safety in text, with a strong emphasis on children's well-being. We present the \\textsc{PG-Story}, a dataset that includes detailed annotations for both sentence-level and discourse-level safety. We demonstrate the potential of identifying unsafe content through self-diagnosis and employing controllable generation techniques during the decoding phase to minimize unsafe elements in generated stories",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DciVFiTW31": {
    "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
    "volume": "review",
    "abstract": "While the impact of social biases in language models has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, limiting our understanding of bias complexities. This paper proposes a novel framework for probing language models for social biases by assessing disparate treatment, which involves treating individuals differently according to their affiliation with a sensitive demographic group. We curate SoFa, a large-scale benchmark designed to address the limitations of existing fairness collections. SoFa expands the analysis beyond the binary comparison of stereotypical versus anti-stereotypical identities to include a diverse range of identities and stereotypes. Comparing our methodology with existing benchmarks, we reveal that biases within language models are more nuanced than acknowledged, indicating a broader scope of encoded biases than previously recognized. Benchmarking LMs on SoFa, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models. Finally, our findings indicate that real-life adversities faced by various groups such as women and people with disabilities are mirrored in the behavior of these models",
    "checked": true,
    "id": "468c1d2d8e384472f313ff0487839839727b8934",
    "semantic_title": "social bias probing: fairness benchmarking for language models",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=5pQ6NVzNfK": {
    "title": "Can LLMs Learn Macroeconomic Narratives from Social Media?",
    "volume": "review",
    "abstract": "This study empirically tests the $\\textit{Narrative Economics}$ hypothesis, which posits that narratives (ideas that are spread virally and affect public beliefs) can influence economic fluctuations. We introduce two curated datasets containing posts from X (formerly Twitter) which capture economy-related narratives (Data will be shared upon paper acceptance). Employing Natural Language Processing (NLP) methods, we extract and summarize narratives from the tweets. We test their predictive power for $\\textit{macroeconomic}$ forecasting by incorporating the tweets' or the extracted narratives' representations in downstream financial prediction tasks. Our work highlights the challenges in improving macroeconomic models with narrative data, paving the way for the research community to realistically address this important challenge. From a scientific perspective, our investigation offers valuable insights and NLP tools for narrative extraction and summarization using Large Language Models (LLMs), contributing to future research on the role of narratives in economics",
    "checked": true,
    "id": "20f6b9a86f7f753c39f44c1001ca8494844854b9",
    "semantic_title": "can llms learn macroeconomic narratives from social media?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LBiqQJH0cr": {
    "title": "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring",
    "volume": "review",
    "abstract": "Generating rationales that justify scoring decisions has been a promising way to facilitate explainability in automated scoring systems. However, existing methods do not match the accuracy of classifier-based methods. Plus, the generated rationales often contain hallucinated information. To address these issues, we propose a novel framework capable of generating more faithful rationales and, more importantly, matching performance with classifier-based black-box scoring systems. We first mimic the human assessment process by querying Large Language Models (LLMs) to generate a thought tree. We then summarise intermediate assessment decisions from each thought tree path for creating synthetic rationale data and rationale preference data. Finally, we utilise the generated synthetic data to calibrate LLMs through a two-step training process: supervised fine-tuning and preference optimisation. Extensive experimental results demonstrate that our framework achieves a 38\\% assessment performance improvement in the QWK score compared to prior work while producing higher-quality rationales, as recognised by human evaluators and LLMs. Our work sheds light on the effectiveness of performing preference optimisation using synthetic preference data obtained from thought tree paths",
    "checked": true,
    "id": "84c7ca14b9dc95fc8a21cf6f4a2194ffa99579cc",
    "semantic_title": "calibrating llms with preference optimization on thought trees for generating rationale in science question scoring",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pGAaIC7wYt": {
    "title": "Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding",
    "volume": "review",
    "abstract": "Visual arguments, often used in advertising or social causes, rely on images to persuade viewers to do or believe something. Understanding these arguments requires selective vision: only specific visual stimuli within an image are relevant to the argument, and relevance can only be understood within the context of a broader argumentative structure. While visual arguments are readily appreciated by human audiences, we ask: are today's AI capable of similar understanding? We collect and release VisArgs, an annotated corpus designed to make explicit the (usually implicit) structures underlying visual arguments. VisArgs includes 1,611 images accompanied by three types of textual annotations: 5,112 visual premises (with region annotations), 5,574 commonsense premises, and reasoning trees connecting them to a broader argument. We propose three tasks over VisArgs to probe machine capacity for visual argument understanding: localization of premises, identification of premises, and deduction of conclusions. Experiments demonstrate that 1) machines cannot fully identify the relevant visual cues. The top-performing model, GPT-4-O, achieved an accuracy of only 78.5%, whereas humans reached 98.0%. All models showed a performance drop, with an average decrease in accuracy of 19.5%, when the comparison set was changed from objects outside the image to irrelevant objects within the image. Furthermore, 2) this limitation is the greatest factor impacting their performance in understanding visual arguments. Most models improved the most when given relevant visual premises as additional inputs, compared to other inputs, for deducing the conclusion of the visual argument",
    "checked": true,
    "id": "1b23817f17a1f5b4ce8020fbfcb80cad6cff1659",
    "semantic_title": "selective vision is the challenge for visual reasoning: a benchmark for visual argument understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sfyL4tX6bc": {
    "title": "Referenceless evaluation of machine translation models by ranking performance in Romaninan to English translate-train settings",
    "volume": "review",
    "abstract": "We propose a referenceless evaluation method for machine translation (MT) models by assessing their performance in translate-train scenarios across a variety of natural language processing (NLP) tasks. We compare four prominent MT tools by using them to translate tasks from Romanian into English and investigate their impact on text summarization, sentiment analysis, and authorship identification. Our findings demonstrate that while translation significantly boosts performance in summarization and sentiment analysis, it adversely affects the identification of authorship in poetry. In response to the observed performance disparities among MT models, we have developed a ranking system that aligns closely with human preferences. This system avoids reliance on professional ground-truth translations, which are typically required by traditional MT evaluation metrics like BLEU but can be biased by the quality of the reference and the translator's proficiency. Our approach provides a more authentic measure of MT quality, reflecting more accurately how these models perform in practical applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bKvGjVZNUk": {
    "title": "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation",
    "volume": "review",
    "abstract": "Most works on gender bias focus on intrinsic bias --- removing traces of information about a protected group from the model's internal representation. However, these works are often disconnected from the impact of such debiasing on downstream applications, which is the main motivation for debiasing in the first place. In this work, we systematically test how methods for intrinsic debiasing affect neural machine translation models, by measuring the extrinsic bias of such systems under different design choices. We highlight three challenges and mismatches between the debiasing techniques and their end-goal usage, including the choice of embeddings to debias, the mismatch between words and sub-word tokens debiasing, and the effect on different target languages. We find that these considerations have a significant impact on downstream performance and the success of debiasing",
    "checked": true,
    "id": "60a02dcd250f07a32ae4264fa58fcd7cd10ad07d",
    "semantic_title": "applying intrinsic debiasing on downstream tasks: challenges and considerations for machine translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=srJFNkOUVt": {
    "title": "CPO-SQL: Boosting Small LLMs for Text-to-SQL via Efficient In-Context Learning and Preference Optimization",
    "volume": "review",
    "abstract": "Most recent researches in Text-to-SQL parsing overly rely on the proprietary Large Language Models (LLMs), raising concerns of data privacy and inference overheads. To narrow the gap between small LLMs and proprietary LLMs in Text-to-SQL, we introduce CPO-SQL, an approach aiming to efficiently boost the capability of small LLMs via In-Context Learning and Preference Optimization. This approach builds the enhanced training set by sampling demonstrations from beta distribution based on the similarity of questions and SQL, and then fine-tune the small LLMs to empower them with ICL capabilities of Text-to-SQL. Further, we propose a new Spider preference set, constructed by an agile semi-automated process, based on six types of SQL optimization. On this basis, we employ SFT-enhanced preference optimization to support the mixed training on the supervised set and the preference set, enabling us to optimize the SQL generation in complex query scenarios while maintaining the learning of original data. By this way, we can balance the generation ability of small LLMs for questions of varying difficulty. Finally, we evaluate our method on Spider and its three robustness-diagnostic variants, shedding light on the strengths and weaknesses of it",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NaFtZu3c5f": {
    "title": "A Multilingual Exploration of Jailbreak Attacks in Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have become increasingly popular for their advanced text generation capabilities across various domains. However, like any software, they face security challenges, including the risk of 'jailbreak' attacks that manipulate LLMs to produce prohibited content. A particularly underexplored area is the Multilingual Jailbreak attack, where malicious questions are translated into various languages to evade safety filters. Currently, there is a lack of comprehensive empirical studies addressing this specific threat. To address this research gap, we conducted an extensive empirical study on Multilingual Jailbreak attacks. We developed a novel semantic-preserving algorithm to create a multilingual jailbreak dataset and conducted an exhaustive evaluation on both widely-used open-source and commercial LLMs, including GPT-4 and LLaMa. Additionally, we performed interpretability analysis to uncover patterns in Multilingual Jailbreak attacks and implemented a fine-tuning mitigation method. Our findings reveal that our mitigation strategy significantly enhances model defense, reducing the attack success rate by 96.2%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ve74z0s17e": {
    "title": "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models",
    "volume": "review",
    "abstract": "Social biases such as gender or racial biases have been reported in language models (LMs), including Masked Language Models (MLMs). Given that MLMs are continuously trained with increasing amounts of additional data collected over time, an important yet unanswered question is how the social biases encoded with MLMs vary over time. In particular, the number of social media users continues to grow at an exponential rate, and it is a valid concern for the MLMs trained specifically on social media data whether their social biases (if any) would also amplify over time. To empirically analyse this problem, we use a series of MLMs pretrained on chronologically ordered temporal snapshots of corpora. Our analysis reveals that, although social biases are present in all MLMs, most types of social bias remain relatively stable over time (with a few exceptions). To further understand the mechanisms that influence social biases in MLMs, we analyse the temporal corpora used to train the MLMs. Our findings show that some demographic groups, such as male, obtain higher preference over the other, such as female on the training corpora constantly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r8cbhMMORU": {
    "title": "ExcluIR: Exclusionary Neural Information Retrieval",
    "volume": "review",
    "abstract": "Exclusion is an important and universal linguistic skill that humans use to express what they do not want. There is little research on exclusionary retrieval, where users express what they do not want to be part of the results produced for their queries. We investigate the scenario of exclusionary retrieval in document retrieval for the first time. We present ExcluIR, a set of resources for exclusionary retrieval, consisting of an evaluation benchmark and a training set for helping retrieval models to comprehend exclusionary queries. The evaluation benchmark includes 3,452 high-quality exclusionary queries, each of which has been manually annotated. The training set contains 70,293 exclusionary queries, each paired with a positive document and a negative document. We conduct detailed experiments and analyses, obtaining three main observations: (i) existing retrieval models with different architectures struggle to comprehend exclusionary queries effectively; (ii) although integrating our training data can improve the performance of retrieval models on exclusionary retrieval, there still exists a gap compared to human performance; and (iii) generative retrieval models have a natural advantage in handling exclusionary queries",
    "checked": true,
    "id": "5b1e194df4663c3dd9b51421ae65a4c386842df3",
    "semantic_title": "excluir: exclusionary neural information retrieval",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=gKHIT7BuRD": {
    "title": "DocCHA: Towards LLM-Augmented Interactive Online diagnosis System",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated great capabilities in addressing many application tasks. Among various applications, one eye-catching domain is Conversational Health Agents (CHAs), which are interactive conversational systems that can provide people with various health-related services. However, existing CHAs mainly focus on providing static health services, and lack interactive online diagnosis for patients. In the clinical setting, the initial symptoms that patients provide may lack comprehensiveness and detail, thus online interaction with patients to request additional information is important. To alleviate this problem, we propose DocCHA, an online interactive diagnosis system that interacts with patients by requesting additional information and continuesly improving the diagnosis confidence until providing patients with a reliable diagnosis. Moreover, DocCHA leverages Retrieval Augmented Generation (RAG) with Google search API, StatPearls and Wikipedia to provide patients with detailed and reliable health suggestions. We evaluate DocCHA's performance on the IMCS21 dataset, a Chinese online diagnosis dataset consists of conversations between patients and doctors. Experimental results show that DocCHA's diagnosis accuracy reaches 89.2\\% with 4 rounds of additional information request interactions with patients. Besides, the generated suggestion after RAG outperforms the direct prompt in terms of relevance, coherence, accuracy and completeness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YSCNip4RDB": {
    "title": "Leveraging In-Context Learning for Political Bias Testing of LLMs",
    "volume": "review",
    "abstract": "A growing body of work has been querying LLMs with political questions to evaluate their potential biases. However, this probing method has limited stability, making comparisons between models unreliable. In this paper, we argue that LLMs need more context. We propose a new probing task, Questionnaire Modeling, that uses human survey data as in-context examples. We show that Questionnaire Modeling improves the stability of question-based bias evaluation, and demonstrate that it may be used to compare instruction-tuned models to their base versions. Experiments with two open-source LLMs indicate that instruction tuning can indeed change the direction of bias. Data and code are publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BqjdRZhe5K": {
    "title": "Datasets for Multilingual Answer Sentence Selection",
    "volume": "review",
    "abstract": "Answer Sentence Selection (AS2) is a critical task for designing effective retrieval-based Question Answering (QA) systems. Most advancements in AS2 focus on English due to the scarcity of annotated datasets for other languages. This lack of resources prevents the training of effective AS2 models in different languages, creating a performance gap between QA systems in English and other locales. In this paper, we introduce new high-quality datasets for AS2 in five European languages (French, German, Italian, Portuguese, and Spanish), obtained through supervised Automatic Machine Translation (AMT) of existing English AS2 datasets such as ASNQ, WikiQA, and TREC-QA using a Large Language Model (LLM). We evaluated our approach and the quality of the translated datasets through multiple experiments with different Transformer architectures. The results indicate that our datasets are pivotal in producing robust and powerful multilingual AS2 models, significantly contributing to closing the performance gap between English and other languages",
    "checked": true,
    "id": "88faf932905250c11cdb52d62e91560958aba27d",
    "semantic_title": "datasets for multilingual answer sentence selection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VzVEKmElJT": {
    "title": "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "Ensuring the verifiability of model answers is a fundamental challenge for retrieval-augmented generation (RAG) in the question answering (QA) domain. Recently, self-citation prompting was proposed to make large language models (LLMs) generate citations to supporting documents along with their answers. However, self-citing LLMs often struggle to match the required format, refer to non-existent sources, and fail to faithfully reflect LLMs' context usage throughout the generation. In this work, we present MIRAGE --Model Internals-based RAG Explanations -- a plug-and-play approach using model internals for faithful answer attribution in RAG applications. MIRAGE detects context-sensitive answer tokens and pairs them with retrieved documents contributing to their prediction via saliency methods. We evaluate our proposed approach on a multilingual extractive QA dataset, finding high agreement with human answer attribution. On open-ended QA, MIRAGE achieves citation quality and efficiency comparable to self-citation while also allowing for a finer-grained control of attribution parameters. Our qualitative evaluation highlights the faithfulness of MIRAGE's attributions and underscores the promising application of model internals for RAG answer attribution. Code and data released at https://anonymized",
    "checked": true,
    "id": "90193735c3a84cf608409007df1bf409fd6635c6",
    "semantic_title": "model internals-based answer attribution for trustworthy retrieval-augmented generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UOxaepuMFy": {
    "title": "Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language",
    "volume": "review",
    "abstract": "We are exposed to much information trying to influence us, such as teaser messages, debates, politically framed news, and propaganda â€” all of which use persuasive language. With the recent interest in Large Language Models (LLMs), we study the ability of LLMs to produce persuasive text. As opposed to prior work which focuses on particular domains or types of persuasion, we conduct a general study across various domains to measure and benchmark to what degree LLMs produce persuasive text - both when explicitly instructed to rewrite text to be more or less persuasive and when only instructed to paraphrase. To this end, we construct a new dataset, Persuasive-Pairs, of pairs each consisting of a short text and of a text rewritten by an LLM to amplify or diminish persuasive language. We multi-annotate the pairs on a relative scale for persuasive language. This data is not only a valuable resource in itself, but we also show that it can be used to train a regression model to predict a score of persuasive language between text pairs. This model can score and benchmark new LLMs across domains, thereby facilitating the comparison of different LLMs. Finally, we discuss effects observed for different system prompts. Notably, we find that different `personas' in the system prompt of LLaMA3 change the persuasive language in the text substantially, even when only instructed to paraphrase. These findings underscore the importance of investigating persuasiveness in LLM generated text",
    "checked": true,
    "id": "ba88238a44b4a3c83e656258d90d9a8643913dae",
    "semantic_title": "measuring and benchmarking large language models' capabilities to generate persuasive language",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rXtGiySLBP": {
    "title": "CTL-Prompt: Contrastive Topic-Length Prompt Learning for Dialogue Summarization",
    "volume": "review",
    "abstract": "The prevalence of online meeting has highlighted the necessity of dialogue summary. Topic summarization is one domain attracted much interest from industry. Anyhow, past work either use topic- or length-prompt which tend to generate almost identical summaries across similar and even different topics. This study proposes Contrastive Topic-Length Prompt Learning (CTL-Prompt), a simple method that generates topic-based summaries. To produce concise yet diverse summaries across topics, we propose contrastive learning on topic-length prompts, which leverages positive and negative pairs to allow the models to learn the similarities and differences of topics. Results showed that our model outperformed baseline models in the ROUGE, BERTscores, and human evaluation scores on the DialogSum and the MACSum dataset. Our work can be found at [anonymized]",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vRHUZt3f07": {
    "title": "On the Robustness of Editing Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have played a pivotal role in building communicative AI, yet they encounter the challenge of efficient updates. Model editing enables the manipulation of specific knowledge memories and the behavior of language generation without retraining. However, the robustness of model editing remains an open question. This work seeks to understand the strengths and limitations of editing methods, facilitating practical applications of communicative AI. We focus on three key research questions. RQ1: Can edited LLMs behave consistently resembling communicative AI in realistic situations? RQ2: To what extent does the rephrasing of prompts lead LLMs to deviate from the edited knowledge memory? RQ3: Which knowledge features are correlated with the performance and robustness of editing? Our empirical studies uncover a substantial disparity between existing editing methods and the practical application of LLMs. On rephrased prompts that are flexible but common in realistic applications, the performance of editing experiences a significant decline. Further analysis shows that more popular knowledge is memorized better, easier to recall, and more challenging to edit effectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uXgGbezEoJ": {
    "title": "RSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework",
    "volume": "review",
    "abstract": "Despite significant advancements in natural language generation, controlling language models to produce texts with desired attributes remains a formidable challenge. In this work, we introduce RSA-Control, a training-free controllable text generation framework grounded in pragmatics. RSA-Control directs the generation process by recursively reasoning between imaginary speakers and listeners, enhancing the likelihood that target attributes are correctly interpreted by listeners amidst distractors. Additionally, we introduce a self-adjustable rationality parameter, which allows for automatic adjustment of control strength based on context. Our experiments, conducted with two task types and two types of language models, demonstrate that RSA-Control achieves strong attribute control while maintaining language fluency and content consistency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7LJsNxtnCJ": {
    "title": "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model",
    "volume": "review",
    "abstract": "Projecting visual features into word embedding space has become a significant fusion strategy adopted by Multimodal Large Language Models (MLLMs). However, its internal mechanisms have yet to be explored. Inspired by multilingual research, we identify domain-specific neurons in multimodal large language models. Specifically, we investigate the distribution of domain-specific neurons and the mechanism of how MLLMs process features from diverse domains. Furthermore, we propose a three-stage framework for language model modules in MLLMs when handling projected image features, and verify this hypothesis using logit lens. Extensive experiments indicate that while current MLLMs exhibit Visual Question Answering (VQA) capability, they may not fully utilize domain-specific information. Manipulating domain-specific neurons properly will result in a 10\\% change of accuracy at most, shedding light on the development of cross-domain, all-encompassing MLLMs in the future. The source code is available at https://anonymous.4open.science/r/MMNeuron",
    "checked": true,
    "id": "729807e6984a7b506ea8019967fb486065200ef6",
    "semantic_title": "mmneuron: discovering neuron-level domain-specific interpretation in multimodal large language model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1esqcgwDzI": {
    "title": "Using LLMs to simulate students' responses to exam questions",
    "volume": "review",
    "abstract": "Previous research leveraged Large Language Models (LLMs) in numerous ways in the educational domain. Here, we show that they can be used to answer exam questions simulating students of different skill levels and share a prompt, engineered for GPT-3.5, that enables the simulation of varying student skill levels on questions from different educational domains. We evaluate the proposed prompt on three publicly available datasets (one from science exams and two from English reading comprehension exams) and three LLMs (two versions of GPT-3.5 and one of GPT-4), and show that it is robust to different educational domains and capable of generalising to data unseen during the prompt engineering phase. We also show that, being engineered for a specific version of GPT-3.5, the prompt does not generalise well to different LLMs, stressing the need for prompt engineering for each model in practical applications. Lastly, we find that there is not a direct correlation between the quality of the rationales obtained with chain-of-thought prompting and the accuracy in the student simulation task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cvgxTh4WHi": {
    "title": "IAO prompting: Forcing Large Language Models to Show their Reasoning through an Input-Action-Output Template",
    "volume": "review",
    "abstract": "The effectiveness of Large Language Models (LLMs) in tackling diverse reasoning problems is further improved by chain-of-thought (CoT) prompting, which makes the intermediate reasoning steps apparent. In this work, we introduce IAO (Input-Action-Output) prompting, a straightforward template based prompting method that allows the complex reasoning process to be explicitly modelled in a structured manner. IAO autonomously breaks down problems into a series of simpler reasoning steps and then solves them in sequence, each with explicit input information, action applied, and intermediate output. The solved steps inform the subsequent steps, facilitating progressive reasoning. This explicit structure not only improves reasoning performance but also interpretability and transparency. Experiments across various reasoning tasks demonstrate IAO's strong zero-shot capabilities. Human evaluation validates the transparency and interpretability of IAO reasoning chains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RV4s4JI6lu": {
    "title": "Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks",
    "volume": "review",
    "abstract": "Medical knowledge is context-dependent and requires consistent reasoning across various natural language expressions of semantically equivalent phrases. This is particularly crucial for drug names, where patients often use brand names like Advil or Tylenol instead of their generic equivalents. To study this, we create a new robustness dataset, \\textbf{RABBITS}, to evaluate performance differences on medical benchmarks after swapping brand and generic drug names using physician expert annotations. We assess both open-source and API-based LLMs on MedQA and MedMCQA, revealing a consistent performance drop ranging from 1-10\\%. Furthermore, we identify a potential source of this fragility as the contamination of test data in widely used pre-training datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NuCvSx6t6V": {
    "title": "Multi-Target Cross-Lingual Summarization: a novel task and a language-neutral approach",
    "volume": "review",
    "abstract": "Cross-lingual summarization aims to bridge language barriers by summarizing documents in different languages. However, ensuring semantic coherence across languages is an overlooked challenge and can be critical in several contexts. To fill this gap, we introduce multi-target cross-lingual summarization as the task of summarizing a document into multiple target languages while ensuring that the produced summaries are semantically similar. We propose a principled re-ranking approach to this problem and a multi-criteria evaluation protocol to assess semantic coherence across target languages, marking a first step that will hopefully stimulate further research on this problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oXMTMJY7Li": {
    "title": "Imitation Game is Not Optimal: Alleviating Autoregressive Bias in Non-Autoregressive Transformers",
    "volume": "review",
    "abstract": "Non-autoregressive Transformer (NART) models predict tokens independently, which presents challenges in capturing token dependencies. Previous approaches have incorporated the Autoregressive (AR) token dependency into the NART models, leading to a discrepancy known as AR exposure bias during the training and decoding processes of NART models, adversely affecting generation quality. We propose two novel approaches that facilitate the recovery of future context information, thereby mitigating AR exposure bias. First, Bidirectional Contextual Knowledge Distillation (BCKD) leverages AR teacher models to distill bidirectional token correlation information, enhancing via data augmentation. Second, the Bidirectional Contextual Transformer (BC-Transformer) captures global contextual information through its innovative graph architecture. Experiments demonstrate that our BC-Transformer achieves translation quality comparable to that of the Autoregressive Transformer (ART) while maintaining the superior generation speed of the DA-Transformer. When both proposed methods are incorporated, NART models significantly outperform ART models (p<0.03). Further analysis reveals that the BC-Transformer surpasses AR baseline models in the translation of long sentences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FaatIWMJ0r": {
    "title": "Harnessing Instruction-Tuned Large Language Model for Guiding End-to-End Speech Recognition",
    "volume": "review",
    "abstract": "Modern large language models (LLMs) are adept at performing various text generation tasks when prompted with instructions designed for specific objectives. These abilities can enhance the quality of text produced by automatic speech recognition (ASR), enabling the selection of words that are more semantically accurate. However, relying solely on LLMs to correct errors in ASR predictions may lead to unintended word generations or modifications that do not accurately reflect the speech input. In this work, we propose a novel ASR model that integrates the text generation capabilities of LLMs, while ensuring proper alignment with speech inputs. Specifically, our model is built on the attention-based encoder-decoder (AED) structure, with the LLM serving as a front-end feature extractor for the decoder. The decoder is trained to predict words from the LLM-derived features, where cross-attention accounts for aligning these features with the speech encodings from the encoder. We also design an effective prompting strategy that uses a hypothesized text sequence to extract linguistic information beneficial for performing ASR. Experimental results demonstrate that our proposed model outperforms conventional AED-based models across major ASR tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kCKirFfXdW": {
    "title": "Transformer-CTP: Current Token Prediction Using Cross-Attention of Queries with Current Position Information",
    "volume": "review",
    "abstract": "Transformer and pre-trained language models have advanced various tasks in artificial intelligence. Typically, transformer decoders for language generation are trained using LM loss. The LM loss function predicts the next token from the outputs of previous tokens. LM loss is trained such that transformer decoders can generate autoregressively. In addition, transformers use self-attention to derive outputs, which generally assigns a high attention weight to self-tokens. Therefore, the transformer decoder may over-focus on the token $t-1$ when predicting token $t$ because it predicts token $t$ through self-attention up to token $t-1$. The proposed method prevents the transformer decoder from overfocusing on token $t-1$ when predicting token $t$. Instead of predicting token $t$ using the output of token $t-1$, we use a new input to predict token $t$. We also add a CPT module to the transformer decoder, which prevents token $t-1$ from being used in the attention query by cross-attention using the new input. Moreover, we measured the performance of machine translation and document summarization to verify that the proposed methodology can mitigate overfocusing problem and improve the performance. In our experiments, the proposed methodology improved performance. Also, it can distribute the focused attention to a few specific tokens, including the self-token. The code for the experiment can be found on our GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ke9x4M7Aw2": {
    "title": "DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection",
    "volume": "review",
    "abstract": "Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to identify a smaller, representative dataset to fine-tune PLMs for text-generation needed for text editing tasks such as simplification, grammar correction, clarity, etc. We examine the efficacy of DEFT-UCS across multiple text-editing tasks, and compare to the state-of-the art text-editing model, CoEDIT. Our results demonstrate that DEFT-UCS models are just as accurate as CoEDIT, across eight different datasets consisting of six different editing tasks, while finetuned on 70% less data",
    "checked": false,
    "id": "5c72a3d6c3e3245b7c2fee751f92e834bc1344d3",
    "semantic_title": "deft: data efficient fine-tuning for pre-trained language models via unsupervised core-set selection",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=efHQ3cTXWs": {
    "title": "Precise Length Control for Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have become integral components of production systems, with applications ranging from chatbots like ChatGPT to tasks such as summarisation and question answering. However, a significant challenge with LLMs is the unpredictability of response length, which is particularly problematic in tasks requiring varying levels of detail, such as document summarisation. Here we present a method to adapt existing LLMs to allow control of response length. We achieve this by extending the length-difference positional encoding (LDPE) proposed by (Takase and Okazaki, 2019) to decoder-only transformer architectures. Our approach, termed offset reverse positional encoding (ORPE), uses a positional encoding that counts down from a predetermined response length. Finetuning with ORPE enables the model to learn to structure its responses to terminate at a given length. Our results, obtained from tasks such as question answering and document summarisation, demonstrate that ORPE provides precise control of the response length during inference",
    "checked": false,
    "id": "55a9e7e09d3ff5df147cf4ed85f0387a4d5da149",
    "semantic_title": "copyright traps for large language models",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=PHAVxHKRZc": {
    "title": "Predicting generalization performance with correctness discriminators",
    "volume": "review",
    "abstract": "The ability to predict an NLP model's accuracy on unseen, potentially out-of-distribution data is a prerequisite for trustworthiness. We present a novel model that establishes upper and lower bounds on the accuracy, without requiring gold labels for the unseen data. We achieve this by training a $discriminator$ which predicts whether the output of a given sequence-to-sequence model is correct or not. We show across a variety of tagging, parsing, and semantic parsing tasks that the gold accuracy is reliably between the predicted upper and lower bounds, and that these bounds are remarkably close together",
    "checked": true,
    "id": "c38738d33c6e74b79f286f0b345e51da71c71375",
    "semantic_title": "predicting generalization performance with correctness discriminators",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pBHePq1B6e": {
    "title": "Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning",
    "volume": "review",
    "abstract": "Fine-tuning is the primary methodology for tailoring pre-trained large language models to specific tasks. As the model's scale and the diversity of tasks expand, parameter-efficient fine-tuning methods are of paramount importance. One of the most widely used family of methods is low-rank adaptation (LoRA) and its variants. LoRA encodes weight update as the product of two low-rank matrices. Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks. We introduce Chain of LoRA (COLA), an iterative optimization framework inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full parameter fine-tuning, without incurring additional computational costs or memory overheads. COLA employs a residual learning procedure where it merges learned LoRA modules into the pre-trained language model parameters and re-initialize optimization for new born LoRA modules. We provide theoretical convergence guarantees as well as empirical results to validate the effectiveness of our algorithm. Across various models (OPT and Llama-2) and 11 benchmarking tasks, we demonstrate that COLA can consistently outperform LoRA without additional computational or memory costs",
    "checked": true,
    "id": "5122bd001ec67d80543abe284bf7e0bf31da45d5",
    "semantic_title": "chain of lora: efficient fine-tuning of language models via residual learning",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=rWzECppYr4": {
    "title": "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention",
    "volume": "review",
    "abstract": "Improving the effectiveness and efficiency of large language models (LLMs) simultaneously is a critical yet challenging research goal. In this paper, we find that low-rank pre-training, normall considered as efficient methods that will compromise performance, can be scalably effective when reduced parameters are precisely targeted. Specifically, by applying low-dimensional module only to the attention layer -- resolves this issue and enhances both effectiveness and efficiency. We refer to this structure as \\textsl{Low-dimensional Projected Attention (LPA)} and provide an explanatory analysis. Through extensive experimentation at parameter scales of 130M, 370M, and scaling up to 3B, we have validated the effectiveness and scalability of LPA. Our results show that LPA model can save up to 12.4\\% in time while achieving an approximate 5\\% improvement in test perplexity (ppl) and on downstream tasks compared with vanilla Transformer",
    "checked": false,
    "id": "d73247905abdea4edc7ff23f8152e6c1bcee8ce1",
    "semantic_title": "scomoe: efficient mixtures of experts with structured communication",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=jouW22PVUo": {
    "title": "AlignSum: Data Pyramid Hierarchical Fine-tuning for Aligning with Human Summarization Preference",
    "volume": "review",
    "abstract": "Text summarization tasks commonly employ Pre-trained Language Models (PLMs) to fit diverse standard datasets. While these PLMs excel in automatic evaluations, they frequently underperform in human evaluations, indicating a deviation between their generated summaries and human summarization preferences. This discrepancy is likely due to the low quality of fine-tuning datasets and the limited availability of high-quality human-annotated data that reflect true human preference. To address this challenge, we introduce a novel human summarization preference alignment framework AlignSum. This framework consists of three parts: Firstly, we construct a Data Pymarid with extractive, abstractive, and human-annotated summary data. Secondly, we conduct the Gaussian Resampling to remove summaries with extreme lengths. Finally, we implement the two-stage hierarchical fine-tuning with Data Pymarid after Gaussian Resampling. We apply AlignSum to PLMs on the human-annotated CNN/DailyMail and BBC XSum datasets. Experiments show that with AlignSum, PLMs like BART-Large surpass 175B GPT-3 in both automatic and human evaluations. This demonstrates that AlignSum significantly enhances the alignment of language models with human summarization preferences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MpoeoSmpiK": {
    "title": "How Effective are State Space Models for Machine Translation?",
    "volume": "review",
    "abstract": "Transformers are the current architecture of choice for NLP, but their attention layers do not scale well to long contexts. Recent works propose to replace attention with linear recurrent layers --- this is the case for state space models, which enjoy efficient training and inference. However, it remains unclear whether these models are competitive with transformers in machine translation (MT). In this paper, we provide a rigorous and comprehensive experimental comparison between transformers and linear recurrent models for MT. Concretely, we experiment with RetNet, Mamba, and hybrid versions of Mamba which incorporate attention mechanisms. Our findings demonstrate that Mamba is highly competitive with transformers on sentence and paragraph-level datasets, where in the latter both models benefit from shifting the training distribution towards longer sequences. Further analysis show that integrating attention into Mamba improves translation quality, robustness to sequence length extrapolation, and the ability to recall named entities",
    "checked": true,
    "id": "34aae207d1ed64c32466f38054ed0a0cc1390158",
    "semantic_title": "how effective are state space models for machine translation?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Mrc2ilbeUw": {
    "title": "A Split-and-Privatize Framework for Large Language Model Fine-Tuning",
    "volume": "review",
    "abstract": "Fine-tuning is a prominent technique to adapt a pre-trained language model to downstream scenarios. In parameter-efficient fine-tuning, only a small subset of modules are trained over the downstream datasets, while leaving the rest of the pre-trained model frozen to save computation resources. In recent years, a popular productization form arises as Model-as-a-Service (MaaS), in which vendors provide abundant pre-trained language models, server resources and core functions, and customers can fine-tune, deploy and invoke their customized model by accessing the one-stop MaaS with their own private dataset. In this paper, we identify the model and data privacy leakage risks in MaaS fine-tuning, and propose a Split-and-Privatize (SAP) framework, which manage to mitigate the privacy issues by adapting the existing split learning architecture. Furthermore, we propose a contributing-token-identification (CTI) method to alleviate the utility degradation caused by privatization. The proposed framework is sufficiently investigated by experiments, and the results indicate that it can enhance the empirical privacy by $68\\%$ at the cost of $1\\%$ model performance degradation on the Stanford Sentiment Treebank dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G7tDvnp4hV": {
    "title": "Evaluating Automatic Metrics with Incremental Machine Translation Systems",
    "volume": "review",
    "abstract": "We introduce a dataset comprising commercial machine translations, gathered weekly over six years across 12 translation directions. Since human A/B testing is commonly used, we assume commercial systems improve over time, which enables us to evaluate machine translation (MT) metrics based on their preference for more recent translations. Our study confirms several previous findings in MT metrics research and demonstrates the dataset's value as a testbed for metric evaluation",
    "checked": true,
    "id": "4425962f05871828d91cad596a3c54c26208b12a",
    "semantic_title": "evaluating automatic metrics with incremental machine translation systems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wlZdXyB5tI": {
    "title": "Dialog2Flow: Pre-training Action-Driven Soft Contrastive Learning Embeddings for Automatic Dialog Flow Extraction",
    "volume": "review",
    "abstract": "Efficiently deriving structured workflows from unannotated dialogs remains an underexplored and formidable challenge in computational linguistics. Automating this process could significantly accelerate the manual design of workflows in new domains and enable the grounding of large language models in domain-specific flowcharts, enhancing transparency and controllability. In this paper, we introduce Dialog2Flow (D2F) embeddings, which differ from conventional sentence embeddings by mapping utterances to a latent space where they are grouped according to their communicative and informative functions (i.e., the actions they represent). D2F allows for modeling dialogs as continuous trajectories in a latent space with distinct action-related regions. By clustering D2F embeddings, the latent space is quantized, and dialogs can be converted into sequences of region/action IDs, facilitating the extraction of the underlying workflow. To pre-train D2F, we build a comprehensive dataset by unifying twenty task-oriented dialog datasets with normalized per-turn action annotations. We also introduce a novel soft contrastive loss that leverages the semantic information of these actions to guide the representation learning process, showing superior performance compared to standard supervised contrastive loss. Evaluation against various sentence embeddings, including dialog-specific ones, demonstrates that D2F yields superior qualitative and quantitative results across diverse domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PrrIxDCML5": {
    "title": "Do Language Models Understand Human Needs on Text Summarization?",
    "volume": "review",
    "abstract": "With the popularity of large language models and their high-quality text generation capabilities, researchers are using them as auxiliary tools for text summary writing. Although summaries generated by these large language models are smooth and capture key information sufficiently, the quality of their output depends on the prompt and the generated text is somewhat procedural to a certain extent. In order to understand whether large language models truly understand human needs, we construct LecSumm, in which we recruit 200 college students to write summaries for lecture notes on ten different machine learning topics, and analyzed real world human summary needs in the dimensions of summary length structure, modality and content depth. We further evaluate fine-tuned and prompt-based language models on LecSumm and show that the commercial GPT models showed better performance in summary coherence, fluency and relevance, but still fall shot in faithfulness and can better capture human needs even with advanced prompt design while fine-tuned models do not effectively learn human needs from the data. Our LecSumm dataset brings new challenges to both fine-tuned models and prompt-based large language models on the task of human-centered text summarization",
    "checked": false,
    "id": "ab0fa7d219908c04a7b8dde51c1bb5a2a4caec3b",
    "semantic_title": "cat-bench: benchmarking language model understanding of causal and temporal dependencies in plans",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xaSLpHVQPH": {
    "title": "Retrieval-Augmented Text-Only Training for Image Captioning",
    "volume": "review",
    "abstract": "Image captioning has drawn remarkable attention from the natural language processing and computer vision fields. Aiming to reduce the reliance on curated data, several studies have explored image captioning without relying on any humanly-annotated image-text pairs, although existing methods are still outperformed by fully supervised approaches. This paper proposes TTLLCap, i.e. a text-only training method for image captioning, based on prompting a pre-trained language model decoder with information obtained from CLIP representations of the inputs. Specifically, we experimented with the combined use of (a) retrieved examples of captions, (b) relevant concepts for the input, and (c) latent vector representations. Through extensive experiments, we show that TTLLCap outperforms previous training-free methods, and is also competitive with other text-only training methods. We also analyze the impact of different choices regarding the configuration of the retrieval-augmentation component. The source code supporting our experiments is available from a public GitHub repository",
    "checked": false,
    "id": "860ce5b4b1c32d129d0975b945df49f0698d8359",
    "semantic_title": "retrieval-augmented egocentric video captioning",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=ab89SCgRad": {
    "title": "Learning Dynamic Multi-attribute Interest for Personalized Product Search",
    "volume": "review",
    "abstract": "Personalized product search aims to learn personalized preferences from search logs and adjust the ranking lists returned by engines. Previous studies have extensively explored excavating valuable features to build accurate interest profiles. However, they overlook that the user's attention varies on product attributes(e.g., brand, category). Users may especially prefer specific attributes or switch their preferences between attributes dynamically. Instead, existing approaches mix up all attribute features and let the model automatically extract useful ones from rather complex scenarios. To solve this problem, in this paper, we propose a dynamic multi-attribute interest learning model to tackle the influences from attributes to user interests. Specifically, we design two interest profiling modules: attribute-centered and attribute-aware profiling. The former focuses on capturing the user's preferences on a single attribute, while the latter focuses on addressing the interests correlated with multi-attribute within the search history. Besides, we devise a dynamic contribution weights strategy that sends explicit signals to the model to determine the impacts of different attributes better. Experimental results on large-scale datasets illustrate that our model significantly improves the results of existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yaevypXikZ": {
    "title": "On Leakage of Code Generation Evaluation Datasets",
    "volume": "review",
    "abstract": "In this paper we consider contamination by code generation test sets, in particular in their use in modern large language models. We discuss three possible sources of such contamination and show findings supporting each of them: (i) direct data leakage, (ii) indirect data leakage through the use of synthetic data and (iii) overfitting to evaluation sets during model selection. Key to our findings is a new dataset of 161 prompts with their associated python solutions, dataset which we plan to release with this paper",
    "checked": true,
    "id": "3f37462afde6c573d78abfc5da73c93ec0998713",
    "semantic_title": "on leakage of code generation evaluation datasets",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lj3uI1XUQl": {
    "title": "Scope-enhanced Compositional Semantic Parsing for DRT",
    "volume": "review",
    "abstract": "Discourse Representation Theory (DRT) distinguishes itself from other semantic representation frameworks by its ability to model complex semantic and discourse phenomena through structural nesting and variable binding. While seq2seq models hold the state of the art on DRT parsing, their accuracy degrades with the complexity of the sentence, and they sometimes struggle to produce well-formed DRT representations. We introduce the AMS parser, a compositional, neurosymbolic semantic parser for DRT. It rests on a novel mechanism for predicting quantifier scope. We show that the AMS parser reliably produces well-formed outputs and performs well on DRT parsing, especially on complex sentences",
    "checked": true,
    "id": "1e463b37c55df14c0d6c406e174439df7f19060a",
    "semantic_title": "scope-enhanced compositional semantic parsing for drt",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cs5PeRshZ1": {
    "title": "AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding",
    "volume": "review",
    "abstract": "AI personal assistants deployed via robots or wearables require embodied understanding to collaborate with humans effectively. However, current Vision-Language Models (VLMs) primarily focus on third-person view videos, neglecting the richness of egocentric perceptual experience. To address this gap, we propose three key contributions. First, we introduce the Egocentric Video Understanding Dataset (EVUD) for training VLMs on video captioning and question answering tasks specific to egocentric videos. Second, we present \\model, a 7B parameter VLM trained using parameter-efficient methods on EVUD. Finally, we evaluate \\model's capabilities on OpenEQA, a challenging benchmark for embodied video question answering. Our model achieves state-of-the-art performance, outperforming open-source models including strong Socratic models using GPT-4 as a planner by 3.6%. Additionally, we outperform Claude 3 and Gemini Pro Vision 1.0 and showcase competitive results compared to Gemini Pro 1.5 and GPT-4V, even surpassing the latter in spatial reasoning. This research paves the way for building efficient VLMs that can be deployed in robots or wearables, leveraging embodied video understanding to collaborate seamlessly with humans in everyday tasks, contributing to the advancement of next-generation Embodied AI",
    "checked": true,
    "id": "1872b0a2ad3d44ca325ac80ccea5788c9b4a6574",
    "semantic_title": "alanavlm: a multimodal embodied ai foundation model for egocentric video understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AUK1LfrOfX": {
    "title": "Raker: A Relation-aware Knowledge Reasoning Model for Inductive Relation Prediction",
    "volume": "review",
    "abstract": "Inductive relation prediction, an important task for knowledge graph completion, is to predict the relations between entities that are unseen at the training stage. The latest methods use pre-trained language models (PLMs) to encode the paths between the head entity and tail entity and achieve state-of-the-art prediction performance. However, these methods cannot well handle no-path situations and are also unable to learn comprehensive relation representations for distinguishing different relations to overcome the difficulty of inductive relation prediction. To tackle this issue, we propose a novel \\textbf{R}elation-\\textbf{a}ware \\textbf{k}nowledg\\textbf{e} \\textbf{r}easoning model entitled Raker which introduces an adaptive reasoning information extraction method to identify relation-aware reasoning neighbors of entities in the target triple to handle no-path situations, and enables the PLM to be more aware of the possible relations by the relation-specific soft prompting. Raker is evaluated on three public datasets and achieves SOTA performance in inductive relation prediction when compared with the baseline methods. Notably, the absolute improvement of Raker is even more than 10\\% on the FB15k-237 dataset in the inductive setting. Moreover, Raker also demonstrates its superiority in transductive and few-shot settings. The code of Raker is available at \\href{https://anonymous.4open.science/r/Raker-9234}{https://anonymous.4open.science/r/Raker-9234}",
    "checked": false,
    "id": "3b7761b12b39d5436823b37a5c5e1ee0feb3ce68",
    "semantic_title": "substructure-aware subgraph reasoning for inductive relation prediction",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=vRkKEWpfsZ": {
    "title": "Unveiling Multi-level and Multi-modal Semantic Representations in the Human Brain using Large Language Models",
    "volume": "review",
    "abstract": "In recent studies, researchers have used large language models (LLMs) to explore semantic representations in the brain; however, they have typically assessed different levels of semantic content, such as speech, objects, and stories, separately. In this study, we recorded brain activity using functional magnetic resonance imaging (fMRI) while participants viewed 8.3 hours of dramas and movies. We annotated these stimuli at multiple semantic levels, which enabled us to extract latent representations of LLMs for this content. Our findings demonstrate that LLMs predict human brain activity more accurately than traditional language models, particularly for complex background stories. Furthermore, we identify distinct brain regions associated with different semantic representations, including multi-modal vision-semantic representations, which highlights the importance of modeling multi-level and multi-modal semantic representations simultaneously. We will make our fMRI dataset publicly available to facilitate further research on aligning LLMs with human brain function",
    "checked": false,
    "id": "f9e5844a6bcff29f0e54374c345818331dae4d36",
    "semantic_title": "mindsemantix: deciphering brain visual experiences with a brain-language model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=33HI0UVKtJ": {
    "title": "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning",
    "volume": "review",
    "abstract": "Reinforcement learning from human feedback (RLHF) and AI-generated feedback (RLAIF) have become prominent techniques that significantly enhance the functionality of pre-trained language models (LMs). These methods harness feedback, sourced either from humans or AI, as direct rewards or to shape reward models that steer LM optimization. Nonetheless, the effective integration of rewards from diverse sources presents a significant challenge due to their disparate characteristics. To address this, recent research has developed algorithms incorporating strategies such as weighting, ranking, and constraining to handle this complexity. Despite these innovations, a bias toward disproportionately high rewards can still skew the reinforcement learning process and negatively impact LM performance. This paper explores a methodology for reward composition that enables simultaneous improvements in LMs across multiple dimensions. Inspired by fairness theory, we introduce a training algorithm that aims to reduce disparity and enhance stability among various rewards. Our method treats the aggregate reward as a dynamic weighted sum of individual rewards, with alternating updates to the weights and model parameters. For efficient and straightforward implementation, we employ an estimation technique rooted in the mirror descent method for weight updates, eliminating the need for gradient computations. The empirical results under various types of rewards across a wide range of scenarios demonstrate the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T61vy7JML8": {
    "title": "ABSEval: An Agent-based Framework for Script Evaluation",
    "volume": "review",
    "abstract": "Recent research indicates that large language models (LLMs) possess a certain degree of script planning capability. However, there is still a lack of focused work on evaluating scripts generated by LLMs. The evaluation of scripts poses challenges due to their logical structure, sequential organization, adherence to commonsense constraints, and open-endedness. In this work, We introduced a novel script evaluation dataset, MCScript, consisting of more than 1,500 script evaluation tasks and steps, and developed an agent-based script evaluation framework, ABSEval, to collaboratively evaluate scripts generated by LLMs. Our experiments demonstrate that ABSEval provides superior accuracy and relevance, aligning closely with human evaluation. We evaluated the script planning capabilities of 15 mainstream LLMs and provided a detailed analysis. Furthermore, we observed phenomena like the key factor influencing the script planning ability of LLM is not parameter size and suggested improvements for evaluating open-ended questions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SMMUERMSlW": {
    "title": "Interpreting and Auditing Biases between Bengali Cultural Dialects in Large Language Models with Evaluation and Mitigation Strategies",
    "volume": "review",
    "abstract": "Though Large Language Models (LLMs) have created a massive technological impact, allowing for human-enabled applications, they have the potential to exhibit stereotypes and biases, particularly when dealing with low-resource languages and sensitive topics like cultural differences. We investigate cultural bias in LLMs by evaluating their performance on Hindu and Muslim-majority cultural dialects of Bengali, and extend this with a user satisfaction study. Through human-centric evaluation and cultural analytics, we assess ChatGPT, Gemini, and Microsoft Copilot using a curated dataset to analyze their handling of culturally-specific words and mitigation of social biases. Our work contributes to human-centric NLP and LLM auditing by exploring reasons for biases observed and strategies for evaluation and mitigation. We aim to promote fairness in LLMs, considering their global impact with over 300 million speakers worldwide",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1PreUjdgE": {
    "title": "Beyond \"Using Their Own Words\": Abstractivity Characterization in Summarization",
    "volume": "review",
    "abstract": "In this work, we present an extension of the definition of abstractivity within the scope of the automatic generation of summaries. We propose to join extractivity and abstractivity in a single dimension, where extractivity would be on one side of the dimension and complete abstractivity on the opposite one, but in between, there would be levels of abstractivity. A dataset manually annotated to characterize the level of abstractivity of the summaries and to measure the presence of a set of actions applied to compose the summaries has been built. Using this dataset, a study of the sample distribution in terms of abstractivity, annotator agreement, and correlation between annotations regarding the set of actions is presented. An experimental work with a double objective is carried out; on the one hand, we want to validate our perception that extractivity and complete abstractivity are extreme points of a single dimension with multiple abstractivity levels, and on the other hand, we want to verify if there is an overall correlation between the frequency of the actions used for creating the summary and the level of abstractivity. The results confirm both objectives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RIDEuxMvhj": {
    "title": "Impact of LLM on Reinforcement Learning",
    "volume": "review",
    "abstract": "Text embedding is essential for language understanding tasks. Large language models (LLMs) have recently emerged for text embedding due to their ability to capture meaningful knowledge. Leveraging text-based adventure games as a test bed, we explore the impact of different language models on Reinforcement Learning (RL) behavior. The results show that contrary to common assumptions, larger embedding models do not guarantee better performance over smaller model sizes. Instead, the optimal model size depends on the specific game environment",
    "checked": false,
    "id": "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817",
    "semantic_title": "a survey of reinforcement learning from human feedback",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=NV2jzRDy6u": {
    "title": "STARD: A Chinese Statute Retrieval Dataset Derived from Real-life Queries by Non-professionals",
    "volume": "review",
    "abstract": "Statute retrieval aims to find relevant statutory articles for specific queries. This process is the basis of a wide range of legal applications such as legal advice, automated judicial decisions, legal document drafting, etc. Existing statute retrieval benchmarks emphasize formal and professional queries from sources like bar exams and legal case documents, thereby neglecting non-professional queries from the general public, which often lack precise legal terminology and references. To address this gap, we introduce the STAtute Retrieval Dataset (STARD), a Chinese dataset comprising 1,543 query cases collected from real-world legal consultations and 55,348 candidate statutory articles. Unlike existing statute retrieval datasets, which primarily focus on professional legal queries, STARD captures the complexity and diversity of real queries from the general public. Through a comprehensive evaluation of various retrieval baselines, we reveal that existing retrieval approaches all fall short of these real queries issued by non-professional users. The best method only achieves a Recall@100 of 0.907, suggesting the necessity for further exploration and additional research in this area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M588ckNRIT": {
    "title": "More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs",
    "volume": "review",
    "abstract": "The performance on general tasks decreases after Large Language Models (LLMs) are fine-tuned on domain-specific tasks, the phenomenon is known as Catastrophic Forgetting (CF). However, this paper presents a further challenge for real application of domain-specific LLMs beyond CF, called General Capabilities Integration (GCI), which necessitates the integration of both the general capabilities and domain knowledge within a single instance. The objective of GCI is not merely to retain previously acquired general capabilities alongside new domain knowledge, but to harmonize and utilize both sets of skills in a cohesive manner to enhance performance on domain-specific tasks. Taking legal domain as an example, we carefully design three groups of training and testing tasks without lacking practicability, and construct the corresponding datasets. To better incorporate general capabilities across domain-specific scenarios, we introduce ALoRA, which utilizes a multi-head attention module upon LoRA, facilitating direct information transfer from preceding tokens to the current one. This enhancement permits the representation to dynamically switch between domain-specific knowledge and general competencies according to the attention. Extensive experiments are conducted on the proposed tasks. The results exhibit the significance of our setting, and the effectiveness of our method",
    "checked": true,
    "id": "2d5fbc53c2a7d142eeb9a53863413b7688a8b00f",
    "semantic_title": "more than catastrophic forgetting: integrating general capabilities for domain-specific llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=8xBMLAOZxq": {
    "title": "Obliviate: Neutralizing Task-agnostic Backdoors within the Parameter-efficient Fine-tuning Paradigm",
    "volume": "review",
    "abstract": "Parameter-efficient fine-tuning (PEFT) has become a key training strategy for large language models. However, its reliance on fewer trainable parameters poses security risks, such as task-agnostic backdoors. Despite their severe impact on a wide range of tasks, there is no practical defense solution available that effectively counters task-agnostic backdoors within the context of PEFT. In this study, we introduce Obliviate, a PEFT-integrable backdoor defense. We develop two techniques aimed at amplifying benign neurons within PEFT layers and penalizing the influence of trigger tokens. Our evaluations across three major PEFT architectures show that our method can significantly reduce the attack success rate of the state-of-the-art task-agnostic backdoors (83.6\\%$\\downarrow$). Furthermore, our method exhibits robust defense capabilities against both task-specific backdoors and adaptive attacks. Source code will be obtained at https://github.com/obliviateARR/Obliviate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jqsVAg2vhe": {
    "title": "Large Language Models Can Not Perform Well in Understanding and Manipulating Natural Language at Both Character and Word Levels?",
    "volume": "review",
    "abstract": "Despite their promising performance across various tasks, recent studies reveal that Large language models (LLMs) still exhibit significant deficiencies in handling several word-level and character-level tasks, e.g., word unscrambling and sentence editing, indicating urgent needs for substantial improvements in basic language understanding and manipulation. To address these challenges, it is crucial to develop large-scale benchmarks that can comprehensively assess the performance of LLMs in basic language tasks. In this paper, we introduce a bilingual benchmark, CWUM, to investigate the capabilities and limitations of LLMs in understanding and manipulating natural language at both character and word levels. CWUM consists of 15 simple text editing tasks, e.g., letter counting, word reversing, Chinese character inserting, etc. We conduct extensive experiments on eight advanced LLMs, including base models and instruction-tuned (chat) variants. The experimental results highlight significant failures of existing LLMs on CWUM tasks that humans can solve perfectly with 100% accuracy. On English tasks of CWUM, the average accuracy of GPT-4, LLaMA-3-70B, and Qwen-72B is 66.64%, 39.32%, and 33.16%, respectively, which lags far behind human performance. Instruction-tuning the base model does not lead to a distinct performance improvement, as the average accuracy of LLaMA-3-70B-Instruct on English tasks is only 1.44% higher than that of the base LLaMA-3-70B. Ultimately, we show that supervised fine-tuning (SFT) can enhance model performance on CWUM without compromising its ability to generalize across general tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nrHm7qGxc5": {
    "title": "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are powerful zero-shot assessors used in real-world situations such as assessing written exams and benchmarking systems. Despite these critical applications, no existing work has analyzed the vulnerability of judge-LLMs to adversarial manipulation. This work presents the first study on the adversarial robustness of assessment LLMs, where we demonstrate that short universal adversarial phrases can be concatenated to deceive judge LLMs to predict inflated scores. Since adversaries may not know or have access to the judge-LLMs, we propose a simple surrogate attack where a surrogate model is first attacked, and the learned attack phrase then transferred to unknown judge-LLMs. We propose a practical algorithm to determine the short universal attack phrases and demonstrate that when transferred to unseen models, scores can be drastically inflated such that irrespective of the assessed text, maximum scores are predicted. It is found that judge-LLMs are signficantly more susceptible to these adversarial attacks when used for absolute scoring, as oppopsed to comparative assessment. Our findings raise concerns on the reliability of LLM-as-a-judge methods, and emphasize the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios",
    "checked": true,
    "id": "09812e529903ff67c5fc5f1dcb2b3586eb3ffd23",
    "semantic_title": "is llm-as-a-judge robust? investigating universal adversarial attacks on zero-shot llm assessment",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=V3rHjGUZll": {
    "title": "Semantic Ontology for Paraphrase Classification",
    "volume": "review",
    "abstract": "Paraphrase classification is a useful NLP task used to identity texts with the same meaning. However, automated paraphrase classification is difficult to apply in practice due to the subjectivity involved in determining if two sentences are similar enough to considered paraphrases. We propose an ontology called Semantic Paraphrase Types (SPT) that describes a set of possible semantic relationships between two texts, covering two types of paraphrases and three types of non-paraphrases. Based on this ontology, we created a new set of labels on top of the commonly-used MRPC dataset, creating a new classification benchmark task called SPT Classification, including explanations for a subset of the dataset. We hope that our contributions will improve the usefulness of automatic paraphrase classification and generation methods for various real-world NLP applications. We will release the dataset and associated models and code for the baselines when the paper is accepted",
    "checked": false,
    "id": "03511b043d32a41dcbdd4109be268f585285672f",
    "semantic_title": "using paraphrasers to detect duplicities in ontologies",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5kbAvWJVtv": {
    "title": "Evaluating Object Hallucination in LVLMs: Can They Still See Removed Objects?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7SoHEUxKMr": {
    "title": "D2A2: Enhancing LLM Knowledge Distillation Efficiency and Performance with Difficulty-Aware and Adaptive Distillation Framework",
    "volume": "review",
    "abstract": "With the proliferation of large language models (LLMs), knowledge distillation (KD) has emerged as a promising methodology to address the challenges of large model sizes and high computational costs in real-world applications. However, existing KD methods often ignore the difficulty variations within the datasets used for distillation, leading to inefficient resource allocation and suboptimal training outcomes. In this paper, we propose Difficulty-Aware and Adaptive Distillation (D2A2), an efficient and performance-enhancing distillation framework. The key idea is to incorporate the inherent difficulty of problems, as indicated by the uncertainty of LLMs shown when making decisions about the ultimate predictions, into the distillation process. Specifically, we integrate this into data filtering and model training phases to enhance the effectiveness of distillation. In difficulty-aware data filtering phase, we use semantic entropy to measure response uncertainty and prioritize difficult samples for further distillation. In difficulty-adaptive training phase, we dynamically adjust the focus on challenging samples by updating the distillation loss based on the student model's performance. Comprehensive experiments demonstrate our framework outperforms existing methods with fewer data and exhibits versatile performance across various models and datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WAcRc6xNeu": {
    "title": "RoleCraft introduces a framework aimed at enhancing role-playing experiences in large language models by focusing on authentic and non-celebrity characters and emotional depth",
    "volume": "review",
    "abstract": "The development of large language models(LLMs) has initiated a new chapter in complex tasks such as role-playing, enhancing user interaction experiences by enabling models to imitate various characters.However, LLMs are somewhat lacking in their ability to portray lesser-known characters, especially in aspects of dialogue delivery and scriptwriting skills. To this end, we aim to swiftly acquire essential language skills for character development, greatly enhancing role-playing comfort. In this work, we present RoleCraft, an innovative framework designed to enrich personalized role-playing experiences. Central to this framework is RoleInstruct, a distinctive dataset featuring emotional annotations, transitioning from traditional celebrity-focused roles to more authentic, daily non-celebrity roles,each accompanied by carefully crafted character descriptions. We combined RoleInstruct with open-source instructions from the general domain, employing a hybrid instruction tuning strategy to create RoleCraft-GLM. Experiments in role-playing demonstrate that our model excels in generating dialogue that accurately reflects character traits and emotions, outperforming most mainstream LLMs, including GPT-4. Our datasets are available at https://anonymous.4open.science/r/RoleCraft-GLM-30C8/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qi80RWAEvg": {
    "title": "Efficient Aspect-Based Summarization with Small Language Models: A Use-Case on Climate Change Reports",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have revolutionized many fields of Natural Language Processing (NLP), including summarization. These systems, however, consist of billions of parameters and, as such, they have the crucial shortcoming of being energy-intensive. In this work, we present a thorough evaluation of very recent, small-sized LLMs (SLMs) on the task of Aspect-Based Summarization of Climate Change Reports. In doing so, we show that modern SLMs are sufficiently good for the task and can bring value in assisting with summarization for policymakers while being more efficient than their bigger counterparts without significant performance deterioration. We also show how energy consumption among SLMs themselves does not correlate with better performance, further proving the point that smaller models can be effectively used for the task. Finally, we release the new dataset that we collected to perform our experiments, from which we hope research in NLP for climate change and research in efficient Aspect-Based Summarization with LLMs can develop further",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=29wc2HNSau": {
    "title": "DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented Generation for Question-Answering",
    "volume": "review",
    "abstract": "Retrieval-Augmented Generation (RAG) has recently demonstrated the performance of Large Language Models (LLMs) in the knowledge-intensive tasks such as Question-Answering (QA). RAG expands the query context by incorporating external knowledge bases to enhance the response accuracy. However, it would be inefficient to access LLMs multiple times for each query and unreliable to retrieve all the relevant documents by a single query. We have found that even though there is low relevance between some critical documents and query, it is possible to retrieve the remaining documents by combining parts of the documents with the query. To mine the relevance, a two-stage retrieval framework called **D**ynamic-**R**elevant **R**etrieval-**A**ugmented **G**eneration (**DR-RAG**) is proposed to improve document retrieval recall and the accuracy of answers while maintaining efficiency. Additionally, a compact classifier is applied to two different selection strategies to determine the contribution of the retrieved documents to answering the query and retrieve the relatively relevant documents. Meanwhile, DR-RAG call the LLMs only once, which significantly improves the efficiency of the experiment. The experimental results on multi-hop QA datasets show that DR-RAG can significantly improve the accuracy of the answers and achieve new progress in QA systems",
    "checked": true,
    "id": "918fb17504fe62438e40c3340669ea53c202be04",
    "semantic_title": "dr-rag: applying dynamic document relevance to retrieval-augmented generation for question-answering",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=laASr36eUG": {
    "title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models",
    "volume": "review",
    "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model performance. In this work, we present a novel method of further improving performance by requiring models to compare multiple reasoning chains before generating a solution in a single inference step. We call this method Divergent CoT (DCoT). We find that instruction tuning on DCoT datasets boosts the performance of even smaller, and therefore more accessible, LLMs. Through a rigorous set of experiments spanning a wide range of tasks that require various reasoning types, we show that fine-tuning on DCoT consistently improves performance over the CoT baseline across model families and scales (1.3B to 70B). Through a combination of empirical and manual evaluation, we additionally show that these performance gains stem from models generating multiple divergent reasoning chains in a single inference step, indicative of the enabling of self-correction in language models. Our code and data are publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TLgsPw0Kay": {
    "title": "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems",
    "volume": "review",
    "abstract": "The inherent ambiguity of cause and effect boundaries poses a challenge in evaluating causal event extraction tasks. Traditional metrics like Exact Match and BertScore poorly reflect model performance, so we trained evaluation models to approximate human evaluation, achieving high agreement. We used them to perform Reinforcement Learning with extraction models to align them with human preference, prioritising semantic understanding. We successfully explored our approach through multiple datasets, including transferring an evaluator trained on one dataset to another as a way to decrease the reliance on human-annotated data. In that vein, we also propose a weak-to-strong supervision method that uses a fraction of the annotated data to train an evaluation model while still achieving high performance in training an RL model",
    "checked": true,
    "id": "6b41e39d73e3724152d8d861e3a90c62214b5397",
    "semantic_title": "weak reward model transforms generative models into robust causal event extraction systems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qJ6vPHLGWK": {
    "title": "From Evidence to Belief: A Bayesian Epistemology Approach to Language Models",
    "volume": "review",
    "abstract": "This paper investigates the knowledge of language models from the perspective of Bayesian epistemology. Specifically, it aims to explore whether language models can accurately incorporate evidence of varying levels of informativeness and reliability into their confidence and responses. As Bayesian epistemology interprets belief as confidence according to evidence, this study offers a new perspective on understanding the beliefs and knowledge of language models. We created a dataset with various types of evidence and analyzed its response and confidence using verbalized confidence, token probability, and sampling. From the perspective of verbalized confidence, our research has shown that we can interpret that language models can generally reflect evidence in their confidence and calibration. We also demonstrated that language models exhibit biases toward correct evidence, exploit unreasonable evidence, and ignore errors in the context, all of which can be interpreted as the epistemic character of language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XHybQ6JNLd": {
    "title": "A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading",
    "volume": "review",
    "abstract": "The utilization of Large Language Models (LLMs) in financial trading has primarily been concentrated within the stock market, aiding in economic and financial decisions. Yet, the unique opportunities presented by the cryptocurrency market, noted for its on-chain data's transparency and the critical influence of off-chain signals like news, remain largely untapped by LLMs. This work aims to bridge the gap by developing an LLM-based trading agent, CryptoTrade, which uniquely combines the analysis of on-chain and off-chain data. This approach leverages the transparency and immutability of on-chain data, as well as the timeliness and influence of off-chain signals, providing a comprehensive overview of the cryptocurrency market. CryptoTrade incorporates a reflective mechanism specifically engineered to refine its daily trading decisions by analyzing the outcomes of prior trading decisions. This research makes two significant contributions. Firstly, it broadens the applicability of LLMs to the domain of cryptocurrency trading. Secondly, it establishes a benchmark for cryptocurrency trading strategies. Through extensive experiments, CryptoTrade has demonstrated superior performance in maximizing returns compared to traditional trading strategies and time-series baselines across various cryptocurrencies and market conditions. Our code and data are available at \\url{https://anonymous.4open.science/r/CryptoTrade-Public-92FC/}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KmHs3XZwJw": {
    "title": "Noise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature",
    "volume": "review",
    "abstract": "We present a framework for detecting and categorizing noise in literary texts, demonstrated through its application to Danish and Norwegian literature from the late 19th century. Noise, understood as ``aberrant sonic behaviour,'' is not only an auditory phenomenon but also a cultural construct tied to the processes of civilization and urbanization. By leveraging topic modeling techniques and fine-tuned BERT-based language models trained on Danish and Norwegian texts, we analyze a corpus of over 800 novels to extract and examine noise-related topics. We identify and track the prevalence of noise in these texts, offering insights into the literary perceptions of noise during the Scandinavian ``Modern Breakthrough'' period (1870-1899). Our contributions include the development of a comprehensive dataset annotated for noise-related segments and their categorization into human-made, non-human-made, and musical noises. This study illustrates the framework's potential for enhancing the understanding of the relationship between noise and its literary representations, providing a deeper appreciation of the auditory elements that enrich literary works",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aumwGAptJW": {
    "title": "More Bang for your Context: Virtual Documents for Question Answering over Long Documents",
    "volume": "review",
    "abstract": "We deal with the problem of Question Answering (QA) over long documents, which poses a challenge for modern Large Language Models (LLMs). Although LLMs can handle increasingly longer context windows, they struggle to effectively utilize the long content. To address this issue, we introduce the concept of a virtual document (VDoc). A VDoc is created by selecting chunks from the original document that are most likely to contain the information needed to answer the user's question, while ensuring they fit within the LLM's context window. We hypothesize that providing a short and focused VDoc to the LLM is more effective than filling the entire context window with less relevant information. Our experiments confirm this hypothesis and demonstrate that using VDocs improves results on the QA task",
    "checked": false,
    "id": "31949afa6c13237a60c8acd51a71a20dd78168f3",
    "semantic_title": "chatgpt's capabilities for use in anatomy education and anatomy research",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=LQfxWLdBBI": {
    "title": "LowREm: A Repository of Word Embeddings for 87 Low-Resource Languages Enhanced with Multilingual Graph Knowledge",
    "volume": "review",
    "abstract": "Contextualized embeddings based on large language models (LLMs) are available for various languages, but their coverage is often limited for lower resourced languages. Training LLMs for such languages is often difficult due to insufficient data and high computational cost. Especially for very low resource languages, static word embeddings thus still offer a viable alternative. There is, however, a notable lack of comprehensive repositories with such embeddings for diverse languages. To address this, we present LowREm, a centralized repository of static embeddings for 87 low-resource languages. We also propose a novel method to enhance GloVe-based embeddings by integrating multilingual graph knowledge, utilizing another source of knowledge, which is beneficial especially for low-resource languages. We demonstrate the superior performance of our enhanced embeddings as compared to contextualized embeddings extracted from XLM-R on sentiment analysis. Our code and data are publicly available under URL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3MaDUX0SXc": {
    "title": "PFME: A Modular Approach for Fine-grained Hallucination Detection and Editing of Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) excel in fluency but risk producing inaccurate content, called \"hallucinations.\" This paper outlines a standardized process for categorizing fine-grained hallucination types~\\cite{mishra2024fine-fava} and proposes an innovative frameworkâ€”the Progressive Fine-grained Model Editor (PFME)â€”specifically designed to detect and correct fine-grained hallucinations in LLMs. PFME consists of two collaborative modules: the Real-time Fact Retrieval Module and the Fine-grained Hallucination Detection and Editing Module. The former identifies key entities in the document and retrieves the latest factual evidence from credible sources. The latter further segments the document into sentence-level text and, based on relevant evidence and previously edited context, identifies, locates, and edits each sentence's hallucination type. Experimental results on FavaBench and FActScore demonstrate that PFME outperforms existing methods in fine-grained hallucination detection tasks. Particularly, when using the Llama3-8B-Instruct model, PFME's performance in fine-grained hallucination detection with external knowledge assistance improves by 8.7 percentage points (pp) compared to ChatGPT. In editing tasks, PFME further enhances the FActScore of FActScore-Alpaca13B and FActScore-ChatGPT datasets, increasing by 16.2pp and 4.6pp, respectively",
    "checked": true,
    "id": "5cfadb18e4e3c471d2dc991cf369608324ca8771",
    "semantic_title": "pfme: a modular approach for fine-grained hallucination detection and editing of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DCpapNkSPo": {
    "title": "Unified Active Retrieval for Retrieval Augmented Generation",
    "volume": "review",
    "abstract": "In Retrieval-Augmented Generation (RAG), retrieval is not always helpful and applying it to every instruction is sub-optimal. Therefore, determining whether to retrieve is crucial for RAG, which is usually referred to as Active Retrieval. However, existing active retrieval methods face two challenges: 1. They usually rely on a single criterion, which struggles with handling various types of instructions. 2. They depend on specialized and highly differentiated procedures, and thus combining them makes the RAG system more complicated and leads to higher response latency. To address these challenges, we propose Unified Active Retrieval (UAR). UAR contains four orthogonal criteria and casts them into plug-and-play classification tasks, which achieves multifaceted retrieval timing judgements with negligible extra inference cost. We further introduce the Unified Active Retrieval Criteria (UAR-Criteria), designed to process diverse active retrieval scenarios through a standardized procedure. Experiments on four representative types of user instructions show that UAR significantly outperforms existing work on the retrieval timing judgement and the performance of downstream tasks, which shows the effectiveness of UAR and its helpfulness to downstream tasks",
    "checked": true,
    "id": "17175b8310d55494cff278a019b7d648fc2308f2",
    "semantic_title": "unified active retrieval for retrieval augmented generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qerJguRqHn": {
    "title": "A Novel Computational Modeling Foundation for Automatic Coherence Assessment",
    "volume": "review",
    "abstract": "Coherence is an essential property of well-written texts, that refers to the way textual units relate to one another. In the era of generative AI, coherence assessment is essential for many NLP tasks; summarization, long-form question-answering, etc. Current NLP approaches for modeling coherence often rely on a proxy task, specifically sentence reordering. However, such an approach may not capture the full range of factors contributing to coherence. To bridge this gap, in this work we employ the formal linguistic definition of \\citet{Reinhart:1980} of what makes a discourse coherent, consisting of three conditions --- {\\em cohesion, consistency} and {\\em relevance} -- and formalize these conditions as respective computational tasks. We hypothesize that (i) a model trained on all of these tasks will learn the features required for coherence detection, and that (ii) a joint model for all tasks will exceed the performance of models trained on each task individually. We evaluate this modeling approach on two human-rated coherence benchmarks: one of automatically-generated stories and one of real-world texts. Our experiments confirm that jointly training on the proposed tasks leads to better performance on each task compared with task-specific models, and to better performance on assessing coherence overall, compared with strong baselines. Our formal coherence framework paves the way for advanced, broad-coverage automatic assessment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3CNNb3wtvT": {
    "title": "Perceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind in Large Language Models",
    "volume": "review",
    "abstract": "While humans naturally develop theory of mind (ToM), the capability to understand other people's mental states and beliefs, state-of-the-art large language models (LLMs) underperform on simple ToM benchmarks. We posit that we can extend our understanding about LLMs' ToM abilities by evaluating key human ToM precursors--perception inference and perception-to-belief inference--in LLMs. We introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate these precursory inferences for ToM in LLMs by annotating characters' perceptions within two existing ToM benchmarks, ToMi and FANToM. Our evaluation of eight state-of-the-art LLMs reveals that the models perform generally well in perception inference while exhibiting limited capability in perception-to-belief inference. Based on these results, we present PercepToM, a novel ToM method leveraging LLMs' strong perception inference capability while supplementing their limited perception-to-belief inference. Experimental results demonstrate that PercepToM significantly enhances LLM performance on the ToMi and FANToM benchmarks, especially in false belief scenarios",
    "checked": true,
    "id": "31aa31257a50530f817a9d35b971758da55d72d0",
    "semantic_title": "perceptions to beliefs: exploring precursory inferences for theory of mind in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hOOFKKDzVB": {
    "title": "Regulation vs. Performance: Interpretable Language Models Help Quantify a Trade-Off",
    "volume": "review",
    "abstract": "Regulation is increasingly cited as the most important and pressing concern in machine learning. However, it is currently unknown how to implement this, and perhaps more importantly, how it would effect model performance alongside human collaboration if actually realized. In this paper, we attempt to answer these questions by building a regulatable large-language model (LLM), and then quantifying how the additional constraints involved affect (1) model performance, alongside (2) human collaboration. Our empirical results reveal that it is possible to force an LLM to use human-defined features in an transparent way, but a ``regulation performance trade-off'' previously not considered reveals itself in the form of a 7.34% classification performance drop. Surprisingly however, we show that despite this, such systems actually improve human task performance speed and *appropriate* confidence in a realistic deployment setting compared to no AI assistance, thus paving a way for fair, regulatable AI, which benefits users",
    "checked": false,
    "id": "aea98a47cccede5bdecd1136b7c3c6a31b4c1edf",
    "semantic_title": "human vs chatgpt: effect of data annotation in interpretable crisis-related microblog classification",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=DHV2xqsxgw": {
    "title": "Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities",
    "volume": "review",
    "abstract": "The parametric knowledge memorized by large language models (LLMs) becomes outdated quickly. In-context editing (ICE) is currently the most effective method for updating the knowledge of LLMs. Recent advancements involve enhancing ICE by modifying the decoding strategy, obviating the need for altering internal model structures or adjusting external prompts. However, this enhancement operates across the entire sequence generation, encompassing a plethora of non-critical tokens. In this work, we introduce $\\textbf{A}$daptive $\\textbf{T*}$oken $\\textbf{Bias}$er ($\\textbf{ATBias}$), a new decoding technique designed to enhance ICE. It focuses on the tokens that are mostly related to knowledge during decoding, biasing their logits by matching key entities related to new and parametric knowledge. Experimental results show that ATBias significantly enhances ICE performance, achieving up to a 32.3\\% improvement over state-of-the-art ICE methods while incurring only half the latency. ATBias not only improves the knowledge editing capabilities of ICE but can also be widely applied to LLMs with negligible cost",
    "checked": true,
    "id": "b92166c63eba57a5d3d225c2db43d81accd4911b",
    "semantic_title": "adaptive token biaser: knowledge editing via biasing key entities",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=uVbEkOnocR": {
    "title": "Analyzing Key Factors Influencing Emotion Prediction Performance of VLLMs in Conversational Contexts",
    "volume": "review",
    "abstract": "Emotional intelligence (EI) in artificial intelligence (AI), which refers to the ability of an AI to understand and respond appropriately to human emotions, has emerged as a crucial research topic. Recent studies have shown that large language models (LLMs) and vision large language models (VLLMs) possess EI and the ability to understand emotional stimuli in the form of text and images, respectively. However, factors influencing the emotion prediction performance of VLLMs in real-world conversational contexts have not been sufficiently explored. This study aims to analyze the key elements affecting the emotion prediction performance of VLLMs in conversational contexts systematically. To achieve this, we reconstructed the MELD dataset, which is based on the popular TV series Friends, and conducted experiments through three sub-tasks: overall emotion tone prediction, character emotion prediction, and contextually appropriate emotion expression selection. We evaluated the performance differences based on various model architectures (e.g., image encoders, modality alignment, and LLMs) and image scopes (e.g., entire scene, person, and facial expression). In addition, we investigated the impact of providing persona information on the emotion prediction performance of the models and analyzed how personality traits and speaking styles influenced the emotion prediction process. We conducted an in-depth analysis of the impact of various other factors, such as gender and regional biases, on the emotion prediction performance of VLLMs. The results revealed that these factors significantly influenced the model performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eekAOta5Vx": {
    "title": "SignCLIP: Connecting Text and Sign Language by Contrastive Learning",
    "volume": "review",
    "abstract": "We present SignCLIP, which re-purposes CLIP (Contrastive Language-Image Pretraining) to project spoken language text and sign language videos, two classes of natural languages of distinct modalities, into the same space. SignCLIP is an efficient method of learning useful visual representations for sign language processing from large-scale, multilingual video-text pairs, without directly optimizing for a specific task or sign language which is often of limited size. We pretrain SignCLIP on Spreadthesign, a prominent sign language dictionary consisting of ~500 thousand video clips in up to 44 sign languages, and evaluate it with various downstream datasets. SignCLIP discerns in-domain signing with notable text-to-video/video-to-text retrieval accuracy. It also performs competitively for out-of-domain downstream tasks such as isolated sign language recognition upon essential few-shot prompting or fine-tuning. We analyze the latent space formed by the spoken language text and sign language poses, which provides additional linguistic insights. Our code and models are openly available",
    "checked": true,
    "id": "75a7a3ab20a620f612db3337fcf6df03b304242d",
    "semantic_title": "signclip: connecting text and sign language by contrastive learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lnsHDSuppb": {
    "title": "Underneath the Numbers: Quantitative and Qualitative Gender Fairness in LLMs for Depression Prediction",
    "volume": "review",
    "abstract": "Recent studies show bias in many machine learning models for depression detection, but bias in LLMs for this task remains unexplored. This work presents the first attempt to investigate the degree of gender bias present in existing LLMs (ChatGPT, LLaMA 2, and Bard) using both quantitative and qualitative approaches. rom our quantitative evaluation, we found that ChatGPT performs the best across various performance metrics and LLaMA 2 outperforms other LLMs in terms of group fairness metrics. As qualitative fairness evaluation remains an open research question we propose several strategies (e.g., word count, thematic analysis) to investigate whether and how a qualitative evaluation can provide valuable insights for bias analysis beyond what is possible with quantitative evaluation. We found that ChatGPT consistently provides a more comprehensive, well-reasoned explanation for its prediction compared to LLaMA 2. We have also identified several themes adopted by LLMs to qualitatively evaluate gender fairness. We hope our results can be used as a stepping stone towards future attempts at improving qualitative evaluation of fairness for LLMs especially for high-stakes tasks such as depression detection",
    "checked": true,
    "id": "61c108bbbaa2903f6b60e79ae7afd733a060fab3",
    "semantic_title": "underneath the numbers: quantitative and qualitative gender fairness in llms for depression prediction",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=8myqhHFvo7": {
    "title": "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity",
    "volume": "review",
    "abstract": "Semantic Textual Similarity (STS) constitutes a critical research direction in computational linguistics and serves as a key indicator of the encoding capabilities of embedding models. Driven by advances in pre-trained language models and contrastive learning techniques, leading sentence representation methods can already achieved average Spearman's correlation scores of approximately 86 across seven STS benchmarks in SentEval. However, further improvements have become increasingly marginal, with no existing method attaining an average score higher than 87 on these tasks. This paper conducts an in-depth analysis of this phenomenon and concludes that the upper limit for Spearman's correlation scores using contrastive learning is 87.5. To transcend this ceiling, we propose an innovative approach termed Pcc-tuning, which employs Pearson's correlation coefficient as a loss function to refine model performance beyond contrastive learning. Experimental results demonstrate that Pcc-tuning markedly surpasses previous state-of-the-art strategies, raising the Spearman's correlation score to above 90",
    "checked": true,
    "id": "31249eaaf157270af7c99c684cf91d754883d2b0",
    "semantic_title": "pcc-tuning: breaking the contrastive learning ceiling in semantic textual similarity",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gp1UcGxe2E": {
    "title": "SELF: Self-Evolution with Language Feedback",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown impressive adaptability in various fields, yet the optimal pathway of autonomous model evolution remains under-explored. Drawing inspiration from the self-driven learning process of humans, we introduce \\textit{SELF} (Self-Evolution with Language Feedback), a novel learning framework that empowers LLMs to continually self-improve their abilities. SELF initiates with a meta-skill learning process that equips the LLMs with capabilities for self-feedback and self-refinement. SELF employs language-based feedback for detailed and nuanced evaluations, pinpointing response flaws and suggesting refinements. Subsequently, the model engages in an iterative process of self-evolution: they autonomously generate responses to unlabeled instructions, refine these responses interactively, and use the refined and filtered data for iterative self-training, thereby progressively boosting their capabilities. Moreover, the SELF framework equips the model with the ability to self-refine during inference, leading to further improved response quality. Our experiments on mathematical and general tasks demonstrate that SELF enables the model to continually self-improve without human intervention. The SELF framework indicates a promising direction for the autonomous evolution of LLMs, transitioning them from passive information receivers to active participants in their development",
    "checked": true,
    "id": "a173b47ee90fb18948d70126f4d40b5561b7d932",
    "semantic_title": "self: self-evolution with language feedback",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=hzyRchjVAW": {
    "title": "Leveraging LLMs for Formal Grammar Generation in Programming Contest Testing",
    "volume": "review",
    "abstract": "Test cases are crucial for ensuring the program's correctness and evaluating performance in programming. The high diversity of test cases within constraints is necessary to distinguish between correct and incorrect answers. Automated source code generation is currently a popular area due to the inefficiency of manually generating test cases. Recent attempts involve generating conditional cases from problem descriptions using deep-learning models that learn from source code. However, this task requires a combination of complex skills such as extracting syntactic and logical constraints for a given test case from a problem, and generating test cases that satisfy the constraints. In this work, we introduce a modified context-free grammar that explicitly represents the syntactical and logical constraints embedded within programming problems. Our innovative framework for automated test case generation separates restriction extraction from test case generation, simplifying the task for the model. Our experimental results show that, compared to current methods, our framework produces test cases that are more precise and effective. All the codes in this paper are available in https://anonymous.4open.science/r/neural_translation_for_test_case_generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GwdQcEvDvJ": {
    "title": "Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?",
    "volume": "review",
    "abstract": "Analogical reasoning is a unique ability of humans to address unfamiliar challenges by transferring strategies from relevant past experiences. One key finding in psychology is that compared with irrelevant past experiences, recalling relevant ones can help humans better handle new tasks. Coincidentally, the NLP community has also recently found that self-generating relevant examples in the context can help large language models (LLMs) better solve a given problem than hand-crafted prompts. However, it is yet not clear whether relevance is the key factor eliciting such capability, i.e., can LLMs benefit more from self-generated relevant examples than irrelevant ones? In this work, we systematically explore whether LLMs can truly perform analogical reasoning on a diverse set of reasoning tasks. With extensive experiments and analysis, we show that self-generated random examples can surprisingly achieve comparable or even better performance, e.g., 4% performance boost on GSM8K with random biological examples. We find that the accuracy of self-generated examples is the key factor and subsequently design two improved methods with significantly reduced inference costs. Overall, we aim to advance a deeper understanding of LLM analogical reasoning and hope this work stimulates further research in the design of self-generated contexts",
    "checked": true,
    "id": "b2ee9afe5142bd47163c4261a544e77a622023e0",
    "semantic_title": "relevant or random: can llms truly perform analogical reasoning?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CusvHZy1Rl": {
    "title": "Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach",
    "volume": "review",
    "abstract": "Direct speech translation (ST) models often struggle with rare words. Incorrect translation of these words can have severe consequences, impacting translation quality and user trust. While rare word translation is inherently challenging for neural models due to sparse learning signals, real-world scenarios often allow access to translations of past recordings on similar topics. To leverage these valuable resources, we propose a retrieval-and-demonstration approach to enhance rare word translation accuracy in direct ST models. First, we adapt existing ST models to incorporate retrieved examples for rare word translation, which allows the model to benefit from prepended examples, similar to in-context learning. We then develop a cross-modal (speech-to-speech, speech-to-text, text-to-text) retriever to locate suitable examples. We demonstrate that standard ST models can be effectively adapted to leverage examples for rare word translation, improving rare word translation accuracy over the baseline by 17.6% with gold examples and 8.5% with retrieved examples. Moreover, our speech-to-speech retrieval approach outperforms other modalities and exhibits higher robustness to unseen speakers. Our code is in the submission",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bx2QhrerpE": {
    "title": "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration",
    "volume": "review",
    "abstract": "Rotary Position Embedding (RoPE) is an efficient position encoding approach and is widely utilized in numerous large language models (LLMs). Recently, a lot of methods have been put forward to further expand the context window based on RoPE. The core concept of those methods is to predefine or search for a set of factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a challenge for existing methods to predefine an optimal factor due to the exponential search space. In view of this, we introduce PSC (Phase Shift Calibration), a small module for calibrating the frequencies predefined by existing methods. With the employment of PSC, we demonstrate that many existing methods can be further enhanced, like PI, YaRN, and LongRoPE. We carry out extensive experiments on many models in various tasks and the results verify the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mOPCBf4ncP": {
    "title": "Quantifying Multilingual Performance of Large Language Models Across Languages",
    "volume": "review",
    "abstract": "The development of Large Language Models (LLMs) relies on extensive text corpora, which are often unevenly distributed across languages. This imbalance results in LLMs performing significantly better on high-resource languages like English, German, and French, while their capabilities in low-resource languages remain inadequate. Currently, there is a lack of quantitative methods to evaluate the performance of LLMs in these low-resource languages. To address this gap, we propose the Language Ranker, an intrinsic metric designed to benchmark and rank languages based on LLM performance using internal representations. By comparing the LLM's internal representation of various languages against a baseline derived from English, we can assess the model's multilingual capabilities in a robust and language-agnostic manner. Our analysis reveals that high-resource languages exhibit higher similarity scores with English, demonstrating superior performance, while low-resource languages show lower similarity scores, underscoring the effectiveness of our metric in assessing language-specific capabilities. Besides, the experiments show that there is a strong correlation between the LLM's performance in different languages and the proportion of those languages in its pre-training corpus. These insights underscore the efficacy of the Language Ranker as a tool for evaluating LLM performance across different languages, particularly those with limited resources",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fzDxIosyWu": {
    "title": "Uni-ETOD: User-Need-Driven Chain of Thought Framework for Fully End-to-end Task-oriented Dialogue System",
    "volume": "review",
    "abstract": "Fully End-to-End Task-Oriented Dialogue Systems (Fully ETOD) retrieve knowledge from a knowledge base in a differentiable manner and generate responses using a language model generator without the need for modular training. However, Fully ETOD faces some challenges. During the retrieval process, the retriever retrieves the knowledge base in a black-box manner, making it difficult for the generator to differentiate the large amount of knowledge obtained by the retriever. This leads to a degradation in the quality of the responses and the trustworthiness of the system. Moreover, as the size of the knowledge base grows, it may exacerbate the risk of this problem. To address this challenge, we first design a dataset for Fully ETOD based on large-scale knowledge bases called FakeRest to solve the scarcity of annotated dialogue data based on large-scale knowledge bases. We also propose a User-need-driven Chain of Thought Framework (Uni-ETOD) for Fully ETOD, which aims to guide LLMs to gradually understand users' thought processes and improve the quality of responses in Fully ETOD. We use ChatGPT, Gemini, Llama3, Mistral, and ChatGLM as the backbone models of the system. On FakeRest, we comprehensively evaluate the capability of each step of Uni-ETOD. The results show that Uni-ETOD will help LLMs better distinguish the retrieved knowledge and enhance the credibility and interpretability of the whole system",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o3SAmsK48q": {
    "title": "Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?",
    "volume": "review",
    "abstract": "Large vision-language models (LVLMs) have recently dramatically pushed the state of the art in image captioning and many image understanding tasks (e.g., visual question answering). LVLMs, however, often \\textit{hallucinate} and produce captions that mention concepts that cannot be found in the image. These hallucinations erode the trustworthiness of LVLMs and are arguably among the main obstacles to their ubiquitous adoption. Recent work suggests that addition of grounding objectives---those that explicitly align image regions or objects to text spans---reduces the amount of LVLM hallucination. Although intuitive, this claim is not empirically justified as the reduction effects have been established, we argue, with flawed evaluation protocols that (i) rely on data (i.e., MSCOCO) that has been extensively used in LVLM training and (ii) measure hallucination via question answering rather than open-ended caption generation. In this work, in contrast, we offer the first systematic analysis of the effect of fine-grained object grounding on LVLM hallucination under an evaluation protocol that more realistically captures LVLM hallucination in open generation. Our extensive experiments over three backbone LLMs reveal that grounding objectives have little to no effect on object hallucination in open caption generation",
    "checked": true,
    "id": "fa5a8e7cbbbb8ee47610733c363bb96bf31e049b",
    "semantic_title": "does object grounding really reduce hallucination of large vision-language models?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tiJcC6JEwQ": {
    "title": "Breaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment",
    "volume": "review",
    "abstract": "Multilingual pre-trained models (mPLMs) have shown impressive performance on cross-lingual transfer tasks. However, the transfer performance is often hindered when a low-resource target language is written in a different script than the high-resource source language, even though the two languages may be related or share parts of their vocabularies. Inspired by recent work that uses transliteration to address this problem, our paper proposes a transliteration-based post-pretraining alignment (PPA) method aiming to improve the cross-lingual alignment between languages using diverse scripts. We select two areal language groups, $\\textbf{Mediterranean-Amharic-Farsi}$ and $\\textbf{South+East Asian Languages}$, wherein the languages are mutually influenced but use different scripts. We apply our method to these language groups and conduct extensive experiments on a spectrum of downstream tasks. The results show that after PPA, models consistently outperform the original model (up to 50\\% for some tasks) in English-centric transfer. In addition, when we use languages other than English as sources in transfer, our method obtains even larger improvements. We will make our code and models publicly available",
    "checked": true,
    "id": "5d1c481fb7538f227352e72c59951e0fc26d2ac5",
    "semantic_title": "breaking the script barrier in multilingual pre-trained language models with transliteration-based post-training alignment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fiQucCdsUe": {
    "title": "ICON: Improving Inter-Report Consistency in Radiology Report Generation via Lesion-aware Mixup Augmentation",
    "volume": "review",
    "abstract": "Previous research on radiology report generation has made significant progress in terms of increasing the clinical accuracy of generated reports. In this paper, we emphasize another crucial quality that it should possess, i.e., inter-report consistency, which refers to the capability of generating consistent reports for semantically equivalent radiographs. This quality is even of greater significance than the overall report accuracy in terms of ensuring the system's credibility, as a system prone to providing conflicting results would severely erode users' trust. Regrettably, existing approaches struggle to maintain inter-report consistency, exhibiting biases towards common patterns and susceptibility to lesion variants. To address this issue, we propose ICon, which improves the inter-report consistency of radiology report generation. Aiming at enhancing the system's ability to capture the similarities in semantically equivalent lesions, our approach involves first extracting lesions from input images and examining their characteristics. Then, we introduce a lesion-aware mixup technique to ensure that the representations of the semantically equivalent lesions align with the same attributes, by linearly interpolating them during the training phase. Extensive experiments on three publicly available chest X-ray datasets verify the effectiveness of our approach, both in terms of improving the consistency and accuracy of the generated reports",
    "checked": false,
    "id": "21093ab7c4590e8eac55cd260541528acfbf6559",
    "semantic_title": "icon: improving inter-report consistency of radiology report generation via lesion-aware mix-up augmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ciPpzGFrfW": {
    "title": "MedThink: Inducing Medical Large-scale Visual Languange Models to Hallucinate Less by Thinking More",
    "volume": "review",
    "abstract": "When Large Vision Language Models (LVLMs) are applied to multimodal medical generative tasks, they suffer from significant model hallucination issues. This severely impairs the model's generative accuracy, making it challenging for LVLMs to be implemented in real-world medical scenarios to assist doctors in diagnosis. Enhancing the training data for downstream medical generative tasks is an effective way to address model hallucination. Moreover, the limited availability of training data in the medical field and privacy concerns greatly hinder the model's accuracy and generalization capabilities. In this paper, we introduce a method that mimics human cognitive processes to construct fine-grained instruction pairs and apply the concept of chain-of-thought (CoT) from inference scenarios to training scenarios, thereby proposing a method called MedThink. Our experiments on various LVLMs demonstrate that our novel data construction method tailored for the medical domain significantly improves the model's performance in medical image report generation tasks and substantially mitigates the hallucinations. All resources of this work will be released soon",
    "checked": false,
    "id": "82b8e64c49e99dc605062172cd4ae2a0135afb21",
    "semantic_title": "medthink: inducing medical large-scale visual language models to hallucinate less by thinking more",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=eH4LZbCNGP": {
    "title": "Explaining Mixtures of Sources in News Articles",
    "volume": "review",
    "abstract": "Human writers plan, _then_ write. For large language models (LLMs) to play a role in longer-form article generation, we must understand the planning steps humans make before writing. We explore one kind of planning, source-selection in news, as a case-study for evaluating plans in long-form generation. We ask: why do specific stories call for specific kinds of sources? We imagine a process where sources are selected to fall into different categories. Learning the article's plan means predicting the categorization scheme chosen by the journalist. Inspired by latent-variable modeling, we first develop metrics to select the most likely plan underlying a story. Then, working with professional journalists, we adapt five existing approaches to planning and introduce three new ones. We find that two approaches, or schemas: stance and social affiliation best explain source plans in most documents. However, other schemas like textual entailment explain source plans in factually rich topics like ``Science''. Finally, we find we can predict the most suitable schema given just the article's headline with reasonable accuracy. We see this as an important case-study for human planning, and provides a framework and approach for evaluating other kinds of plans, like discourse or plot-oriented plans. We release a corpora, NewsSources, with schema annotations for 4M articles, for further study",
    "checked": false,
    "id": "33813e3ae6b8e6a7e8c78155e23230d8460013f2",
    "semantic_title": "nonlinear intraday trading invariance in the russian stock market",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ooolIMhWik": {
    "title": "A Two-dimensional Zero-shot Dialogue State Tracking Evaluation Method using GPT-4",
    "volume": "review",
    "abstract": "Dialogue state tracking (DST) is evaluated by exact matching methods, which rely on large amounts of labeled data and ignore semantic consistency, leading to over-evaluation. Currently, leveraging large language models (LLM) in evaluating natural language processing tasks has achieved promising results. However, using LLM for DST evaluation is still under explored. In this paper, we propose a two-dimensional zero-shot evaluation method for DST using GPT-4, which divides the evaluation into two dimensions: accuracy and completeness. Furthermore, we also design two manual reasoning paths in prompting to further improve the accuracy of evaluation. Experimental results show that our method achieves better performance compared to the baselines, and is consistent with traditional exact matching based methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yoVy1YCU7w": {
    "title": "Retrieval-Augmented Generation Inspired Long Document Classification",
    "volume": "review",
    "abstract": "While being proven to be effective across nearly all natural language processing tasks, transformer-based models have several obvious limitations. Amongst them, arguably the most significant one is the quadratic complexity -- both in time and space -- of the vanilla self-attention mechanism. As a result, most existing pre-trained language models, such as BERT, have a fixed maximum context window. This potentially creates a mismatch between the context window size and the data applied to fine-tuning it. This gives rise to the study of long document classification -- the task to optimize performance when the length of the input document exceeds the model's maximum token. Inspired by retrieval-augmented generation techniques used by large language models in recent years, we propose a method that uses similar techniques to retrieve the most relevant sections of a long document, which is then fed into a traditional transformer-based model. By testing on four standard long document classification datasets, we show that our proposed method on average outperforms all baselines, including both transformer and non-transformer based models",
    "checked": false,
    "id": "fb45f67706e7d11eacdc41d28e7651539d773fe1",
    "semantic_title": "retrieval-augmented multi-label text classification",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=pyUoHWYDUR": {
    "title": "CAST: Sparse Fine-Tuning with Counterfactual Data Augmentation",
    "volume": "review",
    "abstract": "In the domain of transfer learning for pre-trained models, fine-tuning specific parameters rather than the entire model has become a prevalent trend. Sparse fine-tuning has proven effective. Counterfactual Data Augmentation have been shown to enhance the generalization ability of models. This study proposes a fine-tuning method that combines the advantages of both approaches, which is called \"Counterfactual Augmented Sparse Tuning\" (CAST). Inspired by the Lottery Ticket Hypothesis, this method identifies significant parameter changes by comparing models trained on counterfactual data with those trained on original data, thereby constructing a mask table for model parameters. To further enhance model sparsity, we introduce a counterfactual data impact factor, which adjusts the specific influence of counterfactual data on the model training outcomes. The CAST method achieved the best accuracy rates of 90.2\\% and 76\\% in counterfactual data augmentation tasks for sentiment analysis and natural language inference tasks. It was observed that CAST successfully resisted catastrophic shifts in dataset distribution. The CAST model not only improves performance in specific NLP tasks but also reduces the risk of data distribution shift and enhances the model's ability to capture key features",
    "checked": false,
    "id": "f45bee9da1655320b7fc290d2abc20903bd12545",
    "semantic_title": "leveraging domain knowledge for inclusive and bias-aware humanitarian response entry classification",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=wfO5v4Lw4V": {
    "title": "MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL",
    "volume": "review",
    "abstract": "Recent LLM-based Text-to-SQL methods usually suffer from significant performance degradation on ``huge\" databases and complex user questions that require multi-step reasoning. Moreover, most existing methods neglect the crucial significance of LLMs utilizing external tools and model collaboration. To address these challenges, we introduce \\textsc{MAC-SQL}, a novel LLM-based multi-agent collaborative framework. Our framework comprises a core decomposer agent for Text-to-SQL generation with few-shot chain-of-thought reasoning, accompanied by two auxiliary agents that utilize external tools or models to acquire smaller sub-databases and refine erroneous SQL queries. The decomposer agent collaborates with auxiliary agents, which are activated as needed and can be expanded to accommodate new features or tools for effective Text-to-SQL parsing. In our framework, We initially leverage GPT-4 as the strong backbone LLM for all agent tasks to determine the upper bound of our framework. We then fine-tune an open-sourced instruction-followed model, SQL-Llama, by leveraging Code Llama 7B, to accomplish all tasks as GPT-4 does. Experiments show that SQL-Llama achieves a comparable execution accuracy of 43.94, compared to the baseline accuracy of 46.35 for vanilla GPT-4. At the time of writing, MAC-SQL+GPT-4 achieves an execution accuracy of 59.59 when evaluated on the BIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test set",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gz7KSEIg5g": {
    "title": "Language Versatilists vs. Specialists: An Empirical Revisiting on Multilingual Transfer Ability",
    "volume": "review",
    "abstract": "Multilingual transfer ability, which reflects how well the models fine-tuned on one source language can be applied to other languages, has been well studied in multilingual pre-trained models (e.g., BLOOM). However, such ability has not been investigated for English-centric models (e.g., LLaMA). To fill this gap, we study the following research questions. First, does multilingual transfer ability exist in English-centric models and how does it compare with multilingual pretrained models? Second, does it only appears when English is the source language for the English-centric model? Third, how does it vary in different tasks? We take multilingual reasoning ability as our focus and conduct extensive experiments across four types of reasoning tasks. We find that the multilingual pretrained model does not always outperform an English-centric model. Furthermore, English appears to be a less suitable source language, and the choice of source language becomes less important when the English-centric model scales up. In addition, different types of tasks exhibit different multilingual transfer abilities. These findings demonstrate that English-centric models not only possess multilingual transfer ability but may even surpass the transferability of multilingual pretrained models if well-trained. By showing the strength and weaknesses, the experiments also provide valuable insights into enhancing multilingual reasoning abilities for the English-centric models",
    "checked": true,
    "id": "a8b5a20e3a983d96f9dea6fc38e77b155e7bd94f",
    "semantic_title": "language versatilists vs. specialists: an empirical revisiting on multilingual transfer ability",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=9EhsUgBH9T": {
    "title": "Harnessing Dimension-level Contrastive Learning and Information Compensation Mechanism for Sentence Embedding Enhancement",
    "volume": "review",
    "abstract": "Although unsupervised sentence embedding learning has achieved great success through the construction of positive samples and instance-level contrastive learning (ICL), the learned sentence embeddings can be over-compressed or suffer from dimensional pollution due to noisy data augmentation and unconstrained ICL learning processes. To address the above issues, we design a novel sentence embedding enhancement method, namely MSSE, where an information compensation mechanism (ICM) and a dimensional-level contrastive learning mechanism (DCM) are proposed. ICM is motivated by the information bottleneck principle and can prevent excessive compression of representation learning. DCM constrains the learning process of ICL and reduces information contamination across different dimensions. Experimental results demonstrate that our method outperforms the current competitive baselines for 7 STS tasks across unsupervised, few-shot, and supervised learning of sentence embeddings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rNWRnVGfBC": {
    "title": "Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters",
    "volume": "review",
    "abstract": "Large language models (LLMs) have revolutionized natural language processing and broadened their applicability across diverse commercial applications. However, the deployment of these models is constrained by high inference time in multilingual settings. To mitigate this challenge, this paper explores a training recipe of an assistant model in speculative decoding, which are leveraged to draft and-then its future tokens are verified by the target LLM. We show that language-specific draft models, optimized through a targeted pretrain-and-finetune strategy, substantially brings a speedup of inference time compared to the previous methods. We validate these models across various languages in inference time, out-of-domain speedup, and GPT-4o evaluation",
    "checked": true,
    "id": "43fff8982ae0db83ebc001d9f36ff2787cb1d8a5",
    "semantic_title": "towards fast multilingual llm inference: speculative decoding and specialized drafters",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g0ywZtoLXu": {
    "title": "Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement",
    "volume": "review",
    "abstract": "Large language model agents have exhibited exceptional performance across a range of complex interactive tasks. Recent approaches have utilized tuning with expert trajectories to enhance agent performance, yet they primarily concentrate on outcome rewards, which may lead to errors or suboptimal actions due to the absence of process supervision signals. In this paper, we introduce the **I**terative step-level **P**rocess **R**efinement **(IPR)** framework, which provides detailed step-by-step guidance to enhance agent training. Specifically, we adopt the Monte Carlo method to estimate step-level rewards. During each iteration, the agent explores along the expert trajectory and generates new actions. These actions are then evaluated against the corresponding step of expert trajectory using step-level rewards. Such comparison helps identify discrepancies, yielding contrastive action pairs that serve as training data for the agent. Our experiments on three complex agent tasks demonstrate that our framework outperforms a variety of strong baselines. Moreover, our analytical finds highlight the effectiveness of IPR in augmenting action efficiency and its applicability to diverse models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xw65urw8WG": {
    "title": "BlockPruner: Fine-grained Pruning for Large Language Models",
    "volume": "review",
    "abstract": "With the rapid growth in the size and complexity of large language models (LLMs), the costs associated with their training and inference have escalated significantly. Research indicates that certain layers in LLMs harbor substantial redundancy, and pruning these layers has minimal impact on the overall performance. While various layer pruning methods have been developed based on this insight, they generally overlook the finer-grained redundancies within the layers themselves. In this paper, we delve deeper into the architecture of LLMs and demonstrate that finer-grained pruning can be achieved by targeting redundancies in multi-head attention (MHA) and multi-layer perceptron (MLP) blocks. We propose a novel, training-free structured pruning approach called BlockPruner. Unlike existing layer pruning methods, BlockPruner segments each Transformer layer into MHA and MLP blocks. It then assesses the importance of these blocks using perplexity measures and applies a heuristic search for iterative pruning. We applied BlockPruner to LLMs of various sizes and architectures and validated its performance across a wide range of downstream tasks. Experimental results show that BlockPruner achieves more granular and effective pruning compared to state-of-the-art baselines",
    "checked": true,
    "id": "adab2c83bf596e53e24049a8294bd8ca6897ee75",
    "semantic_title": "blockpruner: fine-grained pruning for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=GXCX9Wshwv": {
    "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) are known to be trained on vast amounts of data, which may unintentionally or intentionally include data from commonly used benchmarks. This inclusion can lead to cheatingly high scores on model leaderboards, yet result in disappointing performance in real-world applications. To address this benchmark contamination problem, we first propose a set of requirements that practical contamination detection methods should follow. Following these proposed requirements, we introduce PaCoST, a Paired Confidence Significance Testing to effectively detect benchmark contamination in LLMs. Our method constructs a counterpart for each piece of data with the same distribution, and performs statistical analysis of the corresponding confidence to test whether the model is significantly more confident under the original benchmark. We validate the effectiveness of PaCoST and apply it on popular open-source models and benchmarks. We find that almost all models and benchmarks we tested are suspected contaminated more or less. We finally call for new LLM evaluation methods",
    "checked": true,
    "id": "1e6edf2622ad0910f0e5aeb248f3c3ac88baa415",
    "semantic_title": "pacost: paired confidence significance testing for benchmark contamination detection in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gpjAnHSlRf": {
    "title": "Automated Compliance Checking for Chinese Privacy Policy: A New Task and Dataset",
    "volume": "review",
    "abstract": "Privacy policy texts inform users about how their personal data is handled by online service providers. However, they may be long, complex, and non-compliant with laws and regulations. Therefore, automated compliance checking of privacy policy texts is needed. In this paper, we introduce the first dataset and task for automated compliance checking of Chinese privacy policy texts. Our dataset provides human experts' compliance annotation at both the document level and the fine-grained level. The fine-grained annotation includes both the existing named entity recognition (NER) task and 11 new sentence classification (SC) tasks for compliance checking. We treat the NER and classification subtasks as discriminative legal attributes that can help models to generate reliable compliance results and easy-to-understand explanations. Additionally, we further pretrain BERT-Chinese on a large corpus of compliance-related texts and evaluate it on all the tasks. Our results show that our further pre-trained BERT model outperforms the baseline models and demonstrates the potential of NLP techniques for automated compliance checking of privacy policies. Our dataset and the further pre-trained BERT model will be released soon",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jlMEPzyWIm": {
    "title": "Emosical: An Emotion Annotated Musical Theatre Dataset",
    "volume": "review",
    "abstract": "This paper presents Emosical, a multimodal open-source dataset of musical films. Emosical comprises video, vocal audio, text, and character identity paired samples with annotated emotion tags. Emosical provides rich emotion annotations for each sample by inferring the background story of the characters. To derive the emotion tags, we leverage the musical theater script, which contains the characters' complete background stories and narrative contexts. The annotation pipeline includes feeding the singing character, text, global persona, and context of the dialogue and song track into a large language model (LLM). To verify the effectiveness of our tagging scheme, we perform an ablation study by bypassing each step of the pipeline. A subjective test is conducted to compare the generated tags of each ablation result. We also perform a statistical analysis to find out the global characteristics of the collected emotion tags. Emosical would enable expressive synthesis and tagging of the singing voice in the musical theatre domain in future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PBjQ9tOJhz": {
    "title": "In-Context Learning with Iterative Demonstration Selection",
    "volume": "review",
    "abstract": "Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods",
    "checked": true,
    "id": "b217b6bc340af9a10bebbf8acc36ea30871769bd",
    "semantic_title": "in-context learning with iterative demonstration selection",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=2ebvGUqmiz": {
    "title": "Stable Knowledge Editing in Large Language Models",
    "volume": "review",
    "abstract": "Efficient knowledge editing of large language models is crucial for replacing obsolete information or incorporating specialized knowledge on a large scale. However, previous methods implicitly assume that knowledge is localized and isolated within the model, an assumption that oversimplifies the interconnected nature of model knowledge. The premise of localization results in an incomplete knowledge editing, whereas an isolated assumption may impair both other knowledge and general abilities. These two assumptions result in instability in the performance of knowledge editing methods. To transcend these assumptions, we introduce StableKE, a method that adopts a novel perspective based on knowledge augmentation rather than knowledge localization. Inspired by the diversity and extensiveness of training data in the pretraining phase, StableKE integrates two automated knowledge augmentation strategies in the instruction fine-tuning phase: Semantic Paraphrase Enhancement (SPE), which diversifies knowledge descriptions to facilitate the teaching of new information to the model, and Contextual Description Enrichment (CDE), which expands the surrounding knowledge to prevent the forgetting of related information. StableKE surpasses other knowledge editing methods, demonstrating stability both edited knowledge and multi-hop knowledge, while also preserving unrelated knowledge and general abilities. Moreover, StableKE can edit knowledge parameter on ChatGPT",
    "checked": true,
    "id": "79a6a70a97289362f8504a9841c43c9ad98ea337",
    "semantic_title": "stable knowledge editing in large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=r2wFLsujJQ": {
    "title": "Book2QA: A Framework for Integrating LLMs to Generate High-quality QA Data from Textbooks",
    "volume": "review",
    "abstract": "The scarcity of question-answering data is one of the main bottlenecks restricting the development of intelligent education systems. In this paper, we proposes a new method called Book2QA, which integrates multiple medium-scale language models (e.g., 6B/13B) to cost-effectively generate high-quality question-answering data from textbook content. The Book2QA framework includes three main steps: book data preprocessing, question generation with subsequent filtering, and answer generation with subsequent filtering. Our experimental results demonstrate the fine-tuned model's performance in real scenarios, highlighting the effectiveness of the Book2QA method. Automatic evaluation and advanced LLM evaluation show that data generated by Book2QA can match or surpass data from models with hundreds of billions of parameters. We open-source our data and code at https://anonymous.4open.science/r/Book2QA-F795",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k0JpEnpxT9": {
    "title": "Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation",
    "volume": "review",
    "abstract": "Alignment with human preferences is an important step in developing accurate and safe large language models. This is no exception in machine translation (MT), where better handling of language nuances and context-specific variations leads to improved quality. However, preference data based on human feedback can be very expensive to obtain and curate at a large scale. Automatic metrics, on the other hand, can induce preferences, but they might not match human expectations perfectly. In this paper, we propose an approach that leverages the best of both worlds. We first collect sentence-level quality assessments from professional linguists on translations generated by multiple high-quality MT systems and evaluate the ability of current automatic metrics to recover these preferences. We then use this analysis to curate a new dataset, MT-Pref (metric-induced translation preference) dataset, which comprises 18k instances covering 18 language directions, using texts sourced from multiple domains post-2022. We show that aligning TOWER models on MT-Pref significantly improves translation quality on WMT23 and FLORES benchmarks",
    "checked": false,
    "id": "117053c6c043d46ca0923519b71038d276a45291",
    "semantic_title": "generating usage-related questions for preference elicitation in conversational recommender systems",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=qMv9xh8CqJ": {
    "title": "Unlocking Varied Perspectives: A Persona-Based Multi-Agent Framework with Debate-Driven Text Planning for Argument Generation",
    "volume": "review",
    "abstract": "Writing persuasive arguments is a challenging task for both humans and machines. It entails incorporating high-level beliefs from various perspectives on the topic, along with deliberate reasoning and planning to construct a coherent narrative. Current language models often generate surface tokens autoregressively, lacking explicit integration of these underlying controls, resulting in limited output diversity and coherence. In this work, we propose a persona-based multi-agent framework for argument writing. Inspired by the human debate, we first assign each agent a persona representing its high-level beliefs from a unique perspective, and then design an agent interaction process so that the agents can collaboratively debate and discuss the idea to form an overall plan. Such debate process enables fluid and nonlinear development of ideas. We evaluate our framework on argumentative essay writing. The results show that our proposed framework can generate more diverse and persuasive arguments through both automatic and human evaluations",
    "checked": true,
    "id": "69262e8afd7595ee8d4225aacf36bed07caf4a94",
    "semantic_title": "unlocking varied perspectives: a persona-based multi-agent framework with debate-driven text planning for argument generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FXLzg6Ch4W": {
    "title": "Comparison of Cross-encoder and Bi-encoder Approaches for Arabic question answering task",
    "volume": "review",
    "abstract": "With the recent advancement in Transformer networks and large language models, various encoder-based approaches have been proposed as solutions. When textual data for questions and answers are available, cross-encoder approaches encode them jointly, while bi-encoder approaches encode them separately. In this research, the performance of these approaches for question-answer pairs using an Arabic medical dataset is compared. Five variants of the Transformer model were utilized for this study. These models differ in design but share the objective of leveraging large amounts of text data to build a general language understanding model. Then, fine-tuned on an answer selection task and evaluated for performance using accuracy and execution time metrics. The results indicate that the AraBERT model with a cross-encoder architecture achieved the highest accuracy of 0.96",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2SwOTnpH04": {
    "title": "Mixture-of-Skills: Learning to Optimize Data Usage for Fine-Tuning Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) are typically fine-tuned on diverse and extensive datasets sourced from various origins to develop a comprehensive range of skills, such as writing, reasoning, chatting, coding, and more. Each skill has unique characteristics, and these datasets are often heterogeneous and imbalanced, making the fine-tuning process highly challenging. Balancing the development of each skill while ensuring the model maintains its overall performance requires sophisticated techniques and careful dataset curation. In this work, we propose a general, model-agnostic, reinforcement learning framework, Mixture-of-Skills (MoS), that learns to optimize data usage automatically during the fine-tuning process. This framework ensures the optimal comprehensive skill development of LLMs by dynamically adjusting the focus on different datasets based on their current learning state. To validate the effectiveness of MoS, we conduct extensive experiments using three diverse LLM backbones on two widely used benchmarks and demonstrate that MoS substantially enhances model performance. Building on the success of MoS, we propose MoSpec, an adaptation for task-specific fine-tuning, which harnesses the utilities of various datasets for a specific purpose. Our work underlines the significance of dataset rebalancing and present MoS as a powerful, general solution for optimizing data usage in the fine-tuning of LLMs for various purposes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rDbFH8P82r": {
    "title": "Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models",
    "volume": "review",
    "abstract": "The rise of Multimodal Large Language Models (MLLMs), renowned for their advanced instruction-following and reasoning capabilities, has significantly propelled the field of visual reasoning. However, due to limitations in their image tokenization processes, most MLLMs struggle to capture fine details of text and objects in images, especially in high-resolution samples. To overcome this, we introduce P2G, a novel framework for plug-and-play grounding in MLLMs. P2G utilizes the tool-usage potential of MLLMs to employ expert agents for on-the-fly grounding of critical visual and textual elements in images, thereby enabling deliberate reasoning through multimodal prompting. Additionally, we develop P2GB, a benchmark designed to evaluate MLLMs' proficiency in understanding inter-object relationships and textual content in challenging high-resolution images. Extensive experiments on visual reasoning tasks demonstrate the superiority of P2G, achieving performance comparable to GPT-4V on P2GB with a 7B backbone. Our work underscores the potential of plug-and-play grounding in reasoning, presenting a promising alternative to mere model scaling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ORSrbuU16k": {
    "title": "Emotionally Aligned Responses through Translation",
    "volume": "review",
    "abstract": "Emotional response generation is an area of particular interest within conversational AI. However, many approaches lack control over the response. Potentially, due in part to the widely adopted approach of reflecting the users emotion in the response. As such this paper proposes an independent, but adaptable, emotion for the conversational agent that is separate from the user's, using Valence, Arousal, and Dominance scores which are updated based on user input. Additionally, by treating the alignment of the response as a matter of translation, a set of fine tuned sequence to sequence models are used to translate an initially generated response into one aligned with the agent emotion. This work provides a unique perspective on the topic of emotional response generation and showcases that potential means for improved consistency and controllability may yet be discovered beyond traditional methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bMU3OhBpci": {
    "title": "TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control",
    "volume": "review",
    "abstract": "Zero-shot singing voice synthesis (SVS) with style transfer and style control aims to generate high-quality singing voices with unseen timbres and styles (including singing method, emotion, rhythm, technique, and pronunciation) from audio and text prompts. However, the multifaceted nature of singing styles poses a significant challenge for effective modeling, transfer, and control. Furthermore, current SVS models often fail to generate singing voices rich in stylistic nuances for unseen singers. To address these challenges, we introduce TCSinger, the first zero-shot SVS model for style transfer across cross-lingual speech and singing styles, along with multi-level style control. Specifically, TCSinger proposes three primary modules: 1) the clustering style encoder employs a clustering vector quantization model to stably condense style information into a compact latent space; 2) the Style and Duration Language Model (S&D-LM) concurrently predicts style information and phoneme duration, which benefits both; 3) the style adaptive decoder uses a novel mel-style adaptive normalization method to generate singing voices with enhanced details. Experimental results show that TCSinger outperforms baseline models in synthesis quality, singer similarity, and style controllability across various tasks, including zero-shot style transfer, multi-level style control, cross-lingual style transfer, and speech-to-singing style transfer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bgbXQRZpKG": {
    "title": "Interleaved Vision-and-Language Generation via Generative Vokens",
    "volume": "review",
    "abstract": "The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, \\modelname, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows \\modelname is better than the baseline model on more than 57\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks",
    "checked": false,
    "id": "e7d09b6f2bc878cf2c993acf675f409d0b55f35a",
    "semantic_title": "minigpt-5: interleaved vision-and-language generation via generative vokens",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=nMiRVN1KhG": {
    "title": "Dreaming with ChatGPT: Unraveling the Challenges of LLMs Dream Generation",
    "volume": "review",
    "abstract": "Large Language Models (LLMs), such as ChatGPT, are used daily for different human-like text generation tasks. This motivates us to ask: Can an LLM generate human dreams? For this research, we explore this new avenue through the lens of ChatGPT, and its ability to generate valid dreams. We have three main findings: (i) Chatgpt-4o, the new version of chatGPT, generated all requested dreams. (ii) Generated dreams meet key psychological criteria of dreams. (iii) Generated dreams embed biases towards different groups. We hope our work will set the stage for developing a new task of dream generation for LLMs. This task can help psychologists evaluate patients' dreams based on their demographic factors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EwXpapJLO5": {
    "title": "Adapting Large Language Models for Document-Level Machine Translation",
    "volume": "review",
    "abstract": "Large language models (LLMs) have significantly advanced various natural language processing (NLP) tasks. Recent research indicates that moderately-sized LLMs often outperform larger ones after task-specific fine-tuning. This study focuses on adapting LLMs for document-level machine translation (DocMT) for specific language pairs. We first investigate the impact of prompt strategies on translation performance and then conduct extensive experiments using two fine-tuning methods, three LLM backbones, and 18 translation tasks across nine language pairs. Our results show that specialized models can sometimes surpass GPT-4 in translation performance but still face issues like off-target translation due to error propagation in decoding. We provide an in-depth analysis of these LLMs tailored for DocMT, examining translation errors, discourse phenomena, training strategies, the scaling law of parallel documents, recent test set evaluations, and zero-shot crosslingual transfer. Our findings highlight the strengths and limitations of LLM-based DocMT models and provide a foundation for future research",
    "checked": true,
    "id": "f7c89f1f83595257d6e2bc306d4deee4cf77f573",
    "semantic_title": "adapting large language models for document-level machine translation",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=c3VFBB2zga": {
    "title": "NeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries",
    "volume": "review",
    "abstract": "Extracting and aggregating information from clinical trial registries could provide invaluable insights into the drug development landscape and advance the treatment of neurologic diseases. However, achieving this at scale is hampered by the volume of available data and the lack of an annotated corpus to assist in the development of automation tools. Thus, we introduce NeuroTrialNER, a new and fully open corpus for named entity recognition (NER). It comprises 1093 clinical trial summaries sourced from ClinicalTrials.gov, annotated for neurological diseases, therapeutic interventions, and control treatments. We describe our data collection process and the corpus in detail. We demonstrate its utility for NER using large language models and achieve a close-to-human performance. By bridging the gap in data resources, we hope to foster the development of text processing tools that help researchers navigate clinical trials data more easily",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=asQAaWqNjs": {
    "title": "Virtual Personas for Language Models via an Anthology of Backstories",
    "volume": "review",
    "abstract": "Large language models (LLMs) are trained from vast repositories of text authored by millions of distinct authors, reflecting an enormous diversity of human traits. While these models bear the potential to be used as approximations of human subjects in behavioral studies, prior efforts have been limited in steering model responses to match individual human users. In this work, we introduce Anthology, a method for conditioning LLMs to particular virtual personas by harnessing open-ended life narratives, which we refer to as backstories. We show that our methodology enhances the consistency and reliability of experimental outcomes while ensuring better representation of diverse sub-populations. Across three nationally representative human surveys conducted as part of Pew Research Center's American Trends Panel (ATP), we demonstrate that Anthology achieves up to 18% improvement in matching the response distributions of human respondents and 27% improvement in consistency metrics",
    "checked": true,
    "id": "685b50b738e4597cfc9eb99b30691b0b40a1034b",
    "semantic_title": "virtual personas for language models via an anthology of backstories",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WuH6XWNbeJ": {
    "title": "A good pun is its own reword\": Can Large Language Models Understand Puns?",
    "volume": "review",
    "abstract": "Puns play a vital role in academic research due to their distinct structure and clear definition, which aid in the comprehensive analysis of linguistic humor. However, the understanding of puns in large language models (LLMs) has not been thoroughly examined, limiting their use in creative writing and humor creation. In this paper, we leverage three popular tasks, i.e., pun recognition, explanation and generation to systematically evaluate the capabilities of LLMs in pun understanding. In addition to adopting the automated evaluation metrics from prior research, we introduce new evaluation methods and metrics that are better suited to the in-context learning paradigm of LLMs. These new metrics offer a more rigorous assessment of an LLM's ability to understand puns and align more closely with human cognition than previous metrics. Our findings reveal the \"lazy pun generation\" pattern and identify the primary challenges LLMs encounter in understanding puns",
    "checked": true,
    "id": "4055e37ec2f1dd6c883001d12181a7131010882d",
    "semantic_title": "a good pun is its own reword\": can large language models understand puns?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=FWoXTaEtIj": {
    "title": "Self-evolving Agents with reflective and memory-augmented abilities",
    "volume": "review",
    "abstract": "Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making, lack of long-term memory, and limited context windows in dynamic environments. To address these issues, this paper proposes an innovative frameworkâ€”Self-evolving Agents with Reflective and Memory-augmented Abilities (SAGE). The SAGE framework comprises three agents: the User, the Assistant, and the Checker. By integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents' capabilities in handling multi-tasking and long-span information. The agents, through self-evolution, can adaptively adjust strategies, optimize information storage and transmission, and effectively reduce cognitive load. We evaluate the performance of the SAGE framework on AgentBench and long text tasks. Experimental results show that SAGE significantly improves model performance, achieving a 2.26X improvement on closed-source models and an improvement ranging from 57.7\\% to 100\\% on open-source models, with particularly notable effects on smaller models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KmhVC7FKXN": {
    "title": "Rethinking Word Similarity: Semantic Similarity through Classification Confusion",
    "volume": "review",
    "abstract": "Word similarity is important for NLP and its applications to humanistic and social science tasks, like measuring meaning changes over time, detecting biases, understanding contested terms, and more. Yet the traditional similarity method based on cosine between word embeddings falls short in capturing the context-dependent, asymmetrical, polysemous nature of semantic similarity. We propose a cognitively-inspired model drawing on the proposal of Tversky (1977) that for conceptual tasks, people focus on extracting and compiling only the relevant features. Our Word Confusion model reframes semantic similarity in terms of feature-based classification confusion. We train a classifier to map from contextual embeddings to words and use the classifier confusion (the probability of choosing confound word c instead of correct target t) as a measure of the similarity of c and t. We show that Word Confusion outperforms cosine similarity in matching human similarity judgments across several datasets (MEN, WirdSim353, and SimLex), can measure similarity using predetermined features of interest, and enables qualitative analysis on real-world data. Reframing similarity based on classification confusion offers a cognitively-inspired, directional, and interpretable way of modeling the relationship between concepts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3hgsBxuGfQ": {
    "title": "EVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation",
    "volume": "review",
    "abstract": "The dynamic nature of real-world information necessitates knowledge editing (KE) in large language models (LLMs). The edited knowledge should propagate and facilitate the deduction of new information based on existing model knowledge. We term the existing related knowledge in LLM serving as the origination of knowledge propagation as ''deduction anchors''. However, current KE approaches, which only operate on (subject, relation, object) triple. We both theoretically and empirically observe that this simplified setting often leads to uncertainty when determining the deduction anchors, causing low confidence in their answers. To mitigate this issue, we propose a novel task of event-based knowledge editing that pairs facts with event descriptions. This task manifests not only a closer simulation of real-world editing scenarios but also a more logically sound setting, implicitly defining the deduction anchor and enabling LLMs to propagate knowledge confidently. We curate a new benchmark dataset Evedit derived from the CounterFact dataset and validate its superiority in improving model confidence. Moreover, while we observe that the event-based setting is significantly challenging for existing approaches, we propose a novel approach Self-Edit that showcases stronger performance, achieving 55.6% consistency improvement while maintaining the naturalness of generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rUDrnBi6kH": {
    "title": "LanguaShrink:Reducing Token Overhead with Psycholinguistics",
    "volume": "review",
    "abstract": "As large language models (LLMs) improve their capabilities in handling complex tasks, the issues of computational cost and efficiency due to long prompts are becoming increasingly prominent. To accelerate model inference and reduce costs, we propose an innovative prompt compression framework called LanguaShrink. Inspired by the observation that LLM performance depends on the density and position of key information in the input prompts, LanguaShrink leverages psycholinguistic principles and the Ebbinghaus memory curve to achieve task-agnostic prompt compression. This effectively reduces prompt length while preserving essential information. We referred to the training method of OpenChat.The framework introduces part-of-speech priority compression and data distillation techniques, using smaller models to learn compression targets and employing a KL-regularized reinforcement learning strategy for training.\\cite{wang2023openchat} Additionally, we adopt a chunk-based compression algorithm to achieve adjustable compression rates. We evaluate our method on multiple datasets, including LongBench, ZeroScrolls, Arxiv Articles, and a newly constructed novel test set. Experimental results show that LanguaShrink maintains semantic similarity while achieving up to 26 times compression. Compared to existing prompt compression methods, LanguaShrink improves end-to-end latency by 1.43 times",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OAUGyrzOEM": {
    "title": "Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval",
    "volume": "review",
    "abstract": "Persuasion plays a pivotal role in a wide range of applications, from health intervention, to the promotion of social good. Persuasive chatbots can accelerate the positive effects of persuasion in such applications. Existing methods rely on fine-tuning persuasive chatbots with task-specific training data which is costly, if not infeasible, to collect. To address this issue, we propose a method to leverage the generalizability and inherent persuasive abilities of large language models (LLMs) in creating effective and truthful persuasive chatbot for any given domain in a zero-shot manner. Unlike previous studies which used pre-defined persuasion strategies, our method first uses an LLM to generate responses, then extracts the strategies used on the fly, and replaces any unsubstantiated claims in the response with retrieved facts supporting the strategies. We applied our chatbot, PersuaBot, to three significantly different domains needing persuasion skills: donation solicitation, recommendations, and health intervention. Our experiments on simulated and human conversations show that our zero-shot approach is more persuasive than prior work, while achieving factual accuracy surpassing state-of-the-art knowledge-oriented chatbots. Our study demonstrated that when persuasive chatbots are employed responsibly for social good, it is an enabler of positive individual and social change",
    "checked": true,
    "id": "540937fd9669776b9a2234ea584435aa0f3bd163",
    "semantic_title": "zero-shot persuasive chatbots with llm-generated strategies and information retrieval",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uaE6jlngr4": {
    "title": "An Empirical Study on Robustness of Language Models via Decoupling Representation and Classifier",
    "volume": "review",
    "abstract": "Recent studies indicate that shortcut learning behavior exists in language models, and thus a number of mitigation methods are proposed, such as advanced PLMs and debiasing methods. However, few studies have explored how different factors affect the robustness of language models. To bridge this gap, we study the different PLMs and analyze the effect of representations and classifiers on robustness using probing techniques on the NLU tasks. First, we find that the low robustness of language models is not due to the inseparability of representations on the challenging dataset. Second, we find that a potential reason for the difficulty in improving the robustness of language models is the significantly high similarity between the representations with opposite semantics from in-distribution and out-of-distribution. Third, we find that debiasing methods are likely to distort representations and merely improve performance by better classifiers in some cases. Finally, we propose a probing tool to measure the impact on the robustness of language models from representations and classifiers using the decoupled training strategy with debiasing methods. In addition, we conduct extensive experiments on real-world datasets, suggesting the effectiveness of the proposed methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U4N7erk6Bf": {
    "title": "LLMs Are Prone to Fallacies in Causal Inference",
    "volume": "review",
    "abstract": "Recent work shows that causal facts can be effectively extracted from LLMs through prompting, facilitating the creation of causal graphs for causal inference tasks. However, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the model can memorize. Thus, this work investigates: Can LLMs infer causal relations from other relational data in text? To disentangle the role of memorized causal facts vs inferred causal relations, we finetune LLMs on synthetic data containing temporal, spatial and counterfactual relations, and measure whether the LLM can then infer causal relations. We find that: (a) LLMs are susceptible to inferring causal relations from the order of two entity mentions in text (e.g. X mentioned before Y implies X causes Y); (b) if the order is randomized, LLMs still suffer from the post hoc fallacy, i.e. X occurs before Y (temporal relation) implies X causes Y. We also find that while LLMs can correctly deduce the absence of causal relations from temporal and spatial relations, they have difficulty inferring causal relations from counterfactuals, questioning their understanding of causality",
    "checked": true,
    "id": "a8a2a15b5d51b51c62eeed6045ab1e67130e0867",
    "semantic_title": "llms are prone to fallacies in causal inference",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eVEAO0MHP4": {
    "title": "Improving Factual Consistency of News Summarization by Contrastive Preference Optimization",
    "volume": "review",
    "abstract": "Despite the recent progress in news summarization made by large language models (LLMs), they often generate summaries that are factually inconsistent with original articles, known as \"hallucinations\" in text generation. Unlike previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, overgeneralizing, etc. These hallucinations are challenging to detect through traditional methods, which poses great challenges for improving the factual consistency of text summarization. In this paper, we propose **C**ontrastive **P**reference **O**ptimization (**CPO**) to disentangle the LLMs' propensities to generate faithful and fake content. Furthermore, we adopt a probing-based specific training method to improve their capacity of distinguishing two types of generation. In this way, LLMs can execute the instructions more accurately and have enhanced perception of hallucinations. Experimental results show that CPO significantly improves the reliability of summarization based on LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1gNwBkOYsQ": {
    "title": "DemoRank: Selecting Effective Demonstrations for Large Language Models in Ranking Task",
    "volume": "review",
    "abstract": "Recently, there has been increasing interest in applying large language models (LLMs) as zero-shot passage rankers. However, few studies have explored how to select appropriate in-context demonstrations for the passage ranking task, which is the focus of this paper. Previous studies mainly apply a demonstration retriever to retrieve demonstrations and use top-$k$ demonstrations for in-context learning (ICL). Although effective, this approach overlooks the dependencies between demonstrations, leading to inferior performance of few-shot ICL in the passage ranking task. In this paper, we formulate the demonstration selection as a \\textit{retrieve-then-rerank} process and introduce the DemoRank framework. In this framework, we first use LLM feedback to train a demonstration retriever and construct a novel dependency-aware training samples to train a demonstration reranker to improve few-shot ICL. The construction of such training samples not only considers demonstration dependencies but also performs in an efficient way. Extensive experiments demonstrate DemoRank's effectiveness in in-domain scenarios and strong generalization to out-of-domain scenarios",
    "checked": true,
    "id": "d220ef1b0c58f2739c14b311b3d00c313011399a",
    "semantic_title": "demorank: selecting effective demonstrations for large language models in ranking task",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8xiyaVa25h": {
    "title": "Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision",
    "volume": "review",
    "abstract": "Cross-lingual open domain question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation in the query language. Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages. In this paper, we show that CLQA can be addressed using a single encoder-decoder model. To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia. We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural questions to supervise answer generation. Together, we show our approach, $\\texttt{CLASS}$, outperforms comparable methods on both supervised and zero-shot language adaptation settings, including those using machine translation",
    "checked": true,
    "id": "7d6052c672930172f27e61bbf494018cdcb3f4da",
    "semantic_title": "pre-training cross-lingual open domain question answering with large-scale synthetic supervision",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=qOmQ6AruXm": {
    "title": "SYNTHEMPATHY: A Scalable Empathy Corpus Generated Using LLMs Without Any Crowdsourcing",
    "volume": "review",
    "abstract": "Previous research has shown that humans are more receptive towards language models that that exhibit empathetic behavior. While empathy is essential for developing helpful dialogue agents, very few large corpora containing empathetic dialogues are available for fine-tune LLMs. The few existing corpora have largely relied on crowdsourcing to simulate empathetic conversations, a process that is expensive, time-consuming, and not scalable to larger datasets. We propose a data generation framework for developing SYNTHEMPATHY, a large corpus containing 105k empathetic responses to real-life situations compiled through LLM generation. A base Mistral 7B model fine-tuned on our SYNTHEMPATHY corpus exhibits an increase in the average empathy score",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nAwG5AhMio": {
    "title": "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment",
    "volume": "review",
    "abstract": "Large language models (LLMs) have enhanced the capacity of vision-language models to caption visual text. This generative approach to image caption enrichment further makes textual captions more descriptive, improving alignment with the visual context. However, while many studies focus on benefits of generative caption enrichment (GCE), are there any negative side effects? We compare standard-format captions and recent GCE processes from the perspectives of gender bias and hallucination, showing that enriched captions suffer from increased gender bias and hallucination. Furthermore, models trained on these enriched captions amplify gender bias by an average of 30.9% and increase hallucination by 59.5%. This study serves as a caution against the trend of making captions more descriptive",
    "checked": true,
    "id": "312a747b293aa165d18034f7d57fe29a1e979678",
    "semantic_title": "from descriptive richness to bias: unveiling the dark side of generative image caption enrichment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H2dvPXvfDy": {
    "title": "Stark: Social Long-Term Multi-Modal Conversation with Persona Commonsense Knowledge",
    "volume": "review",
    "abstract": "Humans share a wide variety of images related to their personal experiences within conversations via instant messaging tools. However, existing works focus on (1) image-sharing behavior in singular sessions, leading to limited long-term social interaction, and (2) a lack of personalized image-sharing behavior. In this work, we introduce \\dataset, a large-scale long-term multi-modal dialogue dataset that covers a wide range of social personas in a multi-modality format, time intervals, and images. To construct \\datasetName automatically, we propose a novel multi-modal contextualization framework, \\frameworkName, that generates long-term multi-modal dialogue distilled from ChatGPT and our proposed \\planExecute image aligner. Using our \\dataset, we train a multi-modal conversation model, \\model 7B, which demonstrates impressive visual imagination ability. Furthermore, we demonstrate the effectiveness of our dataset in human evaluation. The code, dataset, and model will be publicly released after publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hb8OFtXOcT": {
    "title": "M5 -- A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks",
    "volume": "review",
    "abstract": "Since the release of ChatGPT, the field of Natural Language Processing has experienced rapid advancements, particularly in Large Language Models (LLMs) and their multimodal counterparts, Large Multimodal Models (LMMs). Despite their impressive capabilities, LLMs often exhibit significant performance disparities across different languages and cultural contexts, as demonstrated by various text-only benchmarks. However, current research lacks such benchmarks for multimodal visio-linguistic settings. This work fills this gap by introducing M5, the first comprehensive benchmark designed to evaluate LMMs on diverse vision-language tasks within a multilingual and multicultural context. M5 includes eight datasets covering five tasks and $41$ languages, with a focus on underrepresented languages and culturally diverse images. Furthermore, we introduce two novel datasets, M5-VGR and M5-VLOD, including a new Visio-Linguistic Outlier Detection task, in which all evaluated open-source models fail to significantly surpass the random baseline. Through extensive evaluation and analyses, we highlight substantial task-agnostic performance disparities between high- and low-resource languages. Moreover, we show that larger models do not necessarily outperform smaller ones in a multilingual setting",
    "checked": false,
    "id": "677953fb705d1bb1f170dcee4ffab9824f8626de",
    "semantic_title": "m5 - a diverse benchmark to assess the performance of large multimodal models across multilingual and multicultural vision-language tasks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5hWKVerHWj": {
    "title": "DiPT: Enhancing LLM Reasoning through Diversified Perspective-Taking",
    "volume": "review",
    "abstract": "Existing work on improving language model reasoning typically explores a single solution path, which can be prone to errors. Inspired by perspective-taking in social studies, this paper introduces DiPT, a novel approach that complements current reasoning methods by explicitly incorporating diversified viewpoints. This approach allows the model to gain a deeper understanding of the problem's context and identify the most effective solution path during the inference stage. Additionally, it provides a general data-centric AI recipe for augmenting existing data to improve their quality for fine-tuning. Our empirical results demonstrate that DiPT can be flexibly integrated into existing methods that focus on a single reasoning approach, enhancing their reasoning performance and stability when presented with paraphrased problems. Furthermore, we illustrate improved context understanding by maintaining the model's safe outputs against \"jailbreaking\" prompts intentionally designed to bypass safeguards built into deployed models. Lastly, we show that fine-tuning with data enriched with diverse perspectives can boost the reasoning capabilities of the model compared to fine-tuning with raw data alone",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X3r0wOuuI3": {
    "title": "Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation",
    "volume": "review",
    "abstract": "This study presents a novel evaluation framework for the Vision-Language Navigation (VLN) task. It aims to diagnose current models for various instruction categories at a finer-grained level. The framework is structured around the context-free grammar (CFG) of the task. The CFG serves as the basis for the problem decomposition and the core premise of the instruction categories design. We propose a semi-automatic method for CFG construction with the help of Large-Language Models (LLMs). Then, we induct and generate data spanning five principal instruction categories (i.e. direction change, landmark recognition, region recognition, vertical movement, and numerical comprehension). Our analysis of different models reveals notable performance discrepancies and recurrent issues. The stagnation of numerical comprehension, heavy selective biases over directional concepts, and other interesting findings contribute to the development of future language-guided navigation systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hx2XnVfK55": {
    "title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision",
    "volume": "review",
    "abstract": "While Large Language Models (LLMs) have demonstrated proficiency in handling complex reasoning, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations poses scalability challenges, particularly as models and data requirements grow. In this work, we begin by analyzing the limitations of existing data-efficient reinforcement learning (RL) methods in LLMs' reasoning enhancement. To mitigate this, we introduce self-reinforcement, an efficient weak-to-strong approach to optimize language models' reasoning abilities utilizing both annotated and unlabeled samples. Our method enhances the quality of synthetic feedback by fully harnessing annotated seed data and introducing a novel self-filtering mechanism to remove invalid pairs. We also present \\textsc{PuzzleBen}, a weakly supervised benchmark for reasoning that comprises 25,147 complex questions, answers, and human-generated rationales across various domains, such as brainteasers, puzzles, riddles, parajumbles, and critical reasoning tasks. Our experiments underscore the significance of \\textsc{PuzzleBen}, as well as the effectiveness of our methodology as a promising direction in future endeavors. Our dataset and code will be published soon on \\texttt{Anonymity Link}",
    "checked": true,
    "id": "5f4317b03c3624bc686d9e270038c4c58fd6fc4d",
    "semantic_title": "optimizing language model's reasoning abilities with weak supervision",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=8aFlHbNALV": {
    "title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
    "volume": "review",
    "abstract": "Finetuning language agents with reasoning-action trajectories is effective, but obtaining these trajectories from human annotations or stronger models is costly and sometimes impractical. In this paper, we investigate the use of self-training in language agents, which can generate supervision from the agent itself, offering a promising alternative without relying on human or stronger model demonstrations. Self-training, however, requires high-quality model-generated samples, which are hard to obtain for challenging language agent tasks. To address this, we present Reflection-Reinforced Self-Training (Re-ReST), which uses a \\textit{reflector} to refine low-quality generated samples during self-training. The reflector takes the agent's output and feedback from an external environment (e.g., unit test results in code generation) to produce improved samples. This technique enhances the quality of inferior samples and efficiently enriches the self-training dataset with higher-quality samples. We conduct extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation. The results demonstrate the effectiveness of self-training and Re-ReST in language agent tasks, with self-training improving baselines by 7.6\\% on HotpotQA and 28.4\\% on AlfWorld, and Re-ReST further boosting performance by 2.0\\% and 14.1\\%, respectively. Our studies also confirm the efficiency of using a reflector to generate high-quality samples for self-training. Moreover, we demonstrate a method to employ reflection during inference without ground-truth feedback, addressing the limitation of previous reflection work. Our code will be publicized upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yDGJBh17WQ": {
    "title": "Zero and Few-Shot Learning Techniques for Cross-lingual Classification Tasks on Arabic and Code-Switched Data",
    "volume": "review",
    "abstract": "Zero-shot and few-shot learning techniques offer promising solutions for addressing data scarcity in Natural Language Processing (NLP), particularly in under-resourced languages such as Arabic and code-switching scenarios. Traditional supervised deep learning methods often struggle in such contexts due to their dependence on extensive labeled data. In this paper, we propose a novel approach that utilizes zero-shot and few-shot learning methodologies for cross-lingual classification tasks, focusing on Named Entity Recognition (NER) in Arabic texts and sentiment analysis in both Arabic and code-switched Arabic-English data. We introduce two approaches, employing Pattern Exploiting Training (PET) and Better-few-shot learning in language models (LM-BFF), which demonstrate versatility across diverse classification tasks. Subsequently, we conduct comprehensive evaluations on NER and sentiment analysis tasks, showcasing the superior performance of LM-BFF, surpassing previous techniques by 1.5\\% f1-score in sentiment analysis of code-switched data. This study emphasizes the importance of zero and few-shot learning methodologies in overcoming data scarcity challenges in Arabic NLP and code-switching research, thereby advancing NLP capabilities in under-resourced linguistic contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FpG7ye7Gvg": {
    "title": "Contextual Metric Meta-Evaluation by Measuring Local Metric Accuracy",
    "volume": "review",
    "abstract": "Meta-evaluation of automatic evaluation metrics---assessing evaluation metrics themselves---is crucial for accurately benchmarking natural language processing systems and has implications for scientific inquiry, production model development, and policy enforcement. While existing approaches to metric meta-evaluation focus on general statements about the absolute and relative quality of metrics across arbitrary system outputs, in practice, metrics are applied in highly contextual settings, often measuring the performance for a highly constrained set of system outputs. For example, we may only be interested in evaluating a specific model or class of models. We introduce a method for contextual metric meta-evaluation by comparing the \\textit{local metric accuracy} of evaluation metrics. Across translation, speech recognition, and ranking tasks, we demonstrate that the local metric accuracies vary both in absolute value and relative effectiveness as we shift across evaluation contexts",
    "checked": false,
    "id": "56bdd4919cfe75c2a739fac217a9496e4026d1ae",
    "semantic_title": "a novel and hybrid whale optimization with restricted crossover and mutation based feature selection method for anxiety and depression",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=4BpEfStZBD": {
    "title": "Devil's Advocate: Anticipatory Reflection for LLM Agents",
    "volume": "review",
    "abstract": "In this work, we introduce a novel approach that equips LLM agents with introspection, enhancing consistency and adaptability in solving complex tasks. Our approach prompts LLM agents to decompose a given task into manageable subtasks (i.e., to make a plan), and to continuously introspect upon the suitability and results of their actions. We implement a three-fold introspective intervention: 1) anticipatory reflection on potential failures and alternative remedy before action execution, 2) post-action alignment with subtask objectives and backtracking with remedy to ensure utmost effort in plan execution, and 3) comprehensive review upon plan completion for future strategy refinement. By deploying and experimenting with this methodology---a zero-shot approach---within WebArena for practical tasks in web environments, our agent demonstrates superior performance with a success rate of 23.5% over existing zero-shot methods by 3.5%. The experimental results suggest that our introspection-driven approach not only enhances the agent's ability to navigate unanticipated challenges through a robust mechanism of plan execution, but also improves efficiency by reducing the number of trials and plan revisions by 45% needed to achieve a task",
    "checked": true,
    "id": "b34c93096b2b3c5f6f5ac7aa52bf85757d74f7fa",
    "semantic_title": "devil's advocate: anticipatory reflection for llm agents",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=WKOZROjPqP": {
    "title": "MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks",
    "volume": "review",
    "abstract": "Information extraction (IE) is a fundamental area in natural language processing where prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets. We observe that while LLMs are not designed for user-specified information types, they have a decent sense of \\emph{important information}, i.e., the meta-understanding of IE. Therefore, we propose a novel framework MetaIE to build a small LM as a meta-model by learning to extract \"important information\", such that this meta-model can be effectively and efficiently adapted to all kinds of (few-shot) IE tasks. Specifically, we obtain the small LM via a symbolic distillation from an LLM. We construct the distillation dataset via sampling sentences from language model pre-training datasets and prompting an LLM to identify the typed spans of ``important information''. Extensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer a better starting point for few-shot adaptation and outperform other strong meta-models, including a multi-task model built upon multiple large IE benchmark training sets. Moreover, we provide comprehensive analyses of MetaIE, such as the size of the distillation dataset, the meta-model architecture, and the size of the meta-model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yodDLcGqXv": {
    "title": "Analyzing Context Contributions in LLM-based Machine Translation",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved state-of-the-art performance in machine translation (MT) and demonstrated the ability to leverage in-context learning through few-shot examples. However, the mechanisms by which LLMs use different parts of the input context remain largely unexplored. In this work, we provide a comprehensive analysis of context utilization in MT, studying how LLMs use various context parts, such as few-shot examples and the source text, when generating translations. We highlight several key findings: (1) the source part of few-shot examples appears to contribute more than its corresponding targets, irrespective of translation direction; (2) finetuning LLMs with parallel data alters the contribution patterns of different context parts; and (3) there is a positional bias where earlier few-shot examples have higher contributions to the translated sequence. Finally, we demonstrate that inspecting anomalous context contributions can uncover pathological translations, such as hallucinations. Our findings shed light on the internal workings of LLM-based MT which go beyond those known for standard encoder-decoder MT models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lrYnqFLCYr": {
    "title": "Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification",
    "volume": "review",
    "abstract": "For extremely weak-supervised text classification, pioneer research generates pseudo labels by mining texts similar to the class names from the raw corpus, which may end up with very limited or even no samples for the minority classes. Recent works have started to generate the relevant texts by prompting LLMs using the class names or definitions; however, there is a high risk that LLMs cannot generate in-distribution (i.e., similar to the corpus where the text classifier will be applied) data, leading to ungeneralizable classifiers. In this paper, we combine the advantages of these two approaches and propose to bridge the gap via a novel framework, \\emph{text grafting}, which aims to obtain clean and near-distribution weak supervision for minority classes. Specifically, we first use LLM-based logits to mine masked templates from the raw corpus, which have a high potential for data synthesis into the target minority class. Then, the templates are filled by state-of-the-art LLMs to synthesize near-distribution texts falling into minority classes. Text grafting shows significant improvement over direct mining or synthesis on minority classes. We also use analysis and case studies to comprehend the property of text grafting",
    "checked": true,
    "id": "9b565bbcbfeb9ce00264c4f524b4a4f76067b852",
    "semantic_title": "text grafting: near-distribution weak supervision for minority classes in text classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3mU0QS3G44": {
    "title": "LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language",
    "volume": "review",
    "abstract": "Despite advancements in English-dominant generative large language models, further development is needed for low-resource languages to enhance global accessibility. The primary methods for representing these languages are monolingual and multilingual pretraining. Monolingual pretraining is expensive due to hardware requirements, and multilingual models often have uneven performance across languages. This study explores an alternative solution by adapting large language models, primarily trained on English, to low-resource languages. We assess various strategies, including continual training, instruction fine-tuning, task-specific fine-tuning, and vocabulary extension. The results show that continual training improves language comprehension, as reflected in perplexity scores, and task-specific tuning generally enhances performance of downstream tasks. However, extending the vocabulary shows no substantial benefits. Additionally, while larger models improve task performance with few-shot tuning, multilingual models perform worse than their monolingual counterparts when adapted",
    "checked": true,
    "id": "2dd7fbb2519676255c26615085a377fde82211fa",
    "semantic_title": "llamaturk: adapting open-source generative large language models for low-resource language",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CwQ7ajvO9i": {
    "title": "Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring Conversations",
    "volume": "review",
    "abstract": "Many open-ended conversations (e.g., tutoring lessons or business meetings) revolve around pre-defined reference materials, like worksheets or meeting bullets. To provide a framework for studying such conversation structure, we introduce **Problem-Oriented Segmentation & Retrieval** (POSR), the task of _jointly_ breaking down conversations into segments and linking each segment to the relevant reference item. As a case study, we apply POSR to education where effectively structuring lessons around problems is critical yet difficult. We present **LessonLink**, the first dataset of real-world tutoring lessons, featuring 3,500 segments, spanning 24,300 minutes of instruction and linked to 116 SAT Math problems. We define and evaluate several joint and independent approaches for POSR, including segmentation (e.g., TextTiling), retrieval (e.g., ColBERT), and large language models (LLMs) methods. Our results highlight that modeling POSR as one joint task is essential: POSR methods outperform independent segmentation and retrieval pipelines by up to +$76$% on joint metrics and surpass traditional segmentation methods by up to +$78$% on segmentation metrics. We demonstrate POSR's practical impact on downstream education applications, deriving new insights on the language and time use in real-world lesson structures.** *Pronounced as ``poser'' (\\textipa{/\\textprimstress poz\\textschwa r/}), a perplexing problem. **You can find our code and LessonLink dataset as a zip file in our submission. If our work is accepted, the public-facing manuscript will include a GitHub link",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jnpCEDhgY6": {
    "title": "All Context Aware Reservoir Transformer",
    "volume": "review",
    "abstract": "The commitment of language processing is largely restricted by knowing the context around it. However, Transformer, as one of the most powerful neural network architectures, restricts has its input length restricted due to a quadratic time and memory complexity. Despite rich work advancing its efficiency, long context is still an issue that requires large computational resources in training. We realize a novel reservoir Transformer that bounds the learning in linear time by handling different input lengths in a cascaded way. For a long-term context, the reservoir with non-linear readout learns sample dependencies from the beginning to the end of a sequential dataset; To learn more accurately the medium-term context such as previous sentences, we apply a recurrent memory mechanism; and finally for the short-term dependencies in one sentence, we learn with the Transformer. Experiments show that our reservoir Transformer improves BERT and Blenderbot performance and significantly increases our prediction accuracy in language modeling, text classification, and chatbot tasks over the state-of-the-art methods",
    "checked": false,
    "id": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca",
    "semantic_title": "recurrent memory transformer",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=108NglI3z0": {
    "title": "FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping",
    "volume": "review",
    "abstract": "Autoregressive Large Language Models (e.g., LLaMa, GPTs) are omnipresent achieving remarkable success in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges for autoregressive token-by-token generation. To mitigate computation overload incurred during generation, several early-exit and layer-dropping strategies have been proposed. Despite some promising success due to the redundancy across LLMs layers on metrics like Rough-L/BLUE, our careful knowledge-intensive evaluation unveils issues such as generation collapse, hallucination, and noticeable performance drop even at the trivial exit ratio of ~10-15\\% of layers. We attribute these errors primarily to ineffective handling of the KV cache through state copying during early exit. In this work, we observe the saturation of computationally expensive feed-forward blocks of LLM layers and propose FFN-SkipLLM, which is a novel fine-grained skip strategy for autoregressive LLMs. FFN-SkipLLM leverages an input-adaptive feed-forward skipping approach that can skip ~25-30\\% of FFN blocks of LLMs with marginal change in performance on knowledge-intensive generation tasks without any requirement to handle the KV cache. Our extensive experiments and ablation studies across benchmarks like MT-Bench, Factoid-QA, and variable-length text summarization illustrate how our simple and easy-to-use method can facilitate faster autoregressive decoding. Related codes will be open-sourced",
    "checked": true,
    "id": "96c3a6156546d0447fa2b3327e55bc5973e01d57",
    "semantic_title": "ffn-skipllm: a hidden gem for autoregressive decoding with adaptive feed forward skipping",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=36LzMpUNFg": {
    "title": "Understanding and Improving Limitations of Multilingual AI Text Detection",
    "volume": "review",
    "abstract": "With the advances in multilingual large language models (LLMs), recent research has embarked on investigating diverse approaches towards multilingual AI-generated text (AI text) detection, including the fine-tuning of monolingual detectors. In this paper, we pinpoint the limitations in the evaluation procedures of current multilingual AI text detection. Our extensive analysis uncovers significant inadequacies in all of the available multilingual datasets, including $\\textbf{(i)}$ a primary focus on a limited set of languages, $\\textbf{(ii)}$ imbalanced data distribution between human and AI-generated samples, and $\\textbf{(iii)}$ a lack of diverse yet rich data collection sources. Amidst these challenges, we propose new methods to $\\textbf{(a)}$ improve cross-lingual transfer, $\\textbf{(b)}$ exploit novel fine-tuning strategies, $\\textbf{(c)}$ analyze the complexities of using neural machine translation (NMT) with monolingual detectors, and $\\textbf{(d)}$ a detailed analysis on adversarial robustness. Our results facilitate the engineering of a more resilient model for multilingual text detection, demonstrating superior performance and adaptability across a spectrum of languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=witGLrCMjz": {
    "title": "M2QA: Multi-domain Multilingual Question Answering",
    "volume": "review",
    "abstract": "Generalization and robustness to input variation are core desiderata of machine learning research. Language varies along several axes, most importantly, language instance (e.g. French) and domain (e.g. news). While adapting NLP models to new languages within a single domain, or to new domains within a single language, is widely studied, research in joint adaptation is hampered by the lack of evaluation datasets. This prevents the transfer of NLP systems from well-resourced languages and domains to non-dominant language-domain combinations. To address this gap, we introduce M2QA, a multi-domain multilingual question answering benchmark. M2QA includes 13,500 SQuAD 2.0-style question-answer instances in German, Turkish, and Chinese for the domains of product reviews, news, and creative writing. We use M2QA to explore cross-lingual cross-domain performance of fine-tuned models and state-of-the-art LLMs, and investigate modular approaches to domain and language adaptation. We witness 1) considerable performance _variations_ across domain-language combinations within model classes and 2) considerable performance _drops_ between source and target language-domain combinations across all model sizes. We demonstrate that M2QA is far from solved and new methods to effectively transfer both linguistic and domain-specific information are necessary",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H42VbGGYvY": {
    "title": "CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmented Generation in Fact-Checking",
    "volume": "review",
    "abstract": "Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems, their effectiveness is often hindered by a lack of integration with entity relationships and community structures, limiting their ability to provide contextually rich and accurate information retrieval for fact-checking. We introduce CommunityKG-RAG (Community Knowledge Graph-Retrieval Augmented Generation), a novel zero-shot framework that integrates community structures within Knowledge Graphs (KGs) with RAG systems to enhance the fact-checking process. Capable of adapting to new domains and queries without additional training, CommunityKG-RAG utilizes the multi-hop nature of community structures within KGs to significantly improve the accuracy and relevance of information retrieval. Our experimental results demonstrate that CommunityKG-RAG outperforms traditional methods, representing a significant advancement in fact-checking by offering a robust, scalable, and efficient solution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VFKhPYVzBm": {
    "title": "Model-based Preference Optimization in Abstractive Summarization without Human Feedback",
    "volume": "review",
    "abstract": "In abstractive summarization, the challenge of producing concise and accurate summaries arises from the vast amount of information contained in the source document. Consequently, although Large Language Models (LLMs) can generate fluent text, they often introduce inaccuracies by hallucinating content not found in the original source. While supervised fine-tuning methods that maximize likelihood contribute to this issue, they do not consistently enhance the faithfulness of the summaries. Preference-based optimization methods, such as Direct Preference Optimization (DPO), can further refine the model to align with human preferences. However, these methods still heavily depend on costly human feedback. In this work, we introduce a novel and straightforward approach called Model-based Preference Optimization (MPO) to fine-tune LLMs for improved summarization abilities without any human feedback. By leveraging the model's inherent summarization capabilities, we create a preference dataset that is fully generated by the model using different decoding strategies. Our experiments on standard summarization datasets and various metrics demonstrate that our proposed MPO significantly enhances the quality of generated summaries without relying on human feedback",
    "checked": false,
    "id": "0d1c76d45afa012ded7ab741194baf142117c495",
    "semantic_title": "direct preference optimization: your language model is secretly a reward model",
    "citation_count": 1365,
    "authors": []
  },
  "https://openreview.net/forum?id=xeMHWnfo7R": {
    "title": "Community-Cross-Instruct: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities",
    "volume": "review",
    "abstract": "Social scientists use surveys to probe the opinions and beliefs of populations, but these methods are slow, costly, and prone to biases. Recent advances in large language models (LLMs) enable creating computational representations or \"digital twins\" of populations that generate human-like responses mimicking the population's language, styles, and attitudes. We introduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs to online communities to elicit their beliefs. Given a corpus of a community's online discussions, Community-Cross-Instruct automatically generates instruction-output pairs by an advanced LLM to (1) finetune an foundational LLM to faithfully represent that community, and (2) evaluate the alignment of the finetuned model to the community. We demonstrate the method's utility in accurately representing political and fitness communities on Reddit. Unlike prior methods requiring human-authored instructions, Community-Cross-Instruct generates instructions in a fully unsupervised manner, enhancing scalability and generalization across domains. This work enables cost-effective and automated surveying of diverse online communities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zyHy5ELF2d": {
    "title": "Contrastive Perplexity for Controlled Generation: An Application in LLM Alignment",
    "volume": "review",
    "abstract": "The generation of toxic content of large language models poses a significant challenge and remains largely an unsolved issue. This paper studies the integration of a contrastive learning objective for fine-tuning LLMs for implicit knowledge editing and controlled text generation. Optimizing the training objective entails aligning text perplexities in a contrastive fashion. To facilitate training the model in a self-supervised fashion, we leverage an off-the-shelf LLM for training data generation. We showcase applicability in the domain of detoxification. Herein, the proposed approach leads to a significant decrease in the generation of toxic content while preserving general utility for downstream tasks such as commonsense reasoning and reading comprehension",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fhcvqpLdU7": {
    "title": "Entangled Relations: Leveraging NLI and Meta-analysis to Enhance Biomedical Relation Extraction",
    "volume": "review",
    "abstract": "Recent research efforts have explored the potential of leveraging natural language inference (NLI) techniques to enhance relation extraction (RE). In this vein, we introduce MetaEntail-RE, a novel adaptation method that harnesses NLI principles to enhance RE performance. Our approach follows past works by verbalizing relation classes into class-indicative hypotheses, aligning a traditionally multi-class classification task to one of textual entailment. We introduce three key enhancements: (1) Meta-class analysis which, instead of labeling non-entailed premise-hypothesis pairs with the less informative \"neutral\" entailment label, provides additional context by analyzing overarching meta-relationships between classes; (2) Feasible hypothesis filtering, which removes unlikely hypotheses from consideration based on pairs of entity types; and (3) Group-based prediction selection, which further improves performance by selecting highly confident predictions. MetaEntail-RE is conceptually simple and empirically powerful, yielding significant improvements over conventional relation extraction techniques and other NLI formulations. We observe F1 gains of 17.6 points on BioRED and 13.4 points on ReTACRED when compared to conventional methods, underscoring the versatility of MetaEntail-RE across both biomedical and general domains",
    "checked": true,
    "id": "548000291907da0fc4e9733190c0a89201aa69eb",
    "semantic_title": "entangled relations: leveraging nli and meta-analysis to enhance biomedical relation extraction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C28tkD1ogp": {
    "title": "ViFactCheck: A New Benchmark Dataset and Methods for Multi-domain News Fact-Checking in Vietnamese",
    "volume": "review",
    "abstract": "The rapid spread of information in the digital age highlights the critical need for effective fact-checking tools, particularly for languages with limited resources, such as Vietnamese. In response to this challenge, we introduce ViFactCheck, the first publicly available benchmark dataset designed specifically for Vietnamese fact-checking across multiple online news domains. This dataset contains 7,232 human-annotated pairs of claim-evidence combinations sourced from reputable Vietnamese online news, covering 12 diverse topics. It has been subjected to a meticulous annotation process to ensure high quality and reliability, achieving a Fleiss Kappa inter-annotator agreement score of 0.83. Our evaluation leverages state-of-the-art pre-trained and large language models, employing fine-tuning and prompting techniques to assess performance. Notably, the Gemma model demonstrated superior effectiveness, with an impressive macro F1 score of 89.90\\%, thereby establishing a new standard for fact-checking benchmarks. This result highlights the robust capabilities of Gemma in accurately identifying and verifying facts in Vietnamese. To further promote advances in fact-checking technology and improve the reliability of digital media, we have made the ViFactCheck dataset, model checkpoints, fact-checking pipelines, and source code freely available on GitHub. This initiative aims to inspire further research and enhance the accuracy of information in low-resource languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FZihRKWnze": {
    "title": "Efficient Fine-Tuning Approaches on HuBERT for Speech Emotion Recognition on Multiple Labels",
    "volume": "review",
    "abstract": "Models like HuBERT have shown significant promise in automatic speech recognition (ASR). In this work, we explore both vanilla fine-tuning and parameter-efficient fine-tuning of the HuBERT model for speech emotion recognition (SER). While most previous research on SER has focused on four basic emotionsâ€”happy, sad, angry, and neutralâ€”we extend this by incorporating additional emotions: surprise, fear, disgust, and calm, bringing the total to eight. Our experiments utilize four diverse datasets to enhance the robustness of our findings. Our methodology involves using the Wav2Vec2FeatureExtractor from the HuBERT model to extract features from raw audio files. These features are fed into a sequence classification model built on the HuBERT architecture. We fine-tuned the model in three different approaches -vanilla Finetuning, Parameter efficient finetuning over QKV projection and classifier using LoRA over a combination of several publicly available emotional speech datasets, including RAVDESS, CREMA-D, TESS, and SAVEE. The vanilla fine-tuned method outperforms all fine-tuned approaches overall. However, parameter-efficient approaches are still satisfactory and can be used in case of low resources and limited computational power",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9dkTWqRy5S": {
    "title": "A Simple Baseline for Zero-shot Visual Question Answering via Synthetic Data Generation",
    "volume": "review",
    "abstract": "Zero-shot Visual Question Answering (VQA) poses a challenging and crucial task in vision and language reasoning, demanding models to generate answers based on questions and images without human annotation. Previous approaches mainly focus on transforming images into captions and utilizing language model knowledge to answer visual questions. Despite the promising results, such a paradigm suffers from hallucination and high inference costs. In this paper, we propose a zero-shot VQA framework MKDG, which transfers knowledge from large language models (LLMs) and multi-modality models through a synthetic data generation approach, thus utilizing the ability of LLMs and mitigating the hallucination. Specifically, our method introduces a three-step synthetic data generation and training pipeline that first creates pseudo questions and answers with caption model and LLMs. To alleviate the hallucination and unbalanced data distribution in synthetic data, we propose a CLIP-based filtering and data selection strategy. Finally, we fine-tune a moderate-sized generative vision language model with the automatically curated synthetic dataset to perform VQA task. Experimental results on popular VQA benchmarks demonstrate the effectiveness of MKDG. We achieve superior performance and outperform outperforming strong baselines incorporating GPT-3 with significantly lower inference cost",
    "checked": false,
    "id": "647928b8d3d4074507ccabd538364d4619566cf5",
    "semantic_title": "indifoodvqa: advancing visual question answering and reasoning with a knowledge-infused synthetic data generation pipeline",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MObsTppWtK": {
    "title": "FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture",
    "volume": "review",
    "abstract": "Food is a rich and varied dimension of cultural heritage, crucial to both individuals and social groups. To bridge the gap in the literature on the often-overlooked regional diversity in this domain, we introduce FoodieQA, a manually curated, fine-grained image-text dataset capturing the intricate features of food cultures across various regions in China. We evaluate vision--language Models (VLMs) and large language models (LLMs) on newly collected, unseen food images and corresponding questions. FoodieQA comprises three multiple-choice question-answering tasks where models need to answer questions based on multiple images, a single image, and text-only descriptions, respectively. While LLMs excel at text-based question answering, surpassing human accuracy, the open-sourced VLMs still fall short by 41% on multi-image and 21% on single-image VQA tasks, although closed-weights models perform closer to human levels (within 10%). Our findings highlight that understanding food and its cultural implications remains a challenging and under-explored direction",
    "checked": true,
    "id": "4898cfa615b4a7fa53387337151b0262d979a9e8",
    "semantic_title": "foodieqa: a multimodal dataset for fine-grained understanding of chinese food culture",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=roMLbP6cGl": {
    "title": "Semantic Change Characterization with LLMs using Rhetorics",
    "volume": "review",
    "abstract": "Languages continually evolve in response to societal events, resulting in new terms and shifts in meanings. These changes have significant implications for computer applications, including automatic translation and chatbots, making it essential to characterize them accurately. The recent development of LLMs has notably advanced natural language understanding, particularly in sense inference and reasoning. In this paper, we investigate the potential of LLMs in characterizing three types of semantic change: dimension, relation, and orientation. We achieve this by combining LLMs' Chain-of-Thought with rhetorical devices and conducting an experimental assessment of our approach using newly created datasets. Our results highlight the effectiveness of LLM in capturing and analyzing semantic changes, providing valuable insights to improve computational linguistic applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6mI6tcvP7u": {
    "title": "Break the Chain: Large Language Models Can be Shortcut Reasoners",
    "volume": "review",
    "abstract": "Recent advancements in Chain-of-Thought (CoT) reasoning utilize complex modules but are hampered by high token consumption, limited applicability, and challenges in reproducibility. This paper conducts a critical evaluation of CoT prompting, extending beyond arithmetic to include complex logical and commonsense reasoning tasks, areas where standard CoT methods fall short. We propose the integration of human-like heuristics and shortcuts into language models (LMs) through \"break the chain\" strategies. These strategies disrupt traditional CoT processes using controlled variables to assess their efficacy. Additionally, we develop innovative zero-shot prompting strategies that encourage the use of shortcuts, enabling LMs to quickly exploit reasoning clues and bypass detailed procedural steps. Our comprehensive experiments across various LMs, both commercial and open-source, reveal that LMs maintain effective performance with \"break the chain\" strategies. We also introduce ShortcutQA, a dataset specifically designed to evaluate reasoning through shortcuts, compiled from competitive tests optimized for heuristic reasoning tasks such as forward/backward reasoning and simplification. Our analysis confirms that ShortcutQA not only poses a robust challenge to LMs but also serves as an essential benchmark for enhancing reasoning efficiency in AI",
    "checked": true,
    "id": "ea987db9daa0af1e495b29e2a03dff975641181b",
    "semantic_title": "break the chain: large language models can be shortcut reasoners",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LkLpG2mnpz": {
    "title": "MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown substantial progress in natural language understanding and generation, proving valuable especially in the medical field. Despite advancements, challenges persist due to the complexity and diversity inherent in medical tasks, which can be categorized as knowledge-intensive tasks and alignment-required tasks. Previous approaches either ignore the latter task or focus on a minority of tasks and hence lose generalization. To address these drawbacks, we propose a progressive fine-tuning pipeline. This pipeline employs a Knowledge Aggregator and a Noise aggregator to encode diverse knowledge in the first stage and filter out detrimental information. In the second stage, we drop the Noise Aggregator to avoid the interference of suboptimal representation and leverage an additional alignment module optimized towards an orthogonal direction to the knowledge space to mitigate knowledge forgetting. Based on this two-stage paradigm, we proposed a Medical LLM through decoupling Clinical Alignment and Knowledge Aggregation (MedCare), which is designed to achieve state-of-the-art (SOTA) performance on over 20 medical tasks, as well as SOTA results on specific medical alignment tasks. Various model sizes of MedCare (1.8B, 7B, 14B) all demonstrate significant improvements over existing models with similar model sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ovFbnJWJmM": {
    "title": "Swap or Skip? Challenging Step Type Identification in Instructional Manuals",
    "volume": "review",
    "abstract": "Large language models (LLMs) have been widely used as procedural planners, providing step-by-step guidance across applications. However, in a human-assistive scenario where the environment and users' knowledge constantly change, their ability to detect various step types for alternative plan generation remains under-explored. To fill this gap, we assess whether models can identify steps that are: (i) sequential, (ii) interchangeable, and (iii) optional in textual instructions. We compare LLMs to two vision-aware models relevant for procedural understanding: a large vision-language model and a heuristic approach that uses video-mined knowledge graphs. Our results indicate that LLMs struggle to capture the notion of mutual exclusivity between sequential and interchangeable steps. Furthermore, we report comprehensive analyses highlighting the advantages and limitations of using LLMs as procedural task guides. While the largest LLM shows expert-level task knowledge, our findings reveal its limitations in several key areas: broad task coverage, robustness towards diverse user phrasings, and physical reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qQbFpi1XzX": {
    "title": "BLSP-Emo: Towards Empathetic Large Speech-Language Models",
    "volume": "review",
    "abstract": "The recent release of GPT-4o showcased the potential of end-to-end multimodal models, not just in terms of low latency but also in their ability to understand and generate expressive speech with rich emotions. While the details are unknown to the open research community, it likely involves significant amounts of curated data and compute, neither of which is readily accessible. In this paper, we present BLSP-Emo (Bootstrapped Language-Speech Pretraining with Emotion support), a novel approach to developing an end-to-end speech-language model capable of understanding both semantics and emotions in speech and generate empathetic responses. BLSP-Emo utilizes existing speech recognition (ASR) and speech emotion recognition (SER) datasets through a two-stage process. The first stage focuses on semantic alignment, following recent work on pretraining speech-language models using ASR data. The second stage performs emotion alignment with the pretrained speech-language model on an emotion-aware continuation task constructed from SER data. Our experiments demonstrate that the BLSP-Emo model excels in comprehending speech and delivering empathetic responses, both in instruction-following tasks and conversations",
    "checked": true,
    "id": "eeed21017b19fde7f29294be8e72cc7e59b9ee94",
    "semantic_title": "blsp-emo: towards empathetic large speech-language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NPU7VzgU6N": {
    "title": "Exploring Spatial Understanding Capability in Large Language Models: Proficiency in Layout Generation sans Visual Perception",
    "volume": "review",
    "abstract": "Large language models (LLMs) consistently demonstrate superior performance in various natural language processing (NLP) tasks. However, research on their abilities to process visual and spatial information, which is essential for understanding visually-rich documents (VRDs), is limited. This paper presents a pioneering study and benchmark specifically designed to evaluate the spatial competencies of LLMs in the context of VRDs. Our assessment covers a comprehensive range of dimensions, including spatial perception, positional prediction, information extraction, and layout generation. The results show that despite the lack of inherent visual perception mechanisms in LLMs, these models can effectively infer spatial relationships within VRDs. In addition, we propose a layout-aware learning strategy with off-the-shelf LLMs that can significantly improve their performance. Our results indicate a significant contribution to the field of document intelligence, confirming the effectiveness of our methodology and pointing the way for future research in document analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3EmwxoK54w": {
    "title": "Self-Evolution Fine-Tuning for Policy Optimization",
    "volume": "review",
    "abstract": "The alignment of large language models (LLMs) is crucial not only for unlocking their potential in specific tasks but also for ensuring that responses meet human expectations and adhere to safety and ethical principles. To address the challenges of current alignment methodologies, we introduce self-evolution fine-tuning (SEFT) for LLM alignment, aiming to eliminate the need for annotated samples while retaining the stability and efficiency of SFT. SEFT first trains an adaptive reviser to elevate low-quality responses while maintaining high-quality ones. The reviser then gradually guides the policy's optimization by fine-tuning it with enhanced responses. The method excels in utilizing unlimited unannotated data to optimize policies via supervised fine-tuning. Our experiments on AlpacaEval and MT-Bench demonstrate the effectiveness of SEFT and its advantages over existing alignment techniques",
    "checked": true,
    "id": "ef010a9038dc02462312f6c1c97bf8d296c1abb5",
    "semantic_title": "self-evolution fine-tuning for policy optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CnnsDrNJ8s": {
    "title": "From Language to Action: Employing Foundation Models in Autonomous Robots",
    "volume": "review",
    "abstract": "Foundation models have demonstrated remarkable capabilities in natural language processing tasks, generating interest in their potential for robotic applications. However, the existing literature lacks a transparent and comprehensive synthesis of these advancements. This paper utilizes the PRISMA framework to systematically review and explore the integration of foundation models in robotic applications. Through an in-depth analysis of 76 studies, we investigate current trends in models, modalities, and experimental methods. Additionally, this study maps the state-of-the-art applications of foundation models in robotics tasks, and illustrate how these tasks are interconnected. Synthesizing these findings, we identified key challenges and future direction. This study establishes a benchmark and offers insights into future research directions for developing safe and autonomous embodied foundation models. All data, and findings are available on the project repository",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xryoosZiX6": {
    "title": "Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving",
    "volume": "review",
    "abstract": "Natural language explanations represent a proxy for evaluating explainable and multi-step Natural Language Inference (NLI) models. However, assessing the validity of explanations for NLI is challenging as it typically involves the crowd-sourcing of apposite datasets, a process that is time-consuming and prone to logical errors. To address existing limitations, this paper investigates the verification and refinement of natural language explanations through the integration of Large Language Models (LLMs) and Theorem Provers (TPs). Specifically, we present a neuro-symbolic framework, named Explanation-Refiner, that integrates TPs with LLMs to generate and formalise explanatory sentences and suggest potential inference strategies for NLI. In turn, the TP is employed to provide formal guarantees on the logical validity of the explanations and to generate feedback for subsequent improvements. We demonstrate how Explanation-Refiner can be jointly used to evaluate explanatory reasoning, autoformalisation, and error correction mechanisms of state-of-the-art LLMs as well as to automatically enhance the quality of explanations of variable complexity in different domains",
    "checked": true,
    "id": "93d9b7ff066e5d8d9dec9701923fa79183188ce6",
    "semantic_title": "verification and refinement of natural language explanations through llm-symbolic theorem proving",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=WJ09vSBYBR": {
    "title": "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering",
    "volume": "review",
    "abstract": "Users post numerous product-related questions on e-commerce platforms, affecting their purchase decisions. Product-related question answering (PQA) entails utilizing product-related resources to provide precise responses to users. We propose a novel task of Multilingual Cross-market Product-based Question Answering (MCPQA) and define the task as providing answers to product-related questions in a main marketplace by utilizing information from another resource-rich auxiliary marketplace in a multilingual context. We introduce a large-scale dataset comprising over 7 million questions from 17 marketplaces across 11 languages. We then perform automatic translation on the Electronics category of our dataset, naming it as McMarket. We focus on two subtasks: review-based answer generation and product-related question ranking. For each subtask, we label a subset of McMarket using an LLM and further evaluate the quality of the annotations via human assessment. We then conduct experiments to benchmark our dataset, using models ranging from traditional lexical models to LLMs in both single-market and cross-market scenarios across McMarket and the corresponding LLM subset. Results show that incorporating cross-market information significantly enhances performance in both tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LbegsBNyAs": {
    "title": "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction",
    "volume": "review",
    "abstract": "When extracting structured information from text, recognizing entities and extracting relationships are essential. Recent advances in both tasks generate a structured representation of the information in an autoregressive manner, a time-consuming and computationally expensive approach. This naturally raises the question of whether autoregressive methods are necessary in order to achieve comparable results. In this work, we propose ITER, an efficient encoder-based relation extraction model, that performs the task in three parallelizable steps, greatly accelerating a recent language modeling approach: ITER achieves an inference throughput of over 600 samples per second for a large model on a single consumer-grade GPU. Furthermore, we achieve state-of-the-art results on the relation extraction datasets ADE and ACE05, and demonstrate competitive performance for both named entity recognition with GENIA and CoNLL03, and for relation extraction with SciERC and CoNLL04",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y6EQiLYGVT": {
    "title": "1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have garnered significant attention due to their remarkable ability to process information across various languages. Despite their capabilities, they exhibit inconsistencies in handling identical queries in different languages, presenting challenges for further advancement. This paper introduces a method to enhance the multilingual performance of LLMs by aggregating knowledge from diverse languages. This approach incorporates a low-resource knowledge detector specific to a language, a strategic language selection process, and mechanisms for answer replacement and integration. Our extensive experiments demonstrate notable performance improvements, particularly in reducing the performance disparity across languages. An ablation study confirms that each component of our method significantly contributes to these enhancements. This research highlights the inherent potential of LLMs to harmonize multilingual capabilities and offers valuable insights for further exploration",
    "checked": true,
    "id": "a4027241c7b5f8ac6e7eb21eedeb21f19320d8b5",
    "semantic_title": "1+1>2: can large language models serve as cross-lingual knowledge aggregators?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6YjJklAAQ9": {
    "title": "Towards Event-intensive Long Video Understanding",
    "volume": "review",
    "abstract": "With the rapid development of video Multimodal Large Language Models (MLLMs), a surge of evaluation datasets is proposed to evaluate their video understanding capability. However, due to the lack of rich events in the videos, these datasets may suffer from the short-cut bias that the answers can be easily deduced by a few frames, without watching the entire video. To address this issue, we construct an event-oriented long video understanding benchmark, \\emph{\\textbf{Event-Bench}}, building upon existing datasets and human annotations. The benchmark includes six event-related tasks and a total of 2,190 test instances to comprehensively evaluate the capability to understand video events. Additionally, we propose \\emph{\\textbf{Video Instruction Merging (VIM)}}, a low-cost method to enhance video MLLMs by using merged event-intensive video instructions, aiming to overcome the scarcity of human-annotated, event-intensive data. Extensive experiments show that the best-performing GPT-4o achieves an overall accuracy of 53.33, significantly outperforming the best open-source model by 15.62. Leveraging the effective instruction synthesis method and model architecture, our VIM outperforms both state-of-the-art open-source video MLLMs and GPT-4V on Event-Bench. All the code, data, and models will be publicly available",
    "checked": false,
    "id": "4407337f4df63816e92866f0deaa842a5001a97d",
    "semantic_title": "towards event-oriented long video understanding",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=nxUL97tGWT": {
    "title": "MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic",
    "volume": "review",
    "abstract": "The advent of large language models (LLMs) like GPT-4 has catalyzed the exploration of multi-task learning (MTL), in which a single model demonstrates proficiency across diverse tasks. Task arithmetic has emerged as a cost-effective approach for MTL. It enables performance enhancement across multiple tasks by adding their corresponding task vectors to a pre-trained model. However, the current lack of a method that can simultaneously achieve optimal performance, computational efficiency, and data privacy limits their application to LLMs. In this paper, we propose \\textbf{M}odel \\textbf{E}xclusive \\textbf{T}ask \\textbf{A}rithmetic for merging \\textbf{GPT}-scale models (MetaGPT) which formalizes the objective of model merging into a multi-task learning framework, aiming to minimize the average loss difference between the merged model and each individual task model. Since data privacy limits the use of multi-task training data, we leverage LLMs' local linearity and task vectors' orthogonality to separate the data term and scaling coefficients term and derive a model-exclusive task arithmetic method. Our proposed MetaGPT is data-agnostic and bypasses the heavy search process, making it cost-effective and easy to implement for LLMs. Extensive experiments demonstrate that MetaGPT leads to improvement of task arithmetic and achieves state-of-the-art performance on multiple tasks",
    "checked": true,
    "id": "652344ac5269e90105d6af9e7bd72665577fe8e6",
    "semantic_title": "metagpt: merging large language models using model exclusive task arithmetic",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=IjVIorgJJQ": {
    "title": "Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions",
    "volume": "review",
    "abstract": "Classification is a core NLP task architecture with many potential applications. While large language models (LLMs) have brought substantial advancements in text generation, their potential for enhancing classification tasks remains underexplored. To address this gap, we propose a framework for thoroughly investigating fine-tuning LLMs for classification, including both generation- and encoding-based approaches. We instantiate this framework in edit intent classification (EIC), a challenging and underexplored classification task. Our extensive experiments and systematic comparisons with various training approaches and a representative selection of LLMs yield new insights into their application for EIC. To demonstrate the proposed methods and address the data shortage for empirical edit analysis, we use our best-performing model to create \\textit{Re3-Sci2.0}, a new large-scale dataset of 1,780 scientific document revisions with over 94k labeled edits. The new dataset enables an in-depth empirical study of human editing behavior in academic writing. We make our experimental framework, models and data publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qZSETTmoh4": {
    "title": "Are there identifiable structural parts in the sentence embedding whole?",
    "volume": "review",
    "abstract": "Sentence embeddings from transformer models encode in a fixed length vector much linguistic information. We explore the hypothesis that these embeddings consist of overlapping layers of information that can be separated, and on which specific types of information -- such as information about chunks and their structural and semantic properties -- can be detected. We show that this is the case using a dataset consisting of sentences with known chunk structure, and two linguistic intelligence datasets, solving which relies on detecting chunks and their grammatical number, and respectively, their semantic roles, and through analyses of the performance on the tasks and of the internal representations built during learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vkgc46fBBA": {
    "title": "Exploring the Impact of Occupational Personas on Domain-Specific QA",
    "volume": "review",
    "abstract": "Recent studies on personas have improved the way Large Language Models (LLMs) interact with users, but the impact of personas on knowledge-based Question Answering (QA) tasks has been underexplored. Inspired by Holland Occupational Themes, this study proposes Profession-Based Personas (PBPs) and Occupational Personality-Based Personas (OPBPs) to enhance performance in domain-specific QA tasks. We investigate the impact of PBP and OPBP on scientific datasets within the Massive Multitask Language Understanding (MMLU) benchmark. Experimental results show that PBPs, exemplified by the \"scientist\", achieve an accuracy improvement of 1.29\\% over the baseline. In contrast, the \"artist\" displays the lowest performance, with a 31.21\\% decrease and significant variability. Our findings demonstrate that assigning PBPs to LLMs enhances models' ability to invoke domain knowledge. Additionally, we observed that OPBPs might lead to lower performance, even when the defined personality type is relevant to the task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HjPBlQ6fGu": {
    "title": "Script-Agnostic Language Identification",
    "volume": "review",
    "abstract": "Language identification is used as the first step in many data collection and crawling efforts because it allows us to sort online text into language-specific buckets. However, many modern languages, such as Konkani, Kashmiri, Punjabi etc., are synchronically written in several scripts. Moreover, languages with different writing systems do not share significant lexical, semantic, and syntactic properties in neural representation spaces, which is a disadvantage for closely related languages and low-resource languages, especially those from the Indian Subcontinent. To counter this, we propose learning script-agnostic representations using several different experimental strategies (upscaling, flattening, and script mixing) focusing on four major Dravidian languages (Tamil, Telugu, Kannada, and Malayalam). We find that word-level script randomization and exposure to a language written in multiple scripts is extremely valuable for downstream script-agnostic language identification, while also maintaining competitive performance on naturally occurring text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4xDpUOKDP5": {
    "title": "How does a text preprocessing pipeline affect ontology matching?",
    "volume": "review",
    "abstract": "The generic text preprocessing pipeline, comprising Tokenisation, Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been implemented in many ontology matching (OM) systems. However, the lack of standardisation in text preprocessing creates diversity in mapping results. In this paper, we investigate the effect of the text preprocessing pipeline on OM tasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation Initiative (OAEI) track repositories with 49 distinct alignments indicate: (1) Tokenisation and Normalisation are currently more effective than Stop Words Removal and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and Stemming is task-specific. We recommend standalone Lemmatisation or Stemming with post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer perform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS) Tagging does not help Lemmatisation. To repair less effective Stop Words Removal and Stemming/Lemmatisation used in OM tasks, we propose a novel context-based pipeline repair approach that significantly improves matching correctness and overall matching performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BAVawpQw7Q": {
    "title": "Make Large Language Model a Better Ranker",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) demonstrate robust capabilities across various fields, leading to a paradigm shift in LLM-enhanced Recommender System (RS). Research to date focuses on point-wise and pair-wise recommendation paradigms, which are inefficient for LLM-based recommenders due to high computational costs. However, existing list-wise approaches also fall short in ranking tasks due to misalignment between ranking objectives and next-token prediction. Moreover, these LLM-based methods struggle to effectively address the order relation among candidates, particularly given the scale of ratings. To address these challenges, this paper introduces the large language model framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks. Specifically, ALRO employs explicit feedback in a listwise manner by introducing soft lambda loss, a customized adaptation of lambda loss designed for optimizing order relations. This mechanism provides more accurate optimization goals, enhancing the ranking process. Additionally, ALRO incorporates a permutation-sensitive learning mechanism that addresses position bias, a prevalent issue in generative models, without imposing additional computational burdens during inference. Our evaluative studies reveal that ALRO outperforms both existing embedding-based recommendation methods and LLM-based recommendation baselines",
    "checked": true,
    "id": "1eb3d80c68d45e6ff09965a3069787e41f10a090",
    "semantic_title": "make large language model a better ranker",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=VLzLj7dU9b": {
    "title": "AudioAgent: Enhancing Task Performance through Modality-Driven Prompt Optimization",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have achieved remarkable progress in serving as controllers to interpret user instructions and select models for audio tasks. However, current LLMs, when selecting tools, only consider the textual input, neglecting valuable information within the audio modality that could aid in choosing appropriate tools. Due to the possible ambiguity of instructions, errors in selection are common. To this end, we introduce AudioAgent, a versatile and adaptable agent framework for audio fields. It is the first system that emphasizes audio comprehension and utilizes these information to autonomously refine user-provided prompt by one finetuned LLM. Through clearer instructions, AudioAgent empowers the controller to precisely select the best tools and enhances the performance of tasks. Our framework also enables users to freely register tools and utilize any LLM as the core controller. Both subjective and objective metrics validate the effectiveness of our work. Result samples are available at https://AudioAgentTool.github.io",
    "checked": false,
    "id": "fd561ae4e760d0956f6b4dbb48b94d6b4e78c914",
    "semantic_title": "adapting segment anything model (sam) through prompt-based learning for enhanced protein identification in cryo-em micrographs",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=SVlkUHepTy": {
    "title": "Claim-Guided Textual Backdoor Attack for Practical Applications",
    "volume": "review",
    "abstract": "Recent advances in natural language processing and the increased use of large language models have exposed new security vulnerabilities, such as backdoor attacks. Previous backdoor attacks require input manipulation after model distribution to activate the backdoor, posing limitations in real-world applicability. Addressing this gap, we introduce a novel Claim-Guided Backdoor Attack (CGBA), which eliminates the need for such manipulations by utilizing inherent textual claims as triggers. CGBA leverages claim extraction, clustering, and targeted training to trick models to misbehave on targeted claims without affecting their performance on clean data. CGBA demonstrates its effectiveness and stealthiness across various datasets and models, significantly enhancing the feasibility of practical backdoor attacks. Our code and data will be available at https://github.com/PaperCGBA/CGBA",
    "checked": false,
    "id": "108fbc3166372689e9ecf701f8a83d2135aa3f06",
    "semantic_title": "badclip: dual-embedding guided backdoor attack on multimodal contrastive learning",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=3OC7RHTkGa": {
    "title": "TEVLA: Text-oriented Enhancement for Vision-Language Alignment in Infomation Extraction",
    "volume": "review",
    "abstract": "With the explosive growth of multimedia data storage, multimodal learning is an inevitable trend for Information Extraction (IE) tasks. Previous researches inadequately address the alignment of cross-modal features. Additionally, they neglect the filtering of noise samples, which cause adverse effects or degradation on each modality respectively. In light of this, we propose a strengthened cross-modal feature alignment module, using a generative text augmentation submodule to reduce the contamination from noise without inference redundancy. We further propose a fusion adapter utilizing a soft-prompt structure to profoundly fuse cross-modal features and enhance information interaction. To activate logical reasoning capabilities, we initially apply a generative language model with multi-turn dialogue format for the multimodal relation extraction (MRE) task. Our method outperforms prior works over the MNRE dataset, surpassing the previous SOTA model (TMR), with 7\\% increase in F1 score. And it has superior generalization capability for cross-task transfer learning, achieving state-of-the-art on named entity recognition (NER) over both Twitter2015/2017 datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZSkzlh1BSG": {
    "title": "RB-SQL: A Retrieval-based LLM Framework for Text-to-SQL",
    "volume": "review",
    "abstract": "Large language models (LLMs) with In-context learning has significantly improved the performance of text-to-SQL task. Previous works generally focus on using exclusive SQL generation prompt methods to improve the LLMs' reasoning ability. However, they usually ignore significance of the tables and columns related to the question, as well as the skeleton with SQL syntactic structure to alleviate errors and hallucination in SQL generation process. In this paper, we propose a novel retrieval-based text-to-SQL framework for In-context learning prompt construction, which consists of three models that retrieve tables, columns, and SQL skeleton respectively. Our experimental results and comprehensive analysis demonstrate the effectiveness of the proposed framework and achieve SOTA performance on two cross-domain text-to-SQL datasets (BIRD and Spider)",
    "checked": true,
    "id": "d55d8437a243c3ab48a8bb1154963ed8d5237158",
    "semantic_title": "rb-sql: a retrieval-based llm framework for text-to-sql",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PRkzXIy5ew": {
    "title": "Efficient Active Learning with Adapters",
    "volume": "review",
    "abstract": "One of the main obstacles for deploying Active Learning (AL) in practical NLP tasks is high computational cost of modern deep learning models. This issue can be partially mitigated by applying lightweight models as an acquisition model, but it can lead to the acquisition-successor mismatch (ASM) problem. Previous works show that the ASM problem can be partially alleviated by using distilled versions of a successor models as acquisition ones. However, distilled versions of pretrained models are not always available. Also, the exact pipeline of model distillation that does not lead to the ASM problem is not clear. To address these issues, we propose to use adapters as an alternative to full fine-tuning for acquisition model training. Since adapters are lightweight, this approach reduces the training cost of the model. We provide empirical evidence that it does not cause the ASM problem and can help to deploy active learning in practical NLP tasks",
    "checked": false,
    "id": "7b50b4e98b7f4a5662dd87e4de929b1037bc37c4",
    "semantic_title": "parameter-efficient language model tuning with active learning in low-resource settings",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=UHAFkcfNdL": {
    "title": "HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models",
    "volume": "review",
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly improved the performance across various natural language processing tasks. However, the huge parameter size of LLMs complicates full fine-tuning under limited computational resources. Consequently, parametric-efficient fine-tuning (PEFT) methods such as Low-rank Adaptation (LoRA) have become popular to reduce resource demands. Nevertheless, the inherent low rank of update parameters in LoRA may limit its expressiveness required for adapting to new sub-tasks. To alleviate this problem, in this paper, we propose a PEFT method called Hadamard high-Rank Adaptation (HiRA) to keep update parameters with a high rank, thereby enhancing the model capacity. The proposed HiRA method consists of two components, including a Hadamard component based on the Hadamard product and an offset component with low-rank structure. By combining those two components in an additive way, the proposed HiRA method can learn high-rank update parameters with introducing little computational overhead. Empirically, the proposed HiRA method surpasses LoRA and its variants in various downstream tasks such as commonsense reasoning and conversational tasks. We provide extensive ablation studies. Our code will be released later",
    "checked": false,
    "id": "f06191042e5b20b7d18672fbe9fc85f7b1bc4dfd",
    "semantic_title": "l4q: parameter efficient quantization-aware fine-tuning on large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3cigyj14Dx": {
    "title": "Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech",
    "volume": "review",
    "abstract": "Deep learning-based end-to-end automatic speech recognition (ASR) has made significant strides but still struggles with performance on out-of-domain (OOD) samples due to domain shifts in real-world scenarios. Test-Time Adaptation (TTA) methods address this issue by adapting models using test samples at inference time. However, current ASR TTA methods have largely focused on non-continual TTA, which limits cross-sample knowledge learning compared to continual TTA. In this work, we propose a Fast-slow TTA framework for ASR, which leverages the advantage of continual and non-continual TTA. Within this framework, we introduce Dynamic SUTA (DSUTA), an entropy-minimization-based continual TTA method for ASR. To enhance DSUTA's robustness on time-varying data, we propose a dynamic reset strategy that automatically detects domain shifts and resets the model, making it more effective at handling multi-domain data. Our method demonstrates superior performance on various noisy ASR datasets, outperforming both non-continual and continual TTA baselines while maintaining robustness to domain changes without requiring domain boundary information",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S0McroVcou": {
    "title": "Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models",
    "volume": "review",
    "abstract": "Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs). However, the former relies on external resources, and both require incorporating explicit documents into the context, which increases execution costs and susceptibility to noise data. Recent works indicate that LLMs have modeled rich knowledge, albeit not effectively triggered or awakened. Inspired by this, we propose a novel knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which simulates the human capacity to compensate for knowledge deficits while answering questions solely through imagination, thereby awakening relevant knowledge in LLMs without relying on external resources. Guided by IAG, we propose an imagine richer context method for question answering (IMcQA). IMcQA consists of two modules: explicit imagination, which generates a short dummy document by learning from long context compression, and implicit imagination, which creates flexible adapters by distilling from a teacher model with a long context. Experimental results on three datasets demonstrate that IMcQA exhibits significant advantages in both open-domain and closed-book settings, as well as in out-of-distribution generalization. Our code will be available at https://anonymous.4open.science/r/IMcQA",
    "checked": true,
    "id": "9e2c8145e394fa247bb29303a3f9ac613c12a18f",
    "semantic_title": "imagination augmented generation: learning to imagine richer context for question answering over large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I0gypxeQte": {
    "title": "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch",
    "volume": "review",
    "abstract": "Merging Large Language Models (LLMs) is a cost-effective technique for combining multiple expert LLMs into a single versatile model, retaining the expertise of the original ones. However, current approaches often overlook the importance of safety alignment during merging, leading to highly misaligned models. This work investigates the effects of model merging on alignment. We evaluate several popular model merging techniques, demonstrating that existing methods do not only transfer domain expertise but also propagate misalignment. We propose a simple two-step approach to address this problem: (i) generating synthetic safety and domain-specific data, and (ii) incorporating these generated data into the optimization process of existing data-aware model merging techniques. This allows us to treat alignment as a skill that can be maximized in the resulting merged LLM. Our experiments illustrate the effectiveness of integrating alignment-related data during merging, resulting in models that excel in both domain expertise and alignment",
    "checked": true,
    "id": "548881f1e934ea87c47f2b0facf4665ac306a7c0",
    "semantic_title": "model merging and safety alignment: one bad model spoils the bunch",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=o1bTN9apZU": {
    "title": "Evaluating LLMs Adversarially with Word Guessing Game",
    "volume": "review",
    "abstract": "The increasing significance of evaluating large language models (LLMs) is addressed in this paper. We present a new evaluation framework, Adversarial Guessing Evaluation (AGE), designed for LLMs. AGE employs a systematic set of rules and metrics to evaluate reading comprehension abilities and confusion capabilities of LLMs across different dimensions. Our framework significantly reduces the need for large datasets, requiring only a few pairs of words. The results align with average outcomes from established comprehensive benchmarks and highlight areas for potential improvements in LLMs",
    "checked": false,
    "id": "6b039a96c551eb17c9bf68ea393cf82e04d7087b",
    "semantic_title": "evaluating dialect robustness of language models via conversation understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bl5ZIQlqxo": {
    "title": "One-Vs-Rest Neural Network English Grapheme Segmentation: A Linguistic Perspective",
    "volume": "review",
    "abstract": "Grapheme-to-Phoneme (G2P) correspondences form foundational frameworks of tasks such as text-to-speech (TTS) synthesis or automatic speech recognition. The G2P process involves taking words in their written form and generating their pronunciation. In this paper, we critique the status quo definition of \\textit{grapheme}, currently a forced alignment process relating a single character to either a phoneme or a blank unit, that underlies the majority of modern approaches. We develop a linguistically-motivated redefinition from simple concepts such as vowel and consonant count and word length and offer a proof-of-concept implementation based on a multi-binary neural classification task. Our model achieves state-of-the-art results with a 31.86% Word Error Rate on a standard benchmark, while generating linguistically meaningful grapheme segmentations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=swyMHo2daP": {
    "title": "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional proficiency in language-related tasks, but their deployment poses significant challenges due to substantial memory and storage requirements. Weight-only quantization has emerged as a promising solution to address these challenges. Previous research suggests that fine-tuning through up and down rounding can enhance performance. In this study, we introduce SignRound, a method that utilizes signed gradient descent (SignSGD) to optimize rounding values and weight clipping within just 200 steps. SignRound integrates the advantages of Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ), achieving exceptional results across 2 to 4 bits while maintaining low tuning costs and avoiding additional inference overhead. For example, SignRound achieves absolute average accuracy improvements ranging from 6.91\\% to 33.22\\% at 2 bits. It also generalizes robustly to recent models and achieves near-lossless quantization in most scenarios at 4 bits. The source code will be publicly available",
    "checked": true,
    "id": "633e3fe49fe9c314f7245f77401c2e4a95e925a9",
    "semantic_title": "optimize weight rounding via signed gradient descent for the quantization of llms",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=CRcpdEluzC": {
    "title": "Cross-Subject Data Splitting for Brain-to-Text Decoding",
    "volume": "review",
    "abstract": "Recent major milestones have successfully decoded non-invasive brain signals (e.g. functional Magnetic Resonance Imaging (fMRI) and electroencephalogram (EEG)) into natural language. Despite the progress in model design, how to split the datasets for training, validating, and testing still remains a matter of debate. Most of the prior researches applied subject-specific data splitting, where the decoding model is trained and evaluated per subject. Such splitting method poses challenges to the utilization efficiency of dataset as well as the generalization of models. In this study, we propose a cross-subject data splitting criterion for brain-to-text decoding on various types of cognitive dataset (fMRI, EEG), aiming to maximize dataset utilization and improve model generalization. We undertake a comprehensive analysis on existing cross-subject data splitting strategies and prove that all these methods suffer from data leakage, namely the leakage of test data to training set, which significantly leads to overfitting and overestimation of decoding models. The proposed cross-subject splitting method successfully addresses the data leakage problem and we re-evaluate some SOTA brain-to-text decoding models as baselines for further research",
    "checked": true,
    "id": "38ccb330d85063a67a12e70379a7fe91ccd5bd21",
    "semantic_title": "cross-subject data splitting for brain-to-text decoding",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=fNy3FcdkX8": {
    "title": "Short Video is not only Video: Multimodal Unified Social Hypergraph Contrastive Enhancement for Fake News Video Detection",
    "volume": "review",
    "abstract": "Nowadays, fake short videos have seriously affected people's perception of news and situational awareness of event development. Previous work mainly focuses on the characteristics and dissemination of the news, and there is no in-depth mining of the social relationships and feature relationships of videos. This paper proposes a Multimodal Unified Social Hypergraph Contrastive Enhancement method MUHC for fake news videos detection. First, a unified social hypergraph is innovatively established for the representation of potential relationships in short videos. Meanwhile, a multimodal contrastive learning method for intra-modal and inter-modal relationships are designed to integrate different modalities. The above approach enhances data scalability while learning deeper about the potential relationships of the videos. Extensive experiments demonstrate that the method outperforms state-of-the-art on benchmark dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hzQf1lIVpI": {
    "title": "ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context",
    "volume": "review",
    "abstract": "Tables play a crucial role in conveying information in various domains. We propose a Plan-then-Reason framework to answer different types of user queries over tables with sentence context. The framework first plans the reasoning paths over the context, then assigns each step to program-based or textual reasoning to reach the final answer. This framework enhances the table reasoning abilities for both in-context learning and fine-tuning methods. GPT-3.5-Turbo following Plan-then-Reason framework surpasses other prompting baselines without self-consistency while using less API calls and in-context demonstrations. We also construct an instruction tuning set TrixInstruct to evaluate the effectiveness of fine-tuning with this framework. We present ProTrix model family by finetuning models on TrixInstruct. Our experiments show that ProTrix family generalizes to diverse unseen tabular tasks with only 6k training instances. We further demonstrate that ProTrix can generate accurate and faithful explanations to answer complex free-form questions. Our work underscores the importance of the planning and reasoning abilities towards a model over tabular tasks with generalizability and interpretability. We will open-source our dataset and models",
    "checked": true,
    "id": "2268d8104f4dd921fd90f4d2d6df9ecf8e7ce3eb",
    "semantic_title": "protrix: building models for planning and reasoning over tables with sentence context",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=sdPeC6Oohf": {
    "title": "Data Augmentation for Text-based Person Retrieval Using Large Language Models",
    "volume": "review",
    "abstract": "Text-based Person Retrieval aims to retrieve person images that match the description given a text query. The performance of the TPR model relies on high-quality data. However it is challenging to construct a large-scale, high-quality TPR dataset due to expensive annotation and privacy protection. Recently, Large Language Models (LLMs) have approached human performance on many NLP tasks, creating the possibility to expand high-quality TPR datasets. This paper proposes the first LLM-based Data Augmentation (LLM-DA) method for TPR. LLM-DA uses LLMs to rewrite the text in the TPR dataset, achieving high-quality expansion concisely and efficiently. These rewritten texts are able to increase text diversity while retaining the original key semantic concepts. To alleviate hallucinations of LLMs, LLM-DA introduces a Text Faithfulness Filter to filter out unfaithful rewritten text. To balance the contributions of original and augmented text, a Balanced Sampling Strategy is proposed to control the proportion of original and augmented text used for training. LLM-DA is a plug-and-play method that can be integrated into various TPR models. Comprehensive experiments show that LLM-DA can improve the retrieval performance of current TPR models",
    "checked": true,
    "id": "5426f16a7614d495f1d60fdbdb97ed0a6f3257f8",
    "semantic_title": "data augmentation for text-based person retrieval using large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CytotQoqNs": {
    "title": "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis",
    "volume": "review",
    "abstract": "We find arithmetic ability resides within a limited number of attention heads, with each head specializing in distinct operations. To delve into the cause of this phenomenon, we introduce the Comparative Neuron Analysis (CNA) method, which identifies an internal logic chain consisting of four distinct stages from input to prediction: feature enhancing with shallow FFN neurons, feature transferring by shallow attention layers, feature predicting by arithmetic heads, and prediction enhancing among deep FFN neurons. Moreover, we identify the human-interpretable FFN neurons within both feature-enhancing and feature-predicting stages. These findings lead us to investigate the mechanism of LoRA, revealing that it enhances prediction probabilities by amplifying the coefficient scores of FFN neurons related to predictions. Finally, we apply our method in model pruning for arithmetic tasks and model editing for reducing gender bias. Our code and data will be released on github",
    "checked": false,
    "id": "60f35bfe967dbce2c8694de8d283de01cc3766c2",
    "semantic_title": "rethinking tabular data understanding with large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zRZdBI7ORQ": {
    "title": "TransMI: A Framework to Create Strong Baselines from Multilingual Pretrained Language Models for Transliterated Data",
    "volume": "review",
    "abstract": "Transliterating related languages that use different scripts into a common script is effective for improving crosslingual transfer in downstream tasks. However, this methodology often makes pretraining a model from scratch unavoidable, as transliteration brings about new subwords not covered in existing multilingual pretrained language models (mPLMs). This is undesirable because it requires a large computation budget. A more promising way is to make full use of available mPLMs. To this end, this paper proposes a simple but effective framework: $\\textbf{Trans}$literate-$\\textbf{M}$erge-$\\textbf{I}$nitialize ($\\textbf{TransMI}$). TransMI is a strong baseline well-suited for data that is transliterated into a common script by exploiting an mPLM and its tokenizer. TransMI has three stages: ($\\textbf{a}$) transliterate the vocabulary of an mPLM into a common script; ($\\textbf{b}$) merge the new vocabulary with the original vocabulary; and ($\\textbf{c}$) initialize the embeddings of the new subwords. We apply TransMI to three strong recent mPLMs. Our experiments demonstrate that TransMI not only preserves the mPLM's ability to handle non-transliterated data, but also enables it to effectively process transliterated data, thereby facilitating crosslingual transfer. The results show consistent improvements of 3\\% to 34\\% for different mPLMs and tasks. We will make our code and models publicly available",
    "checked": true,
    "id": "313fde020ae6ec4ed7f7be6634349b6b33d76adb",
    "semantic_title": "transmi: a framework to create strong baselines from multilingual pretrained language models for transliterated data",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=dq9pIKiZYR": {
    "title": "Enhancing Emotion Recognition in Incomplete Data: A Novel Cross-Modal Alignment, Reconstruction, and Refinement Framework",
    "volume": "review",
    "abstract": "Multimodal emotion recognition systems rely heavily on the full availability of modalities, suffering significant performance declines when modal data is incomplete. To tackle this issue, we present the Cross-Modal Alignment, Reconstruction, and Refinement (CM-ARR) framework, an innovative approach that sequentially engages in cross-modal alignment, reconstruction, and refinement phases to handle missing modalities and enhance emotion recognition. This framework utilizes unsupervised distribution-based contrastive learning to align heterogeneous modal distributions, reducing discrepancies and modeling semantic uncertainty effectively. The reconstruction phase applies normalizing flow models to transform these aligned distributions and recover missing modalities. The refinement phase employs supervised point-based contrastive learning to disrupt semantic correlations and accentuate emotional traits, thereby enriching the affective content of the reconstructed representations. Extensive experiments on the IEMOCAP and MSP-IMPROV datasets confirm the superior performance of CM-ARR under conditions of both missing and complete modalities. Specifically, with all missing conditions on IEMOCAP and MSP-IMPROV, CM-ARR shows absolute improvements of 2.11% and 2.12%, 1.71% and 1.96% in WAR and UAR, respectively. Our code is available at https://anonymous.4open.science/r/CM-ARR-31C7",
    "checked": true,
    "id": "d97c374d5f4148dc2390d9eae3fc6aa2ff100989",
    "semantic_title": "enhancing emotion recognition in incomplete data: a novel cross-modal alignment, reconstruction, and refinement framework",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SnQOZc2az1": {
    "title": "Salience-aware Dialogue Summarization via Parallel Original-Extracted Streams",
    "volume": "review",
    "abstract": "In dialogue summarization, traditional approaches often concatenate utterances in a linear fashion, overlooking the dispersion of actions and intentions inherent in interactive conversations. This tendency frequently results in inaccurate summary generation. In response to this challenge, we formulate dialogue summarization as an extract-then-generate task. To tackle the extraction phase, we introduce an algorithm designed to identify Utterances Most related to speakers' key Intents (UMIs). These UMIs serve as labels to train an extraction model. Moving to the generation phase, we view a dialogue as parallel original-extracted streams. Correspondingly, we present a model named Row-Column Fusion Dual-Encoders and Utterance Prefix for Dialogue Summarization, abbreviated as RCUPS, with the goal of enhancing the model's ability to discern utterances and align with our sentence-level extraction. RCUPS integrates the row-column wise fusion module, which amalgamates vector representations from a dual-branch encoder. In the decoding stage, an utterance-level prefix is strategically employed to emphasize crucial details, while weight decay is applied to non-UMIs to mitigate their influence. To assess the effectiveness of RCUPS, comprehensive experiments on SAMSum, DialogSum, and TODSum datasets show significant improvements over robust baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zPKjzqfXhM": {
    "title": "PIRLS Category-specific Question Generation for Reading Comprehension",
    "volume": "review",
    "abstract": "According to the internationally recognized PIRLS (Progress in International Reading Literacy Study) assessment standards, reading comprehension questions should encompass all four comprehension processes: retrieval, inferencing, integrating and evaluation. This paper investigates whether Large Language Models can produce high-quality questions for each of these categories. Human assessment on a Chinese dataset shows that GPT-4o can generate usable and category-specific questions, ranging from 74% to 90% accuracy depending on the category",
    "checked": false,
    "id": "e94e305f5ac172e398c38df640d1debb82392720",
    "semantic_title": "two-stage aspect sentiment quadruple prediction based on mrc and text generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=MFsR2r0Ejq": {
    "title": "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models",
    "volume": "review",
    "abstract": "We propose VE-KD, a novel method that balances knowledge distillation and vocabulary expansion with the aim of training efficient domain-specific language models. Compared with traditional pre-training approaches, VE-KD exhibits competitive performance in downstream tasks while reducing model size and using fewer computational resources. Additionally, VE-KD refrains from overfitting in domain adaptation. Our experiments with different biomedical domain tasks demonstrate that VE-KD performs well compared with models such as BioBERT (+1% at HoC) and PubMedBERT (+1% at PubMedQA), with about 96% less training time. Furthermore, it outperforms DistilBERT and Adapt-and-Distill, showing a significant improvement in document-level tasks. Investigation of vocabulary size and tolerance, which are hyperparameters of our method, provides insights for further model optimization. The fact that VE-KD consistently maintains its advantages, even when the corpus size is small, suggests that it is a practical approach for domain-specific language tasks and is transferrable to different domains for broader applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=My6R2q5mWz": {
    "title": "Late Inception Prompt Tuning: Improving Prompt Tuning with Inception Reparameterization",
    "volume": "review",
    "abstract": "Prompt tuning, a growing technique in Natural Language Processing (NLP), adapts large language models to various tasks with minimal parameter adjustments using learned prompt vectors. Despite its popularity, optimizing prompt efficiency and performance remains challenging. Past efforts focused on prompt positions or initialization, with limited methods for generating prompts. This study presents Late Inception Prompt Tuning (LIPT), a novel approach improving soft prompt learning by overlaying multiple bottleneck networks. LIPT enhances performance and speeds convergence by integrating late prompt tuning methods. It includes a multi-dimensional linear weighted metric calculation, demonstrating superiority in experiments on GLUE and other tasks. On the RoBERTa-large model, LIPT outperforms Late Prompt Tuning (LPT) by 1.3 points and the original prompt tuning method by 5 points, with faster convergence. Sometimes, LIPT even surpasses full fine-tuning methods. Our research confirms LIPT's theoretical and practical adaptability to Pretrained Language Models (PLMs)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lIbEoTudsZ": {
    "title": "Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs",
    "volume": "review",
    "abstract": "Recent advancements in LLMs have showcased their remarkable role-playing capabilities, able to accurately simulate the dialogue styles and cognitive processes of various roles based on different instructions and contexts. Studies indicate that assigning LLMs the roles of experts, a strategy known as role-play prompting, can enhance their performance in the corresponding domains. However, the prompt needs to be manually designed for the given problem, requiring certain expertise and iterative modifications. To this end, we propose self-prompt tuning, making LLMs themselves generate role-play prompts through fine-tuning. Leveraging the LIMA dataset as our foundational corpus, we employ GPT-4 to annotate role-play prompts for each data points, resulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like Llama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned LLMs can automatically generate expert role prompts for any given question. We extensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and open-ended question test. Our empirical results illustrate that self-prompt tuned LLMs outperform standard instruction tuned baselines across most datasets. This highlights the great potential of utilizing fine-tuning to enable LLMs to self-prompt, thereby automating complex prompting strategies. We release the dataset, models, and code at https://anonymous.4open.science/r/Self-Prompt-Tuning-739E",
    "checked": true,
    "id": "c8e1dc10f302fb5cd748672d5d10d95a27c3bd0b",
    "semantic_title": "self-prompt tuning: enable autonomous role-playing in llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Z4Z3pqrLMw": {
    "title": "Dynamic Tuning and Multi-Task Learning Based Model for Multimodal Sentiment Analysis",
    "volume": "review",
    "abstract": "Multimodal sentiment analysis aims to uncover human affective states by integrating data from multiple sensory sources. However, previous studies have focused on optimizing the model architecture, neglecting the impact of objective function settings on model performance. Given this, this study introduces a new framework - DMMSA, which integrates uni and multimodal sentiment analysis tasks, utilizes the intrinsic correlation of sentimental signals, and enhances the model's understanding of complex sentiments. In addition, it reduces task complexity by incorporating coarse-grained sentiment analysis. Meanwhile, the framework embeds a contrastive learning mechanism within the modality, enhancing the ability to distinguish between similar and dissimilar features. We conducted experiments on CH-SIMS, MOSI, and MOEI. The results showed that DMMSA outperformed the baseline method in classification and regression tasks when the model structure was unchanged, and only the optimization objectives were replaced",
    "checked": false,
    "id": "caaf1e534c85e0e33630bb395ecd937858e3c23d",
    "semantic_title": "dynamically shifting multimodal representations via hybrid-modal attention for multimodal sentiment analysis",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=qGewKcofxZ": {
    "title": "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties",
    "volume": "review",
    "abstract": "A major reason behind the recent success of large language models (LLMs) is their $\\textit{in-context learning}$ capability, which makes it possible to rapidly adapt them to downstream text-based tasks by prompting them with a small number of relevant demonstrations. While large vision-language models (VLMs) have recently been developed for tasks requiring both text and images, they largely lack in-context learning over visual information, especially in understanding and generating text about videos. In this work, we implement $\\textbf{E}$mergent $\\textbf{I}$n-context $\\textbf{Le}$arning on $\\textbf{V}$ideos ($\\textbf{EILeV}$), a novel training paradigm that induces in-context learning over video and text by capturing key properties of pre-training data found by prior work to be essential for in-context learning in transformers. In our experiments, we show that $\\textbf{EILeV}$-trained models outperform other off-the-shelf VLMs in few-shot video narration for novel, rare actions. Furthermore, we demonstrate that these key properties of bursty distributions, skewed marginal distributions, and dynamic meaning each contribute to varying degrees to VLMs' in-context learning capability in narrating procedural videos. Our results, analysis, and $\\textbf{EILeV}$-trained models yield numerous insights about the emergence of in-context learning over video and text, creating a foundation for future work to optimize and scale VLMs for open-domain video understanding and reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U3SmiXUxAq": {
    "title": "Smaug: A Chat Model with Agent-Generated Data for Conversational Recommendations",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated remarkable success in general chat scenarios, delivering coherent and contextually appropriate responses to a wide range of questions. However, current chat models struggle to provide high-quality responses for recommendations, particularly when the recommended items belong to specific domains not covered by common knowledge. In this paper, we propose an efficient method for constructing personalized conversations to fine-tune LLMs for conversational recommendations. Based on this method, we provide a high-quality conversation dataset tailored for the shopping scenario. Using this dataset, we fine-tune a chat model and introduce a chat framework that delivers both high-quality conversations and accurate recommendations. Experimental results show that LLMs fine-tuned on our datasets achieve significant improvements in both recommendation performance and generation quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=irYBP6Rx57": {
    "title": "Zero-Shot Chain-of-Thought Reasoning Guided by Swarm Intelligence Algorithms in Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks and exhibited impressive reasoning abilities by applying zero-shot Chain-of-Thought (CoT) prompting. However, due to the evolving nature of sentence prefixes during the pre-training phase, existing zero-shot CoT prompting methods that employ identical CoT prompting across all task instances may not be optimal. In this paper, we introduce a novel zero-shot prompting method that leverages swarm intelligence algorithms to dynamically generate diverse promptings for LLMs. Our approach involves initializing several CoT promptings, performing evolutionary operations based on LLMs to create a varied set, and utilizing the LLMs to select a suitable CoT prompting for a given problem. Additionally, a rewriting operation, guided by the selected CoT prompting, enhances the understanding of the LLMs about the problem. Extensive experiments conducted across ten reasoning datasets demonstrate the superior performance of our proposed method compared to current zero-shot CoT prompting methods on both black-box and open-source LLMs. Moreover, in-depth analytical experiments underscore the adaptability and effectiveness of our method in various reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U15vRrs5lw": {
    "title": "EVER: Mitigating Hallucination in Large Language Models through Generation-Time Verification and Rectification",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the \"snowballing\" issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Generation-Time Verification and Rectification (EVER). Instead of waiting until the end of the generation process to rectify hallucinations, EVER employs a generation-time, step-wise generation and hallucination rectification strategy. Apart from directly mitigating hallucination, we further demonstrate that both the EVER-rectified response and the original one can serve as preference data to enhance the factuality of the model through preference tuning. When compared to both retrieval-based and non-retrieval-based baselines, EVER demonstrates a significant improvement in generating trustworthy and factually accurate text across a diverse range of tasks, including biography generation and multi-hop reasoning",
    "checked": false,
    "id": "b10482ab3dd1d340c3c926d92c3e617c24ee3949",
    "semantic_title": "ever: mitigating hallucination in large language models through real-time verification and rectification",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=WpdSAan8Tx": {
    "title": "Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated significant potential in revolutionizing clinical applications. In this study, we investigate the efficacy of four techniques in adapting LLMs for clinical use-cases: continuous pretraining, instruct fine-tuning, NEFTune, and prompt engineering. We employ these methods on Mistral 7B and Mixtral 8x7B models, leveraging a large-scale clinical pretraining dataset of 50 billion tokens and an instruct fine-tuning dataset of 500 million tokens. Our evaluation across various clinical tasks reveals nuanced insights. While continuous pretraining beyond 250 billion tokens yields marginal improvements, instruct fine-tuning emerges as a more influential factor. Notably, NEFTune, designed primarily to enhance generation quality, surprisingly demonstrates additional gains on our benchmark. These findings underscore the importance of tailoring fine-tuning strategies and exploring innovative techniques to optimize LLM performance in the clinical domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wgKhZvPojL": {
    "title": "Enhancing Byzantine-Resistant Aggregations with Client Embedding",
    "volume": "review",
    "abstract": "Byzantine-resistant aggregations detect poisonous clients and discard them to ensure that the global model is not poisoned or attacked by malicious clients. However, these aggregations are mainly conducted on the parameter space, and the parameter distances cannot reflect the data distribution divergences between clients. Therefore, existing Byzantine-resistant aggregations cannot defend against backdoor injection by malicious attackers in federated natural language tasks. In this paper, we propose the client embedding for malicious client detection to enhance Byzantine-resistant aggregations. The distances between client embeddings are required to reflect the data distribution divergences of the corresponding clients. Experimental results validate the effectiveness of the proposed client embeddings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy2Z0pSGLi": {
    "title": "Supplemental Enhancement of Action Segments: A Retrieval Optimization for Large Language Models in the Legal Domain",
    "volume": "review",
    "abstract": "Utilizing the Retrieval-Augmented Generation (RAG) framework with large language models for question answering often results in low retrieval precision and recall rates. A solution to address this issue involves retrieving external knowledge at various granularities. However, this strategy typically suffers from decreased precision in coarse-grained retrieval and omissions in fine-grained retrieval. To overcome these challenges, we introduce a novel framework designed for the legal domain, named Supplemental Enhancement of Action Segments (SEAS). SEAS utilizes few-shot prompting to extract action segments from legal texts, which are then used to enhance the retrieval of complete legal texts. In the Japanese Law Retrieval task, SEAS significantly enhances the performance of three distinct embedding models. Furthermore, in the Chinese Legal Question Answering task, SEAS outperforms all baselines across all metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oEd61a1EpR": {
    "title": "TourRank: Utilizing Large Language Models for Documents Ranking with a Tournament-Inspired Strategy",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are increasingly employed in zero-shot documents ranking, yielding commendable results. However, several significant challenges still persist in LLMs for ranking: (1) LLMs are constrained by limited input length, precluding them from processing a large number of documents simultaneously; (2) The output document sequence is influenced by the input order of documents, resulting in inconsistent ranking outcomes; (3) Achieving a balance between cost and ranking performance is quite challenging. To tackle these issues, we introduce a novel documents ranking method called TourRank, which is inspired by the tournament mechanism. This approach alleviates the impact of LLM's limited input length through intelligent grouping, while the tournament-like points system ensures robust ranking, mitigating the influence of the document input sequence. We test TourRank with different LLMs on the TREC DL datasets and the BEIR benchmark. Experimental results show that TourRank achieves state-of-the-art performance at a reasonable cost",
    "checked": true,
    "id": "0d25d0fadeb9091ea8c8e054c699d852eaf80374",
    "semantic_title": "tourrank: utilizing large language models for documents ranking with a tournament-inspired strategy",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DuWc8GEqnB": {
    "title": "Identification of depression and PTSD among Twitter users using pre-trained language model",
    "volume": "review",
    "abstract": "Suicide is a global health issue and early diagnosis is necessary for effective treatment. Recent advancements in natural language processing has aided the identification of mental health disorders in social media. This paper investigated the efficacy of pre-trained language model (PLM) in identifying depression and post-traumatic stress disorder (PTSD) with Twitter data. Leveraging the CLPysch 2015 dataset (which constitutes of tweets from users with depression, PTSD and neither condition), we implemented various experimental designs using Long Short Term Memory (LSTM) and attention. The results demonstrate that while performance decreases for multi-nominal classification, the detection of mental health conditions improves with the implementation of attention. This study also underscores the complexity of differentiating between overlapping lexicons with multiple mental health conditions and highlights the potential of PLMs in supporting mental health diagnosis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B0dAudLu9K": {
    "title": "Integrating Spoken and Signed Languages for Inclusive and Modality-Independent Large Language Models",
    "volume": "review",
    "abstract": "Sign language processing (SLP) is often reduced to translation using state-of-the-art computer vision models combined with neural machine translation systems. Comparatively, a growing field of instruct-tuned large language models can accomplish multiple NLP tasks end-to-end. However, signed languages are not included in these models; instead, special translation models are developed for signed languages. This paper proposes that SLP can be included in the (large) language model development, freeing sign language models from the necessity of low-resource multimodal learning from scratch. We introduce the first text-only and multimodal large (7B) LLaMA-based language models to be pre-trained and then fine-tuned on a sign language recognition task. We propose new prompting and fine-tuning strategies for text-only and multimodal SLP, incorporating both linguistics of signed languages and theoretically motivated strategies to mitigate catastrophic forgetting (of spoken language). We test the generalization of these models to other SLP tasks, showing LLMs are also capable sign language models that are still adept at spoken language tasks and, by changing the prompt, can even generalize to new prosodic and iconic sign translation tasks. Finally, we analyze trade-offs between our text-only and multimodal models. Our code and model checkpoints will be open-source. We will update our model suite as newer open-source LLMs, datasets, and SLP tasks become available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rct6Kqy6Pb": {
    "title": "MoSLD: A Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning",
    "volume": "review",
    "abstract": "Recently, LoRA has emerged as a crucial technique for fine-tuning large pre-trained models, yet its performance in multi-task learning scenarios often falls short. In contrast, the MoE architecture presents a natural solution to this issue. However, it introduces challenges such as mutual interference of data across multiple domains and knowledge forgetting of various tasks. Additionally, MoE significantly increases the number of parameters, posing a computational cost challenge. Therefore, in this paper, we propose MoSLD, a mixture-of-shared-LoRAs model with a dropout strategy. MoSLD addresses these challenges by sharing the upper projection matrix in LoRA among different experts, encouraging the model to learn general knowledge across tasks, while still allowing the lower projection matrix to focus on the unique features of each task. The application of dropout mitigates parameter overfitting in LoRA. Extensive experiments demonstrate that our model exhibits excellent performance in both single-task and multi-task scenarios, with robust out-of-domain generalization capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uPZkAUa2ng": {
    "title": "Large Language Models as a Normalizer for Transliteration and Dialectal Translation",
    "volume": "review",
    "abstract": "NLP models trained on standardized language data often struggle with variations. We assess various Large Language Models (LLMs) for transliteration and dialectal normalization. Tuning open-source LLMs with as little as 10,000 parallel examples using LoRA can achieve results comparable to or better than closed-source LLMs. We perform dialectal normalization experiments for twelve South Asian languages and dialectal translation experiments for six language continua worldwide. The dialectal normalization task can also be a preliminary step for the downstream dialectal translation task. Among the six languages used in dialectal translation, our approach enables Italian and Swiss German to surpass the baseline model by 21.55 and 25.79 BLEU points, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wnZ5gINYfO": {
    "title": "Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) exhibit impressive natural language capabilities but suffer from hallucination -- generating content ungrounded in the realities of training data. Recent work has focused on decoding techniques to improve factuality in decoding by leveraging LLMs' hierarchical representation of factual knowledge, manipulating the predicted distributions at inference time. Current state-of-the-art approaches refine decoding by contrasting logits from a lower layer with the final layer to exploit information related factuality within the model forward procedure. However, such methods often assume the final layer is most reliable one and the lower layer selection process depends on it. In this work, we first propose logit extrapolation of critical token probabilities beyond the last layer for more accurate contrasting. We additionally employ layer-wise entropy-guided lower layer selection, decoupling the selection process from the final layer. Experiments demonstrate strong performance - surpassing state-of-the-art on multiple different datasets by large margins. Analyses show different kinds of prompts respond to different selection strategies",
    "checked": true,
    "id": "6073b9a4856d726c270f03ebee54ea7658f16ec1",
    "semantic_title": "entropy guided extrapolative decoding to improve factuality in large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zdDwzNKOtx": {
    "title": "MICL: Improving In-Context Learning through Multiple-Label Words in Demonstration",
    "volume": "review",
    "abstract": "In-context learning (ICL) enables large language models (LLMs) to perform new tasks by using sample-label pairs as demonstrations. However, variations in demonstrations can lead to significantly different performances. Current research mainly focuses on selecting demonstration samples, preassuming the class name to be the label word when creating sample-label pairs. However, the choice of label words is crucial for ICL performance. In addition, we observe that using a single class name in demonstration may not yield optimal results. In this paper, we propose to use multiple label words in one sample-label pair to enhance ICL performance. Further, we select and order sample-label pairs based on LLM's output distribution, aiming to optimize the demonstration examples from both the samples' and labels' perspectives. Evaluation results on seven classification datasets show that the use of multiple label words, strategically organized by their selection, order and quantity, improves ICL performance through diverse label information",
    "checked": true,
    "id": "c3ed03a289eda82d9ebbae04fe49fc9c6b4b2631",
    "semantic_title": "micl: improving in-context learning through multiple-label words in demonstration",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bHx9ahTdYO": {
    "title": "RevOrder: A Novel Equation Format for Arithmetic Operations in Language Models",
    "volume": "review",
    "abstract": "This paper proposes to understand arithmetic operations in Language Models (LM) by framing them as digit-based reasoning challenges. We introduce a metric called the Count of Sequential Intermediate Digits (CSID), which measures the complexity of arithmetic equations by counting the missing steps in digit reasoning. Our empirical findings suggest that increasing the model size does little to improve the handling of equations with high CSID values. We propose RevOrder, a method that incorporates techniques such as reversing the output order, step-by-step decomposition, and rollback mechanisms to maintain a low CSID, thereby enhancing the solvability of arithmetic equations in LMs. RevOrder also introduces a more compact reasoning process, which reduces the token requirements without affecting the CSID, significantly enhancing token efficiency. Comprehensive testing shows that RevOrder achieves perfect accuracy in operations such as addition, subtraction, and multiplication, and substantially improves performance in division tasks, especially with large numbers where traditional models falter. Additionally, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task led to a significant reduction in equation calculation errors by 46\\% and increased the overall score from 41.6 to 44.4. The data and code can be found at https://anonymous.4open.science/r/RevOrder-D1E1",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rYmiEvm3f0": {
    "title": "An Analysis and Mitigation of the Reversal Curse",
    "volume": "review",
    "abstract": "Recent research observed a noteworthy phenomenon in large language models (LLMs), referred to as the \"reversal curse.\" The reversal curse is that when dealing with two entities, denoted as $a$ and $b$, connected by their relation $R$ and its inverse $R^{-1}$, LLMs excel in handling sequences in the form of \"$aRb$,\" but encounter challenges when processing \"$bR^{-1}a$,\" whether in generation or comprehension. For instance, GPT-4 can accurately respond to the query \"Tom Cruise's mother is?\" with \"Mary Lee Pfeiffer,\" but it struggles to provide a satisfactory answer when asked \"Mary Lee Pfeiffer's son is?\" In this paper, we undertake the first-ever study of how the reversal curse happens in LLMs. Our investigations reveal that the reversal curse can stem from the specific training objectives, which become particularly evident in the widespread use of next-token prediction within most causal language models. We hope this initial investigation can draw more attention to the reversal curse, as well as other underlying limitations in current LLMs",
    "checked": false,
    "id": "55f1cde49846c58b0bedebde15b8f7d939f39432",
    "semantic_title": "are we falling in a middle-intelligence trap? an analysis and mitigation of the reversal curse",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=wG0Xf7DJ2H": {
    "title": "Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages",
    "volume": "review",
    "abstract": "Existing zero-shot cross-lingual NER approaches require substantial prior knowledge of the target language, which is not practical in the case of very low-resource languages. In this paper, we propose a novel approach to NER using phonemic representation based on the International Phonetic Alphabet (IPA) to bridge the gap between representations of different languages. Our experiments show that our method significantly outperforms the baseline models in extremely low-resource languages, particularly demonstrating its robustness with non-latin scripts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=16Fvyqty0u": {
    "title": "Reasoning or a Semblance of it? A Diagnostic Study of Transitive Reasoning in LLMs",
    "volume": "review",
    "abstract": "Evaluating Large Language Models (LLMs) on reasoning benchmarks demonstrates their ability to solve compositional questions. However, little is known of whether these models engage in genuine logical reasoning or simply rely on implicit cues to generate answers. In this paper, we investigate the transitive reasoning capabilities of two distinct LLM architectures, LLaMA 2 and Flan-T5, by manipulating facts within two compositional datasets: QASC and Bamboogle. We controlled for potential cues that might influence the models' performance, including (a) word/phrase overlaps across sections of test input; (b) models' inherent knowledge during pre-training or fine-tuning; and (c) Named Entities. Our findings reveal that while both models leverage (a), Flan-T5 shows more resilience to experiments (b and c), having less variance than LLaMA 2. This suggests that models may develop an understanding of transitivity through fine-tuning on knowingly relevant datasets, a hypothesis we leave to future work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o7GvuFzM0n": {
    "title": "Mitigating Biases to Embracing Diversity: A Comprehensive Annotation Benchmark for Toxic Language",
    "volume": "review",
    "abstract": "This study proposes a prescriptive annotation benchmark grounded in humanities research to enable consistent and reliable offensive language data labeling while mitigating biases against language minorities. We contribute two newly annotated datasets based on the proposed benchmark, leading to higher inter-annotator agreement between human and language model (LLM) annotations compared to original annotations based on descriptive instructions. Experiments show that LLMs could be an alternative when professional annotators are unavailable. Smaller models fine-tuned on a multi-source LLM-annotated dataset outperform models trained on a single, larger human-annotated dataset. The findings demonstrate the effectiveness of structured guidelines in controlling subjective variability while maintaining performance with limited data size and heterogeneous language types, thus embracing language diversity. $\\textbf{Content Warning}$: This article only analyzes offensive language for academic purposes. Discretion is advised",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RFCuycKKcv": {
    "title": "Orchestrating Heterogeneous Architecture for Fast Inference of Mixture-of-Experts Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architectures have shown promising performance on various tasks. However, due to the huge model sizes, running them in resource-constrained environments where GPU memory is not abundant is challenging. Some existing systems propose to use CPU resources to solve that, but they either suffer from significant overhead of frequently moving data between CPU and GPU, or fail to consider different characteristics of CPU and GPU. This paper proposes Twiddler, a resource-efficient inference system for MoE models with limited GPU resources. Twiddler strategically utilizes the heterogeneous computing architecture of CPU and GPU resources by determining the optimal execution strategy. Our evaluation shows that, unlike state-of-the-art systems that optimize for specific scenarios such as single batch inference or long prefill, Twiddler has better performance in all scenarios. Twiddler achieves 1.26 times speed up in single batch inference, 1.30 times in long prefill processing, and 11.57 times in beam search inference, compared against different baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c0VhbnktWV": {
    "title": "Cross-cultural Inspiration Detection and Analysis in Real and LLM-generated Social Media Data",
    "volume": "review",
    "abstract": "Inspiration is linked to various positive outcomes, such as increased creativity, productivity, and happiness. Although inspiration has great potential, there has been limited effort toward identifying content that is inspiring, as opposed to just engaging or positive. Additionally, most research has concentrated on Western data, with little attention paid to other cultures. This work is the first to study cross-cultural inspiration through machine learning methods. We aim to identify and analyze real and AI-generated cross-cultural inspiring posts. To this end, we compile and make publicly available the InspAIred dataset, which consists of 2,000 real inspiring posts, 2,000 real non-inspiring posts, and 2,000 generated inspiring posts evenly distributed across India and the UK. The real posts are sourced from Reddit, while the generated posts are created using the GPT-4 model. Using this dataset, we conduct extensive computational linguistic analyses to (1) compare inspiring content across cultures, (2) compare AI-generated inspiring posts to real inspiring posts, and (3) determine if detection models can accurately distinguish between inspiring content across cultures and data sources",
    "checked": true,
    "id": "188ec977dafd7bca31cd4102c07bd11d0c22416c",
    "semantic_title": "cross-cultural inspiration detection and analysis in real and llm-generated social media data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WmxAerGaoi": {
    "title": "First-Step Advantage: Importance of Starting Right in Multi-Step Math Reasoning",
    "volume": "review",
    "abstract": "Language models can solve complex reasoning tasks better by learning to generate rationales for their predictions. Often these models know how to solve a task but their auto-regressive decoding nature leads to incorrect results if started incorrectly. We observe that smaller models in particular, when corrected, can solve a task that they would otherwise struggle with. We demonstrate this phenomenon by using a larger model to guide smaller models, which leads to significantly improved performance (up to +24 points on the GSM8K dataset by 7B models). To assist smaller models in initiating the starting step, we propose QuestCoT, where a smaller model first *asks how to start* before proceeding with a chain of reasoning. On various multistep mathematical reasoning datasets over multiple smaller models, we show that getting the start right can lead to significant performance gains across all models (gains of up to +6 points on GSM8K, +9 on SVAMP, +5 on ASDiv, and +7 on MultiArith)",
    "checked": true,
    "id": "d60a45ec68c30b88f02f24e43baa9cfdf295109e",
    "semantic_title": "first-step advantage: importance of starting right in multi-step math reasoning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ZDq0AKvpT5": {
    "title": "Misinformation detection with learning from spatial-temporal propagation features and information content on Twitter",
    "volume": "review",
    "abstract": "This study introduces the STU-User model, an innovative approach to detecting misinformation on social media, combining user behaviors with spatial-temporal analysis. The model incorporates an advanced neural network integrating spatial-temporal units (STU) with enhanced long short-term memory (LSTM) structures. Central to its design is the use of Bert-embeddings to analyze the patterns of users' historical interactions and connections in the network and incorporate them with a similarity measure to enhance accuracy. This analysis is then combined with the spatial-temporal aspects of the message propagation structure. We found that the STU-User model surpasses the performances of existing methods based on tests using public Twitter datasets. Theoretical and practical implications for policy-making and regulating social media in the misinformation era are discussed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qzaH3k7zUz": {
    "title": "A Recipe of Parallel Corpora Exploitation for Multilingual Large Language Models",
    "volume": "review",
    "abstract": "Recent studies have highlighted the potential of exploiting parallel corpora to enhance multilingual large language models, improving performance in both bilingual tasks, e.g., machine translation, and general-purpose tasks, e.g., text classification. Building upon these findings, our comprehensive study aims to identify the most effective strategies for leveraging parallel corpora. We investigate the impact of parallel corpora quality and quantity, training objectives, and model size on the performance of multilingual large language models enhanced with parallel corpora across diverse languages and tasks. Our analysis reveals several key insights: (i) filtering noisy translations is essential for effectively exploiting parallel corpora, while language identification and short sentence filtering have little effect; (ii) even a corpus containing just 10K parallel sentences can yield results comparable to those obtained from much larger datasets; (iii) employing only the machine translation objective yields the best results among various training objectives and their combinations; (iv) larger multilingual language models benefit more from parallel corpora than smaller models due to their stronger capacity for cross-task transfer. Our study offers valuable insights into the optimal utilization of parallel corpora to enhance multilingual large language models, extending the generalizability of previous findings from limited languages and tasks to a broader range of scenarios",
    "checked": true,
    "id": "f9468cb201bd3855b3f9a64d40f00d8a163bc629",
    "semantic_title": "a recipe of parallel corpora exploitation for multilingual large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JBWO7EClx4": {
    "title": "Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across various tasks. However, current training approaches combine standard cross-entropy loss with extensive data, human feedback, or ad hoc methods to enhance performance. These solutions are often not scalable or feasible due to their associated costs, complexity, or resource requirements. This study investigates the use of established semantic segmentation loss functions in natural language generation to create a versatile, practical, and scalable solution for fine-tuning different architectures. We evaluate their effectiveness in solving Math Word Problems and question answering across different models of varying sizes. For the analyzed tasks, we found that the traditional Cross-Entropy loss represents a sub-optimal choice, while models trained to minimize alternative (task-dependent) losses, such as Focal or LovÃ¡sz, achieve a mean improvement of +42\\% on exact match without requiring additional data or human feedback. These findings suggest a promising pathway for more efficient and accessible training processes",
    "checked": false,
    "id": "040c3b2104e4b9379498da119cc331252dc450b4",
    "semantic_title": "oh! we freeze: improving quantized knowledge distillation via signal propagation analysis for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=8wI82giPJZ": {
    "title": "XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples",
    "volume": "review",
    "abstract": "Recent studies indicate that leveraging off-the-shelf or fine-tuned retrievers, capable of retrieving relevant in-context examples tailored to the input query, enhances few-shot in-context learning of English. However, adapting these methods to other languages, especially low-resource ones, poses challenges due to the scarcity of cross-lingual retrievers and annotated data. Thus, we introduce XAMPLER: Cross-Lingual Example Retrieval, a method tailored to tackle the challenge of cross-lingual in-context learning \\textbf{using only annotated English data}. \\modelname first trains a retriever based on Glot500, a multilingual small language model, using positive and negative English examples constructed from the predictions of a multilingual large language model, i.e., MaLA500. Leveraging the cross-lingual capacity of the retriever, it can directly retrieve English examples as few-shot examples for in-context learning of target languages. Experiments on the multilingual text classification benchmark SIB200 with 176 languages show that XAMPLER substantially improves the in-context learning performance across languages",
    "checked": true,
    "id": "86b624db374e8ae858312e4a4d08c308564d12d3",
    "semantic_title": "xampler: learning to retrieve cross-lingual in-context examples",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Oax3R1Qmrh": {
    "title": "Evaluating D-MERIT of Partial-annotation on Information Retrieval",
    "volume": "review",
    "abstract": "Retrieval models are often evaluated on partially-annotated datasets. Each query is mapped to a few relevant texts and the remaining corpus is assumed to be irrelevant. As a result, models that successfully retrieve false negatives are punished in evaluation. Unfortunately, completely annotating all texts for every query is not resource efficient. In this work, we show that using partially-annotated datasets in evaluation can paint a distorted picture. We curate D-MERIT, a passage retrieval evaluation set from Wikipedia, aspiring to contain \\emph{all} relevant passages for each query. Queries describe a group (e.g., ``journals about linguistics'') and relevant passages are evidence that entities belong to the group (e.g., a passage indicating that \\textit{Language} is a journal about linguistics). We show that evaluating on a dataset containing annotations for only a subset of the relevant passages might result in misleading ranking of the retrieval systems and that as more relevant texts are included in the evaluation set, the rankings converge. We propose our dataset as a resource for evaluation and our study as a recommendation for balance between resource-efficiency and reliable evaluation when annotating evaluation sets for text retrieval",
    "checked": true,
    "id": "365f9c71eac3da18f42e6361de6746d17b30c081",
    "semantic_title": "evaluating d-merit of partial-annotation on information retrieval",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uft84Ru4rD": {
    "title": "MAiDE-up: Multilingual Deception Detection of GPT-generated Hotel Reviews",
    "volume": "review",
    "abstract": "Deceptive reviews are becoming increasingly common, especially given the increase in performance and the prevalence of LLMs. While work to date has addressed the development of models to differentiate between truthful and deceptive human reviews, much less is known about the distinction between real reviews and AI-authored fake reviews. Moreover, most of the research so far has focused primarily on English, with very little work dedicated to other languages. In this paper, we compile and make publicly available the MAiDE-up dataset, consisting of 10,000 real and 10,000 AI-generated fake hotel reviews, balanced across ten languages. Using this dataset, we conduct extensive linguistic analyses to (1) compare the AI fake hotel reviews to real hotel reviews, and (2) identify the factors that influence the deception detection model performance. We explore the effectiveness of several models for deception detection in hotel reviews across three main dimensions: sentiment, location, and language. We find that these dimensions influence how well we can detect AI-generated fake reviews",
    "checked": true,
    "id": "760fbeddbcef0d58c48d6d1e3c31ffaea761334c",
    "semantic_title": "maide-up: multilingual deception detection of gpt-generated hotel reviews",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=w0vkg53vsV": {
    "title": "MixGR : Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity",
    "volume": "review",
    "abstract": "Recent studies show the growing significance of document retrieval in the generation of LLMs within the scientific domain by bridging their knowledge gap. However, dense retrievers often struggle with domain-specific retrieval and complex query-document relationships, particularly when query segments correspond to various parts of a document. To alleviate such prevalent challenges, this paper introduces $\\texttt{MixGR}$, which improves dense retrievers' awareness of query-document matching across various levels of granularity in queries and documents using a zero-shot approach. $\\texttt{MixGR}$ fuses various metrics based on these granularities to a united score that reflects a comprehensive query-document similarity. Our experiments demonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by 22.6% and 10.4% on nDCG@5 with unsupervised and supervised retrievers, respectively, averaged on queries containing multiple subqueries from four scientific retrieval datasets. Moreover, the efficacy of two downstream scientific question-answering tasks highlights the advantage of $\\texttt{MixGR}$ to boost the application of LLMs in the scientific domain",
    "checked": false,
    "id": "d1df556a0f0cf1090acf88b594e49e30a3cdd1f5",
    "semantic_title": "mixgr: enhancing retriever generalization for scientific domain through complementary granularity",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bxSFccJHUO": {
    "title": "MATE: Meet At The Embedding - Connecting Images with Long Texts",
    "volume": "review",
    "abstract": "While advancements in Vision Language Models (VLMs) have significantly improved the alignment of visual and textual data, these models primarily focus on aligning images with short descriptive captions. This focus limits their ability to handle complex text interactions, particularly with longer texts such as lengthy captions or documents, which have not been extensively explored yet. In this paper, we introduce Meet At The Embedding (MATE), a novel approach that combines the capabilities of VLMs with Large Language Models (LLMs) to overcome this challenge without the need for additional image-long text pairs. Specifically, we replace the text encoder of the VLM with a pretrained LLM-based encoder that excels in understanding long texts. To bridge the gap between VLM and LLM, MATE incorporates a projection module that is trained in a multi-stage manner. It starts by aligning the embeddings from the VLM text encoder with those from the LLM using extensive text pairs. This module is then employed to seamlessly align image embeddings closely with LLM embeddings. We propose two new cross-modal retrieval benchmarks to assess the task of connecting images with long texts (lengthy captions / documents). Extensive experimental results demonstrate that MATE effectively connects images with long texts, uncovering diverse semantic relationships",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ULYnBYISqy": {
    "title": "Extrinsic Evaluation of Cultural Competence in Large Language Models",
    "volume": "review",
    "abstract": "Productive interactions between diverse users and language technologies require outputs from the latter to be culturally relevant and sensitive. Prior works have evaluated models' knowledge of cultural norms, values, and artefacts, without considering how this knowledge manifests in downstream applications. In this work, we focus on extrinsic evaluation of cultural competence in two text generation tasks, open-ended question answering and story generation. We quantitatively and qualitatively evaluate model outputs when an explicit cue of culture, specifically nationality, is perturbed in the prompts. Although we find that model outputs do vary when varying nationalities and feature culturally relevant words, we also find weak correlations between text similarity of outputs for different countries and the cultural values of these countries. Finally, we discuss important considerations in designing comprehensive evaluation of cultural competence in user-facing tasks",
    "checked": true,
    "id": "e672468c6b5c0ce10abf77378ae18a0b58d2cf45",
    "semantic_title": "extrinsic evaluation of cultural competence in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CLVEal3sUI": {
    "title": "Recent Trends in Linear Text Segmentation: A Survey",
    "volume": "review",
    "abstract": "Linear Text Segmentation is the task of automatically tagging text documents with topic shifts, i.e. the places in the text where the topics change. A well-established area of research in Natural Language Processing, drawing from well-understood concepts in linguistic and computational linguistic research, the field has recently seen a lot of interest as a result of the surge of text, video, and audio available on the web, which in turn require ways of summarising and categorizing the mole of content for which linear text segmentation is a fundamental step. In this survey, we provide an extensive overview of current advances in linear text segmentation, describing the state of the art in terms of resources and approaches for the task. Finally, we highlight the limitations of available resources and of the task itself, while indicating ways forward based on the most recent literature and under-explored research directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vnq8vVuLPR": {
    "title": "CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios",
    "volume": "review",
    "abstract": "With the proliferation of Large Language Models (LLMs) in diverse domains, there is a particular need for unified evaluation standards in clinical medical scenarios, where models need to be examined very thoroughly. We present CliMedBench, a comprehensive benchmark with 14 expert-guided core clinical scenarios specifically designed to assess the medical ability of LLMs across 7 pivot dimensions. It comprises 33,735 questions derived from real-world medical reports of top-tier tertiary hospitals and authentic examination exercises. The reliability of this benchmark has been confirmed in several ways. Subsequent experiments with existing LLMs have led to the following findings: (i) Chinese medical LLMs underperform on this benchmark, especially where medical reasoning and factual consistency are vital, underscoring the need for advances in clinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs demonstrate substantial potential in medical clinics, while the limited input capacity of many medical LLMs hinders their practical use. These findings reveal both the strengths and limitations of LLMs in clinical scenarios and offer critical insights for medical research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2iY6CsW5Yp": {
    "title": "Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering",
    "volume": "review",
    "abstract": "Long-form question answering (LFQA) aims to provide thorough and in-depth answers to complex questions, enhancing comprehension. However, such detailed responses are prone to hallucinations and factual inconsistencies, challenging their faithful evaluation. This work introduces HaluQuestQA, the first hallucination dataset with localized error annotations for human-written and model-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 4.7k span-level error annotations for five different error types by expert annotators, along with preference judgments. Using our collected data, we thoroughly analyze the shortcomings of long-form answers and find that they lack comprehensiveness and provide unhelpful references. We train an automatic feedback model on this dataset that predicts error spans with incomplete information and provides associated explanations. Finally, we propose a prompt-based approach, Error-informed refinement, that uses signals from the learned feedback model to refine generated answers, which we show reduces hallucination and improves answer quality. Furthermore, humans find answers generated by our approach comprehensive and highly prefer them (84%) over the baseline answers",
    "checked": true,
    "id": "aa89da2133c1225c9795c3ea4dd2eb61809b8e33",
    "semantic_title": "fine-grained hallucination detection and mitigation in long-form question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HK4sgNcXUp": {
    "title": "EMS-SD: Efficient Multi-sample Speculative Decoding for Accelerating Large Language Models",
    "volume": "review",
    "abstract": "Speculative decoding emerges as a pivotal technique for enhancing the inference speed of Large Language Models (LLMs). Despite recent research aiming to improve prediction efficiency, multi-sample speculative decoding has been overlooked due to varying numbers of accepted tokens within a batch in the verification phase. Vanilla method adds padding tokens in order to ensure that the number of new tokens remains consistent across samples. However, this increases the computational and memory access overhead, thereby reducing the speedup ratio. We propose a novel method that can resolve the issue of inconsistent tokens accepted by different samples without necessitating an increase in memory or computing overhead. Furthermore, our proposed method can handle the situation where the prediction tokens of different samples are inconsistent without the need to add padding tokens. Sufficient experiments demonstrate the efficacy of our method. Our code will be released later",
    "checked": true,
    "id": "2a16a4bb634301c40821e0c1d1987bb7b9e4e9a8",
    "semantic_title": "ems-sd: efficient multi-sample speculative decoding for accelerating large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lPoMt9Pwo5": {
    "title": "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking",
    "volume": "review",
    "abstract": "in zero-shot dialogue state tracking (DST) by enhancing training data diversity through synthetic data generation. Existing DST datasets are severely limited in the number of application domains and slot types they cover due to the high costs of data collection, restricting their adaptability to new domains. This work addresses this challenge with a novel, fully automatic data generation approach that creates synthetic zero-shot DST datasets. Distinguished from previous methods, our approach can generate dialogues across a massive range of application domains, complete with silver-standard dialogue state annotations and slot descriptions. This technique is used to create the D0T dataset for training zero-shot DST models, encompassing an unprecedented 1,000+ domains. Experiments on the MultiWOZ benchmark show that training models on diverse synthetic data improves Joint Goal Accuracy by 6.7\\%, achieving results competitive with models 13.5 times larger than ours",
    "checked": true,
    "id": "9ed1697f3f190788236d6f4eb6bf5830c64a222b",
    "semantic_title": "diverse and effective synthetic data generation for adaptable zero-shot dialogue state tracking",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yb5YhwKWJ4": {
    "title": "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models",
    "volume": "review",
    "abstract": "Knowledge editing is a rising technique for efficiently updating factual knowledge in large language models (LLMs) with minimal alteration of parameters. However, recent studies have identified concerning side effects, such as knowledge distortion and the deterioration of general abilities, that have emerged after editing. This paper conducts a comprehensive study of these side effects, providing a unified view of the challenges associated with knowledge editing in LLMs. We discuss related work and summarize potential research directions to overcome these limitations. Our experiments highlight the limitations of current knowledge editing methods, emphasizing the need for deeper understanding of inner knowledge structures of LLMs and improved knowledge editing methods",
    "checked": true,
    "id": "3827865d55d09bfbc7a0b777cb87a72f6422cfac",
    "semantic_title": "editing the mind of giants: an in-depth exploration of pitfalls of knowledge editing in large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=XupJ7lhMLA": {
    "title": "Investigating the translation capabilities of Large Language Models trained on parallel data only",
    "volume": "review",
    "abstract": "In recent years, Large Language Models (LLMs) have demonstrated exceptional proficiency across a broad spectrum of Natural Language Processing (NLP) tasks, including Machine Translation. However, previous methods predominantly relied on iterative processes such as instruction fine-tuning or continual pre-training, leaving unexplored the challenges of training LLMs solely on parallel data. In this work, we introduce Plume (Parallel Language Model), a collection of three 2B LLMs featuring varying vocabulary sizes (32k, 128k, and 256k) trained exclusively on Catalan-centric parallel examples. These models perform comparably to previous encoder-decoder architectures on 16 supervised translation directions and 56 zero-shot ones. Utilizing this set of models, we conduct a thorough investigation into the translation capabilities of LLMs, probing their performance, the impact of the different elements of the prompt, and their cross-lingual representation space. We will make our models publicly available\\footnote{We release anonymous code at \\url{https://anonymous.4open.science/r/Plume_fork-69D1}}",
    "checked": true,
    "id": "441706881df38147fac80681f79efc40a53227ec",
    "semantic_title": "investigating the translation capabilities of large language models trained on parallel data only",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iIMmumK7bz": {
    "title": "Measuring Progress in Second Language Pronunciation Learning using Automated Assessment Metrics",
    "volume": "review",
    "abstract": "A teaching strategy using repetition has been popular for second language (L2) pronunciation learning. Built upon the strategy, the effectiveness of repetition is known to be enhanced by feedback. This study investigates the effectiveness of repetition with and without feedback as pronunciation learning strategies for Chinese learners of English, utilising multiple automated pronunciation assessment metrics. The use of automatic pronunciation assessment helps avoid the subjectivity of human evaluation, which often shows weak correlations among raters, making automated methods more reliable. A novel corpus, Repetition-based Pronunciation Improvement (RPI), was collected from 50 Chinese learners divided into two groups: repetition only (RPI\\_G1) and repetition with feedback (RPI\\_G2). Eighteen pronunciation assessment metrics, including automatic phone recognition, self-supervised models, and Goodness of Pronunciation (GOP) were used to evaluate learner pronunciations over 12 repetitions of 7 pseudo-words. Results show RPI\\_G2 demonstrated positive learning rates across most metrics, while RPI\\_G1 showed negative learning rates, indicating the importance of feedback for pronunciation improvement. Analysis of the metrics revealed varying levels of consistency and correlation, with self-supervised models showing high correlation",
    "checked": false,
    "id": "135577d422d4e050c9670e7987c118f817981cca",
    "semantic_title": "an ontology and management system for learning outcomes and student mastery",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=58UxkFKQvN": {
    "title": "Comparative Analysis of Acoustic Perception Models in Simulation of Teacher-Learner Interaction in L2 Pronunciation Learning",
    "volume": "review",
    "abstract": "This study presents a comparative analysis of acoustic perception models in simulating teacher-learner interaction for second language (L2) English pronunciation learning, focusing on Chinese native speakers. Three acoustic perception models are evaluated: an English model (M1) based on the XLS-R framework and fine-tuned on the TIMIT corpus, a non-native model (M2) also based on XLS-R but fine-tuned on the L2-ARCTIC corpus, and a Chinese model (M3) using a sequence-to-sequence architecture with connectionist temporal classification (CTC) fine-tuned on the AISHELL-1 corpus. A corpus of seven pseudo-words designed to challenge Chinese learners of English is used to assess the models' performance in capturing the acoustic perception of L2 learners. The Levenshtein distance between recognised sequences and reference sequences for Chinese and English speakers is employed as an evaluation metric, along with the ratio of these distances. Results show that the non-native model (M2) outperforms the English (M1) and Chinese (M3) models in minimising the Levenshtein distance for Chinese speakers and achieves the lowest ratio, indicating its effectiveness in modelling the acoustic perception of L2 learners. These findings suggest that incorporating non-native speech data in acoustic perception models can improve the simulation of teacher-learner interaction in L2 pronunciation learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mPwBL4NkW0": {
    "title": "RWKV-CLIP: A Robust Vision-Language Representation Learner",
    "volume": "review",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) has significantly improved performance in various vision-language tasks by expanding the dataset with image-text pairs obtained from websites. This paper further explores CLIP from the perspectives of data and model architecture. To address the prevalence of noisy data and enhance the quality of large-scale image-text data crawled from the internet, we introduce a diverse description generation framework that can leverage Large Language Models (LLMs) to synthesize and refine content from web-based texts, synthetic captions, and detection tags. Furthermore, we propose RWKV-CLIP, the first RWKV-driven vision-language representation learning model that combines the effective parallel training of transformers with the efficient inference of RNNs. Comprehensive experiments across various model scales and pre-training datasets demonstrate that RWKV-CLIP is a robust and efficient vision-language representation learner; it achieves state-of-the-art performance in several downstream tasks, including linear probe, zero-shot classification, and zero-shot image-text retrieval. To promote the reproducibility of results, we will release pre-processed data, training code, and pre-trained model weights",
    "checked": true,
    "id": "1e8b76a27cc57793c09fff0be151532070f800e7",
    "semantic_title": "rwkv-clip: a robust vision-language representation learner",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=3lpjXd4vMz": {
    "title": "AI-Assisted Human Evaluation of Machine Translation",
    "volume": "review",
    "abstract": "Annually, research teams spend large amounts of money to evaluate the quality of machine translation systems (WMT, Kocmi et al., 2024, inter alia). This is expensive because it requires detailed human labor. The recently proposed annotation protocol, Error Span Annotation (ESA), has annotators highlighting erroneous parts of the translation. In our work, we help the annotators by pre-filling the span annotations with automatic quality estimation. With AI assistance, we obtain more detailed annotations while cutting down the time per span annotation by half (71s/error span $\\rightarrow$ 31s/error span). The biggest advantage of ESA$^\\text{AI}$ protocol is an accurate priming of annotators (pre-filled error spans) before they assign the final score as opposed to starting from scratch. In addition, the annotation budget can be reduced by up to 24% with filtering of examples that the AI deems to be very likely to be correct",
    "checked": true,
    "id": "7e59fe96d896f66abafb6a772b0822a29d2b0568",
    "semantic_title": "ai-assisted human evaluation of machine translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dWdpnlnAum": {
    "title": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models",
    "volume": "review",
    "abstract": "Cognitive dynamics, which refer to the evolution in human cognitive processes, are pivotal to advance human understanding of the world. Recent advancements in large language models (LLMs) highlight their potential for cognitive simulation. However, these LLM-based cognitive studies primarily focus on replicating human cognition in specific contexts, overlooking the inherently dynamic nature of cognition. To bridge this gap, we explore the cognitive dynamics of LLMs and present a corresponding task inspired by longitudinal studies. Toward the task, we develop CogBench, a novel benchmark to assess the cognitive dynamics of LLMs and validate it through participant surveys. We also design two evaluation metrics for CogBench, including Authenticity and Rationality. Recognizing the inherent static nature of LLMs, we further introduce CogGPT for the task, which features an innovative iterative cognitive mechanism to develop lifelong cognitive dynamics. Empirical results demonstrate the superiority of CogGPT over several existing methods, particularly in its ability to facilitate role-specific cognitive dynamics under continuous information flows. We will release the code and data to enable further research",
    "checked": true,
    "id": "c2eeef03f0c0d85237fe64b8da3a44d6170dbf32",
    "semantic_title": "coggpt: unleashing the power of cognitive dynamics on large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=lyCG5ZfQPO": {
    "title": "Leveraging Explicit Reasoning for Inference Integration in Commonsense-Augmented Dialogue Models",
    "volume": "review",
    "abstract": "Open-domain dialogue systems need to grasp social commonsense to understand and respond effectively to human users. Commonsense-augmented dialogue models have been proposed that aim to infer commonsense knowledge from dialogue contexts in order to improve response quality. However, existing approaches to commonsense-augmented dialogue rely on implicit reasoning to integrate commonsense inferences during response generation. In this study, we explore the impact of explicit reasoning against implicit reasoning over commonsense for dialogue response generation. Our findings demonstrate that separating commonsense reasoning into explicit steps for generating, selecting, and integrating commonsense into responses leads to better dialogue interactions, improving naturalness, engagement, specificity, and overall quality. Subsequent analyses of these findings unveil insights into the effectiveness of various types of commonsense in generating responses and the particular response traits enhanced through explicit reasoning for commonsense integration. Our work advances research in open-domain dialogue by achieving a new state-of-the-art in commonsense-augmented response generation",
    "checked": true,
    "id": "f0a0a98e1530c782b78c565e655f4229d301032c",
    "semantic_title": "leveraging explicit reasoning for inference integration in commonsense-augmented dialogue models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ya1c3iGogR": {
    "title": "Bi-DCA: Bi-directional Dual Contrastive Adapting for Alleviating Hallucination in Multimodal Large Language Models",
    "volume": "review",
    "abstract": "Multimodal Large Language Models (MLLMs) demonstrate excellent performance across various multimodal tasks. However, they still tend to generate text with hallucinations in certain scenarios. Previous efforts to alleviate hallucinations approach this issue from fine-tuning, dataset, and inference perspectives. Despite these efforts, there are two existing challenges in MLLMs particularly the confusing image objects and generating persistent hallucinations. In this paper, we propose a novel training-free method called Bi-directional Dual Contrastive Adapting (Bi-DCA) to alleviate the hallucinations in MLLMs that can integrate seamlessly into the existing decoding methods. We first design a bi-directional attention mechanism to expand the visual receptive field to address the problem of confusing image objects. Building on this, to alleviate the persistent hallucinations in generated sentences, we propose a dual contrastive adapting strategy to enhance the positive effect of images during the next token prediction stage. We conduct extensive experiments using various evaluation methods and benchmarks for hallucination. The experimental results demonstrate that our Bi-DCA not only alleviates the above challenges but achieves superior performance compared with previous methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Ark9UioAe": {
    "title": "Revisiting the Othello World Model Hypothesis",
    "volume": "review",
    "abstract": "\\citet{li2023emergent} used the Othello board game as a test case for the ability of GPT2 to induce world models, and were followed up by \\citet{nanda-etal-2023-emergent}. We briefly discuss the original experiments, expanding them to include more language models with more detailed probing. Specifically, we analyze sequences of Othello board states and train the model to predict the next move based on previous moves. We evaluate six language models (GPT2, T5, Bart, Flan-T5, Mistral, and Llama-2) on the Othello task and conclude that these models not only learn to play Othello, but also induce the Othello board layout. We find that all models achieve up to 99\\% accuracy in \\textit{unsupervised} grounding and exhibit high similarity in the board features they learned. This provides much stronger evidence for the Othello World Model Hypothesis than previous works",
    "checked": false,
    "id": "700e55bbc23d90df1bb35cbdf59a632756befbe2",
    "semantic_title": "revisiting lebedev's one-century old experiment",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=gNZ3tMPqx2": {
    "title": "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification",
    "volume": "review",
    "abstract": "Recent Large Vision-Language Models (LVLMs) demonstrate impressive abilities on numerous image understanding and reasoning tasks. The task of fine-grained object classification (e.g., distinction between \\textit{animal species}), however, has been probed insufficiently, despite its downstream importance. We fill this evaluation gap by creating FOCI (\\textbf{F}ine-grained \\textbf{O}bject \\textbf{C}lass\\textbf{I}fication), a difficult multiple-choice benchmark for fine-grained object classification, from existing object classification datasets: (1) multiple-choice avoids ambiguous answers associated with casting classification as open-ended QA task; (2) we retain classification difficulty by mining negative labels with a CLIP model. FOCI complements five popular classification datasets with four domain-specific subsets from ImageNet-21k. We benchmark 12 public LVLMs on \\bench and show that it tests for a \\textit{complementary skill} to established image understanding and reasoning benchmarks. Crucially, CLIP models exhibit dramatically better performance than LVLMs. Since the image encoders of LVLMs come from these CLIP models, this points to inadequate alignment for fine-grained object distinction between the encoder and the LLM and warrants (pre)training data with more fine-grained annotation. We release our code at \\url{ANONYMIZED}",
    "checked": true,
    "id": "10b685acdf642c8c1534515941c5242e514ed270",
    "semantic_title": "african or european swallow? benchmarking large vision-language models for fine-grained object classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H9Weg7vLqE": {
    "title": "Evaluating AI-driven Psychotherapy: Insights from Large Language Models and Human Expert Comparisons",
    "volume": "review",
    "abstract": "The integration of Large Language Models (LLMs), such as GPT-4, has shown great promise in mental health applications for initial assessments based on user-reported symptoms. Traditional assessments often involve subjective evaluations by professional psychologists, leading to inconsistent reproducibility across datasets. To address this, we developed a comprehensive evaluation framework using entropy analysis, keyword frequency analysis, and Latent Dirichlet Allocation (LDA) to quantitatively assess LLM outputs. Our results indicate that LLMs can effectively identify and engage with a range of treatment topics and provide a broader range of treatment opinions than human psychologists. However, LLMs lack depth in their responses, the recommendation generated by LLMs trends to using generalized word instead of using professional words. This study explores the feasibility of LLMs as virtual psychotherapists, highlights their shortcomings in depth, and proposes improved methods for evaluating large model responses. This research provides valuable insights into the potential and challenges of integrating LLMs into mental health practices, paving the way for future research to enhance the effectiveness and reliability of AI-driven therapeutic solutions",
    "checked": false,
    "id": "321a6d509c1f75021f08d9547526674b121c6076",
    "semantic_title": "real estate insights ai: real estate's new roommate â€“ the good, the bad and the algorithmic",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WhGYuOGU2G": {
    "title": "VHASR: A Multimodal Speech Recognition System With Vision Hotwords",
    "volume": "review",
    "abstract": "The image-based multimodal automatic speech recognition (ASR) model enhances speech recognition performance by incorporating audio-related image. However, some works suggest that introducing image information to model does not help improving ASR performance. In this paper, we propose a novel approach effectively utilizing audio-related image information and set up VHASR, a multimodal speech recognition system that uses vision as hotwords to strengthen the model's speech recognition capability. Our system utilizes a dual-stream architecture, which firstly transcribes the text on the two streams separately, and then combines the outputs. We evaluate the proposed model on four datasets: Flickr8k, ADE20k, COCO, and OpenImages. The experimental results show that VHASR can effectively utilize key information in images to enhance the model's speech recognition ability. Its performance not only surpasses unimodal ASR, but also achieves SOTA among existing image-based multimodal ASR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LJqrfGM1f4": {
    "title": "Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval",
    "volume": "review",
    "abstract": "A common retrieve-and-rerank paradigm involves retrieving a broad set of relevant candidates using a fast bi-encoder, followed by applying expensive but accurate cross-encoders to a limited candidate set. However, relying on this small subset is often prone to error propagation from the bi-encoders, restricting the overall performance. To address these issues, we propose the Comparing Multiple Candidates (CMC) framework, which compares a query and multiple candidate embeddings jointly through shallow self-attention layers. While providing contextualized representations, CMC is scalable enough to handle multiple comparisons simultaneously, where comparing 2K candidates takes only twice as long as comparing 100. Practitioners can use CMC as a lightweight and effective reranker to improve top-1 accuracy. Moreover, negligible extra latency through parallelism enables CMC reranking to \\textit{virtually enhance} a neural retriever. Experimental results demonstrate that CMC, virtually enhancing retriever, significantly improves recall@k (+6.7, +3.5\\%-p for R@16, R@64) compared to the first retrieval stage on the ZeSHEL dataset. Also, we conduct experiments for direct reranking on entity, passage, and dialogue ranking. The results indicate that CMC is not only faster (11x) than \\ce\\ but also often more effective, with improved prediction performance in Wikipedia entity linking (+0.7\\%-p) and DSTC7 dialogue ranking (+3.3\\%-p)",
    "checked": true,
    "id": "f20dec2ff4cec8b5aa6c7371317debe14b24a87d",
    "semantic_title": "comparing neighbors together makes it easy: jointly comparing multiple candidates for efficient and effective retrieval",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tjGbJmdRM7": {
    "title": "AdvisorQA: Towards Helpful and Harmless Advice-seeking Question Answering with Collective Intelligence",
    "volume": "review",
    "abstract": "As the integration of large language models into daily life is on the rise, there is still a lack of benchmarks for $\\textit{advising on subjective and personal dilemmas}$. To address this, we introduce AdvisorQA, to assess LLMs' capability in offering advice for deeply personalized concerns, utilizing the LifeProTips Reddit forum. This forum features a dynamic interaction where users post advice-seeking questions, receiving an average of 8.9 advice per query, with 164.2 upvotes from hundreds of users, embodying a $\\textit{collective intelligence}$. Therefore, we've completed a benchmark encompassing daily life questions, diverse corresponding responses, and majority vote ranking to train our helpfulness metric. Baseline experiments with PPO and DPO validate the efficacy of AdvisorQA-trained models through our helpfulness metric, as well as GPT-4 and human evaluations. We also analyze the limitations of each trainer in subjective tasks. AdvisorQA marks a significant leap in enhancing QA systems to provide personalized and empathetic advice, showcasing LLMs' improved understanding of human subjectivity",
    "checked": true,
    "id": "4c10b7de3262cb68e5146f385e6a0a36acb0aba1",
    "semantic_title": "advisorqa: towards helpful and harmless advice-seeking question answering with collective intelligence",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5uy1NxgTwj": {
    "title": "Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models",
    "volume": "review",
    "abstract": "Video-adapted large language models (Video-LLMs) are pivotal for advancing artificial general intelligence (AGI) in video understanding. Despite progress, existing methods rarely undergo comprehensive assessment from an AGI construction perspective. We propose that an ideal video intelligence model should possess three essential abilities: (i) Video-exclusive Understanding, crucial for tasks like event summarization where direct video content analysis is paramount; (ii) Prior Knowledge-based Question-Answering, essential for applications needing contextual insights such as in-depth sports analysis or cultural understanding in music videos and television shows; (iii) Comprehension and Decision-making, vital for predictive tasks in complex environments like 3D scene navigation or autonomous vehicle guidance. To systematically evaluate these abilities, we introduce \\textit{Video-Bench}, an ability-oriented benchmark encompassing real-world video data and meticulously designed QA pairs, accompanied by an automated evaluation toolkit. Our analysis of 8 leading Video-LLMs show a significant gap in achieving human-like video understanding, underscoring the need for advancements in video comprehension AGI",
    "checked": true,
    "id": "b037bb09aa162d8a543e64ec777ca0edc732d2af",
    "semantic_title": "video-bench: a comprehensive benchmark and toolkit for evaluating video-based large language models",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=e9TrT7LGnK": {
    "title": "Can Automatic Metrics Assess High-Quality Translations?",
    "volume": "review",
    "abstract": "Automatic metrics for evaluating translation quality are typically validated by measuring how well they correlate with human assessments. However, correlation methods tend to capture only the ability of metrics to differentiate between good and bad source-translation pairs, overlooking their reliability in distinguishing alternative translations for the same source. In this paper, we confirm that this is indeed the case by showing that current metrics are insensitive to nuanced differences in translation quality. This effect is most pronounced when the quality is high and the variance among alternatives is low. Given this finding, we shift towards detecting high-quality correct translations, an important problem in practical decision-making scenarios where a binary check of correctness is prioritized over a nuanced evaluation of quality. Using the MQM framework as the gold standard, we systematically stress-test the ability of current metrics to identify translations with no errors as marked by humans. Our findings reveal that current metrics often over or underestimate translation quality, indicating significant room for improvement in machine translation evaluation",
    "checked": true,
    "id": "de1c034f8ee77c9f23f4e0d26d7e2520b67f6017",
    "semantic_title": "can automatic metrics assess high-quality translations?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WRXiff8Vw3": {
    "title": "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated exceptional abilities across various domains. However, utilizing LLMs for ubiquitous sensing applications remains challenging as existing text-prompt methods show significant performance degradation when handling long sensor data sequences. In this paper, we propose a visual prompting approach for sensor data using multimodal LLMs (MLLMs). Specifically, we design a visual prompt that directs MLLMs to utilize visualized sensor data alongside descriptions of the target sensory task. Additionally, we introduce a visualization generator that automates the creation of optimal visualizations tailored to a given sensory task, eliminating the need for prior task-specific knowledge. We evaluated our approach on nine sensory tasks involving four sensing modalities, achieving an average of 10% higher accuracy compared to text-based prompts and reducing token costs by 15.8 times. Our findings highlight the effectiveness and cost-efficiency of using visual prompts with MLLMs for various sensory tasks",
    "checked": true,
    "id": "d1a14ef92761331303db78ff2dd0b41cef4c6d8a",
    "semantic_title": "by my eyes: grounding multimodal large language models with sensor data via visual prompting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IHLwnwSzhl": {
    "title": "Attribute or Abstain: Large Language Models as Long Document Assistants",
    "volume": "review",
    "abstract": "LLMs can help humans working with long documents, but are known to hallucinate. *Attribution* can increase trust in LLM responses: The LLM provides evidence that supports its response, which enhances verifiability. Existing approaches to attribution have only been evaluated in RAG settings, where the initial retrieval confounds LLM performance. This is crucially different from the long document setting, where retrieval is not needed, but could help. Thus, a long document specific evaluation of attribution is missing. To fill this gap, we present LAB, a benchmark of 6 diverse long document tasks with attribution, and experiment with different approaches to attribution on 4 LLMs of different sizes, both prompted and fine-tuned. We find that *citation*, i.e. response generation and evidence extraction in one step, mostly performs best. We investigate whether the ``Lost in the Middle'' phenomenon exists for attribution, but do not find this. We also find that evidence quality can predict response quality on datasets with simple responses, but not so for complex responses, as models struggle with providing evidence for complex claims. We release code and data for further investigation. [Link](https://figshare.com/s/0ade22518ac6636550fe)",
    "checked": true,
    "id": "06b3fadf49b2cf9927f8f56539c9cb6e99ea6a24",
    "semantic_title": "attribute or abstain: large language models as long document assistants",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6LvWYaSzK0": {
    "title": "Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models",
    "volume": "review",
    "abstract": "The disconnect between tokenizer creation and model training in language models has been known to allow for certain inputs, such as the infamous SolidGoldMagikarp token, to induce unwanted behaviour. Although such `glitch tokens' that are present in the tokenizer vocabulary, but are nearly or fully absent in training, have been observed across a variety of different models, a consistent way of identifying them has been missing. We present a comprehensive analysis of Large Language Model tokenizers, specifically targeting this issue of detecting under-trained tokens. Through a combination of tokenizer analysis, model weight-based indicators, and prompting techniques, we develop effective methods for automatically detecting these problematic tokens. Our findings demonstrate the prevalence of such tokens across various models and provide insights into improving the efficiency and safety of language models",
    "checked": true,
    "id": "e6ffa22c9f53c7dc97da365bfe868951ff7ddf42",
    "semantic_title": "fishing for magikarp: automatically detecting under-trained tokens in large language models",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=JskM2kZANS": {
    "title": "Leave the Bias in Bias: Mitigating the Label Noise Effects in Continual Visual Instruction Fine-Tuning",
    "volume": "review",
    "abstract": "In recent years, multimodal large language models (MLLMs) with vision processing capability have shown substantial advancements, excelling particularly in interpreting general images. Their application in domain-specific tasks, like those in the medical fields, is further enhanced through continuous visual instruction fine-tuning (CVIF). Despite these advancements, a significant challenge arises from label noise encountered during the collection of domain-specific data. Our studies reveal that this label noise can adversely affect the learning of vision projection embeddings and contribute to inaccuracies in LLMs' fine-tuning, often leading to hallucinations. In this paper, we introduce a novel framework designed to minimize the impact of label noise. Our approach focuses on stabilizing the learning of vision embeddings and reducing the effect of label noise through the inherent semantic understanding of uncertainty in LLMs. Extensive experiments demonstrate that our framework maintains robust performance in general visual question-answer (VQA) tasks while showing significant effectiveness in medical VQA tasks. To the best of our knowledge, this is the first study to specifically address and analyze the impact of label noise in CVIF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0AcjNkohUl": {
    "title": "Multi-modal Knowledge Graphs: Evolution, Methods, and Opportunities",
    "volume": "review",
    "abstract": "Knowledge Graphs (KGs) are pivotal in advancing AI applications, and their extension into multi-modal dimensions (i.e., MMKGs) is opening new avenues for innovation. This survey systematically defines MMKGs, charts their construction progress, and analyzes existing MMKG-related tasks. We provide detailed task definitions, evaluation benchmarks, and insights into significant breakthroughs, while also discussing current challenges and highlighting emerging trends in the field",
    "checked": false,
    "id": "05cdfa832e154559bc19101a5dc8623beeff2308",
    "semantic_title": "multi-modal graph learning over umls knowledge graphs",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=jsE0gw4zub": {
    "title": "InferAct: Inferring Safe Actions for LLMs-Based Agents Through Preemptive Evaluation and Human Feedback",
    "volume": "review",
    "abstract": "A crucial requirement for deploying LLM-based agents in real-life applications is the robustness against risky or even irreversible mistakes. However, the existing research lacks a focus on preemptive evaluation of reasoning trajectories performed by LLM agents, leading to a gap in ensuring safe and reliable operations. To explore better solutions, this paper introduces $\\texttt{InferAct}$, a novel critic that leverages the Theory-of-Mind capability of LLMs to proactively detect potential errors before critical actions are executed (e.g., $\\textit{`buy-now'}$ in automatic online trading or web shopping). $\\texttt{InferAct}$ is also capable of integrating human feedback to prevent irreversible risks as well as enhance the actor agent's decision-making process. Experiments on three widely-used tasks demonstrate the effectiveness of $\\texttt{InferAct}$. The proposed solution presents a novel approach and concrete contributions toward developing LLM agents that can be safely deployed in different environments involving critical decision-making",
    "checked": false,
    "id": "b5abfb8d320abbce7afe9518dc7d91ec1d13b142",
    "semantic_title": "inferact: inferring safe actions for llm-based agents through preemptive evaluation and human feedback",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xC8KTvexaJ": {
    "title": "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination",
    "volume": "review",
    "abstract": "This paper presents a series of investigations into an interesting phenomenon where we observe performance increases in large language models (LLMs) when providing a prompt that causes and exploits hallucination. We propose null-shot prompting, a counter-intuitive approach where we intentionally instruct LLMs to look at and utilize information from a null section. We investigate null-shot prompting on a wide range of tasks, including arithmetic reasoning, commonsense reasoning, and reading comprehension. We observe a substantial increase in performance in arithmetic reasoning tasks for various models, with up to a 44.62% increase compared to a baseline in one model. Therefore, we investigate deeper into this task by utilizing a more challenging mathematics problem-solving benchmark. We observe that LLMs benefit from hallucination in null-shot prompting in this task and discuss the mathematical topics that benefit the most from introducing hallucination in the prompt. We continue our investigation by evaluating hallucination detection abilities of the LLMs when using null-shot prompting. We find surprising results where hallucination in prompts can improve hallucination detection abilities of many LLMs. We also examine the effects of introducing both reasoning, which is known to mitigate hallucination, and hallucination simultaneously in the prompt and observe another surprising turn for the mathematics problem-solving benchmark with many performance improvements. We hope this paper will spark more interest, investigations, and discussions on how hallucination in prompts LLMs and even bolsters them in certain cases",
    "checked": false,
    "id": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4",
    "semantic_title": "gpt-ner: named entity recognition via large language models",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=950A7ddNc9": {
    "title": "Unveiling the Misuse Potential of Base Large Language Models via In-Context Learning",
    "volume": "review",
    "abstract": "The open-sourcing of large language models (LLMs) accelerates application development, innovation, and scientific progress. This includes both base models, which are pre-trained on extensive datasets without alignment, and aligned models, deliberately designed to align with ethical standards and human values. Contrary to the prevalent assumption that the inherent instruction-following limitations of base LLMs serve as a safeguard against misuse, our investigation exposes a critical oversight in this belief. By deploying carefully designed demonstrations, our research demonstrates that base LLMs could effectively interpret and execute malicious instructions. To systematically assess these risks, we introduce a novel set of risk evaluation metrics. Empirical results reveal that the outputs from base LLMs can exhibit risk levels on par with those of models fine-tuned for malicious purposes. This vulnerability, requiring neither specialized knowledge nor training, can be manipulated by almost anyone, highlighting the substantial risk and the critical need for immediate attention to the base LLMs' security protocols",
    "checked": true,
    "id": "388de598bb3609fb793c0bd26c365e98139f00ce",
    "semantic_title": "unveiling the misuse potential of base large language models via in-context learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=8K6BkGtMB3": {
    "title": "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale",
    "volume": "review",
    "abstract": "In recent years, Large Language Models (LLMs) have made significant strides towards Artificial General Intelligence. However, training these models from scratch requires substantial computational resources and vast amounts of text data. In this paper, we explores an alternative approach to constructing a LLM for a new language by continually pre-training (CPT) from existing pre-trained LLMs, instead of using randomly initialized parameters. Based on parallel experiments on 40 model sizes ranging from 40M to 5B parameters, we find that 1) CPT converges faster and saves significant resources in a scalable manner. 2) CPT adheres to an extended scaling law derived from with a joint data-parameter scaling term. 3) The compute-optimal data-parameter allocation for CPT markedly differs based on our estimated scaling factors. 4) The effectiveness of transfer scale is influenced by training duration and linguistic properties, while robust to data replaying, a method that effectively mitigates catastrophic forgetting in CPT. We hope our findings provide deeper insights into the transferability of LLMs at scale for the research community",
    "checked": true,
    "id": "b742c177af2dc80bb7b9d13fabf594662ff81862",
    "semantic_title": "breaking language barriers: cross-lingual continual pre-training at scale",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvI9K24ttS": {
    "title": "CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems",
    "volume": "review",
    "abstract": "Conversational explainable artificial intelligence (ConvXAI) systems based on large language models (LLMs) have garnered significant interest from the research community in natural language processing (NLP) and human-computer interaction (HCI). Such systems can provide answers to user questions about explanations in dialogues, have the potential to enhance users' comprehension and offer more information about the decision-making and generation processes of LLMs. Currently available ConvXAI systems are based on intent recognition rather than free chat, as this has been found to be more precise and reliable in identifying users' intentions. However, the recognition of intents still presents a challenge in the case of ConvXAI, since little training data exist and the domain is highly specific, as there is a broad range of XAI methods to map requests onto. In order to bridge this gap, we present CoXQL, the first dataset for user intent recognition in ConvXAI, covering 31 intents, seven of which require filling multiple slots. Subsequently, we enhance an existing parsing approach by incorporating template validations, and conduct an evaluation of several LLMs on CoXQL using different parsing strategies. We conclude that the improved parsing approach (MP+) surpasses the performance of previous approaches. We also discover that intents with multiple slots remain highly challenging for LLMs",
    "checked": true,
    "id": "7f525a54f2b990f803b7e3309d3c9e416619a12b",
    "semantic_title": "coxql: a dataset for parsing explanation requests in conversational xai systems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4jLDSb8BMs": {
    "title": "Fine Irony: Training Transformers with Ordinal Likelihood Labels",
    "volume": "review",
    "abstract": "In this paper, we investigate a selection of methodologies for fine-tuning transformer-based classifiers for fine-grained irony (likelihood) detection in English Tweets. These methodologies include approaching irony detection as an ordinal classification task and as a regression task for varying label granularity (3, 5 and 7 labels). Our experiments show that training irony detection models using fine-grained likelihood labels is not only possible but also advantageous, as the models reach higher F1-scores for binary classification than models that are specifically trained for this task. In addition, we explore how well the predictions by each of the model setups can be interpreted through Layer Integrated Gradients. The results show that, although performance for irony detection is consistent, the selection of important words for well-performing models does not consistently align with human trigger word annotation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vsRU4v83B0": {
    "title": "SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "Recent studies in Retrieval-Augmented Generation (RAG) have investigated extracting evidence from retrieved passages to reduce computational costs and enhance the final RAG performance, yet it remains challenging. Existing methods heavily rely on data-level augmentation, encountering several issues: (1) Poor generalization due to hand-crafted context filtering; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-level evidence extraction learning framework, SEER, optimizing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG performance, enhances the faithfulness, helpfulness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DgZWp1VqPN": {
    "title": "Word Alignment as Preference for Machine Translation",
    "volume": "review",
    "abstract": "The problem of hallucination and omission, a long-standing problem in machine translation (MT), is more pronounced when a large language model (LLM) is used in MT because an LLM itself is susceptible to these phenomena. In this work, we mitigate the problem in an LLM-based MT model by guiding it to better word alignment. We first study the correlation between word alignment and the phenomena of hallucination and omission in MT. Then we propose to utilize word alignment as preference to optimize the LLM-based MT model. The preference data are constructed by selecting chosen and rejected translations from multiple MT tools. Subsequently, direct preference optimization is used to optimize the LLM-based model towards the preference signal. Given the absence of evaluators specifically designed for hallucination and omission in MT, we further propose selecting hard instances and utilizing GPT-4 to directly evaluate the performance of the models in mitigating these issues. We verify the rationality of these designed evaluation methods by experiments, followed by extensive results demonstrating the effectiveness of word alignment-based preference optimization to mitigate hallucination and omission",
    "checked": true,
    "id": "5f39f57b175a27930dfe26aafc224dab8f34f8fc",
    "semantic_title": "word alignment as preference for machine translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy4brfxJvO": {
    "title": "PE: A Poincare Explanation Method for Fast Text Hierarchy Generation",
    "volume": "review",
    "abstract": "The black-box nature of deep learning models in NLP hinders their widespread application. The research focus has shifted to Hierarchical Attribution (HA) for its ability to model feature interactions. Recent works model non-contiguous combinations with a time-costly greedy search in Eculidean spaces, neglecting underlying linguistic information in feature representations. In this work, we introduce a novel method, namely Poincare Explanation (PE), for modeling feature interactions with hyperbolic spaces in a time efficient manner. Specifically, we take building text hierarchies as finding spanning trees in hyperbolic spaces. First we project the embeddings into hyperbolic spaces to elicit inherit semantic and syntax hierarchical structures. Then we propose a simple yet effective strategy to calculate Shapley score. Finally we build the the hierarchy with proving the constructing process in the projected space could be viewed as building a minimum spanning tree and introduce a time efficient building algorithm. Experimental results demonstrate the effectiveness of our approach. Our code is available at https://anonymous.4open.science/r/PE-747B",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vSx6PPm3wG": {
    "title": "A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns Well with The Key Tokens",
    "volume": "review",
    "abstract": "Text embeddings from large language models (LLMs) have achieved excellent results in tasks such as information retrieval, semantic textual similarity, etc. In this work, we show an interesting finding: when feeding a text into the embedding LLMs, the obtained text embedding will be able to be aligned with the key tokens in the input text. We first fully analyze this phenomenon on eight embedding LLMs and show that this phenomenon is universal and is not affected by model architecture, training strategy, and embedding method. With a deeper analysis, we then find that the main change in embedding space between the embedding LLMs and their original generative LLMs is in the first principal component. By adjusting the first principal component, we can align text embedding with the key tokens. Finally, we give several examples to demonstrate the vast application potential of this finding: (1) we propose a simple and practical sparse retrieval method based on the aligned tokens, which can achieve 80\\% of the dense retrieval effect of the same model while reducing the computation significantly; (2) we show that our findings provide a fresh perspective to help understand fuzzy concepts (e.g., semantic relatedness vs. semantic similarity) and emerging technologies (e.g., instruction following Embedding) in this field",
    "checked": true,
    "id": "0340c278723e35e03189f41da1d1c48befa189bb",
    "semantic_title": "a text is worth several tokens: text embedding from llms secretly aligns well with the key tokens",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1gclnHeyN8": {
    "title": "CW3NE:A Genre-oriented Corpus for Nested Named Entity Recognition in Chinese Web Novels",
    "volume": "review",
    "abstract": "Named entities are important to understand literary works, which emphasize characters, plots and environment. The research on named entity recognition (NER), especially nested named entity recognition in literary domain is still insufficient partly due to lack of enough annotated data. To address this issue, we construct the first Genre-oriented Corpus for $\\textbf{N}$ested $\\textbf{N}$amed $\\textbf{E}$ntity Recognition in $\\textbf{C}$hinese $\\textbf{W}$eb $\\textbf{N}$ovels, namely $\\textbf{CW3NE}$, comprising 400 chapters totaling 1,214,283 tokens under two genres, XuanHuan (Eastern Fantasy) and History. Based on the corpus, we make a deep analysis of the distribution of different types of entities, including person, location and organization. We also make comparison of nesting patterns of nested entities between CW3NE and the English corpus LitBank. Even both belong to literary domain, entities in different genres share few overlaps, making genre adaptation of NER a hard problem. We provide several baseline NER methods and experimental results show that large language model based methods perform poorer than well designed small language model based method. Performance drops sharply on nested NER for all baseline methods, indicating the great challenge posed by the nested named entities. Genre adaptation also results in great performance drop especially on location and organization entities. We will release our corpus to promote research on literary NER",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gzBXGu5wHf": {
    "title": "Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have been demonstrated to generate illegal or unethical responses, particularly when subjected to \"jailbreak.\" Research on jailbreak has highlighted the safety issues of LLMs. However, prior studies have predominantly focused on single-turn dialogue, ignoring the potential complexities and risks presented by multi-turn dialogue, a crucial mode through which humans derive information from LLMs. In this paper, we argue that humans could exploit multi-turn dialogue to induce LLMs into generating harmful information. LLMs may not intend to reject cautionary or borderline unsafe queries, even if each turn is closely served for one malicious purpose in a multi-turn dialogue. Therefore, by decomposing an unsafe query into several sub-queries for multi-turn dialogue, we induced LLMs to answer harmful sub-questions incrementally, culminating in an overall harmful response. Our experiments, conducted across a wide range of LLMs, indicate current inadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our findings expose vulnerabilities of LLMs in complex scenarios involving multi-turn dialogue, presenting new challenges for the safety of LLMs",
    "checked": true,
    "id": "db68cc363587bf82acf5a373b68bbf8a6bc11ac9",
    "semantic_title": "speak out of turn: safety vulnerability of large language models in multi-turn dialogue",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=qofNwM4E0w": {
    "title": "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages",
    "volume": "review",
    "abstract": "Southeast Asia (SEA) is a region rich in linguistic diversity and cultural variety, with over 1,300 indigenous languages and a population of 671 million people. However, prevailing AI models suffer from a significant lack of representation of texts, images, and audio datasets from SEA, compromising the quality of AI models for SEA languages. Evaluating models for SEA languages is challenging due to the scarcity of high-quality datasets, compounded by the dominance of English training data, raising concerns about potential cultural misrepresentation. To address these challenges, through a collaborative movement, we introduce SEACrowd, a comprehensive resource center that fills the resource gap by providing standardized corpora in nearly 1,000 SEA languages across three modalities. Through our SEACrowd benchmarks, we assess the quality of AI models on 36 indigenous languages across 13 tasks, offering valuable insights into the current AI landscape in SEA. Furthermore, we propose strategies to facilitate greater AI advancements, maximizing potential utility and resource equity for the future of AI in SEA",
    "checked": true,
    "id": "ba5284674face6cca3b678fd7a82d691ec29349b",
    "semantic_title": "seacrowd: a multilingual multimodal data hub and benchmark suite for southeast asian languages",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=JJi9Zns9c6": {
    "title": "Self-VEQA Agent Self-Verification Enhanced Question Answering Agent",
    "volume": "review",
    "abstract": "The task of Knowledge Graph Question Answering (KGQA) involves using information stored in a knowledge graph (KG) to answer questions by identifying the relation path between the subject entity and the answer. Traditional KGQA methods require extensive training data and are time-consuming. Recent advancements in Large Language Models (LLMs) have shown potential in various tasks. However, methods leveraging LLMs for KGQA face challenges such as inference errors and excessive reliance on prompt design. To address these issues, we propose the Self-VEQA Agent, which utilizes two agents: a QA Agent for initial answers based on KG and a Verification Agent to iteratively refine these answers, improving accuracy over time. Additionally, our model features a memory mechanism that enables dynamic evolution. As the Self-VEQA Agent performs tasks and accumulates experience, the overall performance improves over time. Evaluated on two KGQA benchmarks, Self-VEQA Agent outperforms most traditional and LLM-based methods, demonstrating its effectiveness",
    "checked": false,
    "id": "a1bff82edcec999dbb5200a0d8f37ab992f26cba",
    "semantic_title": "aupple: augmented physical priors through language enhancement using self-supervised learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9SkMEG7I6V": {
    "title": "Dynamic Entity Alignment with Attribute Integration",
    "volume": "review",
    "abstract": "Entity alignment is a key technology for integrating knowledge graphs (KG). However, existing methods assume KG is static and overlook the fact that KG evolves over time. With the growth of KG, previous alignment results may require adjustments, and new equivalent entities need to be found for the newly added entities. Additionally, new entities often have fewer neighbors, which adds difficulty to their alignment. In this paper, we propose DEA-AttrAlign to address these challenges. The core idea is to quickly generate representations for entities based on their neighborhoods. In cases where neighborhood information for new entities is lacking, we propose utilizing entity attributes and semantic facts derived from triplets as additional alignment information. Extensive experiments show that our approach is more effective than methods based on retraining or inductive learning",
    "checked": false,
    "id": "2c7fc7c9eb1b72bc35838b7a092ec5f12366269e",
    "semantic_title": "person entity alignment method based on multimodal information aggregation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=VQ2aOIolZh": {
    "title": "A Survey of Large Language Models Attribution",
    "volume": "review",
    "abstract": "Open-domain generative systems have gained significant attention in the field of conversational AI (e.g., generative search engines). In this paper, we present a comprehensive review of the attribution mechanisms employed by these systems, particularly with large language models. While attribution or citation improves factuality and verifiability, issues like ambiguous knowledge reservoirs, inherent biases, and the drawbacks of excessive attribution can hinder the effectiveness of these systems. The purpose of this survey is to provide valuable implications for researchers, helping in the refinement of attribution methodologies to improve the reliability and veracity of responses generated by open-domain generative systems. We believe that this field is still in its early stages; therefore, we maintain a repository to keep track of ongoing studies at \\url{AnonymousURL}",
    "checked": true,
    "id": "5d3bdae6cfb2239af70dfe8ae5cf1a4958330ac4",
    "semantic_title": "a survey of large language models attribution",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=ZtVl0jCbnL": {
    "title": "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment",
    "volume": "review",
    "abstract": "As large vision-language models (LVLMs) evolve rapidly, the demand for high-quality and diverse data to align these models becomes increasingly crucial. However, the creation of such data with human supervision proves costly and time-intensive. In this paper, we investigate the efficacy of AI feedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the first large-scale vision-language feedback dataset, comprising over 82K multi-modal instructions and comprehensive rationales generated by off-the-shelf models without human annotations. To evaluate the effectiveness of AI feedback for vision-language alignment, we train Silkie, an LVLM fine-tuned via direct preference optimization on VLFeedback. Silkie showcases exceptional performance regarding helpfulness, visual faithfulness, and safety metrics. It outperforms its base model by 6.9% and 9.5% in perception and cognition tasks, reduces hallucination issues on MMHal-Bench, and exhibits enhanced resilience against red-teaming attacks. Furthermore, our analysis underscores the advantage of AI feedback, particularly in fostering preference diversity to deliver more comprehensive improvements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NhMGDXTDwH": {
    "title": "ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation",
    "volume": "review",
    "abstract": "This study introduces an approach to optimize Parameter Efficient Fine Tuning (PEFT) for Pretrained Language Models (PLMs) by implementing a Shared Low Rank Adaptation (ShareLoRA). By strategically deploying ShareLoRA across different layers and adapting it for the Query, Key, and Value components of self-attention layers, we achieve a substantial reduction in the number of training parameters and memory usage. Importantly, ShareLoRA not only maintains model performance but also exhibits robustness in both classification and generation tasks across a variety of models, including RoBERTa, GPT-2, LLaMA and LLaMA2. It demonstrates superior transfer learning capabilities compared to standard LoRA applications and mitigates overfitting by sharing weights across layers. Our findings affirm that ShareLoRA effectively boosts parameter efficiency while ensuring scalable and high-quality performance across different language model architectures",
    "checked": true,
    "id": "0acc62dc2cf996a9fb0acb4cc08965f7d8059c19",
    "semantic_title": "sharelora: parameter efficient and robust large language model fine-tuning via shared low-rank adaptation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y6Ene4t5Tx": {
    "title": "Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension",
    "volume": "review",
    "abstract": "Machine Reading Comprehension (MRC) poses a significant challenge in the field of Natural Language Processing (NLP). While mainstream MRC methods predominantly leverage extractive strategies using encoder-only models such as BERT, generative approaches face the issue of $\\textit{out-of-control generation}$ -- a critical problem where answers generated are often incorrect, irrelevant, or unfaithful to the source text. To address these limitations in generative models for extractive MRC, we introduce the $\\textbf{Q}$uestion-$\\textbf{A}$ttended $\\textbf{S}$pan $\\textbf{E}$xtraction ($\\textit{QASE}$) module. Integrated during the fine-tuning phase of pre-trained generative language models (PLMs), $\\textit{QASE}$ significantly enhances their performance, allowing them to surpass the extractive capabilities of advanced Large Language Models (LLMs) such as GPT-4 in few-shot settings. Notably, these gains in performance do not come with an increase in computational demands. The efficacy of the $\\textit{QASE}$ module has been rigorously tested across various datasets, consistently achieving or even surpassing state-of-the-art (SOTA) results, thereby bridging the gap between generative and extractive models in extractive MRC tasks. Our code is available at this anonymous repo link: https://anonymous.4open.science/r/QASE-7753/README.md",
    "checked": true,
    "id": "03bd4cef88f02f7c0bdd87e36643b97fef4622f1",
    "semantic_title": "enhancing pre-trained generative language models with question attended span extraction on machine reading comprehension",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gEMLMMG0m9": {
    "title": "Eliminating Positional Bias in LLMs via Attention Weight Averaging",
    "volume": "review",
    "abstract": "Positional bias in LLMs means that changing the order of input sentences leads to semantic inconsistency in the output. Positional bias occurs even though the overall meaning of the input remains the same. Recent studies have observed and verified that positional bias is prevalent across various LLMs and tasks. Our study proposes Average Attention Infer module, which starts from the calculation of the attention mechanism and aims to reduce positional bias by computing the average attention weight of different arrangements. We design experiments to verify the module's effectiveness in mitigating positional bias. It is also verified that the LLMs can still maintain their language functions after debiasing, which makes our module easy to extend to other tasks. Methods for selecting layers and permutations are provided to accelerate the module's computation further. We release the code and hope this research can inspire the design and research of a new generation of attention modules, thereby contributing to the fundamental elimination of positional bias",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=17a5MFji8L": {
    "title": "Making Task-Oriented Dialogue Datasets More Natural by Synthetically Generating Indirect User Requests",
    "volume": "review",
    "abstract": "- Indirect User Requests (IURs), such as \"It's cold in here\" instead of \"Could you please increase the temperature?\" are common in human-human task-oriented dialogue and require world knowledge and pragmatic reasoning from the listener. While large language models (LLMs) can handle these requests effectively, smaller models deployed on virtual assistants often struggle due to resource constraints. Moreover, existing task-oriented dialogue benchmarks lack sufficient examples of complex discourse phenomena such as indirectness. To address this, we propose a set of linguistic criteria along with an LLM-based pipeline for generating realistic IURs to test natural language understanding (NLU) and dialogue state tracking (DST) models before deployment in a new domain. We also release IndirectRequests, a dataset of IURs based on the Schema Guided Dialog (SGD) corpus, as a comparative testbed for evaluating the performance of smaller models in handling indirect requests",
    "checked": true,
    "id": "ffb69b5febe4afc1817377f717b62b69b9d3659f",
    "semantic_title": "making task-oriented dialogue datasets more natural by synthetically generating indirect user requests",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HccN11PPRB": {
    "title": "ManiTweet: A New Benchmark for Identifying Manipulation of News on Social Media",
    "volume": "review",
    "abstract": "Considerable advancements have been made to tackle the misrepresentation of information derived from reference articles in the domains of fact-checking and faithful summarization. However, an unaddressed aspect remains - the identification of social media posts that manipulate information presented within associated news articles. This task presents a significant challenge, primarily due to the prevalence of personal opinions in such posts. We present a novel task, identifying manipulation of news on social media, which aims to detect manipulation in social media posts. To study this task, we have proposed a data collection schema and curated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and corresponding articles. Our analysis demonstrates that this task is highly challenging, with large language models (LLMs) yielding unsatisfactory performance. Additionally, we have developed a simple yet effective framework that outperforms LLMs significantly on the ManiTweet dataset. Finally, we have conducted an exploratory analysis of human-written tweets, unveiling intriguing connections between manipulation and factuality of news articles",
    "checked": true,
    "id": "122dbf4c968a448fb2dde6c5c4a10f2697364531",
    "semantic_title": "manitweet: a new benchmark for identifying manipulation of news on social media",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PPh5KtX8If": {
    "title": "Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation",
    "volume": "review",
    "abstract": "Domain experts across engineering, healthcare, and education follow strict standards for producing quality content such as technical manuals, medication instructions, and children's reading materials. However, current works in controllable text generation have yet to explore using these standards as references for control. Towards this end, we introduce Standardize, a retrieval-style in-context learning-based framework to guide large language models to align with expert-defined standards. Focusing on English language standards in the education domain as a use case, we consider the Common European Framework of Reference for Languages (CEFR) and Common Core Standards (CCS) for the task of open-ended content generation. Our findings show that models can gain 45% to 100% increase in precise accuracy across open and commercial LLMs evaluated, demonstrating that the use of knowledge artifacts extracted from standards and integrating them in the generation process can effectively guide models to produce better standard-aligned content",
    "checked": true,
    "id": "d3f52ab6abc86b269380fbfd8b2a77b69013af3f",
    "semantic_title": "standardize: aligning language models with expert-defined standards for content generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EDCOgqo4dK": {
    "title": "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models",
    "volume": "review",
    "abstract": "Large vision-language models (LVLMs) hallucinate: certain context cues in an image may trigger the language module's overconfident and incorrect reasoning on abnormal or hypothetical objects. Though a few benchmarks have been developed to investigate LVLM hallucinations, they mainly rely on hand-crafted corner cases whose fail patterns may hardly generalize, and finetuning on them could undermine their validity. These motivate us to develop the first automatic benchmark generation approach, AUTOHALLUSION, that harnesses a few principal strategies to create diverse hallucination examples. It probes the language modules in LVLMs for context cues and uses them to synthesize images by: (1) adding objects abnormal to the context cues; (2) for two co-occurring objects, keeping one and excluding the other; or (3) removing objects closely tied to the context cues. It then generates image-based questions whose ground-truth answers contradict the language module's prior. A model has to overcome contextual biases and distractions to reach correct answers, while incorrect or inconsistent answers indicate hallucinations. AUTOHALLUSION enables us to create new benchmarks at the minimum cost and thus overcomes the fragility of hand-crafted benchmarks. It also reveals common failure patterns and reasons, providing key insights to detect, avoid, or control hallucinations. Comprehensive evaluations of top-tier LVLMs, e.g., GPT-4V(ision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, show a 97.7% and 98.7% success rate of hallucination induction on synthetic and real-world datasets of AUTOHALLUSION, paving the way for a long battle against hallucinations",
    "checked": true,
    "id": "a649db6921c327b75df38d4b81e9c8b4173fb175",
    "semantic_title": "autohallusion: automatic generation of hallucination benchmarks for vision-language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=G8PyRFzTEt": {
    "title": "Assessing the Role of Imagery in Multimodal Machine Translation",
    "volume": "review",
    "abstract": "In Multimodal Machine Translation (MMT), the use of visual data has shown only marginal improvements compared to text-only models. Previously, the CoMMuTE dataset and associated metric were proposed to score models on tasks where the imagery is necessary to disambiguate between two possible translations for each ambiguous source sentence. In this work, we introduce new metrics within the CoMMuTE domain to provide deeper insights into image-aware translation models. Our proposed metrics differ from the previous CoMMuTE scoring method by 1) assessing the impact of multiple images on individual translations and 2) evaluating a model's ability to jointly select each translation for each image context. Our results challenge the conventional views of poor visual comprehension capabilities of MMT models and show that models can indeed meaningfully interpret visual information, though they may not leverage it sufficiently in the final decision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TmsQjrlMjH": {
    "title": "Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs",
    "volume": "review",
    "abstract": "Reasoning encompasses two typical types: deductive reasoning and inductive reasoning. Despite extensive research into the reasoning capabilities of Large Language Models (LLMs), most studies have failed to rigorously differentiate between inductive and deductive reasoning, leading to a blending of the two. This raises an essential question: In LLM reasoning, which poses a greater challenge - deductive or inductive reasoning? While the deductive reasoning capabilities of LLMs, (i.e. their capacity to follow instructions in reasoning tasks), have received considerable attention, their abilities in true inductive reasoning remain largely unexplored. To delve into the true inductive reasoning capabilities of LLMs, we propose a novel framework, SolverLearner. This framework enables LLMs to learn the underlying function (i.e., $y = f_w(x)$), that maps input data points $(x)$ to their corresponding output values $(y)$, using only in-context examples. By focusing on inductive reasoning and separating it from LLM-based deductive reasoning, we can isolate and investigate inductive reasoning of LLMs in its pure form via SolverLearner. Our observations reveal that LLMs demonstrate remarkable inductive reasoning capabilities through SolverLearner, achieving near-perfect performance with ACC of 1 in most cases. Surprisingly, despite their strong inductive reasoning abilities, LLMs tend to relatively lack deductive reasoning capabilities, particularly in tasks involving ``counterfactual'' reasoning",
    "checked": true,
    "id": "f42b97cdfbf1a78c02e78cfce6f8b0e277766ae2",
    "semantic_title": "inductive or deductive? rethinking the fundamental reasoning abilities of llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s1dOBTeFDM": {
    "title": "ThatiAR: Subjectivity Detection in Arabic News Sentences",
    "volume": "review",
    "abstract": "Detecting subjectivity in news sentences is crucial for identifying media bias, enhancing credibility, and combating misinformation by flagging opinion-based content. It provides insights into public sentiment, empowers readers to make informed decisions, and encourages critical thinking. While research has developed methods and systems for this purpose, most efforts have focused on English and other high-resourced languages. In this study, we present the first large dataset for subjectivity detection in Arabic, consisting of $\\sim$3.6K manually annotated sentences, and GPT-4o based explanation. In addition, we included instructions (both in English and Arabic) to facilitate LLM based fine-tuning. We provide an in-depth analysis of the dataset, annotation process, and extensive benchmark results, including PLMs and LLMs. Our analysis of the annotation process highlights that annotators were strongly influenced by their political, cultural, and religious backgrounds, especially at the beginning of the annotation process. The experimental results suggest that LLMs with in-context learning provide better performance. We aim to release the dataset and resources for the community",
    "checked": true,
    "id": "f0dca5b54532a6bd9ac55c195ee35dc2240999a4",
    "semantic_title": "thatiar: subjectivity detection in arabic news sentences",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=fcJZkcfA8E": {
    "title": "DRE: Generating Recommendation Explanations by Aligning Large Language Models at Data-level",
    "volume": "review",
    "abstract": "Recommendation systems play a crucial role in various domains, suggesting items based on user behavior. And the lack of transparency in presenting recommendations can lead to user confusion. Thus, recommendation explanation methods are proposed to generate natural language explanations for users, which usually require intermediary representations of the recommendation model or need to conduct latent alignment training to the recommendation model. However, this additional training step usually causes potential performance issues due to the different training objectives between the recommendation task and the explanation task. In this paper, we introduce Data-level Recommendation Explanation (DRE), a non-intrusive explanation framework for black-box recommendation models. We propose a data-level alignment method, leveraging large language models to reason relationships between user data and recommended items, without any additional training or intermediary representations for the recommendation model. Additionally, we also address the challenge of enriching the details of the explanation by introducing target-aware user preference distillation, utilizing item reviews. Experimental results on several benchmark datasets demonstrate the effectiveness of the DRE in providing accurate and user-centric explanations, enhancing user engagement with recommended items",
    "checked": true,
    "id": "b735493dc00b31f78bf7cdb8bd47d8822813bd51",
    "semantic_title": "dre: generating recommendation explanations by aligning large language models at data-level",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZkZ2erwY0w": {
    "title": "PS-Radar: A High-Precision Geoparsing Solution for Real-Time Location Analysis",
    "volume": "review",
    "abstract": "PS-Radar is an advanced geoparsing solution utilizing OSM Nominatim to analyze text messages and generate a curated list of potential locations mentioned within. These results are then disambiguated and ranked based on their likelihood of accurately representing the target locations. Our system integrates data from OSM and other sources, enabling real-time map visualization, geofencing, area-based filtering, and alerting for monitoring social media message streams. Furthermore, we introduce the SonarChallenge dataset, comprising 1489 annotated messages containing location references. In our evaluation using the SonarChallenge dataset, our solution achieves an 88.49\\% recall rate for identifying mentioned locations and a 92.51\\% precision rate for accurately pinpointing the output locations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rCk4ARcI1k": {
    "title": "Dual Modalities of Text: Visual and Textual Generative Pre-Training",
    "volume": "review",
    "abstract": "Harnessing visual texts represents a burgeoning frontier in the evolution of language modeling. In this paper, we introduce a novel pre-training framework for a suite of pixel-based autoregressive language models, pre-training on a corpus of over 400 million document images. Our approach is characterized by a dual-modality training regimen, engaging both visual data through next patch prediction with a regression head and/or textual data via next token prediction with a classification head. This study is particularly focused on investigating the synergistic interplay between visual and textual modalities of language. Our comprehensive evaluation across a diverse array of benchmarks reveals that the confluence of visual and textual data substantially augments the efficacy of pixel-based language models. Notably, our findings show that a unidirectional pixel-based model, devoid of textual data during training, can match the performance levels of advanced bidirectional pixel-based models on various language understanding benchmarks. This work highlights the considerable untapped potential of integrating visual and textual information for language modeling purposes. We will release our code, data, and checkpoints to inspire further research advancement",
    "checked": true,
    "id": "4ad9ea46035ae76ccde5f66068680fd60c8eebef",
    "semantic_title": "dual modalities of text: visual and textual generative pre-training",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=xotWo7JKM9": {
    "title": "DVD: Dynamic Contrastive Decoding for Knowledge Amplification in Multi-Document Question Answering",
    "volume": "review",
    "abstract": "Large language models (LLMs) are widely used in question-answering (QA) systems but often generate information with hallucinations. Retrieval-augmented generation (RAG) offers a potential remedy, yet the uneven retrieval quality and irrelevant contents may distract LLMs. In this work, we address these issues at the generation phase by treating RAG as a multi-document QA task. We propose a novel decoding strategy, Dynamic Contrastive Decoding (DVD), which dynamically amplifies knowledge from selected documents during the generation phase. DVD involves constructing inputs batchwise, designing new selection criteria to identify documents worth amplifying, and applying contrastive decoding with a specialized weight calculation to adjust the final logits used for sampling answer tokens. Zero-shot experimental results on ALCE-ASQA and NQ benchmark show that our method outperforms other decoding strategies. Additionally, we conduct experiments to validate the effectiveness of our selection criteria, weight calculation, and general multi-document scenarios. Our method requires no training and can be integrated with other methods to improve the RAG performance. Our codes are submitted with the paper and will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WKqBCqgeBk": {
    "title": "Improving Minimum Bayes Risk Decoding with Weight Uncertainty",
    "volume": "review",
    "abstract": "Minimum Bayes Risk (MBR) decoding has become a popular decoding strategy for different natural language generation tasks, especially machine translation. MBR relies on an estimator of an expected loss, where we use our learned model as a proxy for the target distribution that we wish to take this expectation with respect to. However, this reliance can be problematic if the model is a flawed proxy, for example, in light of a lack of training data in a specific domain. In this work, we show how using a posterior over model parameters, and decoding with a weighted-averaging over multiple models, can improve the performance of MBR by accounting for uncertainty over the learned model. We benchmark different methods for learning posteriors and show that performance correlates with the diversity of the combined set of models' predictions. Intriguingly, prediction diversity also determines whether risk can be successfully used for selective prediction",
    "checked": false,
    "id": "4fc55e7428c579a6381c8f97e56e8b8ec986dc2b",
    "semantic_title": "rmbr: a regularized minimum bayes risk reranking framework for machine translation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=qjdoPUVmnG": {
    "title": "Cross-Domain Classification of Education Talk-Turns",
    "volume": "review",
    "abstract": "The study of classroom discourse is essential for enhancing child development and educational outcomes in academic settings. Prior research has focused on the annotation of conversational talk-turns within the classroom, offering a statistical analysis of the various types of discourse prevalent in these environments. In this work, we explore the generalizability and transferability of these discourse codes across different educational domains via automatic text classifiers. We examine two distinct English-language classroom datasets from the domains of literacy and mathematics. Our results show that models exhibit high accuracy and generalizability when the training and test datasets originate from the same or similar domains. However, as the distance between the training and test domains increases in terms of subject matter and teaching methodology, we observe a decline in model performance. We also observe that accompanying each talk turn with dialog-level context improves the accuracy of the generative models. We conclude by offering suggestions on how to enhance the generalization of these methods to novel domains, proposing directions for future studies to investigate new methods and techniques for boosting the model adaptability across varied educational domains",
    "checked": false,
    "id": "f87654fd9af3af622c0363238e738b73dd3994c7",
    "semantic_title": "in the classroom: the views of education staff and therapists involved with",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kpEvU0MD2x": {
    "title": "Input Conditioned Graph Generation for Language Agents",
    "volume": "review",
    "abstract": "Recent progress in the areas of Large Language Models (LLMs) and Language Agents has demonstrated significant promise for various future applications across multiple disciplines. While traditional approaches to language agents often rely on fixed, handcrafted designs, our research aims to develop agents that are both learnable and dynamic. In our method, we use an existing framework that abstracts language agents as graphs. Within this graph framework, we aim to learn a model that can generate edges for every given input to the language agent. This allows us to generate edges that represent the flow of communication within the graph based on the given input, thereby adjusting the internal communication of a language agent. We learn to generate these edges using a pretrained LLM that is fine-tuned with reinforcement learning. This LLM can be fine-tuned on several datasets simultaneously, and we hypothesize that the model learns to adapt to these different domains during training, achieving good overall performance when encountering data from different domains during deployment. We demonstrate that our approach surpasses the previous static approach by nearly 6% accuracy on a combined dataset of MMLU and CMMLU, and by more than 10% when trained with a sparsity-inducing loss. It also shows superior performance in additional experiments conducted with the MMLU and Mini Crossword Puzzles datasets",
    "checked": true,
    "id": "a750226cf190d3f776470598b8d16c4211ce8ee8",
    "semantic_title": "input conditioned graph generation for language agents",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T4ZysPbfW4": {
    "title": "R&R: A Role-playing Model Enhanced by Retrieving and Reflecting",
    "volume": "review",
    "abstract": "Role-playing is one of the essential capabilities of large language models (LLMs). However, existing role-playing models make it challenging to fully immerse oneself in a character. They do not understand the limitations of knowledge expected in their current role, nor do they possess the appropriate mindset, which makes it easily apparent that they are not truly fulfilling their role. To solve this, we propose R&R, a role-playing model enhanced by retrieving and reflecting. Before generating responses, our model first retrieves relevant role knowledge and similar dialogues based on the questions asked. Then, it uses reflections extracted from historical dialogues to understand the context. Finally, by establishing knowledge boundaries and inputs for these reflections, our model can produce replies that accurately represent the current role's perspective. To assess the effectiveness of our approach, we build a new dataset and compare our model with other models in \"Values\", \"Personality\", \"Hallucination\", \"Stability\" and \"Mindset\" five dimensions. The results demonstrate that the average performance of our model improves by 8\\% over ChatacterLLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DYqbooTruk": {
    "title": "MobileQuant: Mobile-friendly Quantization for On-device Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have revolutionized language processing, delivering outstanding results across multiple applications. However, deploying LLMs on edge devices poses several challenges with respect to memory, energy, and compute costs, limiting their widespread use in devices such as mobile phones. A promising solution is to reduce the number of bits used to represent weights and activations. While existing works have found partial success at quantizing LLMs to lower bitwidths, e.g. 4-bit weights, quantizing activations beyond 16 bits often leads to large computational overheads due to poor on-device quantization support, or a considerable accuracy drop. Yet, 8-bit activations are very attractive for on-device deployment as they would enable LLMs to fully exploit mobile-friendly hardware, e.g. Neural Processing Units (NPUs). In this work, we make a first attempt to facilitate the on-device deployment of LLMs using integer-only quantization. We first investigate the limitations of existing quantization methods for on-device deployment, with a special focus on activation quantization. We then address these limitations by introducing a simple post-training quantization method, named MobileQuant, that extends previous weight equivalent transformation works by jointly optimizing the weight transformation and activation range parameters in an end-to-end manner. MobileQuant demonstrates superior capabilities over existing methods by 1) achieving near-lossless quantization on a wide range of LLM benchmarks, 2) reducing latency and energy consumption by 20\\%-50\\% compared to current on-device quantization strategies, 3) requiring limited compute budget, 4) being compatible with mobile-friendly compute units, e.g. NPU",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7wKlIsJHLa": {
    "title": "Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language",
    "volume": "review",
    "abstract": "Dehumanization, characterized as a subtle yet harmful manifestation of hate speech, involves denying individuals of their human qualities and often results in violence against marginalized groups. Despite significant progress in Natural Language Processing across various domains, its application in detecting dehumanizing language is limited, largely due to the scarcity of publicly available annotated data for this domain. This paper evaluates the performance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2, in identifying dehumanizing language. Our findings reveal that while these models demonstrate potential, achieving a 70% accuracy rate in distinguishing dehumanizing language from broader hate speech, they also display biases. They are over-sensitive in classifying other forms of hate speech as dehumanization for a specific subset of target groups, while more frequently failing to identify clear cases of dehumanization for other target groups. Moreover, leveraging one of the best-performing models, we automatically annotated a larger dataset for training more accessible models. However, our findings indicate that these models currently do not meet the high-quality data generation threshold necessary for this task",
    "checked": true,
    "id": "d32cc4a0db26c4c508337d51c2fa0c50da32f702",
    "semantic_title": "beyond hate speech: nlp's challenges and opportunities in uncovering dehumanizing language",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VHMKPhzEwr": {
    "title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
    "volume": "review",
    "abstract": "With the rapid development of large language models (LLMs), how to efficiently evaluate them has become an important research question. Existing evaluation methods often suffer from high costs, limited test formats, the needs of human references, and systematic evaluation biases. To address these issues, our study introduces the Auto-PRE, an automatic LLM evaluation framework based on peer review. In contrast to previous studies that rely on human annotations, Auto-PRE selects evaluator LLMs automatically based on their inherent traits including consistency, self-confidence, and pertinence. We have conducted extensive experiments on both summary generation and non-factoid question-answering tasks. Results indicate our Auto-PRE achieves state-of-the-art performance at a lower cost. Moreover, our study highlights the impact of prompt strategies and evaluation formats on evaluation performance, offering guidance for method optimization in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RBkEqV9yME": {
    "title": "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning",
    "volume": "review",
    "abstract": "Generative Commonsense Reasoning (GCR) requires a model to reason about a situation using commonsense knowledge, while generating coherent sentences. Although the quality of the generated sentences is crucial, the diversity of the generation is equally important because it reflects the model's ability to use a range of commonsense knowledge facts. Large Language Models (LLMs) have shown proficiency in enhancing the generation quality across various tasks through in-context learning (ICL) using given examples without the need for any fine-tuning. However, the diversity aspect in LLM outputs has not been systematically studied before. To address this, we propose a simple method that diversifies the LLM generations, while preserving their quality. Experimental results on three benchmark GCR datasets show that our method achieves an ideal balance between the quality and diversity",
    "checked": true,
    "id": "522240c89c7ac6b1365e8308c6a88c4784adc62e",
    "semantic_title": "improving diversity of commonsense generation by large language models via in-context learning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=bO26RZUUOu": {
    "title": "Retrieved Sequence Augmentation for Protein Representation Learning",
    "volume": "review",
    "abstract": "Protein Language Models traditionally depend on Multiple Sequence Alignments (MSA) to incorporate evolutionary knowledge. However, MSA-based approaches suffer from substantial computational overhead and generally underperform in generalizing to de novo proteins. This study reevaluates the role of MSA, proposing it as a retrieval augmentation method and questioning the necessity of sequence alignment. We show that a simple alternative, Retrieved Sequence Augmentation (RSA), can enhance protein representation learning without the need for alignment and cumbersome preprocessing. RSA surpasses MSA Transformer by an average of 5\\% in both structural and property prediction tasks while being 373 times faster. Additionally, RSA demonstrates enhanced transferability for predicting de novo proteins. This methodology addresses a critical need for efficiency in protein prediction and can be rapidly employed to identify homologous sequences, improve representation learning, and enhance the capacity of Large Language Models to interpret protein structures",
    "checked": true,
    "id": "77e5d0a68afcffb27191572590deced60feb9d5d",
    "semantic_title": "retrieved sequence augmentation for protein representation learning",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=HgYIajh9fN": {
    "title": "Reconfidencing LLMs from the Grouping Loss Perspective",
    "volume": "review",
    "abstract": "Large Language Models (LLMs), such as GPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone. While previous efforts to elicit and calibrate confidence scores have shown some success, they often overlook biases towards certain groups, such as specific nationalities. Existing calibration methods typically focus on average performance, failing to address this disparity. In our study, we demonstrate that the concept of grouping loss is an effective metric for understanding and correcting the heterogeneity in confidence levels. We introduce a novel evaluation dataset, derived from a knowledge base, specifically designed to assess the confidence scores of LLM responses across different groups. Our experimental results highlight significant variations in confidence, which are accurately captured by grouping loss. To tackle this issue, we propose a new method to calibrate the confidence scores of LLMs by considering different groups, a process we term \\emph{reconfidencing}. Our findings indicate that this approach effectively mitigates biases against minority groups, contributing to the development of fairer LLMs",
    "checked": true,
    "id": "754f9c903754d909cb754364f4d6416ada0ab2b5",
    "semantic_title": "reconfidencing llms from the grouping loss perspective",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=E6aEhSlwHw": {
    "title": "Humanity in AI: Detecting the Personality of Large Language Models",
    "volume": "review",
    "abstract": "Questionnaires are commonly used to detect the personality of large language models (LLMs). However, LLMs suffer from hallucinations and cannot generate reliable answers making it impossible to detect their true personality through questionnaires. To solve this problem, we propose a new method to detect the personality of LLMs by combining questionnaire and text mining methods in this paper. The text mining method can determine the personality of LLMs based on their generated texts, avoiding the influence of hallucinations. In this paper, we also investigate the source of LLMs' personality by conducting experiments on pre-trained language models (PLMs, such as BERT and GPT) and Chat models (ChatLLMs, such as ChatGPT). The results show that LLMs do contain certain personalities, for example, ChatGPT and ChatGLM exhibit the personality traits of 'Conscientiousness'. Moreover, we find that the personalities of LLMs are derived from their pre-trained data. The instruction data used to train ChatLLMs can enhance the generation of data containing personalities and expose their hidden personality. We compare the results with the human average personality score, and we find that the personality of FLAN-T5 in PLMs and ChatGPT in ChatLLMs is more similar to that of a human, with score differences of 0.34 and 0.22, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATk82ymOUv": {
    "title": "CoEvol: Constructing Better Responses for Instruction Finetuning through Multi-Agent Cooperation",
    "volume": "review",
    "abstract": "In recent years, instruction fine-tuning (IFT) on large language models (LLMs) has garnered considerable attention to enhance model performance on unseen tasks. Attempts have been made on automatic construction and effective selection for IFT data. However, we posit that previous methods have not fully harnessed the potential of LLMs for enhancing data quality. The responses within IFT data could be further enhanced by leveraging the capabilities of LLMs themselves. In this paper, we propose CoEvol, an LLM-based multi-agent cooperation framework for the improvement of responses for instructions. To effectively refine the responses, we develop an iterative framework following a $\\textit{debate-advise-edit-judge}$ paradigm. A two-stage multi-agent debate strategy is further devised to ensure the diversity and reliability of editing suggestions within the framework. Empirically, models equipped with CoEvol outperform competitive baselines evaluated by MT-Bench and AlpacaEval, demonstrating its effectiveness in enhancing instruction-following capabilities for LLMs",
    "checked": true,
    "id": "1e56e3956a86b57d351c305d391cdb65a10c67bb",
    "semantic_title": "coevol: constructing better responses for instruction finetuning through multi-agent cooperation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oGvgcmJDww": {
    "title": "A Road for LLM SQL Bug-Fixing Enhancing",
    "volume": "review",
    "abstract": "Code Large Language Models (Code LLMs), such as Code llama and DeepSeek-Coder, have demonstrated exceptional performance in the code generation tasks. However, most existing models focus on the abilities of generating correct code, but weak in bug code repair. In this paper, we introduce a suit of methods which enhance LLM's bug-fixing abilities on SQL code, which are mainly consisted of two parts: A Progressive Dataset Construction (PDC) from scratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data expansion methods from the perspectives of breadth first and depth first respectively. DM-SFT introduces an efficient bug-fixing supervised learning approach, which effectively reduce the total training steps and mitigate the \"mental disorientation\" in SQL code bug-fixing training. In our evaluation, the code LLM models trained on these two methods have exceeds all current best performing model which size is much larger",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dj6WcAjMH7": {
    "title": "Can we employ LLM to meta-evaluate LLM-based evaluators? A Preliminary Study",
    "volume": "review",
    "abstract": "Large language models (LLMs) are frequently employed to evaluate the instruction-following abilities of other LLMs. A number of recent work focuses on the meta-evaluation of LLM-based evaluation, aiming to understand the efficacy of LLMs as evaluators. However, these studies are limited by the scope of existing benchmarks and the extensive human annotation efforts. Since previous studies show that strong LLMs can effectively evaluatethe instruction-following abilities of other LLMs, a natural question is whether we can use LLMs to meta-evaluate the evaluation abilities of other LLMs by considering LLM-based evaluation as special case of instruction-following tasks. In this work,we investigate the potential of LLMs to conduct meta-evaluation and examine the extent to which the proficiency of the model and the scale of the model impact this meta-evaluation capacity. To this end, we introduce four frameworks within the paradigms of pairwise comparison (JDEval and MDEval) and individual scoring (JDEval-i and BSMEval). Through our experiments, we find that pairwise comparison paradigm is more suitable to conduct meta-evaluation than individual scoring paradigm. JDEval and MDEval have demonstrated strong performance in meta-evaluation tasks, showing high agreement with human annotations",
    "checked": false,
    "id": "f6180645ccd02299f7c063dc8a21a33b6dfa73c1",
    "semantic_title": "a generative text summarization method based on mt5 and large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DrE7KAzwyU": {
    "title": "Enhancing Zero-Shot Relation Triplet Extraction through Staged Interaction with Large Language Models",
    "volume": "review",
    "abstract": "Zero-shot Relation Triplet Extraction (ZeroRTE) is a challenging yet valuable task that extracts relation triplets from unstructured texts for new relation types, significantly reducing the time and effort needed for data labeling. With the enhancement of the zero-shot capability of large language model, the performance of many zero-shot tasks has been further improved only via chatting with Large Language Model(LLM). In this work, we transform the zero-shot triplet extraction task into a two-stage chat with LLM. Specially, followed by the step of triplet extraction, we prompt the LLM to perform the NER(Name Entity Recognition) task in the first stage. Then, in the second stage, we prompt the LLM to perform the relation classification task combining the result of the first stage. To overcome the impact of redundant information of the LLM's output on task evaluation, we design a Post-Processing module to obtain the relation triplet. Experiments on Wiki-ZSL and FewRel datasets show the efficacy of Relation Prompt for the ZeroRTE task. Remarkably, our method outperforms strong baselines by a significant margin, achieving an impressive 15.89\\% increase in F1 scores, particularly when dealing with Wiki-ZSL with 15 unseen relations",
    "checked": false,
    "id": "c5ab539ab10d85a784487324360029fd5cf96539",
    "semantic_title": "detecting any human-object interaction relationship: universal hoi detector with spatial prompt learning on foundation models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=a5En3TZwii": {
    "title": "ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios",
    "volume": "review",
    "abstract": "Existing evaluations of tool learning primarily focus on validating the alignment of selected tools for large language models (LLMs) with expected outcomes. However, these approaches rely on a limited set of scenarios where answers can be pre-determined. Furthermore, a *sole* emphasis on outcomes disregards the intricate capabilities essential for LLMs to effectively utilize tools. To tackle this issue, we propose *ToolEyes*, a fine-grained system tailored for the evaluation of the LLMs' tool learning capabilities in authentic scenarios. The system meticulously examines seven real-world scenarios, analyzing five dimensions crucial to LLMs in tool learning: *format alignment*, *intent comprehension*, *behavior planning*, *tool selection*, and *answer organization*. Additionally, ToolEyes incorporates a tool library boasting approximately 600 tools, serving as an intermediary between LLMs and the physical world. Evaluations involving ten LLMs across three categories reveal a preference for specific scenarios and limited cognitive abilities in tool learning. Intriguingly, expanding the model size even exacerbates the hindrance to tool learning",
    "checked": true,
    "id": "5969ab39b3e5265c294c8fe25f33e880166cdebf",
    "semantic_title": "tooleyes: fine-grained evaluation for tool learning capabilities of large language models in real-world scenarios",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=MJQNbYYt1d": {
    "title": "DRaMI: Dialogue Relation-aware Multi-task with In-context Learning for Emotion Recognition in Multi-party Conversations",
    "volume": "review",
    "abstract": "As an emerging research topic, the emotion recognition in multi-party conversations (ERMC) task garners significant attention from researchers. However, due to the unstructured nature and complicated emotional interactions among speakers within the entangled multi-party dialogues, ERMC task presents substantial challenges. To tackle this issue, we propose a novel three-stage framework, called Dialogue Relation-aware Multi-task with In-context learning (DRaMI). Specifically, a transformer-based discourse parser is designed to extract links and the corresponding relation types between the Elementary Discourse Units (EDUs) for multi-party conversations. With the extracted relations, dialogue discourse parsing task prompt templates are constructed for instruction fine-tuning Large Language Models (LLMs) as the pre-trained model. Additionally, an efficient in-context learning (ICL) mechanism is applied to stably retrieve highly relevant examples across dual dimensions. Finally, through the dialogue relation-aware module and retrieval module, instruction fine-tuning is performed on the emotion recognition in multi-party conversations task. Empirical results on two benchmarks demonstrate the effectiveness of the proposed DRaMI, which achieves state-of-the-art performance on the MELD dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u0nVUnFhfM": {
    "title": "Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching",
    "volume": "review",
    "abstract": "Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. To keep LLMs current, existing approaches typically involve continued pre-training on new documents. However, they frequently face difficulties in extracting stored knowledge. Motivated by the remarkable success of the Feynman Technique in efficient human learning, we introduce Self-Tuning, a learning framework aimed at improving an LLM's ability to effectively acquire new knowledge from raw documents through self-teaching. Specifically, we develop a Self-Teaching strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection. Additionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an in-depth analysis of an LLM's knowledge acquisition ability concerning memorization, extraction, and reasoning. Extensive experimental results on Llama2 family models reveal that Self-Tuning consistently exhibits superior performance across all knowledge acquisition tasks and excels in preserving previous knowledge",
    "checked": true,
    "id": "508f50f171b4aee6cbc573da1bed0472a86eaa8b",
    "semantic_title": "self-tuning: instructing llms to effectively acquire new knowledge through self-teaching",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=KMUUgLfJst": {
    "title": "Noise-powered Multi-modal Knowledge Graph Representation Framework",
    "volume": "review",
    "abstract": "The rise of Multi-modal Pre-training highlights the necessity for a unified Multi-Modal Knowledge Graph (MMKG) representation learning framework. Such a framework is essential for embedding structured knowledge into multi-modal Large Language Models effectively, alleviating issues like knowledge misconceptions and multi-modal hallucinations. In this work, we explore the efficacy of models in accurately embedding entities within MMKGs through two pivotal tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking to robustly integrate multi-modal entity features in KGs. By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets, demonstrating its versatility. Moreover, SNAG can not only function as a standalone model but also enhance other existing methods, providing stable performance improvements. Code and data are available at https://anonymous.4open.science/r/SNAG",
    "checked": false,
    "id": "0f6b86b72e68cb4057bde014131785536fba1814",
    "semantic_title": "the power of noise: toward a unified multi-modal knowledge graph representation framework",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=38K5miAAbL": {
    "title": "How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection",
    "volume": "review",
    "abstract": "To combat the misuse of Large Language Models (LLMs), many recent studies have presented LLM-generated-text detectors with promising performance. When users instruct LLMs to generate texts, the instruction can include different constraints depending on the user's need. However, most recent studies do not cover such diverse instruction patterns when creating datasets for LLM detection. In this paper, we reveal that even task-oriented constraints --- constraints that would naturally be included in an instruction and are not related to detection-evasion --- cause existing powerful detectors to have a large variance in detection performance. We focus on student essay writing as a realistic domain and manually create task-oriented constraints based on several factors for essay quality. Our experiments show that the standard deviation (SD) of current detector performance on texts generated by an instruction with such a constraint is significantly larger (up to an SD of 14.4 F1-score) than that by generating texts multiple times or paraphrasing the instruction. We also observe an overall trend where the constraints can make LLM detection more challenging than without them. Finally, our analysis indicates that the high instruction-following ability of LLMs fosters the large impact of such constraints on detection performance",
    "checked": true,
    "id": "fbf3659c0967703035cfed17481276a576d6f88a",
    "semantic_title": "how you prompt matters! even task-oriented constraints in instructions affect llm-generated text detection",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=U0myZcZ30d": {
    "title": "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully",
    "volume": "review",
    "abstract": "Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that these low-confidence tokens are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ``highlight'' the factual information by selecting key tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in output probabilities brought by the hesitation. Experimental results demonstrate that our SH2, requiring no additional data or models, can effectively help LLMs elicit factual knowledge and distinguish hallucinated contexts by themselves. Significant and consistent improvements are achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on various hallucination tasks",
    "checked": true,
    "id": "a27529194bd61c6013fb0f48797f08e4998a1be8",
    "semantic_title": "sh2: self-highlighted hesitation helps you decode more truthfully",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=b9bHwbxILh": {
    "title": "Unsupervised Morphological Tree Tokenizer",
    "volume": "review",
    "abstract": "As a cornerstone in language modeling, tokenization involves segmenting text inputs into pre-defined atomic units. Conventional statistical tokenizers often disrupt constituent boundaries within words, thereby corrupting semantic information. To address this drawback, we introduce morphological structure guidance to tokenization and propose a deep model to induce character-level structures of words. Specifically, the deep model jointly encodes internal structures and representations of words with a mechanism named *MorphOverriding* to ensure the indecomposability of morphemes. By training the model with self-supervised objectives, our method is capable of inducing character-level structures that align with morphological rules without annotated training data. Based on the induced structures, our algorithm tokenizes words through vocabulary matching in a top-down manner. Empirical results indicate that the proposed method effectively retains complete morphemes and outperforms widely adopted methods such as BPE and WordPiece on both morphological segmentation tasks and language modeling tasks. The code will be released later",
    "checked": true,
    "id": "def2d4b3b0d95c0620521d2ba7c5429d3b9fddc2",
    "semantic_title": "unsupervised morphological tree tokenizer",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HCkNgkS5H8": {
    "title": "How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States",
    "volume": "review",
    "abstract": "Large language models (LLMs) rely on safety alignment to avoid responding to malicious user inputs. Unfortunately, jailbreak can circumvent safety guardrails, resulting in LLMs generating harmful content and raising concerns about LLM safety. Due to language models with intensive parameters often regarded as black boxes, the mechanisms of alignment and jailbreak are challenging to elucidate. In this paper, we employ weak classifiers to explain LLM safety through the intermediate hidden states. We first confirm that LLMs learn ethical concepts during pre-training rather than alignment and can identify malicious and normal inputs in the early layers. Alignment actually associates the early concepts with emotion guesses in the middle layers and then refines them to the specific reject tokens for safe generations. Jailbreak disturbs the transformation of early unethical classification into negative emotions. We conduct experiments on models from 7B to 70B across various model families to prove our conclusion. Overall, our paper indicates the intrinsical mechanism of LLM safety and how jailbreaks circumvent safety guardrails, offering a new perspective on LLM safety and reducing concerns",
    "checked": true,
    "id": "2b01cbe125ed13ccb3ef02e9536582825f2afd57",
    "semantic_title": "how alignment and jailbreak work: explain llm safety through intermediate hidden states",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=F4B2dGGgVm": {
    "title": "DRMR: An Immersing Oriented Role-Playing Framework with Duplex Relationship Modeling",
    "volume": "review",
    "abstract": "Role-playing is an emerging application of large language models (LLMs), allowing users to be immersed in conversations with virtual characters by mimicking their tones and background knowledge. It can be applied in various scenarios such as gaming and virtual reality systems. However, existing methods ignore two challenges: (1) ignoring the relationship with the role played by the user will diminish the immersive experience of the user; (2) insufficient understanding of the character's background knowledge may lead to inconsistent dialogue. In this paper, we introduce the Duplex Relationship Modeling based Role-play framework(DRMR), a novel role-playing framework designed to enhance the immersion of user when interacting with the role-play model. We first propose a graph-based relationship modeling method, utilizing graph structures to model the duplex relationship between the user and the model's played characters. In order to better extract useful personalized information about roles from historical dialogues, we construct a role memory consisting of the description of the duplex relationship. To avoid generating an inconsistent response, we iteratively verify the generated response by updating the role memory according to the current dialogue context. Extensive experiments on benchmark dataset demonstrate the effectiveness of DRMR in enhancing user immersion in role-playing interactions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wEOwKX6j4f": {
    "title": "High-Dimension Human Value Representation in Large Language Models",
    "volume": "review",
    "abstract": "The widespread application of Large Language Models (LLMs) across various tasks and fields has necessitated the alignment of these models with human values and preferences. Given various approaches of human value alignment, such as Reinforcement Learning with Human Feedback (RLHF), constitutional learning, safety fine tuning, etc., there is an urgent need to understand the scope and nature of human values injected into these LLMs before their deployment and adoption. We propose UniVaR, a high-dimensional neural representation of symbolic human value distributions in LLMs, orthogonal to model architecture and training data. This is a continuous and scalable representation, self-supervised from the value-relevant output of 8 LLMs and evaluated on 15 open-source and commercial LLMs. Through UniVaR, we visualize and explore how LLMs prioritize different values in 25 languages and cultures, shedding light on the complex interplay between human values and language modeling",
    "checked": true,
    "id": "8ddd669fad4272223ec94f3cbdfd8d67876a60b5",
    "semantic_title": "high-dimension human value representation in large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=pVqoJiEKue": {
    "title": "Necessary and Sufficient Watermark for Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) can now generate texts that are indistinguishable from those written by humans. Such remarkable performance of LLMs increases their risk of being used for malicious purposes. Thus, it is necessary to develop methods for distinguishing texts written by LLMs from those written by humans. Watermarking is one of the most powerful methods for achieving this. Although existing methods have successfully detected texts generated by LLMs, they inevitably degrade the text quality. In this study, we propose the Necessary and Sufficient Watermark (NS-Watermark) for inserting watermarks into generated texts with minimum text quality degradation. More specifically, we derive minimum constraints required to be imposed on the generated texts to distinguish whether LLMs or humans write the texts, and we formulate the NS-Watermark as a constrained optimization problem. Through the experiments, we demonstrate that the NS-Watermark can generate more natural texts than existing watermarking methods and distinguish more accurately between texts written by LLMs and those written by humans. Especially in machine translation tasks, the NS-Watermark can outperform the existing watermarking method by up to $30$ BLEU scores",
    "checked": true,
    "id": "9c78484d28cfaa71894c28c75aabe83d24d67705",
    "semantic_title": "necessary and sufficient watermark for large language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=3I8bhxI6H8": {
    "title": "Robust Utility-Preserving Text Anonymization Based on Large Language Models",
    "volume": "review",
    "abstract": "Text anonymization is crucial for sharing sensitive data while maintaining privacy. Existing techniques face the emerging challenges of re-identification attack ability of Large Language Models (LLMs), which have shown advanced capability in memorizing detailed information and patterns as well as connecting disparate pieces of information. In defending against LLM-based re-identification attacks, anonymization could jeopardize the utility of the resulting anonymized data in downstream tasks---the trade-off between privacy and data utility requires deeper understanding within the context of LLMs. This paper proposes a framework composed of three LLM-based components---a privacy evaluator, a utility evaluator, and an optimization component, which work collaboratively to perform anonymization. To provide a practical model for large-scale and real-time environments, we distill the anonymization capabilities into a lightweight model using Direct Preference Optimization (DPO). Extensive experiments demonstrate that the proposed models outperform baseline models, showing robustness in reducing the risk of re-identification while preserving greater data utility in downstream tasks",
    "checked": true,
    "id": "da43029b55e56659f9b6f481f6d02f15b920ab2f",
    "semantic_title": "robust utility-preserving text anonymization based on large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jcxniCckdf": {
    "title": "Quantifying the Gap Between Machine Translation and Native Language in Training for Multimodal, Multilingual Retrieval",
    "volume": "review",
    "abstract": "There is a scarcity of multilingual vision-language models that properly account for the perceptual differences that are reflected in image captions across languages and cultures. The existing lack of model flexibility is shown in a performance gap between training on independently written English and German captions in German text-image retrieval. In this work, we first show that using off-the-shelf machine translation is ineffective at bridging this gap. Second, we propose techniques to reduce the drop off from training on native German captions. Third, we show that part of the gap remains, which identifies an open area in which we encourage future work from the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MThh08klxQ": {
    "title": "Bidirectional Transformer Representations of (Spanish) Ambiguous Words in Context: A New Lexical Resource and Empirical Analysis",
    "volume": "review",
    "abstract": "Lexical ambiguity---where a single wordform takes on distinct, context-dependent meanings---serves as a useful tool to compare across different large language models' (LLMs') ability to form distinct, contextualized representations of the same stimulus. Few studies have systematically compared LLMs' contextualized word embeddings for languages beyond English. Here, we evaluate multiple bidirectional transformers' (BERTs') semantic representations of Spanish ambiguous nouns in context. We develop a novel dataset of minimal-pair sentences evoking the same or different sense for a target ambiguous noun. In a pre-registered study, we collect contextualized human relatedness judgments for each sentence pair. We find that various BERT-based LLMs' contextualized semantic representations capture some variance in human judgments but fall short of the human benchmark, and for Spanish---unlike English---model scale is uncorrelated with performance. We also identify stereotyped trajectories of target noun disambiguation as a proportion of traversal through a given LLM family's architecture, which we partially replicate in English. We contribute (1) a dataset of controlled, Spanish sentence stimuli with human relatedness norms, and (2) to our evolving understanding of the impact that LLM specification (architectures, training protocols) exerts on contextualized embeddings",
    "checked": true,
    "id": "7a8b1e8dbd192ec3d2a7974e2bb015e34548fe40",
    "semantic_title": "bidirectional transformer representations of (spanish) ambiguous words in context: a new lexical resource and empirical analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xPvbFLG1fV": {
    "title": "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis",
    "volume": "review",
    "abstract": "Aspect-based sentiment analysis (ABSA) is an important subtask of sentiment analysis, which aims to extract the aspects and predict their sentiments. Most existing studies focus on improving the performance of the target domain by fine-tuning domain-specific models (trained on source domains) based on the target domain dataset. Few works propose continual learning tasks for ABSA, which aim to learn the target domain's ability while maintaining the history domains' abilities. In this paper, we propose a Large Language Model-based Continual Learning (LLM-CL) model for ABSA. First, we design a domain knowledge decoupling module to learn a domain-invariant adapter and separate domain-variant adapters dependently with an orthogonal constraint. Then, we introduce a domain knowledge warmup strategy to align the representation between domain-invariant and domain-variant knowledge. In the test phase, we index the corresponding domain-variant knowledge via domain positioning to not require each sample's domain ID. Extensive experiments over 19 datasets indicate that our LLM-CL model obtains new state-of-the-art performance",
    "checked": true,
    "id": "7a8a753359a42985690e01b1f061b5048e396474",
    "semantic_title": "boosting large language models with continual learning for aspect-based sentiment analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=noWl1ifTk6": {
    "title": "Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling",
    "volume": "review",
    "abstract": "We present GEST -- a new dataset for measuring gender-stereotypical reasoning in language models and machine translation systems. GEST contains samples for 16 gender stereotypes about men and women (e.g., Women are beautiful, Men are leaders) that are compatible with the English language and 9 Slavic languages. The definition of said stereotypes was informed by gender experts. We used GEST to evaluate English and Slavic masked LMs, English generative LMs, and machine translation systems. We discovered significant and consistent amounts of gender-stereotypical reasoning in almost all the evaluated models and languages. Our experiments confirm the previously postulated hypothesis that the larger the model, the more biased it usually is",
    "checked": true,
    "id": "27bb304d93aab9dd4d12f0d15a204df5674545ac",
    "semantic_title": "women are beautiful, men are leaders: gender stereotypes in machine translation and language modeling",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=3E4Gv5kQS4": {
    "title": "Show Less, Instruct More: Enriching Prompts with Definitions and Guidelines for Zero-Shot NER",
    "volume": "review",
    "abstract": "Recently, several specialized instruction-tuned Large Language Models (LLMs) for Named Entity Recognition (NER) have emerged. Compared to traditional NER approaches, these models have strong generalization capabilities. Existing LLMs mainly focus on zero-shot NER in out-of-domain distributions, being fine-tuned on an extensive number of entity classes that often highly or completely overlap with test sets. In this work instead, we propose SLIMER, an approach designed to tackle never-seen-before named entity tags by instructing the model on fewer examples, and by leveraging a prompt enriched with definition and guidelines. Experiments demonstrate that definition and guidelines yield better performance, faster and more robust learning, particularly when labelling unseen Named Entities. Furthermore, SLIMER performs comparably to state-of-the-art approaches in out-of-domain zero-shot NER, while being trained on a reduced tag set",
    "checked": true,
    "id": "58feabef724d212f87bf7fd4bc474a21e81332ee",
    "semantic_title": "show less, instruct more: enriching prompts with definitions and guidelines for zero-shot ner",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LhiA7mjznI": {
    "title": "The effects of distance on NPI illusive effects in BERT",
    "volume": "review",
    "abstract": "Previous studies have examined the syntactic capabilities of large pre-trained language models, such as BERT, by using stimuli from psycholinguistic studies. Studying well-known processing errors, such as NPI illusive effects can reveal whether a model prioritizes linear or hierarchical information when processing language. Recent experiments have found that BERT is mildly susceptible to Negative Polarity Item (NPI) illusion effects (Shin et al., 2023; Vu and Lee, 2022). We expand on these results by examining the effect of distance on the illusive effect, using and modifying stimuli from Parker and Phillips (2016). We also further tease apart whether the model is more affected by hierarchical distance or linear distance. We find that BERT is highly sensitive to syntactic hierarchical information: added hierarchical layers affected its processing capabilities compared to added linear distance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6FQBTdgSXZ": {
    "title": "Personas as a Way to Model Truthfulness in Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. While unintuitive from a classic view of LMs, recent work has shown that the truth value of a statement can be elicited from the model's representations. This paper presents an explanation for why LMs appear to know the truth despite not being trained with truth labels. We hypothesize that the pretraining data is generated by groups of (un)truthful agents whose outputs share common features, and they form a (un)truthful persona. By training on this data, LMs can infer and represent the persona in its activation space. This allows the model to separate truth from falsehoods and controls the truthfulness of its generation. We show evidence for the persona hypothesis via two observations: (1) we can probe whether a model's answer will be truthful before it is generated; (2) finetuning a model on a set of facts improves its truthfulness on unseen topics. Next, using arithmetics as a synthetic environment, we show that structures of the pretraining data are crucial for the model to infer the truthful persona. Overall, our findings suggest that models can exploit hierarchical structures in the data to learn abstract concepts like truthfulness",
    "checked": true,
    "id": "fc5b8b891a4613a073c8ad14c3e2a465b799848f",
    "semantic_title": "personas as a way to model truthfulness in language models",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=KqPPLy9RXc": {
    "title": "External Knowledge-Driven Argument Mining: Leveraging Attention-Enhanced Multi-Network Models",
    "volume": "review",
    "abstract": "Argument mining (AM) involves the identification of argument relations (AR) between Argumentative Discourse Units (ADUs). The essence of ARs among ADUs is context-dependent and lies in maintaining a coherent flow of ideas, often centered around the relations between discussed entities, topics, themes or concepts. However, these relations are not always explicitly stated; rather, inferred from implicit chains of reasoning connecting the concepts addressed in the ADUs. While humans can infer such background knowledge, machines face challenges when contextual cues are not explicitly provided. This paper leverages external resources, including WordNet, ConceptNet, and Wikipedia to identify chain of semantic relations (knowledge paths) connecting the concepts discussed in the ADUs to obtain the implicit chains of reasoning. To effectively leverage these paths for AR prediction, we propose attention-based Multi-Network architectures. Various configurations of the architecture are evaluated on the external resources, and the configuration using Wikipedia achieves a new state-of-the-art performance with F-scores of 0.85, 0.84, 0.70, and 87, respectively, on four diverse datasets",
    "checked": false,
    "id": "56341b4ed36fcac1df34af651649a442ab66d81b",
    "semantic_title": "ekigcn: external knowledge injected graph convolutional networks for aspect-based sentiment analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=btfCvWz5vO": {
    "title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
    "volume": "review",
    "abstract": "When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently",
    "checked": true,
    "id": "54972b2e4304d2164a61036ae947df2503c07009",
    "semantic_title": "does fine-tuning llms on new knowledge encourage hallucinations?",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=4v4F8N7nvJ": {
    "title": "Contribution of Linguistic Typology to Universal Dependency Parsing: An Empirical Investigation",
    "volume": "review",
    "abstract": "Universal Dependencies (UD) is a global initiative to create a standard annotation for the dependency syntax of human languages. Addressing its deviation from typological principles, this study presents an empirical investigation of a typologically motivated transformation of UD proposed by William Croft. Our findings underscore the significance of the transformations across diverse languages and highlight their advantages and limitations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5zZBGBO1ET": {
    "title": "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present **Neeko**, an innovative framework designed for efficient multiple characters imitation. Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences",
    "checked": true,
    "id": "789c5dd71fb6085161931955339de2172539cdc8",
    "semantic_title": "neeko: leveraging dynamic lora for efficient multi-character role-playing agent",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=BF7r9c7HRJ": {
    "title": "The Fall of ROME: Understanding the Collapse of LLMs in Model Editing",
    "volume": "review",
    "abstract": "Despite significant progress in model editing methods, their application in real-world scenarios remains challenging as they often cause large language models (LLMs) to collapse. Among them, ROME is particularly concerning, as it could disrupt LLMs with only a single edit. In this paper, we study the root causes of such collapse. Through extensive analysis, we identify two primary factors that contribute to the collapse: i) inconsistent handling of prefixed and unprefixed keys in the parameter update equation may result in very small denominators, causing excessively large parameter updates; ii) the subject of collapse cases is usually the first token, whose unprefixed key distribution significantly differs from the prefixed key distribution in autoregressive transformers, causing the aforementioned issue to materialize. To validate our analysis, we propose a simple yet effective approach: uniformly using prefixed keys during editing phase and adding prefixes during the testing phase. The experimental results show that the proposed solution can prevent model collapse while maintaining the effectiveness of the edits",
    "checked": true,
    "id": "cadad3350d18281befc143ba6b71f9fc719ea088",
    "semantic_title": "the fall of rome: understanding the collapse of llms in model editing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kWHsiOMhdb": {
    "title": "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities, but still suffer from inconsistency issues (e.g. LLMs can react differently to disturbances like rephrasing or inconsequential order change). In addition to these inconsistencies, we also observe that LLMs, while capable of solving hard problems, can paradoxically fail at easier ones. To evaluate this hard-to-easy inconsistency, we develop the ConsisEval benchmark, where each entry comprises a pair of questions with a strict order of difficulty. Furthermore, we introduce the concept of consistency score to quantitatively measure this inconsistency and analyze the potential for improvement in consistency by relative consistency score. Based on comprehensive experiments across a variety of existing models, we find: (1) GPT-4 achieves the highest consistency score of 92.2\\% but is still inconsistent to specific questions due to distraction by redundant information, misinterpretation of questions, etc.; (2) models with stronger capabilities typically exhibit higher consistency, but exceptions also exist; (3) hard data enhances consistency for both fine-tuning and in-context learning. Our data and code will be publicly available on GitHub",
    "checked": true,
    "id": "af40586366f32af5f912bfb7fc43d56a34bb1f0b",
    "semantic_title": "can large language models always solve easy problems if they can solve harder ones?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CyNw3aWuRl": {
    "title": "A Radiology-Aware Model-Based Evaluation Metric for Report Generation",
    "volume": "review",
    "abstract": "We propose a novel automated evaluation metric for machine-generated radiology reports using the successful COMET architecture adapted for the radiology domain. We train and publish four medically-oriented model checkpoints, including one trained on RadGraph, a radiology knowledge graph. Our results show that our metric correlates moderately to high with established metrics like BERTscore, BLEU, and CheXbert scores. In addition, we demonstrate that one of our checkpoints exhibits a high correlation with human judgment, as assessed by using the publicly available annotations of six board-certified radiologists using a set of 200 reports. We also conducted our own analysis gathering annotations with two radiologists on a collection of 100 reports. The results indicate the potential effectiveness of our method as a radiology-specific evaluation metric. Code, data, and model checkpoints to reproduce our findings will be publicly available",
    "checked": false,
    "id": "e811df37cf44de1772761a6bf64fb2971362f39c",
    "semantic_title": "radiology-aware model-based evaluation metric for report generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=KyXTjn6s7p": {
    "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
    "volume": "review",
    "abstract": "As large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information has become increasingly essential. For instance, LLMs must be capable of providing confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public or unauthorized entities. In response to this challenge, we propose a novel method termed ''in-context knowledge unleaning'', which enables the model to selectively forget information in real-time based on the context of the query. Our method finetunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving the other knowledge. We also propose a F1-based evaluation metric to assess the performance of in-context knowledge unlearning, balancing the trade-off between unlearning target knowledge and retaining the other knowledge. Experiments conducted on the TOFU and AGE datasets with the Llama2-7B/13B and Mistral-7B models demonstrated that our method achieves scores of 70-80 points on the proposed metric, significantly outperforming the baseline method. Further investigation into the model's internal behavior revealed that while finetuned LLMs generate correct predictions in the middle layers and maintain them up to the final layer, they make the decision to forget at the last layer, i.e., ''LLMs pretend to forget''. Our findings offer valuable insights into enhancing the robustness of unlearning mechanisms in LLMs, setting a foundation for future research in the field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bJkx9uvzcg": {
    "title": "Eliminating Language Bias in Visual Question Answering with Potential Causality Models",
    "volume": "review",
    "abstract": "The main goal of Visual Question Answering (VQA) is to effectively learn useful information from vision and language to perform answer reasoning. However, recent studies have shown that VQA models often have language bias, which is the false correlations between questions and answers, rather than truly extracting answers from multi-modal knowledge. Existing methods mainly focus on modeling the question part to capture the language bias, while ignoring the influence of visual content on the model. To address this issue, in this paper, we combine potential causal models with VQA models, using dual-attention as treatment, and treating language bias as a confounding factor in the model. We enhance the role of visual information in the VQA model through the construction of observed and counterfactual outcomes, thus eliminating the impact of language bias on the VQA model. We conduct experiments on the VQA-CP v2 and VQA v2 datasets to demonstrate the effectiveness of our proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wWiPr1syai": {
    "title": "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?",
    "volume": "review",
    "abstract": "Detecting evidence within the context is a key step in the process of reasoning task. Evaluating and enhancing the capabilities of LLMs in evidence detection will strengthen context-based reasoning performance. This paper proposes a benchmark called DetectBench for verifying the ability to detect and piece together implicit evidence within a long context. DetectBench contains 3,928 multiple-choice questions, with an average of 994 tokens per question. Each question contains an average of 4.55 pieces of implicit evidence, and solving the problem typically requires 7.62 logical jumps to find the correct answer. To enhance the performance of LLMs in evidence detection, this paper proposes Detective Reasoning Prompt and Finetune. Experiments demonstrate that the existing LLMs' abilities to detect evidence in long contexts are far inferior to humans. However, the Detective Reasoning Prompt effectively enhances the capability of powerful LLMs in evidence detection, while the Finetuning method shows significant effects in enhancing the performance of weaker LLMs. Moreover, when the abilities of LLMs in evidence detection are improved, their final reasoning performance is also enhanced accordingly",
    "checked": true,
    "id": "ee52a820e35cf5095e681ccf8ca8eea978d336ad",
    "semantic_title": "detectbench: can large language model detect and piece together implicit evidence?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oiep7TST71": {
    "title": "Mitigating Language Biases In Visual Question Answering Through The Forgotten Attention Algorithm",
    "volume": "review",
    "abstract": "At present, in the field of Visual Question Answering (VQA), a model's ability to comprehend various modalities is crucial for accurate answer reasoning. However, recent studies have uncovered prevailing language biases in VQA, where reasoning frequently relies on incorrect associations between questions and answers, rather than genuine multi-modal knowledge-based reasoning. Thus, it is of great challenge to reveal the accurate relationship between image and question. The key idea of this work is inspired by the process of answering questions of human beings, where people always gradually reduce the focus area in the image with the aid of question information until the final related area is retained. More specifically, we introduce a novel attention algorithm, named the Forgotten Attention Algorithm (FAA), where this algorithm gradually \"forgets\" some visual contents after several rounds. This deliberate forgetting process concentrates the model's \"attention\" on the image region that is the most relevant to the question. As a result, it can enhance the integration of image content and thus mitigate language biases. We conducted comprehensive experiments on the VQA-CP v2, VQA v2, and VQA-VS datasets to validate the efficiency and robustness of the algorithm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OGut4W9k0p": {
    "title": "PropTest: Automatic Property Testing for Improved Visual Programming",
    "volume": "review",
    "abstract": "Visual Programming has recently emerged as an alternative to end-to-end black-box visual reasoning models. This type of method leverages Large Language Models (LLMs) to generate the source code for an executable computer program that solves a given problem. This strategy has the advantage of offering an interpretable reasoning path and does not require finetuning a model with task-specific data. We propose PropTest, a general strategy that improves visual programming by further using an LLM to generate code that tests for visual properties in an initial round of proposed solutions. Our method generates tests for data-type consistency, output syntax, and semantic properties. PropTest achieves comparable results to state-of-the-art methods while using publicly available LLMs. This is demonstrated across different benchmarks on visual question answering and referring expression comprehension. Particularly, PropTest improves ViperGPT by obtaining 46.1\\% accuracy (+6.0\\%) on GQA using Llama3-8B and 59.5\\% (+8.1\\%) on RefCOCO+ using CodeLlama-34B",
    "checked": true,
    "id": "25173b4449c520caeec0bf8c352af146a163f6c5",
    "semantic_title": "proptest: automatic property testing for improved visual programming",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mornWjoqQy": {
    "title": "Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model's Personality",
    "volume": "review",
    "abstract": "In this study, we delve into the validity of conventional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs). Our objective is to assess the congruence between the personality traits LLMs claim to possess and their demonstrated tendencies in real-world scenarios. By conducting an extensive examination of LLM outputs against observed human response patterns, we discover the disjunction between self-knowledge and action in LLMs. And we formulate hypotheses grounded in psychological theories and metrics, offering insights into the intricate mechanisms driving the observed discrepancy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6pDG2o41m6": {
    "title": "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models",
    "volume": "review",
    "abstract": "Mainstream approaches to aligning large language models (LLMs) heavily rely on human preference data, particularly when models require periodic updates. The standard process for iterative alignment of LLMs involves collecting new human feedback for each update. However, the data collection process is costly and challenging to scale. To address this issue, we introduce the \"TS-Align\" framework, which fine-tunes a policy model using pairwise feedback data automatically mined from its outputs. This automatic mining process is efficiently accomplished through the collaboration between a large-scale teacher model and a small-scale student model. The policy fine-tuning process can be iteratively repeated using on-policy generations within our proposed teacher-student collaborative framework. Through extensive experiments, we demonstrate that our final aligned policy outperforms the base policy model with an average win rate of 69.7% across seven conversational or instruction-following datasets. Furthermore, we show that the ranking capability of the teacher is effectively distilled into the student through our pipeline, resulting in a small-scale yet effective reward model for policy model alignment",
    "checked": true,
    "id": "200a19739ef76cb91c490be72d409f0fb0468901",
    "semantic_title": "ts-align: a teacher-student collaborative framework for scalable iterative finetuning of large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=hbPCNXh05e": {
    "title": "A Study of Implicit Ranking Unfairness in Large Language Models",
    "volume": "review",
    "abstract": "Recently, Large Language Models (LLMs) have demonstrated a superior ability to serve as ranking models. However, concerns have arisen as LLMs will exhibit discriminatory ranking behaviors based on users' sensitive attributes (\\eg gender). Worse still, in this paper, we identify a subtler form of discrimination in LLMs, termed \\textit{implicit ranking unfairness}, where LLMs exhibit discriminatory ranking patterns based solely on non-sensitive user profiles, such as user names. Such implicit unfairness is more widespread but less noticeable, threatening the ethical foundation. To comprehensively explore such unfairness, our analysis will focus on three research aspects: (1) We propose an evaluation method to investigate the severity of implicit ranking unfairness. (2) We uncover the reasons for causing such unfairness. (3) To mitigate such unfairness effectively, we utilize a pair-wise regression method to conduct fair-aware data augmentation for LLM fine-tuning. The experiment demonstrates that our method outperforms the existing methods regarding ranking fairness. Lastly, we emphasize the need for the community to identify and mitigate the implicit unfairness, aiming to avert the potential deterioration in the reinforced human-LLMs ecosystem deterioration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AHS2ZfTv4w": {
    "title": "Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models",
    "volume": "review",
    "abstract": "The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents. However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures. Our work introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately. The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values. Furthermore, we enhance the tabular data with contextual understanding using the GPT-3.5-turbo through a one-shot prompt. This enriched data is then added to the retrieval database alongside other PDFs. Our approach aims to significantly improve the accuracy of complex table queries, offering a solution to a longstanding challenge in information retrieval",
    "checked": true,
    "id": "4003b1bb28b3f8463accb3b5d57538708099f0ca",
    "semantic_title": "beyond extraction: contextualising tabular data for efficient summarisation by language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gAcieeAmB6": {
    "title": "From Answers to Questions: A Study on Backward Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "Multi-step reasoning through Chain-of-Thought (CoT) prompting has been extensively explored, observing the abilities of Large Language Models (LLMs) to generate answers from a given question. However, the focus is on forward reasoning abilities manifested in a series of general premises leading to a final solution. This leaves backward reasoning, the inference that leads to the causal hypothesis, unexplored. In this paper, we take the reverse perspective by analyzing the backward reasoning abilities of LLMs by exploring their ability to seek a hypothesis that best fits or explains a set of observations. In particular, we contextualize the hypothesis and observations in Question-answering (QA) tasks. Therefore, by proposing the Hiding and the Blanking approaches that strategically revise the input-problem instances, we analyze whether the LLMs are able to reason about the conclusions and deliver the original question that leads to the final answers. Using three Multiple Choice Questions and six Math Word Problems QA tasks: (i) we observe a performance gap between standard and proposed approaches; hence (ii) we propose several methods to elicit the LLMs to generate the answer by considering the backward direction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QvOAQehQEZ": {
    "title": "COOL: Comprehensive Knowledge Enhanced Prompt Learning for Domain Adaptive Few-shot Fake News Detection",
    "volume": "review",
    "abstract": "Most Fake News Detection (FND) methods often struggle with data scarcity for emerging news domain. Recently, prompt learning based on Pre-trained Language Models (PLM) has emerged as a promising approach in domain adaptive few-shot learning, since it greatly reduces the need for labeled data by bridging the gap between pre-training and downstream task. Furthermore, external knowledge is also helpful in verifying emerging news, as emerging news often involves timely knowledge that may not be contained in the PLM's outdated prior knowledge. To this end, we propose COOL, a Comprehensive knOwledge enhanced prOmpt Learning method for domain adaptive few-shot FND. Specifically, we propose a comprehensive knowledge extraction module to extract both structured and unstructured knowledge that are positively or negatively correlated with news from external sources, and adopt an adversarial contrastive enhanced hybrid prompt learning strategy to model the domain-invariant news-knowledge interaction pattern for FND. Experimental results demonstrate the superiority of COOL over various state-of-the-arts",
    "checked": true,
    "id": "4985fe078030daa8af22d097a89f6636ef76ad20",
    "semantic_title": "cool: comprehensive knowledge enhanced prompt learning for domain adaptive few-shot fake news detection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L4ZuKt44U8": {
    "title": "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs",
    "volume": "review",
    "abstract": "In the age of mobile internet, user data, often referred to as memories, is continuously generated on personal devices. Effectively managing and utilizing this data to deliver services to users is a compelling research topic. In this paper, we introduce a novel task of crafting personalized agents powered by large language models (LLMs), which utilize a user's smartphone memories to enhance downstream applications with advanced LLM capabilities. To achieve this goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach is further optimized using Reinforcement Learning to address three distinct challenges: data collection, editability, and selectability. Extensive experiments on a real-world dataset validate the effectiveness of EMG-RAG, achieving an improvement of approximately 10\\% over the best existing approach. Additionally, the personalized agents have been transferred into a real smartphone AI assistant, which leads to enhanced usability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2zpjKw7vvG": {
    "title": "A Survey of Keyphrase Generation",
    "volume": "review",
    "abstract": "Keyphrase generation refers to the task of producing a set of words or phrases that summarises the content of a document. Continuous efforts have been dedicated to this task over the past few years, spreading across multiple lines of research, such as model architectures, data resources, and use-case scenarios. Yet, the current state of keyphrase generation remains unknown as there has been no attempt to review and analyse previous work. This survey bridges that gap and provides a comprehensive overview of the recent progress, limitations and open challenges in keyphrase generation. Our analysis of over 40 research papers reveals interesting new insights, such as that 1) commonly-used datasets are so similar that there is no practical benefit in using them together for evaluation, or that 2) the performance of many models was significantly overestimated due to the application of normalization procedures in ground truth. This paper not only surveys the literature but also addresses some of these concerns by training, documenting and releasing a strong PLM-based model for keyphrase generation, along with an evaluation framework, as an effort to facilitate future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aUmtVlaPRD": {
    "title": "AgentReview: Exploring Peer Review Dynamics with LLM Agents",
    "volume": "review",
    "abstract": "Peer review is fundamental to the integrity and advancement of scientific publication. Traditional methods of peer review analyses often rely on exploration and statistics of existing peer review data, which do not adequately address the multivariate nature of the process, account for the latent variables, and are further constrained by privacy concerns due to the sensitive nature of the data. We introduce AgentReview, the first large language model (LLM) based peer review simulation framework, which effectively disentangles the impacts of multiple latent factors and addresses the privacy issue. Our study reveals significant insights, including a notable 37.1% variation in paper decisions due to reviewers' biases, supported by sociological theories such as the social influence theory, altruism fatigue, and authority bias. We believe that this study could offer valuable insights to improve the design of peer review mechanisms. Our code and data is at https://anonymous.4open.science/r/agent4reviews/README.md",
    "checked": true,
    "id": "9348b7b95982d0a675a767e92c23647aa6915a94",
    "semantic_title": "agentreview: exploring peer review dynamics with llm agents",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Y851NvWAi9": {
    "title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models",
    "volume": "review",
    "abstract": "Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small language models (SLMs) at downstream clients. However, a significant gap remains in the simultaneous mutual enhancement of both the server's LLM and clients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient federated mutual knowledge transfer framework for large and small language models. This framework is designed to adaptively transfer knowledge from the server's LLM to clients' SLMs while concurrently enriching the LLM with clients' unique domain insights. We facilitate token alignment using minimum edit distance (MinED) and then selective mutual knowledge transfer between client-side SLMs and a server-side LLM, aiming to collectively enhance their performance. Through extensive experiments across three distinct scenarios, we evaluate the effectiveness of FedMKT using various public LLMs and SLMs on a range of NLP text generation tasks. Empirical results demonstrate that FedMKT simultaneously boosts the performance of both LLMs and SLMs",
    "checked": true,
    "id": "3bce09c68236ec61f1bd3cbfacabe45bba8c304f",
    "semantic_title": "fedmkt: federated mutual knowledge transfer for large and small language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=BBDOc0TB7i": {
    "title": "Can ChatGPT understand the implicit meaning of language? Discussion of ChatGPT's ability to generate metaphorical samples",
    "volume": "review",
    "abstract": "The effectiveness of large-scale language modeling (LLM) in generating data samples has been widely proven, especially in question answering and textual entailment tasks. However, these tasks are primarily concerned with surface semantics and usually require the model to learn only information about lexical and syntactic structures. In contrast, generating metaphorical samples requires LLMs to develop a deeper understanding of the implicit meanings in the text. Therefore, the aim of this paper is to explore the ability of ChatGPT to generate metaphorical samples. First, we propose two prompt enhancement methods based on definitions and multiple word meanings. The former introduces a metaphor definition, and the latter requires LLM to generate the corresponding metaphorical or literal sample content based on each word sense. Experimental results show that the SPE method performs slightly lower than manually labeled samples in terms of fine-tuning performance (3.5\\% lower than the average F1 value for the three metaphorical datasets), but at 1/250th the cost of the latter. Since most of work focuses on zero- or few-shot methods, we use it as a baseline. We provide an in-depth discussion of the differences between the four sample generation methods mentioned above through manual evaluation, automated evaluation, and example analysis. To enhance the reliability of the study, we introduce ChatGPT, LLaMA3, and Mixtral to further explore the differences in generating implicit semantic content across LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=galuhSN7am": {
    "title": "FIHA: Fine-grained Hallucinations Evaluations in Large Vision Language Models",
    "volume": "review",
    "abstract": "The rapid development of Large Vision Language Models (LVLMs) often comes with widespread hallucination issues, making cost-effective and comprehensive assessments increasingly vital. Therefore, we introduce the FIHA (Fine-graIned Hallucination evAluation), a multidimensional hallucination evaluation method for LVLMs that is LLM-free and annotation-free. FIHA can generate QA pairs on any image dataset at minimal cost, enabling hallucination assessment from both image and caption. Based on this approach, we introduce a benchmark (FIFA-v1) consisting of diverse questions on various images from MS COCO and Foggy Cityscapes. Furthermore, we use the Davidson Scene Graph (DSG) to organize the structure among QA pairs, in which we can increase reliability of the evaluation. We evaluate representative models using FIHA-v1, highlighting their limitations and challenges. Our code and data can be found here: https://anonymous.4open.science/r/FIHA-45BB",
    "checked": false,
    "id": "05839a68bd05880beef2f171cee7aab960bb6d2f",
    "semantic_title": "hal-eval: a universal and fine-grained hallucination evaluation framework for large vision language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=E1USRd80QC": {
    "title": "A Simple Angle-based Approach for Contrastive Learning of Unsupervised Sentence Representation",
    "volume": "review",
    "abstract": "Contrastive learning has been successfully adopted in the field of VRL (visual representation learning) by constructing contrastive pairs. After that, SRL (sentence representation learning) followed the literature established by the promising baseline SimCSE, which has made notable breakthroughs in unsupervised SRL. However, considering the difference between VRL and SRL, there is a still room for designing a novel contrastive framework specially targeted for SRL. We propose a novel angle-based similarity function for contrastive objective. By examining the gradient of our contrastive objective, we show that the angle-based similarity function provides better property for SRL in terms of training dynamics than the off-the-shelf cosine similarity: (1) effectively pulling a positive instance toward an anchor instance in the early stage of training and (2) not excessively repelling the false negative instance during the middle of training. Our experimental results on widely-utilized benchmarks demonstrate the effectiveness and extensibility of our novel angle-based approach, and further analyses also back up the reason for its better sentence representation power",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Trn5nysipv": {
    "title": "ArMeme: Propagandistic Content in Arabic Memes",
    "volume": "review",
    "abstract": "With the rise of digital communication memes have become a significant medium for cultural and political expression that is often used to mislead audience. Identification of such misleading and persuasive multimodal content become more important among various stakeholders, including social media platforms, policymakers, and the broader society as they often cause harm to the individuals, organizations and/or society. While there has been effort to develop AI based automatic system for resource rich languages (e.g., English), it is relatively little to none for medium to low resource languages. In this study, we focused on developing an Arabic memes dataset with manual annotations of propagandistic content. We annotated $\\sim6K$ Arabic memes collected from various social media platforms, which is a first resource for Arabic multimodal research. We provide a comprehensive analysis aiming to develop computational tools for their detection. We will make them publicly available for the community",
    "checked": true,
    "id": "916ed0f604561033d4df141388b691b4d4e65c60",
    "semantic_title": "armeme: propagandistic content in arabic memes",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=xg2qWLEpJ7": {
    "title": "FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback",
    "volume": "review",
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated proficiency in tackling a variety of visual-language tasks. However, current LVLMs suffer from misalignment between text and image modalities which causes three kinds of hallucination problems, i.e., object existence, object attribute, and object relationship. To tackle this issue, existing methods mainly utilize Reinforcement Learning (RL) to align modalities in LVLMs. However, they still suffer from three main limitations: (1) General feedback can not indicate the hallucination type contained in the response; (2) Sparse rewards only give the sequence-level reward for the whole response; and (3)Annotation cost is time-consuming and labor-intensive. To handle these limitations, we propose an innovative method to align modalities in LVLMs through \\textbf{F}ine-\\textbf{G}rained \\textbf{A}rtificial \\textbf{I}ntelligence \\textbf{F}eedback (\\textbf{\\ours}), which mainly consists of three steps: AI-based Feedback Collection, Fine-grained Reward Model Training, and Reinforcement Learning with Fine-grained Reward. Finally, a novel fine-grained feedback module is integrated into the Proximal Policy Optimization (PPO) algorithm. Extensive experiments are conducted on hallucination and general benchmarks, demonstrating the superior performance of our proposed method. Notably, compared with previous models trained with the RL-based aligning method, our proposed method is effective even with fewer parameters",
    "checked": true,
    "id": "fa84ef486184eb1c3d63949b700342bbcaf7b0c7",
    "semantic_title": "fgaif: aligning large vision-language models with fine-grained ai feedback",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=mMB3iPeN6d": {
    "title": "FaithScore: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models",
    "volume": "review",
    "abstract": "We introduce FAITHSCORE (Faithfulness to Atomic Image Facts Score), a reference-free and fine-grained evaluation metric that measures the faithfulness of the generated free-form answers from large vision-language models (LVLMs). The FAITHSCORE evaluation first identifies sub-sentences containing descriptive statements that need to be verified, then extracts a comprehensive list of atomic facts from these sub-sentences, and finally conducts consistency verification between fine-grained atomic facts and the input image. Meta-evaluation demonstrates that our metric highly correlates with human judgments of faithfulness. We collect two benchmark datasets (i.e. LLaVA-1k and MSCOCO-Cap) for evaluating LVLMs instruction-following hallucinations. We measure hallucinations in state-of-the-art LVLMs with FAITHSCORE on the datasets. Results reveal that current systems are prone to generate hallucinated content unfaithful to the image, which leaves room for future improvements. We hope our metric FAITHSCORE can help evaluate future LVLMs in terms of faithfulness and provide insightful advice for enhancing LVLMs' faithfulness",
    "checked": false,
    "id": "fbae34c21a6a0cbf3f9e2710b7fce0e011aec72c",
    "semantic_title": "faithscore: evaluating hallucinations in large vision-language models",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=cw3c4TcGLA": {
    "title": "Partisan Opinions, but Common Language: Similarities in Topic Use by Appellate Judges",
    "volume": "review",
    "abstract": "As the final word on thousands of legal matters each year, appellate courts make some of the most impactful decisions in modern society. Understanding partisan behavior by their judges is therefore critical for the rule of law. However, judicial language is technical, making partisanship challenging to objectively measure and creating a unique opportunity for natural language processing. Using fine-tuned language embeddings from transformer models, we leverage the random assignment of individual judges to three-judge panels, and of those panels to cases, to causally estimate how discussion of legal topics on U.S. appellate courts differs across partisan environments. We show that while Democratic judges write more dispersed opinions, judges of both parties agree on average about the important topics in each legal case. Further, we demonstrate that mandatory bipartisanship does not reduce the range of topics considered. Judicial partisanship is thus driven by disagreements within legal issues rather than disputes about which issues apply. These results provide a clearer understanding of the structure of judicial language and open new directions for natural language processing research and impact",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xUxU1YUKbz": {
    "title": "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs",
    "volume": "review",
    "abstract": "Evaluating the bias of LLMs becomes more crucial with their rapid development. However, existing evaluation approaches rely on fixed-form outputs and cannot adapt to the flexible open-text generation scenarios of LLMs (e.g., sentence completion and question answering). To address this, we introduce BiasAlert, a plug-and-play tool designed to detect social bias in open-text generations of LLMs. BiasAlert integrates external human knowledge with its inherent reasoning capabilities to detect bias reliably. Extensive experiments demonstrate that BiasAlert significantly outperforms existing state-of-the-art methods like GPT-4-as-Judge in detecting bias. Furthermore, through application studies, we showcase the utility of BiasAlert in reliable LLM fairness evaluation and bias mitigation across various scenarios. Model and code will be publicly released",
    "checked": true,
    "id": "3b6d50e2f447b3f6b38c171686433b4d6e8702db",
    "semantic_title": "biasalert: a plug-and-play tool for social bias detection in llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5UKVBsEOWa": {
    "title": "Language Models Know the Value of Numbers",
    "volume": "review",
    "abstract": "Large language models (LLMs) have exhibited impressive competence in various tasks, but their internal mechanisms on mathematical problems are still under-explored. In this paper, we study a fundamental question: whether language models know the value of numbers, a basic element in math. To study the question, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states. Experimental results support the existence of encoded number values in LLMs on different layers, and these values can be extracted via linear probes. Further experiments show that LLMs store their calculation results in a similar manner, and we can intervene the output via simple vector additions, proving the causal connection between encoded numbers and language model outputs. Our research provides evidence that LLMs know the value of numbers, thus offering insights for better exploring, designing, and utilizing numeric information in LLMs",
    "checked": true,
    "id": "55e4727caca2d35a76c97c27a4a74d0774f456be",
    "semantic_title": "language models know the value of numbers",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zpjZl6wSQO": {
    "title": "Red Teaming Language Models for Processing Contradictory Dialogues",
    "volume": "review",
    "abstract": "Most language models currently available are prone to self-contradiction during dialogues. To mitigate this issue, this study explores a novel contradictory dialogue processing task that aims to detect and modify contradictory statements in a conversation. This task is inspired by research on context faithfulness and dialogue comprehension, which have demonstrated that the detection and understanding of contradictions often necessitate detailed explanations. We develop a dataset comprising contradictory dialogues, in which one side of the conversation contradicts itself. Each dialogue is accompanied by an explanatory label that highlights the location and details of the contradiction. With this dataset, we present a Red Teaming framework for contradictory dialogue processing. The framework detects and attempts to explain the dialogue, then modifies the existing contradictory content using the explanation. Our experiments demonstrate that the framework improves the ability to detect contradictory dialogues and provides valid explanations. Additionally, it showcases distinct capabilities for modifying such dialogues. Our study highlights the importance of the logical inconsistency problem in conversational AI",
    "checked": false,
    "id": "36c40921cdc4992479a1451aac3a8f98d4b826c2",
    "semantic_title": "red teaming language models for contradictory dialogues",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FM6xTzjGtA": {
    "title": "NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models",
    "volume": "review",
    "abstract": "The considerable size of Large Language Models (LLMs) presents notable deployment challenges, particularly on resource-constrained hardware. Structured pruning, offers an effective means to compress LLMs, thereby reducing storage costs and enhancing inference speed for more efficient utilization. In this work, we study data-efficient and resource-efficient structure pruning methods to obtain smaller yet still powerful models. Knowledge Distillation is well-suited for pruning, as the intact model can serve as an excellent teacher for pruned students. However, it becomes challenging in the context of LLMs due to memory constraints. To address this, we propose an efficient progressive Numerous-teacher pruning method (NutePrune). NutePrune mitigates excessive memory costs by loading only one intact model and integrating it with various masks and LoRA modules, enabling it to seamlessly switch between teacher and student roles. This approach allows us to leverage numerous teachers with varying capacities to progressively guide the pruned model, enhancing overall performance. Extensive experiments across various tasks demonstrate the effectiveness of NutePrune. In LLaMA-7B zero-shot experiments, NutePrune retains 97.17% of the performance of the original model at 20% sparsity and 95.07% at 25% sparsity",
    "checked": true,
    "id": "9495f0f50ef9a3225e8e230746a8f139d27ce8cc",
    "semantic_title": "nuteprune: efficient progressive pruning with numerous teachers for large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=bvKg3wNxkY": {
    "title": "Language Model Can Do Knowledge Tracing: Simple but Effective Method to Integrate Language Model and Knowledge Tracing Task",
    "volume": "review",
    "abstract": "Knowledge Tracing (KT) is a critical task in online learning for modeling student knowledge over time. Despite the success of deep learning-based KT models, which rely on sequences of numbers as data, most existing approaches fail to leverage the rich semantic information in the text of questions and concepts. This paper proposes Language model-based Knowledge Tracing (LKT), a novel framework that integrates pre-trained language models (PLMs) with KT methods. By leveraging the power of language models to capture semantic representations, LKT effectively incorporates textual information and significantly outperforms previous KT models on large benchmark datasets. Moreover, we demonstrate that LKT can effectively address the cold-start problem in KT by leveraging the semantic knowledge captured by PLMs. Interpretability of LKT is enhanced compared to traditional KT models due to its use of text-rich data. We conducted the local interpretable model-agnostic explanation technique and analysis of attention scores to interpret the model performance further. Our work highlights the potential of integrating PLMs with KT and paves the way for future research in KT domain",
    "checked": true,
    "id": "0d6a63cce33bc83f56fa3145dbd120a8099358df",
    "semantic_title": "language model can do knowledge tracing: simple but effective method to integrate language model and knowledge tracing task",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=erbUEhQbtl": {
    "title": "InstructERC: Reforming Emotion Recognition in Conversation with a Multi-task Retrieval-based LLMs Framework",
    "volume": "review",
    "abstract": "The field of emotion recognition of conversation (ERC) has been focusing on separating sentence feature encoding and context modeling, lacking exploration in generative paradigms based on unified designs. In this study, we propose a novel approach, InstructERC, to reformulate the ERC task from a discriminative framework to a generative framework based on Large Language Models (LLMs). InstructERC makes three significant contributions: (1) it introduces a simple yet effective retrieval template module, which helps the model explicitly integrate multi-granularity dialogue supervision information. (2) We introduce two additional emotion alignment tasks, namely speaker identification and emotion prediction tasks, to implicitly model the dialogue role relationships and future emotional tendencies in conversations. (3) Pioneeringly, we unify emotion labels across benchmarks through the feeling wheel to fit real application scenarios. InstructERC still performs impressively on this unified dataset. Our LLM-based plugin framework significantly outperforms all previous models and achieves comprehensive SOTA on three commonly used ERC datasets. Extensive analysis of parameter-efficient and data-scaling experiments provides empirical guidance for applying it in practical scenarios. Our code and aligned unified dataset (UIME) can be found in the supplementary material",
    "checked": true,
    "id": "2acac242305d6496340956cf72264102985c983b",
    "semantic_title": "instructerc: reforming emotion recognition in conversation with a multi-task retrieval-based llms framework",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yGmTaA8aWu": {
    "title": "Humans or LLMs as the Judge? A Study on Judgement Bias",
    "volume": "review",
    "abstract": "Adopting human and large language models (LLM) as judges (*a.k.a* human- and LLM-as-a-judge) for evaluating the performance of LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLMs, questioning the reliability of the evaluation results. In this paper, we propose a novel framework that is free from referencing groundtruth annotations for investigating **Misinformation Oversight Bias**, **Gender Bias**, **Authority Bias** and **Beauty Bias** on LLM and human judges. We curate a dataset referring to the revised Bloom's Taxonomy and conduct thousands of evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the cutting-edge judges possess considerable biases. We further exploit these biases to conduct attacks on LLM judges. We hope that our work can notify the community of the bias and vulnerability of human- and LLM-as-a-judge, as well as the urgency of developing robust evaluation systems",
    "checked": false,
    "id": "a28071c63963cc59ba500cd00c140ac08eb5ccb0",
    "semantic_title": "humans or llms as the judge? a study on judgement biases",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=5DcsbWHY0O": {
    "title": "CoT-Planner: Chain-of-Thoughts as the Content Planner for Few-shot Table-to-Text Generation Reduces the Hallucinations from LLMs",
    "volume": "review",
    "abstract": "Few-shot table-to-text generation seeks to generate natural language descriptions for the given table in low-resource scenarios. Previous works mostly utilized Pre-trained Language Models (PLMs) and even Large Language Models (LLMs) to generate fluent descriptions of the tables. However, they are prone to hallucinations that do not conform to the table. In this work, we propose CoT-Planner, a simple but efficient Chain-of-Thoughts-based approach that can be used to reduce the generation of hallucinations in the few-shot table-to-text generation. We first use a large language model (such as ChatGPT) to automatically generate ten intermediate content plans in the form of a Chain-of-Thoughts (CoT) for each table and corresponding description pair. Then, we refined the most accurate content plan for each sample and used the table and text pairs with the added content plan (CoT-Plan) as demonstrations for In-Context Learning (ICL). Both automatic and human evaluations on the numericNLG dataset show our method can effectively alleviate hallucinations, thereby improving factual consistency in few-shot table-to-text generation. The code and data will be released upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1V9iGRN8hk": {
    "title": "Novel-WD: Exploring acquisition of Novel World Knowledge in LLMs Using Prefix-Tuning",
    "volume": "review",
    "abstract": "Teaching new information to pre-trained large language models (PLM) is a crucial but challenging task. Model adaptation techniques, such as fine-tuning and parameter-efficient training have been shown to store new facts at a slow rate; continual learning is an option but is costly and prone to catastrophic forgetting. This work studies and quantifies how PLM may learn and remember new world knowledge facts that do not occur in their pre-training corpus, which only contains world knowledge up to a certain date. To that purpose, we first propose Novel-WD, a new dataset consisting of sentences containing novel facts extracted from recent Wikidata updates, along with two evaluation tasks in the form of causal language modeling and multiple choice questions (MCQ). We make this dataset freely available to the community, and release a procedure to later build new versions of similar datasets with up-to-date information. We also explore the use of prefix-tuning for novel information learning, and analyze how much information can be stored within a given prefix. We show that a single fact can reliably be encoded within a single prefix, and that the prefix capacity increases with its length and with the base model size",
    "checked": true,
    "id": "fa2d30eb1d9da515388b52c6bcb385aa66341421",
    "semantic_title": "novel-wd: exploring acquisition of novel world knowledge in llms using prefix-tuning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jlaa645kLo": {
    "title": "BAPO: Base-Anchored Preference Optimization for Personalized Alignment in LLMs",
    "volume": "review",
    "abstract": "While learning to align Large Language Models (LLMs) with human preferences has shown remarkable success, aligning these models to meet the diverse user preferences presents further challenges in preserving previous knowledge. This paper examines the impact of personalized preference optimization on LLMs, revealing that the extent of knowledge loss varies significantly with preference heterogeneity. Although previous approaches have utilized the KL constraint between the reference model and the policy model, we observe that they fail to maintain general knowledge and alignment when facing personalized preferences. To this end, we introduce Base-Anchored Preference Optimization (BAPO), a simple yet effective approach that utilizes the initial responses of reference model to mitigate forgetting while accommodating personalized alignment. BAPO effectively adapts to diverse user preferences while minimally affecting global knowledge or general alignment. Our experiments demonstrate the efficacy of BAPO in various setups",
    "checked": false,
    "id": "d1eeba0dfd9a7d60bd390ed5bcfbefe6ff452d17",
    "semantic_title": "bapo: base-anchored preference optimization for personalized alignment in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aJVwhHwH3G": {
    "title": "FANNO: Augmenting High-Quality Instruction Data with Open-Sourced LLMs Only",
    "volume": "review",
    "abstract": "Instruction fine-tuning stands as a crucial advancement in leveraging large language models (LLMs) for enhanced task performance. However, the annotation of instruction datasets has traditionally been an expensive and laborious process, often reliant on manual annotations or costly API calls of proprietary LLMs. To address these challenges, we introduce Fanno, a fully autonomous, open-sourced framework that revolutionizes the annotation process without the need for pre-existing annotated data. Utilizing a Mistral-7b-instruct model, Fanno efficiently produces diverse and high-quality datasets through a structured process involving document pre-screening, instruction generation, response generation. Experiments on Open LLM Leaderboard and AlpacaEval benchmark show that the Fanno can generate high-quality data with diversity and complexity for free, comparable to human-annotated or cleaned datasets like Alpaca-GPT4-Cleaned",
    "checked": true,
    "id": "2d7776390b61ef860cd51136e41380bcce017825",
    "semantic_title": "fanno: augmenting high-quality instruction data with open-sourced llms only",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4y4EBySL3E": {
    "title": "On Unsupervised Comparisons of Large Language Models",
    "volume": "review",
    "abstract": "Evaluation of LLMs has primarily relied on comparing against \"gold\" answers that often takes months or years to conduct and hence is difficult to scale. Instead of harnessing these supervised approaches that aim to rank LLMs, we propose to assess models by measuring and identifying the significance of their differences. This reduces the difficult supervised learning into an unsupervised task that saves the substantial labeling costs. More specifically, we introduce the notion of topic-categorized distinguisher questions that expose key behavioral differences and hence define distances between LLMs. We design a suite of algorithmic techniques for finding these distinguishers and make three major innovations, including (i) a new correlation specification on objective functions based on topic trees and earth-mover distance of topics, (ii) a theoretically sound embedding technique between EMD induced by topics and $\\ell_2$-space used in Bayesian optimization (BO), and (iii) a Siamese-net based model leveraging our theoretical results that effectively interface topics and BO in practice. Our experiments showed the efficacy of our new algorithms, its power to distinguish LLMs in medical topics, and its application in unsupervised ranking",
    "checked": false,
    "id": "83d6ae4c431cd082be95fe621e3fd7010052c6bb",
    "semantic_title": "unsupervised tokenization learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=ymjVhUuZFU": {
    "title": "GRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients",
    "volume": "review",
    "abstract": "Large language model (LLM) training and finetuning are often bottlenecked by limited GPU memory. While existing projection-based optimization methods address this by projecting gradients into a lower-dimensional subspace to reduce optimizer state memory, they typically rely on \\textit{dense} projection matrices, which can introduce computational and memory overheads. In this work, we propose \\textsc{Grass} (GRAdient Stuctured Sparsification), a novel approach that leverages \\textit{sparse} projections to transform gradients into structured sparse updates. This design not only significantly reduces memory usage for optimizer states but also minimizes gradient memory footprint, computation, and communication costs, leading to substantial throughput improvements. Extensive experiments on pretraining and finetuning tasks demonstrate that \\textsc{Grass} achieves comparable performance to full-rank training and existing projection-based methods. Notably, \\textsc{Grass} enables half-precision pretraining of a 13B parameter LLaMA model on a single 40GB A100 GPU---a feat infeasible for previous methods---and yields up to a $2\\times$ throughput improvement on an 8-GPU system",
    "checked": true,
    "id": "0c909ef8b889dcf751fde42aa9ef97ff7a619232",
    "semantic_title": "grass: compute efficient low-memory llm training with structured sparse gradients",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sa20JogM3C": {
    "title": "Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation",
    "volume": "review",
    "abstract": "Hallucinated translations pose significant threats and safety concerns when it comes to practical deployment of machine translation systems. Previous research works have identified that detectors exhibit complementary performance --- different detectors excel at detecting different types of hallucinations. In this paper, we propose to address the limitations of individual detectors by combining them and introducing a straightforward method for aggregating multiple detectors. Our results demonstrate the efficacy of our aggregated detector, providing a promising step towards evermore reliable machine translation systems",
    "checked": true,
    "id": "130c2241efd4f455f55c570a4f8ba07dd4207e9d",
    "semantic_title": "enhanced hallucination detection in neural machine translation through simple detector aggregation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=RZIWfoI5dj": {
    "title": "Scenarios and Approaches for Situated Natural Language Explanations",
    "volume": "review",
    "abstract": "Large language models (LLMs) can be used to generate natural language explanations (NLE) that are adapted to different users' situations. However, there is yet to be a quantitative evaluation of the extent of such adaptation. To bridge this gap, we collect a benchmarking dataset, Situation-Based Explanation. This dataset contains 100 explanandums. Each explanandum is paired with explanations targeted at three distinct audience types-such as educators, students, and professionals-enabling us to assess how well the explanations meet the specific informational needs and contexts of these diverse groups e.g. students, teachers, and parents. For each \"explanandum paired with an audience\" situation, we include a human-written explanation. These allow us to compute scores that quantify how the LLMs adapt the explanations to the situations. On an array of pretrained language models with varying sizes, we examine three categories of prompting methods: rule-based prompting, meta-prompting, and in-context learning prompting. We find that 1) language models can generate prompts that result in explanations more precisely aligned with the target situations, 2) explicitly modeling an \"assistant\" persona by prompting \"You are a helpful assistant...\" is not a necessary prompt technique for situated NLE tasks, and 3) the in-context learning prompts only can help LLMs learn the demonstration template but can't improve their inference performance. SBE and our analysis facilitate future research towards generating situated natural language explanations",
    "checked": true,
    "id": "56ea073666fac3c6bc79657baafcc6ede1cc4edd",
    "semantic_title": "scenarios and approaches for situated natural language explanations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Cj5OXD4kK": {
    "title": "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks",
    "volume": "review",
    "abstract": "The evaluation of natural language processing (NLP) systems is crucial for advancing the field, but current benchmarking approaches often assume that all systems have scores available for all tasks, which is not always practical. In reality, several factors such as the cost of running baseline, private systems, computational limitations, or incomplete data may prevent some systems from being evaluated on entire tasks. This paper formalize an existing problem in NLP research: benchmarking when some systems scores are missing on the task, and proposes a novel approach to address it. Our method utilizes a compatible partial ranking approach to impute missing data, which is then aggregated using the Borda count method. It includes two refinements designed specifically for scenarios where either task-level or instance-level scores are available. We also introduce an extended benchmark, which contains over 131 million scores, an order of magnitude larger than existing benchmarks. We validate our methods and demonstrate their effectiveness in addressing the challenge of missing system evaluation on an entire task. This work highlights the need for more comprehensive benchmarking approaches that can handle real-world scenarios where not all systems are evaluated on the entire task",
    "checked": true,
    "id": "a43438b65660e69f7c7341b5f3ced15d5ac98c8d",
    "semantic_title": "towards more robust nlp system evaluation: handling missing scores in benchmarks",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=4ko0TkYZuu": {
    "title": "What Affects the Stability of Tool Learning? An Empirical Study on the Robustness of Tool Learning Frameworks",
    "volume": "review",
    "abstract": "Tool learning methods have enhanced the ability of large language models (LLMs) to interact with real-world applications. Many existing works fine-tune LLMs or design prompts to enable LLMs to select appropriate tools and correctly invoke them to meet user requirements. However, it is observed in previous works that the performance of tool learning varies from tasks, datasets, training settings, and algorithms. Without understanding the impact of these factors, it can lead to inconsistent results, inefficient model deployment, and suboptimal tool utilization, ultimately hindering the practical integration and scalability of LLMs in real-world scenarios. Therefore, in this paper, we explore the impact of both internal and external factors on the performance of tool learning frameworks. Through extensive experiments on two benchmark datasets, we find several insightful conclusions for future work, including the observation that LLMs can benefit significantly from increased trial and exploration. We believe our empirical study provides a new perspective for future tool learning research",
    "checked": true,
    "id": "3f2e12ec7080507189108e2f2b71d7a12f8deec5",
    "semantic_title": "what affects the stability of tool learning? an empirical study on the robustness of tool learning frameworks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PHgyifqXoT": {
    "title": "Pre-trained Semantic Interaction based Inductive Graph Neural Networks for Text Classification",
    "volume": "review",
    "abstract": "Nowadays, research of text classification based on graph neural networks (GNNs) is on the rise. Both inductive methods and transductive methods have made significant progress. For transductive methods, the semantic interaction between texts plays a crucial role in the learning of effective text representations. However, it is difficult to perform inductive learning while modeling interactions between texts on the graph. To give a universal solution, we propose the graph neural network based on pre-trained semantic interaction called PaSIG. Firstly, we construct a text-word heterogeneity graph and design an asymmetric structure to ensure one-way message passing from words to the test texts. Meanwhile, we use the context representation capability of the pre-trained language model to construct node features that contain classification semantic information. Afterward, we explore the adaptation methods of PaSIG with different GNN components. Extensive experiments on five datasets have shown the effectiveness of PaSIG, with the accuracy exceeding the baseline by 5.2\\% on average. While achieving state-of-the-art performance, we have also taken measures such as subgraph sampling and intermediate state preservation to achieve fast inference",
    "checked": false,
    "id": "56fac0ced28ad22958da067795c5abfe5f85fce9",
    "semantic_title": "induct-gcn: inductive graph convolutional networks for text classification",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=P8k99b1Nus": {
    "title": "Advancing Semantic Textual Similarity Modeling: A Regression Framework with Translated ReLU and Smooth K2 Loss",
    "volume": "review",
    "abstract": "Since the introduction of BERT and RoBERTa, research on Semantic Textual Similarity (STS) has made groundbreaking progress. Particularly, the adoption of contrastive learning has substantially elevated state-of-the-art performance across various STS benchmarks. However, contrastive learning categorizes text pairs as either semantically similar or dissimilar, failing to leverage fine-grained annotated information and necessitating large batch sizes to prevent model collapse. These constraints pose challenges for researchers engaged in STS tasks that require nuanced similarity levels or those with limited computational resources, compelling them to explore alternatives like Sentence-BERT. Nonetheless, Sentence-BERT tackles STS tasks from a classification perspective, overlooking the progressive nature of semantic relationships, which results in suboptimal performance. To bridge this gap, this paper presents an innovative regression framework and proposes two simple yet effective loss functions: Translated ReLU and Smooth K2 Loss. Experimental analyses demonstrate that our method achieves convincing performance across seven established STS benchmarks, especially when supplemented with task-specific training data",
    "checked": true,
    "id": "48967585b4334f112f023619daff33f3bc16e4b4",
    "semantic_title": "advancing semantic textual similarity modeling: a regression framework with translated relu and smooth k2 loss",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=UvDdkmg14w": {
    "title": "A Thorough Examination of Decoding Methods in the Era of LLMs",
    "volume": "review",
    "abstract": "Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining optimal results and the practicality of implementation in varying contexts",
    "checked": true,
    "id": "7428003193db96ef573155c3e3d3daf3e361a048",
    "semantic_title": "a thorough examination of decoding methods in the era of llms",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=MpXSpER30w": {
    "title": "EM-LoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning",
    "volume": "review",
    "abstract": "Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are highly effective parameter-efficient fine-tuning (PEFT) methods. However, they introduce significant latency in multi-tenant settings due to the LoRA modules (and MOE routers) added to multiple linear modules in the Transformer layer. To address this issue, we propose Efficient Mixture of Low-Rank Adaptation (EM-LoRA), a novel LoRA variant. EM-LoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism. This mechanism calculates expert routing results once before generating the first new token and reuses these results for subsequent tokens, reducing latency. Extensive experiments and analysis on commonsense reasoning tasks, math reasoning tasks, and widely used LLM evaluation benchmarks demonstrate that EM-LoRA consistently outperforms strong PEFT baselines with comparable tunable parameter budgets. Additionally, EM-LoRA significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.\\footnote{Codes and fine-tuned models will be open-sourced to facilitate future research. }",
    "checked": false,
    "id": "2ca52ea75fc02a3f08988794241cad9e30444db4",
    "semantic_title": "expressive and generalizable low-rank adaptation for large models via slow cascaded learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nw7pRpYvNH": {
    "title": "Towards Uncovering How Large Language Models Work: An Interpretability Perspective",
    "volume": "review",
    "abstract": "Large language models (LLMs) have led to breakthroughs in language tasks, yet the internal mechanisms that enable their remarkable generalization and reasoning abilities remain opaque. This lack of transparency presents challenges such as hallucinations, toxicity, and misalignment with human values, hindering the safe and beneficial deployment of LLMs. This survey paper aims to uncover the internal working mechanisms underlying LLM functionality through the lens of explainability. First, we review how knowledge is encoded within LLMs via mechanistic interpretability techniques. Then, we summarize how knowledge is embedded in LLM representations by leveraging probing techniques and representation engineering. Additionally, we investigate the training dynamics through a mechanistic perspective to explain phenomena such as grokking and memorization. Lastly, we explore how the insights gained from these explanations can enhance LLM performance through model editing, improve efficiency through pruning, and better align with human values",
    "checked": false,
    "id": "4f60009234e76f9f8969f6cca23b3b07e944e984",
    "semantic_title": "towards uncovering how large language model works: an explainability perspective",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=AyqtlIFwrx": {
    "title": "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models",
    "volume": "review",
    "abstract": "Recent developments in large speech foundation models like Whisper have led to their widespread use in many automatic speech recognition (ASR) applications. These systems incorporate 'special tokens' in their vocabulary, such as $\\texttt{<|endoftext|>}$, to guide their language generation process. However, we demonstrate that these tokens can be exploited by adversarial attacks to manipulate the model's behavior. We propose a simple yet effective method to learn a universal acoustic realization of Whisper's $\\texttt{<|endoftext|>}$ token, which, when prepended to any speech signal, encourages the model to ignore the speech and only transcribe the special token, effectively 'muting' the model. Our experiments demonstrate that the same, universal 0.64-second adversarial audio segment can successfully mute a target Whisper ASR model for over 97\\% of speech samples. Moreover, we find that this universal adversarial audio segment often transfers to new datasets and tasks. Overall this work demonstrates the vulnerability of Whisper models to `muting' adversarial attacks, where such attacks can pose both risks and potential benefits in real-world settings: for example the attack can be used to bypass speech moderation systems, or conversely the attack can also be used to protect private speech data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jaszo5zZM8": {
    "title": "Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) showcase remarkable abilities, yet they struggle with limitations such as hallucinations, outdated knowledge, opacity, and inexplicable reasoning. To address these challenges, Retrieval-Augmented Generation (RAG) has proven to be a viable solution, leveraging external databases to improve the consistency and coherence of generated content, especially valuable for complex, knowledge-rich tasks, and facilitates continuous improvement by leveraging domain-specific insights. However, RAG is not without its limitations, including a limited context window, irrelevant information, and the high processing overhead for extensive contextual data. In this comprehensive work, we explore the evolution of Contextual Compression paradigms, providing an in-depth examination of the field. We also introduce a state-of-the-art evaluation framework and benchmark. Finally, we outline the current challenges and suggest potential research and development directions, paving the way for future advancements in this area",
    "checked": false,
    "id": "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5",
    "semantic_title": "retrieval-augmented generation for large language models: a survey",
    "citation_count": 476,
    "authors": []
  },
  "https://openreview.net/forum?id=wzYBwxGBSI": {
    "title": "S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs",
    "volume": "review",
    "abstract": "Speculative decoding (SD) has attracted a significant amount of research attention due to the substantial speedup it can achieve for LLM inference. However, speculative decoding methods often achieve optimal performance on high-end devices or with a substantial GPU memory overhead. Given limited memory and the necessity of quantization, a high-performing SD model on a high-end GPU can slow down by up to 7 times. To this end, we propose Skippy Simultaneous Speculative Decoding (or S3D), a cost-effective self-speculative SD method based on simultaneous multi-token decoding and mid-layer skipping. When compared against recent effective open-source SD systems, our method has achieved one of the top performance-memory ratios while requiring minimal architecture changes and training data. Leveraging our memory efficiency, we created a smaller yet more effective SD model based on Phi-3. It is 1.4 to 2 times faster than the quantized EAGLE model and operates in half-precision while using less VRAM",
    "checked": true,
    "id": "0496315e85fbc7cb5c46afc049981d0c50c962cb",
    "semantic_title": "s3d: a simple and cost-effective self-speculative decoding scheme for low-memory gpus",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=zBh9qLz1f6": {
    "title": "Formal Semantic Geometry over Transformer-based Variational AutoEncoder",
    "volume": "review",
    "abstract": "Formal/symbolic semantics can provide canonical, rigid controllability and interpretability to sentence representations due to their \\textit{localisation} or \\textit{composition} property. How can we deliver such property to the current distributional sentence representations to control and interpret the generation of language models (LMs)? In this work, we theoretically frame the sentence semantics as the composition of \\textit{semantic role - word content} features and propose the formal semantic geometrical framework. To induce the formal semantic geometry into Transformer-based LMs (i.e. GPT2), we deploy Transformer-based Variational AutoEncoder with a supervision approach, where the sentence generation can be manipulated and explained over low-dimensional latent Gaussian space. In addition, we propose a new probing algorithm to guide the movement of sentence vectors over such geometry. Experimental results reveal that the formal semantic geometry can potentially deliver better control and interpretation to sentence generation",
    "checked": true,
    "id": "d8c6236a216a9b8d5280e80a74049e8f389c5adb",
    "semantic_title": "formal semantic geometry over transformer-based variational autoencoder",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=5FdJBSMLo3": {
    "title": "LLMs Cannot (Yet) Match the Specificity and Simplicity of Online Communities in Long Form Question Answering",
    "volume": "review",
    "abstract": "Retail investing is on the rise, and users are more dependent on rapidly growing finance-related online communities than ever to educate themselves. However, recent years have positioned Large Language Models (LLMs) as powerful question-answering (QA) tools, shifting users away from interacting in online communities towards discourse with a AI-driven chat interfaces. These AI-driven tools are currently limited by the availability of labelled data and the challenge of capturing financial knowledge. Therefore, in this work, we curate a QA preference dataset SocialFinanceQA for fine-tuning and aligning LLMs, extracted from more than 7.4 million submissions and 82 million comments from 2008 to 2022 in Reddit's 15 largest finance communities. Additionally, we propose a novel framework SocialQA-Eval as a generally-applicable method to evaluate generated QA responses. We evaluate various LLMs fine-tuned on this dataset, using traditional metrics, LLM-based evaluation, and human annotation -- our results demonstrate the value of high-quality Reddit data, with even state-of-the-art LLMs improving on simpler and more specific responses. We contribute to the field with our domain-specific dataset, a range of detailed experiments with a comprehensive and generalizable evaluation, and a",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mSgN1QQUBr": {
    "title": "Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning",
    "volume": "review",
    "abstract": "Model editing aims to correct outdated or erroneous knowledge in large language models (LLMs) without the need for costly retraining. Lifelong model editing is the most challenging task that caters to the continuous editing requirements of LLMs. Prior works primarily focus on single or batch editing; nevertheless, these methods fall short in lifelong editing scenarios due to catastrophic knowledge forgetting and the degradation of model performance. Although retrieval-based methods alleviate these issues, they are impeded by slow and cumbersome processes of integrating the retrieved knowledge into the model. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous Prompt lEarning method, to boost editing efficacy and inference efficiency in lifelong learning. RECIPE first converts knowledge statements into short and informative continuous prompts, prefixed to the LLM's input query embedding, to efficiently refine the response grounded on the knowledge. It further integrates the Knowledge Sentinel (KS) that acts as an intermediary to calculate a dynamic threshold, determining whether the retrieval repository contains relevant knowledge. Our retriever and prompt encoder are jointly trained to achieve editing properties, i.e., reliability, generality, and locality. In our experiments, RECIPE is assessed extensively across multiple LLMs and editing datasets, where it achieves superior editing performance. RECIPE also demonstrates its capability to maintain the overall performance of LLMs alongside showcasing fast editing and inference speed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i2TboPArOr": {
    "title": "UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or hallucination. Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources in different tasks is under-explored. To address these challenges, we categorize four available fact sources: human-written evidence, reference documents, search engine results, and LLM knowledge, along with five text generation tasks containing six representative datasets. Then, we propose UFO, an LLM-based unified and flexible evaluation framework to verify facts against plug-and-play fact sources. We implement six evaluation scenarios based on this framework. Experimental results show that human-written evidence and reference documents are crucial in most QA tasks, but in the news fact generation tasks, introducing human-written evidence leads to a decline in the discriminative power of evaluation. Compared to the LLM knowledge, search engine results are more important in most tasks, but they are less effective in the expert-validated QA task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7roOZvDii6": {
    "title": "Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech",
    "volume": "review",
    "abstract": "Human conversation is usually conducted with language, speech, and visual information. Each communication medium contains rich information and complementary to others, for example, speech (para-lingual) may contain vibe that is not well represented in language. %response is made from conversational history which contains a vibe there. Multimodal LLM consider multimodal information and aim to generate text responses. However, generating more natural and engaging speech response has received little attention even though response only with text cannot give a rich conversation experience. In this paper, we suggest a more human-like agent that makes a speech response based on the conversation mood and responsive style information. Our model is trained to generate text responses along with voice descriptions from multimodal conversation environment. With the voice description, the model generates speech covering para-lingual information. To achieve this goal, we first build a novel multi-sensory conversation dataset mainly focused on speech to enable conversational agents to generate natural speech communication. Then we propose our multimodal LLM based model for generating both text response and voice description. In experimental results, our model demonstrates the effectiveness of utilizing both visual and audio modalities in conversation and generating lively speech",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gFrVsxAEO4": {
    "title": "Scaling Large-Language-Model-based Multi-Agent Collaboration",
    "volume": "review",
    "abstract": "Pioneering advancements in large language model-powered agents have underscored the design pattern of multi-agent collaboration, demonstrating that collective intelligence can surpass the capabilities of each individual. Inspired by the neural scaling law, which posits that increasing neurons leads to emergent abilities, this study investigates whether a similar principle applies to increasing agents in multi-agent collaboration. Technically, we propose multi-agent collaboration networks (MacNet), which utilize directed acyclic graphs to organize agents and streamline their interactive reasoning via topological ordering, with solutions derived from their dialogues. Extensive experiments show that MacNet consistently outperforms baseline models, enabling effective agent collaboration across various topologies. Notably, we observed a small-world collaboration phenomenon, where topologies resembling small-world properties achieved superior performance. Additionally, we identified a collaborative scaling law, indicating that normalized solution quality follows a logistic growth pattern as scaling agents, with collaborative emergence occurring much earlier than previously observed instances of neural emergence",
    "checked": true,
    "id": "208d489c73ebf182faa974191355fb2505ce8da5",
    "semantic_title": "scaling large-language-model-based multi-agent collaboration",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=nNdq8GG9Zs": {
    "title": "Semantic Steganography: A Framework for Robust and High-Capacity Information Hiding using Large Language Models",
    "volume": "review",
    "abstract": "In the era of Large Language Models (LLMs), generative linguistic steganography has become a prevalent technique for hiding information within model-generated texts. However, traditional steganography methods struggle to effectively align steganographic texts with original model-generated texts due to the lower entropy of the predicted probability distribution of LLMs. This results in a decrease in embedding capacity and poses challenges for decoding stegos in real-world communication channels. To address these challenges, we propose a semantic steganography framework based on LLMs, which constructs a semantic space and maps secret messages onto this space using ontology-entity trees. This framework offers robustness and reliability for transmission in complex channels, as well as resistance to text rendering and word blocking. Additionally, the stegos generated by our framework are indistinguishable from the covers and achieve a higher embedding capacity compared to state-of-the-art steganography methods, while producing higher quality stegos",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HOwjJyqRai": {
    "title": "Komodo: A Linguistic Expedition into Indonesia's Regional Languages",
    "volume": "review",
    "abstract": "The recent breakthroughs in Large Language Models (LLMs) have mostly focused on languages with easily available and sufficient resources, such as English. However, there remains a significant gap for languages that lack sufficient linguistic resources in the public domain. Our work introduces Komodo-7B, 7-billion-parameter Large Language Models designed to address this gap by seamlessly operating across Indonesian, English, and 11 regional languages in Indonesia. Komodo-7B is a family of LLMs that consist of Komodo-7B-Base and Komodo-7B-Instruct. Komodo-7B-Instruct stands out by achieving state-of-the-art performance in various tasks and languages, outperforming the benchmarks set by OpenAI's GPT-3.5, Cohere's Aya-101, Llama-2-Chat-13B, Mixtral-8x7B-Instruct-v0.1, Gemma-7B-it , and many more. This model not only demonstrates superior performance in both language-specific and overall assessments but also highlights its capability to excel in linguistic diversity. Our commitment to advancing language models extends beyond well-resourced languages, aiming to bridge the gap for those with limited linguistic assets. Additionally, Komodo-7B-Instruct's better cross-language understanding contributes to addressing educational disparities in Indonesia, offering direct translations from English to 11 regional languages, a significant improvement compared to existing language translation services. Komodo-7B represents a crucial step towards inclusivity and effectiveness in language models, providing to the linguistic needs of diverse communities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YwKcOll596": {
    "title": "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated impressive abilities in generating unstructured natural language according to instructions. However, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE). To address this issue, this paper introduces an efficient method, G&O, to enhance their structured text generation capabilities. It breaks the generation into a two-step pipeline: initially, LLMs generate answers in natural language as intermediate responses. Subsequently, LLMs are asked to organize the output into the desired structure, using the intermediate responses as context. G&O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously. Tested on zero-shot NER and RE, the results indicate a significant improvement in LLM performance with minimal additional efforts. This straightforward and adaptable prompting technique can also be combined with other strategies, like self-consistency, to further elevate LLM capabilities in various structured text generation tasks",
    "checked": true,
    "id": "79ff4eb495094e3b47468515846d507144135ae8",
    "semantic_title": "a simple but effective approach to improve structured language model output for information extraction",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=jDMvwI9IEh": {
    "title": "Knowledge Graphs for Multi-modal Learning: Survey and Perspective",
    "volume": "review",
    "abstract": "Integrated with multi-modal learning, knowledge graphs (KGs) as structured knowledge repositories, enhance AI's capability to process and understand complex, real-world data. This paper provides a comprehensive survey of cutting-edge research on KG-aware multi-modal learning, providing task definitions, evaluation benchmarks, and detailed insights into key breakthroughs. Furthermore, we also discuss current challenges, highlighting emerging trends and future research directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KvTH7eUxsY": {
    "title": "Preview Tools before Using: Enhancing Tool Documentation with Multi-Tool Exploration",
    "volume": "review",
    "abstract": "Enhancing the task-solving capabilities of large language models (LLMs) through utilizing tools has garnered increasing attention. To enable LLMs to use tools accurately, developers often provide documentation of the tools in the LLMs' context. However, such documentation has various issues, such as incomplete tool descriptions and insufficient descriptions of parameters or responses. To address this, we propose ToolBFS+, a method to revise tool documentation by exploring the use of tools. ToolBFS+ adopts a Breadth-First Search (BFS) strategy to explore various tool usage scenarios and collects the information obtained from the exploration to revise the tool documentation, ultimately improving the model's ability to accurately utilize the tools. Extensive experiments on multiple datasets demonstrate that the ToolBFS+ method can substantially reduce errors, such as the selection of incorrect tools, and improve the capability of LLMs to use tools accurately",
    "checked": false,
    "id": "ab97d26c6526d850c59a01d87f1903f0dba68187",
    "semantic_title": "enhancing channelised features interpretability using deep learning predictive modelling",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jmLKEtZsxN": {
    "title": "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback",
    "volume": "review",
    "abstract": "Large language models (LLMs) fine-tuned with alignment methods, such as reinforcement learning from human feedback, have been used to develop some of the most capable AI systems to date. Despite their success, existing methods typically rely on simple binary labels, such as those indicating preferred outputs in pairwise preferences. This overlooks the varying relative quality between pairs, preventing models from capturing these subtleties. To address this limitation, we consider settings in which this information (i.e., margin) can be derived and propose a straightforward generalization of common optimization objectives used in alignment methods. The approach, which we call Margin Matching Preference Optimization (MMPO), integrates per-feedback margin to enhance optimization, making it more robust to overfitting and resulting in better LLM policies and reward models. Specifically, given quality margins in pairwise preferences, we design soft target probabilities based on the Bradley-Terry model, which are then used to train models with the standard cross-entropy objective. Our experiments with both human and AI feedback data demonstrate that MMPO can outperform baseline methods, often by a substantial margin, on popular benchmarks, including MT-bench and RewardBench. Notably, the 7B model trained with MMPO achieves state-of-the-art performance on RewardBench compared to competing models at the same scale, as of June 2024. Our analysis further demonstrates that MMPO is more robust to overfitting, leading to better-calibrated models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6G4V3COOvt": {
    "title": "Integrating Emotional and Linguistic Models for Ethical Compliance in Large Language Models",
    "volume": "review",
    "abstract": "This research develops advanced methodologies for Large Language Models (LLMs) to better manage linguistic behaviors related to emotions and ethics. We introduce DIKE, a framework that enhances the LLMs' ability to internalize and reflect universal human values, adapting to varied cultural contexts to promote transparency and trust among users. The methodology involves detailed modeling of emotions, classification of linguistic behaviors, and implementation of ethical guardrails. Our innovative approaches include mapping emotions and behaviors using self-supervised learning techniques, refining these guardrails through adversarial reviews, and systematically adjusting outputs to ensure ethical alignment. This framework establishes a robust foundation for AI systems to operate with ethical integrity and cultural sensitivity, paving the way for more responsible and context-aware AI interactions",
    "checked": true,
    "id": "4d46800c0a27307e3f68226ba3e6ec31f8fb952d",
    "semantic_title": "integrating emotional and linguistic models for ethical compliance in large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=umVwLUlhcQ": {
    "title": "SyntheT2C: Generating Synthetic Data for Fine-Tuning Large Language Models on the Text2Cypher Task",
    "volume": "review",
    "abstract": "Integrating Large Language Models (LLMs) with existing Knowledge Graph (KG) databases presents a promising avenue for enhancing LLMs' efficacy and mitigating their \"hallucinations\". Given that most KGs reside in graph databases accessible solely through specialized query languages (e.g., Cypher), there exists a critical need to bridge the divide between LLMs and KG databases by automating the translation of natural language into Cypher queries (commonly termed the \"Text2Cypher\" task). Prior efforts tried to bolster LLMs' proficiency in Cypher generation through Supervised Fine-Tuning. However, these explorations are hindered by the lack of annotated datasets of Query-Cypher pairs, resulting from the labor-intensive and domain-specific nature of annotating such datasets. In this study, we propose SyntheT2C, a methodology for constructing a synthetic Query-Cypher pair dataset, comprising two distinct pipelines: (1) LLM-based prompting and (2) template-filling. SyntheT2C facilitates the generation of extensive Query-Cypher pairs with values sampled from an underlying Neo4j graph database. Subsequently, SyntheT2C is applied to two medical databases, culminating in the creation of a synthetic dataset, MedT2C. Comprehensive experiments demonstrate that the MedT2C dataset effectively enhances the performance of backbone LLMs on the Text2Cypher task. Both the SyntheT2C codebase and the MedT2C dataset will be released soon",
    "checked": true,
    "id": "0585e7f28f634fcb646ece9bd2b7aa33789efc0a",
    "semantic_title": "synthet2c: generating synthetic data for fine-tuning large language models on the text2cypher task",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AH1DvHxvzt": {
    "title": "Large Language Models Can Be Contextual Privacy Protection Learners",
    "volume": "review",
    "abstract": "The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains contextually sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of data leakage of sensitive PII during inference time. To address this challenge, we introduce Contextual Privacy Protection Language Models (CPPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding inference-time data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, effectively protecting private data while enhancing the model's knowledge. Our work underscores the potential for Large Language Models as robust contextual privacy protection learners",
    "checked": false,
    "id": "e9ca67b67f4b43650baaef0d03013683eeb4528e",
    "semantic_title": "large language models can be good privacy protection learners",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=oSETxofkqf": {
    "title": "UniTabNet: Bridging Vision and Language Models for Enhanced Table Structure Recognition",
    "volume": "review",
    "abstract": "In the digital era, table structure recognition technology is a critical tool for processing and analyzing large volumes of tabular data. Previous methods primarily focus on visual aspects of table structure recovery but often fail to effectively comprehend the textual semantics within tables, particularly for descriptive textual cells. In this paper, we introduce UniTabNet, a novel framework for table structure parsing based on the image-to-text model. UniTabNet employs a ``divide-and-conquer'' strategy, utilizing an image-to-text model to decouple table cells and integrating both physical and logical decoders to reconstruct the complete table structure. We further enhance our framework with the Vision Guider, which directs the model's focus towards pertinent areas, thereby boosting prediction accuracy. Additionally, we introduce the Language Guider to refine the model's capability to understand textual semantics in table images. Evaluated on prominent table structure datasets such as PubTabNet, PubTables1M, WTW, and iFLYTAB, UniTabNet achieves a new state-of-the-art performance, demonstrating the efficacy of our approach. The code will also be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fLbF0lwrYr": {
    "title": "Evolution without Large Models: Training Language Model with Task Principles",
    "volume": "review",
    "abstract": "A common training approach for language models involves using a large-scale language model to expand a human-provided dataset, which is subsequently used for model training.This method significantly reduces training costs by eliminating the need for extensive human data annotation. However, it still faces challenges such as high carbon emissions during data augmentation and the risk of data leakage when we use closed-source LLMs. To address these issues, we propose a self-evolution method for language models. First, we introduce the Multi-level Principle Generation, which enables a large-scale model to summarize task-completion principles based on a small amount of task data. Then, we propose the Principle-based Instance Generation, in which a smaller-scale language model uses these task principles to generate a large amount of data. This data is then used for model training. Experimental results show that our proposed method significantly improves model performance compared to directly using a smaller-scale language model to generate data. Additionally, since we only use the large-scale language model to generate the task-completion principles, the carbon emissions associated with training the model are greatly reduced. Our code is available at https://anonymous.4open.science/r/PSI-0ED6/",
    "checked": false,
    "id": "64e6d4440d6d3305c5dc4ecee7d27853cc452651",
    "semantic_title": "large language models: principles and practice",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iY9LArskAF": {
    "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches",
    "volume": "review",
    "abstract": "Long context capability is a crucial competency for large language models (LLMs) as it mitigates the human struggle to digest long-form texts. This capability enables complex task-solving scenarios such as book summarization, code assistance, and many more tasks that are traditionally manpower-intensive. However, transformer-based LLMs face significant challenges with long context input due to the growing size of the KV cache and the intrinsic complexity of attending to extended inputs; where multiple schools of efficiency-driven approaches â€” such as KV cache quantization, token dropping, prompt compression, linear-time sequence models, and hybrid architectures â€” have been proposed to produce efficient yet long context-capable models. Despite these advancements, no existing work has comprehensively benchmarked these methods in a reasonably aligned environment. In this work, we fill this gap by providing a taxonomy of current methods and evaluating over 10+ state-of-the-art approaches across seven categories of long context tasks. Our work reveals numerous previously unknown phenomena and offers insights â€” as well as a friendly workbench â€” for the future development of long context-capable LLMs",
    "checked": true,
    "id": "fbfe920579cc1c13358521d403cfce31f2afbead",
    "semantic_title": "kv cache compression, but what must we give in return? a comprehensive benchmark of long context capable approaches",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=tO7XlLK8dN": {
    "title": "Nash CoT: Multi-Path Inference with Preference Equilibrium",
    "volume": "review",
    "abstract": "Chain-of-thought (CoT) prompting has emerged as a powerful technique for enhancing the reasoning capabilities of Large Language Models (LLMs) on complex problems. Among CoT-related studies, self-consistency (Multi-path inference with answer filtering through voting) involves generating multiple reasoning paths using the CoT framework and then selecting the most frequently produced outputs standing out as a concise yet competitive approach. While self-consistency has indeed led to the improvements in LLM inference, the use of multi-path inference also escalates deployment costs. Therefore, maintaining the performance benefits of self-consistency inherited from multi-path inference while reducing the inference costs holds significant value. In this research, we conceptualize language decoding as a preference consensus game, constructing a bi-player gaming system within each local path, and introduce Nash Chain-of-Thought (Nash CoT). Specifically, for a given question, we leverage LLM to autonomously select the contextually relevant template and generate outputs guided by this template, aiming to reach Nash Equilibrium alongside normal generation in each path. This approach allows us to achieve comparable or improved performance compared to self-consistency while using fewer inference paths on various inference tasks, including Arabic reasoning, Commonsense Question Answering, and Symbolic inference",
    "checked": true,
    "id": "5161e996b9d6c83990d69c456423edf9ee9ff12e",
    "semantic_title": "nash cot: multi-path inference with preference equilibrium",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K1vNb5SH2o": {
    "title": "Measuring and Modifying the Readability of English Texts with Large Language Models",
    "volume": "review",
    "abstract": "The success of Large Language Models (LLMs) in other domains has raised the question of whether LLMs can reliably assess and manipulate the \\textit{readability} of text. We approach this question empirically. First, using a published corpus of 4,724 English text excerpts, we find that readability estimates produced \"zero-shot\" from GPT-4 Turbo exhibit relatively high correlation with human judgments ($r = 0.76$), out-performing estimates derived from traditional readability formulas. Then, in a pre-registered human experiment ($N = 59$), we ask whether Turbo can reliably make text easier or harder to read. We find evidence to support this hypothesis, though considerable variance in human judgments remains unexplained. We conclude by discussing the limitations of this approach, including concerns about data contamination, as well as the validity of the \"readability\" construct and its dependence on context, audience, and goal",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hh3e3n0oiv": {
    "title": "Evaluating Creativity in Large Language Models through Creative Problem-Solving: A New Dataset and Benchmark",
    "volume": "review",
    "abstract": "Creative problem-solving, integrating divergent and convergent thinking, is pivotal for leveraging creativity in fields such as AI4Science. As large language models (LLMs) evolve into sophisticated creative assistants, it becomes crucial to effectively assess their problem-solving abilities. Traditional benchmarks, often rooted in cognitive science, focus on a single phase or do not distinguish between the divergent and convergent phases, limiting their ability to fully evaluate LLMs. To bridge this gap, we introduce a novel benchmark comprising an open-ended question answering (QA) dataset alongside traditional creativity tasks, aimed at evaluating the holistic creative capabilities of LLMs. This benchmark utilizes multi-dimensional evaluation metrics to provide a comprehensive assessment that correlates with model parameters, architectural differences, and domain-specific expertise. The benchmark aims to not only advance understanding in the field but also set a new standard for evaluating the creative problem-solving potential of LLMs. The dataset and code are available at: https://anonymous.4open.science/r/LLM-creativity-Benchmark/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Boa0GnqTqh": {
    "title": "UNICORN: A Unified Causal Video-Oriented Language-Modeling Framework for Temporal Video-Language Tasks",
    "volume": "review",
    "abstract": "The great success of large language models has encouraged the development of large multimodal models, with a focus on image-language interaction. Despite promising results in various image-language downstream tasks, it is still challenging and unclear how to extend the capabilities of these models to the more complex video domain, especially when dealing with explicit temporal signals. To address the problem in existing large multimodal models, in this paper we adopt visual instruction tuning to build a unified causal video-oriented language modeling framework, named UNICORN. Specifically, we collect a comprehensive dataset under the instruction-following format, and instruction-tune the model accordingly. Experimental results demonstrate that without customized training objectives and intensive pre-training, UNICORN can achieve comparable or better performance on established temporal video-language tasks including moment retrieval, video paragraph captioning and dense video captioning. Moreover, the instruction-tuned model can be used to automatically annotate internet videos with temporally-aligned captions. Compared to commonly used ASR captions, we show that training on our generated captions improves the performance of video-language models on both zero-shot and fine-tuning settings. Source code can be found at [here](https://anonymous.4open.science/r/UNICORN) and we will release it upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qAXUZFU6Q9": {
    "title": "Global Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents",
    "volume": "review",
    "abstract": "We describe an approach for aligning an LLM based dialogue agent for long-term social dialogue, where there is only a single global score given by the user at the end of the session. In this paper, we propose the usage of denser naturally-occurring multimodal communicative signals as local implicit feedback to improve the turn-level utterance generation. Therefore, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session level reward, using Local Implicit (LI) multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the RLHF pipeline to improve an LLM-based dialog agent. We run quantitative and qualitative human studies on two large-scale datasets to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods",
    "checked": false,
    "id": "3cb1328a9d5f62cfe87ea01ca8f471e3022efc55",
    "semantic_title": "improving dialogue agents by decomposing one global explicit annotation with local implicit multimodal feedback",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JT5TRr6obB": {
    "title": "Pushing The Limit of LLM Capacity for Text Classification",
    "volume": "review",
    "abstract": "The value of text classification's future research has encountered challenges and uncertainties, due to the extraordinary efficacy demonstrated by large language models (LLMs) across numerous downstream NLP tasks. In this era of open-ended language modeling, where task boundaries are gradually fading, an urgent question emerges: have we made significant progress in text classification with the full benefit of LLMs? To answer this question, we propose RGPT, an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling a pool of base learners. The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively fine-tuning LLMs with them. Such base learners are then ensembled to be a specialized text classification LLM, by recurrently incorporating the historical predictions from the previous learners. Through a comprehensive empirical comparison, we show that RGPT significantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by 4.03\\% on average. Further evaluation experiments prove its potential for universal application as an enhancement technique",
    "checked": true,
    "id": "e6e584fc07b6cc6a78cb465019871d751856ba08",
    "semantic_title": "pushing the limit of llm capacity for text classification",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=DYgpiWI2fp": {
    "title": "Learning to Use Tools via Cooperative and Interactive Agents",
    "volume": "review",
    "abstract": "Tool learning empowers large language models (LLMs) as agents to use external tools and extend their utility. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating execution results into the next action prediction. Despite their progress, these methods suffer from performance degradation when addressing practical tasks due to: (1) the pre-defined pipeline with restricted flexibility to calibrate incorrect actions, and (2) the struggle to adapt a general LLM-based agent to perform a variety of specialized actions. To mitigate these problems, we propose ConAgents, a Cooperative and interactive Agents framework, which coordinates three specialized agents for tool selection, tool execution, and action calibration separately. ConAgents introduces two communication protocols to enable the flexible cooperation of agents. To effectively generalize the ConAgents into open-source models, we also propose specialized action distillation, enhancing their ability to perform specialized actions in our framework. Our extensive experiments on three datasets show that the LLMs, when equipped with the ConAgents, outperform baselines with substantial improvement (i.e., up to 14% higher success rate)",
    "checked": true,
    "id": "966ba2acfe0700c2410efe15ed1b6c25340b7a95",
    "semantic_title": "learning to use tools via cooperative and interactive agents",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=GCxvf6qxeW": {
    "title": "OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting",
    "volume": "review",
    "abstract": "Entity Linking (EL) is the process of associating ambiguous textual mentions to specific entities in a knowledge base. Traditional EL methods heavily rely on large datasets to enhance their performance, a dependency that becomes problematic in the context of few-shot entity linking, where only a limited number of examples are available for training. To address this challenge, we present OneNet, an innovative framework that utilizes the few-shot learning capabilities of Large Language Models (LLMs) without the need for fine-tuning. To the best of our knowledge, this marks a pioneering approach to applying LLMs to few-shot entity linking tasks. OneNet is structured around three key components prompted by LLMs: (1) an entity reduction processor that simplifies inputs by summarizing and filtering out irrelevant entities, (2) a dual-perspective entity linker that combines contextual cues and prior knowledge for precise entity linking, and (3) an entity consensus judger that employs a unique consistency algorithm to alleviate the hallucination in the entity linking reasoning. Comprehensive evaluations across six benchmark datasets reveal that OneNet outperforms current state-of-the-art entity linking methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WDZ9kRZxNb": {
    "title": "Making AI Think Lean: Sparse Concept Bottleneck Models for Interpretable Decisions",
    "volume": "review",
    "abstract": "Concept Bottleneck Models (CBMs) provide a promising approach to enhance interpretability in machine learning models. These models excel at disentangling and anchoring visual representations into human-comprehensible concepts. We present an approach to enhance visual model interpretability by incorporating natural language text directly extracted from images. We introduce the Visual-Rationale Alignment Learning (VIRAL) framework, which incorporates natural language text directly extracted from images to improve the interpretability of visual models. Through the use of the Gumbel-Sinkhorn algorithm for sparse alignment and extensive experimental analysis, VIRAL demonstrates its effectiveness in providing human-understandable explanations for predictions, contributing to the development of more transparent and trustworthy AI multimodal systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pY82iRwlOD": {
    "title": "Implicit and Indirect: Computational Identification of Ambiguous Conversational Actions in Asynchronous Crisis-Related Conversations",
    "volume": "review",
    "abstract": "This paper presents a digital conversation analysis based approach to the computational detection of ambiguous actions in asynchronous online conversations. Action detection has been widely studied for synchronous chats. However, models or datasets for asynchronous conversations are scarce, and have not sufficiently considered the special characteristics of asynchronous discussion, most importantly the tendency for comments to involve multiple actions and multiple valid interpretations of actions. We provide a theory-driven annotation scheme for crisis-related asynchronous conversations, and an annotated dataset for Finnish. We show that considering the multi-action characteristics of asynchronous data statistically improves classification performance, and that an ensemble of best models can represent the ambiguity of actions, which is especially characteristic of face-threatening actions in controversial conversations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZCCydvipRt": {
    "title": "Generating Long-form Story Using Dynamic Hierarchical Outlining with Memory-Enhancement",
    "volume": "review",
    "abstract": "Long-form story generation task aims to produce coherent and sufficiently lengthy text, essential for applications such as novel writing and interactive storytelling. However, existing methods, including LLMs, rely on rigid outlines or lack macro-level planning, it difficult to achieve both contextual consistency and coherent plot development in long-form story generation. To address these issues, we propose Dynamic Hierarchical Outlining with Memory-Enhancement long-form story generation method, named DOME, to generate the long-form story with coherent content and plot. Specifically, the Dynamic Hierarchical Outline (DHO) mechanism incorporates the novel writing theory into outline planning and fuses the plan and writing stages together, improving the coherence of the plot by ensuring the plot completeness and fluency of story development. Additionally, a Memory-Enhancement Module (MEM) based on temporal knowledge graphs is introduced to store and access generated content, reducing contextual conflicts and improving story coherence. Finally, we propose a Temporal Conflict Analyzer leveraging temporal knowledge graphs to evaluate contextual consistency. Experiments demonstrate that DOME significantly improves the fluency, coherence, and overall quality of generated long stories compared to state-of-the-art methods",
    "checked": false,
    "id": "e52b353f3e3daf452eb193b9bdb14277bacb7ba0",
    "semantic_title": "brain-to-brain communication during musical improvisation: a performance case study [version 1; peer review: awaiting peer review]",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RDjYDGvtcx": {
    "title": "Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality",
    "volume": "review",
    "abstract": "In this paper, we propose a new method to enhance compositional understanding in pre-trained vision and language models (VLMs) without sacrificing performance in the model's original zero-shot multi-modal tasks. Traditional fine-tuning methods often improve compositional reasoning at the expense of multi-modal capabilities. This drawback stems from the use of global hard negative loss, which contrasts the global representations of images and texts. This can distort multi-modal representations by pushing original texts due to ambiguous global representations. To address this, we propose the Fine-grained Selective Calibrated CLIP (FSC-CLIP). This incorporates local hard negative loss and selective calibrated regularization, designed to provide fine-grained negative supervision while maintaining the integrity of representations. Our extensive evaluation across various benchmarks for compositionality and multi-modal tasks shows that FSC-CLIP not only achieves compositionality on par with state-of-the-art models but also maintains multi-modal capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BlKQRqqSvf": {
    "title": "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown impressive capabilities, yet they still struggle with math reasoning. In this work, we propose CoT-Influx, a novel approach that pushes the boundary of few-shot Chain-of-Thoughts (CoT) learning to improve LLM mathematical reasoning. Motivated by the observation that adding more concise CoT examples in the prompt can improve LLM reasoning performance, CoT-Influx employs a coarse-to-fine pruner to maximize the input of effective and concise CoT examples. The pruner first selects as many crucial CoT examples as possible and then prunes unimportant tokens to fit the context window. As a result, by enabling more CoT examples with double the context window size in tokens, CoT-Influx significantly outperforms various prompting baselines across various LLMs (LLaMA2-7B, 13B, 70B) and 5 math datasets, achieving up to 4.55% absolute improvements. Remarkably, without any fine-tuning, LLaMA2-70B with CoT-Influx surpasses GPT-3.5 and a wide range of larger LLMs (PaLM, Minerva 540B, etc.) on the GSM8K. CoT-Influx is a plug-and-play module for LLMs, adaptable in various scenarios. It's compatible with advanced reasoning prompting techniques, such as self-consistency, and supports different long-context LLMs, including Mistral-7B-v0.3-32K and Yi-6B-200K",
    "checked": false,
    "id": "1f5b4e393a1e02ab49e5ca2e6819cc28841918a2",
    "semantic_title": "fewer is more: boosting llm reasoning with reinforced context pruning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=bCTqtcEySo": {
    "title": "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation",
    "volume": "review",
    "abstract": "Recent advances in image tokenizers, such as VQ-VAE, have enabled text-to-image generation using auto-regressive methods, similar to language modeling. However, these methods have yet to leverage pre-trained language models, despite their adaptability to various downstream tasks. In this work, we explore this gap by adapting a pre-trained language model for auto-regressive text-to-image generation, and find that pre-trained language models offer limited help. We provide a two-fold explanation by analyzing tokens from each modality. First, we demonstrate that image tokens possess significantly different semantics compared to text tokens, rendering pre-trained language models no more effective in modeling them than randomly initialized ones. Second, the text tokens in the image-text datasets are too simple compared to normal language model pre-training data, which causes the catastrophic degradation of language models' capability",
    "checked": true,
    "id": "e36584cf5c53d19a2d2b888ee05cc2f7afd52693",
    "semantic_title": "pre-trained language models do not help auto-regressive text-to-image generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xNa0lDqdTl": {
    "title": "UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis",
    "volume": "review",
    "abstract": "Linguistic steganalysis (LS) tasks aim to detect whether a text contains secret information. Existing LS methods focus on the deep-learning model design and they achieve excellent results in ideal data. However, they overlook the unique user characteristics, leading to weak performance in social networks. And a few stegos here that further complicate detection. We propose the UP4LS, a framework with the User Profile for enhancing LS in realistic scenarios. Three kinds of user attributes like writing habits are explored to build the profile. For each attribute, the specific feature extraction module is designed. The extracted features are mapped to high-dimensional user features via the deep-learning model of the method to be improved. The content feature is extracted by the language model. Then user and content features are integrated. Existing methods can improve LS results by adding the UP4LS framework without changing their deep-learning models. Experiments show that UP4LS can significantly enhance the performance of LS-task baselines in realistic scenarios, with the overall Acc increased by 25%, F1 increased by 51%, and SOTA results. The improvement is especially pronounced in fewer stegos. Additionally, UP4LS also sets the stage for the related-task SOTA methods to efficient LS",
    "checked": true,
    "id": "1840b6363cc9507c4417db262fcc07c2ef3bf619",
    "semantic_title": "up4ls: user profile constructed by multiple attributes for enhancing linguistic steganalysis",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=D1rx4J6AEe": {
    "title": "Automated Tone Transcription and Clustering with Tone2Vec",
    "volume": "review",
    "abstract": "Lexical tones play a crucial role in Sino-Tibetan languages. However, current phonetic fieldwork relies on manual effort, resulting in substantial time and financial costs. This is especially challenging for the numerous endangered languages that are rapidly disappearing, often exacerbated by limited funding. In this paper, we introduce pitch-based similarity representations for tone transcription, named \\texttt{Tone2Vec}. Experiments on dialect clustering and variance show that \\texttt{Tone2Vec} effectively captures fine-grained tone variation. Utilizing \\texttt{Tone2Vec}, we develop the first automatic approach for tone transcription and clustering by presenting a novel representation transformation for transcriptions. Additionally, these algorithms are systematically integrated into an open-sourced and easy-to-use package, \\texttt{ToneLab}, which facilitates automated fieldwork and cross-regional, cross-lexical analysis for tonal languages. Extensive experiments were conducted to demonstrate the effectiveness of our methods. Experiment implementations are available at \\href{https://anonymous.4open.science/r/Tone2vec-E5D4}{https://anonymous.4open.science/r/Tone2Vec-E5D4}~\\footnote{This IPYNB file contains all the experimental details presented in this paper. The official package will be released upon acceptance.}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VvrZGNHg1e": {
    "title": "Text Fluoroscopy: Detecting LLM-Generated Text through Intrinsic Features",
    "volume": "review",
    "abstract": "Large language models (LLMs) have revolutionized the domain of natural language processing because of their excellent performance on various tasks. Despite their impressive capabilities, LLMs also have the potential to generate texts that pose risks of misuse. Consequently, detecting LLM-generated text has become increasingly important. Previous LLM-generated text detection methods use semantic features, which are stored in the last layer. This leads to methods that overfit the training set domain and exhibit shortcomings in generalization. Therefore, We argue that utilizing intrinsic features rather than semantic features for detection results in better performance. In this work, we design Text Fluoroscopy, a black-box method with better generalizability for detecting LLM-generated text by mining the intrinsic features of the text to be detected. Our method captures the text's intrinsic features by identifying the layer with the largest distribution difference from the last and first layers when projected to the vocabulary space. Our method achieves 7.36\\% and 2.84\\% average improvement in detection performance compared to the baselines in detecting texts from different domains generated by GPT-4 and Claude3, respectively",
    "checked": false,
    "id": "1321cda5cb160285de0548a999b4b941156ba764",
    "semantic_title": "threads of subtlety: detecting machine-generated texts through discourse motifs",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CN9EUevAqC": {
    "title": "Bridging the Writing Manner Gap in Visual Instruction Tuning by Creating LLM-aligned Instructions",
    "volume": "review",
    "abstract": "In the realm of Large Multi-modal Models (LMMs), the instruction quality during the visual instruction tuning stage significantly influences the performance of modality alignment. In this paper, we assess the instruction quality from a unique perspective termed Writing Manner, which encompasses the selection of vocabulary, grammar, and sentence structure to convey specific semantics. We argue there exists a substantial writing manner gap between the visual instructions and the inner Large Language Models (LLMs) of LMMs. This gap causes the well-trained inner LLMs to deviate from their original writing styles, leading to capability degradation of both LMMs and inner LLMs. To bridge the writing manner gap while preserving the original semantics, we propose directly leveraging the inner LLM to align the writing manner of soft-format visual instructions with that of the inner LLM itself, resulting in novel LLM-aligned instructions. We develop a novel perplexity-based indicator to quantitatively assess the writing manner gap, and corresponding results show that our approach successfully minimizes this gap. By utilizing LLM-aligned instructions, the baseline models LLaVA-7B and QwenVL demonstrate enhanced resistance to hallucinations and non-trivial comprehensive improvements across all $15$ visual and language benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LfhhTLTpE4": {
    "title": "Cross-Domain Audio Deepfake Detection: Dataset and Analysis",
    "volume": "review",
    "abstract": "Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1% and 6.5% respectively. Additionally, we demonstrate our models' outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research",
    "checked": true,
    "id": "cc7abd01eaa38dc7d1d2ea8bdcea60370749048d",
    "semantic_title": "cross-domain audio deepfake detection: dataset and analysis",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=7aCcfbp6N5": {
    "title": "Creative and Context-Aware Translation of East Asian Idioms with GPT-4",
    "volume": "review",
    "abstract": "East Asian idioms are a type of figurative language, each condensing its rich cultural background into a few characters. Translating such idioms is challenging for human translators, who often resort to choosing a context-aware translation from an existing list of candidates. However, compiling a dictionary of translations demands much time and creativity even for expert translators. To alleviate such burden, we evaluate if GPT-4 can help generate high-quality translations. Based on automatic evaluations of faithfulness and creativity, we first identify Pareto-optimal prompting strategies that can outperform translation engines from Google and DeepL. Then, at a low cost, our context-aware translations can achieve far more and higher-quality translations per idiom than the human baseline. We open-source our code and data to facilitate further research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W4CLqPq1xF": {
    "title": "Entropy Variation and Information Competence: Enhancing Predictive Accuracy of Collaborative Language Models",
    "volume": "review",
    "abstract": "This paper introduces EVINCE (Entropy Variation and INformation CompetencE), a cutting-edge dialogue framework that orchestrates adversarial debates and collaborative insights among multiple large language models (LLMs). By integrating advanced principles from conditional statistics, information theory, and in-context learning, EVINCE masterfully balances the exploration of diverse perspectives with the exploitation of established priors. Central to our innovation is the validation of the dual entropy theory, which we developed to determine the optimal pairing of LLMs with one high and one low entropy for enhanced probabilistic prediction accuracy. We also employ several information-theoretic metrics, such as mutual information, cross-entropy, Wasserstein distance, and Jensen-Shannon divergence, to measure communication opportunities, dialogue progress, and convergence. This meticulous approach fosters an interpretable and productive multi-LLM dialogue, leading to more informed and reliable outcomes. We illustrate EVINCE's potential by applying it to healthcare, demonstrating its effectiveness in improving disease diagnosis, and discuss its broader implications for enhancing decision-making across various domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ui7aRCFuuu": {
    "title": "How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning",
    "volume": "review",
    "abstract": "We investigate the mechanism of in-context learning (ICL) on sentence classification tasks with semantically-unrelated labels (\"foo\"/\"bar\"). We find intervening in only 1\\% heads (named \"in-context heads\") significantly affects ICL accuracy from 87.6\\% to 24.4\\%. To understand this phenomenon, we analyze the value-output vectors in these heads and discover that the vectors at each label position contain substantial information about the corresponding labels. Furthermore, we observe that the prediction shift from \"foo\" to \"bar\" is due to the respective reduction and increase in these heads' attention scores at \"foo\" and \"bar\" positions. Therefore, we propose a hypothesis for ICL: in in-context heads, the value-output matrices extract label features, while the query-key matrices compute the similarity between the features at the last position and those at each label position. The query and key matrices can be considered as two towers that learn the similarity metric between the last position's features and each demonstration at label positions. Using this hypothesis, we explain the majority label bias and recency bias in ICL and propose two methods to reduce these biases by 22\\% and 17\\%, respectively",
    "checked": true,
    "id": "46955794ab197d56b40595fcb8e74b6948097075",
    "semantic_title": "how do large language models learn in-context? query and key matrices of in-context heads are two towers for metric learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=HaaPCU2LvE": {
    "title": "Neuron-Level Knowledge Attribution in Large Language Models",
    "volume": "review",
    "abstract": "Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a static method for pinpointing significant neurons for different outputs. Compared to seven other methods, our approach demonstrates superior performance across three metrics. Additionally, since most static methods typically only identify \"value neurons\" directly contributing to the final prediction, we introduce a static method for identifying \"query neurons\" which activate these \"value neurons\". Finally, we apply our methods to analyze the localization of six distinct types of knowledge across both attention and feed-forward network (FFN) layers. Our method and analysis are helpful for understanding the mechanisms of knowledge storage and set the stage for future research in knowledge editing. We will release our data and code on github",
    "checked": true,
    "id": "9af3c1be7a4cfd38f10b9373e4623f4b64d467cd",
    "semantic_title": "neuron-level knowledge attribution in large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Oc8Cteojey": {
    "title": "Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data Pruning",
    "volume": "review",
    "abstract": "Recent work targeting large language models (LLMs) for code generation demonstrated that increasing the amount of training data through synthetic code generation often leads to exceptional performance. In this paper we explore data pruning methods aimed at enhancing the efficiency of model training specifically for code LLMs. We present techniques that integrate various clustering and pruning metrics to selectively reduce training data without compromising the accuracy and functionality of the generated code. We observe significant redundancies in synthetic training data generation, where our experiments demonstrate that benchmark performance can be largely preserved by training on only 10\\% of the data. Moreover, we observe consistent improvements in benchmark results through moderate pruning of the training data. Our experiments show that these pruning strategies not only reduce the computational resources needed but also enhance the overall quality code generation",
    "checked": true,
    "id": "4359793ae51594fbf18b7a84fe491d57fe9d576c",
    "semantic_title": "code less, align more: efficient llm fine-tuning for code generation with data pruning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JRe8v0lUgU": {
    "title": "Impact of Stickers on Multimodal Chat Sentiment Analysis and Intent Recognition: A New Task, Dataset and Baseline",
    "volume": "review",
    "abstract": "Stickers are increasingly used in social media to express sentiment and intent. When finding typing troublesome, people often use a sticker instead. Despite the significant impact of stickers on sentiment analysis and intent recognition, little research has been conducted. To address this gap, we propose a new task: Multimodal chat Sentiment Analysis and Intent Recognition involving Stickers (MSAIRS). Additionally, we introduce a novel multimodal dataset containing Chinese chat records and stickers excerpted from several mainstream social media platforms. Our dataset includes paired data with the same text but different stickers, and various stickers consisting of the same images with different texts, allowing us to better understand the impact of stickers on chat sentiment and intent. We also propose an effective multimodal joint model, MMSAIR, for our task, which is validated on our datasets and indicates that visual information of stickers counts. Our dataset and code will be publicly available",
    "checked": true,
    "id": "ded2a815a04fa79a27b3109b9dfafa23b1cb70e5",
    "semantic_title": "impact of stickers on multimodal chat sentiment analysis and intent recognition: a new task, dataset and baseline",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3lgVTTzVSM": {
    "title": "BMIL: Self and Cooperative Bias Mitigation in-the-loop in Large Language Models",
    "volume": "review",
    "abstract": "Recent progress in Large Language Models (LLMs) has demonstrated their superior abilities in various Natural Language Processing (NLP) tasks. However, they have also revealed a tendency to learn and unintentionally magnify harmful societal biases. Current bias mitigation methods during the pre-processing and training stages still leave considerable methodological challenges. We propose a novel multi-stage bias mitigation approach called 'Bias Mitigation in-the-loop' (BMIL), which consists of two main strategies: self bias mitigation in-the-loop and cooperative bias mitigation in-the-loop. The first strategy enables LLMs to autonomously assess and reduce their biases, while the second involves collaboration among multiple LLMs with varying bias levels to collectively tackle and reduce various biases through a debate process. Furthermore, we apply these strategies in supervised fine-tuning sessions to alleviate inherent biases in LLMs. Our experiments, involving models like ChatGPT, Gemini, Llama2, Llama3, and Mistral, demonstrate that BMIL effectively mitigates a broad spectrum of biases, significantly improving the quality of model outputs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m2JUBJuKQD": {
    "title": "Learning from Implicit User Feedback, Emotions and Demographic Information in Task-Oriented Document-Grounded Dialogues",
    "volume": "review",
    "abstract": "Implicit user feedback, user emotions and demographic information have shown to be promising sources for improving the accuracy and user engagement of responses generated by dialogue systems. However, the influence of such information on task completion and factual consistency, which are important criteria for task-oriented and document-grounded dialogues, is not yet known. To address this, we introduce FEDI, the first English task-oriented document-grounded dialogue dataset annotated with this information. Our experiments with Flan-T5, GPT-2 and Llama 2 show a particularly positive impact on task completion and factual consistency. Participants in our human evaluation reported that the responses generated by the feedback-trained models were more informative (Flan-T5 and GPT-2), more relevant and more factual consistent (Llama 2)",
    "checked": false,
    "id": "0bc0168f2c92861f6711cd5a2e54c027c9c024d5",
    "semantic_title": "learning from emotions, demographic information and implicit user feedback in task-oriented document-grounded dialogues",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rb380J8pcE": {
    "title": "Cultural Value Differences of LLMs: Prompt, Language, and Model Size",
    "volume": "review",
    "abstract": "Our study aims to identify behavior patterns in cultural values exhibited by large language models (LLMs). The studied variants include question ordering, prompting language, and model size. Our experiments reveal that each tested LLM can efficiently behave with different cultural values. More interestingly: (i) LLMs exhibit relatively consistent cultural values when presented with prompts in a single language. (ii) The prompting language e.g., Chinese or English, can influence the expression of cultural values. The same question can elicit divergent cultural values when the same LLM is queried in a different language. (iii) Differences in sizes of the same model (e.g., Llama2-7B vs 13B vs 70B) have a more significant impact on their demonstrated cultural values than model differences (e.g., Llama2 vs Mixtral). Our experiments reveal that query language and model size of LLM are the main factors resulting in cultural value differences",
    "checked": true,
    "id": "242d5bb1c471102377b9f8efb9cb8bc52a5586f8",
    "semantic_title": "cultural value differences of llms: prompt, language, and model size",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NtJJASK4ey": {
    "title": "StableSynthNet: Disentangled HyperNetworks for Enhanced On-device Multi-modal Model Generalization",
    "volume": "review",
    "abstract": "In the modern interconnected landscape, the proliferation of smart devices leads to the continuous collection of extensive and varied personal multi-modal data. This situation necessitates the development of sophisticated, personalized, and device-aware services. Traditional AI systems, mainly cloud-based, face considerable hurdles in adapting to the dynamic data flow between cloud services and devices. While HyperNetworks have enhanced performance and real-time processing over conventional fine-tuning approaches, they tend to be over-parameterized due to the underutilization of consistent data types. Our solution, StableSynthNet, is a novel system consisting of three components: Driver Contrastive Training, Template-Driver Extraction, and Offset-Driver Separation. This design uniquely separates the template parameter driver, which houses common data characteristics, from the offset parameter driver, where individual data specifics are stored. The resulting combined driver achieves an optimal mix of consistency and adaptability. Our extensive testing in the fields of video question answering and video retrieval has demonstrated the superior efficiency and effectiveness of StableSynthNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YPlIAcmfmR": {
    "title": "Efficient LLM Comparative Assessment: A Product of Experts Framework for Pairwise Comparisons",
    "volume": "review",
    "abstract": "LLM-as-a-judge approaches are a practical and effective way of assessing a range of text tasks. However, when using pairwise comparisons to rank a set of candidates, the computational cost scales quadratically with the number of candidates, which has practical limitations. This paper introduces a Product of Expert (PoE) framework for efficient LLM Comparative Assessment. Here individual comparisons are considered experts that provide information on a pair's score difference. The PoE framework combines the information from these experts to yield an expression that can be maximized with respect to the underlying set of candidates, and is highly flexible where any form of expert can be assumed. When Gaussian experts are used one can derive simple closed-form solutions for the optimal candidate ranking, as well as expressions for selecting which comparisons should be made to maximize the probability of this ranking. Our approach enables efficient comparative assessment, where by using only a small subset of the possible comparisons, one can generate score predictions that correlate well with human judgements. We evaluate the approach on multiple NLG tasks and demonstrate that our framework can yield considerable computational savings when performing pairwise comparative assessment. With many candidate texts, using as few as 2% of comparisons the PoE solution can achieve similar performance to when all comparisons are used",
    "checked": true,
    "id": "4a70c45876cfc39a740ba9851e84608d10921660",
    "semantic_title": "efficient llm comparative assessment: a product of experts framework for pairwise comparisons",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SYEi6o4D1F": {
    "title": "CogLoop: Bridging Planning and Control for Robot Manipulation by Embodied Feedback",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QUmZIx3boG": {
    "title": "Insights into using temporal coordinated behaviour to explore connections between social media posts and influence",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7YEsiDoHdt": {
    "title": "Transfer-Prompting: Enhancing Language Models with Objective-Transferable Prompting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "5c5ab276b00c1f19fbb0a3d2c38d532becac9442",
    "semantic_title": "a brief history of prompt: leveraging language models. (through advanced prompting)",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=AqAF5MI0Lw": {
    "title": "Better Mathematical Reasoners by Bootstrapping from LLM Answers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "b549716038bb167fcc65e3f9f725013d8b2ea649",
    "semantic_title": "reliable natural language understanding with large language models and answer set programming",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=1SNTgwOsyg": {
    "title": "Active Reading Agent through Reading Controller",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "7b74868a7eff20e99387f84d8eb68841d6ea067c",
    "semantic_title": "reading shakespeare through drama",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=A7eAWirXMD": {
    "title": "WEPO: Web Element Preference Optimization for Enhanced Autonomous Web Navigation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9upwOJYJAr": {
    "title": "HalLoc: Token-level Localization of Hallucinations in Large Vision Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9AQBVqeJ8K": {
    "title": "LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "b503050327ba9e01d27dd8d06589544f89f40cbd",
    "semantic_title": "langgpt: rethinking structured reusable prompt design framework for llms from the programming language",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyng2buyYm": {
    "title": "Gender-specific Machine Translation with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "fb2719aa3245a1757144d273be0a9b3a96d43a3f",
    "semantic_title": "gender-specific machine translation with large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4d5ljoYgII": {
    "title": "HyPost: A Dataset for Hypothesis-based Post-Processing of ASR Transcripts using LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EnvMeHoyrj": {
    "title": "Large Language Model Can Continue Evolving From Mistakes",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "da3369c33ed94717166dba73dc0ebb5c9ddfe348",
    "semantic_title": "large language model can continue evolving from mistakes",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=K1gkAIWkwu": {
    "title": "An Effective Data Refinement Strategy Using Annotation Agreements and Model Predictions for Implicit Hate Speech Detection",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=syuip9BKM8": {
    "title": "Each Fake News is Fake in its Own Way: An Attribution Multi-Granularity Benchmark for Multimodal Fake News Detection",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3whj1NCfB5": {
    "title": "CORM: Coarse-to-fine-grained Offloading for SMoE LLM Inference on Consumer-grade GPU",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l1YOKJeavf": {
    "title": "Benchmarking Large Language Models in E-commerce Leveraging Knowledge Graph",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "1aeb53f240f3f509bcc97ef529d9547a936f1099",
    "semantic_title": "vlkeb: a large vision-language model knowledge editing benchmark",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=qH8bArdz9R": {
    "title": "FACTOID: FACtual enTailment fOr hallucInation Detection",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "b1e3eaa8d51d53037d01ebb7805d882e5e7860b1",
    "semantic_title": "factoid: factual entailment for hallucination detection",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=xANK99Zro7": {
    "title": "Are LLMs Rational Investors?\\\\A Study on Detecting and Reducing the Financial Bias in LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "04ad5350251bffa327d848a7cbffe5295140c6e1",
    "semantic_title": "are llms rational investors? a study on detecting and reducing the financial bias in llms",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=CHcUtUkt4u": {
    "title": "This Suits You the Best\": Query Focused Product Comparison Summary",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1FFOFvYItY": {
    "title": "Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "7de1c68121c19529ef5610bb4041f215db464f6b",
    "semantic_title": "laying the foundation first? investigating the generalization from atomic skills to complex reasoning tasks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q7AzOSiatW": {
    "title": "Save It All: Enabling Full Parameter Tuning for Federated Large Language Models via Bicycle Black Gradient Descent",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "9f5e8e6bb1a9771daeb6b003c73f158d3ae18de8",
    "semantic_title": "save it all: enabling full parameter tuning for federated large language models via cycle black gradient descent",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=7vP8ybn81d": {
    "title": "GCFCE: The Evaluation of the Security Vulnerability-Fixing Capabilities of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=quwlR4TI0q": {
    "title": "PrivTextBench: a Unified Benchmark for Text-to-Text Privatization",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1jcPbFBHKZ": {
    "title": "i-SRT: Aligning Large Multimodal Models for Videos by Iterative Self-Retrospective Judgment",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "5657d6f4fd14099623a48a042d21767f56908039",
    "semantic_title": "i-srt: aligning large multimodal models for videos by iterative self-retrospective judgment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8C3Iy2D1Rz": {
    "title": "SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "ee5d700eeccfcfe54f9d497643bc55f8606390cd",
    "semantic_title": "speechgpt-gen: scaling chain-of-information speech generation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=qmZWGs0lQO": {
    "title": "Neuro-Symbolic Commonsense Reasoning: A First-Order Logic and Sub-Symbolic Embeddings Framework",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DVxsnjiXYQ": {
    "title": "Benchmarking Semantic Sensitive Information in LLMs Outputs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "f5f6da6dc15403a31b1f8e44273a37084e495ba5",
    "semantic_title": "benchmarking llms on the semantic overlap summarization task",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yjut3ol0Zb": {
    "title": "SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BdypeVSMNt": {
    "title": "Survival of the Safest: Multi-Objective Pareto Optimization of AI Prompts through Evolutionary Methods",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nz2CpFM9PH": {
    "title": "Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation for Logical Reading Comprehension",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zcFbcwrx7O": {
    "title": "Mitigating Hallucinations in Large Language Model Agent Communication with Uncertainty Estimations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "9fb16a675782ec6913d83c1dff1054007ec23f7a",
    "semantic_title": "debunc: mitigating hallucinations in large language model agent communication with uncertainty estimations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zY9ZqspbW5": {
    "title": "MindShift: Analyzing Language Models' Reactions to Psychological Prompts",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mGocca4jea": {
    "title": "Annotated Dataset of Human Rights Violations in the Russia-Ukraine Conflict: A Comparative Analysis of Human and ChatGPT Annotations",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zE6okqoT0N": {
    "title": "Synergistic Multi-Agent Framework with Trajectory Learning for Knowledge-Intensive Tasks",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3IMINODsts": {
    "title": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dITC0cxUFG": {
    "title": "Jailbreak Paradox: The Achilles' Heel of LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "af6fdb359dc1f1d11e81bb723bc34d06e13c1436",
    "semantic_title": "[wip] jailbreak paradox: the achilles' heel of llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b2HTD5Q3YK": {
    "title": "DiaCBT: A Fine-Grained Dialogue Benchmark with Cognitive Behavioral Therapy for Psychological Counseling",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MpH54T6uSM": {
    "title": "Cognitive Map for language models: Optimal Planning via Verbal Representation of World Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "5d6ba252b04b6ffa9c5e1de6649e638360ffb7ba",
    "semantic_title": "cognitive map for language models: optimal planning via verbally representing the world model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lBPk1LEq0b": {
    "title": "Reward Steering with Evolutionary Heuristics for Decoding-time Alignment",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "11353023c4fd1672c7843fc5c636f4413e7c7dbb",
    "semantic_title": "reward steering with evolutionary heuristics for decoding-time alignment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YnNNcteDmT": {
    "title": "A Generative Framework for Continual Named Entity Recognition",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvF6gyqN0Z": {
    "title": "Persona Dynamics : Unveiling the Impact of Persona Traits on Agents in Text-Based Games",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1T99z4CsMi": {
    "title": "Kunpeng: A Large Language Model, Instruction Data and Evaluation Benchmark for Web Novel Translation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ugKvWZHe42": {
    "title": "On the Decision-Making Abilities in Role-Playing using Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "db39f628fefafccc0932703377261c1910ad8bdb",
    "semantic_title": "on the decision-making abilities in role-playing using large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TTQmSmYbwh": {
    "title": "Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "012e24f7c4dc92f26b41cacca501c0d4355b02fd",
    "semantic_title": "towards vision enhancing llms: empowering multimodal knowledge storage and sharing in llms",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=NZ8VsHx1ls": {
    "title": "MINGLE: Multi-level Integration via Language Guided Learning\\\\for Comprehensive Visual Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "0681c7ad009859c22e7638d4636d13e74cca08c6",
    "semantic_title": "image fusion via vision-language model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2FeactcFrW": {
    "title": "From Unknown to Known: An AI Coaching Problem in Open-World Environments",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sme3gqbIeq": {
    "title": "You Comprehended the Whole Paper: Academic Reviews Enhance LLM Long-Context Capabilities",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0JhjzHfC7a": {
    "title": "SentiXRL: An advanced large language Model Framework for Multilingual Fine-Grained Emotion Classification in Complex Text Environment",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MpMZujJXgs": {
    "title": "Could Large Language Models (LLMs) serve as an alternative to manual evaluation for Detoxification?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6GTC05KX9L": {
    "title": "Psy-Insight: Mental Health Oriented Interpretable Multi-turn Bilingual Counseling Dataset for Large Language Model Finetuning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BBVAk7PXjB": {
    "title": "Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "7d5d3cf01eea6e0be249ed69baed6fdc1fc3f884",
    "semantic_title": "language agents for detecting implicit stereotypes in text-to-image models at scale",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Xiv0VhiEdv": {
    "title": "Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "b396bc377404e188251f66fea092b2cead9396b0",
    "semantic_title": "enhancing fine-grained image classifications via cascaded vision language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UXDcqflTMx": {
    "title": "Text-Vision Interacted and Auxiliary Sample Generation for Document Layout Analysis",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pxTWKPVg2x": {
    "title": "ScaleBiO: Scalable Bilevel Optimization for LLM Data Reweighting",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "275a8fc80dbae72c4b60b14e7cd0d0a85df5f8a1",
    "semantic_title": "scalebio: scalable bilevel optimization for llm data reweighting",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Cr7pVMsj9o": {
    "title": "DynDST: A Dynamic Dialogue State Tracking Dataset for Assessing the Conversational Adaptability of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uwsbtGMM9j": {
    "title": "How to Classify Extremely Long Texts: Age Rating Classification of Movie Scripts Using RAG Pipeline",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u0vN077uKa": {
    "title": "Assessing the Robustness of Retrieval-Augmented Generation Systems in K-12 Educational Question Answering with Knowledge Discrepancies",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QHi4PFXh9a": {
    "title": "CASK: Causal Knowledge Representations for Question-Answering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "8ed2cd0a5c1aa44c4fd16bcf3bac7d2d02b27758",
    "semantic_title": "learning causal representations for multi-hop question answering over knowledge graphs",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=XjCuwJm49E": {
    "title": "Gradient-Mask Tuning Elevates the Upper Limits of LLM Performance",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "c9c5cd8e3ec5a5eb786cf2b255064d96e97c8fec",
    "semantic_title": "gradient-mask tuning elevates the upper limits of llm performance",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p5yIVJ5lj2": {
    "title": "Human-Interpretable Adversarial Prompt Attack on Large Language Models with Situational Context",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "7cf2dd505b53e690ce0ff1fd15cc55f127841819",
    "semantic_title": "human-interpretable adversarial prompt attack on large language models with situational context",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wXiG0ojb6W": {
    "title": "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "72f51c3ef967f7905e3194296cf6fd8337b1a437",
    "semantic_title": "codechameleon: personalized encryption framework for jailbreaking large language models",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=vcQLCwm22f": {
    "title": "Towards a Robust Multimodal Framework for Hate Detection: A Study on Video vs. Image-based Content",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u4xfPr409C": {
    "title": "Not All Bias Bad: Balancing Rational Deviations and Cognitive Biases in Large Language Models Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "89f6b026f44af73e2159d2638d3fe7a09f2033e9",
    "semantic_title": "not all bias is bad: balancing rational deviations and cognitive biases in large language model reasoning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jZeuVLt9Bs": {
    "title": "DocDoc: a general framework for open-ended long text generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jb1oi5Vtj7": {
    "title": "Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ne5dt45ofU": {
    "title": "Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural News Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "b7c8feb968b5bef5096c14a7a7e8f4341b53acc9",
    "semantic_title": "train once, use flexibly: a modular framework for multi-aspect neural news recommendation",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=GA2yx9sf5z": {
    "title": "A Structured Approach to Re-Encoding Short-Term Memory of Large Language Models in Conversations",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G3FVqp9WzB": {
    "title": "Inducing Vulnerable Code Generation in LLM Coding Assistants",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "5759d7c7038bfb7b5f1d47539c8348b04f5e0ee7",
    "semantic_title": "deceptprompt: exploiting llm-driven code generation via adversarial natural language instructions",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=8ftqjYE7rq": {
    "title": "Intelli-Planner: Towards Participatory and Customized Urban Planning via Large Language Model Empowered Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KbjaGPBT2L": {
    "title": "LLM Safety for Children",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "6d5e03343e4982a33bd1633975b0e3ea7b32e79a",
    "semantic_title": "ss-bench: a benchmark for social story generation and evaluation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JnMekh6L0Q": {
    "title": "PhyBench: A Physical Commonsense Benchmark for Evaluating Text-to-Image Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "22afcb17db1ead04338f8fc6ae42ca7f0f7d5ac8",
    "semantic_title": "phybench: a physical commonsense benchmark for evaluating text-to-image models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=VS3rq3tnaG": {
    "title": "Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "e170227c1363b3d4142405948867e49b30bab46e",
    "semantic_title": "graphusion: leveraging large language models for scientific knowledge graph fusion and construction in nlp education",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FDyzPHBW4Q": {
    "title": "ObjVariantEnsemble:Advancing Point Cloud LLM Evaluation in Challenging Scenes with Subtly Distinguished Object",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i0FOif69oW": {
    "title": "Don't Say No: Jailbreaking LLM by Suppressing Refusal",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "bcb694c1fbf55ce884cbe89ff7c489f933586ef7",
    "semantic_title": "don't say no: jailbreaking llm by suppressing refusal",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=P0xzgCm3Oy": {
    "title": "TEaR: Improving LLM-based Machine Translation with Systematic Self-Refinement",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "85727f8633e728d23d9c6d9e77a08675465c555e",
    "semantic_title": "tear: improving llm-based machine translation with systematic self-refinement",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=EYOx0GeRTp": {
    "title": "On the Effect of Steering Latent Representation for Large Language Model Unlearning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "2c2f9edbf199f9699297a2beb30f61fac75ae853",
    "semantic_title": "on effects of steering latent representation for large language model unlearning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P3uy9Sarza": {
    "title": "BDA: Bangla Text Data Augmentation Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "c7c908ace34ca493ee256e4297d1854a0608d8f0",
    "semantic_title": "evaluation metrics for text data augmentation in nlp",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5zDgrgQSuz": {
    "title": "GraphEdit: Large Language Models for Graph Structure Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0TMCh60MyP": {
    "title": "PyBench: Evaluating LLM Agent on various real-world coding tasks",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yFhdCD3clp": {
    "title": "Language-based Valence and Arousal Expressions between the United States and China: a Cross-Cultural Examination",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VWekQGYNTr": {
    "title": "Scoping Matters: Assessing Multimodal Model Editing via Dynamic Evaluation of Vision Questions Answering",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BPDV18lgBX": {
    "title": "A Comprehensive Study of Multimodal Large Models on Knowledge-Intensive Visual Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vmcGwh6NAP": {
    "title": "Enhancing Large Language Models for Document-Level Translation Post-Editing Using Monolingual Data",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uwKkozdTYt": {
    "title": "When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J6CX3N2ups": {
    "title": "REPANA: A Reasoning Path Navigation Framework for Large Language Models to Universally Reason over Heterogeneous Data",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OwlLLuQvzy": {
    "title": "DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NwoMm6kLno": {
    "title": "ProsodyFM: Unsupervised Phrasing and Intonation Control for Intelligible and Expressive Speech Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SYoLl01otv": {
    "title": "Huatuo-26M, a Large-scale Chinese Medical QA Dataset",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XFoxWzolL1": {
    "title": "Adaptive Reinforcement Learning Planning: Harnessing Large Language Models for Complex Information Extraction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mMJndR6tAF": {
    "title": "Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f82fmg18fx": {
    "title": "ClinTexTS: Improving Patient-centered Care through Text Simplification of Clinical Narratives with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=stJoFiTlnc": {
    "title": "Confidence Breeds Success: Improving Fake News Video Detection via LVLM-Assisted Inference",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n08YuP2nZ3": {
    "title": "Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wu7bTonPfG": {
    "title": "Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kRWOrobYxC": {
    "title": "On Context-aware Detection of Cherry-picking in News Reporting",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NevSyaNyx5": {
    "title": "Federated Recommendation with Differentially-Private and Multi-Agent Conversations",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TdJsprtZem": {
    "title": "Tuning Language Models by Mixture-of-Depths Ensemble",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ljJBBhMcON": {
    "title": "Sports Intelligence: Assessing the Sports Understanding Capabilities of Language Models through Question Answering from Text to Video",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ewQzNGOs3l": {
    "title": "Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with Receptive-Field-Aware Attention Weighting",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D4argRUaYh": {
    "title": "Adversarial Noisy Instruction Tuning for Enhancing NLU in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=47u8eyuOr5": {
    "title": "SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NbWt58tRox": {
    "title": "Large Language Models are Good Annotators for Type-aware Data Augmentation in Grammatical Error Correction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvX52YynDr": {
    "title": "Language and Multimodal Models in Sports: A Survey of Datasets and Applications",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V602Oqu718": {
    "title": "Is Generative Communication between Embodied Agents Good for Zero-Shot ObjectNav?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CrojreecOG": {
    "title": "CLERC: A Dataset for Legal Citation Retrieval and Retrieval-Augmented Analysis Generation (which happen to be really really long)",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FKuvYtt4TQ": {
    "title": "Investigating How LLM-generated Data Affects the Evaluation of Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wH7oNs7pqk": {
    "title": "Direct Prompt Optimization with Continuous Representations",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zWlRgVHIfJ": {
    "title": "LLMs Meet Long Video: Advancing Long Video Question Answering with An Interactive Visual Adapter in LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ktkn0qzLYM": {
    "title": "Self-Verification for Computer-aided Design Code Generation with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GYkyOZR7a5": {
    "title": "Latent Dialogue Policy Planning without Policy Annotation and Dynamic Interaction for Effective Proactive Dialogues",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qkOaKOWy5E": {
    "title": "SR-FoT: A Syllogistic-reasoning Framework of Thought for Large Language Models Tackling Knowledge-based Reasoning Tasks",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4TFlX5Ps3u": {
    "title": "News Without Borders: Domain Adaptation of Multilingual Sentence Embeddings for Cross-lingual News Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1x8S05HUOh": {
    "title": "G2RG: Grounding Prompt Guided Framework for Report Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aRDOYsu51E": {
    "title": "Balanced Multi-Objective Optimization as a Multi-Arm Bandit Problem",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d5ZQE0SotN": {
    "title": "SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lF8FmsMByO": {
    "title": "Unc-TTP: A Method for Classifying LLM Uncertainty to Improve In-Context Example Selection",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9i8z3sC6Fe": {
    "title": "LoFTI: Localization and Factuality Transfer to Indian Locales",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LBWttx1WAF": {
    "title": "GAIfE: Using GenAI to Improve Literacy in Low-resourced Settings",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BxyFGHeYLY": {
    "title": "Healing Powers of BERT: How Task-Specific Fine-Tuning Recovers Corrupted Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R38o8dLQnw": {
    "title": "Efficient and Responsible Adaptation of Large Language Models for Robust Top-k Recommendations",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cpkLFypIMO": {
    "title": "From English to Second Language Mastery: Enhancing LLMs with Cross-Lingual Continuation Instruction Tuning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IiLvT3ofe6": {
    "title": "Fact Decomposition is More Effective: A Post-hoc Retrieval Approach to Attributed Question Answering with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IaDLZdBEuo": {
    "title": "Unlock the Power of Frozen LLMs in Knowledge Graph Completion",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zZuSwZTPAY": {
    "title": "Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for Multi-hop Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aTL8DXlRnd": {
    "title": "MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eGn3TYJJbP": {
    "title": "Interactive Evaluation for Medical LLMs via Task-oriented Dialogue System",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UfYfnjrN6e": {
    "title": "No effort is ever wasted\": Learning to Detect AI-generated Text from Unseen LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LQwMOkuVzp": {
    "title": "Valley: Video Assistant with Large Language Model Enhanced Ability",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nuvYQHD8q2": {
    "title": "RRP: Boosting Text-to-SQL Generation by Ranking Reward Policy based on Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5zUh4VA9IO": {
    "title": "TinyLLM: Learning a Small Student from Multiple Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=022O6nI10X": {
    "title": "Beyond sentiment analysis for FOMC stock return prediction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FcBeG1O1aT": {
    "title": "Detection and Pinyin Improved Language Model for Chinese Grammatical Error Correction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NKirbMrl9Z": {
    "title": "Phonetization Improves Lexical Retrieval in Agglutinative Languages",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vGbXNYo9JH": {
    "title": "Large Language Models as Event Forecasters",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pulNBMA5uH": {
    "title": "Fine-Grained Knowledge Unlearning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kQDxNPcDSk": {
    "title": "Graph-Score: A Graph-grounded Metric for Audio Captioning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=plD0EtdQfP": {
    "title": "Exploiting the Vulnerability of Large Language Models via Defense-Aware Architectural Backdoor",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nrh06BSQks": {
    "title": "JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XvZfglYQ3S": {
    "title": "Hierarchical ReAct Ensembles for Human-Level Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4hlrz5DzdB": {
    "title": "ExDDI: Explainable Drug-Drug Interaction Prediction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E8pk1i7ntF": {
    "title": "Graph Reasoning Task: A Step-by-Step Executable Benchmark for Mathematical Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5NErohfcHa": {
    "title": "Quantitative Framework for Word-Color Association and Application to 20th Century Anglo-American Poetry",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DCZ8841uBV": {
    "title": "Bridging Vision and Action for Navigation in Continuous Environments",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g53qiV0h1Y": {
    "title": "Full-ECE: A Metric For Token-level Calibration on Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W62WwI6FVl": {
    "title": "Palo: A Polyglot Large Multimodal Model for 5B People",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lewe4NJY4Z": {
    "title": "HerculesBench: Gradual Steps Towards AGI with a Bilingual Multi-discipline Hierarchical and Multimodal Benchmark",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=scAOC14MmM": {
    "title": "Examining Language Model Self-Play Beyond Zero-Sum Games",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0i2JfLjXbO": {
    "title": "QMixTuner: Learning to Optimize Mixed-Precision Quantization",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jsFzILeYGH": {
    "title": "Dual Grained Quantization: efficient fine-grained quantization for LLM",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pl01EmqWY8": {
    "title": "Words Matter: Can Proprietary Large Language Models Handle Word-Substitutions?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AJxlZCS03G": {
    "title": "Rethinking Sentence Embedding with Mean Pooling: Is it Reasonable to Summarize a Point Cloud with a Single Vector?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ijg2oF9wfg": {
    "title": "Inter-Lingual Semantic Map: Dictionaries that Visualizes Inter-Lingual Semantic Network/Space",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nG1iy3lBGc": {
    "title": "Non-instructional Fine-tuning: Enabling Instruction-Following Capabilities in Pre-trained Language Models without Instruction-Following Data",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FAIFJP1cIF": {
    "title": "CA-TriNet: Digging Image-to-Text Essence by Adaptive Co-Attention Head Distribution and Triple-LSTM Module for Medical Report Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NDjhJ2Tx16": {
    "title": "Harnessing GNN-LLM Synergy for Predicting and Explaining Restaurant Survival",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lox79lYSZB": {
    "title": "Exploring the Jungle of Bias: Political Bias Attribution in Language Models via Dependency Analysis",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XXpJeNX204": {
    "title": "TIFG: Text-Informed Feature Generation with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y4ZGqwJ3D6": {
    "title": "A Deep Learning based Approach for Sindhi Poet Classification using Couplets",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xj1uMpSLPy": {
    "title": "Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mckNboN9dh": {
    "title": "Promises, Outlooks and Challenges of Diffusion Language Modeling",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w2ZSoSzg8v": {
    "title": "Collaborative Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YNEgYGc9GT": {
    "title": "LLM Internal States Reveal Hallucination Risk Faced With a Query",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=czbSpuJgke": {
    "title": "Controlling Summarization Length Through EOS Token Weighting",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GzCrJiUi2R": {
    "title": "Detecting ChatGPT Modification in Global Education Research: A Case Study Across Translated and Open Access Contexts",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pBCa0xjkeD": {
    "title": "MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jV244jQuBT": {
    "title": "Hypothetical-Deductive Reasoning for Event Causality Identification",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PZvuzagvmZ": {
    "title": "Investigating Human Values in Online Communities",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4xC9tgBCzR": {
    "title": "Advancing Self-Enhancement of Reasoning via Historical Data Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y9JEBCbhjz": {
    "title": "Social Norms in Cinema: A Cross-Cultural Analysis of Shame, Pride and Prejudice",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wy5BTAnp4l": {
    "title": "Poetry2Image: An Iterative Correction Framework for Images Generated from Chinese Classical Poetry",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TG5rILd5Z0": {
    "title": "Learning to Poison Large Language Models During Instruction Tuning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RaUZeM5tAA": {
    "title": "ARTICLE: Annotator Reliability Through In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5sqHCFffWo": {
    "title": "Qibo: A Large Language Model for Traditional Chinese Medicine",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nHzMOOr9r4": {
    "title": "NAT3DSound: 3D Spatial Sound Field Synthesis with Multi-Modal Non-Autoregressive Transformer",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7KxqpQpz1z": {
    "title": "Auto-ICL: In-Context Learning without Human Supervision",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oyIyllLNSu": {
    "title": "Fully Inductive Link Prediction on N-ary Relational Facts via Semantic Hypergraph Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FgdcKodbeF": {
    "title": "Two Heads Are Better Than One: Distilling Task-Identify-and-Solve Rationales from Multiple LLM Teachers to Improve Student Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mZag77mJpU": {
    "title": "Multi-Document Grounded Synthetic Dialog Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ojDTdaNvjJ": {
    "title": "All that glitters\": Quality Evaluations with Unreliable Model and Human Annotations",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vcs1WkxXi2": {
    "title": "Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models via Watermarking",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h8wWjJ2gBK": {
    "title": "Multi-modal Multi-agent Debate-based Dialectical Knowledge Transfer Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GuQXNxCM2M": {
    "title": "Bi-LLMs: Bi-directional Large Language Models for Information Extraction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eajmBArdwI": {
    "title": "Generative Annotation for ASR Name Entity Correction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w81ziXFCLT": {
    "title": "On the Consistency of Prompt Preference across LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LqfcQQHHuu": {
    "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7hZyeQxd6n": {
    "title": "Recording for Eyes, Not Echoing to Ears: Contextualized Spoken-to-Written Conversion of ASR Transcripts",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lXcxte1ZuA": {
    "title": "Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eznmHBknuD": {
    "title": "Enhancing the Multi-Attribute Fairness of LLM-based Recommenders: A Mixure-of-Experts Contrastive Learning Approach",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ZP2JcGHIn": {
    "title": "CMed-Eval: A Systematic Chinese Medical Benchmark for Evaluating Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=62EtEWPTsB": {
    "title": "KARLM: Enhancing LLM-based Recommendation Systems with Knowledge Bases",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ulI036LMiw": {
    "title": "DParT: Transferring knowledge between languages changing a few weights",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HjgXnYmwtb": {
    "title": "Fake News Detection: It's All in the Data!",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qdrl0n6cMa": {
    "title": "Can LLMs Recognize Toxicity? Definition-Based Toxicity Metric",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a69IHVYumi": {
    "title": "Is It Live Streaming?\": Examining the Streamer's Understanding of \"Liveness\" From a Semi-Experimental Conversation Analytic Approach",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4YI6uh91Jg": {
    "title": "Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fi9ZUF8A4j": {
    "title": "Enhancing Knowledge Distillation of Large Language Models through Efficient Multi-Modal Distribution Alignment",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CqR5Fx47Mc": {
    "title": "CHEW: A Dataset of CHanging Events in Wikipedia",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DuwmovyYuw": {
    "title": "FlexAlign: A Plug-and-Play Method for Language Models Target Domain Alignment",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDZKKFxY4t": {
    "title": "KGPA: Robustness Evaluation for Large Language Models via Cross-Domain Knowledge Graphs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H2uKPyyLS3": {
    "title": "Similarity is Not All You Need: Endowing Retrieval-Augmented Generation with Multiâ€“layered Thoughts",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p9HJFnByOX": {
    "title": "On Incorporating Prior Knowledge Extracted from Pre-trained Language Models into Causal Discovery",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eZ4leTW30G": {
    "title": "Enhancing Job Matching: BERT-Based Occupation, Skill and Qualification Entity Linking with the ESCO and EQF taxonomies",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rr8PfzoZ4v": {
    "title": "Towards Expert-Level Chinese Tax Consultancy: Enhancing Large Language Models for Accurate and Contextualized Advice",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yYSA1Zgamr": {
    "title": "The Male CEO and the Female Assistant: Gender Biases in Text-To-Image Generation of Dual Subjects",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oG9K4r1aeJ": {
    "title": "CLAIM: Mitigating Catastrophic Forgetting in Continual Instruction Fine-tuning Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NhAXmO4aPy": {
    "title": "LLM Analyst: What stocks do you recommend today?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uVjSlJ7bi1": {
    "title": "Language Models Can Think Faster: Compressing Chain-of-Thought without Compromising Effectiveness",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pwAwwu4Fh6": {
    "title": "eMART: An Efficient Multi-Agent System with LLM Specialized MCTS",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xIrqznJOfj": {
    "title": "Thought-Like-Pro: Enhancing Reasoning of Large Language Models through Self-Driven Prolog-based Chain-of-Though",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=atkEnV4LiY": {
    "title": "When natural language is not enough: The limits of in-context learning demonstrations in multilingual reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ubzPGpYA1V": {
    "title": "Securing Large Language Models: Effective Refusal Pattern Alignment through Self and Cross-Model Distillation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YXeL6szLv2": {
    "title": "Causally Interpretable Offline Reinforcement Learning for Task-oriented Dialogue Policy",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f0M9ODfKoR": {
    "title": "LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nuG8MEfqAF": {
    "title": "Tuning-Free Accountable Intervention for LLM Deployment - A Metacognitive Approach",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ALXFQN4KeJ": {
    "title": "Fairness at Every Intersection: Uncovering and Mitigating Intersectional Biases in Multimodal Clinical Predictions",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YiSLhqKoDM": {
    "title": "EnriCo: Enriched Representation and Globally Constrained Inference for Entity and Relation Extraction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gfe0nh7XCf": {
    "title": "Semantic Side-Channels to Jailbreak Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2FWE8zZIPm": {
    "title": "Transformers at the Edge: Challenges and Recipes for Productizing Neural Machine Translation for Regular Languages",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oMJ0fAlM3y": {
    "title": "Capabilities of LLMs on Casual Inference",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=geJTmUEUcW": {
    "title": "Evaluating Gender Bias in the Translation of Gender-Neutral Educational Professions from English to Gendered Languages",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KTD6M7VkqP": {
    "title": "Argumentative Large Language Models for Explainable and Contestable Claim Verification",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LV66I3Jtey": {
    "title": "LOLAMEME: LOGIC, LANGUAGE, MEMORY, MECH- ANISTIC FRAMEWORK",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pvsLtjDXpA": {
    "title": "RLDF: Reinforcement Learning from Multi-role Debates as Feedback for Bias Mitigation in LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vZUIHtG7kX": {
    "title": "LoRA 2 : Multi-Scale Low-Rank Approximations for Fine-Tuning Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fu4sLIRHF0": {
    "title": "A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mi1kBh3Brp": {
    "title": "GlossGPT: GPT for Word Sense Disambiguation using Few-shot Chain-of-Thought Prompting",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YhX7bb0MYO": {
    "title": "From PEFT to DEFT: Parameter Efficient Finetuning for Reducing Activation Density in Transformers",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fEt1TDEIYj": {
    "title": "LightDefense: A Lightweight Uncertainty-Driven Defense against Jailbreaks via Shifted Token Distribution",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QDXr4VBPgZ": {
    "title": "BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VYT433CufJ": {
    "title": "Learning to Instruct with Implicit Harmfulness: Transferable Black-Box Jailbreak on Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eqj5L7H39n": {
    "title": "Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kWs0B73KAG": {
    "title": "MoESD: Mixture of Experts Stable Diffusion to Mitigate Gender Bias",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E4JmTsdXhr": {
    "title": "Recurrent Context Compression: Efficiently Expanding the Context Window of LLM",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ozHZzzH58": {
    "title": "MedCalc: Bridging Medical Calculator and LLM Agent with Nested Tool Calling",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VHwnKMtC0k": {
    "title": "FinDABench: Benchmarking Financial Data Analysis Ability of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RUW4vDDfnS": {
    "title": "ProSwitch: Knowledge-Guided Instruction Tuning to Switch Between Professional and Non-Professional Text Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yk2mYu4ojN": {
    "title": "From WRIME to WRIME-TC: Enhancing Emotional Analysis of Writers and Readers with Temporal Context",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JtpLNNfQ5T": {
    "title": "Semantic Information: A difference that makes a difference",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B5hkYIa11V": {
    "title": "SimsChat: A Customisable Persona-Driven Role-Playing Agent",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JwVj4izv4c": {
    "title": "Enhancing Relation Extraction via Supervised Rationale Verification and Feedback",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vbUm0UmVXF": {
    "title": "Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction Tuning for Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vb37Py4936": {
    "title": "E2TP: Element to Tuple Prompting Improves Aspect Sentiment Tuple Prediction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SuCdnvyjkl": {
    "title": "Large Language Models are Multi-Task Chain-of-Thought Prompting Optimizers",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b5LdTQCTuP": {
    "title": "Improving Visual Q/A in Autonomous Driving Scenerio's Using LLM",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PdU1ujmDyp": {
    "title": "AutoDCM: A Novel Framework for Automatic Relation Extraction Dataset Construction via Distant Supervision and Large Language Models for Low-Resource Tasks",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8nphDi6E0l": {
    "title": "ObscurePrompt: Jailbreaking Large Language Models via Obscure Input",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LmnvN0C01K": {
    "title": "mGDTP: An End-to-End Framework for Long Document Modeling with mix-Granularity Dynamic Token Pruning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=40vd76KUoi": {
    "title": "The More Moral an LLM is, the Smarter It Becomes: Analyzing the Impact of Moral Development on General Problem-Solving in LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RHo8XFlMcA": {
    "title": "All Against Some: Efficient Integration of Large Language Models for Message Passing in Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZdT1OYeVjP": {
    "title": "Using LLMs to Aid Annotation and Collection of Clinically-Enriched Data in Bipolar Disorder and Schizophrenia",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Oj1rUxbmh": {
    "title": "Who speaks for the autistic community? An NLP-driven investigation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XacdHuNC1L": {
    "title": "MergeNet: Knowledge Migration across Heterogeneous Models, Tasks, and Modalities",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=egYY7YpRFQ": {
    "title": "Integrating information from evolving layers to reduce hallucinations through Contrastive Decoding",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oNiskX6jWP": {
    "title": "Investigating Recurrent Transformers with Dynamic Halt",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RcoX5hqhS4": {
    "title": "Importance Weighting Can Help Large Language Models Self-Improve",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lMAGEzipXK": {
    "title": "Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding for Neural Machine Translation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bb9BBq2oCp": {
    "title": "Text-Conditioned Multimodal Alignment and Consistency Modeling for Fake News Video Detection",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=21SAZbReVg": {
    "title": "Learning to Check: Enhancing Self-Correction Capabilities in Large Language Models for Reasoning Tasks",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h1AlJPngXy": {
    "title": "Empowering Empathetic Dialogue Generation in Large Language Models via Sensible and Visionary Commonsense Inference",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oJzEHBKc0I": {
    "title": "Explanatory Prompt Injection for Pre-trained Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BOXzslNx1B": {
    "title": "GenC: Generative Contrastive Learning for Passage Retrieval with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yMGbN5t2ja": {
    "title": "RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9TK0IGc0Xy": {
    "title": "Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NpErfKmepm": {
    "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mHCYjukiWe": {
    "title": "LLaMA-MoT: A Cost-Effective Framework for Visual-Linguistic Instruction Tuning Based on Multi-Head Adapters and Chain-of-Thought",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w4MnON8YtC": {
    "title": "PREMISE: Matching-based Prediction for Accurate Review Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rHb99ToADK": {
    "title": "CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual Knowledge Alignment",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M8x89u5ZsO": {
    "title": "Computational Sentence-level Metrics for Predicting Comprehension of Entire Sentence by Humans",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oYYbxu6EgF": {
    "title": "A Survey on Effective Invocation Methods of Massive LLM Services",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IV0KyxFQ9h": {
    "title": "PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V0LoeM0udJ": {
    "title": "Highlighting the Safety Concerns of Deploying LLMs/VLMs in Robotics",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OwGmX2Fv7Y": {
    "title": "PreAct: Prediction Enhances Agent's Planning Ability",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LAAgkc5kNb": {
    "title": "Trigger 3 : Refining Query Correction via Adaptive Model Selector",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FwQkprR53P": {
    "title": "FPTQ: Fine-grained Post-Training Quantization for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YQTUpp9NNi": {
    "title": "sDPO: Don't Use Your Data All at Once",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xa9hawIeIr": {
    "title": "SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SrfbyH3wgh": {
    "title": "Enhancing Continual Learning in Speech-Referring Video Object Segmentation with Mutual Information Maximization",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w4PXp9MiFM": {
    "title": "IMPROVED CONTENT UNDERSTANDING WITH EFFECTIVE USE OF MULTI-TASK CONTRASTIVE LEARNING",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oSwW0fhk2c": {
    "title": "InstructionCP: A Simple yet Effective Approach for Transferring Large Language Models to Target Languages",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QuTMQUaxMs": {
    "title": "Probability of Differentiation Reveals Brittleness of Homogeneity Bias in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F2uPrHJ4iB": {
    "title": "A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jbjKZ0lNZm": {
    "title": "Sentence-level Aggregation of Lexical Metrics Correlate Stronger with Human Judgements than Corpus-level Aggregation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PESflaYU8v": {
    "title": "Enhance the Robustness in Text-Centric Multimodal Alignments",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=inOBXcvmlz": {
    "title": "Enhancing Trustworthiness in Multimodal Large Language Models via Penalization of Language Priors",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e5ZYHZEQye": {
    "title": "diffusion models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rQ7w4VhdRQ": {
    "title": "Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzZRdeJNv1": {
    "title": "PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c2UQ6x3t0w": {
    "title": "Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y68B76sbaR": {
    "title": "Advancing African-Accented English Speech Recognition: Epistemic Uncertainty-Driven Data Selection for Generalizable ASR Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0OhL0VmPvO": {
    "title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FSALVvACFQ": {
    "title": "Improving Fuzzy Match Augmented Neural Machine Translation through Synthetic Data",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xCur5qLNln": {
    "title": "Can Large Language Models Create New Knowledge for Spatial Reasoning Tasks?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M9obtJdqgr": {
    "title": "Applying ESGBert to Detect Climate-Related Passages from German Financial Reports",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xVSJ4e9thv": {
    "title": "Cracking Factual Knowledge: A Comprehensive Analysis of Degenerate Knowledge Neurons in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9L4McQnj6J": {
    "title": "Increasing Trust in Language Models through the Reuse of Verified Circuits",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aKusABh8ZS": {
    "title": "Towards a Client-Centered Assessment of LLM Therapists by Client Simulation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7oPGJi0mdr": {
    "title": "How Well Can Multimodal LLMs Write Code as Vision-Enabled Agents?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zSdCoyjHa8": {
    "title": "Grimoire is All You Need for Enhancing Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rMLNzLRl1s": {
    "title": "Intention Analysis Makes LLMs A Good Jailbreak Defender",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8NgaPnk1J3": {
    "title": "UPO: Unpaired Preference Optimization for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MPmZO69Hr1": {
    "title": "ConceptPsy: A Benchmark Suite with Conceptual Comprehensiveness in Psychology",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PRMvw2rKf2": {
    "title": "Xrphonetic: Akshara-based Phonetic String Similarity",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F8tk4TEyQg": {
    "title": "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TgOpyPW8Li": {
    "title": "The Generation Gap: Exploring Age Bias Underlying in the Value Systems of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J92FmsN5Az": {
    "title": "Find Before you Fine-Tune (FiT): How to Identify Small-Scale LLM Suitable for Cybersecurity Question-Answering Tasks?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pDv2FMBBvc": {
    "title": "A Systematic Literature Review of Adapter-based Approaches to Knowledge-enhanced Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v9lPQp0Dyi": {
    "title": "LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qpZWYVxMQ6": {
    "title": "BiMediX: Bilingual Medical Mixture of Experts LLM",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=py5HlgQkQR": {
    "title": "AgentsCourt: Building Judicial Decision-Making Agents with Court Debate Simulation and Legal Knowledge Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2NHJM9R2kP": {
    "title": "Data-Scarce Event Argument Extraction: A Dynamic Modular Prompt Tuning Model Based on Slot Transfer",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=snXfD8ecVK": {
    "title": "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JYOsj0hy39": {
    "title": "What Is Missing in Multilingual Visual Reasoning and How to Fix It",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1qWIPsXFH": {
    "title": "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pGEu03b58f": {
    "title": "People will agree what I think: Investigating LLM's False Consensus Effect",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vpM0pfm7C5": {
    "title": "Rethinking the Role of Proxy Rewards in Language Model Alignment",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zeo4HMnPtt": {
    "title": "EASY: Enhanced Analysis Approach for Implicit Hate Speech Yield â€“ Bridging Human Insight and Algorithmic Precision in Social Media Discourse",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4OkWwjsJfp": {
    "title": "Generation with Dynamic Vocabulary",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0w1lzBv26x": {
    "title": "MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XfOFYVZtRx": {
    "title": "ANALOBENCH: Benchmarking the Identification of Abstract and Long-context Analogies",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mpRckjTbtb": {
    "title": "Can Large Language Models Unlock Novel Scientific Research Idea?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MskMzRfTsS": {
    "title": "Interaction Matters: An Evaluation Framework for Interactive Dialogue Assessment on English Second Language Conversations",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h7PEBMI7eX": {
    "title": "Facts-and-Feelings: Capturing both Objectivity and Subjectivity in Table-to-Text Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=32ojYK2xFE": {
    "title": "Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9r6TZATiMF": {
    "title": "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oiyyllbzra": {
    "title": "PsyGUARD: An Automated System for Suicide Detection and Risk Assessment in Psychological Counseling",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VqqNoSuxSa": {
    "title": "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mHgNzfiApQ": {
    "title": "ReWIRED: Instructional Explanations in Teacher-Student Dialogues",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PF8Autw1kU": {
    "title": "MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7DhygCqaCL": {
    "title": "A âˆ§ B â‡” B âˆ§ A: Evaluating and Improving Logical Reasoning Ability of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f16qBlCJFN": {
    "title": "Pattern-Aware Chain-of-Thought Prompting in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HEjqNfHCCH": {
    "title": "LAiW: A Chinese Legal Large Language Models Benchmark",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wuTIy79GKB": {
    "title": "LLM-based Multi-hop Question Answering with Knowledge Graph Integration in Evolving Environments",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LMSYvl0hCq": {
    "title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=grW9pYNjTC": {
    "title": "Towards Reliable Latent Knowledge Estimation in LLMs: In-Context Learning vs. Prompting Based Factual Knowledge Extraction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2UVtn2SRaN": {
    "title": "Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CIFvYGsSof": {
    "title": "BELIEFs: Bias-resilient, Multifaceted Evaluation of Language Models in Factual Knowledge Understanding",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SKLpAU2yGh": {
    "title": "Understanding ''Democratization'' in NLP Research",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OXE4qq1X30": {
    "title": "A Simple yet Effective Training-free Prompt-free Approach to Chinese Spelling Correction Based on Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0piiZz7bFE": {
    "title": "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pXERcyY4Ht": {
    "title": "LLM with Relation Classifier for Document-Level Relation Extraction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8aYitCuR6l": {
    "title": "Evaluating Task-Oriented Dialogue Consistency through Constraint Satisfaction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X5czq3asqL": {
    "title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MHDQWs2NvV": {
    "title": "Towards Effective Counter-Responses: Aligning Human Preferences with Strategies to Combat Online Trolling",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U3mt8vi0PB": {
    "title": "Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fQjPKAiNbF": {
    "title": "Bridging the Gap: Integrating Knowledge Graphs into Large Language Models for Complex Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qwD7a6QgsU": {
    "title": "Code-Optimise: Optimising Code Language Models for Functional Correctness and Efficiency",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qU6F1OC2CE": {
    "title": "PEMCAMP: Parameter Efficient Model with Continuous Attentive Multimodal Prompt for Few-Shot Multimodal Sarcasm Detection",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hTBD3LYoqd": {
    "title": "The Art of Data Selection: A Survey on Data Selection for Fine-Tuning Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bziyNACv4B": {
    "title": "We Care: Multimodal Depression Detection and Knowledge Infused Mental Health Therapeutic Response Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oary7aJrfK": {
    "title": "On the Robustness of Editing Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pLQskC8sjI": {
    "title": "Dissecting similarities in self-consistency: An analysis on impact of semantic consistency on language model reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E8UWqLxg7z": {
    "title": "Source Attribution for Large Language Model-Generated Data",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hi5k4PDnX7": {
    "title": "ADELIE: Aligning Large Language Models on Information Extraction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kkvbBhUDo1": {
    "title": "Gotta Catch'em All!: Multi-Generator and Multi-Lingual Benchmark for Detecting LLM-Generated Code Snippets",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0aF93512B4": {
    "title": "MotionBoost: Bootstrapping Image-Language Models with Motion Awareness for Efficient Video Understanding",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=adHtqD9UDn": {
    "title": "FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e7IQBIXPQc": {
    "title": "GeNeRTe: Generating Neural Representations from Text for Classification",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=njZo800gEi": {
    "title": "Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P82OfKIgNw": {
    "title": "IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Large Language Models in E-commerce",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4crBgTTCEB": {
    "title": "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zMpZ2HQsOZ": {
    "title": "Measuring the Robustness of NLP Models to Domain Shifts",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8DQyyumKg8": {
    "title": "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UKJWJXaHhz": {
    "title": "Cross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5oM6aIY0Zm": {
    "title": "Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F2SRTowQ2n": {
    "title": "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HGtxNj2fG9": {
    "title": "Leveraging Domain Knowledge for Efficient Reward Modeling in RLHF: A Case-Study in E-Commerce Opinion Summarization",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qn7WNRJvO9": {
    "title": "Aligning Language Models to Explicitly Handle Ambiguity",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i7nGaiDz7H": {
    "title": "A Narrative Framework for Analyzing Partisan Perspectives in Event Discourse",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lf61bKz8L5": {
    "title": "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kfmjnnblfL": {
    "title": "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dzr7iv0ZMQ": {
    "title": "Exploring Fine-Grained Human Motion Video Captioning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K3PODOXQqL": {
    "title": "Rethinking the Instruction Quality: LIFT is What You Need",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MiW7qui4HY": {
    "title": "Generating Zero-shot Abstractive Explanations for Rumour Verification",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RKoxfZuMIO": {
    "title": "MentalGPT: Harnessing AI for Compassionate Mental Health Support",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HGVQy9ZJQc": {
    "title": "Explaining Mixtures of Sources in News Articles",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HvjlLJy1Xg": {
    "title": "Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural News Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Iucj0JFblr": {
    "title": "Approximate Cluster-Based Sparse Document Retrieval with Segmented Maximum Term Weights",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gWPYS4u4sn": {
    "title": "Beyond English-Centric Machine Translation by Multilingual Instruction Tuning Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C9k27kKMKj": {
    "title": "Flee the Flaw: Annotating the Underlying Logic of Fallacious Arguments Through Templates and Slot-filling",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jw6rhpycTu": {
    "title": "Exploring Large Language Models for Knowledge Graph Completion",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2EYu8T9C50": {
    "title": "Alleviate Prompt Forgetting of RNN-based Language Models Through Prompt Synthetic Gradients",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=47hULeg4nf": {
    "title": "PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Fusion in Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jIEmj2V87d": {
    "title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k3dzpooeQL": {
    "title": "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n1L1KT05dQ": {
    "title": "Breaking Boundaries: Neural Approaches to Interlinear Translation of Classic Texts",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WDJ259NPAI": {
    "title": "DPPA: Pruning Method for Large Language Model to Model Merging",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mEWKp64fvf": {
    "title": "M2QA: Multi-domain Multilingual Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VFzYmcKhLM": {
    "title": "Driving Chinese Spelling Correction from a Fine-Grained Perspective",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=20E6oVPztC": {
    "title": "We Demand Justice!\": Towards Social Context Grounding of Political Texts",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IMNnyrC0Ky": {
    "title": "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1LHT5IsOXB": {
    "title": "Robust Claim Verification Through Fact Detection",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bGi69H0atI": {
    "title": "Knowledge Verification to Nip Hallucination in the Bud",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YNoloCgtWT": {
    "title": "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FlXweLwQk5": {
    "title": "Large Language Models Can Plan Your Travels Rigorously with Formal Verification Tools",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4noo5lXUIJ": {
    "title": "DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bOy7jAyDyc": {
    "title": "Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Proactively Responding to Unknown Questions",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4QxIV9nQIf": {
    "title": "Automatic Evaluation for Mental Health Counseling using LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6B0D6VwC0R": {
    "title": "LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QVq6Fheh0b": {
    "title": "Claim Check-Worthiness Detection: How Well do LLMs Grasp Annotation Guidelines?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NgcsbiImqL": {
    "title": "More than Just Printing \"Hijacked!\": Automatic and Universal Prompt Injection Attacks against Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dkli9LbLKh": {
    "title": "Exploring Multilingual Concepts of Human Values in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0OaINsyHhh": {
    "title": "API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9esVkGJLYv": {
    "title": "Evaluating Large Language Models in Olympic-Level Physics Problems: A Benchmark Dataset",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V2NlbQJbds": {
    "title": "SToRI: Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wUi43kk7iX": {
    "title": "REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific Sentences using Public and Proprietary LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YhkZzwHMxX": {
    "title": "Which questions should I answer? Salience Prediction of Inquisitive Questions",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oj8LMcHWi5": {
    "title": "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3DoU4C8ZWb": {
    "title": "Do Large Language Models Speak All Languages Equally? A Comparative Study in Low-Resource Settings",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G3Tmyh8IpM": {
    "title": "DomainRAG: A Chinese Benchmark for Evaluating Domain-specifc Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FDOlg6MBp7": {
    "title": "Revisiting the Iterative Non-Autoregressive Transformer",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r7lSTDCO6y": {
    "title": "Learning to Extract Structured Entities Using Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mACk3ZVHoU": {
    "title": "Designing Draft Models for Speculative Decoding",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OvZa5b4fdr": {
    "title": "STARD: A Statute Retrieval Dataset for Layperson Queries",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lfiY2bIcfD": {
    "title": "JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oXPPBAn45s": {
    "title": "Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=awzqNHY7hC": {
    "title": "From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LnAw7ecjpl": {
    "title": "AutoCrawler : A Progressive Understanding Web Agent for Web Crawler Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WKdcBbuzjv": {
    "title": "nbDescribe: A Dataset for Text Description Generation from Tables and Code in Jupyter Notebooks with Guidelines",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9IUfu0F50J": {
    "title": "On Demonstration Selection for Improving Language Model Fairness",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eCATrqi1FC": {
    "title": "I Could've Asked That: Reformulating Unanswerable Questions",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sfCZV7T41n": {
    "title": "Spatial-Aware Visual Program Reasoning for Complex Visual Questions Answering",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hR4uTiXz6C": {
    "title": "Assessing and Post-Processing Black Box Large Language Models for Knowledge Editing",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JnAHWgKeSt": {
    "title": "AILQA: Evaluating AI-Driven Legal Question Answering Systems for the Indian Legal System",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ycrYUUStqy": {
    "title": "GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LJWIALJlNQ": {
    "title": "Reformatted Alignment",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rb0AjPCAm2": {
    "title": "Data Augmentation for Less Resourced Summarization",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PlrPx39xo5": {
    "title": "Development of Cognitive Intelligence in Pre-trained Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cd525xu7OM": {
    "title": "Automated Data Curation for Robust Language Model Fine-Tuning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q5WTq5dDs7": {
    "title": "A Survey on Open Information Extraction from Rule-based Model to Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GnGXbekH7M": {
    "title": "Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UsdoZJddRo": {
    "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LB8bER1JBS": {
    "title": "Text2Model: Text-based Model Induction for Zero-shot Image Classification",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6SnNOOkkVb": {
    "title": "Automatic Instruction Evolving for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BR97WPMeEF": {
    "title": "DeAL: Decoding-time Alignment Framework for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TQZFKvu02A": {
    "title": "MedReadMe: A Systematic Study for Fine-grained Sentence Readability in Medical Domain",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0zJ3ApCz5W": {
    "title": "Radical Prompting: Enhancing Chinese Language Models with Character Visual Analysis",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DDqiER1toi": {
    "title": "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VIS6nCehyD": {
    "title": "Evaluating Character Understanding of Large Language Models via Character Profiling From Fictional Works",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mqrylyUmZ3": {
    "title": "NewsEdits 2.0: Learning the Intentions Behind Updating News",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8xfJKMzvrm": {
    "title": "A good pun is its own reword\": Can Large Language Models Understand Puns?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K7TlD3mVGN": {
    "title": "On Diversified Preferences of Large Language Model Alignment",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bhzd8kJhNC": {
    "title": "Assessing and Verifying Task Utility in LLM-Powered Applications",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Su7OLRmo3r": {
    "title": "Translation of Multifaceted Data without Re-Training of Machine Translation System",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zHOtAkQulQ": {
    "title": "Leveraging Large Language Models for Adversarial Attacks on Information Retrieval Systems",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CteryEsG7E": {
    "title": "A Unified Framework for Model Editing",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wyWkUe13dW": {
    "title": "Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SMK34VBntD": {
    "title": "GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e1bM1YofLh": {
    "title": "Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ek4XVXc9vj": {
    "title": "Exploring Large Language Models for Hate Speech Detection in Rioplatense Spanish",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aa2Nf8v6L9": {
    "title": "Defense Against Syntactic Textual Backdoor Attacks with Token Substitution",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hn7tT46INc": {
    "title": "Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qhHxRsN7oG": {
    "title": "Don't Just Pay Attention, PLANT It: Transfer L2R Models to Fine-tune Attention in Extreme Multi-Label Text Classification For ICD Coding",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HEvdEjvKSQ": {
    "title": "D3CODE: Disentangling Disagreements in Data across Cultures on Offensiveness Detection and Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rEIZUkGlbM": {
    "title": "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7DicksqbxB": {
    "title": "WordPlay: An Agent Framework for Language Learning Games",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n2J2vmPJzH": {
    "title": "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nha1mwiGdM": {
    "title": "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Sa4uZjKHL": {
    "title": "Cause and Effect: Can Large Language Models Truly Understand Causality?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ilHAWjNfFh": {
    "title": "Explaining the Hardest Errors of Contextual Embedding Based Classifiers",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PYUgeLtn4F": {
    "title": "Updating CLIP to Prefer Descriptions Over Captions",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jlUUoerguq": {
    "title": "Enhancing Reinforcement Learning with Intrinsic Rewards from Language Model Critique",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hzO9G2XayB": {
    "title": "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6OZm8mkwrp": {
    "title": "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bR7tto04wx": {
    "title": "Learning to Poison Large Language Models During Instruction Tuning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IFdjMXAEnk": {
    "title": "Annotator-Centric Active Learning for Subjective NLP Tasks",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0sqP6XHFku": {
    "title": "LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MjhcZO9Nsp": {
    "title": "Assessing the Role of Lexical Semantics in Cross-lingual Transfer through Controlled Manipulations",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7WfuEoGfrT": {
    "title": "A Data-Driven Approach to Idiomaticity in Russian MWEs Based on Experts' Criteria in Theoretical Linguistics",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UEXqI6KSLl": {
    "title": "Scaling Properties of Speech Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qJWA4kLueV": {
    "title": "Unifying Demonstration Selection and Compression for In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rWn9hGiHMa": {
    "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tkbIJpb6tO": {
    "title": "Multilingual Performance Analysis of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0HdVkWv5p8": {
    "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XWLnASxlIu": {
    "title": "Decomposed Prompting: Probing Multilingual Linguistic Structure Knowledge in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RylDUZzJOy": {
    "title": "Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SStY2rV16I": {
    "title": "GREEN: Generative Radiology Report Evaluation and Error Notation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aLNSiCw4pc": {
    "title": "Argumentative Large Language Models for Explainable and Contestable Decision-Making",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gR6RbmZSwu": {
    "title": "CUTE: Measuring LLMs' Understanding of Their Tokens",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rlBYJyZdwi": {
    "title": "Progressive Knowledge Graph Completion",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TNekcamOfv": {
    "title": "SEAVER: Attention Reallocation for Mitigating Distractions in Language Models for Conditional Semantic Textual Similarity Measurement",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=43UQJyD4jU": {
    "title": "PiT: Prompt Injection Tuning for Pre-trained Lanaguage Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uSf5QvBC6j": {
    "title": "Evaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mlY0Un1mm3": {
    "title": "When Emotional Stimuli meet Prompt Designing: An Auto-Prompt Graphical Paradigm",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qt3MGkkXC3": {
    "title": "Dynamic Planning for LLM-based Graphical User Interface Automation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W4KRfyiBzm": {
    "title": "ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k9KuNVnkeR": {
    "title": "General Collaborative Framework between Large Language Model and Experts for Universal Information Extraction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T0pqsBEQRD": {
    "title": "Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wPPcMxVOsV": {
    "title": "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M7YYzGmH5h": {
    "title": "Enhancing Confidence Expression in Large Language Models Through Learning from Past Experience",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ag1PL4eDCO": {
    "title": "SNAP: Unlearning Selective Knowledge in Large Language Models with Negative Instructions",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=255EPHrB2q": {
    "title": "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E71T5czNBb": {
    "title": "Pseudo-Label Enhanced Prototypical Contrastive Learning for Uniformed Intent Discovery",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yGcW7Os1op": {
    "title": "Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATSA2LhP2W": {
    "title": "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I1ZR1DLYTE": {
    "title": "LUQ: Long-text Uncertainty Quantification for LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzrsefUVx8": {
    "title": "Learning to Paraphrase for Alignment with Model Preference",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0bcts3XUrB": {
    "title": "Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7YlEPNFSp3": {
    "title": "Humans or LLMs as the Judge? A Study on Judgement Bias",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=POvqE9wKlg": {
    "title": "Consecutive Model Editing with Batch alongside HooK Layers",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8t844SNkXx": {
    "title": "Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MirhzDaV8Y": {
    "title": "Controlled Cloze-test Question Generation with Surrogate Models for IRT Assessment",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pd2EVKoU6c": {
    "title": "Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qMqUFxTJ2l": {
    "title": "TRoTR: A Framework for Evaluating the Recontextualization of Text",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WXtTdv5O5e": {
    "title": "Understanding Token Probability Encoding in Output Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=veHK4vbnpY": {
    "title": "Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning AMR Parsing",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dKm4jLXZVr": {
    "title": "QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r270rFSQDa": {
    "title": "Parameter-Efficient Tuning with Information Carrier and Partially Unfrozen Component",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V6Jd4TUnKO": {
    "title": "ChatCRS: Incorporating External Knowledge and Goal Guidance for LLM-based Conversational Recommender Systems",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i35MCC7VHt": {
    "title": "Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HeISnPQMET": {
    "title": "ProSwitch: Knowledge-Guided Instruction Tuning to Generate Professional and Non-Professional Styled Text",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EbzqjIy5Hl": {
    "title": "Large Language Models Are Unconscious of Unreasonability in Math Problems",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WItPPXAgAR": {
    "title": "From Answers to Questions: A Study on Backward Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cFHfla6OYq": {
    "title": "Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1zEPBcx1V": {
    "title": "DetectBench: Can LLMs Piece Together Implicit Evidence for Long-Context Multi-Hop Reasoning?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hTr8s95SYA": {
    "title": "Improving Minimum Bayes Risk Decoding with Multi-Prompt",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6vnAOGZkmy": {
    "title": "Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IWns0d4P9M": {
    "title": "Optimizing Machine Translation through Paraphrasing Ranking",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y5COK6zo2Z": {
    "title": "Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QV9Gt2tEpm": {
    "title": "Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kAUJaVppSA": {
    "title": "Towards Uncovering How Large Language Model Works: An Explainability Perspective",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d3CqANZj9F": {
    "title": "Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B6LCAfbB4f": {
    "title": "The Impact of Reasoning Methods across Languages",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tXMcWTbVlJ": {
    "title": "Entity6K: A Large Open-Domain Evaluation Dataset for Real-World Entity Recognition",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZKEXuZ5ksI": {
    "title": "The Program Testing Ability of Large Language Models for Code",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d98xvzlf8l": {
    "title": "Is there really a Citation Age Bias in NLP?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E6uTejGgth": {
    "title": "FaithScore: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1Mikiy7Fzo": {
    "title": "MAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JAzg4CtkXq": {
    "title": "Systematic Biases in LLM Simulations of Debates",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4wzxMJGbx4": {
    "title": "David vs. Goliath: Can small models leverage LLMs for summarization?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WZkT6h6Kbt": {
    "title": "ChOiRe: Characterizing and Predicting Human Opinions with Chain of Opinion Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cM1QDYKKVm": {
    "title": "LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zc4WaM7WZ0": {
    "title": "Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1NfXdBBVcY": {
    "title": "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G6XAUVZFur": {
    "title": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IqcSNVLu3y": {
    "title": "Mitigating Open-Vocabulary Caption Hallucinations",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xvD1zoieJF": {
    "title": "KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E6Mh36I6a0": {
    "title": "How Can Metaphor not Handle Anomaly? Metaphor Detection with Anomalous Text",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YAeHOnzrMo": {
    "title": "LongAlign: A Recipe for Long Context Alignment of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UrYL85oZuL": {
    "title": "Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jZEIsYKslm": {
    "title": "CoT-Planner: Chain-of-Thoughts as the Content Planner for Few-shot Table-to-Text Generation Reduces the Hallucinations from LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kxMFQVpfQu": {
    "title": "Positive Text Reframing under Multi-strategy Optimization",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RRYyr5EmHA": {
    "title": "Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l0v8xOuKDR": {
    "title": "Temporal Fact Reasoning over Hyper-Relational Knowledge Graphs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mtmFt7zu1K": {
    "title": "\\textit{Dial BeInfo for Faithfulness}: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QC2bQjm6Lt": {
    "title": "Fine-grained and Explanable Factuality Evaluation for Multimodal Summarization",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IbpxiUULD8": {
    "title": "ManiTweet: A New Benchmark for Identifying Manipulation of News on Social Media",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MTVGbju4N4": {
    "title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yp7LkttZ2Q": {
    "title": "Reversing the NLP Pipeline: What Do LMs Have to Offer Linguistics?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Ey54SZGVn": {
    "title": "Is ChatGPT a Smart Data Generation Tool? Exploring ChatGPT for Generating Metaphorical Data",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zVI3qVofDk": {
    "title": "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w4PInrVSYU": {
    "title": "Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p0N0RYl7i5": {
    "title": "Incubating Text Classifiers Following User Instruction with Nothing but LLM",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0qns6fSiZK": {
    "title": "Making Task-Oriented Dialogue Datasets More Natural by Synthetically Generating Indirect User Requests",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XibKBU8y6a": {
    "title": "In-Context Learning with Iterative Demonstration Selection",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FF1ksRdveR": {
    "title": "Towards Robustness of Text-to-Visualization Translation against Lexical and Phrasal Variability",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NVATnmPk8v": {
    "title": "How to Insert an Additional Layer Between the Middle Layer of the Pre-trained Model",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=niCsgzEGAY": {
    "title": "Improving LLM-based Unified Event Relation Extraction via Multiple Answer Questions",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PBb2NolNfS": {
    "title": "Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lBc3MXcGqZ": {
    "title": "NL2FOL: Translating Natural Language to First-Order Logic for Logical Fallacy Detection",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YenLE2PvO7": {
    "title": "Exploring the Jungle of Bias: Political Bias Attribution in Language Models via Dependency Analysis",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t0dpw7nrkK": {
    "title": "Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dElj4nJt9X": {
    "title": "Improving Small and Large Language Models Alignment on Chain-of-Thought Reasoning using Curriculum Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I0oWWdH7cS": {
    "title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F3tAkdqhIV": {
    "title": "Transition-based Opinion Generation for Aspect-based Sentiment Analysis",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UvDNTDBAqE": {
    "title": "Large Language Models are Better Logical Fallacy Reasoners with Counterargument, Goal, and Explanation-aware Prompt Formulation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yb1o4Tj68X": {
    "title": "Tuning-Free Accountable Intervention for LLM Deployment - A Metacognitive Approach",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bg2P0xImjY": {
    "title": "Unlocking Multi-View Insights in Knowledge-Dense Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=guSIWloCNz": {
    "title": "Multi-Event Temporal Ordering by Event Order Ranking",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=85FdcTyMxU": {
    "title": "FrCoLA: a French Corpus of Linguistic Acceptability Judgments",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i4Gm9K7mm1": {
    "title": "Uncertainty in Language Models: Assessment through Rank-Calibration",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xm6vXYVH3i": {
    "title": "Beyond the Turn-Based Game: Duplex Models Enable Real-Time Conversations",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9mSUtXbxVw": {
    "title": "Mitigating Prototype Shift: Few-Shot Nested Named Entity Recognition with Prototype-Attention",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MOoZjnA8S8": {
    "title": "Salience-aware Dialogue Summarization via Parallel Original-Extracted Streams",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rEQU0BjJOz": {
    "title": "Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bVWOCdBAKM": {
    "title": "ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T4o5BVsIdI": {
    "title": "Personality Profiling: How informative are social media profiles in predicting personal information?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1OcsjG37jy": {
    "title": "Can't Remember Details in Long Documents? You Need Some R&R",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ha1S5Plcee": {
    "title": "Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ie9T6S9g4S": {
    "title": "70B-parameter large language models in Japanese medical question-answering",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LZpK1N1naV": {
    "title": "MMoE: Robust Spoiler Detection with Multi-modal Information and Domain-aware Mixture-of-Experts",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xbk1WGTP8y": {
    "title": "ChatLog: Carefully Evaluating the Evolution of ChatGPT Across Time",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  }
}