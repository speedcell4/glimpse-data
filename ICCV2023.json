{
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_Towards_Attack-tolerant_Federated_Learning_via_Critical_Parameter_Analysis_ICCV_2023_paper.html": {
    "title": "Towards Attack-tolerant Federated Learning via Critical Parameter Analysis",
    "volume": "main",
    "abstract": "Federated learning is used to train a shared model in a decentralized way without clients sharing private data with each other. Federated learning systems are susceptible to poisoning attacks when malicious clients send false updates to the central server. Existing defense strategies are ineffective under non-IID data settings. This paper proposes a new defense strategy, FedCPA (Federated learning with Critical Parameter Analysis). Our attack-tolerant aggregation method is based on the observation that benign local models have similar sets of top-k and bottom-k critical parameters, whereas poisoned local models do not. Experiments with different attack scenarios on multiple datasets demonstrate that our model outperforms existing defense strategies in defending against poisoning attacks",
    "checked": true,
    "id": "e72d24cb1f45aa4dfc435190f4403a9a5379a919",
    "semantic_title": "towards attack-tolerant federated learning via critical parameter analysis",
    "citation_count": 0,
    "authors": [
      "Sungwon Han",
      "Sungwon Park",
      "Fangzhao Wu",
      "Sundong Kim",
      "Bin Zhu",
      "Xing Xie",
      "Meeyoung Cha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zbinden_Stochastic_Segmentation_with_Conditional_Categorical_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Stochastic Segmentation with Conditional Categorical Diffusion Models",
    "volume": "main",
    "abstract": "Semantic segmentation has made significant progress in recent years thanks to deep neural networks, but the common objective of generating a single segmentation output that accurately matches the image's content may not be suitable for safety-critical domains such as medical diagnostics and autonomous driving. Instead, multiple possible correct segmentation maps may be required to reflect the true distribution of annotation maps. In this context, stochastic semantic segmentation methods must learn to predict conditional distributions of labels given the image, but this is challenging due to the typically multimodal distributions, high-dimensional output spaces, and limited annotation data. To address these challenges, we propose a conditional categorical diffusion model (CCDM) for semantic segmentation based on Denoising Diffusion Probabilistic Models. Our model is conditioned to the input image, enabling it to generate multiple segmentation label maps that account for the aleatoric uncertainty arising from divergent ground truth annotations. Our experimental results show that CCDM achieves state-of-the-art performance on LIDC, a stochastic semantic segmentation dataset, and outperforms established baselines on the classical segmentation dataset Cityscapes",
    "checked": true,
    "id": "5fc1da3886407209151466f9b0b656e5883d9704",
    "semantic_title": "stochastic segmentation with conditional categorical diffusion models",
    "citation_count": 4,
    "authors": [
      "Lukas Zbinden",
      "Lars Doorenbos",
      "Theodoros Pissas",
      "Adrian Thomas Huber",
      "Raphael Sznitman",
      "Pablo MÃ¡rquez-Neila"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yi_Diff-Retinex_Rethinking_Low-light_Image_Enhancement_with_A_Generative_Diffusion_Model_ICCV_2023_paper.html": {
    "title": "Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model",
    "volume": "main",
    "abstract": "In this paper, we rethink the low-light image enhancement task and propose a physically explainable and generative diffusion model for low-light image enhancement, termed as Diff-Retinex. We aim to integrate the advantages of the physical model and the generative network. Furthermore, we hope to supplement and even deduce the information missing in the low-light image through the generative network. Therefore, Diff-Retinex formulates the low-light image enhancement problem into Retinex decomposition and conditional image generation. In the Retinex decomposition, we integrate the superiority of attention in Transformer and meticulously design a Retinex Transformer decomposition network (TDN) to decompose the image into illumination and reflectance maps. Then, we design multi-path generative diffusion networks to reconstruct the normal-light Retinex probability distribution and solve the various degradations in these components respectively, including dark illumination, noise, color deviation, loss of scene contents, etc. Owing to generative diffusion model, Diff-Retinex puts the restoration of low-light subtle detail into practice. Extensive experiments conducted on real-world low-light datasets qualitatively and quantitatively demonstrate the effectiveness, superiority, and generalization of the proposed method",
    "checked": true,
    "id": "4c5c5001ea6338a53b9e8553f12ed1d35b936233",
    "semantic_title": "diff-retinex: rethinking low-light image enhancement with a generative diffusion model",
    "citation_count": 0,
    "authors": [
      "Xunpeng Yi",
      "Han Xu",
      "Hao Zhang",
      "Linfeng Tang",
      "Jiayi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Birds-Eye-View_Scene_Graph_for_Vision-Language_Navigation_ICCV_2023_paper.html": {
    "title": "Bird's-Eye-View Scene Graph for Vision-Language Navigation",
    "volume": "main",
    "abstract": "Vision-language navigation (VLN), which entails an agent to navigate 3D environments following human instructions, has shown great advances. However, current agents are built upon panoramic observations, which hinders their ability to perceive 3D scene geometry and easily leads to ambiguous selection of panoramic view. To address these limitations, we present a BEV Scene Graph (BSG), which leverages multi-step BEV representations to encode scene layouts and geometric cues of indoor environment under the supervision of 3D detection. During navigation, BSG builds a local BEV representation at each step and maintains a BEV-based global scene map, which stores and organizes all the online collected local BEV representations according to their topological relations. Based on BSG, the agent predicts a local BEV grid-level decision score and a global graph-level decision score, combined with a subview selection score on panoramic views, for more accurate action prediction. Our approach significantly outperforms state-of-the-art methods on REVERIE, R2R, and R4R, showing the potential of BEV perception in VLN",
    "checked": true,
    "id": "97d5d2c6a64e49fbf2f0ae59bb0ecc070e30e465",
    "semantic_title": "bird's-eye-view scene graph for vision-language navigation",
    "citation_count": 2,
    "authors": [
      "Rui Liu",
      "Xiaohan Wang",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_PVT_A_Simple_End-to-End_Latency-Aware_Visual_Tracking_Framework_ICCV_2023_paper.html": {
    "title": "PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework",
    "volume": "main",
    "abstract": "Visual object tracking is essential to intelligent robots. Most existing approaches have ignored the online latency that can cause severe performance degradation during real-world processing. Especially for unmanned aerial vehicles (UAVs), where robust tracking is more challenging and onboard computation is limited, the latency issue can be fatal. In this work, we present a simple framework for end-to-end latency-aware tracking, i.e., end-to-end predictive visual tracking (PVT++). Unlike existing solutions that naively append Kalman Filters after trackers, PVT++ can be jointly optimized, so that it takes not only motion information but can also leverage the rich visual knowledge in most pre-trained tracker models for robust prediction. Besides, to bridge the training-evaluation domain gap, we propose a relative motion factor, empowering PVT++ to generalize to the challenging and complex UAV tracking scenes. These careful designs have made the small-capacity lightweight PVT++ a widely effective solution. Additionally, this work presents an extended latency-aware evaluation benchmark for assessing an any-speed tracker in the online setting. Empirical results on a robotic platform from the aerial perspective show that PVT++ can achieve significant performance gain on various trackers and exhibit higher accuracy than prior solutions, largely mitigating the degradation brought by latency. Our code will be made public",
    "checked": true,
    "id": "b8ba368d615d471ae5f329a6747649efbfefc40f",
    "semantic_title": "pvt++: a simple end-to-end latency-aware visual tracking framework",
    "citation_count": 1,
    "authors": [
      "Bowen Li",
      "Ziyuan Huang",
      "Junjie Ye",
      "Yiming Li",
      "Sebastian Scherer",
      "Hang Zhao",
      "Changhong Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_A_Dynamic_Dual-Processing_Object_Detection_Framework_Inspired_by_the_Brains_ICCV_2023_paper.html": {
    "title": "A Dynamic Dual-Processing Object Detection Framework Inspired by the Brain's Recognition Mechanism",
    "volume": "main",
    "abstract": "There are two main approaches to object detection: CNN-based and Transformer-based. The former views object detection as a dense local matching problem, while the latter sees it as a sparse global retrieval problem. Research in neuroscience has shown that the recognition decision in the brain is based on two processes, namely familiarity and recollection. Based on this biological support, we propose an efficient and effective dual-processing object detection framework. It integrates CNN- and Transformer-based detectors into a comprehensive object detection system consisting of a shared backbone, an efficient dual-stream encoder, and a dynamic dual-decoder. To better integrate local and global features, we design a search space for the CNN-Transformer dual-stream encoder to find the optimal fusion solution. To enable better coordination between the CNN- and Transformer-based decoders, we provide the dual-decoder with a selective mask. This mask dynamically chooses the more advantageous decoder for each position in the image based on high-level representation. As demonstrated by extensive experiments, our approach shows flexibility and effectiveness in prompting the mAP of the various source detectors by 3.0 3.7 without increasing FLOPs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minying Zhang",
      "Tianpeng Bu",
      "Lulu Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lu_Hard_No-Box_Adversarial_Attack_on_Skeleton-Based_Human_Action_Recognition_with_ICCV_2023_paper.html": {
    "title": "Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient",
    "volume": "main",
    "abstract": "Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming each dimension in the data is independent. The SMI gradient can augment many gradient-based attack methods, leading to a new family of no-box attack methods. Extensive evaluation and comparison show that our method imposes a real threat to existing classifiers. They also show that the SMI gradient improves the transferability and imperceptibility of adversarial samples in both no-box and transfer-based black-box settings",
    "checked": true,
    "id": "f9ec4390cba65c32c44dd9524e4d45ea79e49113",
    "semantic_title": "hard no-box adversarial attack on skeleton-based human action recognition with skeleton-motion-informed gradient",
    "citation_count": 0,
    "authors": [
      "Zhengzhi Lu",
      "He Wang",
      "Ziyi Chang",
      "Guoan Yang",
      "Hubert P. H. Shum"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_GameFormer_Game-theoretic_Modeling_and_Learning_of_Transformer-based_Interactive_Prediction_and_ICCV_2023_paper.html": {
    "title": "GameFormer: Game-theoretic Modeling and Learning of Transformer-based Interactive Prediction and Planning for Autonomous Driving",
    "volume": "main",
    "abstract": "Autonomous vehicles operating in complex real-world environments require accurate predictions of interactive behaviors between traffic participants. This paper tackles the interaction prediction problem by formulating it with hierarchical game theory and proposing the GameFormer model for its implementation. The model incorporates a Transformer encoder, which effectively models the relationships between scene elements, alongside a novel hierarchical Transformer decoder structure. At each decoding level, the decoder utilizes the prediction outcomes from the previous level, in addition to the shared environmental context, to iteratively refine the interaction process. Moreover, we propose a learning process that regulates an agent's behavior at the current level to respond to other agents' behaviors from the preceding level. Through comprehensive experiments on large-scale real-world driving datasets, we demonstrate the state-of-the-art accuracy of our model on the Waymo interaction prediction task. Additionally, we validate the model's capacity to jointly reason about the motion plan of the ego agent and the behaviors of multiple agents in both open-loop and closed-loop planning tests, outperforming various baseline methods. Furthermore, we evaluate the efficacy of our model on the nuPlan planning benchmark, where it achieves leading performance",
    "checked": true,
    "id": "4c667a69a3d788e4ddbaf900dd36b78d845fd287",
    "semantic_title": "gameformer: game-theoretic modeling and learning of transformer-based interactive prediction and planning for autonomous driving",
    "citation_count": 9,
    "authors": [
      "Zhiyu Huang",
      "Haochen Liu",
      "Chen Lv"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Towards_Better_Robustness_against_Common_Corruptions_for_Unsupervised_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "Towards Better Robustness against Common Corruptions for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "Recent studies have investigated how to achieve robustness for unsupervised domain adaptation (UDA). While most efforts focus on adversarial robustness, i.e. how the model performs against unseen malicious adversarial perturbations, robustness against benign common corruption (RaCC) surprisingly remains under-explored for UDA. Towards improving RaCC for UDA methods in an unsupervised manner, we propose a novel Distributionally and Discretely Adversarial Regularization (DDAR) framework in this paper. Formulated as a min-max optimization with a distribution distance, DDAR is theoretically well-founded to ensure generalization over unknown common corruptions. Meanwhile, we show that our regularization scheme effectively reduces a surrogate of RaCC, i.e., the perceptual distance between natural data and common corruption. To enable a better adversarial regularization, the design of the optimization pipeline relies on an image discretization scheme that can transform \"out-of-distribution\" adversarial data into \"in-distribution\" data augmentation. Through extensive experiments, in terms of RaCC, our method is superior to conventional unsupervised regularization mechanisms, widely improves the robustness of existing UDA methods, and achieves state-of-the-art performance",
    "checked": false,
    "id": "d10b47f6355854317026db32db67817352460e71",
    "semantic_title": "towards corruption-agnostic robust domain adaptation",
    "citation_count": 1,
    "authors": [
      "Zhiqiang Gao",
      "Kaizhu Huang",
      "Rui Zhang",
      "Dawei Liu",
      "Jieming Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Learning_in_Imperfect_Environment_Multi-Label_Classification_with_Long-Tailed_Distribution_and_ICCV_2023_paper.html": {
    "title": "Learning in Imperfect Environment: Multi-Label Classification with Long-Tailed Distribution and Partial Labels",
    "volume": "main",
    "abstract": "Conventional multi-label classification (MLC) methods assume that all samples are fully labeled and identically distributed. Unfortunately, this assumption is unrealistic in large-scale MLC data that has long-tailed (LT) distribution and partial labels (PL). To address the problem, we introduce a novel task, Partial labeling and Long-Tailed Multi-Label Classification (PLT-MLC), to jointly consider the above two imperfect learning environments. Not surprisingly, we find that most LT-MLC and PL-MLC approaches fail to solve the PLT-MLC, resulting in significant performance degradation on the two proposed PLT-MLC benchmarks. Therefore, we propose an end-to-end learning framework: COrrection -> ModificatIon -> balanCe, abbreviated as COMC. Our bootstrapping philosophy is to simultaneously correct the missing labels (Correction) with convinced prediction confidence over a class-aware threshold and to learn from these recall labels during training. We next propose a novel multi-focal modifier loss that simultaneously addresses head-tail imbalance and positive-negative imbalance to adaptively modify the attention to different samples (Modification) under the LT class distribution. We also develop a balanced training strategy by distilling the model's learning effect from head and tail samples, and thus design the balanced classifier (Balance) conditioned on the head and tail learning effect to maintain a stable performance. Our experimental study shows that the proposed method significantly outperforms the general MLC, LT-MLC and ML-MLC methods in terms of effectiveness and robustness on our newly created PLT-MLC datasets",
    "checked": true,
    "id": "c0c78ae65213be03271c77d690792fb62a2d26ca",
    "semantic_title": "learning in imperfect environment: multi-label classification with long-tailed distribution and partial labels",
    "citation_count": 1,
    "authors": [
      "Wenqiao Zhang",
      "Changshuo Liu",
      "Lingze Zeng",
      "Bengchin Ooi",
      "Siliang Tang",
      "Yueting Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html": {
    "title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance",
    "volume": "main",
    "abstract": "In real-world scenarios, typical visual recognition systems could fail under two major causes, i.e., the misclassification between known classes and the excusable misbehavior on unknown-class images. To tackle these deficiencies, flexible visual recognition should dynamically predict multiple classes when they are unconfident between choices and reject making predictions when the input is entirely out of the training distribution. Two challenges emerge along with this novel task. First, prediction uncertainty should be separately quantified as confusion depicting inter-class uncertainties and ignorance identifying out-of-distribution samples. Second, both confusion and ignorance should be comparable between samples to enable effective decision-making. In this paper, we propose to model these two sources of uncertainty explicitly with the theory of Subjective Logic. Regarding recognition as an evidence-collecting process, confusion is then defined as conflicting evidence, while ignorance is the absence of evidence. By predicting Dirichlet concentration parameters for singletons, comprehensive subjective opinions, including confusion and ignorance, could be achieved via further evidence combinations. Through a series of experiments on synthetic data analysis, visual recognition, and open-set detection, we demonstrate the effectiveness of our methods in quantifying two sources of uncertainties and dealing with flexible recognition",
    "checked": true,
    "id": "e47ffd4831d2f4912f6215ade992c6c9e7845375",
    "semantic_title": "flexible visual recognition by evidential modeling of confusion and ignorance",
    "citation_count": 0,
    "authors": [
      "Lei Fan",
      "Bo Liu",
      "Haoxiang Li",
      "Ying Wu",
      "Gang Hua"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Texture_Generation_on_3D_Meshes_with_Point-UV_Diffusion_ICCV_2023_paper.html": {
    "title": "Texture Generation on 3D Meshes with Point-UV Diffusion",
    "volume": "main",
    "abstract": "In this work, we focus on synthesizing high-quality textures on 3D meshes. We present Point-UV diffusion, a coarse-to-fine pipeline that marries the denoising diffusion model with UV mapping to generate 3D consistent and high-quality texture images in UV space. We start with introducing a point diffusion model to synthesize low-frequency texture components with our tailored style guidance to tackle the biased color distribution. The derived coarse texture offers global consistency and serves as a condition for the subsequent UV diffusion stage, aiding in regularizing the model to generate a 3D consistent UV texture image. Then, a UV diffusion model with hybrid conditions is developed to enhance the texture fidelity in the 2D UV space. Our method can process meshes of any genus, generating diversified, geometry-compatible, and high-fidelity textures",
    "checked": true,
    "id": "efda2446411fcef4eade3099ea0766d25959a668",
    "semantic_title": "texture generation on 3d meshes with point-uv diffusion",
    "citation_count": 2,
    "authors": [
      "Xin Yu",
      "Peng Dai",
      "Wenbo Li",
      "Lan Ma",
      "Zhengzhe Liu",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Supervised_Homography_Learning_with_Realistic_Dataset_Generation_ICCV_2023_paper.html": {
    "title": "Supervised Homography Learning with Realistic Dataset Generation",
    "volume": "main",
    "abstract": "In this paper, we propose an iterative framework, which consists of two phases: a generation phase and a training phase, to generate realistic training data and yield a supervised homography network. In the generation phase, given an unlabeled image pair, we utilize the pre-estimated dominant plane masks and homography of the pair, along with another sampled homography that serves as ground truth to generate a new labeled training pair with realistic motion. In the training phase, the generated data is used to train the supervised homography network, in which the training data is refined via a content consistency module and a quality assessment module. Once an iteration is finished, the trained network is used in the next data generation phase to update the pre-estimated homography. Through such an iterative strategy, the quality of the dataset and the performance of the network can be gradually and simultaneously improved. Experimental results show that our method achieves state-of-the-art performance and existing supervised methods can be also improved based on the generated dataset. Code and dataset are available at https://github.com/JianghaiSCU/RealSH",
    "checked": true,
    "id": "2cd4971c0285878ce59e7e44e65d7e1b5727f2f6",
    "semantic_title": "supervised homography learning with realistic dataset generation",
    "citation_count": 0,
    "authors": [
      "Hai Jiang",
      "Haipeng Li",
      "Songchen Han",
      "Haoqiang Fan",
      "Bing Zeng",
      "Shuaicheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cao_E2E-LOAD_End-to-End_Long-form_Online_Action_Detection_ICCV_2023_paper.html": {
    "title": "E2E-LOAD: End-to-End Long-form Online Action Detection",
    "volume": "main",
    "abstract": "Recently, feature-based methods for Online Action Detection (OAD) have been gaining traction. However, these methods are constrained by their fixed backbone design, which fails to leverage the potential benefits of a trainable backbone. This paper introduces an end-to-end learning network that revises these approaches, incorporating a backbone network design that improves effectiveness and efficiency. Our proposed model utilizes a shared initial spatial model for all frames and maintains an extended sequence cache, which enables low-cost inference. We promote an asymmetric spatiotemporal model that caters to long-form and short-form modeling. Additionally, we propose an innovative and efficient inference mechanism that accelerates extensive spatiotemporal exploration. Through comprehensive ablation studies and experiments, we validate the performance and efficiency of our proposed method. Remarkably, we achieve an end-to-end learning OAD of 17.3 (+12.6) FPS with 72.4% (+1.2%), 90.3% (+0.7%), and 48.1% (+26.0%) mAP on THMOUS'14, TVSeries, and HDD, respectively",
    "checked": true,
    "id": "a2ef9e6f15c7e3e6e827bea62906474d5881dda6",
    "semantic_title": "e2e-load: end-to-end long-form online action detection",
    "citation_count": 0,
    "authors": [
      "Shuqiang Cao",
      "Weixin Luo",
      "Bairui Wang",
      "Wei Zhang",
      "Lin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_TALL_Thumbnail_Layout_for_Deepfake_Video_Detection_ICCV_2023_paper.html": {
    "title": "TALL: Thumbnail Layout for Deepfake Video Detection",
    "volume": "main",
    "abstract": "The growing threats of deepfakes to society and cybersecurity have raised enormous public concerns, and increasing efforts have been devoted to this critical topic of deepfake video detection. Existing video methods achieve good performance but are computationally intensive. This paper introduces a simple yet effective strategy named Thumbnail Layout (TALL), which transforms a video clip into a pre-defined layout to realize the preservation of spatial and temporal dependencies. Specifically, consecutive frames are masked in a fixed position in each frame to improve generalization, then resized to sub-images and rearranged into a pre-defined layout as the thumbnail. TALL is model-agnostic and extremely simple by only modifying a few lines of code. Inspired by the success of vision transformers, we incorporate TALL into Swin Transformer, forming an efficient and effective method TALL-Swin. Extensive experiments on intra-dataset and cross-dataset validate the validity and superiority of TALL and SOTA TALL-Swin. TALL-Swin achieves 90.79% AUC on the challenging cross-dataset task, FaceForensics++ - Celeb-DF. The code is available at https://github.com/rainy-xu/TALL4Deepfake",
    "checked": true,
    "id": "af532e29f2435c30dad56b904a7c35649183b96c",
    "semantic_title": "tall: thumbnail layout for deepfake video detection",
    "citation_count": 0,
    "authors": [
      "Yuting Xu",
      "Jian Liang",
      "Gengyun Jia",
      "Ziming Yang",
      "Yanhao Zhang",
      "Ran He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Enhanced_Soft_Label_for_Semi-Supervised_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Enhanced Soft Label for Semi-Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "As a mainstream framework in the field of semi-supervised learning (SSL), self-training via pseudo labeling and its variants have witnessed impressive progress in semi-supervised semantic segmentation with the recent advance of deep neural networks. However, modern self-training based SSL algorithms use a pre-defined constant threshold to select unlabeled pixel samples that contribute to the training, thus failing to be compatible with different learning difficulties of variant categories and different learning status of the model. To address these issues, we propose Enhanced Soft Label (ESL), a curriculum learning approach to fully leverage the high-value supervisory signals implicit in the untrustworthy pseudo label. ESL believes that pixels with unconfident predictions can be pretty sure about their belonging to a subset of dominant classes though being arduous to determine the exact one. It thus contains a Dynamic Soft Label (DSL) module to dynamically maintain the high probability classes, keeping the label \"soft\" so as to make full use of the high entropy prediction. However, the DSL itself will inevitably introduce ambiguity between dominant classes, thus blurring the classification boundary. Therefore, we further propose a pixel-to-part contrastive learning method cooperated with an unsupervised object part grouping mechanism to improve its ability to distinguish between different classes. Extensive experimental results on Pascal VOC 2012 and Cityscapes show that our approach achieves remarkable improvements over existing state-of-the-art approaches",
    "checked": false,
    "id": "b30e1ff313012c16bc7de737ce72d9a2eca7ab87",
    "semantic_title": "pruning-guided curriculum learning for semi-supervised semantic segmentation",
    "citation_count": 2,
    "authors": [
      "Jie Ma",
      "Chuan Wang",
      "Yang Liu",
      "Liang Lin",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Saunders_Self-supervised_Monocular_Depth_Estimation_Lets_Talk_About_The_Weather_ICCV_2023_paper.html": {
    "title": "Self-supervised Monocular Depth Estimation: Let's Talk About The Weather",
    "volume": "main",
    "abstract": "Current, self-supervised depth estimation architectures rely on clear and sunny weather scenes to train deep neural networks. However, in many locations, this assumption is too strong. For example in the UK (2021), 149 days consisted of rain. For these architectures to be effective in real-world applications, we must create models that can generalise to all weather conditions, times of the day and image qualities. Using a combination of computer graphics and generative models, one can augment existing sunny-weather data in a variety of ways that simulate adverse weather effects. While it is tempting to use such data augmentations for self-supervised depth, in the past this was shown to degrade performance instead of improving it. In this paper, we put forward a method that uses augmentations to remedy this problem. By exploiting the correspondence between unaugmented and augmented data we introduce a pseudo-supervised loss for both depth and pose estimation. This brings back some of the benefits of supervised learning while still not requiring any labels. We also make a series of practical recommendations which collectively offer a reliable, efficient framework for weather-related augmentation of self-supervised depth from monocular video. We present extensive testing to show that our method, Robust-Depth, achieves SotA performance on the KITTI dataset while significantly surpassing SotA on challenging, adverse condition data such as DrivingStereo, Foggy CityScape and NuScenes-Night. The project website can be found at https://kieran514.github.io/Robust-Depth-Project/",
    "checked": true,
    "id": "ae47aa90176deae6e8460939f98238369afe9a82",
    "semantic_title": "self-supervised monocular depth estimation: let's talk about the weather",
    "citation_count": 2,
    "authors": [
      "Kieran Saunders",
      "George Vogiatzis",
      "Luis J. Manso"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_Bidirectional_Alignment_for_Domain_Adaptive_Detection_with_Transformers_ICCV_2023_paper.html": {
    "title": "Bidirectional Alignment for Domain Adaptive Detection with Transformers",
    "volume": "main",
    "abstract": "We propose a Bidirectional Alignment for domain adaptive Detection with Transformers (BiADT) to improve cross domain object detection performance. Existing adversarial learning based methods use gradient reverse layer (GRL) to reduce the domain gap between the source and target domains in feature representations. Since different image parts and objects may exhibit various degrees of domain-specific characteristics, directly applying GRL on a global image or object representation may not be suitable. Our proposed BiADT explicitly estimates token-wise domain-invariant and domain-specific features in the image and object token sequences. BiADT has a novel deformable attention and self-attention, aimed at bi-directional domain alignment and mutual information minimization. These two objectives reduce the domain gap in domain-invariant representations, and simultaneously increase the distinctiveness of domain-specific features. Our experiments show that BiADT achieves very competitive performance to SOTA consistently on Cityscapes-to-FoggyCityscapes, Sim10K-to-Citiscapes and Cityscapes-to-BDD100K, outperforming the strong baseline, AQT, by 2.0, 2.1, and 2.4 in mAP50, respectively",
    "checked": true,
    "id": "7769cdd91babe131792a730a7e6e55ba76e096c8",
    "semantic_title": "bidirectional alignment for domain adaptive detection with transformers",
    "citation_count": 0,
    "authors": [
      "Liqiang He",
      "Wei Wang",
      "Albert Chen",
      "Min Sun",
      "Cheng-Hao Kuo",
      "Sinisa Todorovic"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Fast_Neural_Scene_Flow_ICCV_2023_paper.html": {
    "title": "Fast Neural Scene Flow",
    "volume": "main",
    "abstract": "Neural Scene Flow Prior (NSFP) is of significant interest to the vision community due to its inherent robustness to out-of-distribution (OOD) effects and its ability to deal with dense lidar points. The approach utilizes a coordinate neural network to estimate scene flow at runtime, without any training. However, it is up to 100 times slower than current state-of-the-art learning methods. In other applications such as image, video, and radiance function reconstruction innovations in speeding up the runtime performance of coordinate networks have centered upon architectural changes. In this paper, we demonstrate that scene flow is different---with the dominant computational bottleneck stemming from the loss function itself (i.e., Chamfer distance). Further, we rediscover the distance transform (DT) as an efficient, correspondence-free loss function that dramatically speeds up the runtime optimization. Our fast neural scene flow (FNSF) approach reports for the first time real-time performance comparable to learning methods, without any training or OOD bias on two of the largest open autonomous driving (AV) lidar datasets Waymo Open [62] and Argoverse [8]",
    "checked": true,
    "id": "f2c38480908ef6c199a94be44036923dc543fff5",
    "semantic_title": "fast neural scene flow",
    "citation_count": 1,
    "authors": [
      "Xueqian Li",
      "Jianqiao Zheng",
      "Francesco Ferroni",
      "Jhony Kaesemodel Pontes",
      "Simon Lucey"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Peng_CAME_Contrastive_Automated_Model_Evaluation_ICCV_2023_paper.html": {
    "title": "CAME: Contrastive Automated Model Evaluation",
    "volume": "main",
    "abstract": "The Automated Model Evaluation (AutoEval) framework entertains the possibility of evaluating a trained machine learning model without resorting to a labeled testing set. Despite the promise and some decent results, the existing AutoEval methods heavily rely on computing distribution shifts between the unlabelled testing set and the training set. We believe this reliance on the training set becomes another obstacle in shipping this technology to real-world ML development. In this work, we propose Contrastive Automatic Model Evaluation (CAME), a novel AutoEval framework that is rid of involving training set in the loop. The core idea of CAME bases on a theoretical analysis which bonds the model performance with a contrastive loss. Further, with extensive empirical validation, we manage to set up a predictable relationship between the two, simply by deducing on the unlabeled/unseen testing set. The resulting framework CAME establishes a new SOTA results for AutoEval by surpassing prior work significantly",
    "checked": true,
    "id": "7ea4b3fa348973ac2204320fa7440dc5d87e052b",
    "semantic_title": "came: contrastive automated model evaluation",
    "citation_count": 0,
    "authors": [
      "Ru Peng",
      "Qiuyang Duan",
      "Haobo Wang",
      "Jiachen Ma",
      "Yanbo Jiang",
      "Yongjun Tu",
      "Xiu Jiang",
      "Junbo Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_ExposureDiffusion_Learning_to_Expose_for_Low-light_Image_Enhancement_ICCV_2023_paper.html": {
    "title": "ExposureDiffusion: Learning to Expose for Low-light Image Enhancement",
    "volume": "main",
    "abstract": "Previous raw image-based low-light image enhancement methods predominantly relied on feed-forward neural networks to learn deterministic mappings from low-light to normally-exposed images. However, they failed to capture critical distribution information, leading to visually undesirable results. This work addresses the issue by seamlessly integrating a diffusion model with a physics-based exposure model. Different from a vanilla diffusion model that has to perform Gaussian denoising, with the injected physics-based exposure model, our restoration process can directly start from a noisy image instead of pure noise. As such, our method obtains significantly improved performance and reduced inference time compared with vanilla diffusion models. To make full use of the advantages of different intermediate steps, we further propose an adaptive residual layer that effectively screens out the side-effect in the iterative refinement when the intermediate results have been already well-exposed. The proposed framework can work with both real-paired datasets, SOTA noise models, and different backbone networks. We evaluate the proposed method on various public benchmarks, achieving promising results with consistent improvements using different exposure models and backbones. Besides, the proposed method achieves better generalization capacity for unseen amplifying ratios and better performance than a larger feedforward neural model when few parameters are adopted. The code is released at https://github.com/wyf0912/ExposureDiffusion",
    "checked": true,
    "id": "00aa073b0eec68863a81fbd114886c28efa42f1b",
    "semantic_title": "exposurediffusion: learning to expose for low-light image enhancement",
    "citation_count": 2,
    "authors": [
      "Yufei Wang",
      "Yi Yu",
      "Wenhan Yang",
      "Lanqing Guo",
      "Lap-Pui Chau",
      "Alex C. Kot",
      "Bihan Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xiang_HM-ViT_Hetero-Modal_Vehicle-to-Vehicle_Cooperative_Perception_with_Vision_Transformer_ICCV_2023_paper.html": {
    "title": "HM-ViT: Hetero-Modal Vehicle-to-Vehicle Cooperative Perception with Vision Transformer",
    "volume": "main",
    "abstract": "Vehicle-to-Vehicle technologies have enabled autonomous vehicles to share information to see through occlusions, greatly enhancing perception performance. Nevertheless, existing works all focused on homogeneous traffic where vehicles are equipped with the same type of sensors, which significantly hampers the scale of collaboration and benefit of cross-modality interactions. In this paper, we investigate the multi-agent hetero-modal cooperative perception problem where agents may have distinct sensor modalities. We present HM-ViT, the first unified multi-agent hetero-modal cooperative perception framework that can collaboratively predict 3D objects for highly dynamic Vehicle-to-Vehicle (V2V) collaborations with varying numbers and types of agents. To effectively fuse features from multi-view images and LiDAR point clouds, we design a novel heterogeneous 3D graph transformer to jointly reason inter-agent and intra-agent interactions. The extensive experiments on the V2V perception dataset OPV2V demonstrate that the HM-ViT outperforms SOTA cooperative perception methods for V2V hetero-modal cooperative perception. Our code will be released at https://github.com/XHwind/HM-ViT",
    "checked": true,
    "id": "a8cc6cee62158bdcaa6b3b1fa9486872fd698b1a",
    "semantic_title": "hm-vit: hetero-modal vehicle-to-vehicle cooperative perception with vision transformer",
    "citation_count": 9,
    "authors": [
      "Hao Xiang",
      "Runsheng Xu",
      "Jiaqi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bounareli_HyperReenact_One-Shot_Reenactment_via_Jointly_Learning_to_Refine_and_Retarget_ICCV_2023_paper.html": {
    "title": "HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces",
    "volume": "main",
    "abstract": "In this paper, we present our method for neural face reenactment, called HyperReenact, that aims to generate realistic talking head images of a source identity, driven by a target facial pose. Existing state-of-the-art face reenactment methods train controllable generative models that learn to synthesize realistic facial images, yet producing reenacted faces that are prone to significant visual artifacts, especially under the challenging condition of extreme head pose changes, or requiring expensive few-shot fine-tuning to better preserve the source identity characteristics. We propose to address these limitations by leveraging the photorealistic generation ability and the disentangled properties of a pretrained StyleGAN2 generator, by first inverting the real images into its latent space and then using a hypernetwork to perform: (i) refinement of the source identity characteristics and (ii) facial pose re-targeting, eliminating this way the dependence on external editing methods that typically produce artifacts. Our method operates under the one-shot setting (i.e., using a single source frame) and allows for cross-subject reenactment, without requiring any subject-specific fine-tuning. We compare our method both quantitatively and qualitatively against several state-of-the-art techniques on the standard benchmarks of VoxCeleb1 and VoxCeleb2, demonstrating the superiority of our approach in producing artifact-free images, exhibiting remarkable robustness even under extreme head pose changes. We make the code and the pretrained models publicly available at: https://github.com/StelaBou/HyperReenact",
    "checked": true,
    "id": "4fa85f06ad62139aa10b16112ddffae257ae4ce0",
    "semantic_title": "hyperreenact: one-shot reenactment via jointly learning to refine and retarget faces",
    "citation_count": 0,
    "authors": [
      "Stella Bounareli",
      "Christos Tzelepis",
      "Vasileios Argyriou",
      "Ioannis Patras",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jing_Order-preserving_Consistency_Regularization_for_Domain_Adaptation_and_Generalization_ICCV_2023_paper.html": {
    "title": "Order-preserving Consistency Regularization for Domain Adaptation and Generalization",
    "volume": "main",
    "abstract": "Deep learning models fail on cross-domain challenges if the model is oversensitive to domain-specific attributes, e.g., lightning, background, camera angle, etc. To alleviate this problem, data augmentation coupled with consistency regularization are commonly adopted to make the model less sensitive to domain-specific attributes. Consistency regularization enforces the model to output the same representation or prediction for two views of one image. These constraints, however, are either too strict or not order-preserving for the classification probabilities. In this work, we propose the Order-preserving Consistency Regularization (OCR) for cross-domain tasks. The order-preserving property for the prediction makes the model robust to task-irrelevant transformations. As a result, the model becomes less sensitive to the domain-specific attributes. The comprehensive experiments show that our method achieves clear advantages on five different cross-domain tasks",
    "checked": true,
    "id": "8304945a8af225f57e2c5d93524e7c42043b7f97",
    "semantic_title": "order-preserving consistency regularization for domain adaptation and generalization",
    "citation_count": 0,
    "authors": [
      "Mengmeng Jing",
      "Xiantong Zhen",
      "Jingjing Li",
      "Cees G. M. Snoek"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kurita_RefEgo_Referring_Expression_Comprehension_Dataset_from_First-Person_Perception_of_Ego4D_ICCV_2023_paper.html": {
    "title": "RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D",
    "volume": "main",
    "abstract": "Grounding textual expressions on scene objects from first-person views is a truly demanding capability in developing agents that are aware of their surroundings and behave following intuitive text instructions. Such capability is of necessity for glass-devices or autonomous robots to localize referred objects in the real-world. In the conventional referring expression comprehension tasks of images, however, datasets are mostly constructed based on the web-crawled data and don't reflect diverse real-world structures on the task of grounding textual expressions in diverse objects in the real world. Recently, a massive-scale egocentric video dataset of Ego4D was proposed. Ego4D covers around the world diverse real-world scenes including numerous indoor and outdoor situations such as shopping, cooking, walking, talking, manufacturing, etc. Based on egocentric videos of Ego4D, we constructed a broad coverage of the video-based referring expression comprehension dataset: RefEgo. Our dataset includes more than 12k video clips and 41 hours for video-based referring expression comprehension annotation. In experiments, we combine the state-of-the-art 2D referring expression comprehension models with the object tracking algorithm, achieving the video-wise referred object tracking even in difficult conditions: the referred object becomes out-of-frame in the middle of the video or multiple similar objects are presented in the video",
    "checked": true,
    "id": "9ab07ac971f9673490d0eb68980623543bcb2a9c",
    "semantic_title": "refego: referring expression comprehension dataset from first-person perception of ego4d",
    "citation_count": 0,
    "authors": [
      "Shuhei Kurita",
      "Naoki Katsura",
      "Eri Onami"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Exploring_Temporal_Frequency_Spectrum_in_Deep_Video_Deblurring_ICCV_2023_paper.html": {
    "title": "Exploring Temporal Frequency Spectrum in Deep Video Deblurring",
    "volume": "main",
    "abstract": "Video deblurring aims to restore the latent video frames from their blurred counterparts. Despite the remarkable progress, most promising video deblurring methods only investigate the temporal priors in the spatial domain and rarely explore their its potential in the frequency domain. In this paper, we revisit the blurred sequence in the Fourier space and figure out some intrinsic frequency-temporal priors that imply the temporal blur degradation can be accessibly decoupled in the potential frequency domain. Based on these priors, we propose a novel Fourier-based frequency-temporal video deblurring solution, where the core design accommodates the temporal spectrum to a popular video deblurring pipeline of feature extraction, alignment, aggregation, and optimization. Specifically, we design a Spectrum Prior-guided Alignment module by leveraging enlarged blur information in the potential spectrum to mitigate the blur effects on the alignment. Then, Temporal Energy prior-driven Aggregation is implemented to replenish the original local features by estimating the temporal spectrum energy as the global sharpness guidance. In addition, the customized frequency loss is devised to optimize the proposed method for decent spectral distribution. Extensive experiments demonstrate that our model performs favorably against other state-of-the-art methods, thus confirming the effectiveness of frequency-temporal prior modeling",
    "checked": false,
    "id": "a132844518e757c8c34d356c8f1f05a724d0aeab",
    "semantic_title": "wideband beamforming www.modernh.com for to for",
    "citation_count": 29,
    "authors": [
      "Qi Zhu",
      "Man Zhou",
      "Naishan Zheng",
      "Chongyi Li",
      "Jie Huang",
      "Feng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Unified_Visual_Relationship_Detection_with_Vision_and_Language_Models_ICCV_2023_paper.html": {
    "title": "Unified Visual Relationship Detection with Vision and Language Models",
    "volume": "main",
    "abstract": "This work focuses on training a single visual relationship detector predicting over the union of label spaces from multiple datasets. Merging labels spanning different datasets could be challenging due to inconsistent taxonomies. The issue is exacerbated in visual relationship detection when second-order visual semantics are introduced between pairs of objects. To address this challenge, we propose UniVRD, a novel bottom-up method for Unified Visual Relationship Detection by leveraging vision and language models (VLMs). VLMs provide well-aligned image and text embeddings, where similar relationships are optimized to be close to each other for semantic unification. Our bottom-up design enables the model to enjoy the benefit of training with both object detection and visual relationship datasets. Empirical results on both human-object interaction detection and scene-graph generation demonstrate the competitive performance of our model. UniVRD achieves 38.07 mAP on HICO-DET, outperforming the current best bottom-up HOI detector by 14.26 mAP. More importantly, we show that our unified detector performs as well as dataset-specific models in mAP, and achieves further improvements when we scale up the model. Our code will be made publicly available on GitHub",
    "checked": true,
    "id": "2474160b8517c854d1258a6d142d0af007133e13",
    "semantic_title": "unified visual relationship detection with vision and language models",
    "citation_count": 1,
    "authors": [
      "Long Zhao",
      "Liangzhe Yuan",
      "Boqing Gong",
      "Yin Cui",
      "Florian Schroff",
      "Ming-Hsuan Yang",
      "Hartwig Adam",
      "Ting Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fan_Occ2Net_Robust_Image_Matching_Based_on_3D_Occupancy_Estimation_for_ICCV_2023_paper.html": {
    "title": "Occ^2Net: Robust Image Matching Based on 3D Occupancy Estimation for Occluded Regions",
    "volume": "main",
    "abstract": "Image matching is a fundamental and critical task in various visual applications, such as Simultaneous Localization and Mapping (SLAM) and image retrieval, which require accurate pose estimation. However, most existing methods ignore the occlusion relations between objects caused by camera motion and scene structure. In this paper, we propose Occ^2Net, a novel image matching method that models occlusion relations using 3D occupancy and infers matching points in occluded regions. Thanks to the inductive bias encoded in the Occupancy Estimation (OE) module, it greatly simplifies bootstrapping of a multi-view consistent 3D representation that can then integrate information from multiple views. Together with an Occlusion-Aware (OA) module, it incorporates attention layers and rotation alignment to enable matching between occluded and visible points. We evaluate our method on both real-world and simulated datasets and demonstrate its superior performance over state-of-the-art methods on several metrics, especially in occlusion scenarios",
    "checked": false,
    "id": "b04dbb76162f857c90877b980ccb063105178eb2",
    "semantic_title": "occ2net: robust image matching based on 3d occupancy estimation for occluded regions",
    "citation_count": 0,
    "authors": [
      "Miao Fan",
      "Mingrui Chen",
      "Chen Hu",
      "Shuchang Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Azadi_Make-An-Animation_Large-Scale_Text-conditional_3D_Human_Motion_Generation_ICCV_2023_paper.html": {
    "title": "Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation",
    "volume": "main",
    "abstract": "Text-guided human motion generation has drawn significant interest because of its impactful applications spanning animation and robotics. Recently, application of diffusion models for motion generation has enabled improvements in the quality of generated motions. However, existing approaches are limited by their reliance on relatively small-scale motion capture data, leading to poor performance on more diverse, in-the-wild prompts. In this paper, we introduce Make-An-Animation, a text-conditioned human motion generation model which learns more diverse poses and prompts from large-scale image-text datasets, enabling significant improvement in performance over prior works. Make-An-Animation is trained in two stages. First, we train on a curated large-scale dataset of (text, static pseudo-pose) pairs extracted from image-text datasets. Second, we fine-tune on motion capture data, adding additional layers to model the temporal dimension. Unlike prior diffusion models for motion generation, Make-An-Animation uses a U-Net architecture similar to recent text-to-video generation models. Human evaluation of motion realism and alignment with input text shows that our model reaches state-of-the-art performance on text-to-motion generation",
    "checked": true,
    "id": "0254732f85b73b86695a3a4f5a83391295e73a4d",
    "semantic_title": "make-an-animation: large-scale text-conditional 3d human motion generation",
    "citation_count": 3,
    "authors": [
      "Samaneh Azadi",
      "Akbar Shah",
      "Thomas Hayes",
      "Devi Parikh",
      "Sonal Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Struppek_Rickrolling_the_Artist_Injecting_Backdoors_into_Text_Encoders_for_Text-to-Image_ICCV_2023_paper.html": {
    "title": "Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "While text-to-image synthesis currently enjoys great popularity among researchers and the general public, the security of these models has been neglected so far. Many text-guided image generation models rely on pre-trained text encoders from external sources, and their users trust that the retrieved models will behave as promised. Unfortunately, this might not be the case. We introduce backdoor attacks against text-guided generative models and demonstrate that their text encoders pose a major tampering risk. Our attacks only slightly alter an encoder so that no suspicious model behavior is apparent for image generations with clean prompts. By then inserting a single character trigger into the prompt, e.g., a non-Latin character or emoji, the adversary can trigger the model to either generate images with pre-defined attributes or images following a hidden, potentially malicious description. We empirically demonstrate the high effectiveness of our attacks on Stable Diffusion and highlight that the injection process of a single backdoor takes less than two minutes. Besides phrasing our approach solely as an attack, it can also force an encoder to forget phrases related to certain concepts, such as nudity or violence, and help to make image generation safer",
    "checked": true,
    "id": "929844531ce69c56edbe19f5b03dc87ff83851aa",
    "semantic_title": "rickrolling the artist: injecting backdoors into text encoders for text-to-image synthesis",
    "citation_count": 3,
    "authors": [
      "Lukas Struppek",
      "Dominik Hintersdorf",
      "Kristian Kersting"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/PNVR_LD-ZNet_A_Latent_Diffusion_Approach_for_Text-Based_Image_Segmentation_ICCV_2023_paper.html": {
    "title": "LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation",
    "volume": "main",
    "abstract": "Large-scale pre-training tasks like image classification, captioning, or self-supervised techniques do not incentivize learning the semantic boundaries of objects. However, recent generative foundation models built using text-based latent diffusion techniques may learn semantic boundaries. This is because they have to synthesize intricate details about all objects in an image based on a text description. Therefore, we present a technique for segmenting real and AI-generated images using latent diffusion models (LDMs) trained on internet-scale datasets. First, we show that the latent space of LDMs (z-space) is a better input representation compared to other feature representations like RGB images or CLIP encodings for text-based image segmentation. By training the segmentation models on the latent z-space, which creates a compressed representation across several domains like different forms of art, cartoons, illustrations, and photographs, we are also able to bridge the domain gap between real and AI-generated images. We show that the internal features of LDMs contain rich semantic information and present a technique in the form of LD-ZNet to further boost the performance of text-based segmentation. Overall, we show up to 6% improvement over standard baselines for text-to-image segmentation on natural images. For AI-generated imagery, we show close to 20% improvement compared to state-of-the-art techniques. The project is available at https://koutilya-pnvr.github.io/LD-ZNet/",
    "checked": true,
    "id": "a960f30c7a265678d7765dc0d81d6a5131b475fa",
    "semantic_title": "ld-znet: a latent diffusion approach for text-based image segmentation",
    "citation_count": 1,
    "authors": [
      "Koutilya PNVR",
      "Bharat Singh",
      "Pallabi Ghosh",
      "Behjat Siddiquie",
      "David Jacobs"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Workie-Talkie_Accelerating_Federated_Learning_by_Overlapping_Computing_and_Communications_via_ICCV_2023_paper.html": {
    "title": "Workie-Talkie: Accelerating Federated Learning by Overlapping Computing and Communications via Contrastive Regularization",
    "volume": "main",
    "abstract": "Federated learning (FL) over mobile devices is a promising distributed learning paradigm for various mobile applications. However, practical deployment of FL over mobile devices is very challenging because (i) conventional FL incurs huge training latency for mobile devices due to interleaved local computing and communications of model updates, (ii) there are heterogeneous training data across mobile devices, and (iii) mobile devices have hardware heterogeneity in terms of computing and communication capabilities. To address aforementioned challenges, in this paper, we propose a novel \"workie-talkie\" FL scheme, which can accelerate FL's training by overlapping local computing and wireless communications via contrastive regularization (FedCR). FedCR can reduce FL's training latency and almost eliminate straggler issues since it buries/embeds the time consumption of communications into that of local training. To resolve the issue of model staleness and data heterogeneity co-existing, we introduce class-wise contrastive regularization to correct the local training in FedCR. Besides, we jointly exploit contrastive regularization and subnetworks to further extend our FedCR approach to accommodate edge devices with hardware heterogeneity. We deploy FedCR in our FL testbed and conduct extensive experiments. The results show that FedCR outperforms its status quo FL approaches on various datasets and models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Chen",
      "Qiyu Wan",
      "Pavana Prakash",
      "Lan Zhang",
      "Xu Yuan",
      "Yanmin Gong",
      "Xin Fu",
      "Miao Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Downstream-agnostic_Adversarial_Examples_ICCV_2023_paper.html": {
    "title": "Downstream-agnostic Adversarial Examples",
    "volume": "main",
    "abstract": "Self-supervised learning usually uses a large amount of unlabeled data to pre-train an encoder which can be used as a general-purpose feature extractor, such that downstream users only need to perform fine-tuning operations to enjoy the benefit of \"big model\". Despite this promising prospect, the security of pre-trained encoder has not been thoroughly investigated yet, especially when the pre-trained encoder is publicly available for commercial use. In this paper, we propose AdvEncoder, the first framework for generating downstream-agnostic universal adversarial examples based on the pre-trained encoder. AdvEncoder aims to construct a universal adversarial perturbation or patch for a set of natural images that can fool all the downstream tasks inheriting the victim pre-trained encoder. Unlike traditional adversarial example works, the pre-trained encoder only outputs feature vectors rather than classification labels. Therefore, we first exploit the high frequency component information of the image to guide the generation of adversarial examples. Then we design a generative attack framework to construct adversarial perturbations/patches by learning the distribution of the attack surrogate dataset to improve their attack success rates and transferability. Our results show that an attacker can successfully attack downstream tasks without knowing either the pre-training dataset or the downstream dataset. We also tailor four defenses for pre-trained encoders, the results of which further prove the attack ability of AdvEncoder",
    "checked": true,
    "id": "107a04026fee14c7c3cca3f5e305b90d22e4f145",
    "semantic_title": "downstream-agnostic adversarial examples",
    "citation_count": 5,
    "authors": [
      "Ziqi Zhou",
      "Shengshan Hu",
      "Ruizhi Zhao",
      "Qian Wang",
      "Leo Yu Zhang",
      "Junhui Hou",
      "Hai Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_Late_Stopping_Avoiding_Confidently_Learning_from_Mislabeled_Examples_ICCV_2023_paper.html": {
    "title": "Late Stopping: Avoiding Confidently Learning from Mislabeled Examples",
    "volume": "main",
    "abstract": "Sample selection is a prevalent method in learning with noisy labels, where small-loss data are typically considered as correctly labeled data. However, this method may not effectively identify clean hard examples with large losses, which are critical for achieving the model's closeto-optimal generalization performance. In this paper, we propose a new framework, Late Stopping, which leverages the intrinsic robust learning ability of DNNs through a prolonged training process. Specifically, Late Stopping gradually shrinks the noisy dataset by removing high-probability mislabeled examples while retaining the majority of clean hard examples in the training set throughout the learning process. We empirically observe that mislabeled and clean examples exhibit differences in the number of epochs required for them to be consistently and correctly classified, and thus high-probability mislabeled examples can be removed. Experimental results on benchmark-simulated and real-world noisy datasets demonstrate that the proposed method outperforms state-of-the-art counterparts",
    "checked": true,
    "id": "74c60e83538049469f24e0142da6dfb76a135cd7",
    "semantic_title": "late stopping: avoiding confidently learning from mislabeled examples",
    "citation_count": 1,
    "authors": [
      "Suqin Yuan",
      "Lei Feng",
      "Tongliang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_AerialVLN_Vision-and-Language_Navigation_for_UAVs_ICCV_2023_paper.html": {
    "title": "AerialVLN: Vision-and-Language Navigation for UAVs",
    "volume": "main",
    "abstract": "Recently emerged Vision-and-Language Navigation(VLN) tasks have drawn significant attention in both computer vision and natural language processing communities. Existing VLN tasks are built for agents that navigate on the ground, either indoors or outdoors. However, many tasks require intelligent agents to carry out in the sky, such as UAV-based goods delivery, traffic/security patrol, and scenery tour, to name a few. Navigating in the sky is more complicated than on the ground because agents need to consider the flying height and more complex spatial relationship reasoning. To fill this gap and facilitate research in this field, we propose a new task named AerialVLN, which is UAV-based and towards outdoor environments. We develop a 3D simulator rendered by near-realistic pictures of 25 city-level scenarios. Our simulator supports continuous navigation, environment extension and configuration. We also proposed an extended baseline model based on the widely-used cross modal-alignment (CMA) navigation methods. We find that there is still a significant gap between the baseline model and human performance, which suggests AerialVLN is a new challenging task",
    "checked": true,
    "id": "25d8e3c541f996e366d8fa48cba248ca330e8a78",
    "semantic_title": "aerialvln: vision-and-language navigation for uavs",
    "citation_count": 1,
    "authors": [
      "Shubo Liu",
      "Hongsheng Zhang",
      "Yuankai Qi",
      "Peng Wang",
      "Yanning Zhang",
      "Qi Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_On_the_Robustness_of_Open-World_Test-Time_Training_Self-Training_with_Dynamic_ICCV_2023_paper.html": {
    "title": "On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion",
    "volume": "main",
    "abstract": "Generalizing deep learning models to unknown target domain distribution with low latency has motivated research into test-time training/adaptation (TTT/TTA). Existing approaches often focus on improving test-time training performance under well-curated target domain data. As figured out in this work, many state-of-the-art methods fail to maintain the performance when the target domain is contaminated with strong out-of-distribution (OOD) data, a.k.a. open-world test-time training (OWTTT). The failure is mainly due to the inability to distinguish strong OOD samples from regular weak OOD samples. To improve the robustness of OWTTT we first develop an adaptive strong OOD pruning which improves the efficacy of the self-training TTT method. We further propose a way to dynamically expand the prototypes to represent strong OOD samples for an improved weak/strong OOD data separation. Finally, we regularize self-training with distribution alignment and the combination yields the state-of-the-art performance on 5 OWTTT benchmarks. The code is available at https://github.com/Yushu-Li/OWTTT",
    "checked": true,
    "id": "9229d8c956a11932fb16242dc2d2c162767596bf",
    "semantic_title": "on the robustness of open-world test-time training: self-training with dynamic prototype expansion",
    "citation_count": 0,
    "authors": [
      "Yushu Li",
      "Xun Xu",
      "Yongyi Su",
      "Kui Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Rao_Studying_How_to_Efficiently_and_Effectively_Guide_Models_with_Explanations_ICCV_2023_paper.html": {
    "title": "Studying How to Efficiently and Effectively Guide Models with Explanations",
    "volume": "main",
    "abstract": "Despite being highly performant, deep neural networks might base their decisions on features that spuriously correlate with the provided labels, thus hurting generalization. To mitigate this, 'model guidance' has recently gained popularity, i.e. the idea of regularizing the models' explanations to ensure that they are \"right for the right reasons\". While various techniques to achieve such model guidance have been proposed, experimental validation of these approaches has thus far been limited to relatively simple and / or synthetic datasets. To better understand the effectiveness of the various design choices that have been explored in the context of model guidance, in this work we conduct an in-depth evaluation across various loss functions, attribution methods, models, and 'guidance depths' on the PASCAL VOC 2007 and MS COCO 2014 datasets. As annotation costs for model guidance can limit its applicability, we also place a particular focus on efficiency. Specifically, we guide the models via bounding box annotations, which are much cheaper to obtain than the commonly used segmentation masks, and evaluate the robustness of model guidance under limited (e.g. with only 1% of annotated images) or overly coarse annotations. Further, we propose using the EPG score as an additional evaluation metric and loss function ('Energy loss'). We show that optimizing for the Energy loss leads to models that exhibit a distinct focus on object-specific features, despite only using bounding box annotations that also include background regions. Lastly, we show that such model guidance can improve generalization under distribution shifts. Code available at: https://github.com/sukrutrao/Model-Guidance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sukrut Rao",
      "Moritz BÃ¶hle",
      "Amin Parchami-Araghi",
      "Bernt Schiele"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xie_Most_Important_Person-Guided_Dual-Branch_Cross-Patch_Attention_for_Group_Affect_Recognition_ICCV_2023_paper.html": {
    "title": "Most Important Person-Guided Dual-Branch Cross-Patch Attention for Group Affect Recognition",
    "volume": "main",
    "abstract": "Group affect refers to the subjective emotion that is evoked by an external stimulus in a group, which is an important factor that shapes group behavior and outcomes. Recognizing group affect involves identifying important individuals and salient objects among a crowd that can evoke emotions. However, most existing methods lack attention to affective meaning in group dynamics and fail to account for the contextual relevance of faces and objects in group-level images. In this work, we propose a solution by incorporating the psychological concept of the Most Important Person (MIP), which represents the most noteworthy face in a crowd and has affective semantic meaning. We present the Dual-branch Cross-Patch Attention Transformer (DCAT) which uses global image and MIP together as inputs. Specifically, we first learn the informative facial regions produced by the MIP and the global context separately. Then, the Cross-Patch Attention module is proposed to fuse the features of MIP and global context together to complement each other. Our proposed method outperforms state-of-the-art methods on GAF 3.0, GroupEmoW, and HECO datasets. Moreover, we demonstrate the potential for broader applications by showing that our proposed model can be transferred to another group affect task, group cohesion, and achieve comparable results",
    "checked": true,
    "id": "34abe5a7786c16994b0499f40454b8dfdf4a8ff3",
    "semantic_title": "most important person-guided dual-branch cross-patch attention for group affect recognition",
    "citation_count": 1,
    "authors": [
      "Hongxia Xie",
      "Ming-Xian Lee",
      "Tzu-Jui Chen",
      "Hung-Jen Chen",
      "Hou-I Liu",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yan_SkeletonMAE_Graph-based_Masked_Autoencoder_for_Skeleton_Sequence_Pre-training_ICCV_2023_paper.html": {
    "title": "SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training",
    "volume": "main",
    "abstract": "Skeleton sequence representation learning has shown great advantages for action recognition due to its promising ability to model human joints and topology. However, the current methods usually require sufficient labeled data for training computationally expensive models. Moreover, these methods ignore how to utilize the fine-grained dependencies among different skeleton joints to pre-train an efficient skeleton sequence learning model that can generalize well across different datasets. In this paper, we propose an efficient skeleton sequence learning framework, named Skeleton Sequence Learning (SSL). To comprehensively capture the human pose and obtain discriminative skeleton sequence representation, we build an asymmetric graph-based encoder-decoder pre-training architecture named SkeletonMAE, which embeds skeleton joint sequence into graph convolutional network and reconstructs the masked skeleton joints and edges based on the prior human topology knowledge. Then, the pre-trained SkeletonMAE encoder is integrated with the Spatial-Temporal Representation Learning (STRL) module to build the SSL framework. Extensive experimental results show that our SSL generalizes well across different datasets and outperforms the state-of-the-art self-supervised skeleton-based methods on FineGym, Diving48, NTU 60 and NTU 120 datasets. Moreover, we obtain comparable performance to some fully supervised methods. The code is avaliable at https://github.com/HongYan1123/SkeletonMAE",
    "checked": true,
    "id": "c8d4c8f2c72a19d91f26aed9f9a343bc751fe70a",
    "semantic_title": "skeletonmae: graph-based masked autoencoder for skeleton sequence pre-training",
    "citation_count": 0,
    "authors": [
      "Hong Yan",
      "Yang Liu",
      "Yushen Wei",
      "Zhen Li",
      "Guanbin Li",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yun_Achievement-Based_Training_Progress_Balancing_for_Multi-Task_Learning_ICCV_2023_paper.html": {
    "title": "Achievement-Based Training Progress Balancing for Multi-Task Learning",
    "volume": "main",
    "abstract": "Multi-task learning faces two challenging issues: (1) the high cost of annotating labels for all tasks and (2) balancing the training progress of various tasks with different natures. To resolve the label annotation issue, we construct a large-scale \"partially annotated\" multi-task dataset by combining task-specific datasets. However, the numbers of annotations for individual tasks are imbalanced, which may escalate an imbalance in training progress. To balance the training progress, we propose an achievement-based multi-task loss to modulate training speed based on the \"achievement,\" defined as the ratio of current accuracy to single-task accuracy. Then, we formulate the multitask loss as a weighted geometric mean of individual task losses instead of a weighted sum to prevent any task from dominating the loss. In experiments, we evaluated the accuracy and training speed of the proposed multi-task loss on the large-scale multi-task dataset against recent multitask losses. The proposed loss achieved the best multi-task accuracy without incurring training time overhead. Compared to single-task models, the proposed one achieved 1.28%, 1.65%, and 1.18% accuracy improvement in object detection, semantic segmentation, and depth estimation, respectively, while reducing computations to 33.73%. Source code is available at https://github.com/ samsung/Achievement-based-MTL",
    "checked": false,
    "id": "badf8ef25d26b95845821d9b61be3fb26d5eff18",
    "semantic_title": "joint analysis of acoustic scenes and sound events based on multitask learning with dynamic weight adaptation",
    "citation_count": 1,
    "authors": [
      "Hayoung Yun",
      "Hanjoo Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Pose-Free_Neural_Radiance_Fields_via_Implicit_Pose_Regularization_ICCV_2023_paper.html": {
    "title": "Pose-Free Neural Radiance Fields via Implicit Pose Regularization",
    "volume": "main",
    "abstract": "Pose-free neural radiance fields (NeRF) aim to train NeRF with unposed multi-view images and it has achieved very impressive success in recent years. Most existing works share the pipeline of training a coarse pose estimator with rendered images at first, followed by a joint optimization of estimated poses and neural radiance field. However, as the pose estimator is trained with only rendered images, the pose estimation is usually biased or inaccurate for real images due to the domain gap between real images and rendered images, leading to poor robustness for the pose estimation of real images and further local min- ima in joint optimization. We design IR-NeRF, an innovative pose-free NeRF that introduces implicit pose regularization to refine pose estimator with unposed real images and improve the robustness of the pose estimation for real images. With a collection of 2D images of a specific scene, IR-NeRF constructs a scene codebook that stores scene features and captures the scene-specific pose distribution implicitly as priors. Thus, the robustness of pose estimation can be promoted with the scene priors according to the rationale that a 2D real image can be well reconstructed from the scene codebook only when its estimated pose lies within the pose distribution. Extensive experiments show that IR-NeRF achieves superior novel view synthesis and outperforms the state-of-the-art consistently across multiple synthetic and real datasets",
    "checked": true,
    "id": "2f94bbbd50f0ccc55eb714fb51406bc2575934e6",
    "semantic_title": "pose-free neural radiance fields via implicit pose regularization",
    "citation_count": 0,
    "authors": [
      "Jiahui Zhang",
      "Fangneng Zhan",
      "Yingchen Yu",
      "Kunhao Liu",
      "Rongliang Wu",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shang_Self-supervised_Learning_to_Bring_Dual_Reversed_Rolling_Shutter_Images_Alive_ICCV_2023_paper.html": {
    "title": "Self-supervised Learning to Bring Dual Reversed Rolling Shutter Images Alive",
    "volume": "main",
    "abstract": "Modern consumer cameras usually employ the rolling shutter (RS) mechanism, where images are captured by scanning scenes row-by-row, yielding RS distortions for dynamic scenes. To correct RS distortions, existing methods adopt a fully supervised learning manner, where high framerate global shutter (GS) images should be collected as ground-truth supervision. In this paper, we propose a Self-supervised learning framework for Dual reversed RS distortions Correction (SelfDRSC), where a DRSC network can be learned to generate a high framerate GS video only based on dual RS images with reversed distortions. In particular, a bidirectional distortion warping module is proposed for reconstructing dual reversed RS images, and then a self-supervised loss can be deployed to train DRSC network by enhancing the cycle consistency between input and reconstructed dual reversed RS images. Besides start and end RS scanning time, GS images at arbitrary intermediate scanning time can also be supervised in SelfDRSC, thus enabling the learned DRSC network to generate a high framerate GS video. Moreover, a simple yet effective self-distillation strategy is introduced in self-supervised loss for mitigating boundary artifacts in generated GS images. On synthetic dataset, SelfDRSC achieves better or comparable quantitative metrics in comparison to state-of-the-art methods trained in the full supervision manner. On real-world RS cases, our SelfDRSC can produce high framerate GS videos with finer correction textures and better temporary consistency. The source code and trained models are made publicly available at https://github.com/ shangwei5/SelfDRSC",
    "checked": true,
    "id": "696f1e6641e4ee5d256a176f5e99a39ef3aefddd",
    "semantic_title": "self-supervised learning to bring dual reversed rolling shutter images alive",
    "citation_count": 0,
    "authors": [
      "Wei Shang",
      "Dongwei Ren",
      "Chaoyu Feng",
      "Xiaotao Wang",
      "Lei Lei",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liang_Logic-induced_Diagnostic_Reasoning_for_Semi-supervised_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Logic-induced Diagnostic Reasoning for Semi-supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Recent advances in semi-supervised semantic segmentation have been heavily reliant on pseudo labeling to compensate for limited labeled data, disregarding the valuable relational knowledge among semantic concepts. To bridge this gap, we devise LogicDiag, a brand new neural-logic semi-supervised learning framework. Our key insight is that conflicts within pseudo labels, identified through symbolic knowledge, can serve as strong yet commonly ignored learning signals. LogicDiag resolves such conflicts via reasoning with logic-induced diagnoses, enabling the recovery of (potentially) erroneous pseudo labels, ultimately alleviating the notorious error accumulation problem. We showcase the practical application of LogicDiag in the data-hungry segmentation scenario, where we formalize the structured abstraction of semantic concepts as a set of logic rules. Extensive experiments on three standard semi-supervised semantic segmentation benchmarks demonstrate the effectiveness and generality of LogicDiag. Moreover, LogicDiag highlights the promising opportunities arising from the systematic integration of symbolic reasoning into the prevalent statistical, neural learning approaches",
    "checked": true,
    "id": "bafda899d07a5ab8d3ab253ab180699f316fc2ef",
    "semantic_title": "logic-induced diagnostic reasoning for semi-supervised semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Chen Liang",
      "Wenguan Wang",
      "Jiaxu Miao",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_Self-Supervised_Monocular_Depth_Estimation_by_Direction-aware_Cumulative_Convolution_Network_ICCV_2023_paper.html": {
    "title": "Self-Supervised Monocular Depth Estimation by Direction-aware Cumulative Convolution Network",
    "volume": "main",
    "abstract": "Monocular depth estimation is known as an ill-posed task that objects in a 2D image usually do not contain sufficient information to predict their depth. Thus, it acts differently from other tasks (e.g., classification and segmentation) in many ways. In this paper, we find that self-supervised monocular depth estimation shows a direction sensitivity and environmental dependency in the feature representation. But the current CNN backbones borrowed from other tasks cannot handle different types of environmental information efficiently, limiting the overall depth accuracy. To bridge this gap, we propose a new Direction-aware Cumulative Convolution Network (DaCCN), which improves the depth feature representation in two aspects. First, we propose a direction-aware module, which can learn to adjust the feature extraction in each direction, facilitating the encoding of different types of information. Secondly, we design a new cumulative convolution to improve the efficiency for aggregating important environmental information. Experiments show that our method achieves significant improvements on three widely used benchmarks and sets a new state-of-the-art performance on the popular benchmarks with all three types of self-supervision",
    "checked": true,
    "id": "29cb229cc84c0c8a85aaf211b145b8d7f1135b0e",
    "semantic_title": "self-supervised monocular depth estimation by direction-aware cumulative convolution network",
    "citation_count": 0,
    "authors": [
      "Wencheng Han",
      "Junbo Yin",
      "Jianbing Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mensink_Encyclopedic_VQA_Visual_Questions_About_Detailed_Properties_of_Fine-Grained_Categories_ICCV_2023_paper.html": {
    "title": "Encyclopedic VQA: Visual Questions About Detailed Properties of Fine-Grained Categories",
    "volume": "main",
    "abstract": "We propose Encyclopedic-VQA, a large scale visual question answering (VQA) dataset featuring visual questions about detailed properties of fine-grained categories and instances. It contains 221k unique question+answer pairs each matched with (up to) 5 images, resulting in a total of 1M VQA samples. Moreover, our dataset comes with a controlled knowledge base derived from Wikipedia, marking the evidence to support each answer. Empirically, we show that our dataset poses a hard challenge for large vision+language models as they perform poorly on our dataset: PaLI [9] is state-of-the-art on OK-VQA [29], yet it only achieves 13.0% accuracy on our dataset. Moreover, we experimentally show that progress on answering our encyclopedic questions can be achieved by augmenting large models with a mechanism that retrieves relevant information for the knowledge base. An oracle experiment with perfect retrieval achieves 87.0% accuracy on the single-hop portion of our dataset, and an automatic retrieval- augmented prototype yields 48.8%. We believe that our dataset enables future research on retrieval-augmented vision+language models",
    "checked": true,
    "id": "3d0eb04ca61c47161cd8eee3fb89f12da164f6e6",
    "semantic_title": "encyclopedic vqa: visual questions about detailed properties of fine-grained categories",
    "citation_count": 2,
    "authors": [
      "Thomas Mensink",
      "Jasper Uijlings",
      "Lluis Castrejon",
      "Arushi Goel",
      "Felipe Cadar",
      "Howard Zhou",
      "Fei Sha",
      "AndrÃ© Araujo",
      "Vittorio Ferrari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yao_Towards_Understanding_the_Generalization_of_Deepfake_Detectors_from_a_Game-Theoretical_ICCV_2023_paper.html": {
    "title": "Towards Understanding the Generalization of Deepfake Detectors from a Game-Theoretical View",
    "volume": "main",
    "abstract": "This paper aims to explain the generalization of deepfake detectors from the novel perspective of multi-order interactions among visual concepts. Specifically, we propose three hypotheses: 1. Deepfake detectors encode multi-order interactions among visual concepts, in which the low-order interactions usually have substantially negative contributions to deepfake detection. 2. Deepfake detectors with better generalization abilities tend to encode low-order interactions with fewer negative contributions. 3. Generalized deepfake detectors usually weaken the negative contributions of low-order interactions by suppressing their strength. Accordingly, we design several mathematical metrics to evaluate the effect of low-order interaction for deepfake detectors. Extensive comparative experiments are conducted, which verify the soundness of our hypotheses. Based on the analyses, we further propose a generic method, which directly reduces the toxic effects of low-order interactions to improve the generalization of deepfake detectors to some extent. The code will be released when the paper is accepted",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kelu Yao",
      "Jin Wang",
      "Boyu Diao",
      "Chao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Few-Shot_Common_Action_Localization_via_Cross-Attentional_Fusion_of_Context_and_ICCV_2023_paper.html": {
    "title": "Few-Shot Common Action Localization via Cross-Attentional Fusion of Context and Temporal Dynamics",
    "volume": "main",
    "abstract": "The goal of this paper is to localize action instances in a long untrimmed query video using just meager trimmed support videos representing a common action whose class information is not given. In this task, it is crucial to mine reliable temporal cues representing a common action from handful support videos. In our work, we develop an attention mechanism using cross-correlation. Based on this cross-attention, we first transform the support videos into query video's context to emphasize query-relevant important frames, and suppress less relevant ones. Next, we summarize sub-sequences of support video frames to represent temporal dynamics in coarse temporal granularity, which is then propagated to the fine-grained support video features through the cross-attention. In each case, the cross-attentions are applied to each support video in the individual-to-all strategy to balance heterogeneity and compatibility of the support videos. In contrast, the candidate instances in the query video are lastly attended by the resulting support video features, at once. In addition, we also develop a relational classifier head based on the query and support video representations. We show the effectiveness of our work with the state-of-the-art (SOTA) performance in benchmark datasets (ActivityNet1.3 and THUMOS14), and analyze each component extensively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juntae Lee",
      "Mihir Jain",
      "Sungrack Yun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ershov_Physically-Plausible_Illumination_Distribution_Estimation_ICCV_2023_paper.html": {
    "title": "Physically-Plausible Illumination Distribution Estimation",
    "volume": "main",
    "abstract": "A camera's auto-white-balance (AWB) module operates under the assumption that there is a single dominant illumination in a captured scene. AWB methods estimate an image's dominant illumination and use it as the target \"white point\" for correction. However, in natural scenes, there are often many light sources present. We performed a user study that revealed that non-dominant illuminations often produce visually pleasing white-balanced images and, in some cases, are even preferred over the dominant illumination. Motivated by this observation, we revisit AWB to predict a distribution of plausible illuminations for use in white balance. As part of this effort, we extend the Cube++ illumination estimation dataset to provide ground truth illumination distributions per image. Using this new ground truth data, we describe how to train a lightweight neural network method to predict the scene's illumination distribution. We describe how our idea can be used with existing image formats by embedding the estimated distribution in the RAW image to enable users to generate visually plausible white-balance images",
    "checked": false,
    "id": "4ef935dcb9ee081fb8fab24bac535dad2fff4754",
    "semantic_title": "hdr map reconstruction from a single ldr sky panoramic image for outdoor illumination estimation",
    "citation_count": 0,
    "authors": [
      "Egor Ershov",
      "Vasily Tesalin",
      "Ivan Ermakov",
      "Michael S. Brown"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shu_3DPPE_3D_Point_Positional_Encoding_for_Transformer-based_Multi-Camera_3D_Object_ICCV_2023_paper.html": {
    "title": "3DPPE: 3D Point Positional Encoding for Transformer-based Multi-Camera 3D Object Detection",
    "volume": "main",
    "abstract": "Transformer-based methods have swept the benchmarks on 2D and 3D detection on images. Because tokenization before the attention mechanism drops the spatial information, positional encoding becomes critical for those methods. Recent works found that encodings based on samples of the 3D viewing rays can significantly improve the quality of multi-camera 3D object detection. We hypothesize that 3D point locations can provide more information than rays. Therefore, we introduce 3D point positional encoding, 3DPPE, to the 3D detection Transformer decoder. Although 3D measurements are not available at the inference time of monocular 3D object detection, 3DPPE uses predicted depth to approximate the real point positions. Our hybrid-depth module combines direct and categorical depth to estimate the refined depth of each pixel. Despite the approximation, 3DPPE achieves 46.0 mAP and 51.4 NDS on the competitive nuScenes dataset, significantly outperforming encodings based on ray samples. The codes are available at https://github.com/drilistbox/3DPPE",
    "checked": false,
    "id": "bdc32ab125b9ab0c374fe506426daa280eaa142c",
    "semantic_title": "3dppe: 3d point positional encoding for multi-camera 3d object detection transformers",
    "citation_count": 2,
    "authors": [
      "Changyong Shu",
      "Jiajun Deng",
      "Fisher Yu",
      "Yifan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Revisiting_Foreground_and_Background_Separation_in_Weakly-supervised_Temporal_Action_Localization_ICCV_2023_paper.html": {
    "title": "Revisiting Foreground and Background Separation in Weakly-supervised Temporal Action Localization: A Clustering-based Approach",
    "volume": "main",
    "abstract": "Weakly-supervised temporal action localization aims to localize action instances in videos with only video-level action labels. Existing methods mainly embrace a localization-by-classification pipeline that optimizes the snippet-level prediction with a video classification loss. However, this formulation suffers from the discrepancy between classification and detection, resulting in inaccurate separation of foreground and background (F&B) snippets. To alleviate this problem, we propose to explore the underlying structure among the snippets by resorting to unsupervised snippet clustering, rather than heavily relying on the video classification loss. Specifically, we propose a novel clustering-based F&B separation algorithm. It comprises two core components: a snippet clustering component that groups the snippets into multiple latent clusters and a cluster classification component that further classifies the cluster as foreground or background. As there are no ground-truth labels to train these two components, we introduce a unified self-labeling mechanism based on optimal transport to produce high-quality pseudo-labels that match several plausible prior distributions. This ensures that the cluster assignments of the snippets can be accurately associated with their F&B labels, thereby boosting the F&B separation. We evaluate our method on three benchmarks: THUMOS14, ActivityNet v1.2 and v1.3. Our method achieves promising performance on all three benchmarks while being significantly more lightweight than previous methods. Code is available at https://github.com/Qinying-Liu/CASE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinying Liu",
      "Zilei Wang",
      "Shenghai Rong",
      "Junjie Li",
      "Yixin Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ding_VertexSerum_Poisoning_Graph_Neural_Networks_for_Link_Inference_ICCV_2023_paper.html": {
    "title": "VertexSerum: Poisoning Graph Neural Networks for Link Inference",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have brought superb performance to various applications utilizing graph structural data, such as social analysis and fraud detection. The graph links, e.g., social relationships and transaction history, are sensitive and valuable information, which raises privacy concerns when using GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novel graph poisoning attack that increases the effectiveness of graph link stealing by amplifying the link connectivity leakage. To infer node adjacency more accurately, we propose an attention mechanism that can be embedded into the link detection network. Our experiments demonstrate that VertexSerum significantly outperforms the SOTA link inference attack, improving the AUC scores by an average of 9.8% across four real-world datasets and three different GNN structures. Furthermore, our experiments reveal the effectiveness of VertexSerum in both black-box and online learning settings, further validating its applicability in real-world scenarios",
    "checked": true,
    "id": "b35b2060b483cedbb18e4f3606ceedb601c6dec1",
    "semantic_title": "vertexserum: poisoning graph neural networks for link inference",
    "citation_count": 0,
    "authors": [
      "Ruyi Ding",
      "Shijin Duan",
      "Xiaolin Xu",
      "Yunsi Fei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_NeRF-Det_Learning_Geometry-Aware_Volumetric_Representation_for_Multi-View_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection",
    "volume": "main",
    "abstract": "We present NeRF-Det, a novel method for indoor 3D detection with posed RGB images as input. Unlike existing indoor 3D detection methods that struggle to model scene geometry, our method makes novel use of NeRF in an end-to-end manner to explicitly estimate 3D geometry, thereby improving 3D detection performance. Specifically, to avoid the significant extra latency associated with per-scene optimization of NeRF, we introduce sufficient geometry priors to enhance the generalizability of NeRF-MLP. Furthermore, we subtly connect the detection and NeRF branches through a shared MLP, enabling an efficient adaptation of NeRF to detection and yielding geometry-aware volumetric representations for 3D detection. Our method outperforms state-of-the-arts by 3.9 mAP and 3.1 mAP on the ScanNet and ARKITScenes benchmarks, respectively. We provide extensive analysis to shed light on how NeRF-Det works. As a result of our joint-training design, NeRF-Det is able to generalize well to unseen scenes for object detection, view synthesis, and depth estimation tasks without requiring per-scene optimization. Code is available at https://github.com/facebookresearch/NeRF-Det",
    "checked": true,
    "id": "4daaecd6230523ff346cacbea102dfde922bab0d",
    "semantic_title": "nerf-det: learning geometry-aware volumetric representation for multi-view 3d object detection",
    "citation_count": 2,
    "authors": [
      "Chenfeng Xu",
      "Bichen Wu",
      "Ji Hou",
      "Sam Tsai",
      "Ruilong Li",
      "Jialiang Wang",
      "Wei Zhan",
      "Zijian He",
      "Peter Vajda",
      "Kurt Keutzer",
      "Masayoshi Tomizuka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Spatio-Temporal_Domain_Awareness_for_Multi-Agent_Collaborative_Perception_ICCV_2023_paper.html": {
    "title": "Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception",
    "volume": "main",
    "abstract": "Multi-agent collaborative perception as a potential application for vehicle-to-everything communication could significantly improve the perception performance of autonomous vehicles over single-agent perception. However, several challenges remain in achieving pragmatic information sharing in this emerging research. In this paper, we propose SCOPE, a novel collaborative perception framework that aggregates the spatio-temporal awareness characteristics across on-road agents in an end-to-end manner. Specifically, SCOPE has three distinct strengths: i) it considers effective semantic cues of the temporal context to enhance current representations of the target agent; ii) it aggregates perceptually critical spatial information from heterogeneous agents and overcomes localization errors via multi-scale feature interactions; iii) it integrates multi-source representations of the target agent based on their complementary contributions by an adaptive fusion paradigm. To thoroughly evaluate SCOPE, we consider both real-world and simulated scenarios of collaborative 3D object detection tasks on three datasets. Extensive experiments show the superiority of our approach and the necessity of the proposed components. The project link is https://ydk122024.github.io/SCOPE/",
    "checked": true,
    "id": "ccf7d65aabe1cd511c6fc524d813c134affd9c28",
    "semantic_title": "spatio-temporal domain awareness for multi-agent collaborative perception",
    "citation_count": 4,
    "authors": [
      "Kun Yang",
      "Dingkang Yang",
      "Jingyu Zhang",
      "Mingcheng Li",
      "Yang Liu",
      "Jing Liu",
      "Hanqi Wang",
      "Peng Sun",
      "Liang Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_LPFF_A_Portrait_Dataset_for_Face_Generators_Across_Large_Poses_ICCV_2023_paper.html": {
    "title": "LPFF: A Portrait Dataset for Face Generators Across Large Poses",
    "volume": "main",
    "abstract": "Existing face generators exhibit exceptional performance on faces in small to medium poses (with respect to frontal faces) but struggle to produce realistic results for large poses. The distorted rendering results on large poses in 3D-aware generators further show that the generated 3D face shapes are far from the distribution of 3D faces in reality. We find that the above issues are caused by the training dataset's pose imbalance. To this end, we present LPFF, a large-pose Flickr face dataset comprised of 19,590 high-quality real large-pose portrait images. We utilize our dataset to train a 2D face generator that can process large-pose face images, as well as a 3D-aware generator that can generate realistic human face geometry. To better validate our pose-conditional 3D-aware generators, we develop a new FID measure to evaluate the 3D-level performance. Through this novel FID measure and other experiments, we show that LPFF can help 2D face generators extend their latent space and better manipulate the large-pose data, and help 3D-aware face generators achieve better view consistency and more realistic 3D reconstruction results",
    "checked": true,
    "id": "833f972496f802e83d94de58e89bffc69190251f",
    "semantic_title": "lpff: a portrait dataset for face generators across large poses",
    "citation_count": 3,
    "authors": [
      "Yiqian Wu",
      "Jing Zhang",
      "Hongbo Fu",
      "Xiaogang Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Pseudo-label_Alignment_for_Semi-supervised_Instance_Segmentation_ICCV_2023_paper.html": {
    "title": "Pseudo-label Alignment for Semi-supervised Instance Segmentation",
    "volume": "main",
    "abstract": "Pseudo-labeling is significant for semi-supervised instance segmentation, which generates instance masks and classes from unannotated images for subsequent training. However, in existing pipelines, pseudo-labels that contain valuable information may be directly filtered out due to mismatches in class and mask quality. To address this issue, we propose a novel framework, called pseudo-label aligning instance segmentation (PAIS), in this paper. In PAIS, we devise a dynamic aligning loss (DALoss) that adjusts the weights of semi-supervised loss terms with varying class and mask score pairs. Through extensive experiments conducted on the COCO and Cityscapes datasets, we demonstrate that PAIS is a promising framework for semi-supervised instance segmentation, particularly in cases where labeled data is severely limited. Notably, with just 1% labeled data, PAIS achieves 21.2 mAP (based on Mask-RCNN) and 19.9 mAP (based on K-Net) on the COCO dataset, outperforming the current state-of-the-art model, i.e., NoisyBoundary with 7.7 mAP, by a margin of over 12 points. Code is available at: https://github.com/hujiecpp/PAIS",
    "checked": true,
    "id": "3f55c729fede176e90fe17164cbbeb049cdf0480",
    "semantic_title": "pseudo-label alignment for semi-supervised instance segmentation",
    "citation_count": 0,
    "authors": [
      "Jie Hu",
      "Chen Chen",
      "Liujuan Cao",
      "Shengchuan Zhang",
      "Annan Shu",
      "Guannan Jiang",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Siyao_Deep_Geometrized_Cartoon_Line_Inbetweening_ICCV_2023_paper.html": {
    "title": "Deep Geometrized Cartoon Line Inbetweening",
    "volume": "main",
    "abstract": "We aim to address a significant but understudied problem in the anime industry, namely the inbetweening of cartoon line drawings. Inbetweening involves generating intermediate frames between two black-and-white line drawings and is a time-consuming and expensive process that can benefit from automation. However, existing frame interpolation methods that rely on matching and warping whole raster images are unsuitable for line inbetweening and often produce blurring artifacts that damage the intricate line structures. To preserve the precision and detail of the line drawings, we propose a new approach, called AnimeInbet, which geometrizes raster line drawings into graphs of endpoints and reframes the inbetweening task as a graph fusion problem with vertex repositioning. Our method can effectively capture the sparsity and unique structure of line drawings while preserving the details during inbetweening. This is made possible through our novel modules, i.e., vertex encoding, a vertex correspondence Transformer, an effective mechanism for vertex repositioning and a visibility predictor. To train our method, we introduce MixamoLine240, a new dataset of line drawings with ground truth vectorization and matching labels. Our experiments demonstrate that AnimeInbet synthesizes high-quality, clean, and complete intermediate line drawings, outperforming existing methods quantitatively and qualitatively, especially in cases with large motions",
    "checked": true,
    "id": "d5befc6bb18b371738536d3ce2f56b345bdd816a",
    "semantic_title": "deep geometrized cartoon line inbetweening",
    "citation_count": 0,
    "authors": [
      "Li Siyao",
      "Tianpei Gu",
      "Weiye Xiao",
      "Henghui Ding",
      "Ziwei Liu",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Asanomi_MixBag_Bag-Level_Data_Augmentation_for_Learning_from_Label_Proportions_ICCV_2023_paper.html": {
    "title": "MixBag: Bag-Level Data Augmentation for Learning from Label Proportions",
    "volume": "main",
    "abstract": "Learning from label proportions (LLP) is a promising weakly supervised learning problem. In LLP, a set of instances (bag) has label proportions but no instance-level labels. LLP aims to train an instance-level classifier by using the label proportions of the bag. In this paper, we propose a bag-level data augmentation method for LLP called MixBag, which is based on the key observation from our preliminary experiments; that the instance-level classification accuracy improves as the number of labeled bags increases even though the total number of instances is fixed. We also propose a confidence interval loss designed based on statistical theory in order to use the augmented bags effectively. To the best of our knowledge, this is the first attempt to propose bag-level data augmentation for LLP. The advantage of MixBag is that it can be applied to instance-level data augmentation techniques and any LLP method that uses the proportion loss. Experimental results demonstrate this advantage and the effectiveness of our method",
    "checked": true,
    "id": "f892febe0462fa548bee587865c10b9334c3c6ab",
    "semantic_title": "mixbag: bag-level data augmentation for learning from label proportions",
    "citation_count": 0,
    "authors": [
      "Takanori Asanomi",
      "Shinnosuke Matsuo",
      "Daiki Suehiro",
      "Ryoma Bise"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pan_Effective_Real_Image_Editing_with_Accelerated_Iterative_Diffusion_Inversion_ICCV_2023_paper.html": {
    "title": "Effective Real Image Editing with Accelerated Iterative Diffusion Inversion",
    "volume": "main",
    "abstract": "Despite all recent progress, it is still challenging to edit and manipulate natural images with modern generative models. When using Generative Adversarial Network (GAN), one major hurdle is in the inversion process mapping a real image to its corresponding noise vector in the latent space, since its necessary to be able to reconstruct an image to edit its contents. Likewise for Denoising Diffusion Implicit Models (DDIM), the linearization assumption in each inversion step makes the whole deterministic inversion process unreliable. Existing approaches that have tackled the problem of inversion stability often incur in significant trade-offs in computational efficiency. In this work we propose an Accelerated Iterative Diffusion Inversion method, dubbed AIDI, that significantly improves reconstruction accuracy with minimal additional overhead in space and time complexity. By using a novel blended guidance technique, we show that effective results can be obtained on a large range of image editing tasks without large classifier-free guidance in inversion. Furthermore, when compared with other diffusion inversion based works, our proposed process is shown to be more robust for fast image editing in the 10 and 20 diffusion steps' regimes",
    "checked": true,
    "id": "da3a188c227d817b90203ab5294685d8424ad1e2",
    "semantic_title": "effective real image editing with accelerated iterative diffusion inversion",
    "citation_count": 0,
    "authors": [
      "Zhihong Pan",
      "Riccardo Gherardi",
      "Xiufeng Xie",
      "Stephen Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_3D-Aware_Neural_Body_Fitting_for_Occlusion_Robust_3D_Human_Pose_ICCV_2023_paper.html": {
    "title": "3D-Aware Neural Body Fitting for Occlusion Robust 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "Regression-based methods for 3D human pose estimation directly predict the 3D pose parameters from a 2D image using deep networks. While achieving state-of-the-art performance on standard benchmarks, their performance degrades under occlusion. In contrast, optimization-based methods fit a parametric body model to 2D features in an iterative manner. The localized reconstruction loss can potentially make them robust to occlusion, but they suffer from the 2D-3D ambiguity. Motivated by the recent success of generative models in rigid object pose estimation, we propose 3D-aware Neural Body Fitting (3DNBF) - an approximate analysis-by-synthesis approach to 3D human pose estimation with SOTA performance and occlusion robustness. In particular, we propose a generative model of deep features based on a volumetric human representation with Gaussian ellipsoidal kernels emitting 3D pose-dependent feature vectors. The neural features are trained with contrastive learning to become 3D-aware and hence to overcome the 2D-3D ambiguity. Experiments show that 3DNBF outperforms other approaches on both occluded and standard benchmarks",
    "checked": true,
    "id": "8a46058e5353ae5cc66dd8fc67e635aaad3a5f17",
    "semantic_title": "3d-aware neural body fitting for occlusion robust 3d human pose estimation",
    "citation_count": 0,
    "authors": [
      "Yi Zhang",
      "Pengliang Ji",
      "Angtian Wang",
      "Jieru Mei",
      "Adam Kortylewski",
      "Alan Yuille"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Chinese_Text_Recognition_with_A_Pre-Trained_CLIP-Like_Model_Through_Image-IDS_ICCV_2023_paper.html": {
    "title": "Chinese Text Recognition with A Pre-Trained CLIP-Like Model Through Image-IDS Aligning",
    "volume": "main",
    "abstract": "Scene text recognition has been studied for decades due to its broad applications. However, despite Chinese characters possessing different characteristics from Latin characters, such as complex inner structures and large categories, few methods have been proposed for Chinese Text Recognition (CTR). Particularly, the characteristic of large categories poses challenges in dealing with zero-shot and few-shot Chinese characters. In this paper, inspired by the way humans recognize Chinese texts, we propose a two-stage framework for CTR. Firstly, we pre-train a CLIP-like model through aligning printed character images and Ideographic Description Sequences (IDS). This pre-training stage simulates humans recognizing Chinese characters and obtains the canonical representation of each character. Subsequently, the learned representations are employed to supervise the CTR model, such that traditional single-character recognition can be improved to text-line recognition through image-IDS matching. To evaluate the effectiveness of the proposed method, we conduct extensive experiments on both Chinese character recognition (CCR) and CTR. The experimental results demonstrate that the proposed method performs best in CCR and outperforms previous methods in most scenarios of the CTR benchmark. It is worth noting that the proposed method can recognize zero-shot Chinese characters in text images without fine-tuning, whereas previous methods require fine-tuning when new classes appear. The code is available at https://github.com/FudanVI/FudanOCR/tree/main/image-ids-CTR",
    "checked": true,
    "id": "b87c58fd2f2eec6aa29f7428fffaf09ef98e7f99",
    "semantic_title": "chinese text recognition with a pre-trained clip-like model through image-ids aligning",
    "citation_count": 1,
    "authors": [
      "Haiyang Yu",
      "Xiaocong Wang",
      "Bin Li",
      "Xiangyang Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_MatrixCity_A_Large-scale_City_Dataset_for_City-scale_Neural_Rendering_and_ICCV_2023_paper.html": {
    "title": "MatrixCity: A Large-scale City Dataset for City-scale Neural Rendering and Beyond",
    "volume": "main",
    "abstract": "Neural radiance fields (NeRF) and its subsequent variants have led to remarkable progress in neural rendering. While most of recent neural rendering works focus on objects and small-scale scenes, developing neural rendering methods for city-scale scenes is of great potential in many real-world applications. However, this line of research is impeded by the absence of a comprehensive and high-quality dataset, yet collecting such a dataset over real city-scale scenes is costly, sensitive, and technically infeasible. To this end, we build a large-scale, comprehensive, and high-quality synthetic dataset for city-scale neural rendering researches. Leveraging the Unreal Engine 5 City Sample project, we developed a pipeline to easily collect aerial and street city views, accompanied by ground-truth camera poses and a range of additional data modalities. Flexible controls on environmental factors like light, weather, human and car crowd are also available in our pipeline, supporting the need of various tasks covering city-scale neural rendering and beyond. The resulting pilot dataset, MatrixCity, contains 67k aerial images and 452k street images from two city maps of total size 28km^2. On top of MatrixCity, a thorough benchmark is also conducted, which not only reveals unique challenges of the task of city-scale neural rendering, but also highlights potential improvements for future works. The dataset and code will be publicly available at the project page: https://city-super.github.io/matrixcity/",
    "checked": true,
    "id": "861593f632f98aa597802b95056a05b37e8955f8",
    "semantic_title": "matrixcity: a large-scale city dataset for city-scale neural rendering and beyond",
    "citation_count": 0,
    "authors": [
      "Yixuan Li",
      "Lihan Jiang",
      "Linning Xu",
      "Yuanbo Xiangli",
      "Zhenzhi Wang",
      "Dahua Lin",
      "Bo Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_LinkGAN_Linking_GAN_Latents_to_Pixels_for_Controllable_Image_Synthesis_ICCV_2023_paper.html": {
    "title": "LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis",
    "volume": "main",
    "abstract": "This work presents an easy-to-use regularizer for GAN training, which helps explicitly link some axes of the latent space to a set of pixels in the synthesized image. Establishing such a connection facilitates a more convenient local control of GAN generation, where users can alter the image content only within a spatial area simply by partially resampling the latent code. Experimental results confirm four appealing properties of our regularizer, which we call LinkGAN. (1) The latent-pixel linkage is applicable to either a fixed region (i.e., same for all instances) or a particular semantic category (i.e., varying across instances), like the sky. (2) Two or multiple regions can be independently linked to different latent axes, which further supports joint control. (3) Our regularizer can improve the spatial controllability of both 2D and 3D-aware GAN models, barely sacrificing the synthesis performance. (4) The models trained with our regularizer are compatible with GAN inversion techniques and maintain editability on real images",
    "checked": true,
    "id": "0fc8bc437bd741410a5688d475bf8b0687aef039",
    "semantic_title": "linkgan: linking gan latents to pixels for controllable image synthesis",
    "citation_count": 6,
    "authors": [
      "Jiapeng Zhu",
      "Ceyuan Yang",
      "Yujun Shen",
      "Zifan Shi",
      "Bo Dai",
      "Deli Zhao",
      "Qifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cancelli_Exploiting_Proximity-Aware_Tasks_for_Embodied_Social_Navigation_ICCV_2023_paper.html": {
    "title": "Exploiting Proximity-Aware Tasks for Embodied Social Navigation",
    "volume": "main",
    "abstract": "Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key ability required to embodied agents to be integrated into our society. In this paper, we propose an end-to-end architecture that exploits Proximity-Aware Tasks (referred as to Risk and Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviours. To this end, our tasks exploit the notion of immediate and future dangers of collision. Furthermore, we propose an evaluation protocol specifically designed for the Social Navigation Task in simulated environments. This is done to capture fine-grained features and characteristics of the policy by analyzing the minimal unit of human-robot spatial interaction, called Encounter. We validate our approach on Gibson4+ and Habitat-Matterport3D datasets",
    "checked": true,
    "id": "823a71d74e22a7ccdf0da32b413cbac7b1f0e59b",
    "semantic_title": "exploiting proximity-aware tasks for embodied social navigation",
    "citation_count": 0,
    "authors": [
      "Enrico Cancelli",
      "Tommaso Campari",
      "Luciano Serafini",
      "Angel X. Chang",
      "Lamberto Ballan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_SVDiff_Compact_Parameter_Space_for_Diffusion_Fine-Tuning_ICCV_2023_paper.html": {
    "title": "SVDiff: Compact Parameter Space for Diffusion Fine-Tuning",
    "volume": "main",
    "abstract": "Recently, diffusion models have achieved remarkable success in text-to-image generation, enabling the creation of high-quality images from text prompts and various conditions. However, existing methods for customizing these models are limited by handling multiple personalized subjects and the risk of overfitting. Moreover, the large parameter space is inefficient for model storage. In this paper, we propose a novel approach to address the limitations in existing text-to-image diffusion models for personalization and customization. Our method involves fine-tuning the singular values of the weight matrices, leading to a compact and efficient parameter space that reduces the risk of overfitting and language-drifting. Our approach also includes a Cut-Mix-Unmix data-augmentation technique to enhance the quality of multi-subject image generation and a simple text-based image editing framework. Our proposed SVDiff method has a significantly smaller model size (1.7MB for StableDiffusion) compared to existing methods, making it more practical for real-world applications",
    "checked": true,
    "id": "6ae34677bc41e1818a899583b25e379dadc42a85",
    "semantic_title": "svdiff: compact parameter space for diffusion fine-tuning",
    "citation_count": 31,
    "authors": [
      "Ligong Han",
      "Yinxiao Li",
      "Han Zhang",
      "Peyman Milanfar",
      "Dimitris Metaxas",
      "Feng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_UniFace_Unified_Cross-Entropy_Loss_for_Deep_Face_Recognition_ICCV_2023_paper.html": {
    "title": "UniFace: Unified Cross-Entropy Loss for Deep Face Recognition",
    "volume": "main",
    "abstract": "As a widely used loss function in deep face recognition, the softmax loss cannot guarantee that the minimum positive sample-to-class similarity is larger than the maximum negative sample-to-class similarity. As a result, no unified threshold is available to separate positive sample-to-class pairs from negative sample-to-class pairs. To bridge this gap, we design a UCE (Unified Cross-Entropy) loss for face recognition model training, which is built on the vital constraint that all the positive sample-to-class similarities shall be larger than the negative ones. Our UCE loss can be integrated with margins for a further performance boost. The face recognition model trained with the proposed UCE loss, UniFace, was intensively evaluated using a number of popular public datasets like MFR, IJB-C, LFW, CFP-FP, AgeDB, and MegaFace. Experimental results show that our approach outperforms SOTA methods like SphereFace, CosFace, ArcFace, Partial FC, etc. Especially, till the submission of this work (Mar. 8, 2023), the proposed UniFace achieves the highest TAR@MR-All on the academic track of the MFR-ongoing challenge. Code is publicly available",
    "checked": false,
    "id": "8490426a6cb4d11128667479bdbc6e1e91389dd6",
    "semantic_title": "multimodal biometric verification using deep neural network",
    "citation_count": 0,
    "authors": [
      "Jiancan Zhou",
      "Xi Jia",
      "Qiufu Li",
      "Linlin Shen",
      "Jinming Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Frumkin_Jumping_through_Local_Minima_Quantization_in_the_Loss_Landscape_of_ICCV_2023_paper.html": {
    "title": "Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers",
    "volume": "main",
    "abstract": "Quantization scale and bit-width are the most important parameters when considering how to quantize a neural network. Prior work focuses on optimizing quantization scales in a global manner through gradient methods (gradient descent & Hessian analysis). Yet, when applying perturbations to quantization scales, we observe a very jagged, highly non-smooth test loss landscape. In fact, small perturbations in quantization scale can greatly affect accuracy, yielding a 0.5-0.8% accuracy boost in 4-bit quantized vision transformers (ViTs). In this regime, gradient methods break down, since they cannot reliably reach local minima. In our work, dubbed Evol-Q, we use evolutionary search to effectively traverse the non-smooth landscape. Additionally, we propose using an infoNCE loss, which not only helps combat overfitting on the small (1,000 images) calibration dataset but also makes traversing such a highly non-smooth surface easier. Evol-Q improves the top-1 accuracy of a fully quantized ViT-Base by 10.30%, 0.78%, and 0.15% for 3-bit, 4-bit, and 8-bit weight quantization levels. Extensive experiments on a variety of CNN and ViT architectures further demonstrate its robustness in extreme quantization scenarios",
    "checked": true,
    "id": "a8816be7ce30825eb5b48c336696fd68f08f9689",
    "semantic_title": "jumping through local minima: quantization in the loss landscape of vision transformers",
    "citation_count": 0,
    "authors": [
      "Natalia Frumkin",
      "Dibakar Gope",
      "Diana Marculescu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Feng_Hierarchical_Contrastive_Learning_for_Pattern-Generalizable_Image_Corruption_Detection_ICCV_2023_paper.html": {
    "title": "Hierarchical Contrastive Learning for Pattern-Generalizable Image Corruption Detection",
    "volume": "main",
    "abstract": "Effective image restoration with large-size corruptions, such as blind image inpainting, entails precise detection of corruption region masks which remains extremely challenging due to diverse shapes and patterns of corruptions. In this work, we present a novel method for automatic corruption detection, which allows for blind corruption restoration without known corruption masks. Specifically, we develop a hierarchical contrastive learning framework to detect corrupted regions by capturing the intrinsic semantic distinctions between corrupted and uncorrupted regions. In particular, our model detects the corrupted mask in a coarse-to-fine manner by first predicting a coarse mask by contrastive learning in low-resolution feature space and then refines the uncertain area of the mask by high-resolution contrastive learning. A specialized hierarchical interaction mechanism is designed to facilitate the knowledge propagation of contrastive learning in different scales, boosting the modeling performance substantially. The detected multi-scale corruption masks are then leveraged to guide the corruption restoration. Detecting corrupted regions by learning the contrastive distinctions rather than the semantic patterns of corruptions, our model has well generalization ability across different corruption patterns. Extensive experiments demonstrate following merits of our model: 1) the superior performance over other methods on both corruption detection and various image restoration tasks including blind inpainting and watermark removal, and 2) strong generalization across different corruption patterns such as graffiti, random noise or other image content. Codes and trained weights are available at https://github.com/xyfJASON/HCL",
    "checked": true,
    "id": "c142fbbed90e865938f7a346550463b5b1802750",
    "semantic_title": "hierarchical contrastive learning for pattern-generalizable image corruption detection",
    "citation_count": 0,
    "authors": [
      "Xin Feng",
      "Yifeng Xu",
      "Guangming Lu",
      "Wenjie Pei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Luo_Learning_Optical_Flow_from_Event_Camera_with_Rendered_Dataset_ICCV_2023_paper.html": {
    "title": "Learning Optical Flow from Event Camera with Rendered Dataset",
    "volume": "main",
    "abstract": "We study the problem of estimating optical flow from event cameras. One important issue is how to build a high-quality event-flow dataset with accurate event values and flow labels. Previous datasets are created by either capturing real scenes by event cameras or synthesizing from images with pasted foreground objects. The former case can produce real event values but with calculated flow labels, which are sparse and inaccurate. The later case can generate dense flow labels but the interpolated events are prone to errors. In this work, we propose to render a physically correct event-flow dataset using computer graphics models. In particular, we first create indoor and outdoor 3D scenes by Blender with rich scene content variations. Second, diverse camera motions are included for the virtual capturing, producing images and accurate flow labels. Third, we render high-framerate videos between images for accurate events. The rendered dataset can adjust the density of events, based on which we further introduce an adaptive density module (ADM). Experiments show that our proposed dataset can facilitate event-flow learning, whereas previous approaches when trained on our dataset can improve their performances constantly by a relatively large margin. In addition, event-flow pipelines when equipped with our ADM can further improve performances. Our code and dataset will be publicly available",
    "checked": true,
    "id": "ce3b604a362d09df224de065b1dbfed5d762a8a5",
    "semantic_title": "learning optical flow from event camera with rendered dataset",
    "citation_count": 1,
    "authors": [
      "Xinglong Luo",
      "Kunming Luo",
      "Ao Luo",
      "Zhengning Wang",
      "Ping Tan",
      "Shuaicheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Levi_EPiC_Ensemble_of_Partial_Point_Clouds_for_Robust_Classification_ICCV_2023_paper.html": {
    "title": "EPiC: Ensemble of Partial Point Clouds for Robust Classification",
    "volume": "main",
    "abstract": "Robust point cloud classification is crucial for real-world applications,as consumer-type 3D sensors often yield partial and noisy data, degraded by various artifacts. In this work we propose a general ensemble framework, based on partial point cloud sampling. Each ensemble member is exposed to only partial input data. Three sampling strategies are used jointly, two local ones, based on patches and curves, and a global one of random sampling. We demonstrate the robustness of our method to various local and global degradations. We show that our framework significantly improves the robustness of top classification netowrks by a large margin. Our experimental setting uses the recently introduced ModelNet-C database by Ren et al., where we reach SOTA both on unaugmented and on augmented data. Our unaugmented mean Corruption Error (mCE) is 0.64 (current SOTA is 0.86) and 0.50 for augmented data (current SOTA is 0.57). We analyze and explain these remarkable results through diversity analysis. Our code is availabe at: https://github.com/yossilevii100/EPiC",
    "checked": true,
    "id": "111ddd59def7479b793c8a89c4c42072072da402",
    "semantic_title": "epic: ensemble of partial point clouds for robust classification",
    "citation_count": 1,
    "authors": [
      "Meir Yossef Levi",
      "Guy Gilboa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Distilling_Large_Vision-Language_Model_with_Out-of-Distribution_Generalizability_ICCV_2023_paper.html": {
    "title": "Distilling Large Vision-Language Model with Out-of-Distribution Generalizability",
    "volume": "main",
    "abstract": "Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a small- or mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enriching the teacher's language representations with informative and finegrained semantic attributes to effectively distinguish between different labels. We propose several metrics and conduct extensive experiments to investigate their techniques. The results demonstrate significant improvements in zero-shot and few-shot student performance on open-vocabulary out-of-distribution classification, highlighting the effectiveness of our proposed approaches",
    "checked": true,
    "id": "97ffad903208c7bea48e4c8be0a68e27fc33a478",
    "semantic_title": "distilling large vision-language model with out-of-distribution generalizability",
    "citation_count": 1,
    "authors": [
      "Xuanlin Li",
      "Yunhao Fang",
      "Minghua Liu",
      "Zhan Ling",
      "Zhuowen Tu",
      "Hao Su"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Cross-Modal_Learning_with_3D_Deformable_Attention_for_Action_Recognition_ICCV_2023_paper.html": {
    "title": "Cross-Modal Learning with 3D Deformable Attention for Action Recognition",
    "volume": "main",
    "abstract": "An important challenge in vision-based action recognition is the embedding of spatiotemporal features with two or more heterogeneous modalities into a single feature. In this study, we propose a new 3D deformable transformer for action recognition with adaptive spatiotemporal receptive fields and a cross-modal learning scheme. The 3D deformable transformer consists of three attention modules: 3D deformability, local joint stride, and temporal stride attention. The two cross-modal tokens are input into the 3D deformable attention module to create a cross-attention token with a reflected spatiotemporal correlation. Local joint stride attention is applied to spatially combine attention and pose tokens. Temporal stride attention temporally reduces the number of input tokens in the attention module and supports temporal expression learning without the simultaneous use of all tokens. The deformable transformer iterates L-times and combines the last cross-modal token for classification. The proposed 3D deformable transformer was tested on the NTU60, NTU120, FineGYM, and PennAction datasets, and showed results better than or similar to pre-trained state-of-the-art methods even without a pre-training process. In addition, by visualizing important joints and correlations during action recognition through spatial joint and temporal stride attention, the possibility of achieving an explainable potential for action recognition is presented",
    "checked": true,
    "id": "76b428366d3cb42299bc76aebc935573da6fe02a",
    "semantic_title": "cross-modal learning with 3d deformable attention for action recognition",
    "citation_count": 1,
    "authors": [
      "Sangwon Kim",
      "Dasom Ahn",
      "Byoung Chul Ko"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_What_do_neural_networks_learn_in_image_classification_A_frequency_ICCV_2023_paper.html": {
    "title": "What do neural networks learn in image classification? A frequency shortcut perspective",
    "volume": "main",
    "abstract": "Frequency analysis is useful for understanding the mechanisms of representation learning in neural networks (NNs). Most research in this area focuses on the learning dynamics of NNs for regression tasks, while little for classification. This study empirically investigates the latter and expands the understanding of frequency shortcuts. First, we perform experiments on synthetic datasets, designed to have a bias in different frequency bands. Our results demonstrate that NNs tend to find simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics, which can be either low- or high-frequencies. Second, we confirm this phenomenon on natural images. We propose a metric to measure class-wise frequency characteristics and a method to identify frequency shortcuts. The results show that frequency shortcuts can be texture-based or shape-based, depending on what best simplifies the objective. Third, we validate the transferability of frequency shortcuts on out-of-distribution (OOD) test sets. Our results suggest that frequency shortcuts can be transferred across datasets and cannot be fully avoided by larger model capacity and data augmentation. We recommend that future research should focus on effective training schemes mitigating frequency shortcut learning. Codes and data are available at https://github.com/nis-research/nn-frequency-shortcuts",
    "checked": true,
    "id": "e039424da6356fb5b2bfe7a24142b653f4422202",
    "semantic_title": "what do neural networks learn in image classification? a frequency shortcut perspective",
    "citation_count": 3,
    "authors": [
      "Shunxin Wang",
      "Raymond Veldhuis",
      "Christoph Brune",
      "Nicola Strisciuglio"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Rozumnyi_Tracking_by_3D_Model_Estimation_of_Unknown_Objects_in_Videos_ICCV_2023_paper.html": {
    "title": "Tracking by 3D Model Estimation of Unknown Objects in Videos",
    "volume": "main",
    "abstract": "Most model-free visual object tracking methods formulate the tracking task as object location estimation given by a 2D segmentation or a bounding box in each video frame. We argue that this representation is limited and instead propose to guide and improve 2D tracking with an explicit object representation, namely the textured 3D shape and 6DoF pose in each video frame. Our representation tackles a complex long-term dense correspondence problem between all 3D points on the object for all video frames, including frames where some points are invisible. To achieve that, the estimation is driven by re-rendering the input video frames as well as possible through differentiable rendering, which has not been used for tracking before. The proposed optimization minimizes a novel loss function to estimate the best 3D shape, texture, and 6DoF pose. We improve the state-of-the-art in 2D segmentation tracking on three different datasets with mostly rigid objects",
    "checked": true,
    "id": "09abc0707dde676741d55e98fd8a0b4e0e0aecf0",
    "semantic_title": "tracking by 3d model estimation of unknown objects in videos",
    "citation_count": 0,
    "authors": [
      "Denys Rozumnyi",
      "JiÅÃ­ Matas",
      "Marc Pollefeys",
      "Vittorio Ferrari",
      "Martin R. Oswald"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ramazzina_ScatterNeRF_Seeing_Through_Fog_with_Physically-Based_Inverse_Neural_Rendering_ICCV_2023_paper.html": {
    "title": "ScatterNeRF: Seeing Through Fog with Physically-Based Inverse Neural Rendering",
    "volume": "main",
    "abstract": "Vision in adverse weather conditions, whether it be snow, rain, or fog is challenging. In these scenarios, scattering and attenuation severly degrades image quality. Handling such inclement weather conditions, however, is essential to operate autonomous vehicles, drones and robotic applications where human performance is impeded the most. A large body of work explores removing weather-induced image degradations with dehazing methods. Most methods rely on single images as input and struggle to generalize from synthetic fully-supervised training approaches or to generate high fidelity results from unpaired real-world datasets. With data as bottleneck and most of today's training data relying on good weather conditions with inclement weather as outlier, we rely on an inverse rendering approach to reconstruct the scene content. We introduce ScatterNeRF, a neural rendering method which adequately renders foggy scenes and decomposes the fog-free background from the participating media -- exploiting the multiple views from a short automotive sequence without the need for a large training data corpus. Instead, the rendering approach is optimized on the multi-view scene itself, which can be typically captured by an autonomous vehicle, robot or drone during operation. Specifically, we propose a disentangled representation for the scattering volume and the scene objects, and learn the scene reconstruction with physics-inspired losses. We validate our method by capturing multi-view In-the-Wild data and controlled captures in a large-scale fog chamber. Our code and datasets are available at https://light.princeton.edu/scatternerf",
    "checked": true,
    "id": "bb8536acdec514e95543e699f5f804ba0859afb7",
    "semantic_title": "scatternerf: seeing through fog with physically-based inverse neural rendering",
    "citation_count": 0,
    "authors": [
      "Andrea Ramazzina",
      "Mario Bijelic",
      "Stefanie Walz",
      "Alessandro Sanvito",
      "Dominik Scheuble",
      "Felix Heide"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.html": {
    "title": "Sigmoid Loss for Language Image Pre-Training",
    "volume": "main",
    "abstract": "We propose a simple pairwise sigmoid loss for image-text pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k batch size and a Large LiT model at 20k batch size, the latter achieves 84.5% ImageNet zero-shot accuracy in two days. This disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training",
    "checked": true,
    "id": "35aba190f28b5c39df333c06ca21f46bd4845eba",
    "semantic_title": "sigmoid loss for language image pre-training",
    "citation_count": 15,
    "authors": [
      "Xiaohua Zhai",
      "Basil Mustafa",
      "Alexander Kolesnikov",
      "Lucas Beyer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_PromptCap_Prompt-Guided_Image_Captioning_for_VQA_with_GPT-3_ICCV_2023_paper.html": {
    "title": "PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3",
    "volume": "main",
    "abstract": "Knowledge-based visual question answering (VQA) involves questions that require world knowledge beyond the image to yield the correct answer. Large language models (LMs) like GPT-3 are particularly helpful for this task because of their strong knowledge retrieval and reasoning capabilities. To enable LM to understand images, prior work uses a captioning model to convert images into text. However, when summarizing an image in a single caption sentence, which visual entities to describe are often underspecified. Generic image captions often miss visual details essential for the LM to answer visual questions correctly. To address this challenge, we propose PromptCap (Prompt-guided image Captioning), a captioning model designed to serve as a better connector between images and black-box LMs. Different from generic captions, PromptCap takes a natural-language prompt to control the visual entities to describe in the generated caption. The prompt contains a question that the caption should aid in answering. To avoid extra annotation, PromptCap is trained by examples synthesized with GPT-3 and existing datasets. We demonstrate PromptCap's effectiveness on an existing pipeline in which GPT-3 is prompted with image captions to carry out VQA. PromptCap outperforms generic captions by a large margin and achieves state-of-the-art accuracy on knowledge-based VQA tasks (60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that PromptCap generalizes well to unseen domains",
    "checked": false,
    "id": "00c1ff63468305ea3fa430c2b3aef156d580c4ff",
    "semantic_title": "p rompt c ap : prompt-guided image captioning for vqa with gpt-3",
    "citation_count": 2,
    "authors": [
      "Yushi Hu",
      "Hang Hua",
      "Zhengyuan Yang",
      "Weijia Shi",
      "Noah A. Smith",
      "Jiebo Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Neural_Video_Depth_Stabilizer_ICCV_2023_paper.html": {
    "title": "Neural Video Depth Stabilizer",
    "volume": "main",
    "abstract": "Video depth estimation aims to infer temporally consistent depth. Some methods achieve temporal consistency by finetuning a single-image depth model during test time using geometry and re-projection constraints, which is inefficient and not robust. An alternative approach is to learn how to enforce temporal consistency from data, but this requires well-designed models and sufficient video depth data. To address these challenges, we propose a plug-and-play framework called Neural Video Depth Stabilizer (NVDS) that stabilizes inconsistent depth estimations and can be applied to different single-image depth models without extra effort. We also introduce a large-scale dataset, Video Depth in the Wild (VDW), which consists of 14,203 videos with over two million frames, making it the largest natural-scene video depth dataset to our knowledge. We evaluate our method on the VDW dataset as well as two public benchmarks and demonstrate significant improvements in consistency, accuracy, and efficiency compared to previous approaches. Our work serves as a solid baseline and provides a data foundation for learning-based video depth models. We will release our dataset and code for future research",
    "checked": true,
    "id": "7b9862d731a4e83ed1898824cb95f0bd87899dea",
    "semantic_title": "neural video depth stabilizer",
    "citation_count": 1,
    "authors": [
      "Yiran Wang",
      "Min Shi",
      "Jiaqi Li",
      "Zihao Huang",
      "Zhiguo Cao",
      "Jianming Zhang",
      "Ke Xian",
      "Guosheng Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Learning_Symmetry-Aware_Geometry_Correspondences_for_6D_Object_Pose_Estimation_ICCV_2023_paper.html": {
    "title": "Learning Symmetry-Aware Geometry Correspondences for 6D Object Pose Estimation",
    "volume": "main",
    "abstract": "Current 6D pose estimation methods focus on handling objects that are previously trained, which limits their applications in real dynamic world. To this end, we propose a geometry correspondence-based framework, termed GCPose, to estimate 6D pose of arbitrary unseen objects without any re-training. Specifically, the proposed method draws the idea from point cloud registration and resorts to object-agnostic geometry features to establish the 3D-3D correspondences between the object-scene point cloud and object-model point cloud. Then the 6D pose parameters are solved by a least-squares fitting algorithm. Taking the symmetry properties of objects into consideration, we design a symmetry-aware matching loss to facilitate the learning of dense point-wise geometry features and improve the performance considerably. Moreover, we introduce an online training data generation with special data augmentation and normalization to empower the network to learn diverse geometry prior. With training on synthetic objects from ShapeNet, our method outperforms previous approaches for unseen object pose estimation by a large margin on T-LESS, LINEMOD, Occluded-LINEMOD, and TUD-L datasets. Code is available at https://github.com/hikvision-research/GCPose",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Zhao",
      "Shenxing Wei",
      "Dahu Shi",
      "Wenming Tan",
      "Zheyang Li",
      "Ye Ren",
      "Xing Wei",
      "Yi Yang",
      "Shiliang Pu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mancusi_TrackFlow_Multi-Object_tracking_with_Normalizing_Flows_ICCV_2023_paper.html": {
    "title": "TrackFlow: Multi-Object tracking with Normalizing Flows",
    "volume": "main",
    "abstract": "The field of multi-object tracking has recently seen a renewed interest in the good old schema of tracking-by-detection, as its simplicity and strong priors spare it from the complex design and painful babysitting of tracking-by-attention approaches. In view of this, we aim at extending tracking-by-detection to multi-modal settings, where a comprehensive cost has to be computed from heterogeneous information e.g., 2D motion cues, visual appearance, and pose estimates. More precisely, we follow a case study where a rough estimate of 3D information is also available and must be merged with other traditional metrics (e.g., the IoU). To achieve that, recent approaches resort to either simple rules or complex heuristics to balance the contribution of each cost. However, i) they require careful tuning of tailored hyperparameters on a hold-out set, and ii) they imply these costs to be independent, which does not hold in reality. We address these issues by building upon an elegant probabilistic formulation, which considers the cost of a candidate association as the negative log-likelihood yielded by a deep density estimator, trained to model the conditional joint probability distribution of correct associations. Our experiments, conducted on both simulated and real benchmarks, show that our approach consistently enhances the performance of several tracking-by-detection algorithms",
    "checked": true,
    "id": "3aa164d5c40fc5fb9035c41049259c47474531e8",
    "semantic_title": "trackflow: multi-object tracking with normalizing flows",
    "citation_count": 0,
    "authors": [
      "Gianluca Mancusi",
      "Aniello Panariello",
      "Angelo Porrello",
      "Matteo Fabbri",
      "Simone Calderara",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Towards_Generic_Image_Manipulation_Detection_with_Weakly-Supervised_Self-Consistency_Learning_ICCV_2023_paper.html": {
    "title": "Towards Generic Image Manipulation Detection with Weakly-Supervised Self-Consistency Learning",
    "volume": "main",
    "abstract": "As advanced image manipulation techniques emerge, detecting the manipulation becomes increasingly important. Despite the success of recent learning-based approaches for image manipulation detection, they typically require expensive pixel-level annotations to train, while exhibiting degraded performance when testing on images that are differently manipulated compared with training images. To address these limitations, we propose weakly-supervised image manipulation detection, such that only binary image-level labels (authentic or tampered with) are required for training purpose. Such weakly-supervised setting can leverage more training images and has the potential to adapt quickly to new manipulation techniques. To improve the generalization ability, we propose weakly-supervised self-consistency learning (WSCL) to leverage the weakly annotated images. For the second problem, we propose an end-to-end learnable method, which takes advantage of image self-consistency properties. Specifically, two consistency properties are learned: multi-source consistency (MSC) and inter-patch consistency (IPC). MSC exploits different content-agnostic information and enables cross-source learning via an online pseudo label generation and refinement process. IPC performs global pair-wise patch-patch relationship reasoning to discover a complete region of manipulation. Extensive experiments validate that our WSCL, even though is weakly supervised, exhibits competitive performance compared with fully-supervised counterpart under both in-distribution and out-of-distribution evaluations, as well as reasonable manipulation localization ability",
    "checked": true,
    "id": "76a5bf61a80a962af0c71ef475a3de44d636e2be",
    "semantic_title": "towards generic image manipulation detection with weakly-supervised self-consistency learning",
    "citation_count": 0,
    "authors": [
      "Yuanhao Zhai",
      "Tianyu Luan",
      "David Doermann",
      "Junsong Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ying_PARF_Primitive-Aware_Radiance_Fusion_for_Indoor_Scene_Novel_View_Synthesis_ICCV_2023_paper.html": {
    "title": "PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis",
    "volume": "main",
    "abstract": "This paper proposes a method for fast scene radiance field reconstruction with strong novel view synthesis performance and convenient scene editing functionality. The key idea is to fully utilize semantic parsing and primitive extraction for constraining and accelerating the radiance field reconstruction process. To fulfill this goal, a primitive-aware hybrid rendering strategy was proposed to enjoy the best of both volumetric and primitive rendering. We further contribute a reconstruction pipeline conducts primitive parsing and radiance field learning iteratively for each input frame which successfully fuses semantic, primitive, and radiance information into a single framework. Extensive evaluations demonstrate the fast reconstruction ability, high rendering quality, and convenient editing functionality of our method",
    "checked": true,
    "id": "f5bdafc7bccc4d345b7c7c3e2dd7873f78d3c213",
    "semantic_title": "parf: primitive-aware radiance fusion for indoor scene novel view synthesis",
    "citation_count": 0,
    "authors": [
      "Haiyang Ying",
      "Baowei Jiang",
      "Jinzhi Zhang",
      "Di Xu",
      "Tao Yu",
      "Qionghai Dai",
      "Lu Fang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nakamura_DeePoint_Visual_Pointing_Recognition_and_Direction_Estimation_ICCV_2023_paper.html": {
    "title": "DeePoint: Visual Pointing Recognition and Direction Estimation",
    "volume": "main",
    "abstract": "In this paper, we realize automatic visual recognition and direction estimation of pointing. We introduce the first neural pointing understanding method based on two key contributions. The first is the introduction of a first-of-its-kind large-scale dataset for pointing recognition and direction estimation, which we refer to as the DP Dataset. DP Dataset consists of more than 2 million frames of 33 people pointing in various styles annotated for each frame with pointing timings and 3D directions. The second is DeePoint, a novel deep network model for joint recognition and 3D direction estimation of pointing. DeePoint is a Transformer-based network which fully leverages the spatio-temporal coordination of the body parts, not just the hands. Through extensive experiments, we demonstrate the accuracy and efficiency of DeePoint. We believe DP Dataset and DeePoint will serve as a sound foundation for visual human intention understanding",
    "checked": true,
    "id": "45e13b064fedc76fccd64e6351003474c2cf9764",
    "semantic_title": "deepoint: visual pointing recognition and direction estimation",
    "citation_count": 0,
    "authors": [
      "Shu Nakamura",
      "Yasutomo Kawanishi",
      "Shohei Nobuhara",
      "Ko Nishino"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Periodically_Exchange_Teacher-Student_for_Source-Free_Object_Detection_ICCV_2023_paper.html": {
    "title": "Periodically Exchange Teacher-Student for Source-Free Object Detection",
    "volume": "main",
    "abstract": "Source-free object detection (SFOD) aims to adapt the source detector to unlabeled target domain data in the absence of source domain data. Most SFOD methods follow the same self-training paradigm using mean-teacher (MT) framework where the student model is guided by only one single teacher model. However, such paradigm can easily fall into a training instability problem that when the teacher model collapses uncontrollably due to the domain shift, the student model also suffers drastic performance degradation. To address this issue, we propose the Periodically Exchange Teacher-Student (PETS) method, a simple yet novel approach that introduces a multiple-teacher framework consisting of a static teacher, a dynamic teacher, and a student model. During the training phase, we periodically exchange the weights between the static teacher and the student model. Then, we update the dynamic teacher using the moving average of the student model that has already been exchanged by the static teacher. In this way, the dynamic teacher can integrate knowledge from past periods, effectively reducing error accumulation and enabling a more stable training process within the MT-based framework. Further, we develop a consensus mechanism to merge the predictions of two teacher models to provide higher-quality pseudo labels for student model. Extensive experiments on multiple SFOD benchmarks show that the proposed method achieves state-of-the-art performance compared with other related methods, demonstrating the effectiveness and superiority of our method on SFOD task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qipeng Liu",
      "Luojun Lin",
      "Zhifeng Shen",
      "Zhifeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jung_Generating_Instance-level_Prompts_for_Rehearsal-free_Continual_Learning_ICCV_2023_paper.html": {
    "title": "Generating Instance-level Prompts for Rehearsal-free Continual Learning",
    "volume": "main",
    "abstract": "We introduce Domain-Adaptive Prompt (DAP), a novel method for continual learning using Vision Transformers (ViT). Prompt-based continual learning has recently gained attention due to its rehearsal-free nature. Currently, the prompt pool, which is suggested by prompt-based continual learning, is key to effectively exploiting the frozen pre-trained ViT backbone in a sequence of tasks. However, we observe that the use of a prompt pool creates a domain scalability problem between pre-training and continual learning. This problem arises due to the inherent encoding of group-level instructions within the prompt pool. To address this problem, we propose DAP, a pool-free approach that generates a suitable prompt in an instance-level manner at inference time. We optimize an adaptive prompt generator that creates instance-specific fine-grained instructions required for each input, enabling enhanced model plasticity and reduced forgetting. Our experiments on seven datasets with varying degrees of domain similarity to ImageNet demonstrate the superiority of DAP over state-of-the-art prompt-based methods. Code is publicly available at https://github.com/naver-ai/dap-cl",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dahuin Jung",
      "Dongyoon Han",
      "Jihwan Bang",
      "Hwanjun Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fu_Deformer_Dynamic_Fusion_Transformer_for_Robust_Hand_Pose_Estimation_ICCV_2023_paper.html": {
    "title": "Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation",
    "volume": "main",
    "abstract": "Accurately estimating 3D hand pose is crucial for understanding how humans interact with the world. Despite remarkable progress, existing methods often struggle to generate plausible hand poses when the hand is heavily occluded or blurred. In videos, the movements of the hand allow us to observe various parts of the hand that may be occluded or blurred in a single frame. To adaptively leverage the visual clue before and after the occlusion or blurring for robust hand pose estimation, we propose the Deformer: a framework that implicitly reasons about the relationship between hand parts within the same image (spatial dimension) and different timesteps (temporal dimension). We show that a naive application of the transformer self-attention mechanism is not sufficient because motion blur or occlusions in certain frames can lead to heavily distorted hand features and generate imprecise keys and queries. To address this challenge, we incorporate a Dynamic Fusion Module into Deformer, which predicts the deformation of the hand and warps the hand mesh predictions from nearby frames to explicitly support the current frame estimation. Furthermore, we have observed that errors are unevenly distributed across different hand parts, with vertices around fingertips having disproportionately higher errors than those around the palm. We mitigate this issue by introducing a new loss function called maxMSE that automatically adjusts the weight of every vertex to focus the model on critical hand parts. Extensive experiments show that our method significantly outperforms state-of-the-art methods by 10%, and is more robust to occlusions (over 14%)",
    "checked": true,
    "id": "604098f2b7644ac2bbe356985809e38e651073eb",
    "semantic_title": "deformer: dynamic fusion transformer for robust hand pose estimation",
    "citation_count": 0,
    "authors": [
      "Qichen Fu",
      "Xingyu Liu",
      "Ran Xu",
      "Juan Carlos Niebles",
      "Kris M. Kitani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_HSE_Hybrid_Species_Embedding_for_Deep_Metric_Learning_ICCV_2023_paper.html": {
    "title": "HSE: Hybrid Species Embedding for Deep Metric Learning",
    "volume": "main",
    "abstract": "Deep metric learning is crucial for finding an embedding function that can generalize to training and testing data, including unknown test classes. However, limited training samples restrict the model's generalization to downstream tasks. While adding new training samples is a promising solution, determining their labels remains a significant challenge. Here, we introduce Hybrid Species Embedding (HSE), which employs mixed sample data augmentations to generate hybrid species and provide additional training signals. We demonstrate that HSE outperforms multiple state-of-the-art methods in improving the metric Recall@K on the CUB-200 , CAR-196 and SOP datasets, thus offering a novel solution to deep metric learning's limitations",
    "checked": false,
    "id": "a3a1d65c088f7ff7deb0327aaf2d9227f419a135",
    "semantic_title": "journal pre-proof adh-ppi: an attention based deep hybrid model for protein protein interaction prediction",
    "citation_count": 0,
    "authors": [
      "Bailin Yang",
      "Haoqiang Sun",
      "Frederick W. B. Li",
      "Zheng Chen",
      "Jianlu Cai",
      "Chao Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Online_Continual_Learning_on_Hierarchical_Label_Expansion_ICCV_2023_paper.html": {
    "title": "Online Continual Learning on Hierarchical Label Expansion",
    "volume": "main",
    "abstract": "Continual learning (CL) enables models to adapt to new tasks and environments without forgetting previously learned knowledge. While current CL setups have ignored the relationship between labels in the past task and the new task with or without small task overlaps, real-world scenarios often involve hierarchical relationships between old and new tasks, posing another challenge for traditional CL approaches. To address this challenge, we propose a novel multi-level hierarchical class incremental task configuration with an online learning constraint, called hierarchical label expansion (HLE). Our configuration allows a network to first learn coarse-grained classes, with data labels continually expanding to more fine-grained classes in various hierarchy depths. To tackle this new setup, we propose a rehearsal-based method that utilizes hierarchy-aware pseudo-labeling to incorporate hierarchical class information. Additionally, we propose a simple yet effective memory management and sampling strategy that selectively adopts samples of newly encountered classes. Our experiments demonstrate that our proposed method can effectively use hierarchy on our HLE setup to improve classification accuracy across all levels of hierarchies, regardless of depth and class imbalance ratio, outperforming prior state-of-the-art works by significant margins while also outperforming them on the conventional disjoint, blurry and i-Blurry CL setups",
    "checked": true,
    "id": "6653ab4c3ae943ff1d66a26601c27e75b567fdcb",
    "semantic_title": "online continual learning on hierarchical label expansion",
    "citation_count": 0,
    "authors": [
      "Byung Hyun Lee",
      "Okchul Jung",
      "Jonghyun Choi",
      "Se Young Chun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_iDAG_Invariant_DAG_Searching_for_Domain_Generalization_ICCV_2023_paper.html": {
    "title": "iDAG: Invariant DAG Searching for Domain Generalization",
    "volume": "main",
    "abstract": "Existing machine learning (ML) models are often fragile in open environments because the data distribution frequently shifts. To address this problem, domain generalization (DG) aims to explore underlying invariant patterns for stable prediction across domains. In this work, we first characterize that this failure of conventional ML models in DG is attributed to an inadequate identification of causal structures. We further propose a novel and theoretically grounded invariant Directed Acyclic Graph (dubbed iDAG) searching framework that attains an invariant graphical relation as the proxy to the causality structure from the intrinsic data-generating process. To enable tractable computation, iDAG solves a constrained optimization objective built on a set of representative class-conditional prototypes. Additionally, we integrate a hierarchical contrastive learning module, which poses a strong effect of clustering, for enhanced prototypes as well as stabler prediction. Extensive experiments on the synthetic and real-world benchmarks demonstrate that iDAG outperforms the state-of-the-art approaches, verifying the superiority of causal structure identification for DG. The code of iDAG is available at https://github.com/lccurious/iDAG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zenan Huang",
      "Haobo Wang",
      "Junbo Zhao",
      "Nenggan Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Choe_Spacetime_Surface_Regularization_for_Neural_Dynamic_Scene_Reconstruction_ICCV_2023_paper.html": {
    "title": "Spacetime Surface Regularization for Neural Dynamic Scene Reconstruction",
    "volume": "main",
    "abstract": "We propose an algorithm, 4DRegSDF, for the spacetime surface regularization to improve the fidelity of neural rendering and reconstruction in dynamic scenes. The key idea is to impose local rigidity on the deformable Signed Distance Function (SDF) for temporal coherency. Our approach works by (1) sampling points on the deformed surface by taking gradient steps toward the steepest direction along SDF, (2) extracting differential surface geometry, such as tangent plane or curvature, at each sample, and (3) adjusting the local rigidity at different timestamps. This enables our dynamic surface regularization to align 4D spacetime geometry via 3D canonical space more accurately. Experiments demonstrate that our 4DRegSDF achieves state-of-the-art performance in both reconstruction and rendering quality over synthetic and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaesung Choe",
      "Christopher Choy",
      "Jaesik Park",
      "In So Kweon",
      "Anima Anandkumar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_GasMono_Geometry-Aided_Self-Supervised_Monocular_Depth_Estimation_for_Indoor_Scenes_ICCV_2023_paper.html": {
    "title": "GasMono: Geometry-Aided Self-Supervised Monocular Depth Estimation for Indoor Scenes",
    "volume": "main",
    "abstract": "This paper tackles the challenges of self-supervised monocular depth estimation in indoor scenes caused by large rotation between frames and low texture. We ease the learning process by obtaining coarse camera poses from monocular sequences through multi-view geometry to deal with the former. However, we found that limited by the scale ambiguity across different scenes in the training dataset, a naive introduction of geometric coarse poses cannot play a positive role in performance improvement, which is counter-intuitive. To address this problem, we propose to refine those poses during training through rotation and translation/scale optimization. To soften the effect of the low texture, we combine the global reasoning of vision transformers with an overfitting-aware, iterative self-distillation mechanism, providing more accurate depth guidance coming from the network itself. Experiments on NYUv2, ScanNet, 7scenes, and KITTI datasets support the effectiveness of each component in our framework, which sets a new state-of-the-art for indoor self-supervised monocular depth estimation, as well as outstanding generalization ability. Code and models are available at https://github.com/zxcqlf/GasMono",
    "checked": true,
    "id": "0bd8c2d9689a1e619ace46c643f025db851f71e8",
    "semantic_title": "gasmono: geometry-aided self-supervised monocular depth estimation for indoor scenes",
    "citation_count": 0,
    "authors": [
      "Chaoqiang Zhao",
      "Matteo Poggi",
      "Fabio Tosi",
      "Lei Zhou",
      "Qiyu Sun",
      "Yang Tang",
      "Stefano Mattoccia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Feng_3D_Motion_Magnification_Visualizing_Subtle_Motions_from_Time-Varying_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "3D Motion Magnification: Visualizing Subtle Motions from Time-Varying Radiance Fields",
    "volume": "main",
    "abstract": "Motion magnification helps us visualize subtle, imperceptible motion. However, prior methods only work for 2D videos captured with a fixed camera. We present a 3D motion magnification method that can magnify subtle motions from scenes captured by a moving camera, while supporting novel view rendering. We represent the scene with time-varying radiance fields and leverage the Eulerian principle for motion magnification to extract and amplify the variation of the embedding of a fixed point over time. We study and validate our proposed principle for 3D motion magnification using both implicit and tri-plane-based radiance fields as our underlying 3D scene representation. We evaluate the effectiveness of our method on both synthetic and real-world scenes captured under various camera setups",
    "checked": false,
    "id": "f9730814187e5fbe9e8a1a3c1c548ec9a0e3202a",
    "semantic_title": "3d motion magnification: visualizing subtle motions with time varying radiance fields",
    "citation_count": 0,
    "authors": [
      "Brandon Y. Feng",
      "Hadi Alzayer",
      "Michael Rubinstein",
      "William T. Freeman",
      "Jia-bin Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Singhal_Learning_to_Transform_for_Generalizable_Instance-wise_Invariance_ICCV_2023_paper.html": {
    "title": "Learning to Transform for Generalizable Instance-wise Invariance",
    "volume": "main",
    "abstract": "Computer vision research has long aimed to build systems that are robust to transformations found in natural data. Traditionally, this is done using data augmentation or hard-coding invariances into the architecture. However, too much or too little invariance can hurt, and the correct amount is unknown a priori and dependent on the instance. Ideally, the appropriate invariance would be learned from data and inferred at test-time. We treat invariance as a prediction problem. Given any image, we predict a distribution over transformations. We use variational inference to learn this distribution end-to-end. Combined with a graphical model approach, this distribution forms a flexible, generalizable, and adaptive form of invariance. Our experiments show that it can be used to align datasets and discover prototypes, adapt to out-of-distribution poses, and generalize invariances across classes. When used for data augmentation, our method shows consistent gains in accuracy and robustness on CIFAR 10, CIFAR10-LT, and TinyImageNet",
    "checked": true,
    "id": "bfb29c72dcbde72e5507f2ecf0c6a30f5c7290fe",
    "semantic_title": "learning to transform for generalizable instance-wise invariance",
    "citation_count": 0,
    "authors": [
      "Utkarsh Singhal",
      "Carlos Esteves",
      "Ameesh Makadia",
      "Stella X. Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Audio-Visual_Deception_Detection_DOLOS_Dataset_and_Parameter-Efficient_Crossmodal_Learning_ICCV_2023_paper.html": {
    "title": "Audio-Visual Deception Detection: DOLOS Dataset and Parameter-Efficient Crossmodal Learning",
    "volume": "main",
    "abstract": "Deception detection in conversations is a challenging yet important task, having pivotal applications in many fields such as credibility assessment in business, multimedia anti-frauds, and custom security. Despite this, deception detection research is hindered by the lack of high-quality deception datasets, as well as the difficulties of learning multimodal features effectively. To address this issue, we introduce DOLOS, the largest gameshow deception detection dataset with rich deceptive conversations. DOLOS includes 1,675 video clips featuring 213 subjects, and it has been labeled with audio-visual feature annotations. We provide train-test, duration, and gender protocols to investigate the impact of different factors. We benchmark our dataset on previously proposed deception detection approaches. To further improve the performance by fine-tuning fewer parameters, we propose Parameter-Efficient Crossmodal Learning (PECL), where a Uniform Temporal Adapter (UT-Adapter) explores temporal attention in transformer-based architectures, and a crossmodal fusion module, Plug-in Audio-Visual Fusion (PAVF), combines crossmodal information from audio-visual features. Based on the rich fine-grained audio-visual annotations on DOLOS, we also exploit multi-task learning to enhance performance by concurrently predicting deception and audio-visual features. Experimental results demonstrate the desired quality of the DOLOS dataset and the effectiveness of the PECL. The DOLOS dataset and the source codes are available",
    "checked": true,
    "id": "565a241925d89c4561886cbe6681ce194e01fe4a",
    "semantic_title": "audio-visual deception detection: dolos dataset and parameter-efficient crossmodal learning",
    "citation_count": 0,
    "authors": [
      "Xiaobao Guo",
      "Nithish Muthuchamy Selvaraj",
      "Zitong Yu",
      "Adams Wai-Kin Kong",
      "Bingquan Shen",
      "Alex Kot"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Multiple_Instance_Learning_Framework_with_Masked_Hard_Instance_Mining_for_ICCV_2023_paper.html": {
    "title": "Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification",
    "volume": "main",
    "abstract": "The whole slide image (WSI) classification is often formulated as a multiple instance learning (MIL) problem. Since the positive tissue is only a small fraction of the gigapixel WSI, existing MIL methods intuitively focus on identifying salient instances via attention mechanisms. However, this leads to a bias towards easy-to-classify instances while neglecting hard-to-classify instances. Some literature has revealed that hard examples are beneficial for modeling a discriminative boundary accurately. By applying such an idea at the instance level, we elaborate a novel MIL framework with masked hard instance mining (MHIM-MIL), which uses a Siamese structure (Teacher-Student) with a consistency constraint to explore the potential hard instances. With several instance masking strategies based on attention scores, MHIM-MIL employs a momentum teacher to implicitly mine hard instances for training the student model, which can be any attention-based MIL model. This counter-intuitive strategy essentially enables the student to learn a better discriminating boundary. Moreover, the student is used to update the teacher with an exponential moving average (EMA), which in turn identifies new hard instances for subsequent training iterations and stabilizes the optimization. Experimental results on the CAMELYON-16 and TCGA Lung Cancer datasets demonstrate that MHIM-MIL outperforms other latest methods in terms of performance and training cost. The code is available at: https://github.com/DearCaat/MHIM-MIL",
    "checked": true,
    "id": "30f9b1fd46e9192b41d2b0d877e06688d0b2e9b5",
    "semantic_title": "multiple instance learning framework with masked hard instance mining for whole slide image classification",
    "citation_count": 0,
    "authors": [
      "Wenhao Tang",
      "Sheng Huang",
      "Xiaoxian Zhang",
      "Fengtao Zhou",
      "Yi Zhang",
      "Bo Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Unsupervised_Compositional_Concepts_Discovery_with_Text-to-Image_Generative_Models_ICCV_2023_paper.html": {
    "title": "Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models",
    "volume": "main",
    "abstract": "Text-to-image generative models have enabled high-resolution image synthesis across different domains, but require users to specify the content they wish to generate. In this paper, we consider the inverse problem - given a collection of different images, can we discover the generative concepts that represent each image? We present an unsupervised approach to discover generative concepts from a collection of images, disentangling different art styles in paintings, objects, and lighting from kitchen scenes, and discovering image classes given ImageNet images. We show how such generative concepts can accurately represent the content of images, be recombined and composed to generate new artistic and hybrid images, and be further used as a representation for downstream classification tasks",
    "checked": true,
    "id": "e38ba8795bcd7488a831f52b5911a85ca94f5387",
    "semantic_title": "unsupervised compositional concepts discovery with text-to-image generative models",
    "citation_count": 4,
    "authors": [
      "Nan Liu",
      "Yilun Du",
      "Shuang Li",
      "Joshua B. Tenenbaum",
      "Antonio Torralba"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Partition-And-Debias_Agnostic_Biases_Mitigation_via_a_Mixture_of_Biases-Specific_Experts_ICCV_2023_paper.html": {
    "title": "Partition-And-Debias: Agnostic Biases Mitigation via a Mixture of Biases-Specific Experts",
    "volume": "main",
    "abstract": "Bias mitigation in image classification has been widely researched, and existing methods have yielded notable results. However, most of these methods implicitly assume that a given image contains only one type of known or unknown bias, failing to consider the complexities of real-world biases. We introduce a more challenging scenario, agnostic biases mitigation, aiming at bias removal regardless of whether the type of bias or the number of types is unknown in the datasets. To address this difficult task, we present the Partition-and-Debias (PnD) method that uses a mixture of biases-specific experts to implicitly divide the bias space into multiple subspaces and a gating module to find a consensus among experts to achieve debiased classification. Experiments on both public and constructed benchmarks demonstrated the efficacy of the PnD. Code is available at: https://github.com/Jiaxuan-Li/PnD",
    "checked": true,
    "id": "58a9fd742f74f6e2f57a4ed0994a31c195f1e0e2",
    "semantic_title": "partition-and-debias: agnostic biases mitigation via a mixture of biases-specific experts",
    "citation_count": 1,
    "authors": [
      "Jiaxuan Li",
      "Duc Minh Vo",
      "Hideki Nakayama"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Spatial_Self-Distillation_for_Object_Detection_with_Inaccurate_Bounding_Boxes_ICCV_2023_paper.html": {
    "title": "Spatial Self-Distillation for Object Detection with Inaccurate Bounding Boxes",
    "volume": "main",
    "abstract": "Object detection via inaccurate bounding box supervision has boosted a broad interest due to the expensive high-quality annotation data or the occasional inevitability of low annotation quality (e.g. tiny objects). The previous works usually utilize multiple instance learning (MIL), which highly depends on category information, to select and refine a low-quality box. Those methods suffer from part domination, object drift and group prediction problems without exploring spatial information. In this paper, we heuristically propose a Spatial Self-Distillation based Object Detector (SSD-Det) to mine spatial information to refine the inaccurate box in a self-distillation fashion. SSD-Det utilizes a Spatial Position Self-Distillation SPSD) module to exploit spatial information and an interactive structure to combine spatial information and category information, thus constructing a high-quality proposal bag. To further improve the selection procedure, a Spatial Identity Self-Distillation (SISD) module is introduced in SSD-Det to obtain spatial confidence to help select the best proposals. Experiments on MS-COCO and VOC datasets with noisy box annotation verify our method's effectiveness and achieve state-of-the-art performance. The code is available at https://github.com/ucas-vg/PointTinyBenchmark/tree/SSD-Det",
    "checked": true,
    "id": "8d9990836b4613828772472e07c2635c65580bc9",
    "semantic_title": "spatial self-distillation for object detection with inaccurate bounding boxes",
    "citation_count": 0,
    "authors": [
      "Di Wu",
      "Pengfei Chen",
      "Xuehui Yu",
      "Guorong Li",
      "Zhenjun Han",
      "Jianbin Jiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bahmani_CC3D_Layout-Conditioned_Generation_of_Compositional_3D_Scenes_ICCV_2023_paper.html": {
    "title": "CC3D: Layout-Conditioned Generation of Compositional 3D Scenes",
    "volume": "main",
    "abstract": "In this work, we introduce CC3D, a conditional generative model that synthesizes complex 3D scenes conditioned on 2D semantic scene layouts, trained using single-view images. Different from most existing 3D GANs that limit their applicability to aligned single objects, we focus on generating complex scenes with multiple objects, by modeling the compositional nature of 3D scenes. By devising a 2D layout-based approach for 3D synthesis and implementing a new 3D field representation with a stronger geometric inductive bias, we have created a 3D GAN that is both efficient and of high quality, while allowing for a more controllable generation process. Our evaluations on synthetic 3D-FRONT and real-world KITTI-360 datasets demonstrate that our model generates scenes of improved visual and geometric quality in comparison to previous works",
    "checked": true,
    "id": "93fa94a75e91cb0de6db54d3461bbef0185a7b77",
    "semantic_title": "cc3d: layout-conditioned generation of compositional 3d scenes",
    "citation_count": 4,
    "authors": [
      "Sherwin Bahmani",
      "Jeong Joon Park",
      "Despoina Paschalidou",
      "Xingguang Yan",
      "Gordon Wetzstein",
      "Leonidas Guibas",
      "Andrea Tagliasacchi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kang_Alleviating_Catastrophic_Forgetting_of_Incremental_Object_Detection_via_Within-Class_and_ICCV_2023_paper.html": {
    "title": "Alleviating Catastrophic Forgetting of Incremental Object Detection via Within-Class and Between-Class Knowledge Distillation",
    "volume": "main",
    "abstract": "Incremental object detection (IOD) task requires a model to learn continually from newly added data. However, directly fine-tuning a well-trained detection model on a new task will sharply decrease the performance on old tasks, which is known as catastrophic forgetting. Knowledge distillation, including feature distillation and response distillation, has been proven to be an effective way to alleviate catastrophic forgetting. However, previous works on feature distillation heavily rely on low-level feature information, while under-exploring the importance of high-level semantic information. In this paper, we discuss the cause of catastrophic forgetting in IOD task as destruction of semantic feature space. We propose a method that dynamically distills both semantic and feature information with consideration of both between-class discriminativeness and within-class consistency on Transformer-based detector. Between-class discriminativeness is preserved by distilling class-level semantic distance and feature distance among various categories, while within-class consistency is preserved by distilling instance-level semantic information and feature information within each category. Extensive experiments are conducted on both Pascal VOC and MS COCO benchmarks. Our method outperforms all the previous CNN-based SOTA methods under various experimental scenarios, with a remarkable mAP improvement from 36.90% to 39.80% under one-step IOD task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxue Kang",
      "Jinpeng Zhang",
      "Jinming Zhang",
      "Xiashuang Wang",
      "Yang Chen",
      "Zhe Ma",
      "Xuhui Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_TextPSG_Panoptic_Scene_Graph_Generation_from_Textual_Descriptions_ICCV_2023_paper.html": {
    "title": "TextPSG: Panoptic Scene Graph Generation from Textual Descriptions",
    "volume": "main",
    "abstract": "Panoptic Scene Graph has recently been proposed for comprehensive scene understanding. However, previous works adopt a fully-supervised learning manner, requiring large amounts of pixel-wise densely-annotated data, which is always tedious and expensive to obtain. To address this limitation, we study a new problem of Panoptic Scene Graph Generation from Purely Textual Descriptions (Caption-to-PSG). The key idea is to leverage the large collection of free image-caption data on the Web alone to generate panoptic scene graphs. The problem is very challenging for three constraints: 1) no location priors; 2) no explicit links between visual regions and textual entities; and 3) no pre-defined concept sets. To tackle this problem, we propose a new framework TextPSG consisting of four modules, i.e., a region grouper, an entity grounder, a segment merger, and a label generator, with several novel techniques. The region grouper first groups image pixels into different segments and the entity grounder then aligns visual segments with language entities based on the textual description of the segment being referred to. The grounding results can thus serve as pseudo labels enabling the segment merger to learn the segment similarity as well as guiding the label generator to learn object semantics and relation predicates, resulting in a fine-grained structured scene understanding. Our framework is effective, significantly outperforming the baselines and achieving strong out-of-distribution robustness. We perform comprehensive ablation studies to corroborate the effectiveness of our design choices and provide an in-depth analysis to highlight future directions. Our code, data, and results are available on our project page: https://vis-www.cs.umass.edu/TextPSG",
    "checked": true,
    "id": "2b3999f3c4b172ce3c1419dfe705a77424ba6a86",
    "semantic_title": "textpsg: panoptic scene graph generation from textual descriptions",
    "citation_count": 0,
    "authors": [
      "Chengyang Zhao",
      "Yikang Shen",
      "Zhenfang Chen",
      "Mingyu Ding",
      "Chuang Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jie_Revisiting_the_Parameter_Efficiency_of_Adapters_from_the_Perspective_of_ICCV_2023_paper.html": {
    "title": "Revisiting the Parameter Efficiency of Adapters from the Perspective of Precision Redundancy",
    "volume": "main",
    "abstract": "Current state-of-the-art results in computer vision depend in part on fine-tuning large pre-trained vision models. However, with the exponential growth of model sizes, the conventional full fine-tuning, which needs to store a individual network copy for each tasks, leads to increasingly huge storage and transmission overhead. Adapter-based Parameter-Efficient Tuning (PET) methods address this challenge by tuning lightweight adapters inserted into the frozen pre-trained models. In this paper, we investigate how to make adapters even more efficient, reaching a new minimum size required to store a task-specific fine-tuned network. Inspired by the observation that the parameters of adapters converge at flat local minima, we find that adapters are resistant to noise in parameter space, which means they are also resistant to low numerical precision. To train low-precision adapters, we propose a computational-efficient quantization method which minimizes the quantization error. Through extensive experiments, we find that low-precision adapters exhibit minimal performance degradation, and even 1-bit precision is sufficient for adapters. The results of the experiments demonstrate that 1-bit adapters outperform all other PET methods on both the VTAB-1K benchmark and few-shot FGVC datasets, while requiring the smallest storage size. Our findings show, for the first time, the significant potential of quantization techniques in PET, providing a general solution to enhance the parameter efficiency of adapter-based PET methods",
    "checked": true,
    "id": "d0a70f8ff7b34b26e825f732fd31973c6f530d59",
    "semantic_title": "revisiting the parameter efficiency of adapters from the perspective of precision redundancy",
    "citation_count": 0,
    "authors": [
      "Shibo Jie",
      "Haoqing Wang",
      "Zhi-Hong Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_EMQ_Evolving_Training-free_Proxies_for_Automated_Mixed_Precision_Quantization_ICCV_2023_paper.html": {
    "title": "EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization",
    "volume": "main",
    "abstract": "Mixed-Precision Quantization (MQ) can achieve a competitive accuracy-complexity trade-off for models. Conventional training-based search methods require time-consuming candidate training to search optimized per-layer bit-width configurations in MQ. Recently, some training-free approaches have presented various MQ proxies and significantly improve search efficiency. However, the correlation between these proxies and quantization accuracy is poorly understood. To address the gap, we first build the MQ-Bench-101, which involves different bit configurations and quantization results. Then, we observe that the existing training-free proxies perform weak correlations on the MQ-Bench-101. To efficiently seek superior proxies, we develop an automatic search of proxies framework for MQ via evolving algorithms. In particular, we devise an elaborate search space involving the existing proxies and perform an evolution search to discover the best correlated MQ proxy. We proposed a diversity-prompting selection strategy and compatibility screening protocol to avoid premature convergence and improve search efficiency. In this way, our Evolving proxies for Mixed-precision Quantization (EMQ) framework allows the auto-generation of proxies without heavy tuning and expert knowledge. Extensive experiments on ImageNet with various ResNet and MobileNet families demonstrate that our EMQ obtains superior performance than state-of-the-art mixed-precision methods at a significantly reduced cost. The code will be released",
    "checked": true,
    "id": "bc88c99658ff4e258a153e152efe7aaf4e781d4e",
    "semantic_title": "emq: evolving training-free proxies for automated mixed precision quantization",
    "citation_count": 1,
    "authors": [
      "Peijie Dong",
      "Lujun Li",
      "Zimian Wei",
      "Xin Niu",
      "Zhiliang Tian",
      "Hengyue Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Face_Clustering_via_Graph_Convolutional_Networks_with_Confidence_Edges_ICCV_2023_paper.html": {
    "title": "Face Clustering via Graph Convolutional Networks with Confidence Edges",
    "volume": "main",
    "abstract": "Face clustering is a method for unlabeled image annotation and has attracted increasing attention. Existing methods have made significant breakthroughs by introducing Graph Convolutional Networks (GCNs) on the affinity graph. However, such graphs will contain many vertex pairs with inconsistent similarities and labels, thus degrading the model's performance. There are already relevant efforts for this problem, but the information about features needs to be mined further. In this paper, we define a new concept called confidence edge and guide the construction of graphs. Furthermore, a novel confidence-GCN is proposed to cluster face images by deriving more confidence edges. Firstly, Local Information Fusion is advanced to obtain a more accurate similarity metric by considering the neighbors of vertices. Then Unsupervised Neighbor Determination is used to discard low-quality edges based on similarity differences. Moreover, we elaborate that the remaining edges retain the most beneficial information to demonstrate the validity. At last, the confidence-GCN takes the graph as the input and fully uses the confidence edges to complete the clustering. Experiments show that our method outperforms existing methods on the face and person datasets to achieve state-of-the-art. At the same time, comparable results are obtained on the fashion dataset",
    "checked": false,
    "id": "30ed9950e10d1024b003ae2125967da50d79d662",
    "semantic_title": "face clustering via adaptive aggregation of clean neighbors",
    "citation_count": 0,
    "authors": [
      "Yang Wu",
      "Zhiwei Ge",
      "Yuhao Luo",
      "Lin Liu",
      "Sulong Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Learning_Spatial-context-aware_Global_Visual_Feature_Representation_for_Instance_Image_Retrieval_ICCV_2023_paper.html": {
    "title": "Learning Spatial-context-aware Global Visual Feature Representation for Instance Image Retrieval",
    "volume": "main",
    "abstract": "In instance image retrieval, considering local spatial information within an image has proven effective to boost retrieval performance, as demonstrated by local visual descriptor based geometric verification. Nevertheless, it will be highly valuable to make ordinary global image representations spatial-context-aware because global representation based image retrieval is appealing thanks to its algorithmic simplicity, low memory cost, and being friendly to sophisticated data structures. To this end, we propose a novel feature learning framework for instance image retrieval, which embeds local spatial context information into the learned global feature representations. Specifically, in parallel to the visual feature branch in a CNN backbone, we design a spatial context branch that consists of two modules called online token learning and distance encoding. For each local descriptor learned in CNN, the former module is used to indicate the types of its surrounding descriptors, while their spatial distribution information is captured by the latter module. After that, the visual feature branch and the spatial context branch are fused to produce a single global feature representation per image. As experimentally demonstrated, with the spatial-context-aware characteristic, we can well improve the performance of global representation based image retrieval while maintaining all of its appealing properties. Our code is available at https://github.com/Zy-Zhang/SpCa",
    "checked": false,
    "id": "f6bdb7b982878c38080c10b9562a865cb34b4144",
    "semantic_title": "sr-gnn: spatial relation-aware graph neural network for fine-grained image categorization",
    "citation_count": 10,
    "authors": [
      "Zhongyan Zhang",
      "Lei Wang",
      "Luping Zhou",
      "Piotr Koniusz"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/de_Guevara_Cross-modal_Latent_Space_Alignment_for_Image_to_Avatar_Translation_ICCV_2023_paper.html": {
    "title": "Cross-modal Latent Space Alignment for Image to Avatar Translation",
    "volume": "main",
    "abstract": "We present a novel method for automatic vectorized avatar generation from a single portrait image. Most existing approaches that create avatars rely on image-to-image translation methods, which present some limitations when applied to 3D rendering, animation, or video. Instead, we leverage modality-specific autoencoders trained on large-scale unpaired portraits and parametric avatars, and then learn a mapping between both modalities via an alignment module trained on a significantly smaller amount of data. The resulting cross-modal latent space preserves facial identity, producing more visually appealing and higher fidelity avatars than previous methods, as supported by our quantitative and qualitative evaluations. Moreover, our method's virtue of being resolution-independent makes it highly versatile and applicable in a wide range of settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Ladron de Guevara",
      "Jose Echevarria",
      "Yijun Li",
      "Yannick Hold-Geoffroy",
      "Cameron Smith",
      "Daichi Ito"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Basu_Inspecting_the_Geographical_Representativeness_of_Images_from_Text-to-Image_Models_ICCV_2023_paper.html": {
    "title": "Inspecting the Geographical Representativeness of Images from Text-to-Image Models",
    "volume": "main",
    "abstract": "Recent progress in generative models has resulted in models that produce both realistic as well as relevant images for most textual inputs. These models are being used to generate millions of images everyday, and hold the potential to drastically impact areas such as generative art, digital marketing and data augmentation. Given their outsized impact, it is important to ensure that the generated content reflects the artifacts and surroundings across the globe, rather than over-representing certain parts of the world. In this paper, we measure the geographical representativeness of common nouns (e.g., a house) generated through DALL.E 2 and Stable Diffusion models using a crowdsourced study comprising 540 participants across 27 countries. For deliberately underspecified inputs without country names, the generated images most reflect the surroundings of the United States followed by India, and the top generations rarely reflect surroundings from all other countries (average score less than 3 out of 5). Specifying the country names in the input increases the representativeness by 1.44 points on average on a 5-point Likert scale for DALL.E 2 and 0.75 for Stable Diffusion, however, the overall scores for many countries still remain low, highlighting the need for future models to be more geographically inclusive. Lastly, we examine the feasibility of quantifying the geographical representativeness of generated images without conducting user studies",
    "checked": true,
    "id": "45e10b9e55268c9e55cbc66bd5e0ee37a1e43f9b",
    "semantic_title": "inspecting the geographical representativeness of images from text-to-image models",
    "citation_count": 2,
    "authors": [
      "Abhipsa Basu",
      "R. Venkatesh Babu",
      "Danish Pruthi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pei_Space-time_Prompting_for_Video_Class-incremental_Learning_ICCV_2023_paper.html": {
    "title": "Space-time Prompting for Video Class-incremental Learning",
    "volume": "main",
    "abstract": "Recently, prompt-based learning has made impressive progress on image class-incremental learning, but it still lacks sufficient exploration in the video domain. In this paper, we will fill this gap by learning multiple prompts based on a powerful image-language pre-trained model, i.e., CLIP, making it fit for video class-incremental learning (VCIL). For this purpose, we present a space-time prompting approach (ST-Prompt) which contains two kinds of prompts, i.e., task-specific prompts and task-agnostic prompts. The task-specific prompts are to address the catastrophic forgetting problem by learning multi-grained prompts, i.e., spatial prompts, temporal prompts and comprehensive prompts, for accurate task identification. The task-agnostic prompts maintain a globally-shared prompt pool, which can empower the pre-trained image models with temporal perception abilities by exchanging contexts between frames. By this means, ST-Prompt can transfer the plentiful knowledge in the image-language pre-trained models to the VCIL task with only a tiny set of prompts to be optimized. To evaluate ST-Prompt, we conduct extensive experiments on three standard benchmarks. The results show that ST-Prompt can significantly surpass the state-of-the-art VCIL methods, especially it gains 9.06% on HMDB51 dataset under the 1*25 stage setting",
    "checked": false,
    "id": "da7bf9af6b5dca0bfbb787b420acb350c6155de2",
    "semantic_title": "stochastic coherence over attention trajectory for continuous learning in video streams",
    "citation_count": 2,
    "authors": [
      "Yixuan Pei",
      "Zhiwu Qing",
      "Shiwei Zhang",
      "Xiang Wang",
      "Yingya Zhang",
      "Deli Zhao",
      "Xueming Qian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Baldrati_Multimodal_Garment_Designer_Human-Centric_Latent_Diffusion_Models_for_Fashion_Image_ICCV_2023_paper.html": {
    "title": "Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing",
    "volume": "main",
    "abstract": "Fashion illustration is used by designers to communicate their vision and to bring the design idea from conceptualization to realization, showing how clothes interact with the human body. In this context, computer vision can thus be used to improve the fashion design process. Differently from previous works that mainly focused on the virtual try-on of garments, we propose the task of multimodal-conditioned fashion image editing, guiding the generation of human-centric fashion images by following multimodal prompts, such as text, human body poses, and garment sketches. We tackle this problem by proposing a new architecture based on latent diffusion models, an approach that has not been used before in the fashion domain. Given the lack of existing datasets suitable for the task, we also extend two existing fashion datasets, namely Dress Code and VITON-HD, with multimodal annotations collected in a semi-automatic manner. Experimental results on these new datasets demonstrate the effectiveness of our proposal, both in terms of realism and coherence with the given multimodal inputs. Source code and collected multimodal annotations are publicly available at: https://github.com/aimagelab/multimodal-garment-designer",
    "checked": true,
    "id": "6c925427841ea4a776a578d438f9e47a64c3014e",
    "semantic_title": "multimodal garment designer: human-centric latent diffusion models for fashion image editing",
    "citation_count": 3,
    "authors": [
      "Alberto Baldrati",
      "Davide Morelli",
      "Giuseppe Cartella",
      "Marcella Cornia",
      "Marco Bertini",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nunes_Time-to-Contact_Map_by_Joint_Estimation_of_Up-to-Scale_Inverse_Depth_and_ICCV_2023_paper.html": {
    "title": "Time-to-Contact Map by Joint Estimation of Up-to-Scale Inverse Depth and Global Motion using a Single Event Camera",
    "volume": "main",
    "abstract": "Event cameras asynchronously report brightness changes with a temporal resolution in the order of microseconds, which makes them inherently suitable to address problems that involve rapid motion perception. In this paper, we address the problem of time-to-contact (TTC) estimation using a single event camera. This problem is typically addressed by estimating a single global TTC measure, which explicitly assumes that the surface/obstacle is planar and fronto-parallel. We relax this assumption by proposing an incremental event-based method to estimate the TTC that jointly estimates the (up-to scale) inverse depth and global motion using a single event camera. The proposed method is reliable and fast while asynchronously maintaining a TTC map (TTCM), which provides per-pixel TTC estimates. As a side product, the proposed method can also estimate per-event optical flow. We achieve state-of-the-art performances on TTC estimation in terms of accuracy and runtime per event while achieving competitive performance on optical flow estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Urbano Miguel Nunes",
      "Laurent Udo Perrinet",
      "Sio-Hoi Ieng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Sparse_Sampling_Transformer_with_Uncertainty-Driven_Ranking_for_Unified_Removal_of_ICCV_2023_paper.html": {
    "title": "Sparse Sampling Transformer with Uncertainty-Driven Ranking for Unified Removal of Raindrops and Rain Streaks",
    "volume": "main",
    "abstract": "In the real world, image degradations caused by rain often exhibit a combination of rain streaks and raindrops, thereby increasing the challenges of recovering the underlying clean image. Note that the rain streaks and raindrops have diverse shapes, sizes, and locations in the captured image, and thus modeling the correlation relationship between irregular degradations caused by rain artifacts is a necessary prerequisite for image deraining. This paper aims to present an efficient and flexible mechanism to learn and model degradation relationships in a global view, thereby achieving a unified removal of intricate rain scenes. To do so, we propose a Sparse Sampling Transformer based on Uncertainty-Driven Ranking, dubbed UDR-S2Former. Compared to previous methods, our UDR-S2Former has three merits. First, it can adaptively sample relevant image degradation information to model underlying degradation relationships. Second, explicit application of the uncertainty-driven ranking strategy can facilitate the network to attend to degradation features and understand the reconstruction process. Finally, experimental results show that our UDR-S2Former clearly outperforms state-of-the-art methods for all benchmarks",
    "checked": true,
    "id": "cb4f2f44d5ce37b7d7bf69fe7139f204eb79d199",
    "semantic_title": "sparse sampling transformer with uncertainty-driven ranking for unified removal of raindrops and rain streaks",
    "citation_count": 1,
    "authors": [
      "Sixiang Chen",
      "Tian Ye",
      "Jinbin Bai",
      "Erkang Chen",
      "Jun Shi",
      "Lei Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_A_Benchmark_for_Chinese-English_Scene_Text_Image_Super-Resolution_ICCV_2023_paper.html": {
    "title": "A Benchmark for Chinese-English Scene Text Image Super-Resolution",
    "volume": "main",
    "abstract": "Scene Text Image Super-resolution (STISR) aims to recover high-resolution (HR) scene text images with visually pleasant and readable text content from the given low-resolution (LR) input. Most existing works focus on recovering English texts, which have simple structures in the characters, while little work has been done on the more challenging Chinese texts with diverse and complex character structures. In this paper, we propose a real-world Chinese-English benchmark dataset, namely Real-CE, for the task of STISR with the emphasis on restoring structurally complex Chinese characters. The benchmark provides 1,935/783 real-world LR-HR text image pairs (contains 33,789 text lines in total) for training/testing in 2x and 4x zooming modes, complemented by detailed annotations, including detection boxes and text transcripts. Moreover, we design an edge-aware learning method, which provides structural supervision in image and feature domain, to effectively reconstruct the dense structures of Chinese characters. We conduct experiments on the proposed Real-CE benchmark and evaluate the existing STISR models with and without our edge-aware loss. The benchmark, including data and source code, will be made publicly available",
    "checked": true,
    "id": "10fdca507ab770e7a31fae8abbaee3ca470d5a78",
    "semantic_title": "a benchmark for chinese-english scene text image super-resolution",
    "citation_count": 0,
    "authors": [
      "Jianqi Ma",
      "Zhetong Liang",
      "Wangmeng Xiang",
      "Xi Yang",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_HSR-Diff_Hyperspectral_Image_Super-Resolution_via_Conditional_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "HSR-Diff: Hyperspectral Image Super-Resolution via Conditional Diffusion Models",
    "volume": "main",
    "abstract": "Despite the proven significance of hyperspectral images (HSIs) in performing various computer vision tasks, its potential is adversely affected by the low-resolution (LR) property in the spatial domain, resulting from multiple physical factors. Inspired by recent advancements in deep generative models, we propose an HSI Super-resolution (SR) approach with Conditional Diffusion Models (HSR-Diff) that merges a high-resolution (HR) multispectral image (MSI) with the corresponding LR-HSI. HSR-Diff generates an HR-HSI via repeated refinement, in which the HR-HSI is initialized with pure Gaussian noise and iteratively refined. At each iteration, the noise is removed with a Conditional Denoising Transformer (CDFormer) that is trained on denoising at different noise levels, conditioned on the hierarchical feature maps of HR-MSI and LR-HSI. In addition, a progressive learning strategy is employed to exploit the global information of full-resolution images. Systematic experiments have been conducted on four public datasets, demonstrating that HSR-Diff outperforms state-of-the-art methods",
    "checked": true,
    "id": "dc737074e25f91005367babd10e806246a17bd11",
    "semantic_title": "hsr-diff: hyperspectral image super-resolution via conditional diffusion models",
    "citation_count": 1,
    "authors": [
      "Chanyue Wu",
      "Dong Wang",
      "Yunpeng Bai",
      "Hanyu Mao",
      "Ying Li",
      "Qiang Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shapovalov_Replay_Multi-modal_Multi-view_Acted_Videos_for_Casual_Holography_ICCV_2023_paper.html": {
    "title": "Replay: Multi-modal Multi-view Acted Videos for Casual Holography",
    "volume": "main",
    "abstract": "We introduce Replay, a collection of multi-view, multi-modal videos of humans interacting socially. Each scene is filmed in high production quality, from different viewpoints with several static cameras, as well as wearable action cameras, and recorded with a large array of microphones at different positions in the room. Overall, the dataset contains over 3000 minutes of footage and over 5 million timestamped high-resolution frames annotated with camera poses and partially with foreground masks. The Replay dataset has many potential applications, such as novel-view synthesis, 3D reconstruction, novel-view acoustic synthesis, human body and face analysis, and training generative models. We provide a benchmark for training and evaluating novel-view synthesis, with two scenarios of different difficulty. Finally, we evaluate several baseline state-of-the-art methods on the new benchmark",
    "checked": true,
    "id": "49be716e433f47cd040c1a0e159ec5ec95bf71a0",
    "semantic_title": "replay: multi-modal multi-view acted videos for casual holography",
    "citation_count": 0,
    "authors": [
      "Roman Shapovalov",
      "Yanir Kleiman",
      "Ignacio Rocco",
      "David Novotny",
      "Andrea Vedaldi",
      "Changan Chen",
      "Filippos Kokkinos",
      "Ben Graham",
      "Natalia Neverova"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ge_Advancing_Example_Exploitation_Can_Alleviate_Critical_Challenges_in_Adversarial_Training_ICCV_2023_paper.html": {
    "title": "Advancing Example Exploitation Can Alleviate Critical Challenges in Adversarial Training",
    "volume": "main",
    "abstract": "Deep neural networks have achieved remarkable results across various tasks. However, they are susceptible to adversarial examples, which are generated by adding adversarial perturbations to original data. Adversarial training (AT) is the most effective defense mechanism against adversarial examples and has received significant attention. Recent studies highlight the importance of example exploitation, where the model's learning intensity is altered for specific examples to extend classic AT approaches. However, the analysis methodologies employed by these studies are varied and contradictory, which may lead to confusion in future research. To address this issue, we provide a comprehensive summary of representative strategies focusing on exploiting examples within a unified framework. Furthermore, we investigate the role of examples in AT and find that examples which contribute primarily to accuracy or robustness are distinct. Based on this finding, we propose a novel example-exploitation idea that can further improve the performance of advanced AT methods. This new idea suggests that critical challenges in AT, such as the accuracy-robustness trade-off, robust overfitting, and catastrophic overfitting, can be alleviated simultaneously from an example-exploitation perspective. The code can be found in https://github.com/geyao1995/advancing-example-exploitation-in-adversarial-training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Ge",
      "Yun Li",
      "Keji Han",
      "Junyi Zhu",
      "Xianzhong Long"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Affine-Consistent_Transformer_for_Multi-Class_Cell_Nuclei_Detection_ICCV_2023_paper.html": {
    "title": "Affine-Consistent Transformer for Multi-Class Cell Nuclei Detection",
    "volume": "main",
    "abstract": "Multi-class cell nuclei detection is a fundamental prerequisite in the diagnosis of histopathology. It is critical to efficiently locate and identify cells with diverse morphology and distributions in digital pathological images. Most existing methods take complex intermediate representations as learning targets and rely on inflexible post-refinements while paying less attention to various cell density and fields of view. In this paper, we propose a novel Affine-Consistent Transformer (AC-Former), which directly yields a sequence of nucleus positions and is trained collaboratively through two sub-networks, a global and a local network. The local branch learns to infer distorted input images of smaller scales while the global network outputs the large-scale predictions as extra supervision signals. We further introduce an Adaptive Affine Transformer (AAT) module, which can automatically learn the key spatial transformations to warp original images for local network training. The AAT module works by learning to capture the transformed image regions that are more valuable for training the model. Experimental results demonstrate that the proposed method significantly outperforms existing state-of-the-art algorithms on various benchmarks",
    "checked": true,
    "id": "b7146afa580524fbf3b7dbfe0ec43aab4ea17ed2",
    "semantic_title": "affine-consistent transformer for multi-class cell nuclei detection",
    "citation_count": 0,
    "authors": [
      "Junjia Huang",
      "Haofeng Li",
      "Xiang Wan",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lu_Removing_Anomalies_as_Noises_for_Industrial_Defect_Localization_ICCV_2023_paper.html": {
    "title": "Removing Anomalies as Noises for Industrial Defect Localization",
    "volume": "main",
    "abstract": "Unsupervised anomaly detection aims to train models with only anomaly-free images to detect and localize unseen anomalies. Previous reconstruction-based methods have been limited by inaccurate reconstruction results. This work presents a denoising model to detect and localize the anomalies with a generative diffusion model. In particular, we introduce random noise to overwhelm the anomalous pixels and obtain pixel-wise precise anomaly scores from the intermediate denoising process. We find that the KL divergence of the diffusion model serves as a better anomaly score compared with the traditional RGB space score. Furthermore, we reconstruct the features from a pre-trained deep feature extractor as our feature level score to improve localization performance. Moreover, we propose a gradient denoising process to smoothly transform an anomalous image into a normal one. Our denoising model outperforms the state-of-the-art reconstruction-based anomaly detection methods for precise anomaly localization and high-quality normal image reconstruction on the MVTec-AD benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanbin Lu",
      "Xufeng Yao",
      "Chi-Wing Fu",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fu_GPGait_Generalized_Pose-based_Gait_Recognition_ICCV_2023_paper.html": {
    "title": "GPGait: Generalized Pose-based Gait Recognition",
    "volume": "main",
    "abstract": "Recent works on pose-based gait recognition have demonstrated the potential of using such simple information to achieve results comparable to silhouette-based methods. However, the generalization ability of pose-based methods on different datasets is undesirably inferior to that of silhouette-based ones, which has received little attention but hinders the application of these methods in real-world scenarios. To improve the generalization ability of pose-based methods across datasets, we propose a Generalized Pose-based Gait recognition (GPGait) framework. First, a Human-Oriented Transformation (HOT) and a series of Human-Oriented Descriptors (HOD) are proposed to obtain a unified pose representation with discriminative multi-features. Then, given the slight variations in the unified representation after HOT and HOD, it becomes crucial for the network to extract local-global relationships between the keypoints. To this end, a Part-Aware Graph Convolutional Network (PAGCN) is proposed to enable efficient graph partition and local-global spatial feature extraction. Experiments on four public gait recognition datasets, CASIA-B, OUMVLP-Pose, Gait3D and GREW, show that our model demonstrates better and more stable cross-domain capabilities compared to existing skeleton-based methods, achieving comparable recognition results to silhouette-based ones. Code is available at https://github.com/BNU-IVC/FastPoseGait",
    "checked": true,
    "id": "7c9d7342300a3ec72788c7617099c4d469b9f353",
    "semantic_title": "gpgait: generalized pose-based gait recognition",
    "citation_count": 3,
    "authors": [
      "Yang Fu",
      "Shibei Meng",
      "Saihui Hou",
      "Xuecai Hu",
      "Yongzhen Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Stable_and_Causal_Inference_for_Discriminative_Self-supervised_Deep_Visual_Representations_ICCV_2023_paper.html": {
    "title": "Stable and Causal Inference for Discriminative Self-supervised Deep Visual Representations",
    "volume": "main",
    "abstract": "In recent years, discriminative self-supervised methods have made significant strides in advancing various visual tasks. The central idea of learning a data encoder that is robust to data distortions/augmentations is straightforward yet highly effective. Although many studies have demonstrated the empirical success of various learning methods, the resulting learned representations can exhibit instability and hinder downstream performance. In this study, we analyze discriminative self-supervised methods from a causal perspective to explain these unstable behaviors and propose solutions to overcome them. Our approach draws inspiration from prior works that empirically demonstrate the ability of discriminative self-supervised methods to demix ground truth causal sources to some extent. Unlike previous work on causality-empowered representation learning, we do not apply our solutions during the training process but rather during the inference process to improve time efficiency. Through experiments on both controlled image datasets and realistic image datasets, we show that our proposed solutions, which involve tempering a linear transformation with controlled synthetic data, are effective in addressing these issues",
    "checked": true,
    "id": "3296c62457d13efbfa23634b87c69d7c36b93bfc",
    "semantic_title": "stable and causal inference for discriminative self-supervised deep visual representations",
    "citation_count": 0,
    "authors": [
      "Yuewei Yang",
      "Hai Li",
      "Yiran Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_ShiftNAS_Improving_One-shot_NAS_via_Probability_Shift_ICCV_2023_paper.html": {
    "title": "ShiftNAS: Improving One-shot NAS via Probability Shift",
    "volume": "main",
    "abstract": "One-shot Neural architecture search (One-shot NAS) has been proposed as a time-efficient approach to obtain optimal subnet architectures and weights under different complexity cases by training only once. However, the subnet performance obtained by weight sharing is often inferior to the performance achieved by retraining. In this paper, we investigate the performance gap and attribute it to the use of uniform sampling, which is a common approach in supernet training. Uniform sampling concentrates training resources on subnets with intermediate computational resources, which are sampled with high probability. However, subnets with different complexity regions require different optimal training strategies for optimal performance. To address the problem of uniform sampling, we propose ShiftNAS, a method that can adjust the sampling probability based on the complexity of subnets. We achieve this by evaluating the performance variation of subnets with different complexity and designing an architecture generator that can accurately and efficiently provide subnets with the desired complexity. Both the sampling probability and the architecture generator can be trained end-to-end in a gradient-based manner. With ShiftNAS, we can directly obtain the optimal model architecture and parameters for a given computational complexity. We evaluate our approach on multiple visual network models, including convolutional neural networks (CNNs) and vision transformers (ViTs), and demonstrate that ShiftNAS is model-agnostic. Experimental results on ImageNet show that ShiftNAS can improve the performance of one-shot NAS without additional computational consumption. Source codes are available at GitHub",
    "checked": true,
    "id": "f6a70b2bb4506d05d46e6b17001849f1aa395c0f",
    "semantic_title": "shiftnas: improving one-shot nas via probability shift",
    "citation_count": 0,
    "authors": [
      "Mingyang Zhang",
      "Xinyi Yu",
      "Haodong Zhao",
      "Linlin Ou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liang_Semantic_Attention_Flow_Fields_for_Monocular_Dynamic_Scene_Decomposition_ICCV_2023_paper.html": {
    "title": "Semantic Attention Flow Fields for Monocular Dynamic Scene Decomposition",
    "volume": "main",
    "abstract": "From video, we reconstruct a neural volume that captures time-varying color, density, scene flow, semantics, and attention information. The semantics and attention let us identify salient foreground objects separately from the background across spacetime. To mitigate low resolution semantic and attention features, we compute pyramids that trade detail with whole-image context. After optimization, we perform a saliency-aware clustering to decompose the scene. To evaluate real-world scenes, we annotate object masks in the NVIDIA Dynamic Scene and DyCheck datasets. We demonstrate that this method can decompose dynamic scenes in an unsupervised way with competitive performance to a supervised method, and that it improves foreground/background segmentation over recent static/dynamic split methods. Project webpage: https://visual.cs.brown.edu/saff",
    "checked": true,
    "id": "3a134bc50da5a592380d4b88bcfbcf6067374797",
    "semantic_title": "semantic attention flow fields for monocular dynamic scene decomposition",
    "citation_count": 1,
    "authors": [
      "Yiqing Liang",
      "Eliot Laidlaw",
      "Alexander Meyerowitz",
      "Srinath Sridhar",
      "James Tompkin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Luo_LexLIP_Lexicon-Bottlenecked_Language-Image_Pre-Training_for_Large-Scale_Image-Text_Sparse_Retrieval_ICCV_2023_paper.html": {
    "title": "LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Sparse Retrieval",
    "volume": "main",
    "abstract": "Image-text retrieval (ITR) aims to retrieve images or texts that match a query originating from the other modality. The conventional dense retrieval paradigm relies on encoding images and texts into dense representations with dual-stream encoders. However, this approach is limited by slow retrieval speeds in large-scale scenarios. To address this issue, we propose a novel sparse retrieval paradigm for ITR that exploits sparse representations in the vocabulary space for images and texts. This paradigm enables us to leverage bag-of-words models and efficient inverted indexes, significantly reducing retrieval latency. A critical gap emerges from representing continuous image data in a sparse vocabulary space. To bridge this gap, we introduce a novel pre-training framework, Lexicon-Bottlenecked Language-Image Pre-Training (LexLIP), that learns importance-aware lexicon representations. By using lexicon-bottlenecked modules between the dual-stream encoders and weakened text decoders, we are able to construct continuous bag-of-words bottlenecks and learn lexicon-importance distributions. Upon pre-training with same-scale data, our LexLIP achieves state-of-the-art performance on two ITR benchmarks, MSCOCO and Flickr30k. Furthermore, in large-scale retrieval scenarios, LexLIP outperforms CLIP with 5.8x faster retrieval speed and 19.1x less index storage memory. Beyond this, LexLIP surpasses CLIP across 8 out of 10 zero-shot image classification tasks",
    "checked": false,
    "id": "f59ca53eec8f8c379f70e035b7f2c124d23c3dfa",
    "semantic_title": "lexlip: lexicon-bottlenecked language-image pre-training for large-scale image-text retrieval",
    "citation_count": 0,
    "authors": [
      "Ziyang Luo",
      "Pu Zhao",
      "Can Xu",
      "Xiubo Geng",
      "Tao Shen",
      "Chongyang Tao",
      "Jing Ma",
      "Qingwei Lin",
      "Daxin Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Heitzinger_A_Fast_Unified_System_for_3D_Object_Detection_and_Tracking_ICCV_2023_paper.html": {
    "title": "A Fast Unified System for 3D Object Detection and Tracking",
    "volume": "main",
    "abstract": "We present FUS3D, a fast and lightweight system for real-time 3D object detection and tracking on edge devices. Our approach seamlessly integrates stages for 3D object detection and multi-object-tracking into a single, end-to-end trainable model. FUS3D is specially tuned for indoor 3D human behavior analysis, with target applications in Ambient Assisted Living (AAL) or surveillance. The system is optimized for inference on the edge, thus enabling sensor-near processing of potentially sensitive data. In addition, our system relies exclusively on the less privacy-intrusive 3D depth imaging modality, thus further highlighting the potential of our method for application in sensitive areas. FUS3D achieves best results when utilized in a joint detection and tracking configuration. Nevertheless, the proposed detection stage can function as a fast standalone object detection model if required. We have evaluated FUS3D extensively on the MIPT dataset and demonstrated its superior performance over comparable existing state-of-the-art methods in terms of 3D object detection, multi-object tracking, and most importantly, runtime. Model code will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Heitzinger",
      "Martin Kampel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Adaptive_Testing_of_Computer_Vision_Models_ICCV_2023_paper.html": {
    "title": "Adaptive Testing of Computer Vision Models",
    "volume": "main",
    "abstract": "Vision models often fail systematically on groups of data that share common semantic characteristics (e.g., rare objects or unusual scenes), but identifying these failure modes is a challenge. We introduce AdaVision, an interactive process for testing vision models which helps users identify and fix coherent failure modes. Given a natural language description of a coherent group, AdaVision retrieves relevant images from LAION-5B with CLIP. The user then labels a small amount of data for model correctness, which is used in successive retrieval rounds to hill-climb towards high-error regions, refining the group definition. Once a group is saturated, AdaVision uses GPT-3 to suggest new group descriptions for the user to explore. We demonstrate the usefulness and generality of AdaVision in user studies, where users find major bugs in state-of-the-art classification, object detection, and image captioning models. These user-discovered groups have failure rates 2-3x higher than those surfaced by automatic error clustering methods. Finally, finetuning on examples found with AdaVision fixes the discovered bugs when evaluated on unseen examples, without degrading in-distribution accuracy, and while also improving performance on out-of-distribution datasets",
    "checked": true,
    "id": "841f5c091ed8491d9fd50cf124de7c67d500bdb8",
    "semantic_title": "adaptive testing of computer vision models",
    "citation_count": 10,
    "authors": [
      "Irena Gao",
      "Gabriel Ilharco",
      "Scott Lundberg",
      "Marco Tulio Ribeiro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Seo_LFS-GAN_Lifelong_Few-Shot_Image_Generation_ICCV_2023_paper.html": {
    "title": "LFS-GAN: Lifelong Few-Shot Image Generation",
    "volume": "main",
    "abstract": "We address a challenging lifelong few-shot image generation task for the first time. In this situation, a generative model learns a sequence of tasks using only a few samples per task. Consequently, the learned model encounters both catastrophic forgetting and overfitting problems at a time. Existing studies on lifelong GANs have proposed modulation-based methods to prevent catastrophic forgetting. However, they require considerable additional parameters and cannot generate high-fidelity and diverse images from limited data. On the other hand, the existing few-shot GANs suffer from severe catastrophic forgetting when learning multiple tasks. To alleviate these issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can generate high-quality and diverse images in lifelong few-shot image generation task. Our proposed framework learns each task using an efficient task-specific modulator - Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and has a rich representation ability due to its unique reconstruction technique. Furthermore, we propose a novel mode seeking loss to improve the diversity of our model in low-data circumstances. Extensive experiments demonstrate that the proposed LFS-GAN can generate high-fidelity and diverse images without any forgetting and mode collapse in various domains, achieving state-of-the-art in lifelong few-shot image generation task. Surprisingly, we find that our LFS-GAN even outperforms the existing few-shot GANs in the few-shot image generation task. The code is available at Github",
    "checked": true,
    "id": "6569899d8c3b812f83250c8b172e519910ec7303",
    "semantic_title": "lfs-gan: lifelong few-shot image generation",
    "citation_count": 0,
    "authors": [
      "Juwon Seo",
      "Ji-Su Kang",
      "Gyeong-Moon Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_AIDE_A_Vision-Driven_Multi-View_Multi-Modal_Multi-Tasking_Dataset_for_Assistive_Driving_ICCV_2023_paper.html": {
    "title": "AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception",
    "volume": "main",
    "abstract": "Driver distraction has become a significant cause of severe traffic accidents over the past decade. Despite the growing development of vision-driven driver monitoring systems, the lack of comprehensive perception datasets restricts road safety and traffic security. In this paper, we present an AssIstive Driving pErception dataset (AIDE) that considers context information both inside and outside the vehicle in naturalistic scenarios. AIDE facilitates holistic driver monitoring through three distinctive characteristics, including multi-view settings of driver and scene, multi-modal annotations of face, body, posture, and gesture, and four pragmatic task designs for driving understanding. To thoroughly explore AIDE, we provide experimental benchmarks on three kinds of baseline frameworks via extensive methods. Moreover, two fusion strategies are introduced to give new insights into learning effective multi-stream/modal representations. We also systematically investigate the importance and rationality of the key components in AIDE and benchmarks. The project link is https://github.com/ydk122024/AIDE",
    "checked": true,
    "id": "59fe35493ebd810bc6dd6914781619fedd77218c",
    "semantic_title": "aide: a vision-driven multi-view, multi-modal, multi-tasking dataset for assistive driving perception",
    "citation_count": 7,
    "authors": [
      "Dingkang Yang",
      "Shuai Huang",
      "Zhi Xu",
      "Zhenpeng Li",
      "Shunli Wang",
      "Mingcheng Li",
      "Yuzheng Wang",
      "Yang Liu",
      "Kun Yang",
      "Zhaoyu Chen",
      "Yan Wang",
      "Jing Liu",
      "Peixuan Zhang",
      "Peng Zhai",
      "Lihua Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Song_Feature_Proliferation_--_the_Cancer_in_StyleGAN_and_its_Treatments_ICCV_2023_paper.html": {
    "title": "Feature Proliferation -- the \"Cancer\" in StyleGAN and its Treatments",
    "volume": "main",
    "abstract": "Despite the success of StyleGAN in image synthesis, the images it synthesizes are not always perfect and the well-known truncation trick has become a standard post-processing technique for StyleGAN to synthesize high-quality images. Although effective, it has long been noted that the truncation trick tends to reduce the diversity of synthesized images and unnecessarily sacrifices many distinct image features. To address this issue, in this paper, we first delve into the StyleGAN image synthesis mechanism and discover an important phenomenon, namely Feature Proliferation, which demonstrates how specific features reproduce with forward propagation. Then, we show how the occurrence of Feature Proliferation results in StyleGAN image artifacts. As an analogy, we refer to it as the \"cancer\" in StyleGAN from its proliferating and malignant nature. Finally, we propose a novel feature rescaling method that identifies and modulates risky features to mitigate feature proliferation. Thanks to our discovery of Feature Proliferation, the proposed feature rescaling method is less destructive and retains more useful image features than the truncation trick, as it is more fine-grained and works in a lower-level feature space rather than a high-level latent space. Experimental results justify the validity of our claims and the effectiveness of the proposed feature rescaling method. Our code is available at https://github.com/songc42/Feature-proliferation",
    "checked": false,
    "id": "c367255e0d053a2214246fca3bf71e6e96eae508",
    "semantic_title": "feature proliferation - the \"cancer\" in stylegan and its treatments",
    "citation_count": 0,
    "authors": [
      "Shuang Song",
      "Yuanbang Liang",
      "Jing Wu",
      "Yu-Kun Lai",
      "Yipeng Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guan_Self-Supervised_Character-to-Character_Distillation_for_Text_Recognition_ICCV_2023_paper.html": {
    "title": "Self-Supervised Character-to-Character Distillation for Text Recognition",
    "volume": "main",
    "abstract": "When handling complicated text images (e.g., irregular structures, low resolution, heavy occlusion, and uneven illumination), existing supervised text recognition methods are data-hungry. Although these methods employ large-scale synthetic text images to reduce the dependence on annotated real images, the domain gap still limits the recognition performance. Therefore, exploring the robust text feature representations on unlabeled real images by self-supervised learning is a good solution. However, existing self-supervised text recognition methods conduct sequence-to-sequence representation learning by roughly splitting the visual features along the horizontal axis, which limits the flexibility of the augmentations, as large geometric-based augmentations may lead to sequence-to-sequence feature inconsistency. Motivated by this, we propose a novel self-supervised Character-to-Character Distillation method, CCD, which enables versatile augmentations to facilitate general text representation learning. Specifically, we delineate the character structures of unlabeled real images by designing a self-supervised character segmentation module. Following this, CCD easily enriches the diversity of local characters while keeping their pairwise alignment under flexible augmentations, using the transformation matrix between two augmented views from images. Experiments demonstrate that CCD achieves state-of-the-art results, with average performance gains of 1.38% in text recognition, 1.7% in text segmentation, 0.24 dB (PSNR) and 0.0321 (SSIM) in text super-resolution. Code will be released soon",
    "checked": false,
    "id": "3ac1a5ab4c8de7cd4baa7d050dc78cfdfb323e33",
    "semantic_title": "self-supervised character-to-character distillation",
    "citation_count": 2,
    "authors": [
      "Tongkun Guan",
      "Wei Shen",
      "Xue Yang",
      "Qi Feng",
      "Zekun Jiang",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_MixCycle_Mixup_Assisted_Semi-Supervised_3D_Single_Object_Tracking_with_Cycle_ICCV_2023_paper.html": {
    "title": "MixCycle: Mixup Assisted Semi-Supervised 3D Single Object Tracking with Cycle Consistency",
    "volume": "main",
    "abstract": "3D single object tracking (SOT) is an indispensable part of automated driving. Existing approaches rely heavily on large, densely labeled datasets. However, annotating point clouds is both costly and time-consuming. Inspired by the great success of cycle tracking in unsupervised 2D SOT, we introduce the first semi-supervised approach to 3D SOT. Specifically, we introduce two cycle-consistency strategies for supervision: 1) Self tracking cycles, which leverage labels to help the model converge better in the early stages of training; 2) forward-backward cycles, which strengthen the tracker's robustness to motion variations and the template noise caused by the template update strategy. Furthermore, we propose a data augmentation strategy named SOTMixup to improve the tracker's robustness to point cloud diversity. SOTMixup generates training samples by sampling points in two point clouds with a mixing rate and assigns a reasonable loss weight for training according to the mixing rate. The resulting MixCycle approach generalizes to appearance matching-based trackers. On the KITTI benchmark, based on the P2B tracker, MixCycle trained with 10% labels outperforms P2B trained with 100% labels, and achieves a 28.4% precision improvement when using 1% labels. Our code will be released at https://github.com/Mumuqiao/MixCycle",
    "checked": true,
    "id": "3ab3ebf863d855f112c257bfd0957dfd556e3120",
    "semantic_title": "mixcycle: mixup assisted semi-supervised 3d single object tracking with cycle consistency",
    "citation_count": 0,
    "authors": [
      "Qiao Wu",
      "Jiaqi Yang",
      "Kun Sun",
      "Chu'ai Zhang",
      "Yanning Zhang",
      "Mathieu Salzmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Multi-Label_Self-Supervised_Learning_with_Scene_Images_ICCV_2023_paper.html": {
    "title": "Multi-Label Self-Supervised Learning with Scene Images",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) methods targeting scene images have seen a rapid growth recently, and they mostly rely on either a dedicated dense matching mechanism or a costly unsupervised object discovery module. This paper shows that instead of hinging on these strenuous operations, quality image representations can be learned by treating scene/multi-label image SSL simply as a multi-label classification problem, which greatly simplifies the learning framework. Specifically, multiple binary pseudo-labels are assigned for each input image by comparing its embeddings with those in two dictionaries, and the network is optimized using the binary cross entropy loss. The proposed method is named Multi-Label Self-supervised learning (MLS). Visualizations qualitatively show that clearly the pseudo-labels by MLS can automatically find semantically similar pseudo-positive pairs across different images to facilitate contrastive learning. MLS learns high quality representations on MS-COCO and achieves state-of-the-art results on classification, detection and segmentation benchmarks. At the same time, MLS is much simpler than existing methods, making it easier to deploy and for further exploration",
    "checked": true,
    "id": "90653b71fb69ce5a2e128e01e77a990aad29ad24",
    "semantic_title": "multi-label self-supervised learning with scene images",
    "citation_count": 1,
    "authors": [
      "Ke Zhu",
      "Minghao Fu",
      "Jianxin Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pal_Domain_Adaptive_Few-Shot_Open-Set_Learning_ICCV_2023_paper.html": {
    "title": "Domain Adaptive Few-Shot Open-Set Learning",
    "volume": "main",
    "abstract": "Few-shot learning has made impressive strides in addressing the crucial challenges of recognizing unknown samples from novel classes in target query sets and managing visual shifts between domains. However, existing techniques fall short when it comes to identifying target outliers under domain shifts by learning to reject pseudo-outliers from the source domain, resulting in an incomplete solution to both problems. To address these challenges comprehensively, we propose a novel approach called Domain Adaptive Few-Shot Open Set Recognition (DA-FSOS) and introduce a meta-learning-based architecture named DAFOS-Net. During training, our model learns a shared and discriminative embedding space while creating a pseudo-open-space decision boundary, given a fully-supervised source domain and a label-disjoint few-shot target domain. To enhance data density, we use a pair of conditional adversarial networks with tunable noise variances to augment both domains' closed and pseudo-open spaces. Furthermore, we propose a domain-specific batch-normalized class prototypes alignment strategy to align both domains globally while ensuring class-discriminativeness through novel metric objectives. Our training approach ensures that DAFOS-Net can generalize well to new scenarios in the target domain. We present three benchmarks for DA-FSOS based on the Office-Home, mini-ImageNet/CUB, and DomainNet datasets and demonstrate the efficacy of DAFOS-Net through extensive experimentation",
    "checked": true,
    "id": "a7f5334b9dcee89408f633c8f91c94defa5bc4ac",
    "semantic_title": "domain adaptive few-shot open-set learning",
    "citation_count": 0,
    "authors": [
      "Debabrata Pal",
      "Deeptej More",
      "Sai Bhargav",
      "Dipesh Tamboli",
      "Vaneet Aggarwal",
      "Biplab Banerjee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nakayama_DiffFacto_Controllable_Part-Based_3D_Point_Cloud_Generation_with_Cross_Diffusion_ICCV_2023_paper.html": {
    "title": "DiffFacto: Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion",
    "volume": "main",
    "abstract": "While the community of 3D point cloud generation has witnessed a big growth in recent years, there still lacks an effective way to enable intuitive user control in the generation process, hence limiting the general utility of such methods. Since an intuitive way of decomposing a shape is through its parts, we propose to tackle the task of controllable part-based point cloud generation. We introduce DiffFacto, a novel probabilistic generative model that learns the distribution of shapes with part-level control. We propose a factorization that models independent part style and part configuration distributions, and present a novel cross diffusion network that enables us to generate coherent and plausible shapes under our proposed factorization. Experiments show that our method is able to generate novel shapes with multiple axes of control. It achieves state-of-the-art part-level generation quality and generates plausible and coherent shape, while enabling various downstream editing applications such as shape interpolation, mixing and transformation editing. Code will be made publicly available",
    "checked": true,
    "id": "7b9646b33c96fb43c51c979637b10651aff97d87",
    "semantic_title": "difffacto: controllable part-based 3d point cloud generation with cross diffusion",
    "citation_count": 1,
    "authors": [
      "George Kiyohiro Nakayama",
      "Mikaela Angelina Uy",
      "Jiahui Huang",
      "Shi-Min Hu",
      "Ke Li",
      "Leonidas Guibas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Interactive_Class-Agnostic_Object_Counting_ICCV_2023_paper.html": {
    "title": "Interactive Class-Agnostic Object Counting",
    "volume": "main",
    "abstract": "We propose a novel framework for interactive class-agnostic object counting, where a human user can interactively provide feedback to improve the accuracy of a counter. Our framework consists of two main components: a user-friendly visualizer to gather feedback and an efficient mechanism to incorporate it. In each iteration, we produce a density map to show the current prediction result, and we segment it into non-overlapping regions with an easily verifiable number of objects. The user can provide feedback by selecting a region with obvious counting errors and specifying the range for the estimated number of objects within it. To improve the counting result, we develop a novel adaptation loss to force the visual counter to output the predicted count within the user-specified range. For effective and efficient adaptation, we propose a refinement module that can be used with any density-based visual counter, and only the parameters in the refinement module will be updated during adaptation. Our experiments on two challenging class-agnostic object counting benchmarks, FSCD-LVIS and FSC-147, show that our method can reduce the mean absolute error of multiple state-of-the-art visual counters by roughly 30% to 40% with minimal user input. Our project can be found at https://yifehuang97.github.io/ICACountProjectPage/",
    "checked": true,
    "id": "988fc3ffba8545bc735a9651f41d416e099981fd",
    "semantic_title": "interactive class-agnostic object counting",
    "citation_count": 0,
    "authors": [
      "Yifeng Huang",
      "Viresh Ranjan",
      "Minh Hoai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Spatio-temporal_Prompting_Network_for_Robust_Video_Feature_Extraction_ICCV_2023_paper.html": {
    "title": "Spatio-temporal Prompting Network for Robust Video Feature Extraction",
    "volume": "main",
    "abstract": "The frame quality deterioration problem is one of the main challenges in the field of video understanding. To compensate for the information loss caused by deteriorated frames, recent approaches exploit transformer-based integration modules to obtain spatio-temporal information. However, these integration modules are heavy and complex. Furthermore, each integration module is specifically tailored for its target task, making it difficult to generalise to multiple tasks. In this paper, we present a neat and unified framework, called Spatio-Temporal Prompting Network (STPN). It can efficiently extract robust and accurate video features by dynamically adjusting the input features in the backbone network. Specifically, STPN predicts several video prompts containing spatio-temporal information of neighbour frames. Then, these video prompts are prepended to the patch embeddings of the current frame as the updated input for video feature extraction. Moreover, STPN is easy to generalise to various video tasks because it does not contain task-specific modules. Without bells and whistles, STPN achieves state-of-the-art performance on three widely-used datasets for different video understanding tasks, i.e., ImageNetVID for video object detection, YouTubeVIS for video instance segmentation, and GOT-10k for visual object tracking. Codes are available at https://github.com/guanxiongsun/STPN",
    "checked": false,
    "id": "f58e5c12d219bbf991984de6b07c2ad2b07fb57d",
    "semantic_title": "modeling human skeleton joint dynamics for fall detection",
    "citation_count": 1,
    "authors": [
      "Guanxiong Sun",
      "Chi Wang",
      "Zhaoyu Zhang",
      "Jiankang Deng",
      "Stefanos Zafeiriou",
      "Yang Hua"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Enhancing_Fine-Tuning_Based_Backdoor_Defense_with_Sharpness-Aware_Minimization_ICCV_2023_paper.html": {
    "title": "Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization",
    "volume": "main",
    "abstract": "Backdoor defense, which aims to detect or mitigate the effect of malicious triggers introduced by attackers, is becoming increasingly critical for machine learning security and integrity. Fine-tuning based on benign data is a natural defense to erase the backdoor effect in a backdoored model. However, recent studies show that, given limited benign data, vanilla fine-tuning has poor defense performance. In this work, we firstly investigate the vanilla fine-tuning process for backdoor mitigation from the neuron weight perspective, and find that backdoor-related neurons are only slightly perturbed in the vanilla fine-tuning process, which explains its poor backdoor defense performance. To enhance the fine-tuning based defense, inspired by the observation that the backdoor-related neurons often have larger weight norms, we propose FT-SAM, a novel backdoor defense paradigm that aims to shrink the norms of backdoorrelated neurons by incorporating sharpness-aware minimization with fine-tuning. We demonstrate the effectiveness of our method on several benchmark datasets and network architectures, where it achieves state-of-the-art defense performance, and provide extensive analysis to reveal the FTSAM's mechanism. Overall, our work provides a promising avenue for improving the robustness of machine learning models against backdoor attacks. Codes are available at https://github.com/SCLBD/BackdoorBench",
    "checked": true,
    "id": "16903a59a6a4b2170899b76a96edac64b4be3369",
    "semantic_title": "enhancing fine-tuning based backdoor defense with sharpness-aware minimization",
    "citation_count": 5,
    "authors": [
      "Mingli Zhu",
      "Shaokui Wei",
      "Li Shen",
      "Yanbo Fan",
      "Baoyuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hagemann_Deep_Geometry-Aware_Camera_Self-Calibration_from_Video_ICCV_2023_paper.html": {
    "title": "Deep Geometry-Aware Camera Self-Calibration from Video",
    "volume": "main",
    "abstract": "Accurate intrinsic calibration is essential for camera-based 3D perception, yet, it typically requires targets of well-known geometry. Here, we propose a camera self-calibration approach that infers camera intrinsics during application, from monocular videos in the wild. We propose to explicitly model projection functions and multi-view geometry, while leveraging the capabilities of deep neural networks for feature extraction and matching. To achieve this, we build upon recent research on integrating bundle adjustment into deep learning models, and introduce a self-calibrating bundle adjustment layer. The self-calibrating bundle adjustment layer optimizes camera intrinsics through classical Gauss-Newton steps and can be adapted to different camera models without re-training. As a specific realization, we implemented this layer within the deep visual SLAM system DROID-SLAM, and show that the resulting model, DroidCalib, yields state-of-the-art calibration accuracy across multiple public datasets. Our results suggest that the model generalizes to unseen environments and different camera models, including significant lens distortion. Thereby, the approach enables performing 3D perception tasks without prior knowledge about the camera. Code is available at https://github.com/boschresearch/droidcalib",
    "checked": false,
    "id": "0920663b452b6eccf1d0ffc24c3fc5f53f66edee",
    "semantic_title": "self-supervised camera self-calibration from video",
    "citation_count": 11,
    "authors": [
      "Annika Hagemann",
      "Moritz Knorr",
      "Christoph Stiller"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_A_Simple_Vision_Transformer_for_Weakly_Semi-supervised_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "A Simple Vision Transformer for Weakly Semi-supervised 3D Object Detection",
    "volume": "main",
    "abstract": "Advanced 3D object detection methods usually rely on large-scale, elaborately labeled datasets to achieve good performance. However, labeling the bounding boxes for the 3D objects is difficult and expensive. Although semi-supervised (SS3D) and weakly-supervised 3D object detection (WS3D) methods can effectively reduce the annotation cost, they suffer from two limitations: 1) their performance is far inferior to the fully-supervised counterparts; 2) they are difficult to adapt to different detectors or scenes (e.g, indoor or outdoor). In this paper, we study weakly semi-supervised 3D object detection (WSS3D) with point annotations, where the dataset comprises a small number of fully labeled and massive weakly labeled data with a single point annotated for each 3D object. To fully exploit the point annotations, we employ the plain and non-hierarchical vision transformer to form a point-to-box converter, termed ViT-WSS3D. By modeling global interactions between LiDAR points and corresponding weak labels, our ViT-WSS3D can generate high-quality pseudo-bounding boxes, which are then used to train any 3D detectors without exhaustive tuning. Extensive experiments on indoor and outdoor datasets (SUN RGBD and KITTI) show the effectiveness of our method. In particular, when only using 10% fully labeled and the rest as point labeled data, our ViT-WSS3D can enable most detectors to achieve similar performance with the oracle model using 100% fully labeled data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingyuan Zhang",
      "Dingkang Liang",
      "Zhikang Zou",
      "Jingyu Li",
      "Xiaoqing Ye",
      "Zhe Liu",
      "Xiao Tan",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Estimator_Meets_Equilibrium_Perspective_A_Rectified_Straight_Through_Estimator_for_ICCV_2023_paper.html": {
    "title": "Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training",
    "volume": "main",
    "abstract": "Binarization of neural networks is a dominant paradigm in neural networks compression. The pioneering work BinaryConnect uses Straight Through Estimator (STE) to mimic the gradients of the sign function, but it also causes the crucial inconsistency problem. Most of the previous methods design different estimators instead of STE to mitigate it. However, they ignore the fact that when reducing the estimating error, the gradient stability will decrease concomitantly. These highly divergent gradients will harm the model training and increase the risk of gradient vanishing and gradient exploding. To fully take the gradient stability into consideration, we present a new perspective to the BNNs training, regarding it as the equilibrium between the estimating error and the gradient stability. In this view, we firstly design two indicators to quantitatively demonstrate the equilibrium phenomenon. In addition, in order to balance the estimating error and the gradient stability well, we revise the original straight through estimator and propose a power function based estimator, Rectified Straight Through Estimator (ReSTE for short). Comparing to other estimators, ReSTE is rational and capable of flexibly balancing the estimating error with the gradient stability. Extensive experiments on CIFAR-10 and ImageNet datasets show that ReSTE has excellent performance and surpasses the state-of-the-art methods without any auxiliary modules or losses",
    "checked": true,
    "id": "6ea40abfa887709e2b8da79c35ec75e4003291fa",
    "semantic_title": "estimator meets equilibrium perspective: a rectified straight through estimator for binary neural networks training",
    "citation_count": 0,
    "authors": [
      "Xiao-Ming Wu",
      "Dian Zheng",
      "Zuhao Liu",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Exploring_Object-Centric_Temporal_Modeling_for_Efficient_Multi-View_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection",
    "volume": "main",
    "abstract": "In this paper, we propose a long-sequence modeling framework, named StreamPETR, for multi-view 3D object detection. Built upon the sparse query design in the PETR series, we systematically develop an object-centric temporal mechanism. The model is performed in an online manner and the long-term historical information is propagated through object queries frame by frame. Besides, we introduce a motion-aware layer normalization to model the movement of the objects. StreamPETR achieves significant performance improvements only with negligible computation cost, compared to the single-frame baseline. On the standard nuScenes benchmark, it is the first online multi-view method that achieves comparable performance (67.6% NDS & 65.3% AMOTA) with lidar-based methods. The lightweight version realizes 45.0% mAP and 31.7 FPS, outperforming the state-of-the-art method (SOLOFusion) by 2.3% mAP and 1.8x faster FPS. Code has been available at https://github.com/exiawsh/StreamPETR.git",
    "checked": true,
    "id": "73385e2aac9890073669759dfde8800b5704ab6e",
    "semantic_title": "exploring object-centric temporal modeling for efficient multi-view 3d object detection",
    "citation_count": 12,
    "authors": [
      "Shihao Wang",
      "Yingfei Liu",
      "Tiancai Wang",
      "Ying Li",
      "Xiangyu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Open-domain_Visual_Entity_Recognition_Towards_Recognizing_Millions_of_Wikipedia_Entities_ICCV_2023_paper.html": {
    "title": "Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities",
    "volume": "main",
    "abstract": "Large-scale multi-modal pre-training models such as CLIP and PaLI exhibit strong generalization on various visual domains and tasks. However, existing image classification benchmarks often evaluate recognition on a specific domain (e.g., outdoor images) or a specific task (e.g., classifying plant species), which falls short of evaluating whether pre-trained foundational models are universal visual recognizers. To address this, we formally present the task of Open-domain Visual Entity recognitioN (OVEN), where a model need to link an image onto a Wikipedia entity with respect to a text query. We construct OVEN by re-purposing 14 existing datasets with all labels grounded onto one single label space: Wikipedia entities. OVEN challenges models to select among six million possible Wikipedia entities, making it a general visual recognition benchmark with largest number of labels. Our study on state-of-the-art pre-trained models reveals large headroom in generalizing to the massive-scale label space. We show that a PaLI-based auto-regressive visual recognition model performs surprisingly well, even on Wikipedia entities that have never been seen during fine-tuning. We also find existing pre-trained models yield different unique strengths: while PaLI-based models obtains higher overall performance, CLIP-based models are better at recognizing tail entities",
    "checked": true,
    "id": "2bac138a23380ee1af43dae2dae38f918cb710ae",
    "semantic_title": "open-domain visual entity recognition: towards recognizing millions of wikipedia entities",
    "citation_count": 7,
    "authors": [
      "Hexiang Hu",
      "Yi Luan",
      "Yang Chen",
      "Urvashi Khandelwal",
      "Mandar Joshi",
      "Kenton Lee",
      "Kristina Toutanova",
      "Ming-Wei Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_MedKLIP_Medical_Knowledge_Enhanced_Language-Image_Pre-Training_for_X-ray_Diagnosis_ICCV_2023_paper.html": {
    "title": "MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training for X-ray Diagnosis",
    "volume": "main",
    "abstract": "In this paper, we consider enhancing medical visual-language pre-training (VLP) with domain-specific knowledge, by exploiting the paired image-text reports from the radiological daily practice. In particular, we make the following contributions: First, unlike existing works that directly process the raw reports, we adopt a novel triplet extraction module to extract the medical-related information, avoiding unnecessary complexity from language grammar and enhancing the supervision signals; Second, we propose a novel triplet encoding module with entity translation by querying a knowledge base, to exploit the rich domain knowledge in medical field, and implicitly build relationships between medical entities in the language embedding space; Third, we propose to use a Transformer-based fusion model for spatially aligning the entity description with visual signals at the image patch level, enabling the ability for medical diagnosis; Fourth, we conduct thorough experiments to validate the effectiveness of our architecture, and benchmark on numerous public benchmarks e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax, COVIDx CXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuning settings, our model has demonstrated strong performance compared with the former methods on disease classification and grounding",
    "checked": false,
    "id": "5d937b7811d8fd4208b2810971cb2e33f64bcfa2",
    "semantic_title": "knowledge-enhanced visual-language pre-training on chest radiology images",
    "citation_count": 8,
    "authors": [
      "Chaoyi Wu",
      "Xiaoman Zhang",
      "Ya Zhang",
      "Yanfeng Wang",
      "Weidi Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Automated_Knowledge_Distillation_via_Monte_Carlo_Tree_Search_ICCV_2023_paper.html": {
    "title": "Automated Knowledge Distillation via Monte Carlo Tree Search",
    "volume": "main",
    "abstract": "In this paper, we present Auto-KD, the first automated search framework for optimal knowledge distillation design. Traditional distillation techniques typically require handcrafted designs by experts and extensive tuning costs for different teacher-student pairs. To address these issues, we empirically study different distillers, finding that they can be decomposed, combined, and simplified. Based on these observations, we build our uniform search space with advanced operations in transformations, distance functions, and hyperparameters components. For instance, the transformation parts are optional for global, intra-spatial, and inter-spatial operations, such as attention, mask, and multi-scale. Then, we introduce an effective search strategy based on the Monte Carlo tree search, modeling the search space as a Monte Carlo Tree (MCT) to capture the dependency among options. The MCT is updated using test loss and representation gap of student trained by candidate distillers as the reward for better exploration-exploitation balance. To accelerate the search process, we exploit offline processing without teacher inference, sparse training for student, and proxy settings based on distillation properties. In this way, our Auto-KD only needs small costs to search for optimal distillers before the distillation phase. Moreover, we expand Auto-KD for multi-layer and multi-teacher scenarios with training-free weighted factors. Our method is promising yet practical, and extensive experiments demonstrate that it generalizes well to different CNNs and Vision Transformer models and attains state-of-the-art performance across a range of vision tasks, including image classification, object detection, and semantic segmentation. Code is provided at https://github.com/lilujunai/Auto-KD",
    "checked": false,
    "id": "1d6a55fa8d900eb2039b9b2ed08986c1513c6ff8",
    "semantic_title": "an extensible and modular design and implementation of monte carlo tree search for the jvm",
    "citation_count": 1,
    "authors": [
      "Lujun Li",
      "Peijie Dong",
      "Zimian Wei",
      "Ya Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Peng_EmoTalk_Speech-Driven_Emotional_Disentanglement_for_3D_Face_Animation_ICCV_2023_paper.html": {
    "title": "EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation",
    "volume": "main",
    "abstract": "Speech-driven 3D face animation aims to generate realistic facial expressions that match the speech content and emotion. However, existing methods often neglect emotional facial expressions or fail to disentangle them from speech content. To address this issue, this paper proposes an end-to-end neural network to disentangle different emotions in speech so as to generate rich 3D facial expressions. Specifically, we introduce the emotion disentangling encoder (EDE) to disentangle the emotion and content in the speech by cross-reconstructed speech signals with different emotion labels. Then an emotion-guided feature fusion decoder is employed to generate a 3D talking face with enhanced emotion. The decoder is driven by the disentangled identity, emotional, and content embeddings so as to generate controllable personal and emotional styles. Finally, considering the scarcity of the 3D emotional talking face data, we resort to the supervision of facial blendshapes, which enables the reconstruction of plausible 3D faces from 2D emotional data, and contribute a large-scale 3D emotional talking face dataset (3D-ETF) to train the network. Our experiments and user studies demonstrate that our approach outperforms state-of-the-art methods and exhibits more diverse facial movements. We recommend watching the supplementary video: https://ziqiaopeng. github.io/emotalk",
    "checked": true,
    "id": "25689bc978d97f7e8e50d9614e2e1e3d42550b35",
    "semantic_title": "emotalk: speech-driven emotional disentanglement for 3d face animation",
    "citation_count": 6,
    "authors": [
      "Ziqiao Peng",
      "Haoyu Wu",
      "Zhenbo Song",
      "Hao Xu",
      "Xiangyu Zhu",
      "Jun He",
      "Hongyan Liu",
      "Zhaoxin Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kang_A_Soft_Nearest-Neighbor_Framework_for_Continual_Semi-Supervised_Learning_ICCV_2023_paper.html": {
    "title": "A Soft Nearest-Neighbor Framework for Continual Semi-Supervised Learning",
    "volume": "main",
    "abstract": "Despite significant advances, the performance of state-of-the-art continual learning approaches hinges on the unrealistic scenario of fully labeled data. In this paper, we tackle this challenge and propose an approach for continual semi-supervised learning--a setting where not all the data samples are labeled. A primary issue in this scenario is the model forgetting representations of unlabeled data and overfitting the labeled samples. We leverage the power of nearest-neighbor classifiers to nonlinearly partition the feature space and flexibly model the underlying data distribution thanks to its non-parametric nature. This enables the model to learn a strong representation for the current task, and distill relevant information from previous tasks. We perform a thorough experimental evaluation and show that our method outperforms all the existing approaches by large margins, setting a solid state of the art on the continual semi-supervised learning paradigm. For example, on CIFAR-100 we surpass several others even when using at least 30 times less supervision (0.8% vs. 25% of annotations). Finally, our method works well on both low and high resolution images and scales seamlessly to more complex datasets such as ImageNet-100",
    "checked": true,
    "id": "c9b70e03b9146ca8fa1ac8fea4fca5544d1434cc",
    "semantic_title": "a soft nearest-neighbor framework for continual semi-supervised learning",
    "citation_count": 3,
    "authors": [
      "Zhiqi Kang",
      "Enrico Fini",
      "Moin Nabi",
      "Elisa Ricci",
      "Karteek Alahari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Text-Conditioned_Sampling_Framework_for_Text-to-Image_Generation_with_Masked_Generative_Models_ICCV_2023_paper.html": {
    "title": "Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models",
    "volume": "main",
    "abstract": "Token-based masked generative models are gaining popularity for their fast inference time with parallel decoding. While recent token-based approaches achieve competitive performance to diffusion-based models, their generation performance is still suboptimal as they sample multiple tokens simultaneously without considering the dependence among them. We empirically investigate this problem and propose a learnable sampling model, Text-Conditioned Token Selection (TCTS), to select optimal tokens via localized supervision with text information. TCTS improves not only the image quality but also the semantic alignment of the generated images with the given texts. To further improve the image quality, we introduce a cohesive sampling strategy, Frequency Adaptive Sampling (FAS), to each group of tokens divided according to the self-attention maps. We validate the efficacy of TCTS combined with FAS with various generative tasks, demonstrating that it significantly outperforms the baselines in image-text alignment and image quality. Our text-conditioned sampling framework further reduces the original inference time by more than 50% without modifying the original generative model",
    "checked": true,
    "id": "4d99b3f18d4f72b30ba88591b08e96f82d264bb1",
    "semantic_title": "text-conditioned sampling framework for text-to-image generation with masked generative models",
    "citation_count": 3,
    "authors": [
      "Jaewoong Lee",
      "Sangwon Jang",
      "Jaehyeong Jo",
      "Jaehong Yoon",
      "Yunji Kim",
      "Jin-Hwa Kim",
      "Jung-Woo Ha",
      "Sung Ju Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yeshwanth_ScanNet_A_High-Fidelity_Dataset_of_3D_Indoor_Scenes_ICCV_2023_paper.html": {
    "title": "ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes",
    "volume": "main",
    "abstract": "We present ScanNet++, a large-scale dataset that couples together capture of high-quality and commodity-level geometry and color of indoor scenes. Each scene is captured with a high-end laser scanner at sub-millimeter resolution, along with registered 33-megapixel images from a DSLR camera, and RGB-D streams from an iPhone. Scene reconstructions are further annotated with an open vocabulary of semantics, with label-ambiguous scenarios explicitly annotated for comprehensive semantic understanding. ScanNet++ enables a new real-world benchmark for novel view synthesis, both from high-quality RGB capture, and importantly also from commodity-level images, in addition to a new benchmark for 3D semantic scene understanding that comprehensively encapsulates diverse and ambiguous semantic labeling scenarios. Currently, ScanNet++ contains 460 scenes, 280,000 captured DSLR images, and over 3.7M iPhone RGBD frames",
    "checked": true,
    "id": "fc96c4a7d4708bfd6138bfd16482229975404499",
    "semantic_title": "scannet++: a high-fidelity dataset of 3d indoor scenes",
    "citation_count": 2,
    "authors": [
      "Chandan Yeshwanth",
      "Yueh-Cheng Liu",
      "Matthias NieÃner",
      "Angela Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nakano_Minimal_Solutions_to_Uncalibrated_Two-view_Geometry_with_Known_Epipoles_ICCV_2023_paper.html": {
    "title": "Minimal Solutions to Uncalibrated Two-view Geometry with Known Epipoles",
    "volume": "main",
    "abstract": "This paper proposes minimal solutions to uncalibrated two-view geometry with known epipoles. Exploiting the epipoles, we can reduce the number of point correspondences needed to find the fundamental matrix together with the intrinsic parameters: the focal length and the radial lens distortion. We define four cases by the number of available epipoles and unknown intrinsic parameters, then derive a closed-form solution for each case formulated as a higher-order polynomial in a single variable. The proposed solvers are more numerically stable and faster by orders of magnitude than the conventional 6- or 7-point algorithms. Moreover, we demonstrate by experiments on the human pose dataset that the proposed method can solve two-view geometry even with 2D human pose, of which point localization is noisier than general feature point detectors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaku Nakano"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jeon_Improving_Diversity_in_Zero-Shot_GAN_Adaptation_with_Semantic_Variations_ICCV_2023_paper.html": {
    "title": "Improving Diversity in Zero-Shot GAN Adaptation with Semantic Variations",
    "volume": "main",
    "abstract": "Training deep generative models usually requires a large amount of data. To alleviate the data collection cost, the task of zero-shot GAN adaptation aims to reuse well-trained generators to synthesize images of an unseen target domain without any further training samples. Due to the data absence, the textual description of the target domain and the vision-language models, e.g., CLIP, are utilized to effectively guide the generator. However, with only a single representative text feature instead of real images, the synthesized images gradually lose diversity as the model is optimized, which is also known as mode collapse. To tackle the problem, we propose a novel method to find semantic variations of the target text in the CLIP space. Specifically, we explore diverse semantic variations based on the informative text feature of the target domain while regularizing the uncontrolled deviation of the semantic information. With the obtained variations, we design a novel directional moment loss that matches the first and second moments of image and text direction distributions. Moreover, we introduce elastic weight consolidation and a relation consistency loss to effectively preserve valuable content information from the source domain, e.g., appearances. Through extensive experiments, we demonstrate the efficacy of the proposed methods in ensuring sample diversity in various scenarios of zero-shot GAN adaptation. We also conduct ablation studies to validate the effect of each proposed component. Notably, our model achieves a new state-of-the-art on zero-shot GAN adaptation in terms of both diversity and quality",
    "checked": true,
    "id": "73a0bc81715ddf0fb2ca777bf0f6ce0d4592f177",
    "semantic_title": "improving diversity in zero-shot gan adaptation with semantic variations",
    "citation_count": 0,
    "authors": [
      "Seogkyu Jeon",
      "Bei Liu",
      "Pilhyeon Lee",
      "Kibeom Hong",
      "Jianlong Fu",
      "Hyeran Byun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Context-Aware_Planning_and_Environment-Aware_Memory_for_Instruction_Following_Embodied_Agents_ICCV_2023_paper.html": {
    "title": "Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents",
    "volume": "main",
    "abstract": "Accomplishing household tasks such as 'bringing a cup of water' requires to plan step-by-step actions by maintaining the knowledge about the spatial arrangement of objects and consequences of previous actions. Perception models of current embodied AI agents, however, often make mistakes due to lack of such knowledge but rely on imperfect learning of imitating agents or an algorithmic planner without the knowledge about the changed environment by the previous actions. To address the issue, we propose the CPEM (Context-aware Planner and Environment-aware Memory) embodied agent to incorporate the contextual information of previous actions for planning and maintaining spatial arrangement of objects with their states (e.g., if an object has been already moved or not) in the environment to the perception model for improving both visual navigation and object interactions. We observe that the proposed model achieves state-of-the-art task success performance in various metrics using a challenging interactive instruction following benchmark both in seen and unseen environments by large margins (up to +10.70% in unseen env.)",
    "checked": true,
    "id": "8e1b062df7eca0177106a2309a291583dae5c485",
    "semantic_title": "context-aware planning and environment-aware memory for instruction following embodied agents",
    "citation_count": 1,
    "authors": [
      "Byeonghwi Kim",
      "Jinyeon Kim",
      "Yuyeong Kim",
      "Cheolhong Min",
      "Jonghyun Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sella_Vox-E_Text-Guided_Voxel_Editing_of_3D_Objects_ICCV_2023_paper.html": {
    "title": "Vox-E: Text-Guided Voxel Editing of 3D Objects",
    "volume": "main",
    "abstract": "Large scale text-guided diffusion models have garnered significant attention due to their ability to synthesize diverse images that convey complex visual concepts. This generative power has more recently been leveraged to perform text-to-3D synthesis. In this work, we present a technique that harnesses the power of latent diffusion models for editing existing 3D objects. Our method takes oriented 2D images of a 3D object as input and learns a grid-based volumetric representation of it. To guide the volumetric representation to conform to a target text prompt, we follow unconditional text-to-3D methods and optimize a Score Distillation Sampling (SDS) loss. However, we observe that combining this diffusion-guided loss with an image-based regularization loss that encourages the representation not to deviate too strongly from the input object is challenging, as it requires achieving two conflicting goals while viewing only structure-and-appearance coupled 2D projections. Thus, we introduce a novel volumetric regularization loss that operates directly in 3D space, utilizing the explicit nature of our 3D representation to enforce correlation between the global structure of the original and edited object. Furthermore, we present a technique that optimizes cross-attention volumetric grids to refine the spatial extent of the edits. Extensive experiments and comparisons demonstrate the effectiveness of our approach in creating a myriad of edits which cannot be achieved by prior works. Our code and data will be made publicly available",
    "checked": true,
    "id": "602417aec279a68efeacfbf2df587384cbfef370",
    "semantic_title": "vox-e: text-guided voxel editing of 3d objects",
    "citation_count": 8,
    "authors": [
      "Etai Sella",
      "Gal Fiebelman",
      "Peter Hedman",
      "Hadar Averbuch-Elor"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Prost_Inverse_Problem_Regularization_with_Hierarchical_Variational_Autoencoders_ICCV_2023_paper.html": {
    "title": "Inverse Problem Regularization with Hierarchical Variational Autoencoders",
    "volume": "main",
    "abstract": "In this paper, we propose to regularize ill-posed inverse problems using a deep hierarchical Variational AutoEncoder (HVAE) as an image prior. The proposed method synthesizes the advantages of i) denoiser-based Plug & Play approaches and ii) generative model based approaches to inverse problems. First, we exploit VAE properties to design an efficient algorithm that benefits from convergence guarantees of Plug-and-Play (PnP) methods. Second, our approach is not restricted to specialized datasets and the proposed PnP-HVAE model is able to solve image restoration problems on natural images of any size. Our experiments show that the proposed PnP-HVAE method is competitive with both SOTA denoiser-based PnP approaches, and other SOTA restoration methods based on generative models. The code for this project is available at https://github.com/jprost76/PnP-HVAE",
    "checked": true,
    "id": "bac80c4698a08bda7d417c6df367e1d2b2546ca9",
    "semantic_title": "inverse problem regularization with hierarchical variational autoencoders",
    "citation_count": 0,
    "authors": [
      "Jean Prost",
      "Antoine Houdard",
      "AndrÃ©s Almansa",
      "Nicolas Papadakis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fan_Unpaired_Multi-domain_Attribute_Translation_of_3D_Facial_Shapes_with_a_ICCV_2023_paper.html": {
    "title": "Unpaired Multi-domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric Map",
    "volume": "main",
    "abstract": "While impressive progress has recently been made in image-oriented facial attribute translation, shape-oriented 3D facial attribute translation remains an unsolved issue. This is primarily limited by the lack of 3D generative models and ineffective usage of 3D facial data. We propose a learning framework for 3D facial attribute translation to relieve these limitations. Firstly, we customize a novel geometric map for 3D shape representation and embed it in an end-to-end generative adversarial network. The geometric map represents 3D shapes symmetrically on a square image grid, while preserving the neighboring relationship of 3D vertices in a local least-square sense. This enables effective learning for the latent representation of data with different attributes. Secondly, we employ a unified and unpaired learning framework for multi-domain attribute translation. It not only makes effective usage of data correlation from multiple domains, but also mitigates the constraint for hardly accessible paired data. Finally, we propose a hierarchical architecture for the discriminator to guarantee robust results against both global and local artifacts. We conduct extensive experiments to demonstrate the advantage of the proposed framework over the state-of-the-art in generating high-fidelity facial shapes. Given an input 3D facial shape, the proposed framework is able to synthesize novel shapes of different attributes, which covers some downstream applications, such as expression transfer, gender translation, and aging. Code at https://github.com/NaughtyZZ/3D_facial_shape_attribute_translation_ssgmap",
    "checked": true,
    "id": "eb61e61c485e9d8f6d0cd7fd4490839092e6c6d4",
    "semantic_title": "unpaired multi-domain attribute translation of 3d facial shapes with a square and symmetric geometric map",
    "citation_count": 0,
    "authors": [
      "Zhenfeng Fan",
      "Zhiheng Zhang",
      "Shuang Yang",
      "Chongyang Zhong",
      "Min Cao",
      "Shihong Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Passive_Ultra-Wideband_Single-Photon_Imaging_ICCV_2023_paper.html": {
    "title": "Passive Ultra-Wideband Single-Photon Imaging",
    "volume": "main",
    "abstract": "We consider the problem of imaging a dynamic scene over an extreme range of timescales simultaneously--seconds to picoseconds--and doing so passively, without much light, and without any timing signals from the light source(s) emitting it. Because existing flux estimation techniques for single-photon cameras break down in this regime, we develop a flux probing theory that draws insights from stochastic calculus to enable reconstruction of a pixel's time-varying flux from a stream of monotonically-increasing photon detection timestamps. We use this theory to (1) show that passive free-running SPAD cameras have an attainable frequency bandwidth that spans the entire DC-to-31 GHz range in low-flux conditions, (2) derive a novel Fourier-domain flux reconstruction algorithm that scans this range for frequencies with statistically-significant support in the timestamp data, and (3) ensure the algorithm's noise model remains valid even for very low photon counts or non-negligible dead times. We show the potential of this asynchronous imaging regime by experimentally demonstrating several never-seen-before abilities: (1) imaging a scene illuminated simultaneously by sources operating at vastly different speeds without synchronization (bulbs, projectors, multiple pulsed lasers), (2) passive non-line-of-sight video acquisition, and (3) recording ultra-wideband video, which can be played back later at 30 Hz to show everyday motions--but can also be played a billion times slower to show the propagation of light itself",
    "checked": true,
    "id": "99cf1ca431950f0420068ae1a38269816ad26d48",
    "semantic_title": "passive ultra-wideband single-photon imaging",
    "citation_count": 0,
    "authors": [
      "Mian Wei",
      "Sotiris Nousias",
      "Rahul Gulve",
      "David B. Lindell",
      "Kiriakos N. Kutulakos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shahreza_Template_Inversion_Attack_against_Face_Recognition_Systems_using_3D_Face_ICCV_2023_paper.html": {
    "title": "Template Inversion Attack against Face Recognition Systems using 3D Face Reconstruction",
    "volume": "main",
    "abstract": "Face recognition systems are increasingly being used in different applications. In such systems, some features (also known as embeddings or templates) are extracted from each face image. Then, the extracted templates are stored in the system's database during the enrollment stage and are later used for recognition. In this paper, we focus on template inversion attacks against face recognition systems and introduce a novel method (dubbed GaFaR) to reconstruct 3D face from facial templates. To this end, we use a geometry-aware generator network based on generative neural radiance fields (GNeRF), and learn a mapping from facial templates to the intermediate latent space of the generator network. We train our network with a semi-supervised learning approach using real and synthetic images simultaneously. For the real training data, we use a Generative Adversarial Network (GAN) based framework to learn the distribution of the latent space. For the synthetic training data, where we have the true latent code, we directly train in the latent space of the generator network. In addition, during the inference stage, we also propose optimization on the camera parameters to generate face images to improve the success attack rate (up to 17.14% in our experiments). We evaluate the performance of our method in the whitebox and blackbox attacks against state-of-the-art face recognition models on the LFW and MOBIO datasets. To our knowledge, this paper is the first work on 3D face reconstruction from facial templates. The project page is available at: https://www.idiap.ch/paper/gafar",
    "checked": false,
    "id": "2d7378cb4f8a430734b3094a7b3807050ca429c3",
    "semantic_title": "comprehensive vulnerability evaluation of face recognition systems to template inversion attacks via 3d face reconstruction",
    "citation_count": 0,
    "authors": [
      "Hatef Otroshi Shahreza",
      "SÃ©bastien Marcel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gholami_ETran_Energy-Based_Transferability_Estimation_ICCV_2023_paper.html": {
    "title": "ETran: Energy-Based Transferability Estimation",
    "volume": "main",
    "abstract": "This paper addresses the problem of ranking pre-trained models for object detection and image classification. Selecting the best pre-trained model by fine-tuning is an expensive and time-consuming task. Previous works have proposed transferability estimation based on features extracted by the pre-trained models. We argue that quantifying whether the target dataset is in-distribution (IND) or out-of-distribution (OOD) for the pre-trained model is an important factor in the transferability estimation. To this end, we propose ETran, an energy-based transferability assessment metric, which includes three scores: 1) energy score, 2) classification score, and 3) regression score. We use energy-based models to determine whether the target dataset is OOD or IND for the pre-trained model. In contrast to the prior works, ETran is applicable to a wide range of tasks including classification, regression, and object detection (classification+regression). This is the first work that proposes transferability estimation for object detection task. Our extensive experiments on four benchmarks and two tasks show that ETran outperforms previous works on object detection and classification benchmarks by an average of 21% and 12%, respectively, and achieves SOTA in transferability assessment",
    "checked": true,
    "id": "c6a548dccaaaaa7c8ca1eea63f4312e0c3f92a77",
    "semantic_title": "etran: energy-based transferability estimation",
    "citation_count": 0,
    "authors": [
      "Mohsen Gholami",
      "Mohammad Akbari",
      "Xinglu Wang",
      "Behnam Kamranian",
      "Yong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Predict_to_Detect_Prediction-guided_3D_Object_Detection_using_Sequential_Images_ICCV_2023_paper.html": {
    "title": "Predict to Detect: Prediction-guided 3D Object Detection using Sequential Images",
    "volume": "main",
    "abstract": "Recent camera-based 3D object detection methods have introduced sequential frames to improve the detection performance hoping that multiple frames would mitigate the large depth estimation error. Despite improved detection performance, prior works rely on naive fusion methods (e.g., concatenation) or are limited to static scenes (e.g., temporal stereo), neglecting the importance of the motion cue of objects. These approaches do not fully exploit the potential of sequential images and show limited performance improvements. To address this limitation, we propose a novel 3D object detection model, P2D (Predict to Detect), that integrates a prediction scheme into a detection framework to explicitly extract and leverage motion features. P2D predicts object information in the current frame using solely past frames to learn temporal motion features. We then introduce a novel temporal feature aggregation method that attentively exploits Bird's-Eye-View (BEV) features based on predicted object information, resulting in accurate 3D object detection. Experimental results demonstrate that P2D improves mAP and NDS by 3.0% and 3.7% compared to the sequential image-based baseline, proving that incorporating a prediction scheme can significantly improve detection accuracy",
    "checked": true,
    "id": "4f0a4dd63a1333ad392294078df1a44ee73b4d4f",
    "semantic_title": "predict to detect: prediction-guided 3d object detection using sequential images",
    "citation_count": 0,
    "authors": [
      "Sanmin Kim",
      "Youngseok Kim",
      "In-Jae Lee",
      "Dongsuk Kum"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Unilaterally_Aggregated_Contrastive_Learning_with_Hierarchical_Augmentation_for_Anomaly_Detection_ICCV_2023_paper.html": {
    "title": "Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation for Anomaly Detection",
    "volume": "main",
    "abstract": "Anomaly detection (AD), aiming to find samples that deviate from the training distribution, is essential in safety-critical applications. Though recent self-supervised learning based attempts achieve promising results by creating virtual outliers, their training objectives are less faithful to AD which requires a concentrated inlier distribution as well as a dispersive outlier distribution. In this paper, we propose Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation (UniCon-HA), taking into account both the requirements above. Specifically, we explicitly encourage the concentration of inliers and the dispersion of virtual outliers via supervised and unsupervised contrastive losses, respectively. Considering that standard contrastive data augmentation for generating positive views may induce outliers, we additionally introduce a soft mechanism to re-weight each augmented inlier according to its deviation from the inlier distribution, to ensure a purified concentration. Moreover, to prompt a higher concentration, inspired by curriculum learning, we adopt an easy-to-hard hierarchical augmentation strategy and perform contrastive aggregation at different depths of the network based on the strengths of data augmentation. Our method is evaluated under three AD settings including unlabeled one-class, unlabeled multi-class, and labeled multi-class, demonstrating its consistent superiority over other competitors",
    "checked": true,
    "id": "eb677d76a38bd8a98fa75e6b1452a25c68bae3fc",
    "semantic_title": "unilaterally aggregated contrastive learning with hierarchical augmentation for anomaly detection",
    "citation_count": 0,
    "authors": [
      "Guodong Wang",
      "Yunhong Wang",
      "Jie Qin",
      "Dongming Zhang",
      "Xiuguo Bao",
      "Di Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Learning_Image-Adaptive_Codebooks_for_Class-Agnostic_Image_Restoration_ICCV_2023_paper.html": {
    "title": "Learning Image-Adaptive Codebooks for Class-Agnostic Image Restoration",
    "volume": "main",
    "abstract": "Recent work of discrete generative priors, in the form of codebooks, has shown exciting performance for image reconstruction and restoration, since the discrete prior space spanned by the codebooks increases the robustness against diverse image degradations. Nevertheless, these methods require separate training of codebooks for different image categories, which limits their use to specific image categories only (e.g. face, architecture, etc.), and fail to handle arbitrary natural images. In this paper, we propose AdaCode for learning image-adaptive codebooks for class-agnostic image restoration. Instead of learning a single codebook for all categories of images, we learn a set of basis codebooks. For a given input image, AdaCode learns a weight map with which we compute a weighted combination of these basis codebooks for adaptive image restoration. Intuitively, AdaCode is a more flexible and expressive discrete generative prior than previous work. Experimental results show that AdaCode achieves state-of-the-art performance on image reconstruction and restoration tasks, including image super-resolution and inpainting",
    "checked": true,
    "id": "81c739551f9122f5dc5ddf78900c577e716ad49a",
    "semantic_title": "learning image-adaptive codebooks for class-agnostic image restoration",
    "citation_count": 1,
    "authors": [
      "Kechun Liu",
      "Yitong Jiang",
      "Inchang Choi",
      "Jinwei Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Takmaz_3D_Segmentation_of_Humans_in_Point_Clouds_with_Synthetic_Data_ICCV_2023_paper.html": {
    "title": "3D Segmentation of Humans in Point Clouds with Synthetic Data",
    "volume": "main",
    "abstract": "Segmenting humans in 3D indoor scenes has become increasingly important with the rise of human-centered robotics and AR/VR applications. To this end, we propose the task of joint 3D human semantic segmentation, instance segmentation and multi-human body-part segmentation. Few works have attempted to directly segment humans in cluttered 3D scenes, which is largely due to the lack of annotated training data of humans interacting with 3D scenes. We address this challenge and propose a framework for generating training data of synthetic humans interacting with real 3D scenes. Furthermore, we propose a novel transformer-based model, Human3D, which is the first end-to-end model for segmenting multiple human instances and their body-parts in a unified manner. The key advantage of our synthetic data generation framework is its ability to generate diverse and realistic human-scene interactions, with highly accurate ground truth. Our experiments show that pre-training on synthetic data improves performance on a wide variety of 3D human segmentation tasks. Finally, we demonstrate that Human3D outperforms even task-specific state-of-the-art 3D segmentation methods",
    "checked": true,
    "id": "015a1105154cf8d4cb2755f5d870c7ac13abe720",
    "semantic_title": "3d segmentation of humans in point clouds with synthetic data",
    "citation_count": 0,
    "authors": [
      "AyÃ§a Takmaz",
      "Jonas Schult",
      "Irem Kaftan",
      "Mertcan AkÃ§ay",
      "Bastian Leibe",
      "Robert Sumner",
      "Francis Engelmann",
      "Siyu Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sotiris_Mastering_Spatial_Graph_Prediction_of_Road_Networks_ICCV_2023_paper.html": {
    "title": "Mastering Spatial Graph Prediction of Road Networks",
    "volume": "main",
    "abstract": "Accurately predicting road networks from satellite images requires a global understanding of the network topology. We propose to capture such high-level information by introducing a graph-based framework that given a partially generated graph, sequentially adds new edges. To deal with misalignment between the model predictions and the intended purpose, and to optimize over complex, non-continuous metrics of interest, we adopt a reinforcement learning (RL) approach that nominates modifications that maximize a cumulative reward. As opposed to standard supervised techniques that tend to be more restricted to commonly used surrogate losses, our framework yields more power and flexibility to encode problem-dependent knowledge. Empirical results on several benchmark datasets demonstrate enhanced performance and increased high-level reasoning about the graph topology when using a tree-based search. We further demonstrate the superiority of our approach in handling examples with substantial occlusion and additionally provide evidence that our predictions better match the statistical properties of the ground dataset",
    "checked": true,
    "id": "1be31154080edaea160d36abacdc009961dcafb1",
    "semantic_title": "mastering spatial graph prediction of road networks",
    "citation_count": 0,
    "authors": [
      "Anagnostidis Sotiris",
      "Aurelien Lucchi",
      "Thomas Hofmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Boutros_IDiff-Face_Synthetic-based_Face_Recognition_through_Fizzy_Identity-Conditioned_Diffusion_Model_ICCV_2023_paper.html": {
    "title": "IDiff-Face: Synthetic-based Face Recognition through Fizzy Identity-Conditioned Diffusion Model",
    "volume": "main",
    "abstract": "The availability of large-scale authentic face databases has been crucial to the significant advances made in face recognition research over the past decade. However, legal and ethical concerns led to the recent retraction of many of these databases by their creators, raising questions about the continuity of future face recognition research without one of its key resources. Synthetic datasets have emerged as a promising alternative to privacy-sensitive authentic data for face recognition development. However, recent synthetic datasets that are used to train face recognition models suffer either from limitations in intra-class diversity or cross-class (identity) discrimination, leading to less optimal accuracies, far away from the accuracies achieved by models trained on authentic data. This paper targets this issue by proposing IDiff-Face, a novel approach based on conditional latent diffusion models for synthetic identity generation with realistic identity variations for face recognition training. Through extensive evaluations, our proposed synthetic-based face recognition approach pushed the limits of state-of-the-art performances, achieving, for example, 98.00% accuracy on the Labeled Faces in the Wild (LFW) benchmark, far ahead from the recent synthetic-based face recognition solutions with 95.40% and bridging the gap to authentic-based face recognition with 99.82% accuracy",
    "checked": false,
    "id": "6fc834e9015be0d24ee76217c18993d3721f638d",
    "semantic_title": "idiff-face: synthetic-based face recognition through fizzy identity-conditioned diffusion models",
    "citation_count": 1,
    "authors": [
      "Fadi Boutros",
      "Jonas Henry Grebe",
      "Arjan Kuijper",
      "Naser Damer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Quan_Deep_Video_Demoireing_via_Compact_Invertible_Dyadic_Decomposition_ICCV_2023_paper.html": {
    "title": "Deep Video Demoireing via Compact Invertible Dyadic Decomposition",
    "volume": "main",
    "abstract": "Removing moire patterns from videos recorded on screens or complex textures is known as video demoireing. It is a challenging task as both structures and textures of an image usually exhibit strong periodic patterns, which thus are easily confused with moire patterns and can be significantly erased in the removal process. By interpreting video demoireing as a multi-frame decomposition problem, we propose a compact invertible dyadic network called CIDNet that progressively decouples latent frames and the moire patterns from an input video sequence. Using a dyadic cross-scale coupling structure with coupling layers tailored for multi-scale processing, CIDNet aims at disentangling the features of image patterns from that of moire patterns at different scales, while retaining all latent image features to facilitate reconstruction. In addition, a compressed form for the network's output is introduced to reduce computational complexity and alleviate overfitting. The experiments show that CIDNet outperforms existing methods and enjoys the advantages in model size and computational efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Quan",
      "Haoran Huang",
      "Shengfeng He",
      "Ruotao Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Rethinking_Multi-Contrast_MRI_Super-Resolution_Rectangle-Window_Cross-Attention_Transformer_and_Arbitrary-Scale_Upsampling_ICCV_2023_paper.html": {
    "title": "Rethinking Multi-Contrast MRI Super-Resolution: Rectangle-Window Cross-Attention Transformer and Arbitrary-Scale Upsampling",
    "volume": "main",
    "abstract": "Recently, several methods have explored the potential of multi-contrast magnetic resonance imaging (MRI) super-resolution (SR) and obtain results superior to single-contrast SR methods. However, existing approaches still have two shortcomings: (1) They can only address fixed integer upsampling scales, such as 2x, 3x, and 4x, which require training and storing the corresponding model separately for each upsampling scale in clinic. (2) They lack direct interaction among different windows as they adopt the square window (e.g., 8x8) transformer network architecture, which results in inadequate modelling of longer-range dependencies. Moreover, the relationship between reference images and target images is not fully mined. To address these issues, we develop a novel network for multi-contrast MRI arbitrary-scale SR, dubbed as McASSR. Specifically, we design a rectangle-window cross-attention transformer to establish longer-range dependencies in MR images without increasing computational complexity and fully use reference information. Besides, we propose the reference-aware implicit attention as an upsampling module, achieving arbitrary-scale super-resolution via implicit neural representation, further fusing supplementary information of the reference image. Extensive and comprehensive experiments on both public and clinical datasets show that our McASSR yields superior performance over SOTA methods, demonstrating its great potential to be applied in clinical practice",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyuan Li",
      "Lei Zhao",
      "Jiakai Sun",
      "Zehua Lan",
      "Zhanjie Zhang",
      "Jiafu Chen",
      "Zhijie Lin",
      "Huaizhong Lin",
      "Wei Xing"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Domain_Generalization_via_Rationale_Invariance_ICCV_2023_paper.html": {
    "title": "Domain Generalization via Rationale Invariance",
    "volume": "main",
    "abstract": "This paper offers a new perspective to ease the challenge of domain generalization, which involves maintaining robust results even in unseen environments. Our design focuses on the decision-making process in the final classifier layer. Specifically, we propose treating the element-wise contributions to the final results as the rationale for making a decision and representing the rationale for each sample as a matrix. For a well-generalized model, we suggest the rationale matrices for samples belonging to the same category should be similar, indicating the model relies on domain-invariant clues to make decisions, thereby ensuring robust results. To implement this idea, we introduce a rationale invariance loss as a simple regularization technique, requiring only a few lines of code. Our experiments demonstrate that the proposed approach achieves competitive results across various datasets, despite its simplicity. Code is available at https://github.com/liangchen527/RIDG",
    "checked": true,
    "id": "934c25c60fbb19bf7c34657fcd0eb6e5b1b5f6ea",
    "semantic_title": "domain generalization via rationale invariance",
    "citation_count": 2,
    "authors": [
      "Liang Chen",
      "Yong Zhang",
      "Yibing Song",
      "Anton van den Hengel",
      "Lingqiao Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Upadhyay_ProbVLM_Probabilistic_Adapter_for_Frozen_Vison-Language_Models_ICCV_2023_paper.html": {
    "title": "ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models",
    "volume": "main",
    "abstract": "Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model selection as two real-world downstream tasks for VLMs and show that the estimated uncertainty aids both tasks. Lastly, we present a novel technique for visualizing the embedding distributions using a large-scale pre-trained latent diffusion model",
    "checked": true,
    "id": "8d1f2e1beaf6905641740c6fee995f0b3f3e0938",
    "semantic_title": "probvlm: probabilistic adapter for frozen vison-language models",
    "citation_count": 0,
    "authors": [
      "Uddeshya Upadhyay",
      "Shyamgopal Karthik",
      "Massimiliano Mancini",
      "Zeynep Akata"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Towards_Open-Set_Test-Time_Adaptation_Utilizing_the_Wisdom_of_Crowds_in_ICCV_2023_paper.html": {
    "title": "Towards Open-Set Test-Time Adaptation Utilizing the Wisdom of Crowds in Entropy Minimization",
    "volume": "main",
    "abstract": "Test-time adaptation (TTA) methods, which generally rely on the model's predictions (e.g., entropy minimization) to adapt the source pretrained model to the unlabeled target domain, suffer from noisy signals originating from 1) incorrect or 2) open-set predictions. Long-term stable adaptation is hampered by such noisy signals, so training models without such error accumulation is crucial for practical TTA. To address these issues, including open-set TTA, we propose a simple yet effective sample selection method inspired by the following crucial empirical finding. While entropy minimization compels the model to increase the probability of its predicted label (i.e., confidence values), we found that noisy samples rather show decreased confidence values. To be more specific, entropy minimization attempts to raise the confidence values of an individual sample's prediction, but individual confidence values may rise or fall due to the influence of signals from numerous other predictions (i.e., wisdom of crowds). Due to this fact, noisy signals misaligned with such 'wisdom of crowds', generally found in the correct signals, fail to raise the individual confidence values of wrong samples, despite attempts to increase them. Based on such findings, we filter out the samples whose confidence values are lower in the adapted model than in the original model, as they are likely to be noisy. Our method is widely applicable to existing TTA methods and improves their long-term adaptation performance in both image classification (e.g., 49.4% reduced error rates with TENT) and semantic segmentation (e.g., 11.7% gain in mIoU with TENT)",
    "checked": true,
    "id": "997a7ebc742d2f39576a1efa539a63e890084d0f",
    "semantic_title": "towards open-set test-time adaptation utilizing the wisdom of crowds in entropy minimization",
    "citation_count": 1,
    "authors": [
      "Jungsoo Lee",
      "Debasmit Das",
      "Jaegul Choo",
      "Sungha Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Singh_Scene_Graph_Contrastive_Learning_for_Embodied_Navigation_ICCV_2023_paper.html": {
    "title": "Scene Graph Contrastive Learning for Embodied Navigation",
    "volume": "main",
    "abstract": "Training effective embodied AI agents often involves expert imitation, specialized components such as maps, or leveraging additional sensors for depth and localization. Another approach is to use neural architectures alongside self-supervised objectives which encourage better representation learning. However, in practice, there are few guarantees that these self-supervised objectives encode task-relevant information. We propose the Scene Graph Contrastive (SGC) loss, which uses scene graphs as training-only supervisory signals. The SGC loss does away with explicit graph decoding and instead uses contrastive learning to align an agent's representation with a rich graphical encoding of its environment. The SGC loss is simple to implement and encourages representations that encode objects' semantics, relationships, and history. By using the SGC loss, we attain gains on three embodied tasks: Object Navigation, Multi-Object Navigation, and Arm Point Navigation. Finally, we present studies and analyses which demonstrate the ability of our trained representation to encode semantic cues about the environment",
    "checked": true,
    "id": "568147e21a7992cd10ffed8db8c3e1a11fe1768c",
    "semantic_title": "scene graph contrastive learning for embodied navigation",
    "citation_count": 0,
    "authors": [
      "Kunal Pratap Singh",
      "Jordi Salvador",
      "Luca Weihs",
      "Aniruddha Kembhavi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Long-Range_Grouping_Transformer_for_Multi-View_3D_Reconstruction_ICCV_2023_paper.html": {
    "title": "Long-Range Grouping Transformer for Multi-View 3D Reconstruction",
    "volume": "main",
    "abstract": "Nowadays, transformer networks have demonstrated superior performance in many computer vision tasks. In a multi-view 3D reconstruction algorithm following this paradigm, self-attention processing has to deal with intricate image tokens including massive information when facing heavy amounts of view input. The curse of information content leads to the extreme difficulty of model learning. To alleviate this problem, recent methods compress the token number representing each view or discard the attention operations between the tokens from different views. Obviously, they give a negative impact on performance. Therefore, we propose long-range grouping attention (LGA) based on the divide-and-conquer principle. Tokens from all views are grouped for separate attention operations. The tokens in each group are sampled from all views and can provide macro representation for the resided view. The richness of feature learning is guaranteed by the diversity among different groups. An effective and efficient encoder can be established which connects inter-view features using LGA and extract intra-view features using the standard self-attention layer. Moreover, a novel progressive upsampling decoder is also designed for voxel generation with relatively high resolution. Hinging on the above, we construct a powerful transformer-based network, called LRGT. Experimental results on ShapeNet verify our method achieves SOTA accuracy in multi-view reconstruction. Code is available at https://github.com/LiyingCV/Long-Range-Grouping-Transformer",
    "checked": true,
    "id": "a28ad386ec48929e3d9eee76faa5d1c72f9ecb52",
    "semantic_title": "long-range grouping transformer for multi-view 3d reconstruction",
    "citation_count": 0,
    "authors": [
      "Liying Yang",
      "Zhenwei Zhu",
      "Xuxin Lin",
      "Jian Nong",
      "Yanyan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Latent-OFER_Detect_Mask_and_Reconstruct_with_Latent_Vectors_for_Occluded_ICCV_2023_paper.html": {
    "title": "Latent-OFER: Detect, Mask, and Reconstruct with Latent Vectors for Occluded Facial Expression Recognition",
    "volume": "main",
    "abstract": "Most research on facial expression recognition (FER) is conducted in highly controlled environments, but its performance is often unacceptable when applied to real-world situations. This is because when unexpected objects occlude the face, the FER network faces difficulties extracting facial features and accurately predicting facial expressions. Therefore, occluded FER (OFER) is a challenging problem. Previous studies on occlusion-aware FER have typically required fully annotated facial images for training. However, collecting facial images with various occlusions and expression annotations is time-consuming and expensive. Latent-OFER, the proposed method, can detect occlusions, restore occluded parts of the face as if they were unoccluded, and recognize them, improving FER accuracy. This approach involves three steps: First, the vision transformer (ViT)-based occlusion patch detector masks the occluded position by training only latent vectors from the unoccluded patches using the support vector data description algorithm. Second, the hybrid reconstruction network generates the masking position as a complete image using the ViT and convolutional neural network (CNN). Last, the expression-relevant latent vector extractor retrieves and uses expression-related information from all latent vectors by applying a CNN-based class activation map. This mechanism has a significant advantage in preventing performance degradation from occlusion by unseen objects. The experimental results on several databases demonstrate the superiority of the proposed method over state-of-the-art methods",
    "checked": true,
    "id": "c794822c167996ebedfbcf329830d884f19e5423",
    "semantic_title": "latent-ofer: detect, mask, and reconstruct with latent vectors for occluded facial expression recognition",
    "citation_count": 0,
    "authors": [
      "Isack Lee",
      "Eungi Lee",
      "Seok Bong Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_DenseShift_Towards_Accurate_and_Efficient_Low-Bit_Power-of-Two_Quantization_ICCV_2023_paper.html": {
    "title": "DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization",
    "volume": "main",
    "abstract": "Efficiently deploying deep neural networks on low-resource edge devices is challenging due to their ever-increasing resource requirements. To address this issue, researchers have proposed multiplication-free neural networks, such as Power-of-Two quantization, or also known as Shift networks, which aim to reduce memory usage and simplify computation. However, existing low-bit Shift networks are not as accurate as their full-precision counterparts, typically suffering from limited weight range encoding schemes and quantization loss. In this paper, we propose the DenseShift network, which significantly improves the accuracy of Shift networks, achieving competitive performance to full-precision networks for vision and speech applications. In addition, we introduce a method to deploy an efficient DenseShift network using non-quantized floating-point activations, while obtaining 1.6X speed-up over existing methods. To achieve this, we demonstrate that zero-weight values in low-bit Shift networks do not contribute to model capacity and negatively impact inference computation. To address this issue, we propose a zero-free shifting mechanism that simplifies inference and increases model capacity. We further propose a sign-scale decomposition design to enhance training efficiency and a low-variance random initialization strategy to improve the model's transfer learning performance. Our extensive experiments on various computer vision and speech tasks demonstrate that DenseShift outperforms existing low-bit multiplication-free networks and achieves competitive performance compared to full-precision networks. Furthermore, our proposed approach exhibits strong transfer learning performance without a drop in accuracy. Our code was released on GitHub",
    "checked": true,
    "id": "9e669bb8595dc54463ae4a51f292959c41e60f55",
    "semantic_title": "denseshift: towards accurate and efficient low-bit power-of-two quantization",
    "citation_count": 0,
    "authors": [
      "Xinlin Li",
      "Bang Liu",
      "Rui Heng Yang",
      "Vanessa Courville",
      "Chao Xing",
      "Vahid Partovi Nia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Preparing_the_Future_for_Continual_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Preparing the Future for Continual Semantic Segmentation",
    "volume": "main",
    "abstract": "In this study, we focus on Continual Semantic Segmentation (CSS) and present a novel approach to tackle the issue of existing methods struggling to learn new classes. The primary challenge of CSS is to learn new knowledge while retaining old knowledge, which is commonly known as the rigidity-plasticity dilemma. Existing approaches strive to address this by carefully balancing the learning of new and old classes during training on new data. Differently, this work aims to avoid this dilemma fundamentally rather than handling the difficulties involved in it. Specifically, we reveal that this dilemma mainly arises from the greater fluctuation of knowledge for new classes because they have never been learned before the current step. Additionally, the data available in incremental steps are usually inadequate, which can impede the model's ability to learn discriminative features for both new and old classes. To address these challenges, we introduce a novel concept of pre-learning for future knowledge. Our approach entails optimizing the feature space and output space for unlabeled data, which thus enables the model to acquire knowledge for future classes. With this approach, updating the model for new classes becomes as smooth as for old classes, effectively avoiding the rigidity-plasticity dilemma. We conducted extensive experiments and the results demonstrate a significant improvement in the learning of new classes compared to previous state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Lin",
      "Zilei Wang",
      "Yixin Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shoouri_Efficient_Computation_Sharing_for_Multi-Task_Visual_Scene_Understanding_ICCV_2023_paper.html": {
    "title": "Efficient Computation Sharing for Multi-Task Visual Scene Understanding",
    "volume": "main",
    "abstract": "Solving multiple visual tasks using individual models can be resource-intensive, while multi-task learning can conserve resources by sharing knowledge across different tasks. Despite the benefits of multi-task learning, such techniques can struggle with balancing the loss for each task, leading to potential performance degradation. We present a novel computation- and parameter-sharing framework that balances efficiency and accuracy to perform multiple visual tasks utilizing individually-trained single-task transformers. Our method is motivated by transfer learning schemes to reduce computational and parameter storage costs while maintaining the desired performance. Our approach involves splitting the tasks into a base task and the other sub-tasks, and sharing a significant portion of activations and parameters/weights between the base and sub-tasks to decrease inter-task redundancies and enhance knowledge sharing. The evaluation conducted on NYUD-v2 and PASCAL-context datasets shows that our method is superior to the state-of-the-art transformer-based multi-task learning techniques with higher accuracy and reduced computational resources. Moreover, our method is extended to video stream inputs, further reducing computational costs by efficiently sharing information across the temporal domain as well as the task domain. Our codes are available at https://github.com/sarashoouri/EfficientMTL",
    "checked": true,
    "id": "1bbe6e99a95d9a198bc41173a4dece107f801cee",
    "semantic_title": "efficient computation sharing for multi-task visual scene understanding",
    "citation_count": 0,
    "authors": [
      "Sara Shoouri",
      "Mingyu Yang",
      "Zichen Fan",
      "Hun-Seok Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tu_Self-supervised_Cross-view_Representation_Reconstruction_for_Change_Captioning_ICCV_2023_paper.html": {
    "title": "Self-supervised Cross-view Representation Reconstruction for Change Captioning",
    "volume": "main",
    "abstract": "Change captioning aims to describe the difference between a pair of similar images. Its key challenge is how to learn a stable difference representation under pseudo changes caused by viewpoint change. In this paper, we address this by proposing a self-supervised cross-view representation reconstruction (SCORER) network. Concretely, we first design a multi-head token-wise matching to model relationships between cross-view features from similar/dissimilar images. Then, by maximizing cross-view contrastive alignment of two similar images, SCORER learns two view-invariant image representations in a self-supervised way. Based on these, we reconstruct the representations of unchanged objects by cross-attention, thus learning a stable difference representation for caption generation. Further, we devise a cross-modal backward reasoning to improve the quality of caption. This module reversely models a \"hallucination\" representation with the caption and \"before\" representation. By pushing it closer to the \"after\" representation, we enforce the caption to be informative about the difference in a self-supervised manner. Extensive experiments show our method achieves the state-of-the-art results on four datasets. The code is available at https://github.com/tuyunbin/SCORER",
    "checked": true,
    "id": "72813e372a7748302c631f7da9b367d0cce2554a",
    "semantic_title": "self-supervised cross-view representation reconstruction for change captioning",
    "citation_count": 0,
    "authors": [
      "Yunbin Tu",
      "Liang Li",
      "Li Su",
      "Zheng-Jun Zha",
      "Chenggang Yan",
      "Qingming Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Unify_Align_and_Refine_Multi-Level_Semantic_Alignment_for_Radiology_Report_ICCV_2023_paper.html": {
    "title": "Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology Report Generation",
    "volume": "main",
    "abstract": "Automatic radiology report generation has attracted enormous research interest due to its practical value in reducing the workload of radiologists. However, simultaneously establishing global correspondences between the image (e.g., Chest X-ray) and its related report and local alignments between image patches and keywords remains challenging. To this end, we propose an Unify, Align and then Refine (UAR) approach to learn multi-level cross-modal alignments and introduce three novel modules: Latent Space Unifier (LSU), Cross-modal Representation Aligner (CRA) and Text-to-Image Refiner (TIR). Specifically, LSU unifies multimodal data into discrete tokens, making it flexible to learn common knowledge among modalities with a shared network. The modality-agnostic CRA learns discriminative features via a set of orthonormal basis and a dual-gate mechanism first and then globally aligns visual and textual representations under a triplet contrastive loss. TIR boosts token-level local alignment via calibrating text-to-image attention with a learnable mask. Additionally, we design a two-stage training procedure to make UAR gradually grasp cross-modal alignments at different levels, which imitates radiologists' workflow: writing sentence by sentence first and then checking word by word. Extensive experiments and analyses on IU-Xray and MIMIC-CXR benchmark datasets demonstrate the superiority of our UAR against varied state-of-the-art methods",
    "checked": true,
    "id": "9798bb994504561cde9d24a776e4b5c010b9ecf2",
    "semantic_title": "unify, align and refine: multi-level semantic alignment for radiology report generation",
    "citation_count": 7,
    "authors": [
      "Yaowei Li",
      "Bang Yang",
      "Xuxin Cheng",
      "Zhihong Zhu",
      "Hongxiang Li",
      "Yuexian Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Synthesizing_Diverse_Human_Motions_in_3D_Indoor_Scenes_ICCV_2023_paper.html": {
    "title": "Synthesizing Diverse Human Motions in 3D Indoor Scenes",
    "volume": "main",
    "abstract": "We present a novel method for populating 3D indoor scenes with virtual humans that can navigate in the environment and interact with objects in a realistic manner. Existing approaches rely on high-quality training sequences that contain captured human motions and the 3D scenes they interact with. However, such interaction data are costly, difficult to capture, and can hardly cover the full range of plausible human-scene interactions in complex indoor environments. To address these challenges, we propose a reinforcement learning-based approach that enables virtual humans to navigate in 3D scenes and interact with objects realistically and autonomously, driven by learned motion control policies. The motion control policies employ latent motion action spaces, which correspond to realistic motion primitives and are learned from large-scale motion capture data using a powerful generative motion model. For navigation in a 3D environment, we propose a scene-aware policy with novel state and reward designs for collision avoidance. Combined with navigation mesh-based path-finding algorithms to generate intermediate waypoints, our approach enables the synthesis of diverse human motions navigating in 3D indoor scenes and avoiding obstacles. To generate fine-grained human-object interactions, we carefully curate interaction goal guidance using a marker-based body representation and leverage features based on the signed distance field (SDF) to encode human-scene proximity relations. Our method can synthesize realistic and diverse human-object interactions (e.g., sitting on a chair and then getting up) even for out-of-distribution test scenarios with different object shapes, orientations, starting body positions, and poses. Experimental results demonstrate that our approach outperforms state-of-the-art human-scene interaction synthesis methods in terms of both motion naturalness and diversity. Code, models, and demonstrative video results are publicly available at: https://zkf1997.github.io/DIMOS",
    "checked": true,
    "id": "76822be30e76b8f4f7dfdb15d6cca1ba1e8e617e",
    "semantic_title": "synthesizing diverse human motions in 3d indoor scenes",
    "citation_count": 7,
    "authors": [
      "Kaifeng Zhao",
      "Yan Zhang",
      "Shaofei Wang",
      "Thabo Beeler",
      "Siyu Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Deep_Optics_for_Video_Snapshot_Compressive_Imaging_ICCV_2023_paper.html": {
    "title": "Deep Optics for Video Snapshot Compressive Imaging",
    "volume": "main",
    "abstract": "Video snapshot compressive imaging (SCI) aims to capture a sequence of video frames with only a single shot of a 2D detector, whose backbones rest in optical modulation patterns (also known as masks) and a computational reconstruction algorithm. Advanced deep learning algorithms and mature hardware are putting video SCI into practical applications. Yet, there are two clouds in the sunshine of SCI: i) low dynamic range as a victim of high temporal multiplexing, and ii) existing deep learning algorithms' degradation on real system. To address these challenges, this paper presents a deep optics framework to jointly optimize masks and a reconstruction network. Specifically, we first propose a new type of structural mask to realize motionaware and full-dynamic-range measurement. Considering the motion awareness property in measurement domain, we develop an efficient network for video SCI reconstruction using Transformer to capture long-term temporal dependencies, dubbed Res2former. Moreover, sensor response is introduced into the forward model of video SCI to guarantee end-to-end model training close to real system. Finally, we implement the learned structural masks on a digital micro-mirror device. Experimental results on synthetic and real data validate the effectiveness of the proposed framework. We believe this is a miestone for real-world video SCI. The source code and data are available at https://github.com/pwangcs/DeepOpticsSCI",
    "checked": false,
    "id": "34cc4dafccb7a91c8d2bbe5a8c6d0b198fec40de",
    "semantic_title": "deep unfolding for snapshot compressive imaging",
    "citation_count": 0,
    "authors": [
      "Ping Wang",
      "Lishun Wang",
      "Xin Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_DDIT_Semantic_Scene_Completion_via_Deformable_Deep_Implicit_Templates_ICCV_2023_paper.html": {
    "title": "DDIT: Semantic Scene Completion via Deformable Deep Implicit Templates",
    "volume": "main",
    "abstract": "Scene reconstructions are often incomplete due to occlusions and limited viewpoints. There have been efforts to use semantic information for scene completion. However, the completed shapes may be rough and imprecise since respective methods rely on 3D convolution and/or lack effective shape constraints. To overcome these limitations, we propose a semantic scene completion method based on deformable deep implicit templates (DDIT). Specifically, we complete each segmented instance in a scene by deforming a template with a latent code. Such a template is expressed by a deep implicit function in the canonical frame. It abstracts the shape prior of a category, and thus can provide constraints on the overall shape of an instance. Latent code controls the deformation of template to guarantee fine details of an instance. For code prediction, we design a neural network that leverages both intra- and inter-instance information. We also introduce an algorithm to transform instances between the world and canonical frames based on geometric constraints and a hierarchical tree. To further improve accuracy, we jointly optimize the latent code and transformation by enforcing the zero-valued isosurface constraint. In addition, we establish a new dataset to solve different problems of existing datasets. Experiments showed that our DDIT outperforms state-of-the-art approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoang Li",
      "Jinhu Dong",
      "Binghui Wen",
      "Ming Gao",
      "Tianyu Huang",
      "Yun-Hui Liu",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Joint_Demosaicing_and_Deghosting_of_Time-Varying_Exposures_for_Single-Shot_HDR_ICCV_2023_paper.html": {
    "title": "Joint Demosaicing and Deghosting of Time-Varying Exposures for Single-Shot HDR Imaging",
    "volume": "main",
    "abstract": "The quad-Bayer patterned image sensor has made significant improvements in spatial resolution over recent years due to advancements in image sensor technology. This has enabled single-shot high-dynamic-range (HDR) imaging using spatially varying multiple exposures. Popular methods for multi-exposure array sensors involve varying the gain of each exposure, but this does not effectively change the photoelectronic energy in each exposure. Consequently, HDR images produced using gain-based exposure variation may suffer from noise and details being saturated. To address this problem, we intend to use time-varying exposures in quad-Bayer patterned sensors. This approach allows long-exposure pixels to receive more photon energy than short- or middle-exposure pixels, resulting in higher-quality HDR images. However, time-varying exposures are not ideal for dynamic scenes and require an additional deghosting method. To tackle this issue, we propose a single-shot HDR demosaicing method that takes time-varying multiple exposures as input and jointly solves both the demosaicing and deghosting problems. Our method uses a feature-extraction module to handle mosaiced multiple exposures and a multiscale transformer module to register spatial displacements of multiple exposures and colors. We also created a dataset of quad-Bayer sensor input with time-varying exposures and trained our network using this dataset. Results demonstrate that our method outperforms baseline HDR reconstruction methods with both synthetic and real datasets. With our method, we can achieve high-quality HDR images in challenging lighting conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungwoo Kim",
      "Min H. Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lu_Scene-Aware_Feature_Matching_ICCV_2023_paper.html": {
    "title": "Scene-Aware Feature Matching",
    "volume": "main",
    "abstract": "Current feature matching methods focus on point-level matching, pursuing better representation learning of individual features, but lacking further understanding of the scene. This results in significant performance degradation when handling challenging scenes such as scenes with large viewpoint and illumination changes. To tackle this problem, we propose a novel model named SAM, which applies attentional grouping to guide Scene-Aware feature Matching. SAM handles multi-level features, i.e., image tokens and group tokens, with attention layers, and groups the image tokens with the proposed token grouping module. Our model can be trained by ground-truth matches only and produce reasonable grouping results. With the sense-aware grouping guidance, SAM is not only more accurate and robust but also more interpretable than conventional feature matching models. Sufficient experiments on various applications, including homography estimation, pose estimation, and image matching, demonstrate that our model achieves state-of-the-art performance",
    "checked": true,
    "id": "66bfa4ffa21e947679a6ba737bfe5ad8b5b8720d",
    "semantic_title": "scene-aware feature matching",
    "citation_count": 0,
    "authors": [
      "Xiaoyong Lu",
      "Yaping Yan",
      "Tong Wei",
      "Songlin Du"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_FDViT_Improve_the_Hierarchical_Architecture_of_Vision_Transformer_ICCV_2023_paper.html": {
    "title": "FDViT: Improve the Hierarchical Architecture of Vision Transformer",
    "volume": "main",
    "abstract": "Despite the fact that transformer-based models have yielded great success in computer vision tasks, they suffer from the challenge of high computational costs that limits their use on resource-constrained devices. One major reason is that vision transformers have redundant calculations since the self-attention operation generates patches with high similarity at a later stage in the network. Hierarchical architectures have been proposed for vision transformers to alleviate this challenge. However, by shrinking the spatial dimensions to half of the originals with downsampling layers, the challenge is actually overcompensated, as too much information is lost. In this paper, we propose FDViT to improve the hierarchical architecture of the vision transformer by using a flexible downsampling layer that is not limited to integer stride to smoothly reduce the sizes of the middle feature maps. Furthermore, a masked auto-encoder architecture is used to facilitate the training of the proposed flexible downsampling layer and produces informative outputs. Experimental results on benchmark datasets demonstrate that the proposed method can reduce computational costs while increasing classification performance and achieving state-of-the-art results. For example, the proposed FDViT-S model achieves a top-1 accuracy of 81.5%, which is 1.7 percent points higher than the ViT-S model and reduces 39% FLOPs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixing Xu",
      "Chao Li",
      "Dong Li",
      "Xiao Sheng",
      "Fan Jiang",
      "Lu Tian",
      "Ashish Sirasao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Tuning_Pre-trained_Model_via_Moment_Probing_ICCV_2023_paper.html": {
    "title": "Tuning Pre-trained Model via Moment Probing",
    "volume": "main",
    "abstract": "Recently, efficient fine-tuning of large-scale pre-trained models has attracted increasing research interests, where linear probing (LP) as a fundamental module is involved in exploiting the final representations for task-dependent classification. However, most of the existing methods focus on how to effectively introduce a few of learnable parameters, and little work pays attention to the commonly used LP module. In this paper, we propose a novel Moment Probing (MP) method to further explore the potential of LP. Distinguished from LP which builds a linear classification head based on the mean of final features (e.g., word tokens for ViT) or classification tokens, our MP performs a linear classifier on feature distribution, which provides a stronger representation ability by exploiting richer statistical information inherent in features. Specifically, we represent feature distribution by its characteristic function, which is efficiently approximated by using first- and second-order moments of features. Furthermore, we propose a multi-head convolutional cross-covariance to compute second-order moments in an efficient and effective manner. By considering that MP could affect feature learning, we introduce a partially shared module to learn two recalibrating parameters (PSRP) for backbones based on MP, namely MP+. Extensive experiments on ten benchmarks using various models show that our MP significantly outperforms LP and is competitive with counterparts at less training cost, while our MP+ achieves state-of-the-art performance",
    "checked": true,
    "id": "0d40b812ffd9735d46a1a9a928b06f759fb54fea",
    "semantic_title": "tuning pre-trained model via moment probing",
    "citation_count": 0,
    "authors": [
      "Mingze Gao",
      "Qilong Wang",
      "Zhenyi Lin",
      "Pengfei Zhu",
      "Qinghua Hu",
      "Jingbo Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cao_Attention_Where_It_Matters_Rethinking_Visual_Document_Understanding_with_Selective_ICCV_2023_paper.html": {
    "title": "Attention Where It Matters: Rethinking Visual Document Understanding with Selective Region Concentration",
    "volume": "main",
    "abstract": "We propose a novel end-to-end document understanding model called SeRum (SElective Region Understanding Model) for extracting meaningful information from document images, including document analysis, retrieval, and office automation. Unlike state-of-the-art approaches that rely on multi-stage technical schemes and are computationally expensive, SeRum converts document image understanding and recognition tasks into a local decoding process of the vision tokens of interest, using a content-aware token merge module. This mechanism enables the model to pay more attention to regions of interest generated by the query decoder, improving the model's effectiveness and speeding up the decoding speed of the generative scheme. We also designed several pre-training tasks to enhance the understanding and local awareness of the model. Experimental results demonstrate that SeRum achieves state-of-the-art performance on document understanding tasks and competitive results on text spotting tasks. SeRum represents a substantial advancement towards enabling efficient and effective end-to-end document understanding",
    "checked": true,
    "id": "5c10ea9d53956ba919929233acb2c609bb673149",
    "semantic_title": "attention where it matters: rethinking visual document understanding with selective region concentration",
    "citation_count": 0,
    "authors": [
      "Haoyu Cao",
      "Changcun Bao",
      "Chaohu Liu",
      "Huang Chen",
      "Kun Yin",
      "Hao Liu",
      "Yinsong Liu",
      "Deqiang Jiang",
      "Xing Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ali_Task_Agnostic_Restoration_of_Natural_Video_Dynamics_ICCV_2023_paper.html": {
    "title": "Task Agnostic Restoration of Natural Video Dynamics",
    "volume": "main",
    "abstract": "In many video restoration/translation tasks, image processing operations are naively extended to the video domain by processing each frame independently, disregarding the temporal connection of the video frames. This disregard for the temporal connection often leads to severe temporal inconsistencies. State-Of-The-Art (SOTA) techniques that address these inconsistencies rely on the availability of unprocessed videos to implicitly siphon and utilize consistent video dynamics to restore the temporal consistency of frame-wise processed videos which often jeopardizes the translation effect. We propose a general framework for this task that learns to infer and utilize consistent motion dynamics from inconsistent videos to mitigate the temporal flicker while preserving the perceptual quality for both the temporally neighboring and relatively distant frames without requiring the raw videos at test time. The proposed framework produces SOTA results on two benchmark datasets, DAVIS and videvo.net, processed by numerous image processing applications. The code and the trained models will be open-sourced upon acceptance",
    "checked": true,
    "id": "d30c474a28f275bf61fbf5bab4283824092507b5",
    "semantic_title": "task agnostic restoration of natural video dynamics",
    "citation_count": 0,
    "authors": [
      "Muhammad Kashif Ali",
      "Dongjin Kim",
      "Tae Hyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Petrovich_TMR_Text-to-Motion_Retrieval_Using_Contrastive_3D_Human_Motion_Synthesis_ICCV_2023_paper.html": {
    "title": "TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis",
    "volume": "main",
    "abstract": "In this paper, we present TMR, a simple yet effective approach for text to 3D human motion retrieval. While previous work has only treated retrieval as a proxy evaluation metric, we tackle it as a standalone task. Our method extends the state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a contrastive loss to better structure the cross-modal latent space. We show that maintaining the motion generation loss, along with the contrastive training, is crucial to obtain good performance. We introduce a benchmark for evaluation and provide an in-depth analysis by reporting results on several protocols. Our extensive experiments on the KIT-ML and HumanML3D datasets show that TMR outperforms the prior work by a significant margin, for example reducing the median rank from 54 to 19. Finally, we showcase the potential of our approach on moment retrieval. Our code and models are publicly available at https://mathis.petrovich.fr/tmr",
    "checked": true,
    "id": "daec9129f0fe200493d204963cd1a71e640725cb",
    "semantic_title": "tmr: text-to-motion retrieval using contrastive 3d human motion synthesis",
    "citation_count": 6,
    "authors": [
      "Mathis Petrovich",
      "Michael J. Black",
      "GÃ¼l Varol"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_3D_Neural_Embedding_Likelihood_Probabilistic_Inverse_Graphics_for_Robust_6D_ICCV_2023_paper.html": {
    "title": "3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation",
    "volume": "main",
    "abstract": "The ability to perceive and understand 3D scenes is crucial for many applications in computer vision and robotics. Inverse graphics is an appealing approach to 3D scene understanding that aims to infer the 3D scene structure from 2D images. In this paper, we introduce probabilistic modeling to the inverse graphics framework to quantify uncertainty and achieve robustness in 6D pose estimation tasks. Specifically, we propose 3D Neural Embedding Likelihood (3DNEL) as a unified probabilistic model over RGB-D images, and develop efficient inference procedures on 3D scene descriptions. 3DNEL effectively combines learned neural embeddings from RGB with depth information to improve robustness in sim-to-real 6D object pose estimation from RGB-D images. Performance on the YCB-Video dataset is on par with state-of-the-art yet is much more robust in challenging regimes. In contrast to discriminative approaches, 3DNEL's probabilistic generative formulation jointly models multiple objects in a scene, quantifies uncertainty in a principled way, and handles object pose tracking under heavy occlusion. Finally, 3DNEL provides a principled framework for incorporating prior knowledge about the scene and objects, which allows natural extension to additional tasks like camera pose tracking from video",
    "checked": true,
    "id": "21b960a3aa68021ca997acc7761c004be37b1d89",
    "semantic_title": "3d neural embedding likelihood: probabilistic inverse graphics for robust 6d pose estimation",
    "citation_count": 0,
    "authors": [
      "Guangyao Zhou",
      "Nishad Gothoskar",
      "Lirui Wang",
      "Joshua B. Tenenbaum",
      "Dan Gutfreund",
      "Miguel LÃ¡zaro-Gredilla",
      "Dileep George",
      "Vikash K. Mansinghka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gan_Towards_Robust_Model_Watermark_via_Reducing_Parametric_Vulnerability_ICCV_2023_paper.html": {
    "title": "Towards Robust Model Watermark via Reducing Parametric Vulnerability",
    "volume": "main",
    "abstract": "Deep neural networks are valuable assets considering their commercial benefits and huge demands for costly annotation and computation resources. To protect the copyright of DNNs, backdoor-based ownership verification becomes popular recently, in which the model owner can watermark the model by embedding a specific backdoor behavior before releasing it. The defenders (usually the model owners) can identify whether a suspicious third-party model is \"stolen\" from them based on the presence of the behavior. Unfortunately, these watermarks are proven to be vulnerable to removal attacks even like fine-tuning. To further explore this vulnerability, we investigate the parametric space and find there exist many watermark-removed models in the vicinity of the watermarked one, which may be easily used by removal attacks. Inspired by this finding, we propose a minimax formulation to find these watermark-removed models and recover their watermark behavior. Extensive experiments demonstrate that our method improves the robustness of the model watermarking against parametric changes and numerous watermark-removal attacks. The codes for reproducing our main experiments are available at https://github.com/GuanhaoGan/robust-model-watermarking",
    "checked": true,
    "id": "2d57ea1bca6819a08f22b3664b0225a924d9703a",
    "semantic_title": "towards robust model watermark via reducing parametric vulnerability",
    "citation_count": 4,
    "authors": [
      "Guanhao Gan",
      "Yiming Li",
      "Dongxian Wu",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qin_SupFusion_Supervised_LiDAR-Camera_Fusion_for_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "SupFusion: Supervised LiDAR-Camera Fusion for 3D Object Detection",
    "volume": "main",
    "abstract": "LiDAR-Camera fusion-based 3D detection is a critical task for automatic driving. In recent years, many LiDAR-Camera fusion approaches sprung up and gained promising performances compared with single-modal detectors, but always lack carefully designed and effective supervision for the fusion process. In this paper, we propose a novel training strategy called SupFusion, which provides an auxiliary feature level supervision for effective LiDAR-Camera fusion and significantly boosts detection performance. Our strategy involves a data enhancement method named Polar Sampling, which densifies sparse objects and trains an assistant model to generate high-quality features as the supervision. These features are then used to train the LiDAR-Camera fusion model, where the fusion feature is optimized to simulate the generated high-quality features. Furthermore, we propose a simple yet effective deep fusion module, which contiguously gains superior performance compared with previous fusion methods with SupFusion strategy. In such a manner, our proposal shares the following advantages. Firstly, SupFusion introduces auxiliary feature-level supervision which could boost LiDAR-Camera detection performance without introducing extra inference costs. Secondly, the proposed deep fusion could continuously improve the detector's abilities. Our proposed SupFusion and deep fusion module is plug-and-play, we make extensive experiments to demonstrate its effectiveness. Specifically, we gain around 2% 3D mAP improvements on KITTI benchmark based on multiple LiDAR-Camera 3D detectors. Our code is available at https://github.com/IranQin/SupFusion",
    "checked": true,
    "id": "26ae44413959791950782ad44a02a6d3d1a42283",
    "semantic_title": "supfusion: supervised lidar-camera fusion for 3d object detection",
    "citation_count": 0,
    "authors": [
      "Yiran Qin",
      "Chaoqun Wang",
      "Zijian Kang",
      "Ningning Ma",
      "Zhen Li",
      "Ruimao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tan_EMMN_Emotional_Motion_Memory_Network_for_Audio-driven_Emotional_Talking_Face_ICCV_2023_paper.html": {
    "title": "EMMN: Emotional Motion Memory Network for Audio-driven Emotional Talking Face Generation",
    "volume": "main",
    "abstract": "Synthesizing expression is essential to create realistic talking faces. Previous works consider expressions and mouth shapes as a whole and predict them solely from audio inputs. However, the limited information contained in audio, such as phonemes and coarse emotion embedding, may not be suitable as the source of elaborate expressions. Besides, since expressions are tightly coupled to lip motions, generating expression from other sources is tricky and always neglects expression performed on mouth region, leading to inconsistency between them. To tackle the issues, this paper proposes Emotional Motion Memory Net (EMMN) that synthesizes expression overall on the talking face via emotion embedding and lip motion instead of the sole audio. Specifically, we extract emotion embedding from audio and design Motion Reconstruction module to decompose ground truth videos into mouth features and expression features before training, where the latter encode all facial factors about expression. During training, the emotion embedding and mouth features are used as keys, and the corresponding expression features are used as values to create key-value pairs stored in the proposed Motion Memory Net. Hence, once the audio-relevant mouth features and emotion embedding are individually predicted from audio at inference time, we treat them as a query to retrieve the best-matching expression features, performing expression overall on the face and thus avoiding inconsistent results. Extensive experiments demonstrate that our method can generate high-quality talking face videos with accurate lip movements and vivid expressions on unseen subjects",
    "checked": false,
    "id": "6e5252b1c70c92b917e5c29dc0618c940a9d22fb",
    "semantic_title": "continuously controllable facial expression editing in talking face videos",
    "citation_count": 2,
    "authors": [
      "Shuai Tan",
      "Bin Ji",
      "Ye Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Rethinking_Vision_Transformers_for_MobileNet_Size_and_Speed_ICCV_2023_paper.html": {
    "title": "Rethinking Vision Transformers for MobileNet Size and Speed",
    "volume": "main",
    "abstract": "With the success of Vision Transformers (ViTs) in computer vision tasks, recent arts try to optimize the performance and complexity of ViTs to enable efficient deployment on mobile devices. Multiple approaches are proposed to accelerate attention mechanism, improve inefficient designs, or incorporate mobile-friendly lightweight convolutions to form hybrid architectures. However, ViT and its variants still have higher latency or considerably more parameters than lightweight CNNs, even true for the years-old MobileNet. In practice, latency and size are both crucial for efficient deployment on resource-constraint hardware. In this work, we investigate a central question, can transformer models run as fast as MobileNet and maintain a similar size? We revisit the design choices of ViTs and propose a novel supernet with low latency and high parameter efficiency. We further introduce a novel fine-grained joint search strategy for transformer models that can find efficient architectures by optimizing latency and number of parameters simultaneously. The proposed models, EfficientFormerV2, achieve 3.5% higher top-1 accuracy than MobileNetV2 on ImageNet-1K with similar latency and parameters. This work demonstrate that properly designed and optimized vision transformers can achieve high performance even with MobileNet-level size and speed",
    "checked": true,
    "id": "f35016b3180808fa97d59acbdecf47d6e2ed2819",
    "semantic_title": "rethinking vision transformers for mobilenet size and speed",
    "citation_count": 20,
    "authors": [
      "Yanyu Li",
      "Ju Hu",
      "Yang Wen",
      "Georgios Evangelidis",
      "Kamyar Salahi",
      "Yanzhi Wang",
      "Sergey Tulyakov",
      "Jian Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hong_Implicit_Identity_Representation_Conditioned_Memory_Compensation_Network_for_Talking_Head_ICCV_2023_paper.html": {
    "title": "Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation",
    "volume": "main",
    "abstract": "Talking head video generation aims to animate a human face in a still image with dynamic poses and expressions using motion information derived from a target-driving video, while maintaining the person's identity in the source image. However, dramatic and complex motions in the driving video cause ambiguous generation, because the still source image cannot provide sufficient appearance information for occluded regions or delicate expression variations, which produces severe artifacts and significantly degrades the generation quality. To tackle this problem, we propose to learn a global facial representation space, and design a novel implicit identity representation conditioned memory compensation network, coined as MCNet, for high-fidelity talking head generation. Specifically, we devise a network module to learn a unified spatial facial meta-memory bank from all training samples, which can provide rich facial structure and appearance priors to compensate warped source facial features for the generation. Furthermore, we propose an effective query mechanism based on implicit identity representations learned from the discrete keypoints of the source image. It can greatly facilitate the retrieval of more correlated information from the memory bank for the compensation. Extensive experiments demonstrate that MCNet can learn representative and complementary facial memory, and can clearly outperform previous state-of-the-art talking head generation methods on VoxCeleb1 and CelebV datasets",
    "checked": true,
    "id": "27c667bf7bc83fcb07bd539e7a29eabdfb539b2c",
    "semantic_title": "implicit identity representation conditioned memory compensation network for talking head video generation",
    "citation_count": 0,
    "authors": [
      "Fa-Ting Hong",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_SINC_Self-Supervised_In-Context_Learning_for_Vision-Language_Tasks_ICCV_2023_paper.html": {
    "title": "SINC: Self-Supervised In-Context Learning for Vision-Language Tasks",
    "volume": "main",
    "abstract": "Large Pre-trained Transformers exhibit an intriguing capacity for in-context learning. Without gradient updates, these models can rapidly construct new predictors from demonstrations presented in the inputs. Recent works promote this ability in the vision-language domain by incorporating visual information into large language models that can already make in-context predictions. However, these methods could inherit issues in the language domain, such as template sensitivity and hallucination. Also, the scale of these language models raises a significant demand for computations, making learning and operating these models resource-intensive. To this end, we raise a question: \"How can we enable in-context learning without relying on the intrinsic in-context ability of large language models?\". To answer it, we propose a succinct and general framework, Self-supervised IN-Context learning (SINC), that introduces a meta-model to learn on self-supervised prompts consisting of tailored demonstrations. The learned models can be transferred to downstream tasks for making in-context predictions on-the-fly. Extensive experiments show that SINC outperforms gradient-based methods in various vision-language tasks under few-shot settings. Furthermore, the designs of SINC help us investigate the benefits of in-context learning across different tasks, and the analysis further reveals the essential components for the emergence of in-context learning in the vision-language domain",
    "checked": true,
    "id": "7fc133b3a61e88338ae15a2bf72f08fdc2beb504",
    "semantic_title": "sinc: self-supervised in-context learning for vision-language tasks",
    "citation_count": 0,
    "authors": [
      "Yi-Syuan Chen",
      "Yun-Zhu Song",
      "Cheng Yu Yeo",
      "Bei Liu",
      "Jianlong Fu",
      "Hong-Han Shuai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qian_LEA2_A_Lightweight_Ensemble_Adversarial_Attack_via_Non-overlapping_Vulnerable_Frequency_ICCV_2023_paper.html": {
    "title": "LEA2: A Lightweight Ensemble Adversarial Attack via Non-overlapping Vulnerable Frequency Regions",
    "volume": "main",
    "abstract": "Recent work shows that well-designed adversarial examples can fool deep neural networks (DNNs). Due to their transferability, adversarial examples can also attack target models without extra information, called black-box attacks. However, most existing ensemble attacks depend on numerous substitute models to cover the vulnerable subspace of a target model. In this work, we find three types of models with non-overlapping vulnerable frequency regions, which can cover a large enough vulnerable subspace. Based on this finding, we propose a lightweight ensemble adversarial attack named LEA2, integrated by standard, weakly robust, and robust models. Moreover, we analyze Gaussian noise from the perspective of frequency and find that Gaussian noise is located in the vulnerable frequency regions of standard models. Therefore, we substitute standard models with Gaussian noise to ensure the use of high-frequency vulnerable regions while reducing attack time consumption. Experiments on several image datasets indicate that LEA^2 achieves better transferability under different defended models compared with extensive baselines and state-of-the-art attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaguan Qian",
      "Shuke He",
      "Chenyu Zhao",
      "Jiaqiang Sha",
      "Wei Wang",
      "Bin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Chupa_Carving_3D_Clothed_Humans_from_Skinned_Shape_Priors_using_ICCV_2023_paper.html": {
    "title": "Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "We propose a 3D generation pipeline that uses diffusion models to generate realistic human digital avatars. Due to the wide variety of human identities, poses, and stochastic details, the generation of 3D human meshes has been a challenging problem. To address this, we decompose the problem into 2D normal map generation and normal map-based 3D reconstruction. Specifically, we first simultaneously generate realistic normal maps for the front and backside of a clothed human, dubbed dual normal maps, using a pose-conditional diffusion model. For 3D reconstruction, we \"carve\" the prior SMPL-X mesh to a detailed 3D mesh according to the normal maps through mesh optimization. To further enhance the high-frequency details, we present a diffusion resampling scheme on both body and facial regions, thus encouraging the generation of realistic digital avatars. We also seamlessly incorporate a recent text-to-image diffusion model to support text-based human identity control. Our method, namely, Chupa, is capable of generating realistic 3D clothed humans with better perceptual quality and identity variety",
    "checked": true,
    "id": "fbbc507dd88fd1aa55e531f4a66d69f80880d779",
    "semantic_title": "chupa: carving 3d clothed humans from skinned shape priors using 2d diffusion probabilistic models",
    "citation_count": 2,
    "authors": [
      "Byungjun Kim",
      "Patrick Kwon",
      "Kwangho Lee",
      "Myunggi Lee",
      "Sookwan Han",
      "Daesik Kim",
      "Hanbyul Joo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Unsupervised_Domain_Adaptive_Detection_with_Network_Stability_Analysis_ICCV_2023_paper.html": {
    "title": "Unsupervised Domain Adaptive Detection with Network Stability Analysis",
    "volume": "main",
    "abstract": "Domain adaptive detection aims to improve the generality of a detector, learned from the labeled source domain, on the unlabeled target domain. In this work, drawing inspiration from the concept of stability from the control theory that a robust system requires to remain consistent both externally and internally regardless of disturbances, we propose a novel framework that achieves unsupervised domain adaptive detection through stability analysis. In specific, we treat discrepancies between images and regions from different domains as disturbances, and introduce a novel simple but effective Network Stability Analysis (NSA) framework that considers various disturbances for domain adaptation. Particularly, we explore three types of perturbations including heavy and light image-level disturbances and instance-level disturbance. For each type, NSA performs external consistency analysis on the outputs from raw and perturbed images and/or internal consistency analysis on their features, using teacher-student models. By integrating NSA into Faster R-CNN, we immediately achieve state-of-the-art results. In particular, we set a new record of 52.7% mAP on Cityscapes-to-FoggyCityscapes, showing the potential of NSA for domain adaptive detection. It is worth noticing, our NSA is designed for general purpose, and thus applicable to one-stage detection model (e.g., FCOS) besides the adopted one, as shown by experiments. Code is released at https://github.com/tiankongzhang/NSA",
    "checked": true,
    "id": "3666865f3eae5cf3d4802a9da531a5d4373aa8c5",
    "semantic_title": "unsupervised domain adaptive detection with network stability analysis",
    "citation_count": 0,
    "authors": [
      "Wenzhang Zhou",
      "Heng Fan",
      "Tiejian Luo",
      "Libo Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lyu_Learning_a_Room_with_the_Occ-SDF_Hybrid_Signed_Distance_Function_ICCV_2023_paper.html": {
    "title": "Learning a Room with the Occ-SDF Hybrid: Signed Distance Function Mingled with Occupancy Aids Scene Representation",
    "volume": "main",
    "abstract": "Implicit neural rendering, using signed distance function (SDF) representation with geometric priors like depth or surface normal, has made impressive strides in the surface reconstruction of large-scale scenes. However, applying this method to reconstruct a room-level scene from images may miss structures in low-intensity areas and/or small, thin objects. We have conducted experiments on three datasets to identify limitations of the original color rendering loss and priors-embedded SDF scene representation.Our findings show that the color rendering loss creates an optimization bias against low-intensity areas, resulting in gradient vanishing and leaving these areas unoptimized. To address this issue, we propose a feature-based color rendering loss that utilizes non-zero feature values to bring back optimization signals. Additionally, the SDF representation can be influenced by objects along a ray path, disrupting the monotonic change of SDF values when a single object is present. Accordingly, we explore using the occupancy representation, which encodes each point separately and is unaffected by objects along a querying ray. Our experimental results demonstrate that the joint forces of the feature-based rendering loss and Occ-SDF hybrid representation scheme can provide high-quality reconstruction results, especially in challenging room-level scenarios. The code is available at https://github.com/shawLyu/Occ-SDF_Hybrid",
    "checked": true,
    "id": "d4bc70f71a7baa01b37f043263df952f9aee303e",
    "semantic_title": "learning a room with the occ-sdf hybrid: signed distance function mingled with occupancy aids scene representation",
    "citation_count": 0,
    "authors": [
      "Xiaoyang Lyu",
      "Peng Dai",
      "Zizhang Li",
      "Dongyu Yan",
      "Yi Lin",
      "Yifan Peng",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dai_Cloth2Body_Generating_3D_Human_Body_Mesh_from_2D_Clothing_ICCV_2023_paper.html": {
    "title": "Cloth2Body: Generating 3D Human Body Mesh from 2D Clothing",
    "volume": "main",
    "abstract": "In this paper, we define and study a new Cloth2Body problem which has a goal of generating 3d human body meshes from a 2D clothing image. Unlike the existing human mesh recovery problem, Cloth2Body needs to address new and emerging challenges raised by the partial observation of the input and the high diversity of the output. Indeed, there are three specific challenges. First, how to locate and pose human bodies into the clothes. Second, how to effectively estimate body shapes out of various clothing types. Finally, how to generate diverse and plausible results from a 2D clothing image. To this end, we propose an end-to-end framework that can accurately estimate 3D body mesh parameterized by pose and shape from a 2D clothing image. Along this line, we first utilize Kinematics-aware Pose Estimation to estimate body pose parameters. 3D skeleton is employed as a proxy followed by an inverse kinematics module to boost the estimation accuracy. We additionally design an adaptive depth trick to align the re-projected 3D mesh better with 2D clothing image by disentangling the effects of object size and camera extrinsic. Next, we propose Physics-informed Shape Estimation to estimate body shape parameters. 3D shape parameters are predicted based on partial body measurements estimated from RGB image, which not only improves pixel-wise human-cloth alignment, but also enables flexible user editing. Finally, we design Evolution based pose generation method , a skeleton transplanting method inspired by genetic algorithms to generate diverse reasonable poses during inference. As shown by experimental results on both synthetic and real-world data, the proposed framework achieves state-of-the-art performance and can effectively recover natural and diverse 3D body meshes from 2D images that align well with clothing",
    "checked": true,
    "id": "c48ea351df004dc6a97c04f203fd7d09c70299b4",
    "semantic_title": "cloth2body: generating 3d human body mesh from 2d clothing",
    "citation_count": 0,
    "authors": [
      "Lu Dai",
      "Liqian Ma",
      "Shenhan Qian",
      "Hao Liu",
      "Ziwei Liu",
      "Hui Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Spatially_and_Spectrally_Consistent_Deep_Functional_Maps_ICCV_2023_paper.html": {
    "title": "Spatially and Spectrally Consistent Deep Functional Maps",
    "volume": "main",
    "abstract": "Cycle consistency has long been exploited as a powerful prior for jointly optimizing maps within a collection of shapes. In this paper, we investigate its utility in the approaches of Deep Functional Maps, which are considered state-of-the-art in non-rigid shape matching. We first justify that under certain conditions, the learned maps, when represented in the spectral domain, are already cycle consistent. Furthermore, we identify the discrepancy that spectrally consistent maps are not necessarily spatially, or point-wise, consistent. In light of this, we present a novel design of unsupervised Deep Functional Maps, which effectively enforces the harmony of learned maps under the spectral and the point-wise representation. By taking advantage of cycle consistency, our framework produces state-of-the-art results in mapping shapes even under significant distortions. Beyond that, by independently estimating maps in both spectral and spatial domains, our method naturally alleviates over-fitting in network training, yielding superior generalization performance and accuracy within an array of challenging tests for both near-isometric and non-isometric datasets",
    "checked": true,
    "id": "fd8c2fe20829982cc450bc39b563d1703da117f6",
    "semantic_title": "spatially and spectrally consistent deep functional maps",
    "citation_count": 0,
    "authors": [
      "Mingze Sun",
      "Shiwei Mao",
      "Puhua Jiang",
      "Maks Ovsjanikov",
      "Ruqi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yao_Sparse_Point_Guided_3D_Lane_Detection_ICCV_2023_paper.html": {
    "title": "Sparse Point Guided 3D Lane Detection",
    "volume": "main",
    "abstract": "3D lane detection usually builds a dense correspondence between the front-view space and the BEV space to estimate lane points in the 3D space. 3D lanes only occupy a small ratio of the dense correspondence, while most correspondence belongs to the redundant background. This sparsity phenomenon bottlenecks valuable computation and raises the computation cost of building a high-resolution correspondence for accurate results. In this paper, we propose a sparse point-guided 3D lane detection, focusing on points related to 3D lanes. Our method runs in a coarse-to-fine manner, including coarse-level lane detection and iterative fine-level sparse point refinements. In coarse-level lane detection, we build a dense but efficient correspondence between the front view and BEV space at a very low resolution to compute coarse lanes. Then in fine-level sparse point refinement, we sample sparse points around coarse lanes to extract local features from the high-resolution front-view feature map. The high-resolution local information brought by sparse points refines 3D lanes in the BEV space hierarchically from low resolution to high resolution. The sparse point guides a more effective information flow and greatly promotes the SOTA result by 3 points on the overall F1-score and 6 points on several hard situations while reducing almost half memory cost and speeding up 2 times",
    "checked": false,
    "id": "f24b71092b94c5fca56556fec789be55bc7c6caf",
    "semantic_title": "structure guided lane detection",
    "citation_count": 41,
    "authors": [
      "Chengtang Yao",
      "Lidong Yu",
      "Yuwei Wu",
      "Yunde Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ponghiran_Event-based_Temporally_Dense_Optical_Flow_Estimation_with_Sequential_Learning_ICCV_2023_paper.html": {
    "title": "Event-based Temporally Dense Optical Flow Estimation with Sequential Learning",
    "volume": "main",
    "abstract": "Event cameras provide an advantage over traditional frame-based cameras when capturing fast-moving objects without a motion blur. They achieve this by recording changes in light intensity (known as events), thus allowing them to operate at a much higher frequency and making them suitable for capturing motions in a highly dynamic scene. Many recent studies have proposed methods to train neural networks (NNs) for predicting optical flow from events. However, they often rely on a spatio-temporal representation constructed from events over a fixed interval, such as 10Hz used in training on the DSEC dataset. This limitation restricts the flow prediction to the same interval (10Hz) whereas the fast speed of event cameras, which can operate up to 3kHz, has not been effectively utilized. In this work, we show that a temporally dense flow estimation at 100Hz can be achieved by treating the flow estimation as a sequential problem using two different variants of recurrent networks - Long-short term memory (LSTM) and spiking neural network (SNN). First, We utilize the NN model constructed similar to the popular EV-FlowNet but with LSTM layers to demonstrate the efficiency of our training method. The model not only produces 10x more frequent optical flow than the existing ones, but the estimated flows also have 13% lower errors than predictions from the baseline EV-FlowNet. Second, we construct an EV-FlowNet SNN but with leaky integrate and fire neurons to efficiently capture the temporal dynamics. We found that simple inherent recurrent dynamics of SNN lead to significant parameter reduction compared to the LSTM model. In addition, because of its event-driven computation, the spiking model is estimated to consume only 1.5% energy of the LSTM model, highlighting the efficiency of SNN in processing events and the potential for achieving temporally dense flow",
    "checked": true,
    "id": "727b9f0046c2aa25a175cfa429e77d145157ab4c",
    "semantic_title": "event-based temporally dense optical flow estimation with sequential learning",
    "citation_count": 0,
    "authors": [
      "Wachirawit Ponghiran",
      "Chamika Mihiranga Liyanagedera",
      "Kaushik Roy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cascante-Bonilla_Going_Beyond_Nouns_With_Vision__Language_Models_Using_Synthetic_ICCV_2023_paper.html": {
    "title": "Going Beyond Nouns With Vision & Language Models Using Synthetic Data",
    "volume": "main",
    "abstract": "Large-scale pre-trained Vision & Language (VL) models have shown remarkable performance in many applications, enabling replacing a fixed set of supported classes with zero-shot open vocabulary reasoning over (almost arbitrary) natural language prompts. However, recent works have uncovered a fundamental weakness of these models. For example, their difficulty to understand Visual Language Concepts (VLC) that go 'beyond nouns' such as the meaning of non-object words (e.g., attributes, actions, relations, states, etc.), or difficulty in performing compositional reasoning such as understanding the significance of the order of the words in a sentence. In this work, we investigate to which extent purely synthetic data could be leveraged to teach these models to overcome such shortcomings without compromising their zero-shot capabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable data to improve VLC understanding and compositional reasoning of VL models. Additionally, we propose a general VL finetuning strategy for effectively leveraging SyViC towards achieving these improvements. Our extensive experiments and ablations on VL-Checklist, Winoground, and ARO benchmarks demonstrate that it is possible to adapt strong pre-trained VL models with synthetic data significantly enhancing their VLC understanding (e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their zero-shot accuracy",
    "checked": true,
    "id": "73e57768aa115e376cd97542f2f7982a4864c2b3",
    "semantic_title": "going beyond nouns with vision & language models using synthetic data",
    "citation_count": 8,
    "authors": [
      "Paola Cascante-Bonilla",
      "Khaled Shehada",
      "James Seale Smith",
      "Sivan Doveh",
      "Donghyun Kim",
      "Rameswar Panda",
      "Gul Varol",
      "Aude Oliva",
      "Vicente Ordonez",
      "Rogerio Feris",
      "Leonid Karlinsky"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Continual_Zero-Shot_Learning_through_Semantically_Guided_Generative_Random_Walks_ICCV_2023_paper.html": {
    "title": "Continual Zero-Shot Learning through Semantically Guided Generative Random Walks",
    "volume": "main",
    "abstract": "Learning novel concepts, remembering previous knowledge, and adapting it to future tasks occur simultaneously throughout a human's lifetime. To model such comprehensive abilities, continual zero-shot learning (CZSL) has recently been introduced. However, most existing methods overused the unseen semantic information that may not be continually accessible in realistic settings. In this paper, we address the challenge of continual zero-shot learning where unseen information is not provided during training, by leveraging generative modeling. The heart of the generative-based methods is to learn quality representations from seen classes to improve the generative understanding of the unseen visual space. Motivated by this, we introduce generalization-bound tools and provide the first theoretical explanation for the benefits of generative modeling to CZSL tasks. Guided by the theoretical analysis, we then propose our learning algorithm that employs a novel semantically guided Generative Random Walk (GRW) loss. The GRW loss augments the training by continually encouraging the model to generate realistic and characterized samples to represent the unseen space. Our algorithm achieves state-of-the-art performance on AWA1, AWA2, CUB, and SUN datasets, surpassing existing CZSL methods by 3-7%. The code is available here https://github.com/wx-zhang/IGCZSL",
    "checked": true,
    "id": "c458a723db6cd597aff7b717909491e00c43516d",
    "semantic_title": "continual zero-shot learning through semantically guided generative random walks",
    "citation_count": 0,
    "authors": [
      "Wenxuan Zhang",
      "Paul Janson",
      "Kai Yi",
      "Ivan Skorokhodov",
      "Mohamed Elhoseiny"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Foreground-Background_Distribution_Modeling_Transformer_for_Visual_Object_Tracking_ICCV_2023_paper.html": {
    "title": "Foreground-Background Distribution Modeling Transformer for Visual Object Tracking",
    "volume": "main",
    "abstract": "Visual object tracking is a fundamental research topic with a broad range of applications. Benefiting from the rapid development of Transformer, pure Transformer trackers have achieved great progress. However, the feature learning of these Transformer-based trackers is easily disturbed by complex backgrounds. To address the above limitations, we propose a novel foreground-background distribution modeling transformer for visual object tracking (F-BDMTrack), including a fore-background agent learning (FBAL) module and a distribution-aware attention (DA2) module in a unified transformer architecture. The proposed F-BDMTrack enjoys several merits. First, the proposed FBAL module can effectively mine fore-background information with designed fore-background agents. Second, the DA2 module can suppress the incorrect interaction between foreground and background by modeling fore-background distribution similarities. Finally, F-BDMTrack can extract discriminative features under ever-changing tracking scenarios for more accurate target state estimation. Extensive experiments show that our F-BDMTrack outperforms previous state-of-the-art trackers on eight tracking benchmarks",
    "checked": false,
    "id": "83cbefd67569886c05aca1e350531869f8f399fa",
    "semantic_title": "causal attention for unbiased visual recognition",
    "citation_count": 56,
    "authors": [
      "Dawei Yang",
      "Jianfeng He",
      "Yinchao Ma",
      "Qianjin Yu",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ding_MeViS_A_Large-scale_Benchmark_for_Video_Segmentation_with_Motion_Expressions_ICCV_2023_paper.html": {
    "title": "MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions",
    "volume": "main",
    "abstract": "This paper strives for motion expressions guided video segmentation, which focuses on segmenting objects in video content based on a sentence describing the motion of the objects. Existing referring video object datasets typically focus on salient objects and use language expressions that contain excessive static attributes that could potentially enable the target object to be identified in a single frame. These datasets downplay the importance of motion in video content for language-guided video object segmentation. To investigate the feasibility of using motion expressions to ground and segment objects in videos, we propose a large-scale dataset called MeViS, which contains numerous motion expressions to indicate target objects in complex environments. We benchmarked 5 existing referring video object segmentation (RVOS) methods and conducted a comprehensive comparison on the MeViS dataset. The results show that current RVOS methods cannot effectively address motion expression-guided video segmentation. We further analyze the challenges and propose a baseline approach for the proposed MeViS dataset. The goal of our benchmark is to provide a platform that enables the development of effective language-guided video segmentation algorithms that leverage motion expressions as a primary cue for object segmentation in complex video scenes. The proposed MeViS dataset has been released at https://henghuiding.github.io/MeViS",
    "checked": true,
    "id": "1798c7cc0351957dd1f9551c2c8ddec5a98a25a1",
    "semantic_title": "mevis: a large-scale benchmark for video segmentation with motion expressions",
    "citation_count": 3,
    "authors": [
      "Henghui Ding",
      "Chang Liu",
      "Shuting He",
      "Xudong Jiang",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_OPERA_Omni-Supervised_Representation_Learning_with_Hierarchical_Supervisions_ICCV_2023_paper.html": {
    "title": "OPERA: Omni-Supervised Representation Learning with Hierarchical Supervisions",
    "volume": "main",
    "abstract": "The pretrain-finetune paradigm in modern computer vision facilitates the success of self-supervised learning, which tends to achieve better transferability than supervised learning. However, with the availability of massive labeled data, a natural question emerges: how to train a better model with both self and full supervision signals? In this paper, we propose Omni-suPErvised Representation leArning with hierarchical supervisions (OPERA) as a solution. We provide a unified perspective of supervisions from labeled and unlabeled data and propose a unified framework of fully supervised and self-supervised learning. We extract a set of hierarchical proxy representations for each image and impose self and full supervisions on the corresponding proxy representations. Extensive experiments on both convolutional neural networks and vision transformers demonstrate the superiority of OPERA in image classification, segmentation, and object detection",
    "checked": true,
    "id": "7ea46df5856f589a174c98d84a5057e3d2fde09d",
    "semantic_title": "opera: omni-supervised representation learning with hierarchical supervisions",
    "citation_count": 1,
    "authors": [
      "Chengkun Wang",
      "Wenzhao Zheng",
      "Zheng Zhu",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_GPFL_Simultaneously_Learning_Global_and_Personalized_Feature_Information_for_Personalized_ICCV_2023_paper.html": {
    "title": "GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning (FL) is popular for its privacy-preserving and collaborative learning capabilities. Recently, personalized FL (pFL) has received attention for its ability to address statistical heterogeneity and achieve personalization in FL. However, from the perspective of feature extraction, most existing pFL methods only focus on extracting global or personalized feature information during local training, which fails to meet the collaborative learning and personalization goals of pFL. To address this, we propose a new pFL method, named GPFL, to simultaneously learn global and personalized feature information on each client. We conduct extensive experiments on six datasets in three statistically heterogeneous settings and show the superiority of GPFL over ten state-of-the-art methods regarding effectiveness, scalability, fairness, stability, and privacy. Besides, GPFL mitigates overfitting and outperforms the baselines by up to 8.99% in accuracy",
    "checked": true,
    "id": "1b2141be33ce12d8e9e683b73c199b413793ac7f",
    "semantic_title": "gpfl: simultaneously learning global and personalized feature information for personalized federated learning",
    "citation_count": 0,
    "authors": [
      "Jianqing Zhang",
      "Yang Hua",
      "Hao Wang",
      "Tao Song",
      "Zhengui Xue",
      "Ruhui Ma",
      "Jian Cao",
      "Haibing Guan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Zero-Shot_Contrastive_Loss_for_Text-Guided_Diffusion_Image_Style_Transfer_ICCV_2023_paper.html": {
    "title": "Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer",
    "volume": "main",
    "abstract": "Diffusion models have shown great promise in text-guided image style transfer, but there is a trade-off between style transformation and content preservation due to their stochastic nature. Existing methods require computationally expensive fine-tuning of diffusion models or additional neural network. To address this, here we propose a zero-shot contrastive loss for diffusion models that doesn't require additional fine-tuning or auxiliary networks. By leveraging patch-wise contrastive loss between generated samples and original image embeddings in the pre-trained diffusion model, our method can generate images with the same semantic content as the source image in a zero-shot manner. Our approach outperforms existing methods while preserving content and requiring no additional training, not only for image style transfer but also for image-to-image translation and manipulation. Our experimental results validate the effectiveness of our proposed method",
    "checked": true,
    "id": "bc443a2c19e44f11d96d76f180748868e602e749",
    "semantic_title": "zero-shot contrastive loss for text-guided diffusion image style transfer",
    "citation_count": 7,
    "authors": [
      "Serin Yang",
      "Hyunmin Hwang",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Efficient_Region-Aware_Neural_Radiance_Fields_for_High-Fidelity_Talking_Portrait_Synthesis_ICCV_2023_paper.html": {
    "title": "Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis",
    "volume": "main",
    "abstract": "This paper presents ER-NeRF, a novel conditional Neural Radiance Fields (NeRF) based architecture for talking portrait synthesis that can concurrently achieve fast convergence, real-time rendering, and state-of-the-art performance with small model size. Our idea is to explicitly exploit the unequal contribution of spatial regions to guide talking portrait modeling. Specifically, to improve the accuracy of dynamic head reconstruction, a compact and expressive NeRF-based Tri-Plane Hash Representation is introduced by pruning empty spatial regions with three planar hash encoders. For speech audio, we propose a Region Attention Module to generate region-aware condition feature via an attention mechanism. Different from existing methods that utilize an MLP-based encoder to learn the cross-modal relation implicitly, the attention mechanism builds an explicit connection between audio features and spatial regions to capture the priors of local motions. Moreover, a direct and fast Adaptive Pose Encoding is introduced to optimize the head-torso separation problem by mapping the complex transformation of the head pose into spatial coordinates. Extensive experiments demonstrate that our method renders better high-fidelity and audio-lips synchronized talking portrait videos, with realistic details and high efficiency compared to previous methods",
    "checked": true,
    "id": "39c25a2b5c54bbf5cd7e0882437fa35cb81a3b29",
    "semantic_title": "efficient region-aware neural radiance fields for high-fidelity talking portrait synthesis",
    "citation_count": 2,
    "authors": [
      "Jiahe Li",
      "Jiawei Zhang",
      "Xiao Bai",
      "Jun Zhou",
      "Lin Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Roessle_End2End_Multi-View_Feature_Matching_with_Differentiable_Pose_Optimization_ICCV_2023_paper.html": {
    "title": "End2End Multi-View Feature Matching with Differentiable Pose Optimization",
    "volume": "main",
    "abstract": "Erroneous feature matches have severe impact on subsequent camera pose estimation and often require additional, time-costly measures, like RANSAC, for outlier rejection. Our method tackles this challenge by addressing feature matching and pose optimization jointly. To this end, we propose a graph attention network to predict image correspondences along with confidence weights. The resulting matches serve as weighted constraints in a differentiable pose estimation. Training feature matching with gradients from pose optimization naturally learns to down-weight outliers and boosts pose estimation on image pairs compared to SuperGlue by 6.7% on ScanNet. At the same time, it reduces the pose estimation time by over 50% and renders RANSAC iterations unnecessary. Moreover, we integrate information from multiple views by spanning the graph across multiple frames to predict the matches all at once. Multi-view matching combined with end-to-end training improves the pose estimation metrics on Matterport3D by 18.5% compared to SuperGlue",
    "checked": true,
    "id": "3203459648621099a7bf250645bb89c8e6143897",
    "semantic_title": "end2end multi-view feature matching with differentiable pose optimization",
    "citation_count": 6,
    "authors": [
      "Barbara Roessle",
      "Matthias NieÃner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Low-Light_Image_Enhancement_with_Illumination-Aware_Gamma_Correction_and_Complete_Image_ICCV_2023_paper.html": {
    "title": "Low-Light Image Enhancement with Illumination-Aware Gamma Correction and Complete Image Modelling Network",
    "volume": "main",
    "abstract": "This paper presents a novel network structure with illumination-aware gamma correction and complete image modelling to solve the low-light image enhancement problem. Low-light environments usually lead to less informative large-scale dark areas, directly learning deep representations from low-light images is insensitive to recovering normal illumination. We propose to integrate the effectiveness of gamma correction with the strong modelling capacities of deep networks, which enables the correction factor gamma to be learned in a coarse to elaborate manner via adaptively perceiving the deviated illumination. Because exponential operation introduces high computational complexity, we propose to use Taylor Series to approximate gamma correction, accelerating the training and inference speed. Dark areas usually occupy large scales in low-light images, common local modelling structures, e.g., CNN, SwinIR, are thus insufficient to recover accurate illumination across whole low-light images. We propose a novel Transformer block to completely simulate the dependencies of all pixels across images via a local-to-global hierarchical attention mechanism, so that dark areas could be inferred by borrowing the information from far informative regions in a highly effective manner. Extensive experiments on several benchmark datasets demonstrate that our approach outperforms state-of-the-art methods",
    "checked": true,
    "id": "85439ee3e715fcaef720b4bdce7b23c54b68afe7",
    "semantic_title": "low-light image enhancement with illumination-aware gamma correction and complete image modelling network",
    "citation_count": 0,
    "authors": [
      "Yinglong Wang",
      "Zhen Liu",
      "Jianzhuang Liu",
      "Songcen Xu",
      "Shuaicheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Both_Diverse_and_Realism_Matter_Physical_Attribute_and_Style_Alignment_ICCV_2023_paper.html": {
    "title": "Both Diverse and Realism Matter: Physical Attribute and Style Alignment for Rainy Image Generation",
    "volume": "main",
    "abstract": "Although considerable progress has been made in the deraining task under synthetic data, it is still a tough problem under real rain scenes, due to the domain gap between the synthetic and real data. Besides, difficulties in collecting and labeling diverse real rain images hinder the progress of this field. Consequently, we attempt to promote real rain removal from rain image generation (RIG) perspective. Existing RIG methods mainly focus on diversity but miss realistic, or the realistic but neglect diversity of the generation. To solve this dilemma, we propose a physical alignment and controllable generation network (PCGNet) for diverse and realistic rain generation. Our key idea is to simultaneously utilize the controllability of attributes from synthetic and the realism of appearance from real data. Specifically, we devise a unified framework to disentangle background, rain attributes, and appearance style from synthetic and real data. Then we collaboratively align the factors with a novel semi-supervised weight moving strategy for attribute, an explicit distribution modeling method for real rain style. Furthermore, we pack these aligned factors into the generation model, achieving physical controllable mapping from the attributes to real rainy with image-level and attribute-level consistency loss. Extensive experiments show that PCGNet can effectively generate appealing rainy results, which sifnicantltly improve the performance under synthetic and real scenes for all existing deraining methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changfeng Yu",
      "Shiming Chen",
      "Yi Chang",
      "Yibing Song",
      "Luxin Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Exploring_the_Benefits_of_Visual_Prompting_in_Differential_Privacy_ICCV_2023_paper.html": {
    "title": "Exploring the Benefits of Visual Prompting in Differential Privacy",
    "volume": "main",
    "abstract": "Visual Prompting (VP) is an emerging and powerful technique that allows sample-efficient adaptation to downstream tasks by engineering a well-trained frozen source model. In this work, we explore the benefits of VP in constructing compelling neural network classifiers with differential privacy (DP). We explore and integrate VP into canonical DP training methods and demonstrate its simplicity and efficiency. In particular, we discover that VP in tandem with PATE, a state-of-the-art DP training method that leverages the knowledge transfer from an ensemble of teachers, achieves the state-of-the-art privacy-utility trade-off with minimum expenditure of privacy budget. Moreover, we conduct additional experiments on cross-domain image classification with a sufficient domain gap to further unveil the advantage of VP in DP. Lastly, we also conduct extensive ablation studies to validate the effectiveness and contribution of VP under DP consideration",
    "checked": true,
    "id": "38b722315873e4519c47cd27b578a14989a0a453",
    "semantic_title": "exploring the benefits of visual prompting in differential privacy",
    "citation_count": 3,
    "authors": [
      "Yizhe Li",
      "Yu-Lin Tsai",
      "Chia-Mu Yu",
      "Pin-Yu Chen",
      "Xuebin Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Single_Image_Reflection_Separation_via_Component_Synergy_ICCV_2023_paper.html": {
    "title": "Single Image Reflection Separation via Component Synergy",
    "volume": "main",
    "abstract": "The reflection superposition phenomenon is complex and widely distributed in the real world, which derives various simplified linear and nonlinear formulations of the problem. In this paper, based on the investigation of the weaknesses of existing models, we propose a more general form of the superposition model by introducing a learnable residue term, which can effectively capture residual information during decomposition, guiding the separated layers to be complete. In order to fully capitalize on its advantages, we further design the network structure elaborately, including a novel dual-stream interaction mechanism and a powerful decomposition network with a semantic pyramid encoder. Extensive experiments and ablation studies are conducted to verify our superiority over state-of-the-art approaches on multiple real-world benchmark datasets",
    "checked": true,
    "id": "a3d7eb0fab1e171986c23fe4113a28f8260c121d",
    "semantic_title": "single image reflection separation via component synergy",
    "citation_count": 0,
    "authors": [
      "Qiming Hu",
      "Xiaojie Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nahon_Mining_bias-target_Alignment_from_Voronoi_Cells_ICCV_2023_paper.html": {
    "title": "Mining bias-target Alignment from Voronoi Cells",
    "volume": "main",
    "abstract": "Despite significant research efforts, deep neural networks remain vulnerable to biases: this raises concerns about their fairness and limits their generalization. In this paper, we propose a bias-agnostic approach to mitigate the impact of biases in deep neural networks. Unlike traditional debiasing approaches, we rely on a metric to quantify \"bias alignment/misalignment\" on target classes and use this information to discourage the propagation of bias-target alignment information through the network. We conduct experiments on several commonly used datasets for debiasing and compare our method with supervised and bias-specific approaches. Our results indicate that the proposed method achieves comparable performance to state-of-the-art supervised approaches, despite being bias-agnostic, even in the presence of multiple biases in the same sample",
    "checked": true,
    "id": "75e4ea35613b5099607eb47b54bdb88d63708923",
    "semantic_title": "mining bias-target alignment from voronoi cells",
    "citation_count": 0,
    "authors": [
      "RÃ©mi Nahon",
      "Van-Tam Nguyen",
      "Enzo Tartaglione"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_The_Victim_and_The_Beneficiary_Exploiting_a_Poisoned_Model_to_ICCV_2023_paper.html": {
    "title": "The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data",
    "volume": "main",
    "abstract": "Recently, backdoor attacks have posed a serious security threat to the training process of deep neural networks (DNNs). The attacked model behaves normally on benign samples but outputs a specific result when the trigger is present. However, compared with the rocketing progress of backdoor attacks, existing defenses are difficult to deal with these threats effectively or require benign samples to work, which may be unavailable in real scenarios. In this paper, we find that the poisoned samples and benign samples can be distinguished with prediction entropy. This inspires us to propose a novel dual-network training framework: The Victim and The Beneficiary (V&B), which exploits a poisoned model to train a clean model without extra benign samples. Firstly, we sacrifice the Victim network to be a powerful poisoned sample detector by training on suspicious samples. Secondly, we train the Beneficiary network on the credible samples selected by the Victim to inhibit backdoor injection. Thirdly, a semi-supervised suppression strategy is adopted for erasing potential backdoors and improving model performance. Furthermore, to better inhibit missed poisoned samples, we propose a strong data augmentation method, AttentionMix, which works well with our proposed V&B framework. Extensive experiments on two widely used datasets against 6 state-of-the-art attacks demonstrate that our framework is effective in preventing backdoor injection and robust to various attacks while maintaining the performance on benign samples. Our code is available at https://github.com/Zixuan-Zhu/VaB",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Zhu",
      "Rui Wang",
      "Cong Zou",
      "Lihua Jing"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_DIFFGUARD_Semantic_Mismatch-Guided_Out-of-Distribution_Detection_Using_Pre-Trained_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "DIFFGUARD: Semantic Mismatch-Guided Out-of-Distribution Detection Using Pre-Trained Diffusion Models",
    "volume": "main",
    "abstract": "Given a classifier, the inherent property of semantic Out-of-Distribution (OOD) samples is that their contents differ from all legal classes in terms of semantics, namely semantic mismatch. There is a recent work that directly applies it to OOD detection, which employs a conditional Generative Adversarial Network (cGAN) to enlarge semantic mismatch in the image space. While achieving remarkable OOD detection performance on small datasets, it is not applicable to ImageNet-scale datasets due to the difficulty in training cGANs with both input images and labels as conditions. As diffusion models are much easier to train and amenable to various conditions compared to cGANs, in this work, we propose to directly use pre-trained diffusion models for semantic mismatch-guided OOD detection, named DiffGuard. Specifically, given an OOD input image and the predicted label from the classifier, we try to enlarge the semantic difference between the reconstructed OOD image under these conditions and the original input image. We also present several test-time techniques to further strengthen such differences. Experimental results show that DiffGuard is effective on both Cifar-10 and hard cases of the large-scale ImageNet, and it can be easily combined with existing OOD detection techniques to achieve state-of-the-art OOD detection results",
    "checked": true,
    "id": "57f4b117744112e4000894a5f939e114f1907719",
    "semantic_title": "diffguard: semantic mismatch-guided out-of-distribution detection using pre-trained diffusion models",
    "citation_count": 1,
    "authors": [
      "Ruiyuan Gao",
      "Chenchen Zhao",
      "Lanqing Hong",
      "Qiang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dou_Identity-Seeking_Self-Supervised_Representation_Learning_for_Generalizable_Person_Re-Identification_ICCV_2023_paper.html": {
    "title": "Identity-Seeking Self-Supervised Representation Learning for Generalizable Person Re-Identification",
    "volume": "main",
    "abstract": "This paper aims to learn a domain-generalizable (DG) person re-identification (ReID) representation from large-scale videos without any annotation. Prior DG ReID methods employ limited labeled data for training due to the high cost of annotation, which restricts further advances. To overcome the barriers of data and annotation, we propose to utilize large-scale unsupervised data for training. The key issue lies in how to mine identity information. To this end, we propose an Identity-seeking Self-supervised Representation learning (ISR) method. ISR constructs positive pairs from inter-frame images by modeling the instance association as a maximum-weight bipartite matching problem. A reliability-guided contrastive loss is further presented to suppress the adverse impact of noisy positive pairs, ensuring that reliable positive pairs dominate the learning process. The training cost of ISR scales approximately linearly with the data size, making it feasible to utilize large-scale data for training. The learned representation exhibits superior generalization ability. Without human annotation and fine-tuning, ISR achieves 87.0% Rank-1 on Market-1501 and 56.4% Rank-1 on MSMT17, outperforming the best supervised domain-generalizable method by 5.0% and 19.5%, respectively. In the pre-training-to-fine-tuning scenario, ISR achieves state-of-the-art performance, with 88.4% Rank-1 on MSMT17",
    "checked": true,
    "id": "68c366a828ed909922e4e6339a2f7e764569c966",
    "semantic_title": "identity-seeking self-supervised representation learning for generalizable person re-identification",
    "citation_count": 0,
    "authors": [
      "Zhaopeng Dou",
      "Zhongdao Wang",
      "Yali Li",
      "Shengjin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jo_3D-Aware_Generative_Model_for_Improved_Side-View_Image_Synthesis_ICCV_2023_paper.html": {
    "title": "3D-Aware Generative Model for Improved Side-View Image Synthesis",
    "volume": "main",
    "abstract": "While recent 3D-aware generative models have shown photo-realistic image synthesis with multi-view consistency, the synthesized image quality degrades depending on the camera pose (e.g., a face with a blurry and noisy boundary at a side viewpoint). Such degradation is mainly caused by the difficulty of learning both pose consistency and photo-realism simultaneously from a dataset with heavily imbalanced poses. In this paper, we propose SideGAN, a novel 3D GAN training method to generate photo-realistic images irrespective of the camera pose, especially for faces of side-view angles. To ease the challenging problem of learning photo-realistic and pose-consistent image synthesis, we split the problem into two subproblems, each of which can be solved more easily. Specifically, we formulate the problem as a combination of two simple discrimination problems, one of which learns to discriminate whether a synthesized image looks real or not, and the other learns to discriminate whether a synthesized image agrees with the camera pose. Based on this, we propose a dual-branched discriminator with two discrimination branches. We also propose a pose-matching loss to learn the pose consistency of 3D GANs. In addition, we present a pose sampling strategy to increase learning opportunities for steep angles in a pose-imbalanced dataset. With extensive validation, we demonstrate that our approach enables 3D GANs to generate high-quality geometries and photo-realistic images irrespective of the camera pose",
    "checked": false,
    "id": "dbb771dfed15157d7680f37162aa65475ca236f1",
    "semantic_title": "sidegan: 3d-aware generative model for improved side-view image synthesis",
    "citation_count": 0,
    "authors": [
      "Kyungmin Jo",
      "Wonjoon Jin",
      "Jaegul Choo",
      "Hyunjoon Lee",
      "Sunghyun Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_Tracking_Anything_with_Decoupled_Video_Segmentation_ICCV_2023_paper.html": {
    "title": "Tracking Anything with Decoupled Video Segmentation",
    "volume": "main",
    "abstract": "Training data for video segmentation are expensive to annotate. This impedes extensions of end-to-end algorithms to new video segmentation tasks, especially in large-vocabulary settings. To 'track anything' without training on video data for every individual task, we develop a decoupled video segmentation approach (DEVA), composed of task-specific image-level segmentation and class/task-agnostic bi-directional temporal propagation. Due to this design, we only need an image-level model for the target task (which is cheaper to train) and a universal temporal propagation model which is trained once and generalizes across tasks. To effectively combine these two modules, we use bi-directional propagation for (semi-)online fusion of segmentation hypotheses from different frames to generate a coherent segmentation. We show that this decoupled formulation compares favorably to end-to-end approaches in several data-scarce tasks including large-vocabulary video panoptic segmentation, open-world video segmentation, referring video segmentation, and unsupervised video object segmentation. Code is available at: https://hkchengrex.github.io/Tracking-Anything-with-DEVA",
    "checked": true,
    "id": "ac7357374fcbfd48caa02777ddcc30bd19295328",
    "semantic_title": "tracking anything with decoupled video segmentation",
    "citation_count": 2,
    "authors": [
      "Ho Kei Cheng",
      "Seoung Wug Oh",
      "Brian Price",
      "Alexander Schwing",
      "Joon-Young Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Generative_Gradient_Inversion_via_Over-Parameterized_Networks_in_Federated_Learning_ICCV_2023_paper.html": {
    "title": "Generative Gradient Inversion via Over-Parameterized Networks in Federated Learning",
    "volume": "main",
    "abstract": "Federated learning has gained recognitions as a secure approach for safeguarding local private data in collaborative learning. But the advent of gradient inversion research has posed significant challenges to this premise by enabling a third-party to recover groundtruth images via gradients. While prior research has predominantly focused on low-resolution images and small batch sizes, this study highlights the feasibility of reconstructing complex images with high resolutions and large batch sizes. The success of the proposed method is contingent on constructing an over-parameterized convolutional network, so that images are generated before fitting to the gradient matching requirement. Practical experiments demonstrate that the proposed algorithm achieves high-fidelity image recovery, surpassing state-of-the-art competitors that commonly fail in more intricate scenarios. Consequently, our study shows that local participants in a federated learning system are vulnerable to potential data leakage issues. Source code is available at https://github.com/czhang024/CI-Net",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Zhang",
      "Zhang Xiaoman",
      "Ekanut Sotthiwat",
      "Yanyu Xu",
      "Ping Liu",
      "Liangli Zhen",
      "Yong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_EQ-Net_Elastic_Quantization_Neural_Networks_ICCV_2023_paper.html": {
    "title": "EQ-Net: Elastic Quantization Neural Networks",
    "volume": "main",
    "abstract": "Current model quantization methods have shown their promising capability in reducing storage space and computation complexity. However, due to the diversity of quantization forms supported by different hardware, one limitation of existing solutions is that usually require repeated optimization for different scenarios. How to construct a model with flexible quantization forms has been less studied. In this paper, we explore a one-shot network quantization regime, named Elastic Quantization Neural Networks (EQ-Net), which aims to train a robust weight-sharing quantization supernet. First of all, we propose an elastic quantization space (including elastic bit-width, granularity, and symmetry) to adapt to various mainstream quantitative forms. Secondly, we propose the Weight Distribution Regularization Loss (WDR-Loss) and Group Progressive Guidance Loss (GPG-Loss) to bridge the inconsistency of the distribution for weights and output logits in the elastic quantization space gap. Lastly, we incorporate genetic algorithms and the proposed Conditional Quantization-Aware Accuracy Predictor (CQAP) as an estimator to quickly search mixed-precision quantized neural networks in supernet. Extensive experiments demonstrate that our EQ-Net is close to or even better than its static counterparts as well as state-of-the-art robust bit-width methods. Code can be available at https://github.com/xuke225/EQ-Net.git",
    "checked": true,
    "id": "6c6dce2267e5de7d17251a7ea040f1a194b4608a",
    "semantic_title": "eq-net: elastic quantization neural networks",
    "citation_count": 1,
    "authors": [
      "Ke Xu",
      "Lei Han",
      "Ye Tian",
      "Shangshang Yang",
      "Xingyi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_OxfordTVG-HIC_Can_Machine_Make_Humorous_Captions_from_Images_ICCV_2023_paper.html": {
    "title": "OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?",
    "volume": "main",
    "abstract": "This paper presents OxfordTVG-HIC (Humorous Image Captions), a large-scale dataset for humour generation and understanding. Humour is an abstract, subjective, and context-dependent cognitive construct involving several cognitive factors, making it a challenging task to generate and interpret. Hence, humour generation and understanding can serve as a new task for evaluating the ability of deep-learning methods to process abstract and subjective information. Due to the scarcity of data, humour-related generation tasks such as captioning remain underexplored. To address this gap, OxfordTVG-HIC offers approximately 2.9M image-text pairs with humour scores to train a generalizable humour captioning model. Contrary to existing captioning datasets, OxfordTVG-HIC features a wide range of emotional and semantic diversity resulting in out-of-context examples that are particularly conducive to generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive content. We also show how OxfordTVGHIC can be leveraged for evaluating the humour of a generated text. Through explainability analysis of the trained models, we identify the visual and linguistic cues influential for evoking humour prediction (and generation). We observe qualitatively that these cues are aligned with the benign violation theory of humour in cognitive psychology",
    "checked": true,
    "id": "bd54aa49b6a16916ba12eedbbee63eb33f6ac391",
    "semantic_title": "oxfordtvg-hic: can machine make humorous captions from images?",
    "citation_count": 0,
    "authors": [
      "Runjia Li",
      "Shuyang Sun",
      "Mohamed Elhoseiny",
      "Philip Torr"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Exploring_Open-Vocabulary_Semantic_Segmentation_from_CLIP_Vision_Encoder_Distillation_Only_ICCV_2023_paper.html": {
    "title": "Exploring Open-Vocabulary Semantic Segmentation from CLIP Vision Encoder Distillation Only",
    "volume": "main",
    "abstract": "Semantic segmentation is a crucial task in computer vision that involves segmenting images into semantically meaningful regions at the pixel level. However, existing approaches often rely on expensive human annotations as supervision for model training, limiting their scalability to large, unlabeled datasets. To address this challenge, we present ZeroSeg, a novel method that leverages the existing pretrained vision-language (VL) model (e.g. CLIP vision encoder) to train open-vocabulary zero-shot semantic segmentation models. Although acquired extensive knowledge of visual concepts, it is non-trivial to exploit knowledge from these VL models to the task of semantic segmentation, as they are usually trained at an image level. ZeroSeg overcomes this by distilling the visual concepts learned by VL models into a set of segment tokens, each summarizing a localized region of the target image. We evaluate ZeroSeg on multiple popular segmentation benchmarks, including PASCAL VOC 2012, PASCAL Context, and COCO, in a zero-shot manner Our approach achieves state-of-the-art performance when compared to other zero-shot segmentation methods under the same training data, while also performing competitively compared to strongly supervised methods. Finally, we also demonstrated the effectiveness of ZeroSeg on open-vocabulary segmentation, through both human studies and qualitative visualizations. The code is publicly available at https://github.com/facebookresearch/ZeroSeg",
    "checked": false,
    "id": "0867f7029b3726740fb41ca8171833bf6f82e483",
    "semantic_title": "exploring open-vocabulary semantic segmentation without human labels",
    "citation_count": 1,
    "authors": [
      "Jun Chen",
      "Deyao Zhu",
      "Guocheng Qian",
      "Bernard Ghanem",
      "Zhicheng Yan",
      "Chenchen Zhu",
      "Fanyi Xiao",
      "Sean Chang Culatana",
      "Mohamed Elhoseiny"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Saha_EDAPS_Enhanced_Domain-Adaptive_Panoptic_Segmentation_ICCV_2023_paper.html": {
    "title": "EDAPS: Enhanced Domain-Adaptive Panoptic Segmentation",
    "volume": "main",
    "abstract": "With autonomous industries on the rise, domain adaptation of the visual perception stack is an important research direction due to the cost savings promise. Much prior art was dedicated to domain-adaptive semantic segmentation in the synthetic-to-real context. Despite being a crucial output of the perception stack, panoptic segmentation has been largely overlooked by the domain adaptation community. Therefore, we revisit well-performing domain adaptation strategies from other fields, adapt them to panoptic segmentation, and show that they can effectively enhance panoptic domain adaptation. Further, we study the panoptic network design and propose a novel architecture (EDAPS) designed explicitly for domain-adaptive panoptic segmentation. It uses a shared, domain-robust transformer encoder to facilitate the joint adaptation of semantic and instance features, but task-specific decoders tailored for the specific requirements of both domain-adaptive semantic and instance segmentation. As a result, the performance gap seen in challenging panoptic benchmarks is substantially narrowed. EDAPS significantly improves the state-of-the-art performance for panoptic segmentation UDA by a large margin of 20% on SYNTHIA-to-Cityscapes and even 72% on the more challenging SYNTHIA-to-Mapillary Vistas. The implementation is available at https://github.com/susaha/edaps",
    "checked": true,
    "id": "fbfe1032039704a8b308a29b2ea5f3e2124e642f",
    "semantic_title": "edaps: enhanced domain-adaptive panoptic segmentation",
    "citation_count": 1,
    "authors": [
      "Suman Saha",
      "Lukas Hoyer",
      "Anton Obukhov",
      "Dengxin Dai",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nie_Parallax-Tolerant_Unsupervised_Deep_Image_Stitching_ICCV_2023_paper.html": {
    "title": "Parallax-Tolerant Unsupervised Deep Image Stitching",
    "volume": "main",
    "abstract": "Traditional image stitching approaches tend to leverage increasingly complex geometric features (point, line, edge, etc.) for better performance. However, these hand-crafted features are only suitable for specific natural scenes with adequate geometric structures. In contrast, deep stitching schemes overcome adverse conditions by adaptively learning robust semantic features, but they cannot handle large-parallax cases. To solve these issues, we propose a parallax-tolerant unsupervised deep image stitching technique. First, we propose a robust and flexible warp to model the image registration from global homography to local thin-plate spline motion. It provides accurate alignment for overlapping regions and shape preservation for non-overlapping regions by joint optimization concerning alignment and distortion. Subsequently, to improve the generalization capability, we design a simple but effective iterative strategy to enhance the warp adaption in cross-dataset and cross-resolution applications. Finally, to further eliminate the parallax artifacts, we propose to composite the stitched image seamlessly by unsupervised learning for seam-driven composition masks. Compared with existing methods, our solution is parallax-tolerant and free from laborious designs of complicated geometric features for specific scenes. Extensive experiments show our superiority over the SoTA methods, both quantitatively and qualitatively. The code will be available soon",
    "checked": true,
    "id": "3e971224741c8c59a023b95d757a27c9689c56dd",
    "semantic_title": "parallax-tolerant unsupervised deep image stitching",
    "citation_count": 0,
    "authors": [
      "Lang Nie",
      "Chunyu Lin",
      "Kang Liao",
      "Shuaicheng Liu",
      "Yao Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qiu_Scratch_Each_Others_Back_Incomplete_Multi-Modal_Brain_Tumor_Segmentation_via_ICCV_2023_paper.html": {
    "title": "Scratch Each Other's Back: Incomplete Multi-Modal Brain Tumor Segmentation via Category Aware Group Self-Support Learning",
    "volume": "main",
    "abstract": "Although Magnetic Resonance Imaging (MRI) is very helpful for brain tumor segmentation and discovery, it often lacks some modalities in clinical practice. As a result, degradation of prediction performance is inevitable. According to current implementations, different modalities are considered to be independent and non-interfering with each other during the training process of modal feature extraction, however they are complementary. In this paper, considering the sensitivity of different modalities to diverse tumor regions, we propose a Category Aware Group Self-Support Learning framework, called GSS, to make up for the information deficit among the modalities in the individual modal feature extraction phase. Precisely, within each prediction category, predictions of all modalities form a group, where the prediction with the most extraordinary sensitivity is selected as the group leader. Collaborative efforts between group leaders and members identify the communal learning target with high consistency and certainty. As our minor contribution, we introduce a random mask to reduce the possible biases. GSS adopts the standard training strategy without specific architectural choices and thus can be easily plugged into existing incomplete multi-modal brain tumor segmentation. Remarkably, extensive experiments on BraTS2020, BraTS2018, and BraTS2015 datasets demonstrate that GSS can improve the performance of existing SOTA algorithms by 1.27-3.20% in Dice on average. The code is released at https://github.com/qysgithubopen/GSS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yansheng Qiu",
      "Delin Chen",
      "Hongdou Yao",
      "Yongchao Xu",
      "Zheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dinsdale_SFHarmony_Source_Free_Domain_Adaptation_for_Distributed_Neuroimaging_Analysis_ICCV_2023_paper.html": {
    "title": "SFHarmony: Source Free Domain Adaptation for Distributed Neuroimaging Analysis",
    "volume": "main",
    "abstract": "To represent the biological variability of clinical neuroimaging populations, it is vital to be able to combine data across scanners and studies. However, different MRI scanners produce images with different characteristics, resulting in a domain shift known as the 'harmonisation problem'. Additionally, neuroimaging data is inherently personal in nature, leading to data privacy concerns when sharing the data. To overcome these barriers, we propose an Unsupervised Source-Free Domain Adaptation (SFDA) method, SFHarmony. Through modelling the imaging features as a Gaussian Mixture Model and minimising an adapted Bhattacharyya distance between the source and target features, we can create a model that performs well for the target data whilst having a shared feature representation across the data domains, without needing access to the source data for adaptation or target labels. We demonstrate the performance of our method on simulated and real domain shifts, showing that the approach is applicable to classification, segmentation and regression tasks, requiring no changes to the algorithm. Our method outperforms existing SFDA approaches across a range of realistic data scenarios, demonstrating the potential utility of our approach for MRI harmonisation and general SFDA problems. Our code is available at https://github.com/nkdinsdale/SFHarmony",
    "checked": true,
    "id": "44e0fd4dc70d7c1cbc6fbef33b766e647ba2bab3",
    "semantic_title": "sfharmony: source free domain adaptation for distributed neuroimaging analysis",
    "citation_count": 0,
    "authors": [
      "Nicola K Dinsdale",
      "Mark Jenkinson",
      "Ana IL Namburete"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mentzer_M2T_Masking_Transformers_Twice_for_Faster_Decoding_ICCV_2023_paper.html": {
    "title": "M2T: Masking Transformers Twice for Faster Decoding",
    "volume": "main",
    "abstract": "We show how bidirectional transformers trained for masked token prediction can be applied to neural image compression to achieve state-of-the-art results. Such models were previously used for image_generation_ by progressive sampling groups of masked tokens according to uncertainty-adaptive schedules. Unlike these works, we demonstrate that predefined, deterministic schedules perform as well or better for image compression. This insight allows us to use masked attention during training in addition to masked inputs, and activation caching during inference, to significantly speed up our models (4x higher inference speed) at a small increase in bitrate",
    "checked": true,
    "id": "1dc8e3726eeadfcafe9bb866d704a65c9053b823",
    "semantic_title": "m2t: masking transformers twice for faster decoding",
    "citation_count": 2,
    "authors": [
      "Fabian Mentzer",
      "Eirikur Agustson",
      "Michael Tschannen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xia_CoIn_Contrastive_Instance_Feature_Mining_for_Outdoor_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "CoIn: Contrastive Instance Feature Mining for Outdoor 3D Object Detection with Very Limited Annotations",
    "volume": "main",
    "abstract": "Recently, 3D object detection with sparse annotations has received great attention. However, current detectors usually perform poorly under very limited annotations. To address this problem, we propose a novel Contrastive Instance feature mining method, named CoIn. To better identify indistinguishable features learned through limited supervision, we design a Multi-Class contrastive learning module (MCcont) to enhance feature discrimination. Meanwhile, we propose a feature-level pseudo-label mining framework consisting of an instance feature mining module (InF-Mining) and a Labeled-to-Pseudo contrastive learning module (LPcont). These two modules exploit latent instances in feature space to supervise the training of detectors with limited annotations. Extensive experiments with KITTI dataset, Waymo open dataset, and nuScenes dataset show that under limited annotations, our method greatly improves the performance of baseline detectors: CenterPoint, Voxel-RCNN, and CasA. Combining CoIn with an iterative training strategy, we propose a CoIn++ pipeline, which requires only 2% annotations in the KITTI dataset to achieve performance comparable to the fully supervised methods. The code is available at https://github.com/xmuqimingxia/CoIn",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiming Xia",
      "Jinhao Deng",
      "Chenglu Wen",
      "Hai Wu",
      "Shaoshuai Shi",
      "Xin Li",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_3D_Human_Mesh_Recovery_with_Sequentially_Global_Rotation_Estimation_ICCV_2023_paper.html": {
    "title": "3D Human Mesh Recovery with Sequentially Global Rotation Estimation",
    "volume": "main",
    "abstract": "Model-based 3D human mesh recovery aims to reconstruct a 3D human body mesh by estimating its parameters from monocular RGB images. Most of recent works adopt the Skinned Multi-Person Linear (SMPL) model to regress relative rotations for each body joint along the kinematics chain. This pipeline needs to transform each relative rotation matrix into a global rotation matrix to articulate the canonical mesh, and suffers from accumulated errors along the kinematics chain. This paper proposes to directly estimate the global rotation of each joint to avoid error accumulation and pursue better accuracy. The proposed Sequentially Global Rotation Estimation (SGRE) directly predicts the global rotation matrix of each joint on the kinematics chain. SGRE features a residual learning module to leverage complementary features and previously predicted rotations of parent joints to guide the estimation of subsequent child joints. Thanks to this global estimation pipeline and residual learning module, SGRE alleviates error accumulation and produces more accurate 3D human mesh. It can be flexibly integrated into existing regression-based methods and achieves superior performance on various benchmarks. For example, it improves the latest method 3DCrowdNet by 3.3 mm MPJPE and 5.0 mm PVE on 3DPW dataset and 3.2 AP on COCO dataset, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongkai Wang",
      "Shiliang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_DREAMWALKER_Mental_Planning_for_Continuous_Vision-Language_Navigation_ICCV_2023_paper.html": {
    "title": "DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation",
    "volume": "main",
    "abstract": "VLN-CE is a recently released embodied task, where AI agents need to navigate a freely traversable environment to reach a distant target location, given language instructions. It poses great challenges due to the huge space of possible strategies. Driven by the belief that the ability to anticipate the consequences of future actions is crucial for the emergence of intelligent and interpretable planning behavior, we propose Dreamwalker --- a world model based VLN-CE agent. The world model is built to summarize the visual, topological, and dynamic properties of the complicated continuous environment into a discrete, structured, and compact representation. Dreamwalker can simulate and evaluate possible plans entirely in such internal abstract world, before executing costly actions. As opposed to existing model-free VLN-CE agents simply making greedy decisions in the real world, which easily results in shortsighted behaviors, Dreamwalker is able to make strategic planning through large amounts of \"mental experiments.\" Moreover, the imagined future scenarios reflect our agent's intention, making its decision-making process more transparent. Extensive experiments and ablation studies on VLN-CE dataset confirm the effectiveness of the proposed approach and outline fruitful directions for future work. Our code will be released",
    "checked": true,
    "id": "8d097d73c29c2ce3af3f7a3d67e2d96f0b43908a",
    "semantic_title": "dreamwalker: mental planning for continuous vision-language navigation",
    "citation_count": 1,
    "authors": [
      "Hanqing Wang",
      "Wei Liang",
      "Luc Van Gool",
      "Wenguan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Computation_and_Data_Efficient_Backdoor_Attacks_ICCV_2023_paper.html": {
    "title": "Computation and Data Efficient Backdoor Attacks",
    "volume": "main",
    "abstract": "Backdoor attacks against deep learning have been widely studied. Various attack techniques have been proposed for different domains and paradigms, e.g., image, point cloud, natural language processing, transfer learning, etc. These works normally adopt the data poisoning strategy to embed the backdoor. They randomly select samples from the benign training set for poisoning, without considering the distinct contribution of each sample to the backdoor effectiveness, making the attack less optimal. A recent work (IJCAI-22) proposed to use the forgetting score to measure the importance of each poisoned sample and then filter out redundant data for effective backdoor training. However, this method is empirically designed without theoretical proofing. It is also very time-consuming as it needs to go through almost all the training stages for data selection. To address such limitations, we propose a novel confidence-based scoring methodology, which can efficiently measure the contribution of each poisoning sample based on the distance posteriors. We further introduce a greedy search algorithm to find the most informative samples for backdoor injection more promptly. Experimental evaluations on both 2D image and 3D point cloud classification tasks show that our approach can achieve comparable performance or even surpass the forgetting score-based searching method while requiring only several extra epochs' computation of a standard training process",
    "checked": false,
    "id": "9adcf96784c51c8fb30375d930be03e05de0838c",
    "semantic_title": "data-efficient backdoor attacks",
    "citation_count": 10,
    "authors": [
      "Yutong Wu",
      "Xingshuo Han",
      "Han Qiu",
      "Tianwei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tu_Agglomerative_Transformer_for_Human-Object_Interaction_Detection_ICCV_2023_paper.html": {
    "title": "Agglomerative Transformer for Human-Object Interaction Detection",
    "volume": "main",
    "abstract": "We propose an agglomerative Transformer (AGER) that enables Transformer-based human-object interaction (HOI) detectors to flexibly exploit extra instance-level cues in a single-stage and end-to-end manner for the first time. AGER acquires instance tokens by dynamically clustering patch tokens and aligning cluster centres to instances with textual guidance, thus enjoying two benefits: 1) Intergrality: each instance token is encouraged to contain all discriminative feature regions of an instance, which demonstrates a significant improvement in the extraction of different instance-level cues, and subsequently leads to a new state-of-the-art performance of HOI detection with 36.75 mAP on HICO-Det. 2) Efficiency: the dynamical clustering mechanism allows AGER to generate instance tokens jointly with the feature learning of the Transformer encoder, eliminating the need of an additional object detector or instance decoder in prior methods, thus allowing the extraction of desirable extra cues for HOI detection in a single-stage and end-to-end pipeline. Concretely, AGER reduces GFLOPs by 8.5% and improves FPS by 36%, even compared to a vanilla DETR-like pipeline without extra cue extraction",
    "checked": true,
    "id": "67a4c900f38e715c68850b392d36d357169519dd",
    "semantic_title": "agglomerative transformer for human-object interaction detection",
    "citation_count": 1,
    "authors": [
      "Danyang Tu",
      "Wei Sun",
      "Guangtao Zhai",
      "Wei Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qian_Decouple_Before_Interact_Multi-Modal_Prompt_Learning_for_Continual_Visual_Question_ICCV_2023_paper.html": {
    "title": "Decouple Before Interact: Multi-Modal Prompt Learning for Continual Visual Question Answering",
    "volume": "main",
    "abstract": "In the real world, a desirable Visual Question Answering model is expected to provide correct answers to new questions and images in a continual setting (recognized as CL-VQA). However, existing works formulate CLVQA from a vision-only or language-only perspective, and straightforwardly apply the uni-modal continual learning (CL) strategies to this multi-modal task, which is improper and suboptimal. On the one hand, such a partial formulation may result in limited evaluations. On the other hand, neglecting the interactions between modalities will lead to poor performance. To tackle these challenging issues, we propose a comprehensive formulation for CL-VQA from the perspective of multi-modal vision-language fusion. Based on our formulation, we further propose MulTi-Modal PRompt LearnIng with DecouPLing bEfore InTeraction (TRIPLET), a novel approach that builds on a pre-trained vision-language model and consists of decoupled prompts and prompt interaction strategies to capture the complex interactions between modalities. In particular, decoupled prompts contain learnable parameters that are decoupled w.r.t different aspects, and the prompt interaction strategies are in charge of modeling interactions between inputs and prompts. Additionally, we build two CL-VQA benchmarks for a more comprehensive evaluation. Extensive experiments demonstrate that our TRIPLET outperforms state-of-the-art methods in both uni-modal and multi-modal continual settings for CL-VQA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi Qian",
      "Xin Wang",
      "Xuguang Duan",
      "Pengda Qin",
      "Yuhong Li",
      "Wenwu Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chu_Rethinking_Fast_Fourier_Convolution_in_Image_Inpainting_ICCV_2023_paper.html": {
    "title": "Rethinking Fast Fourier Convolution in Image Inpainting",
    "volume": "main",
    "abstract": "Recently proposed image inpainting method LaMa builds its network upon Fast Fourier Convolution (FFC), which was originally proposed for high-level vision tasks like image classification. FFC empowers the fully convolutional network to have a global receptive field in its early layers. Thanks to the unique character of the FFC module, LaMa has the ability to produce robust repeating texture, which can not be achieved by the previous inpainting methods. However, is the vanilla FFC module suitable for low-level vision tasks like image inpainting? In this paper, we analyze the fundamental flaws of using FFC in image inpainting, which are 1) spectrum shifting, 2) unexpected spatial activation, and 3) limited frequency receptive field. Such flaws make FFC-based inpainting framework difficult in generating complicated texture and performing faithful reconstruction. Based on the above analysis, we propose a novel Unbiased Fast Fourier Convolution (UFFC) module, which modifies the vanilla FFC module with 1) range transform and inverse transform, 2) absolute position embedding, 3) dynamic skip connection, and 4) adaptive clip, to overcome such flaws, achieving better inpainting results. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our method, outperforming the state-of-the-art methods in both texture-capturing ability and expressiveness",
    "checked": false,
    "id": "4709f061e5768bd661bf27f2df20255a6077c18f",
    "semantic_title": "parallel fast fourier convolutions enhanced image inpainting based on residual transformer",
    "citation_count": 0,
    "authors": [
      "Tianyi Chu",
      "Jiafu Chen",
      "Jiakai Sun",
      "Shuobin Lian",
      "Zhizhong Wang",
      "Zhiwen Zuo",
      "Lei Zhao",
      "Wei Xing",
      "Dongming Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Learning_Robust_Representations_with_Information_Bottleneck_and_Memory_Network_for_ICCV_2023_paper.html": {
    "title": "Learning Robust Representations with Information Bottleneck and Memory Network for RGB-D-based Gesture Recognition",
    "volume": "main",
    "abstract": "Although previous RGB-D-based gesture recognition methods have shown promising performance, researchers often overlook the interference of task-irrelevant cues like illumination and background. These unnecessary factors are learned together with the predictive ones by the network and hinder accurate recognition. In this paper, we propose a convenient and analytical framework to learn a robust feature representation that is impervious to gesture-irrelevant factors. Based on the Information Bottleneck theory, two rules of Sufficiency and Compactness are derived to develop a new information-theoretic loss function, which cultivates a more sufficient and compact representation from the feature encoding and mitigates the impact of gesture-irrelevant information. To highlight the predictive information, we further integrate a memory network. Using our proposed content-based and contextual memory addressing scheme, we weaken the nuisances while preserving the task-relevant information, providing guidance for refining the feature representation. Experiments conducted on three public datasets demonstrate that our approach leads to a better feature representation and achieves better performance than state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunan Li",
      "Huizhou Chen",
      "Guanwen Feng",
      "Qiguang Miao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ventura_P1AC_Revisiting_Absolute_Pose_From_a_Single_Affine_Correspondence_ICCV_2023_paper.html": {
    "title": "P1AC: Revisiting Absolute Pose From a Single Affine Correspondence",
    "volume": "main",
    "abstract": "Affine correspondences have traditionally been used to improve feature matching over wide baselines. While recent work has successfully used affine correspondences to solve various relative camera pose estimation problems, less attention has been given to their use in absolute pose estimation. We introduce the first general solution to the problem of estimating the pose of a calibrated camera given a single observation of an oriented point and an affine correspondence. The advantage of our approach (P1AC) is that it requires only a single correspondence, in comparison to the traditional point-based approach (P3P), significantly reducing the combinatorics in robust estimation. P1AC provides a general solution that removes restrictive assumptions made in prior work and is applicable to large-scale image-based localization. We propose a minimal solution to the P1AC problem and evaluate our novel solver on synthetic data, showing its numerical stability and performance under various types of noise. On standard image-based localization benchmarks we show that P1AC achieves more accurate results than the widely used P3P algorithm. Code for our method is available at https://github.com/jonathanventura/P1AC/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Ventura",
      "Zuzana Kukelova",
      "Torsten Sattler",
      "DÃ¡niel BarÃ¡th"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chung_LAN-HDR_Luminance-based_Alignment_Network_for_High_Dynamic_Range_Video_Reconstruction_ICCV_2023_paper.html": {
    "title": "LAN-HDR: Luminance-based Alignment Network for High Dynamic Range Video Reconstruction",
    "volume": "main",
    "abstract": "As demands for high-quality videos continue to rise, high-resolution and high-dynamic range (HDR) imaging techniques are drawing attention. To generate an HDR video from low dynamic range (LDR) images, one of the critical steps is the motion compensation between LDR frames, for which most existing works employed the optical flow algorithm. However, these methods suffer from flow estimation errors when saturation or complicated motions exist. In this paper, we propose an end-to-end HDR video composition framework, which aligns LDR frames in the feature space and then merges aligned features into an HDR frame, without relying on pixel-domain optical flow. Specifically, we propose a luminance-based alignment network for HDR (LAN-HDR) consisting of an alignment module and a hallucination module. The alignment module aligns a frame to the adjacent reference by evaluating luminance-based attention, excluding color information. The hallucination module generates sharp details, especially for washed-out areas due to saturation. The aligned and hallucinated features are then blended adaptively to complement each other. Finally, we merge the features to generate a final HDR frame. In training, we adopt a temporal loss, in addition to frame reconstruction losses, to enhance temporal consistency and thus reduce flickering. Extensive experiments demonstrate that our method performs better or comparable to state-of-the-art methods on several benchmarks. Codes are available at https://github.com/haesoochung/LAN-HDR",
    "checked": true,
    "id": "0a09eb8c9f2dad58f0ac8aafe5cd42fc34a2525c",
    "semantic_title": "lan-hdr: luminance-based alignment network for high dynamic range video reconstruction",
    "citation_count": 0,
    "authors": [
      "Haesoo Chung",
      "Nam Ik Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fu_Dancing_in_the_Dark_A_Benchmark_towards_General_Low-light_Video_ICCV_2023_paper.html": {
    "title": "Dancing in the Dark: A Benchmark towards General Low-light Video Enhancement",
    "volume": "main",
    "abstract": "Low-light video enhancement is a challenging task with broad applications. However, current research in this area is limited by the lack of high-quality benchmark datasets. To address this issue, we design a camera system and collect a high-quality low-light video dataset with multiple exposures and cameras. Our dataset provides dynamic video pairs with pronounced camera motion and strict spatial alignment. To achieve general low-light video enhancement, we also propose a novel Retinex-based method named Light Adjustable Network (LAN). LAN iteratively refines the illumination and adaptively adjusts it under varying lighting conditions, leading to visually appealing results even in diverse real-world scenarios. The extensive experiments demonstrate the superiority of our low-light video dataset and enhancement method. Our dataset and code will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiyuan Fu",
      "Wenkai Zheng",
      "Xicong Wang",
      "Jiaxuan Wang",
      "Heng Zhang",
      "Huadong Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Iskender_RED-PSM_Regularization_by_Denoising_of_Partially_Separable_Models_for_Dynamic_ICCV_2023_paper.html": {
    "title": "RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging",
    "volume": "main",
    "abstract": "Dynamic imaging involves the recovery of a time-varying 2D or 3D object at each time instant using its undersampled measurements. In particular, in dynamic tomography, only a single projection at a single view angle may be available at a time, making the problem severely ill-posed. In this work, we propose an approach, RED-PSM, which combines for the first time two powerful techniques to address this challenging imaging problem. The first, are partially separable models, which have been used to introduce a low-rank prior for the spatio-temporal object. The second is the recent Regularization by Denoising (RED), which provides a flexible framework to exploit the impressive performance of state-of-the-art image denoising algorithms, for various inverse problems. We propose a partially separable objective with RED and an optimization scheme with variable splitting and ADMM. Our objective is proved to converge to a value corresponding to a stationary point satisfying the first-order optimality conditions. Convergence is accelerated by a particular projection-domain-based initialization. We demonstrate the performance and computational improvements of our proposed RED-PSM with a learned image denoiser by comparing it to a recent deep-prior-based method TD-DIP. Although the emphasis is on dynamic tomography, we also demonstrate the performance advantages of RED-PSM in a dynamic cardiac MRI setting",
    "checked": true,
    "id": "081eecffd061ac191b9c1899f7238fad2838ba47",
    "semantic_title": "red-psm: regularization by denoising of partially separable models for dynamic imaging",
    "citation_count": 0,
    "authors": [
      "Berk Iskender",
      "Marc L. Klasky",
      "Yoram Bresler"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ding_Unsupervised_Manifold_Linearizing_and_Clustering_ICCV_2023_paper.html": {
    "title": "Unsupervised Manifold Linearizing and Clustering",
    "volume": "main",
    "abstract": "We consider the problem of simultaneously clustering and learning a linear representation of data lying close to a union of low-dimensional manifolds, a fundamental task in machine learning and computer vision. When the manifolds are assumed to be linear subspaces, this reduces to the classical problem of subspace clustering, which has been studied extensively over the past two decades. Unfortunately, many real-world datasets such as natural images can not be well approximated by linear subspaces. On the other hand, numerous works have attempted to learn an appropriate transformation of the data, such that data is mapped from a union of general non-linear manifolds to a union of linear subspaces (with points from the same manifold being mapped to the same subspace). However, many existing works have limitations such as assuming knowledge of the membership of samples to clusters, requiring high sampling density, or being shown theoretically to learn trivial representations. In this paper, we propose to optimize the Maximal Coding Rate Reduction metric with respect to both the data representation and a novel doubly stochastic cluster membership, inspired by state-of-the-art subspace clustering results. We give a parameterization of such a representation and membership, allowing efficient mini-batching and one-shot initialization. Experiments on CIFAR-10, -20, -100, and TinyImageNet-200 datasets show that the proposed method is much more accurate and scalable than state-of-the-art deep clustering methods, and further learns a latent linear representation of the data",
    "checked": true,
    "id": "15069b27b74b79fb6bcb3ba239d07820ba0895c4",
    "semantic_title": "unsupervised manifold linearizing and clustering",
    "citation_count": 6,
    "authors": [
      "Tianjiao Ding",
      "Shengbang Tong",
      "Kwan Ho Ryan Chan",
      "Xili Dai",
      "Yi Ma",
      "Benjamin D. Haeffele"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shi_Lossy_and_Lossless_L2_Post-training_Model_Size_Compression_ICCV_2023_paper.html": {
    "title": "Lossy and Lossless (L2) Post-training Model Size Compression",
    "volume": "main",
    "abstract": "Deep neural networks have delivered remarkable performance and have been widely used in various visual tasks. However, their huge sizes cause significant inconvenience for transmission and storage. Many previous studies have explored model size compression. However, these studies often approach various lossy and lossless compression methods in isolation, leading to challenges in achieving high compression ratios efficiently. This work proposes a post-training model size compression method that combines lossy and lossless compression in a unified way. We first propose a unified parametric weight transformation, which ensures different lossy compression methods can be performed jointly in a post-training manner. Then, a dedicated differentiable counter is introduced to guide the optimization of lossy compression to arrive at a more suitable point for later lossless compression. Additionally, our method can easily control a desired global compression ratio and allocate adaptive ratios for different layers. Finally, our method can achieve a stable 10 times compression ratio without sacrificing accuracy and a 20 times compression ratio with minor accuracy loss in a short time. Our code is available at https://github.com/ModelTC/L2_Compression",
    "checked": true,
    "id": "6089199e95310f21a1175437140e2232d06a891b",
    "semantic_title": "lossy and lossless (l2) post-training model size compression",
    "citation_count": 0,
    "authors": [
      "Yumeng Shi",
      "Shihao Bai",
      "Xiuying Wei",
      "Ruihao Gong",
      "Jianlei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_C2ST_Cross-Modal_Contextualized_Sequence_Transduction_for_Continuous_Sign_Language_Recognition_ICCV_2023_paper.html": {
    "title": "C2ST: Cross-Modal Contextualized Sequence Transduction for Continuous Sign Language Recognition",
    "volume": "main",
    "abstract": "Continuous Sign Language Recognition (CSLR) aims to transcribe the signs of an untrimmed video into written words or glosses. The mainstream framework for CSLR consists of a spatial module for visual representation learning, a temporal module aggregating the local and global temporal information of frame sequence, and the connectionist temporal classification (CTC) loss, which aligns video features with gloss sequence. Unfortunately, the language prior implicit in the gloss sequence is ignored throughout the modeling process. Furthermore, the contextualization of glosses is further ignored in alignment learning, as CTC makes an independence assumption between glosses. In this paper, we propose a Cross-modal Contextualized Sequence Transduction (C2ST) for CSLR, which effectively incorporates the knowledge of gloss sequence into the process of video representation learning and sequence transduction. Specifically, we introduce a cross-modal context learning framework for CSLR, in which the linguistic features of gloss sequences is extracted by a language model, and recurrently integrate with visual features for video modelling. Moreover, we introduce the contextualized sequence transduction loss that incorporates the contextual information of gloss sequences in label prediction, without making any independence assumptions between the glosses. Our method sets the new state of the art on three widely used large-scale sign language recognition datasets: Phoenix-2014, Phoenix-2014-T, and CSL-Daily. On CSL-Daily, our approach achieves an absolute gain of 4.9% WER compared to the best published results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huaiwen Zhang",
      "Zihang Guo",
      "Yang Yang",
      "Xin Liu",
      "De Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cai_ObjectFusion_Multi-modal_3D_Object_Detection_with_Object-Centric_Fusion_ICCV_2023_paper.html": {
    "title": "ObjectFusion: Multi-modal 3D Object Detection with Object-Centric Fusion",
    "volume": "main",
    "abstract": "Recent progress on multi-modal 3D object detection has featured BEV (Bird-Eye-View) based fusion, which effectively unifies both LiDAR point clouds and camera images in a shared BEV space. Nevertheless, it is not trivial to perform camera-to-BEV transformation due to the inherently ambiguous depth estimation of each pixel, resulting in spatial misalignment between these two multi-modal features. Moreover, such transformation also inevitably leads to projection distortion of camera image features in BEV space. In this paper, we propose a novel Object-centric Fusion (ObjectFusion) paradigm, which completely gets rid of camera-to-BEV transformation during fusion to align object-centric features across different modalities for 3D object detection. ObjectFusion first learns three kinds of modality-specific feature maps (i.e., voxel, BEV, and image features) from LiDAR point clouds and its BEV projections, camera images. Then a set of 3D object proposals are produced from the BEV features via a heatmap-based proposal generator. Next, the 3D object proposals are reprojected back to voxel, BEV, and image spaces. We leverage voxel and RoI pooling to generate spatially aligned object-centric features for each modality. All the object-centric features of three modalities are further fused at object level, which is finally fed into the detection heads. Extensive experiments on nuScenes dataset demonstrate the superiority of our ObjectFusion, by achieving 69.8% mAP on nuScenes validation set and improving BEVFusion by 1.3%",
    "checked": false,
    "id": "02882d2dc6e3938a327a66663e55339190784534",
    "semantic_title": "leveraging vision-centric multi-modal expertise for 3d object detection",
    "citation_count": 0,
    "authors": [
      "Qi Cai",
      "Yingwei Pan",
      "Ting Yao",
      "Chong-Wah Ngo",
      "Tao Mei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_D-IF_Uncertainty-aware_Human_Digitization_via_Implicit_Distribution_Field_ICCV_2023_paper.html": {
    "title": "D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field",
    "volume": "main",
    "abstract": "Realistic virtual humans play a crucial role in numerous industries, such as metaverse, intelligent healthcare, and self-driving simulation. But creating them on a large scale with high levels of realism remains a challenge. The utilization of deep implicit function sparks a new era of image-based 3D clothed human reconstruction, enabling pixel-aligned shape recovery with fine details. Subsequently, the vast majority of works locate the surface by regressing the deterministic implicit value for each point. However, should all points be treated equally regardless of their proximity to the surface? In this paper, we propose replacing the implicit value with an adaptive uncertainty distribution, to differentiate between points based on their distance to the surface. This simple \"value to distribution\" transition yields significant improvements on nearly all the baselines. Furthermore, qualitative results demonstrate that the models trained using our uncertainty distribution loss, can capture more intricate wrinkles, and realistic limbs. Code and models are available for research purposes at https://github.com/psyai-net/D-IF_release",
    "checked": true,
    "id": "54e5882abe19937475592badb407108e1d70fa1f",
    "semantic_title": "d-if: uncertainty-aware human digitization via implicit distribution field",
    "citation_count": 2,
    "authors": [
      "Xueting Yang",
      "Yihao Luo",
      "Yuliang Xiu",
      "Wei Wang",
      "Hao Xu",
      "Zhaoxin Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhong_MMVP_Motion-Matrix-Based_Video_Prediction_ICCV_2023_paper.html": {
    "title": "MMVP: Motion-Matrix-Based Video Prediction",
    "volume": "main",
    "abstract": "A central challenge of video prediction lies where the system has to reason the object's future motion from image frames while simultaneously maintaining the consistency of its appearance across frames. This work introduces an end-to-end trainable two-stream video prediction framework, Motion-Matrix-based Video Prediction (MMVP), to tackle this challenge. Unlike previous methods that usually handle motion prediction and appearance maintenance within the same set of modules, MMVP decouples motion and appearance information by constructing appearance-agnostic motion matrices. The motion matrices represent the temporal similarity of each and every pair of feature patches in the input frames, and are the sole input of the motion prediction module in MMVP. This design improves video prediction in both accuracy and efficiency, and reduces the model size. Results of extensive experiments demonstrate that MMVP outperforms state-of-the-art systems on public data sets by non-negligible large margins (approx. 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the size or smaller). Please refer to https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction for the official code and the datasets used in this paper",
    "checked": true,
    "id": "6eb7aea517be123c089dca40c63dafabe18560b4",
    "semantic_title": "mmvp: motion-matrix-based video prediction",
    "citation_count": 0,
    "authors": [
      "Yiqi Zhong",
      "Luming Liang",
      "Ilya Zharkov",
      "Ulrich Neumann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Human_Preference_Score_Better_Aligning_Text-to-Image_Models_with_Human_Preference_ICCV_2023_paper.html": {
    "title": "Human Preference Score: Better Aligning Text-to-Image Models with Human Preference",
    "volume": "main",
    "abstract": "Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classifier with the collected dataset and derive a Human Preference Score (HPS) based on the classifier. Using HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human preferences. Our experiments show that HPS outperforms CLIP in predicting human choices and has good generalization capability toward images generated from other models. By tuning Stable Diffusion with the guidance of HPS, the adapted model is able to generate images that are more preferred by human users. The project page is available here: https://tgxs002.github.io/align_sd_web/",
    "checked": true,
    "id": "9550076d9930dd3533ab9276126f1a9265fad7b4",
    "semantic_title": "human preference score: better aligning text-to-image models with human preference",
    "citation_count": 23,
    "authors": [
      "Xiaoshi Wu",
      "Keqiang Sun",
      "Feng Zhu",
      "Rui Zhao",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Karunratanakul_Guided_Motion_Diffusion_for_Controllable_Human_Motion_Synthesis_ICCV_2023_paper.html": {
    "title": "Guided Motion Diffusion for Controllable Human Motion Synthesis",
    "volume": "main",
    "abstract": "Denoising diffusion models have shown great promise in human motion synthesis conditioned on natural language descriptions. However, integrating spatial constraints, such as pre-defined motion trajectories and obstacles, remains a challenge despite being essential for bridging the gap between isolated human motion and its surrounding environment. To address this issue, we propose Guided Motion Diffusion (GMD), a method that incorporates spatial constraints into the motion generation process. Specifically, we propose an effective feature projection scheme that manipulates motion representation to enhance the coherency between spatial information and local poses. Together with a new imputation formulation, the generated motion can reliably conform to spatial constraints such as global motion trajectories. Furthermore, given sparse spatial constraints (e.g. sparse keyframes), we introduce a new dense guidance approach to turn a sparse signal, which is susceptible to being ignored during the reverse steps, into denser signals to guide the generated motion to the given constraints. Our extensive experiments justify the development of \\methodname, which achieves a significant improvement over state-of-the-art methods in text-based motion generation while allowing control of the synthesized motions with spatial constraints",
    "checked": false,
    "id": "d64755f140ad742495518714ebd457b4d95ce341",
    "semantic_title": "motion-conditioned diffusion model for controllable video synthesis",
    "citation_count": 5,
    "authors": [
      "Korrawe Karunratanakul",
      "Konpat Preechakul",
      "Supasorn Suwajanakorn",
      "Siyu Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jian_AffordPose_A_Large-Scale_Dataset_of_Hand-Object_Interactions_with_Affordance-Driven_Hand_ICCV_2023_paper.html": {
    "title": "AffordPose: A Large-Scale Dataset of Hand-Object Interactions with Affordance-Driven Hand Pose",
    "volume": "main",
    "abstract": "How human interact with objects depends on the functional roles of the target objects, which introduces the problem of affordance-aware hand-object interaction. It requires a large number of human demonstrations for the learning and understanding of plausible and appropriate hand-object interactions. In this work, we present AffordPose, a large-scale dataset of hand-object interactions with affordance-driven hand pose. We first annotate the specific part-level affordance labels for each object, e.g. twist, pull, handle-grasp, etc, instead of the general intents such as use or handover, to indicate the purpose and guide the localization of the hand-object interactions. The fine-grained hand-object interactions reveal the influence of hand-centered affordances on the detailed arrangement of the hand poses, yet also exhibit a certain degree of diversity. We collect a total of 26.7K hand-object interactions, each including the 3D object shape, the part-level affordance label, and the manually adjusted hand poses. The comprehensive data analysis shows the common characteristics and diversity of hand-object interactions per affordance via the parameter statistics and contacting computation. We also conduct experiments on the tasks of hand-object affordance understanding and affordance-oriented hand-object interaction generation, to validate the effectiveness of our dataset in learning the fine-grained hand-object interactions. Project page: https://github.com/GentlesJan/AffordPose",
    "checked": true,
    "id": "e7e3f6bd319cb8a79107dfba6eaf0a5d5b512c7a",
    "semantic_title": "affordpose: a large-scale dataset of hand-object interactions with affordance-driven hand pose",
    "citation_count": 0,
    "authors": [
      "Juntao Jian",
      "Xiuping Liu",
      "Manyi Li",
      "Ruizhen Hu",
      "Jian Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Locomotion-Action-Manipulation_Synthesizing_Human-Scene_Interactions_in_Complex_3D_Environments_ICCV_2023_paper.html": {
    "title": "Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments",
    "volume": "main",
    "abstract": "Synthesizing interaction-involved human motions has been challenging due to the high complexity of 3D environments and the diversity of possible human behaviors within. We present LAMA, Locomotion-Action-MAnipulation, to synthesize natural and plausible long term human movements in complex indoor environments. The key motivation of LAMA is to build a unified framework to encompass a series of everyday motions including locomotion, scene interaction, and object manipulation. Unlike existing methods that require motion data \"paired\" with scanned 3D scenes for supervision, we formulate the problem as a test-time optimization by using human motion capture data only for synthesis. LAMA leverages a reinforcement learning framework coupled with motion matching algorithm for optimization, and further exploits a motion editing framework via manifold learning to cover possible variations in interaction and manipulation. Throughout extensive experiments, we demonstrate that LAMA outperforms previous approaches in synthesizing realistic motions in various challenging scenarios",
    "checked": true,
    "id": "57e84ab9083cb62e4fdbd24479963ea992c1c605",
    "semantic_title": "locomotion-action-manipulation: synthesizing human-scene interactions in complex 3d environments",
    "citation_count": 6,
    "authors": [
      "Jiye Lee",
      "Hanbyul Joo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shao_NDDepth_Normal-Distance_Assisted_Monocular_Depth_Estimation_ICCV_2023_paper.html": {
    "title": "NDDepth: Normal-Distance Assisted Monocular Depth Estimation",
    "volume": "main",
    "abstract": "Monocular depth estimation has drawn widespread attention from the vision community due to its broad applications. In this paper, we propose a novel physics (geometry)-driven deep learning framework for monocular depth estimation by assuming that 3D scenes are constituted by piece-wise planes. Particularly, we introduce a new normal-distance head that outputs pixel-level surface normal and plane-to-origin distance for deriving depth at each position. Meanwhile, the normal and distance are regularized by a developed plane-aware consistency constraint. We further integrate an additional depth head to improve the robustness of the proposed framework. To fully exploit the strengths of these two heads, we develop an effective contrastive iterative refinement module that refines depth in a complementary manner according to the depth uncertainty. Extensive experiments indicate that the proposed method exceeds previous state-of-the-art competitors on the NYU-Depth-v2, KITTI and SUN RGB-D datasets. Notably, it ranks 1st among all submissions on the KITTI depth prediction online benchmark at the submission time. The source code is available at https://github.com/ShuweiShao/NDDepth",
    "checked": true,
    "id": "b20fc8e605a80980e3015737ae4245baaac7c6cc",
    "semantic_title": "nddepth: normal-distance assisted monocular depth estimation",
    "citation_count": 2,
    "authors": [
      "Shuwei Shao",
      "Zhongcai Pei",
      "Weihai Chen",
      "Xingming Wu",
      "Zhengguo Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Sequential_Texts_Driven_Cohesive_Motions_Synthesis_with_Natural_Transitions_ICCV_2023_paper.html": {
    "title": "Sequential Texts Driven Cohesive Motions Synthesis with Natural Transitions",
    "volume": "main",
    "abstract": "The intelligent synthesis/generation of daily-life motion sequences is fundamental and urgently needed for many VR/metaverse-related applications. However, existing approaches commonly focus on monotonic motion generation (e.g., walking, jumping, etc.) based on single instruction-like text, which is still not intelligent enough and can't meet practical demands. To this end, we propose a cohesive human motion sequence synthesis framework based on free-form sequential texts while ensuring semantic connection and natural transitions between adjacent motions. At the technical level, we explore the local-to-global semantic features of previous and current texts to extract relevant information. This information is used to guide the framework in understanding the semantics of the current moment. Moreover, we propose learnable tokens to adaptively learn the influence range of the previous motions towards natural transitions. These tokens can be trained to encode the relevant information into well-designed transition loss. To demonstrate the efficacy of our method, we conduct extensive experiments and comprehensive evaluations on the public dataset as well as a new dataset produced by us. All the experiments confirm that our method outperforms the state-of-the-art methods in terms of semantic matching, realism, and transition fluency. Our project is public available. https://druthrie.github.io/sequential-texts-to-motion/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Li",
      "Sisi Zhuang",
      "Wenfeng Song",
      "Xinyu Zhang",
      "Hejia Chen",
      "Aimin Hao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lan_Efficient_Converted_Spiking_Neural_Network_for_3D_and_2D_Classification_ICCV_2023_paper.html": {
    "title": "Efficient Converted Spiking Neural Network for 3D and 2D Classification",
    "volume": "main",
    "abstract": "Spiking Neural Networks (SNNs) have attracted enormous research interest due to their low-power and biologically plausible nature. Existing ANN-SNN conversion methods can achieve lossless conversion by converting a well-trained Artificial Neural Network (ANN) into an SNN. However, converted SNN requires a large amount of time steps to achieve competitive performance with the well-trained ANN, which means a large latency. In this paper, we propose an efficient unified ANN-SNN conversion method for point cloud classification and image classification to significantly reduce the time step to meet the fast and lossless ANN-SNN transformation. Specifically, we first adaptively adjust the threshold according to the activation state of spiking neurons, ensuring a certain proportion of spiking neurons are activated at each time step to reduce the time for accumulation of membrane potential. Next, we use an adaptive firing mechanism to enlarge the range of spiking output, getting more discrimination features in short time steps. Extensive experimental results on challenging point cloud and image datasets demonstrate that the suggested approach significantly outmatches state-of-the-art ANN-SNN conversion based methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Lan",
      "Yachao Zhang",
      "Xu Ma",
      "Yanyun Qu",
      "Yun Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gupta_Eulerian_Single-Photon_Vision_ICCV_2023_paper.html": {
    "title": "Eulerian Single-Photon Vision",
    "volume": "main",
    "abstract": "Single-photon sensors measure light signals at the finest possible resolution -- individual photons. These sensors introduce two major challenges in the form of strong Poisson noise and extremely large data acquisition rates, which are also inherited by downstream computer vision tasks. Previous work has largely focused on solving the image reconstruction problem first and then using off-the-shelf methods for downstream tasks, but the most general solutions that account for motion are costly and not scalable to large data volumes produced by single-photon sensors. This work forgoes the image reconstruction problem. Instead, we demonstrate computationally light-weight phase-based algorithms for the tasks of edge detection and motion estimation. These methods directly process the raw single-photon data as a 3D volume with a bank of velocity-tuned filters, achieving speed-ups of more than two orders of magnitude compared to explicit reconstruction-based methods. Project webpage: https://wisionlab.com/project/eulerian-single-photon-vision/",
    "checked": true,
    "id": "23c7fc05accb75b45217f42ad81ec2c88f16f17c",
    "semantic_title": "eulerian single-photon vision",
    "citation_count": 0,
    "authors": [
      "Shantanu Gupta",
      "Mohit Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zou_Adaptive_Calibrator_Ensemble_Navigating_Test_Set_Difficulty_in_Out-of-Distribution_Scenarios_ICCV_2023_paper.html": {
    "title": "Adaptive Calibrator Ensemble: Navigating Test Set Difficulty in Out-of-Distribution Scenarios",
    "volume": "main",
    "abstract": "Model calibration usually requires optimizing some parameters (e.g., temperature) w.r.t an objective function like negative log-likelihood. This work uncovers a significant aspect often overlooked that the objective function is influenced by calibration set difficulty: the ratio of misclassified to correctly classified samples. If a test set has a drastically different difficulty level from the calibration set, a phenomenon out-of-distribution (OOD) data often exhibit: the optimal calibration parameters of the two datasets would be different, rendering an optimal calibrator on the calibration set suboptimal on the OOD test set and thus degraded calibration performance. With this knowledge, we propose a simple and effective method named adaptive calibrator ensemble (ACE) to calibrate OOD datasets whose difficulty is usually higher than the calibration set. Specifically, two calibration functions are trained, one for in-distribution data (low difficulty), and the other for severely OOD data (high difficulty). To achieve desirable calibration on a new OOD dataset, ACE uses an adaptive weighting method that strikes a balance between the two extreme functions. When plugged in, ACE generally improves the performance of a few state-of-the-art calibration schemes on a series of OOD benchmarks. Importantly, such improvement does not come at the cost of the in-distribution calibration performance. Project Website: https://github.com/insysgroup/Adaptive-Calibrators-Ensemble.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuli Zou",
      "Weijian Deng",
      "Liang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhong_Contrastive_Learning_Relies_More_on_Spatial_Inductive_Bias_Than_Supervised_ICCV_2023_paper.html": {
    "title": "Contrastive Learning Relies More on Spatial Inductive Bias Than Supervised Learning: An Empirical Study",
    "volume": "main",
    "abstract": "Though self-supervised contrastive learning (CL) has shown its potential to achieve state-of-the-art accuracy without any supervision, its behavior still remains under investigated by academia. Different from most previous work that understands CL from learning objectives, we focus on an unexplored yet natural aspect: the spatial inductive bias which seems to be implicitly exploited via data augmentations in CL. We design an experiment to study the reliance of CL on such spatial inductive bias, by destroying the global or local spatial structures of image with global or local patch shuffling, and comparing the performance drop between experiments on original and corrupted dataset to quantify the reliance of certain inductive bias. We also use the uniformity of feature space to further research on how CL-pre-trained model behave with the corrupted dataset. Our results and analysis show that CL has a much higher reliance on spatial inductive bias than SL, regardless of specific CL algorithm or backbones, opening a new direction for studying the behavior of CL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyi Zhong",
      "Haoran Tang",
      "Jun-Kun Chen",
      "Yu-Xiong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_DiffuMask_Synthesizing_Images_with_Pixel-level_Annotations_for_Semantic_Segmentation_Using_ICCV_2023_paper.html": {
    "title": "DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models",
    "volume": "main",
    "abstract": "Collecting and annotating images with pixel-wise labels is time-consuming and laborious. In contrast, synthetic data can be freely available using a generative model (e.g., DALL-E, Stable Diffusion). In this paper, we show that it is possible to automatically obtain accurate semantic masks of synthetic images generated by the pre-trained Stable Diffusion, which uses only text-image pairs during training. Our approach, called DiffuMask, exploits the potential of the cross-attention map between text and image, which is natural and seamless to extend the text-driven image synthesis to semantic mask generation. DiffuMask uses text-guided cross-attention information to localize class/word-specific regions, which are combined with practical techniques to create a novel high-resolution and class-discriminative pixel-wise mask. The methods help to reduce data collection and annotation costs obviously. Experiments demonstrate that the existing segmentation methods trained on synthetic data of DiffuMask can achieve a competitive performance over the counterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird), DiffuMask presents promising performance, close to the state-of-the-art result of real data (within 3% mIoU gap). Moreover, in the open-vocabulary segmentation (zero-shot) setting, DiffuMask achieves a new SOTA result on Unseen class of VOC 2012",
    "checked": true,
    "id": "5b84bab8a8f18e803db8d3db7ab6e4fe08fc3959",
    "semantic_title": "diffumask: synthesizing images with pixel-level annotations for semantic segmentation using diffusion models",
    "citation_count": 28,
    "authors": [
      "Weijia Wu",
      "Yuzhong Zhao",
      "Mike Zheng Shou",
      "Hong Zhou",
      "Chunhua Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xue_NSF_Neural_Surface_Fields_for_Human_Modeling_from_Monocular_Depth_ICCV_2023_paper.html": {
    "title": "NSF: Neural Surface Fields for Human Modeling from Monocular Depth",
    "volume": "main",
    "abstract": "Obtaining personalized 3D animatable avatars from a monocular camera has several real world applications in gaming, virtual try-on, animation, and VR/XR, etc. However, it is very challenging to model dynamic and fine-grained clothing deformations from such sparse data. Existing methods for modeling 3D humans from depth data have limitations in terms of computational efficiency, mesh coherency, and flexibility in resolution and topology. For instance, reconstructing shapes using implicit functions and extracting explicit meshes per frame is computationally expensive and cannot ensure coherent meshes across frames. Moreover, predicting per-vertex deformations on a pre-designed human template with a discrete surface lacks flexibility in resolution and topology. To overcome these limitations, we propose a novel method 'NSF: Neural Surface Fields' for modeling 3D clothed humans from monocular depth. NSF defines a neural field solely on the base surface which models a continuous and flexible displacement field. NSF can be adapted to the base surface with different resolution and topology without retraining at inference time. Compared to existing approaches, our method eliminates the expensive per-frame surface extraction while maintaining mesh coherency, and is capable of reconstructing meshes with arbitrary resolution without retraining. To foster research in this direction, we release our code in project page at: https://yuxuan-xue.com/nsf",
    "checked": true,
    "id": "289682ea4401e03e92e8f5d989b6cc73698b66de",
    "semantic_title": "nsf: neural surface fields for human modeling from monocular depth",
    "citation_count": 1,
    "authors": [
      "Yuxuan Xue",
      "Bharat Lal Bhatnagar",
      "Riccardo Marin",
      "Nikolaos Sarafianos",
      "Yuanlu Xu",
      "Gerard Pons-Moll",
      "Tony Tung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Corona-Figueroa_Unaligned_2D_to_3D_Translation_with_Conditional_Vector-Quantized_Code_Diffusion_ICCV_2023_paper.html": {
    "title": "Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code Diffusion using Transformers",
    "volume": "main",
    "abstract": "Generating 3D images of complex objects conditionally from a few 2D views is a difficult synthesis problem, compounded by issues such as domain gap and geometric misalignment. For instance, a unified framework such as Generative Adversarial Networks cannot achieve this unless they explicitly define both a domain-invariant and geometric-invariant joint latent distribution, whereas Neural Radiance Fields are generally unable to handle both issues as they optimize at the pixel level. By contrast, we propose a simple and novel 2D to 3D synthesis approach based on conditional diffusion with vector-quantized codes. Operating in an information-rich code space enables high-resolution 3D synthesis via full-coverage attention across the views. Specifically, we generate the 3D codes (e.g. for CT images) conditional on previously generated 3D codes and the entire codebook of two 2D views (e.g. 2D X-rays). Qualitative and quantitative results demonstrate state-of-the-art performance over specialized methods across varied evaluation criteria, including fidelity metrics such as density, coverage, and distortion metrics for two complex volumetric imagery datasets from in real-world scenarios",
    "checked": true,
    "id": "a9dc07473ee0f9f70a5159fbce9ee2756c68c70b",
    "semantic_title": "unaligned 2d to 3d translation with conditional vector-quantized code diffusion using transformers",
    "citation_count": 0,
    "authors": [
      "Abril Corona-Figueroa",
      "Sam Bond-Taylor",
      "Neelanjan Bhowmik",
      "Yona Falinie A. Gaus",
      "Toby P. Breckon",
      "Hubert P. H. Shum",
      "Chris G. Willcocks"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_DMNet_Delaunay_Meshing_Network_for_3D_Shape_Representation_ICCV_2023_paper.html": {
    "title": "DMNet: Delaunay Meshing Network for 3D Shape Representation",
    "volume": "main",
    "abstract": "Recently, there has been a growing interest in learning-based explicit methods due to their ability to respect the original input and preserve details. However, the connectivity on complex structures is still difficult to infer due to the limited local shape perception, resulting in artifacts and non-watertight triangles. In this paper, we present a novel learning-based method with Delaunay triangulation to achieve high-precision reconstruction. We model the Delaunay triangulation as a dual graph, extract local geometric information from the points, and embed it into the structural representation of Delaunay triangulation in an organic way, benefiting fine-grained details reconstruction. To encourage neighborhood information interaction of edges and nodes in the graph, we introduce a local graph iteration algorithm, which is a variant of graph neural network. Moreover, a geometric constraint loss further improves the classification of tetrahedrons. Benefiting from our fully local network, a scaling strategy is designed to enable large-scale reconstruction. Experiments show that our method yields watertight and high-quality meshes. Especially for some thin structures and sharp edges, our method shows better performance than the current state-of-the-art methods. Furthermore, it has a strong adaptability to point clouds of different densities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Zhang",
      "Ganzhangqin Yuan",
      "Wenbing Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Alanov_StyleDomain_Efficient_and_Lightweight_Parameterizations_of_StyleGAN_for_One-shot_and_ICCV_2023_paper.html": {
    "title": "StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation",
    "volume": "main",
    "abstract": "Domain adaptation of GANs is a problem of fine-tuning GAN models pretrained on a large dataset (e.g. StyleGAN) to a specific domain with few samples (e.g. painting faces, sketches, etc.). While there are many methods that tackle this problem in different ways, there are still many important questions that remain unanswered. In this paper, we provide a systematic and in-depth analysis of the domain adaptation problem of GANs, focusing on the StyleGAN model. We perform a detailed exploration of the most important parts of StyleGAN that are responsible for adapting the generator to a new domain depending on the similarity between the source and target domains. As a result of this study, we propose new efficient and lightweight parameterizations of StyleGAN for domain adaptation. Particularly, we show that there exist directions in StyleSpace (StyleDomain directions) that are sufficient for adapting to similar domains. For dissimilar domains, we propose Affine+ and AffineLight+ parameterizations that allows us to outperform existing baselines in few-shot adaptation while having significantly less training parameters. Finally, we examine StyleDomain directions and discover their many surprising properties that we apply for domain mixing and cross-domain image morphing. Source code can be found at https://github.com/AIRI-Institute/StyleDomain",
    "checked": true,
    "id": "53a85ad75831fb0780c3a3a10ef78a833bf2dd58",
    "semantic_title": "styledomain: efficient and lightweight parameterizations of stylegan for one-shot and few-shot domain adaptation",
    "citation_count": 0,
    "authors": [
      "Aibek Alanov",
      "Vadim Titov",
      "Maksim Nakhodnov",
      "Dmitry Vetrov"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Noh_RankMixup_Ranking-Based_Mixup_Training_for_Network_Calibration_ICCV_2023_paper.html": {
    "title": "RankMixup: Ranking-Based Mixup Training for Network Calibration",
    "volume": "main",
    "abstract": "Network calibration aims to accurately estimate the level of confidences, which is particularly important for employing deep neural networks in real-world systems. Recent approaches leverage mixup to calibrate the network's predictions during training. However, they do not consider the problem that mixtures of labels in mixup may not accurately represent the actual distribution of augmented samples. In this paper, we present RankMixup, a novel mixup-based framework alleviating the problem of the mixture of labels for network calibration. To this end, we propose to use an ordinal ranking relationship between raw and mixup-augmented samples as an alternative supervisory signal to the label mixtures for network calibration. We hypothesize that the network should estimate a higher level of confidence for the raw samples than the augmented ones (Fig.1). To implement this idea, we introduce a mixup-based ranking loss (MRL) that encourages lower confidences for augmented samples compared to raw ones, maintaining the ranking relationship. We also propose to leverage the ranking relationship among multiple mixup-augmented samples to further improve the calibration capability. Augmented samples with larger mixing coefficients are expected to have higher confidences and vice versa (Fig.1). That is, the order of confidences should be aligned with that of mixing coefficients. To this end, we introduce a novel loss, M-NDCG, in order to reduce the number of misaligned pairs of the coefficients and confidences. Extensive experimental results on standard benchmarks for network calibration demonstrate the effectiveness of RankMixup",
    "checked": true,
    "id": "deec4779098ae23cdf278430f93f4032e1165043",
    "semantic_title": "rankmixup: ranking-based mixup training for network calibration",
    "citation_count": 0,
    "authors": [
      "Jongyoun Noh",
      "Hyekang Park",
      "Junghyup Lee",
      "Bumsub Ham"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Body_Knowledge_and_Uncertainty_Modeling_for_Monocular_3D_Human_Body_ICCV_2023_paper.html": {
    "title": "Body Knowledge and Uncertainty Modeling for Monocular 3D Human Body Reconstruction",
    "volume": "main",
    "abstract": "While 3D body reconstruction methods have made remarkable progress recently, it remains difficult to acquire the sufficiently accurate and numerous 3D supervisions required for training. In this paper, we propose KNOWN, a framework that effectively utilizes body KNOWledge and uNcertainty modeling to compensate for insufficient 3D supervisions. KNOWN exploits a comprehensive set of generic body constraints derived from well-established body knowledge. These generic constraints precisely and explicitly characterize the reconstruction plausibility and enable 3D reconstruction models to be trained without any 3D data. Moreover, existing methods typically use images from multiple datasets during training, which can result in data noise (e.g., inconsistent joint annotation) and data imbalance (e.g., minority images representing unusual poses or captured from challenging camera views). KNOWN solves these problems through a novel probabilistic framework that models both aleatoric and epistemic uncertainty. Aleatoric uncertainty is encoded in a robust Negative Log-Likelihood (NLL) training loss, while epistemic uncertainty is used to guide model refinement. Experiments demonstrate that KNOWN's body reconstruction outperforms prior weakly-supervised approaches, particularly on the challenging minority images",
    "checked": true,
    "id": "172326b2ace220f5f4fe5cc2fba1115c2a6de80d",
    "semantic_title": "body knowledge and uncertainty modeling for monocular 3d human body reconstruction",
    "citation_count": 0,
    "authors": [
      "Yufei Zhang",
      "Hanjing Wang",
      "Jeffrey O. Kephart",
      "Qiang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Randomized_Quantization_A_Generic_Augmentation_for_Data_Agnostic_Self-supervised_Learning_ICCV_2023_paper.html": {
    "title": "Randomized Quantization: A Generic Augmentation for Data Agnostic Self-supervised Learning",
    "volume": "main",
    "abstract": "Self-supervised representation learning follows a paradigm of withholding some part of the data and tasking the network to predict it from the remaining part. Among many techniques, data augmentation lies at the core for creating the information gap. Towards this end, masking has emerged as a generic and powerful tool where content is withheld along the sequential dimension, e.g., spatial in images, temporal in audio, and syntactic in language. In this paper, we explore the orthogonal channel dimension for generic data augmentation by exploiting precision redundancy. The data for each channel is quantized through a non-uniform quantizer, with the quantized value sampled randomly within randomly sampled quantization bins. From another perspective, quantization is analogous to channel-wise masking, as it removes the information within each bin, but preserves the information across bins. Our approach significantly surpasses existing generic data augmentation methods, while showing on par performance against modality- specific augmentations. We comprehensively evaluate our approach on vision, audio, 3D point clouds, as well as the DABS benchmark which is comprised of various data modalities. The code is available at https: //github.com/microsoft/random_quantize",
    "checked": true,
    "id": "b159aa9f0ab0bb5389645816cc7980bade610ba8",
    "semantic_title": "randomized quantization: a generic augmentation for data agnostic self-supervised learning",
    "citation_count": 0,
    "authors": [
      "Huimin Wu",
      "Chenyang Lei",
      "Xiao Sun",
      "Peng-Shuai Wang",
      "Qifeng Chen",
      "Kwang-Ting Cheng",
      "Stephen Lin",
      "Zhirong Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Park_Learning_to_Generate_Semantic_Layouts_for_Higher_Text-Image_Correspondence_in_ICCV_2023_paper.html": {
    "title": "Learning to Generate Semantic Layouts for Higher Text-Image Correspondence in Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "Existing text-to-image generation approaches have set high standards for photorealism and text-image correspondence, largely benefiting from web-scale text-image datasets, which can include up to 5 billion pairs. However, text-to-image generation models trained on domain-specific datasets, such as urban scenes, medical images, and faces, still suffer from low text-image correspondence due to the lack of text-image pairs. Additionally, collecting billions of text-image pairs for a specific domain can be time-consuming and costly. Thus, ensuring high text-image correspondence without relying on web-scale text-image datasets remains a challenging task. In this paper, we present a novel approach for enhancing text-image correspondence by leveraging available semantic layouts. Specifically, we propose a Gaussian-categorical diffusion process that simultaneously generates both images and corresponding layout pairs. Our experiments reveal that we can guide text-to-image generation models to be aware of the semantics of different image regions, by training the model to generate semantic labels for each pixel. We demonstrate that our approach achieves higher text-image correspondence compared to existing text-to-image generation approaches in the Multi-Modal CelebA-HQ and the Cityscapes dataset, where text-image pairs are scarce",
    "checked": true,
    "id": "f5ddcbb7ac8dce7aae001b57330369a52aa24766",
    "semantic_title": "learning to generate semantic layouts for higher text-image correspondence in text-to-image synthesis",
    "citation_count": 0,
    "authors": [
      "Minho Park",
      "Jooyeol Yun",
      "Seunghwan Choi",
      "Jaegul Choo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chang_Neural_Radiance_Field_with_LiDAR_maps_ICCV_2023_paper.html": {
    "title": "Neural Radiance Field with LiDAR maps",
    "volume": "main",
    "abstract": "We address outdoor Neural Radiance Fields (NeRF) with LiDAR maps. Existing NeRF methods usually require specially collected hypersampled source views and do not perform well with the open source camera-LiDAR datasets - significantly limiting the approach's practical utility. In this paper, we demonstrate an approach that allows for these datasets to be utilized for high quality neural renderings. Our design leverages 1) LiDAR sensors for strong 3D geometry priors that significantly improve the ray sampling locality, and 2) Conditional Adversarial Networks (cGANs) to recover image details since aggregating embeddings from imperfect LiDAR maps causes artifacts in the synthesized images. Our experiments show that while NeRF baselines produce either noisy or blurry results on Argoverse 2, the images synthesized by our system not only outperform baselines in image quality metrics under both clean and noisy conditions, but also obtain closer Detectron2 results to the ground truth images. Furthermore, to show the substantial applicability of our system, we demonstrate that our system can be used in data augmentation for training a pose regression network and multi-season view synthesis. Our dataset and code will be released",
    "checked": false,
    "id": "9b0e426ece36be9f743c5bfe6e261ecba7699b30",
    "semantic_title": "single-view neural radiance fields with depth teacher",
    "citation_count": 0,
    "authors": [
      "MingFang Chang",
      "Akash Sharma",
      "Michael Kaess",
      "Simon Lucey"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_AREA_Adaptive_Reweighting_via_Effective_Area_for_Long-Tailed_Classification_ICCV_2023_paper.html": {
    "title": "AREA: Adaptive Reweighting via Effective Area for Long-Tailed Classification",
    "volume": "main",
    "abstract": "Large-scale data from real-world usually follow a long-tailed distribution (i.e., a few majority classes occupy plentiful training data, while most minority classes have few samples), making the hyperplanes heavily skewed to the minority classes. Traditionally, reweighting is adopted to make the hyperplanes fairly split the feature space, where the weights are designed according to the number of samples. However, we find that the number of samples in a class can not accurately measure the size of its spanned space, especially for the majority class, where the size of its spanned space is usually larger than the samples' number because of the high diversity. Therefore, weights designed based on the samples' number will still compress the space of minority classes. In this paper, we reconsider reweighting from a totally new perspective of analyzing the spanned space of each class. We argue that, besides statistical numbers, relations between samples are also significant for sufficiently depicting the spanned space. Consequently, we estimate the size of the spanned space for each category, namely effective area, by detailedly analyzing its samples' distribution. By treating samples of a class as identically distributed random variables and analyzing their correlations, a simple and non-parametric formula is derived to estimate the effective area. Then, the weight simply calculated inversely proportional to the effective area of each class is adopted to achieve fairer training. Note that our weights are more flexible as they can be adaptively adjusted along with the optimizing features during training. Experiments on four long-tailed datasets show that the proposed weights outperform the state-of-the-art reweighting methods. Moreover, our method can also achieve better results on statistically balanced CIFAR-10/100. Code is available at https://github.com/xiaohua-chen/AREA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohua Chen",
      "Yucan Zhou",
      "Dayan Wu",
      "Chule Yang",
      "Bo Li",
      "Qinghua Hu",
      "Weiping Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gandikota_Erasing_Concepts_from_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Erasing Concepts from Diffusion Models",
    "volume": "main",
    "abstract": "Motivated by concerns that large-scale diffusion models can produce undesirable output such as sexually explicit content or copyrighted artistic styles, we study erasure of specific concepts from diffusion model weights. We propose a fine-tuning method that can erase a visual concept from a pre-trained diffusion model, given only the name of the style and using negative guidance as a teacher. We benchmark our method against previous approaches that remove sexually explicit content and demonstrate its effectiveness, performing on par with Safe Latent Diffusion and censored training. To evaluate artistic style removal, we conduct experiments erasing five modern artists from the network and conduct a user study to assess the human perception of the removed styles. Unlike previous methods, our approach can remove concepts from a diffusion model permanently rather than modifying the output at the inference time, so it cannot be circumvented even if a user has access to model weights. Our code, data, and results are available at erasing.baulab.info",
    "checked": true,
    "id": "7a106b9e32a40b523e80ef1ef262f39213aeed81",
    "semantic_title": "erasing concepts from diffusion models",
    "citation_count": 40,
    "authors": [
      "Rohit Gandikota",
      "Joanna Materzynska",
      "Jaden Fiotto-Kaufman",
      "David Bau"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Fully_Attentional_Networks_with_Self-emerging_Token_Labeling_ICCV_2023_paper.html": {
    "title": "Fully Attentional Networks with Self-emerging Token Labeling",
    "volume": "main",
    "abstract": "Recent studies indicate that Vision Transformers (ViTs) are robust against out-of-distribution scenarios. In particular, the Fully Attentional Network (FAN) - a family of ViT backbones, has achieved state-of-the-art robustness. In this paper, we revisit the FAN models and improve their pre-training with a self-emerging token labeling (STL) framework. Our method contains a two-stage training framework. Specifically, we first train a FAN token labeler (FAN-TL) to generate semantically meaningful patch token labels, followed by a FAN student model training stage that uses both the token labels and the original class label. With the proposed STL framework, our best model based on FAN-L-Hybrid (77.3M parameters) achieves 84.8% Top-1 accuracy and 42.1% mCE on ImageNet-1K and ImageNet-C, and sets a new state-of-the-art for ImageNet-A (46.1%) and ImageNet-R (56.6%) without using extra data, outperforming the original FAN counterpart by significant margins. The proposed framework also demonstrates significantly enhanced performance on downstream tasks such as semantic segmentation, with up to 1.7% improvement in robustness over the counterpart model",
    "checked": false,
    "id": "3d51590745b8771029dd46879cdcc05d5d5c7f36",
    "semantic_title": "hybrid-transcd: a hybrid transformer remote sensing image change detection network via token aggregation",
    "citation_count": 14,
    "authors": [
      "Bingyin Zhao",
      "Zhiding Yu",
      "Shiyi Lan",
      "Yutao Cheng",
      "Anima Anandkumar",
      "Yingjie Lao",
      "Jose M. Alvarez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Suryanto_ACTIVE_Towards_Highly_Transferable_3D_Physical_Camouflage_for_Universal_and_ICCV_2023_paper.html": {
    "title": "ACTIVE: Towards Highly Transferable 3D Physical Camouflage for Universal and Robust Vehicle Evasion",
    "volume": "main",
    "abstract": "Adversarial camouflage has garnered attention for its ability to attack object detectors from any viewpoint by covering the entire object's surface. However, universality and robustness in existing methods often fall short as the transferability aspect is often overlooked, thus restricting their application only to a specific target with limited performance. To address these challenges, we present Adversarial Camouflage for Transferable and Intensive Vehicle Evasion (ACTIVE), a state-of-the-art physical camouflage attack framework designed to generate universal and robust adversarial camouflage capable of concealing any 3D vehicle from detectors. Our framework incorporates innovative techniques to enhance universality and robustness, including a refined texture rendering that enables common texture application to different vehicles without being constrained to a specific texture map, a novel stealth loss that renders the vehicle undetectable, and a smooth and camouflage loss to enhance the naturalness of the adversarial camouflage. Our extensive experiments on 15 different models show that ACTIVE consistently outperforms existing works on various public detectors, including the latest YOLOv7. Notably, our universality evaluations reveal promising transferability to other vehicle classes, tasks (segmentation models), and the real world, not just other vehicles",
    "checked": true,
    "id": "dabb268b8bf3cdb99487219b4ff6d8948266105b",
    "semantic_title": "active: towards highly transferable 3d physical camouflage for universal and robust vehicle evasion",
    "citation_count": 0,
    "authors": [
      "Naufal Suryanto",
      "Yongsu Kim",
      "Harashta Tatimma Larasati",
      "Hyoeun Kang",
      "Thi-Thu-Huong Le",
      "Yoonyoung Hong",
      "Hunmin Yang",
      "Se-Yoon Oh",
      "Howon Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Saha_Learning_Adaptive_Neighborhoods_for_Graph_Neural_Networks_ICCV_2023_paper.html": {
    "title": "Learning Adaptive Neighborhoods for Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph convolutional networks (GCNs) enable end-to-end learning on graph structured data. However, many works assume a given graph structure. When the input graph is noisy or unavailable, one approach is to construct or learn a latent graph structure. These methods typically fix the choice of node degree for the entire graph, which is suboptimal. Instead, we propose a novel end-to-end differentiable graph generator which builds graph topologies where each node selects both its neighborhood and its size. Our module can be readily integrated into existing pipelines involving graph convolution operations, replacing the predetermined or existing adjacency matrix with one that is learned, and optimized, as part of the general objective. As such it is applicable to any GCN. We integrate our module into trajectory prediction, point cloud classification and node classification pipelines resulting in improved accuracy over other structure-learning methods across a wide range of datasets and GCN backbones",
    "checked": true,
    "id": "f73f57f60fdbe404450ef93685cffd3de870c6b4",
    "semantic_title": "learning adaptive neighborhoods for graph neural networks",
    "citation_count": 1,
    "authors": [
      "Avishkar Saha",
      "Oscar Mendez",
      "Chris Russell",
      "Richard Bowden"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Equivariant_Similarity_for_Vision-Language_Foundation_Models_ICCV_2023_paper.html": {
    "title": "Equivariant Similarity for Vision-Language Foundation Models",
    "volume": "main",
    "abstract": "This study explores the concept of equivariance in vision-language foundation models (VLMs), focusing specifically on the multimodal similarity function that is not only the major training objective but also the core delivery to support downstream tasks. Unlike the existing image-text similarity objective which only categorizes matched pairs as similar and unmatched as dissimilar, equivariance also requires similarity to vary faithfully according to the semantic changes. This allows VLMs to generalize better to nuanced and unseen multimodal compositions. However, modeling equivariance is challenging as the ground truth of semantic change is difficult to collect. For example, given an image-text pair about a dog, it is unclear to what extent the similarity changes when the pixel is changed from dog to cat? To this end, we propose EqSim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning. Meanwhile, to further diagnose the equivariance of VLMs, we present a new challenging benchmark EqBen. Compared to the existing evaluation sets, EqBen is the first to focus on \"visual-minimal change\". Extensive experiments show the lack of equivariance in current VLMs and validate the effectiveness of EqSim",
    "checked": true,
    "id": "fc8988585c6846fdeee33b34779a6a87b92c3e86",
    "semantic_title": "equivariant similarity for vision-language foundation models",
    "citation_count": 4,
    "authors": [
      "Tan Wang",
      "Kevin Lin",
      "Linjie Li",
      "Chung-Ching Lin",
      "Zhengyuan Yang",
      "Hanwang Zhang",
      "Zicheng Liu",
      "Lijuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_ReST_A_Reconfigurable_Spatial-Temporal_Graph_Model_for_Multi-Camera_Multi-Object_Tracking_ICCV_2023_paper.html": {
    "title": "ReST: A Reconfigurable Spatial-Temporal Graph Model for Multi-Camera Multi-Object Tracking",
    "volume": "main",
    "abstract": "Multi-Camera Multi-Object Tracking (MC-MOT) utilizes information from multiple views to better handle problems with occlusion and crowded scenes. Recently, the use of graph-based approaches to solve tracking problems has become very popular. However, many current graph-based methods do not effectively utilize information regarding spatial and temporal consistency. Instead, they rely on single-camera trackers as input, which are prone to fragmentation and ID switch errors. In this paper, we propose a novel reconfigurable graph model that first associates all detected objects across cameras spatially before reconfiguring it into a temporal graph for Temporal Association. This two-stage association approach enables us to extract robust spatial and temporal-aware features and address the problem with fragmented tracklets. Furthermore, our model is designed for online tracking, making it suitable for real-world applications. Experimental results show that the proposed graph model is able to extract more discriminating features for object tracking, and our model achieves state-of-the-art performance on several public datasets. Code is available at https://github.com/chengche6230/ReST",
    "checked": true,
    "id": "d6a7b9a8cc20e8c1e3b32380d13ea16f5e5c5bce",
    "semantic_title": "rest: a reconfigurable spatial-temporal graph model for multi-camera multi-object tracking",
    "citation_count": 1,
    "authors": [
      "Cheng-Che Cheng",
      "Min-Xuan Qiu",
      "Chen-Kuo Chiang",
      "Shang-Hong Lai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Too_Large_Data_Reduction_for_Vision-Language_Pre-Training_ICCV_2023_paper.html": {
    "title": "Too Large; Data Reduction for Vision-Language Pre-Training",
    "volume": "main",
    "abstract": "This paper examines the problems of severe image-text misalignment and high redundancy in the widely-used large-scale Vision-Language Pre-Training (VLP) datasets. To address these issues, we propose an efficient and straightforward Vision-Language learning algorithm called TL;DR which aims to compress the existing large VLP data into a small, high-quality set. Our approach consists of two major steps. First, a codebook-based encoder-decoder captioner is developed to select representative samples. Second, a new caption is generated to complement the original captions for selected samples, mitigating the text-image misalignment problem while maintaining uniqueness. As the result, TL;DR enables us to reduce the large dataset into a small set of high-quality data, which can serve as an alternative pre-training dataset. This algorithm significantly speeds up the time-consuming pretraining process. Specifically, TL;DR can compress the mainstream VLP datasets at a high ratio, e.g., reduce well-cleaned CC3M dataset from 2.8M to 0.67M ( 24%) and noisy YFCC15M from 15M to 2.5M ( 16.7%). Extensive experiments with three popular VLP models over seven downstream tasks show that VLP model trained on the compressed dataset provided by TL;DR can perform similar or even better results compared with training on the full-scale dataset",
    "checked": true,
    "id": "d1efa2cde9adc02169e73f07e82e06f0f7b2862b",
    "semantic_title": "too large; data reduction for vision-language pre-training",
    "citation_count": 0,
    "authors": [
      "Alex Jinpeng Wang",
      "Kevin Qinghong Lin",
      "David Junhao Zhang",
      "Stan Weixian Lei",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Make-It-3D_High-fidelity_3D_Creation_from_A_Single_Image_with_Diffusion_ICCV_2023_paper.html": {
    "title": "Make-It-3D: High-fidelity 3D Creation from A Single Image with Diffusion Prior",
    "volume": "main",
    "abstract": "In this work, we investigate the problem of creating high-fidelity 3D content from only a single image. This is inherently challenging: it essentially involves estimating the underlying 3D geometry while hallucinating unseen textures. To address this challenge, we leverage prior knowledge in a well-trained 2D diffusion model to serve as a 3D-aware supervision for 3D creation. Our proposed method, Make-It-3D, employs a two-stage optimization pipeline: the first stage optimizes a neural radiance field with constraints from the reference image and diffusion prior; the second stage builds textured point clouds from the coarse model and further enhances the textures with diffusion prior leveraging the availability of high-quality textures from the reference image. Extensive experiments show that our method achieves a clear improvement over previous works, displaying faithful reconstruction and impressive visual quality. Our method presents the first attempt to achieve high-quality 3D creation from a single image for general objects, and enables various applications such as text-to-3D creation and texture editing",
    "checked": true,
    "id": "d84616f108ccbd958735fef7622e58d148b32139",
    "semantic_title": "make-it-3d: high-fidelity 3d creation from a single image with diffusion prior",
    "citation_count": 42,
    "authors": [
      "Junshu Tang",
      "Tengfei Wang",
      "Bo Zhang",
      "Ting Zhang",
      "Ran Yi",
      "Lizhuang Ma",
      "Dong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_Towards_Deeply_Unified_Depth-aware_Panoptic_Segmentation_with_Bi-directional_Guidance_Learning_ICCV_2023_paper.html": {
    "title": "Towards Deeply Unified Depth-aware Panoptic Segmentation with Bi-directional Guidance Learning",
    "volume": "main",
    "abstract": "Depth-aware panoptic segmentation is an emerging topic in computer vision which combines semantic and geometric understanding for more robust scene interpretation. Recent works pursue unified frameworks to tackle this challenge but mostly still treat it as two individual learning tasks, which limits their potential for exploring cross-domain information. We propose a deeply unified framework for depth-aware panoptic segmentation, which performs joint segmentation and depth estimation both in a per-segment manner with identical object queries. To narrow the gap between the two tasks, we further design a geometric query enhancement method, which is able to integrate scene geometry into object queries using latent representations. In addition, we propose a bi-directional guidance learning approach to facilitate cross-task feature learning by taking advantage of their mutual relations. Our method sets the new state of the art for depth-aware panoptic segmentation on both Cityscapes-DVPS and SemKITTI-DVPS datasets. Moreover, our guidance learning approach is shown to deliver performance improvement even under incomplete supervision labels. Code and models are available at https://github.com/jwh97nn/DeepDPS",
    "checked": true,
    "id": "b441d9da7ab790da509d152c881665015cc8a692",
    "semantic_title": "towards deeply unified depth-aware panoptic segmentation with bi-directional guidance learning",
    "citation_count": 0,
    "authors": [
      "Junwen He",
      "Yifan Wang",
      "Lijun Wang",
      "Huchuan Lu",
      "Bin Luo",
      "Jun-Yan He",
      "Jin-Peng Lan",
      "Yifeng Geng",
      "Xuansong Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fan_Taxonomy_Adaptive_Cross-Domain_Adaptation_in_Medical_Imaging_via_Optimization_Trajectory_ICCV_2023_paper.html": {
    "title": "Taxonomy Adaptive Cross-Domain Adaptation in Medical Imaging via Optimization Trajectory Distillation",
    "volume": "main",
    "abstract": "The success of automated medical image analysis depends on large-scale and expert-annotated training sets. Unsupervised domain adaptation (UDA) has been raised as a promising approach to alleviate the burden of labeled data collection. However, they generally operate under the closed-set adaptation setting assuming an identical label set between the source and target domains, which is over-restrictive in clinical practice where new classes commonly exist across datasets due to taxonomic inconsistency. While several methods have been presented to tackle both domain shifts and incoherent label sets, none of them take into account the common characteristics of the two issues and consider the learning dynamics along network training. In this work, we propose optimization trajectory distillation, a unified approach to address the two technical challenges from a new perspective. It exploits the low-rank nature of gradient space and devises a dual-stream distillation algorithm to regularize the learning dynamics of insufficiently annotated domain and classes with the external guidance obtained from reliable sources. Our approach resolves the issue of inadequate navigation along network optimization, which is the major obstacle in the taxonomy adaptive cross-domain adaptation scenario. We evaluate the proposed method extensively on several tasks towards various endpoints with clinical significance. The results demonstrate its effectiveness and improvements over previous methods",
    "checked": true,
    "id": "39aed00aa60add541e26663b14695196d006d75d",
    "semantic_title": "taxonomy adaptive cross-domain adaptation in medical imaging via optimization trajectory distillation",
    "citation_count": 0,
    "authors": [
      "Jianan Fan",
      "Dongnan Liu",
      "Hang Chang",
      "Heng Huang",
      "Mei Chen",
      "Weidong Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nag_DiffTAD_Temporal_Action_Detection_with_Proposal_Denoising_Diffusion_ICCV_2023_paper.html": {
    "title": "DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion",
    "volume": "main",
    "abstract": "We propose a new formulation of temporal action detection (TAD) with denoising diffusion, DiffTAD in short. Taking as input random temporal proposals, it can yield action proposals accurately given an untrimmed long video. This presents a generative modeling perspective, against previous discriminative learning manners. This capability is achieved by first diffusing the ground-truth proposals to random ones (i.e, the forward/noising process) and then learning to reverse the noising process (i.e, the backward/denoising process). Concretely, we establish the denoising process in the Transformer decoder (e.g, DETR) by introducing a temporal location query design with faster convergence in training. We further propose a cross-step selective conditioning algorithm for inference acceleration. Extensive evaluations on ActivityNet and THUMOS show that our DiffTAD achieves top performance compared to previous art alternatives. The code is available at https://github.com/sauradip/DiffusionTAD",
    "checked": true,
    "id": "e6b767442bc3e9442651016a7db121777614946d",
    "semantic_title": "difftad: temporal action detection with proposal denoising diffusion",
    "citation_count": 4,
    "authors": [
      "Sauradip Nag",
      "Xiatian Zhu",
      "Jiankang Deng",
      "Yi-Zhe Song",
      "Tao Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Ray_Conditioning_Trading_Photo-consistency_for_Photo-realism_in_Multi-view_Image_Generation_ICCV_2023_paper.html": {
    "title": "Ray Conditioning: Trading Photo-consistency for Photo-realism in Multi-view Image Generation",
    "volume": "main",
    "abstract": "Multi-view image generation attracts particular attention these days due to its promising 3D-related applications, e.g., image viewpoint editing. Most existing methods follow a paradigm where a 3D representation is first synthesized, and then rendered into 2D images to ensure photo-consistency across viewpoints. However, such explicit bias for photo-consistency sacrifices photo-realism, causing geometry artifacts and loss of fine-scale details when these methods are applied to edit real images. To address this issue, we propose ray conditioning, a geometry-free alternative that relaxes the photo-consistency constraint. Our method generates multi-view images by conditioning a 2D GAN on a light field prior. With explicit viewpoint control, state-of-the-art photo-realism and identity consistency, our method is particularly suited for the viewpoint editing task",
    "checked": true,
    "id": "1f2e729e103f2832699e16f62a45b489a516bff6",
    "semantic_title": "ray conditioning: trading photo-consistency for photo-realism in multi-view image generation",
    "citation_count": 0,
    "authors": [
      "Eric Ming Chen",
      "Sidhanth Holalkere",
      "Ruyu Yan",
      "Kai Zhang",
      "Abe Davis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_SCOB_Universal_Text_Understanding_via_Character-wise_Supervised_Contrastive_Learning_with_ICCV_2023_paper.html": {
    "title": "SCOB: Universal Text Understanding via Character-wise Supervised Contrastive Learning with Online Text Rendering for Bridging Domain Gap",
    "volume": "main",
    "abstract": "Inspired by the great success of language model (LM)-based pre-training, recent studies in visual document understanding have explored LM-based pre-training methods for modeling text within document images. Among them, pre-training that reads all text from an image has shown promise, but often exhibits instability and even fails when applied to broader domains, such as those involving both visual documents and scene text images. This is a substantial limitation for real-world scenarios, where the processing of text image inputs in diverse domains is essential. In this paper, we investigate effective pre-training tasks in the broader domains and also propose a novel pre-training method called SCOB that leverages character-wise supervised contrastive learning with online text rendering to effectively pre-train document and scene text domains by bridging the domain gap. Moreover, SCOB enables weakly supervised learning, significantly reducing annotation costs. Extensive benchmarks demonstrate that SCOB generally improves vanilla pre-training methods and achieves comparable performance to state-of-the-art methods. Our findings suggest that SCOB can be served generally and effectively for read-type pre-training methods. The code will be available at https://github.com/naver-ai/scob",
    "checked": true,
    "id": "30f780ca1e4f5ad09477ca70e13c5a5c1d81a0ad",
    "semantic_title": "scob: universal text understanding via character-wise supervised contrastive learning with online text rendering for bridging domain gap",
    "citation_count": 0,
    "authors": [
      "Daehee Kim",
      "Yoonsik Kim",
      "DongHyun Kim",
      "Yumin Lim",
      "Geewook Kim",
      "Taeho Kil"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Point-Query_Quadtree_for_Crowd_Counting_Localization_and_More_ICCV_2023_paper.html": {
    "title": "Point-Query Quadtree for Crowd Counting, Localization, and More",
    "volume": "main",
    "abstract": "We show that crowd counting can be viewed as a decomposable point querying process. This formulation enables arbitrary points as input and jointly reasons whether the points are crowd and where they locate. The querying processing, however, raises an underlying problem on the number of necessary querying points. Too few imply underestimation; too many increase computational overhead. To address this dilemma, we introduce a decomposable structure, i.e., the point-query quadtree, and propose a new counting model, termed Point quEry Transformer (PET). PET implements decomposable point querying via data-dependent quadtree splitting, where each querying point could split into four new points when necessary, thus enabling dynamic processing of sparse and dense regions. Such a querying process yields an intuitive, universal modeling of crowd as both the input and output are interpretable and steerable. We demonstrate the applications of PET on a number of crowd-related tasks, including fully-supervised crowd counting and localization, partial annotation learning, and point annotation refinement, and also report state-of-the-art performance. For the first time, we show that a single counting model can address multiple crowd-related tasks across different learning paradigms. Code is available at https://github.com/cxliu0/PET",
    "checked": true,
    "id": "a29bf46e3fed01644344392f7b5eb92e707dfa03",
    "semantic_title": "point-query quadtree for crowd counting, localization, and more",
    "citation_count": 0,
    "authors": [
      "Chengxin Liu",
      "Hao Lu",
      "Zhiguo Cao",
      "Tongliang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Heterogeneous_Diversity_Driven_Active_Learning_for_Multi-Object_Tracking_ICCV_2023_paper.html": {
    "title": "Heterogeneous Diversity Driven Active Learning for Multi-Object Tracking",
    "volume": "main",
    "abstract": "The existing one-stage multi-object tracking (MOT) algorithms have achieved satisfactory performance benefiting from a large amount of labeled data. However, acquiring plenty of laborious annotated frames is not practical in real applications. To reduce the cost of human annotations, we propose Heterogeneous Diversity driven Active Multi-Object Tracking (HD-AMOT), to infer the most informative frames for any MOT tracker by observing the heterogeneous cues of samples. HD-AMOT defines the diversified informative representation by encoding the geometric and semantic information, and formulates the frame inference strategy as a Markov decision process to learn an optimal sampling policy based on the designed informative representation. Specifically, HD-AMOT consists of a diversified informative representation module as well as an informative frame selection network. The former produces the signal characterizing the diversity and distribution of frames, and the latter receives the signal and conducts multi-frame cooperation to enable batch frame sampling. Extensive experiments conducted on the MOT15, MOT17, MOT20, and Dancetrack datasets demonstrate the efficacy and effectiveness of HD-AMOT. Experiments show that under 50% budget our HD-AMOT can achieve similar or even higher performance as fully-supervised learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Li",
      "Baopeng Zhang",
      "Jun Liu",
      "Wei Liu",
      "Jian Zhao",
      "Zhu Teng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sanchez_Domain_Generalization_of_3D_Semantic_Segmentation_in_Autonomous_Driving_ICCV_2023_paper.html": {
    "title": "Domain Generalization of 3D Semantic Segmentation in Autonomous Driving",
    "volume": "main",
    "abstract": "Using deep learning, 3D autonomous driving semantic segmentation has become a well-studied subject, with methods that can reach very high performance. Nonetheless, because of the limited size of the training datasets, these models cannot see every type of object and scene found in real-world applications. The ability to be reliable in these various unknown environments is called domain generalization. Despite its importance, domain generalization is relatively unexplored in the case of 3D autonomous driving semantic segmentation. To fill this gap, this paper presents the first benchmark for this application by testing state-of-the-art methods and discussing the difficulty of tackling Laser Imaging Detection and Ranging (LiDAR) domain shifts. We also propose the first method designed to address this domain generalization, which we call 3DLabelProp. This method relies on leveraging the geometry and sequentiality of the LiDAR data to enhance its generalization performances by working on partially accumulated point clouds. It reaches a mean Intersection over Union (mIoU) of 50.4% on SemanticPOSS and of 55.2% on PandaSet solid-state LiDAR while being trained only on SemanticKITTI, making it the state-of-the-art method for generalization (+5% and +33% better, respectively, than the second best method)",
    "checked": true,
    "id": "225c56b54c480c48e719f0cd9a2c2326a9d4f188",
    "semantic_title": "domain generalization of 3d semantic segmentation in autonomous driving",
    "citation_count": 6,
    "authors": [
      "Jules Sanchez",
      "Jean-Emmanuel Deschaud",
      "FranÃ§ois Goulette"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_HaMuCo_Hand_Pose_Estimation_via_Multiview_Collaborative_Self-Supervised_Learning_ICCV_2023_paper.html": {
    "title": "HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning",
    "volume": "main",
    "abstract": "Recent advancements in 3D hand pose estimation have shown promising results, but its effectiveness has primarily relied on the availability of large-scale annotated datasets, the creation of which is a laborious and costly process. To alleviate the label-hungry limitation, we propose a self-supervised learning framework, HaMuCo, that learns a single view hand pose estimator from multi-view pseudo 2D labels. However, one of the main challenges of self-supervised learning is the presence of noisy labels and the \"groupthink\" effect from multiple views. To overcome these issues, we introduce a cross-view interaction network that distills the single view estimator by utilizing the cross-view correlated features and enforcing multi-view consistency to achieve collaborative learning. Both the single view estimator and the cross-view interaction network are trained jointly in an end-to-end manner. Extensive experiments show that our method can achieve state-of-the-art performance on multi-view self-supervised hand pose estimation. Furthermore, the proposed cross-view interaction network can also be applied to hand pose estimation from multi-view input and outperforms previous methods under same settings",
    "checked": true,
    "id": "f156251f22d38e24d85d29ff80955f7bd74986cc",
    "semantic_title": "hamuco: hand pose estimation via multiview collaborative self-supervised learning",
    "citation_count": 0,
    "authors": [
      "Xiaozheng Zheng",
      "Chao Wen",
      "Zhou Xue",
      "Pengfei Ren",
      "Jingyu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Efficient_Model_Personalization_in_Federated_Learning_via_Client-Specific_Prompt_Generation_ICCV_2023_paper.html": {
    "title": "Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation",
    "volume": "main",
    "abstract": "Federated learning (FL) emerges as a decentralized learning framework which trains models from multiple distributed clients without sharing their data to preserve privacy. Recently, large-scale pre-trained models (e.g., Vision Transformer) have shown a strong capability of deriving robust representations. However, the data heterogeneity among clients, the limited computation resources, and the communication bandwidth restrict the deployment of large-scale models in FL frameworks. To leverage robust representations from large-scale models while enabling efficient model personalization for heterogeneous clients, we propose a novel personalized FL framework of client-specific Prompt Generation (pFedPG), which learns to deploy a personalized prompt generator at the server for producing client-specific visual prompts that efficiently adapts frozen backbones to local data distributions. Our proposed framework jointly optimizes the stages of personalized prompt adaptation locally and personalized prompt generation globally. The former aims to train visual prompts that adapt foundation models to each client, while the latter observes local optimization directions to generate personalized prompts for all clients. Through extensive experiments on benchmark datasets, we show that our pFedPG is favorable against state-of-the-art personalized FL methods under various types of data heterogeneity, allowing computation and communication efficient model personalization",
    "checked": true,
    "id": "b53dba04b2518ebed943daa9ab58f19af81e2012",
    "semantic_title": "efficient model personalization in federated learning via client-specific prompt generation",
    "citation_count": 2,
    "authors": [
      "Fu-En Yang",
      "Chien-Yi Wang",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Dual_Aggregation_Transformer_for_Image_Super-Resolution_ICCV_2023_paper.html": {
    "title": "Dual Aggregation Transformer for Image Super-Resolution",
    "volume": "main",
    "abstract": "Transformer has recently gained considerable popularity in low-level vision tasks, including image super-resolution (SR). These networks utilize self-attention along different dimensions, spatial or channel, and achieve impressive performance. This inspires us to combine the two dimensions in Transformer for a more powerful representation capability. Based on the above idea, we propose a novel Transformer model, Dual Aggregation Transformer (DAT), for image SR. Our DAT aggregates features across spatial and channel dimensions, in the inter-block and intra-block dual manner. Specifically, we alternately apply spatial and channel self-attention in consecutive Transformer blocks. The alternate strategy enables DAT to capture the global context and realize inter-block feature aggregation. Furthermore, we propose the adaptive interaction module (AIM) and the spatial-gate feed-forward network (SGFN) to achieve intra-block feature aggregation. AIM complements two self-attention mechanisms from corresponding dimensions. Meanwhile, SGFN introduces additional non-linear spatial information in the feed-forward network. Extensive experiments show that our DAT surpasses current methods. Code and models are obtainable at https://github.com/zhengchen1999/DAT",
    "checked": true,
    "id": "f41c3fafa5c14f73984161551a9ca314effbe731",
    "semantic_title": "dual aggregation transformer for image super-resolution",
    "citation_count": 2,
    "authors": [
      "Zheng Chen",
      "Yulun Zhang",
      "Jinjin Gu",
      "Linghe Kong",
      "Xiaokang Yang",
      "Fisher Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Couairon_Zero-Shot_Spatial_Layout_Conditioning_for_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Zero-Shot Spatial Layout Conditioning for Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "Large-scale text-to-image diffusion models have significantly improved the state of the art in generative image modeling and allow for an intuitive and powerful user interface to drive the image generation process. Expressing spatial constraints, e.g. to position specific objects in particular locations, is cumbersome using text; and current text-based image generation models are not able to accurately follow such instructions. In this paper we consider image generation from text associated with segments on the image canvas, which combines an intuitive natural language interface with precise spatial control over the generated content. We propose ZestGuide, a \"ZEro-shot\" SegmenTation Guidance approach that can be plugged into pre-trained text-to-image diffusion models, and does not require any additional training. It leverages implicit segmentation maps that can be extracted from cross-attention layers, and uses them to align the generation with input masks. Our experimental results combine high image quality with accurate alignment of generated content with input segmentations, and improve over prior work both quantitatively and qualitatively, including methods that require training on images with corresponding segmentations. Compared to Paint with Words, the previous state-of-the art in image generation with zero-shot segmentation conditioning, we improve by 5 to 10 mIoU points on the COCO dataset with similar FID scores",
    "checked": true,
    "id": "d115238c6ee8fcdd635247f871d25732b457d1d3",
    "semantic_title": "zero-shot spatial layout conditioning for text-to-image diffusion models",
    "citation_count": 1,
    "authors": [
      "Guillaume Couairon",
      "MarlÃ¨ne Careil",
      "Matthieu Cord",
      "StÃ©phane LathuiliÃ¨re",
      "Jakob Verbeek"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_SegGPT_Towards_Segmenting_Everything_in_Context_ICCV_2023_paper.html": {
    "title": "SegGPT: Towards Segmenting Everything in Context",
    "volume": "main",
    "abstract": "We present SegGPT, a generalist model for segmenting everything in context. We unify various segmentation tasks into a generalist in-context learning framework that accommodates different kinds of segmentation data by transforming them into the same format of images. The training of SegGPT is formulated as an in-context coloring problem with random color mapping for each data sample. The objective is to accomplish diverse tasks according to the context, rather than relying on specific colors. After training, SegGPT can perform arbitrary segmentation tasks in images or videos via in-context inference, such as object instance, stuff, part, contour, and text. SegGPT is evaluated on a broad range of tasks, including few-shot semantic segmentation, video object segmentation, semantic segmentation, and panoptic segmentation. Our results show strong capabilities in segmenting in-domain and out-of-domain targets, either qualitatively or quantitatively",
    "checked": false,
    "id": "90af7c6cdf4b3359f6d275afb436f54f60082364",
    "semantic_title": "seggpt: segmenting everything in context",
    "citation_count": 51,
    "authors": [
      "Xinlong Wang",
      "Xiaosong Zhang",
      "Yue Cao",
      "Wen Wang",
      "Chunhua Shen",
      "Tiejun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gralnik_Semantify_Simplifying_the_Control_of_3D_Morphable_Models_Using_CLIP_ICCV_2023_paper.html": {
    "title": "Semantify: Simplifying the Control of 3D Morphable Models Using CLIP",
    "volume": "main",
    "abstract": "We present Semantify: a self-supervised method that utilizes the semantic power of CLIP language-vision foundation model to simplify the control of 3D morphable models. Given a parametric model, training data is created by randomly sampling the model's parameters, creating various shapes and rendering them. The similarity between the output images and a set of word descriptors is calculated in CLIP's latent space. Our key idea is first to choose a small set of semantically meaningful and disentangled descriptors that characterize the 3DMM, and then learn a non-linear mapping from scores across this set to the parametric coefficients of the given 3DMM. The non-linear mapping is defined by training a neural network without a human-in-the-loop. We present results on numerous 3DMMs: body shape models, face shape and expression models, as well as animal shapes. We demonstrate how our method defines a simple slider interface for intuitive modeling, and show how the mapping can be used to instantly fit a 3D parametric body shape to in-the-wild images",
    "checked": true,
    "id": "650535039f319c7a1ffe87da0cd6eddecce418e5",
    "semantic_title": "semantify: simplifying the control of 3d morphable models using clip",
    "citation_count": 0,
    "authors": [
      "Omer Gralnik",
      "Guy Gafni",
      "Ariel Shamir"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_From_Sky_to_the_Ground_A_Large-scale_Benchmark_and_Simple_ICCV_2023_paper.html": {
    "title": "From Sky to the Ground: A Large-scale Benchmark and Simple Baseline Towards Real Rain Removal",
    "volume": "main",
    "abstract": "Learning-based image deraining methods have made great progress. However, the lack of large-scale high-quality paired training samples is the main bottleneck to hamper the real image deraining (RID). To address this dilemma and advance RID, we construct a Large-scale High-quality Paired real rain benchmark (LHP-Rain), including 3000 video sequences with 1 million high-resolution (1920*1080) frame pairs. The advantages of the proposed dataset over the existing ones are three-fold: rain with higher-diversity and larger-scale, image with higher-resolution and higher-quality ground-truth. Specifically, the real rains in LHP-Rain not only contain the classical rain streak/veiling/occlusion in the sky, but also the splashing on the ground overlooked by deraining community. Moreover, we propose a novel robust low-rank tensor recovery model to generate the GT with better separating the static background from the dynamic rain. In addition, we design a simple transformer-based single image deraining baseline, which simultaneously utilize the self-attention and cross-layer attention within the image and rain layer with discriminative feature representation. Extensive experiments verify the superiority of the proposed dataset and deraining method over state-of-the-art",
    "checked": true,
    "id": "4785f6e385b769aee4a60309c0e3d84534e13d8b",
    "semantic_title": "from sky to the ground: a large-scale benchmark and simple baseline towards real rain removal",
    "citation_count": 0,
    "authors": [
      "Yun Guo",
      "Xueyao Xiao",
      "Yi Chang",
      "Shumin Deng",
      "Luxin Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_Knowledge_Restore_and_Transfer_for_Multi-Label_Class-Incremental_Learning_ICCV_2023_paper.html": {
    "title": "Knowledge Restore and Transfer for Multi-Label Class-Incremental Learning",
    "volume": "main",
    "abstract": "Current class-incremental learning research mainly focuses on single-label classification tasks while multi-label class-incremental learning (MLCIL) with more practical application scenarios is rarely studied. Although there have been many anti-forgetting methods to solve the problem of catastrophic forgetting in single-label class-incremental learning, these methods have difficulty in solving the MLCIL problem due to label absence and information dilution problems. To solve these problems, we propose a Knowledge Restore and Transfer (KRT) framework including a dynamic pseudo-label (DPL) module to solve the label absence problem by restoring the knowledge of old classes to the new data and an incremental cross-attention (ICA) module with session-specific knowledge retention tokens storing knowledge and a unified knowledge transfer token transferring knowledge to solve the information dilution problem. Comprehensive experimental results on MS-COCO and PASCAL VOC datasets demonstrate the effectiveness of our method for improving recognition performance and mitigating forgetting on multi-label class-incremental learning tasks",
    "checked": true,
    "id": "90d9dac65c117d849463417e5e074f3935dfeac2",
    "semantic_title": "knowledge restore and transfer for multi-label class-incremental learning",
    "citation_count": 0,
    "authors": [
      "Songlin Dong",
      "Haoyu Luo",
      "Yuhang He",
      "Xing Wei",
      "Jie Cheng",
      "Yihong Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kang_DDColor_Towards_Photo-Realistic_Image_Colorization_via_Dual_Decoders_ICCV_2023_paper.html": {
    "title": "DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders",
    "volume": "main",
    "abstract": "Image colorization is a challenging problem due to multi-modal uncertainty and high ill-posedness. Directly training a deep neural network usually leads to incorrect semantic colors and low color richness. While transformer-based methods can deliver better results, they often rely on manually designed priors, suffer from poor generalization ability, and introduce color bleeding effects. To address these issues, we propose DDColor, an end-to-end method with dual decoders for image colorization. Our approach includes a pixel decoder and a query-based color decoder. The former restores the spatial resolution of the image, while the latter utilizes rich visual features to refine color queries, thus avoiding hand-crafted priors. Our two decoders work together to establish correlations between color and multi-scale semantic representations via cross-attention, significantly alleviating the color bleeding effect. Additionally, a simple yet effective colorfulness loss is introduced to enhance the color richness. Extensive experiments demonstrate that DDColor achieves superior performance to existing state-of-the-art works both quantitatively and qualitatively. The codes and models are publicly available",
    "checked": true,
    "id": "a3cd1e50cf950db0ec503f639a264d1a0ae7f2c6",
    "semantic_title": "ddcolor: towards photo-realistic image colorization via dual decoders",
    "citation_count": 1,
    "authors": [
      "Xiaoyang Kang",
      "Tao Yang",
      "Wenqi Ouyang",
      "Peiran Ren",
      "Lingzhi Li",
      "Xuansong Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Barkan_Visual_Explanations_via_Iterated_Integrated_Attributions_ICCV_2023_paper.html": {
    "title": "Visual Explanations via Iterated Integrated Attributions",
    "volume": "main",
    "abstract": "We introduce Iterated Integrated Attributions (IIA) - a generic method for explaining the predictions of vision models. IIA employs iterative integration across the input image, the internal representations generated by the model, and their gradients, yielding precise and focused explanation maps. We demonstrate the effectiveness of IIA through comprehensive evaluations across various tasks, datasets, and network architectures. Our results showcase that IIA produces accurate explanation maps, outperforming other state-of-the-art explanation techniques",
    "checked": false,
    "id": "34ca3843cb4e20649004ca7447233597693b3a3a",
    "semantic_title": "visual explanations from deep networks via riemann-stieltjes integrated gradient-based localization",
    "citation_count": 3,
    "authors": [
      "Oren Barkan",
      "âªYehonatan Elishaâ¬â",
      "Yuval Asher",
      "Amit Eshel",
      "Noam Koenigstein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_PanFlowNet_A_Flow-Based_Deep_Network_for_Pan-Sharpening_ICCV_2023_paper.html": {
    "title": "PanFlowNet: A Flow-Based Deep Network for Pan-Sharpening",
    "volume": "main",
    "abstract": "Pan-sharpening aims to generate a high-resolution multispectral (HRMS) image by integrating the spectral information of a low-resolution multispectral (LRMS) image with the texture details of a high-resolution panchromatic (PAN) image. It essentially inherits the ill-posed nature of the super-resolution (SR) task that diverse HRMS images can degrade into an LRMS image. However, existing deep learning-based methods recover only one HRMS image from the LRMS image and PAN image using a deterministic mapping, thus ignoring the diversity of the HRMS image. In this paper, to alleviate this ill-posed issue, we propose a flow-based pan-sharpening network (PanFlowNet) to directly learn the conditional distribution of HRMS image given LRMS image and PAN image instead of learning a deterministic mapping. Specifically, we first transform this unknown conditional distribution into a given Gaussian distribution by an invertible network, and the conditional distribution can thus be explicitly defined. Then, we design an invertible Conditional Affine Coupling Block (CACB) and further build the architecture of PanFlowNet by stacking a series of CACBs. Finally, the PanFlowNet is trained by maximizing the log-likelihood of the conditional distribution given a training set and can then be used to predict diverse HRMS images. The experimental results verify that the proposed PanFlowNet can generate various HRMS images given an LRMS image and a PAN image. Additionally, the experimental results on different kinds of satellite datasets also demonstrate the superiority of our PanFlowNet compared with other state-of-the-art methods both visually and quantitatively. Code is available at Github",
    "checked": true,
    "id": "b986bda71d7e8f5297c0c43f107bd9098a4c7fc4",
    "semantic_title": "panflownet: a flow-based deep network for pan-sharpening",
    "citation_count": 1,
    "authors": [
      "Gang Yang",
      "Xiangyong Cao",
      "Wenzhe Xiao",
      "Man Zhou",
      "Aiping Liu",
      "Xun Chen",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Domain_Generalization_via_Balancing_Training_Difficulty_and_Model_Capability_ICCV_2023_paper.html": {
    "title": "Domain Generalization via Balancing Training Difficulty and Model Capability",
    "volume": "main",
    "abstract": "Domain generalization (DG) aims to learn domaingeneralizable models from one or multiple source domains that can perform well in unseen target domains. Despite its recent progress, most existing work suffers from the misalignment between the difficulty level of training samples and the capability of contemporarily trained models, leading to over-fitting or under-fitting in the trained generalization model. We design MoDify, a Momentum Difficulty framework that tackles the misalignment by balancing the seesaw between the model's capability and the samples' difficulties along the training process. MoDify consists of two novel designs that collaborate to fight against the misalignment while learning domain-generalizable models. The first is MoDify-based Data Augmentation which exploits an RGB Shuffle technique to generate difficulty-aware training samples on the fly. The second is MoDify-based Network Optimization which dynamically schedules the training samples for balanced and smooth learning with appropriate difficulty. Without bells and whistles, a simple implementation of MoDify achieves superior performance across multiple benchmarks. In addition, MoDify can complement existing methods as a plug-in, and it is generic and can work for different visual recognition tasks",
    "checked": true,
    "id": "9ec4d93235f0e1924e073df42ca877b6dceb09df",
    "semantic_title": "domain generalization via balancing training difficulty and model capability",
    "citation_count": 0,
    "authors": [
      "Xueying Jiang",
      "Jiaxing Huang",
      "Sheng Jin",
      "Shijian Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wen_Pairwise_Similarity_Learning_is_SimPLE_ICCV_2023_paper.html": {
    "title": "Pairwise Similarity Learning is SimPLE",
    "volume": "main",
    "abstract": "In this paper, we focus on a general yet important learning problem, pairwise similarity learning (PSL). PSL subsumes a wide range of important applications, such as open-set face recognition, speaker verification, image retrieval and person re-identification. The goal of PSL is to learn a pairwise similarity function assigning a higher similarity score to positive pairs (i.e., a pair of samples with the same label) than to negative pairs (i.e., a pair of samples with different label). We start by identifying a key desideratum for PSL, and then discuss how existing methods can achieve this desideratum. We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification. Comprehensive experimental results on large-scale benchmarks show that our method performs significantly better than current state-of-the-art methods",
    "checked": true,
    "id": "f969f059b01be02f9995396b6cc397959b574635",
    "semantic_title": "pairwise similarity learning is simple",
    "citation_count": 0,
    "authors": [
      "Yandong Wen",
      "Weiyang Liu",
      "Yao Feng",
      "Bhiksha Raj",
      "Rita Singh",
      "Adrian Weller",
      "Michael J. Black",
      "Bernhard SchÃ¶lkopf"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_GO-SLAM_Global_Optimization_for_Consistent_3D_Instant_Reconstruction_ICCV_2023_paper.html": {
    "title": "GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction",
    "volume": "main",
    "abstract": "Neural implicit representations have recently demonstrated compelling results on dense Simultaneous Localization And Mapping (SLAM) but suffer from the accumulation of errors in camera tracking and distortion in the reconstruction. Purposely, we present GO-SLAM, a deep-learning-based dense visual SLAM framework globally optimizing poses and 3D reconstruction in real-time. Robust pose estimation is at its core, supported by efficient loop closing and online full bundle adjustment, which optimize per frame by utilizing the learned global geometry of the complete history of input frames. Simultaneously, we update the implicit and continuous surface representation on-the-fly to ensure global consistency of 3D reconstruction. Results on various synthetic and real-world datasets demonstrate that GO-SLAM outperforms state-of-the-art approaches at tracking robustness and reconstruction accuracy. Furthermore, GO-SLAM is versatile and can run with monocular, stereo, and RGB-D input",
    "checked": true,
    "id": "63685d33cbe058f0f693e983bc42d308f22fbd3e",
    "semantic_title": "go-slam: global optimization for consistent 3d instant reconstruction",
    "citation_count": 3,
    "authors": [
      "Youmin Zhang",
      "Fabio Tosi",
      "Stefano Mattoccia",
      "Matteo Poggi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_JOTR_3D_Joint_Contrastive_Learning_with_Transformers_for_Occluded_Human_ICCV_2023_paper.html": {
    "title": "JOTR: 3D Joint Contrastive Learning with Transformers for Occluded Human Mesh Recovery",
    "volume": "main",
    "abstract": "In this study, we focus on the problem of 3D human mesh recovery from a single image under obscured conditions. Most state-of-the-art methods aim to improve 2D alignment technologies, such as spatial averaging and 2D joint sampling. However, they tend to neglect the crucial aspect of 3D alignment by improving 3D representations. Furthermore, recent methods struggle to separate target human from occlusion or background in crowded scenes as they optimize the 3D space of target human with 3D joint coordinates as local supervision. To address these issues, a desirable method would involve a framework for fusing 2D and 3D features and a strategy for optimizing the 3D space globally. Therefore, this paper presents 3D JOint contrastive learning with TRansformers (JOTR) framework for handling occluded 3D human mesh recovery. Our method includes an encoder-decoder transformer architecture to fuse 2D and 3D representations for achieving 2D&3D aligned results in a coarse-to-fine manner and a novel 3D joint contrastive learning approach for adding explicitly global supervision for the 3D feature space. The contrastive learning approach includes two contrastive losses: joint-to-joint contrast for enhancing the similarity of semantically similar voxels (i.e., human joints), and joint-to-non-joint contrast for ensuring discrimination from others (e.g., occlusions and background). Qualitative and quantitative analyses demonstrate that our method outperforms state-of-the-art competitors on both occlusion-specific and standard benchmarks, significantly improving the reconstruction of occluded humans",
    "checked": true,
    "id": "7467a65d7d5f5de00aa9a3bf0dd53e90d750d96e",
    "semantic_title": "jotr: 3d joint contrastive learning with transformers for occluded human mesh recovery",
    "citation_count": 0,
    "authors": [
      "Jiahao Li",
      "Zongxin Yang",
      "Xiaohan Wang",
      "Jianxin Ma",
      "Chang Zhou",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_CLIP-Driven_Universal_Model_for_Organ_Segmentation_and_Tumor_Detection_ICCV_2023_paper.html": {
    "title": "CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection",
    "volume": "main",
    "abstract": "An increasing number of public datasets have shown a marked impact on automated organ segmentation and tumor detection. However, due to the small size and partially labeled problem of each dataset, as well as a limited investigation of diverse types of tumors, the resulting models are often limited to segmenting specific organs/tumors and ignore the semantics of anatomical structures, nor can they be extended to novel domains. To address these issues, we propose the CLIP-Driven Universal Model, which incorporates text embedding learned from Contrastive Language-Image Pre-training (CLIP) to segmentation models. This CLIP-based label encoding captures anatomical relationships, enabling the model to learn a structured feature embedding and segment 25 organs and 6 types of tumors. The proposed model is developed from an assembly of 14 datasets, using a total of 3,410 CT scans for training and then evaluated on 6,162 external CT scans from 3 additional datasets. We rank first on the Medical Segmentation Decathlon (MSD) public leaderboard and achieve state-of-the-art results on Beyond The Cranial Vault (BTCV). Additionally, the Universal Model is computationally more efficient (6xfaster) compared with dataset-specific models, generalized better to CT scans from varying sites, and shows stronger transfer learning performance on novel tasks",
    "checked": true,
    "id": "125632627bfad80c2c688bcbed7f3ee915de7359",
    "semantic_title": "clip-driven universal model for organ segmentation and tumor detection",
    "citation_count": 25,
    "authors": [
      "Jie Liu",
      "Yixiao Zhang",
      "Jie-Neng Chen",
      "Junfei Xiao",
      "Yongyi Lu",
      "Bennett A Landman",
      "Yixuan Yuan",
      "Alan Yuille",
      "Yucheng Tang",
      "Zongwei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Niu_NIR-assisted_Video_Enhancement_via_Unpaired_24-hour_Data_ICCV_2023_paper.html": {
    "title": "NIR-assisted Video Enhancement via Unpaired 24-hour Data",
    "volume": "main",
    "abstract": "Low-light video enhancement in the visible (VIS) range is important yet technically challenging, and it is likely to become more tractable by introducing near-infrared (NIR) information for assistance, which in turn arouses a new challenge on how to obtain appropriate multispectral data for model training. In this paper, we defend the feasibility and superiority of NIR-assisted low-light video enhancement results by using unpaired 24-hour data for the first time, which significantly eases data collection and improves generalization performance on in-the-wild data. By accounting for different physical characteristics between unpaired daytime and nighttime videos, we first propose to turn daytime NIR & VIS into \"nighttime mode\". Specifically, we design a heuristic yet physics-inspired relighting algorithm to produce realistic pseudo nighttime NIR, and use a resampling strategy followed by a noiseGAN for nighttime VIS conversion. We further devise a temporal-aware network for video enhancement that extracts and fuses bi-directional temporal streams and is trained using real daytime videos and pseudo nighttime videos. We capture multi-spectral data using a co-axial camera and contribute Fulltime Multi-Spectral Video Dataset (FMSVD), the first dataset including aligned 24-hour NIR & VIS videos. Compared to alternative methods, we achieve significantly improved video quality as well as generalization ability on in-the-wild data in terms of both evaluation metrics and visual judgment. Codes and Data Available: https://github.com/MyNiuuu/NVEU",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muyao Niu",
      "Zhihang Zhong",
      "Yinqiang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yenamandra_FACTS_First_Amplify_Correlations_and_Then_Slice_to_Discover_Bias_ICCV_2023_paper.html": {
    "title": "FACTS: First Amplify Correlations and Then Slice to Discover Bias",
    "volume": "main",
    "abstract": "Computer vision datasets frequently contain spurious correlations between task-relevant labels and (easy to learn) latent task-irrelevant attributes (e.g. context). Models trained on such datasets learn \"shortcuts\" and underperform on bias-conflicting slices of data where the correlation does not hold. In this work, we study the problem of identifying such slices to inform downstream bias mitigation strategies. We propose First Amplify Correlations and Then Slice (FACTS), wherein we first amplify correlations to fit a simple bias-aligned hypothesis via strongly regularized empirical risk minimization. Next, we perform correlation-aware slicing via mixture modeling in bias-aligned feature space to discover underperforming data slices that capture distinct correlations. Despite its simplicity, our method considerably improves over prior work (by as much as 35% precision@10) in correlation bias identification across a range of diverse evaluation settings. Our code is available at https://github.com/yvsriram/FACTS",
    "checked": true,
    "id": "363d760d0b7f727e83bb9df4086fc4be4712abee",
    "semantic_title": "facts: first amplify correlations and then slice to discover bias",
    "citation_count": 0,
    "authors": [
      "Sriram Yenamandra",
      "Pratik Ramesh",
      "Viraj Prabhu",
      "Judy Hoffman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ji_Anchor_Structure_Regularization_Induced_Multi-view_Subspace_Clustering_via_Enhanced_Tensor_ICCV_2023_paper.html": {
    "title": "Anchor Structure Regularization Induced Multi-view Subspace Clustering via Enhanced Tensor Rank Minimization",
    "volume": "main",
    "abstract": "The tensor-based multi-view subspace clustering algorithms have received widespread attention due to the powerful ability to capture high-order correlation across views. Although such algorithms have achieved remarkable success, they still suffer from three main issues: 1) The extremely high computational complexity makes tensor-based methods difficult to handle large-scale data sets. 2) The commonly used Tensor Nuclear Norm (TNN) treats different singular values equally and under-penalizes the noise components, resulting in a sub-optimal representation tensor. 3) The subspace-based methods usually ignore the local geometric structure of the original data. Being aware of these, we propose Anchor Structure Regularitation Induced Multi-view Subspace Clustering via Enhanced Tensor Rank Minimization (ASR-ETR). Specifically, an anchor representation tensor is constructed by using the anchor representation strategy rather than the self-representation strategy to reduce the time complexity, and an Anchor Structure Regularization (ASR) is employed to enhance the local geometric structure in the learned anchor-representation tensor. We further define an Enhanced Tensor Rank (ETR), which is a tighter surrogate of the tensor rank and more effective to drive the noise out. Moreover, an efficient iterative optimization algorithm is designed to solve the ASR-ETR, which enjoys both linear complexity and favorable convergence. Extensive experimental results on various data sets demonstrate the superiority of the proposed algorithm as compared to state-of-the-art methods",
    "checked": false,
    "id": "e51421fe9f330429b717c50fb67062ec1e067666",
    "semantic_title": "high-order complementarity induced fast multi-view clustering with enhanced tensor rank minimization",
    "citation_count": 0,
    "authors": [
      "Jintian Ji",
      "Songhe Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_VeRi3D_Generative_Vertex-based_Radiance_Fields_for_3D_Controllable_Human_Image_ICCV_2023_paper.html": {
    "title": "VeRi3D: Generative Vertex-based Radiance Fields for 3D Controllable Human Image Synthesis",
    "volume": "main",
    "abstract": "Unsupervised learning of 3D-aware generative adversarial networks has lately made much progress. Some recent work demonstrates promising results of learning human generative models using neural articulated radiance fields, yet their generalization ability and controllability lag behind parametric human models, i.e., they do not perform well when generalizing to novel pose/shape and are not part controllable. To solve these problems, we propose VeRi3D, a generative human vertex-based radiance field parameterized by vertices of the parametric human template, SMPL. We map each 3D point to the local coordinate system defined on its neighboring vertices, and use the corresponding vertex feature and local coordinates for mapping it to color and density values. We demonstrate that our simple approach allows for generating photorealistic human images with free control over camera pose, human pose, shape, as well as enabling part-level editing",
    "checked": true,
    "id": "c9994a9d14ca72bed0834a59cada4e4e05bd8d78",
    "semantic_title": "veri3d: generative vertex-based radiance fields for 3d controllable human image synthesis",
    "citation_count": 0,
    "authors": [
      "Xinya Chen",
      "Jiaxin Huang",
      "Yanrui Bin",
      "Lu Yu",
      "Yiyi Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ding_MOSE_A_New_Dataset_for_Video_Object_Segmentation_in_Complex_ICCV_2023_paper.html": {
    "title": "MOSE: A New Dataset for Video Object Segmentation in Complex Scenes",
    "volume": "main",
    "abstract": "Video object segmentation (VOS) aims at segmenting a particular object throughout the entire video clip sequence. The state-of-the-art VOS methods have achieved excellent performance (e.g., 90+% J&F) on existing datasets. However, since the target objects in these existing datasets are usually relatively salient, dominant, and isolated, VOS under complex scenes has rarely been studied. To revisit VOS and make it more applicable in the real world, we collect a new VOS dataset called coMplex video Object SEgmentation (MOSE) to study the tracking and segmenting objects in complex environments. MOSE contains 2,149 video clips and 5,200 objects from 36 categories, with 431,725 high-quality object segmentation masks. The most notable feature of MOSE dataset is complex scenes with crowded and occluded objects. The target objects in the videos are commonly occluded by others and disappear in some frames. To analyze the proposed MOSE dataset, we benchmark 18 existing VOS methods under 4 different settings on the proposed MOSE dataset and conduct comprehensive comparisons. The experiments show that current VOS algorithms cannot well perceive objects in complex scenes. For example, under the semi-supervised VOS setting, the highest J&F by existing state-of-the-art VOS methods is only 59.4% on MOSE, much lower than their 90% J&F performance on DAVIS. The results reveal that although excellent performance has been achieved on existing benchmarks, there are unresolved challenges under complex scenes and more efforts are desired to explore these challenges in the future",
    "checked": true,
    "id": "d69670ad6095b8c74c62a9c54980655dbaaba3b8",
    "semantic_title": "mose: a new dataset for video object segmentation in complex scenes",
    "citation_count": 21,
    "authors": [
      "Henghui Ding",
      "Chang Liu",
      "Shuting He",
      "Xudong Jiang",
      "Philip H.S. Torr",
      "Song Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_BoMD_Bag_of_Multi-label_Descriptors_for_Noisy_Chest_X-ray_Classification_ICCV_2023_paper.html": {
    "title": "BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification",
    "volume": "main",
    "abstract": "Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-Ray (CXR) classifiers have been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current multi-class noisy-label learning methods cannot be easily adapted. In this paper, we propose a new method designed for noisy multi-label CXR learning, which detects and smoothly re-labels noisy samples from the dataset to be used in the training of common multi-label classifiers. The proposed method optimises a bag of multi-label descriptors (BoMD) to promote their similarity with the semantic descriptors produced by language models from multi-label image annotations. Our experiments on noisy multi-label training sets and clean testing sets show that our model has state-of-the-art accuracy and robustness in many CXR multi-label classification benchmarks, including a new benchmark that we propose to systematically assess noisy multi-label methods",
    "checked": true,
    "id": "581aab89677a19631c1e99d48b9f65286460d3ba",
    "semantic_title": "bomd: bag of multi-label descriptors for noisy chest x-ray classification",
    "citation_count": 2,
    "authors": [
      "Yuanhong Chen",
      "Fengbei Liu",
      "Hu Wang",
      "Chong Wang",
      "Yuyuan Liu",
      "Yu Tian",
      "Gustavo Carneiro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lai_Mask-Attention-Free_Transformer_for_3D_Instance_Segmentation_ICCV_2023_paper.html": {
    "title": "Mask-Attention-Free Transformer for 3D Instance Segmentation",
    "volume": "main",
    "abstract": "Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer",
    "checked": true,
    "id": "1ae4de4f601b32636f52154e258187c98daaa82b",
    "semantic_title": "mask-attention-free transformer for 3d instance segmentation",
    "citation_count": 1,
    "authors": [
      "Xin Lai",
      "Yuhui Yuan",
      "Ruihang Chu",
      "Yukang Chen",
      "Han Hu",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_SHIFT3D_Synthesizing_Hard_Inputs_For_Tricking_3D_Detectors_ICCV_2023_paper.html": {
    "title": "SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors",
    "volume": "main",
    "abstract": "We present SHIFT3D, a differentiable pipeline for generating 3D shapes that are structurally plausible yet challenging to 3D object detectors. In safety-critical applications like autonomous driving, discovering such novel challenging objects can offer insight into unknown vulnerabilities of 3D detectors. By representing objects with a signed distanced function (SDF), we show that gradient error signals allow us to smoothly deform the shape or pose of a 3D object in order to confuse a downstream 3D detector. Importantly, the objects generated by SHIFT3D physically differ from the baseline object yet retain a semantically recognizable shape. Our approach provides interpretable failure modes for modern 3D object detectors, and can aid in preemptive discovery of potential safety risks within 3D perception systems before these risks become critical failures",
    "checked": true,
    "id": "dcd0410e49db0aeac786a4a8700ab27b91f50d52",
    "semantic_title": "shift3d: synthesizing hard inputs for tricking 3d detectors",
    "citation_count": 0,
    "authors": [
      "Hongge Chen",
      "Zhao Chen",
      "Gregory P. Meyer",
      "Dennis Park",
      "Carl Vondrick",
      "Ashish Shrivastava",
      "Yuning Chai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mai_EgoLoc_Revisiting_3D_Object_Localization_from_Egocentric_Videos_with_Visual_ICCV_2023_paper.html": {
    "title": "EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries",
    "volume": "main",
    "abstract": "With the recent advances in video and 3D understanding, novel 4D spatio-temporal methods fusing both concepts have emerged. Towards this direction, the Ego4D Episodic Memory Benchmark proposed a task for Visual Queries with 3D Localization (VQ3D). Given an egocentric video clip and an image crop depicting a query object, the goal is to localize the 3D position of the center of that query object with respect to the camera pose of a query frame. Current methods tackle the problem of VQ3D by unprojecting the 2D localization results of the sibling task Visual Queries with 2D Localization (VQ2D) into 3D predictions. Yet, we point out that the low number of camera poses caused by camera re-localization from previous VQ3D methods severally hinders their overall success rate. In this work, we formalize a pipeline (we dub EgoLoc) that better entangles 3D multiview geometry with 2D object retrieval from egocentric videos. Our approach involves estimating more robust camera poses and aggregating multi-view 3D displacements by leveraging the 2D detection confidence, which enhances the success rate of object queries and leads to a significant improvement in the VQ3D baseline performance. Specifically, our approach achieves an overall success rate of up to 87.12%, which sets a new state-of-the-art result in the VQ3D task. We provide a comprehensive empirical analysis of the VQ3D task and existing solutions, and highlight the remaining challenges in VQ3D. The code is available at https://github.com/Wayne-Mai/EgoLoc",
    "checked": true,
    "id": "2038a06da1f5df512d92055576dc8ab3049d1d10",
    "semantic_title": "egoloc: revisiting 3d object localization from egocentric videos with visual queries",
    "citation_count": 2,
    "authors": [
      "Jinjie Mai",
      "Abdullah Hamdi",
      "Silvio Giancola",
      "Chen Zhao",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Coordinate_Transformer_Achieving_Single-stage_Multi-person_Mesh_Recovery_from_Videos_ICCV_2023_paper.html": {
    "title": "Coordinate Transformer: Achieving Single-stage Multi-person Mesh Recovery from Videos",
    "volume": "main",
    "abstract": "Multi-person 3D mesh recovery from videos is a critical first step towards automatic perception of group behavior in virtual reality, physical therapy and beyond. However, existing approaches rely on multi-stage paradigms, where the person detection and tracking stages are performed in a multi-person setting, while temporal dynamics are only modeled for one person at a time. Consequently, their performance is severely limited by the lack of inter-person interactions in the spatial-temporal mesh recovery, as well as by detection and tracking defects. To address these challenges, we propose the Coordinate transFormer (CoordFormer) that directly models multi-person spatial-temporal relations and simultaneously performs multi-mesh recovery in an end-to-end manner. Instead of partitioning the feature map into coarse-scale patch-wise tokens, CoordFormer leverages a novel Coordinate-Aware Attention to preserve pixel-level spatial-temporal coordinate information. Additionally, we propose a simple, yet effective Body Center Attention mechanism to fuse position information. Extensive experiments on the 3DPW dataset demonstrate that CoordFormer significantly improves the state-of-the-art, outperforming the previously best results by 4.2%, 8.8% and 4.7% according to the MPJPE, PAMPJPE, and PVE metrics, respectively, while being 40% faster than recent video-based approaches. The released code can be found at https://github.com/Li-Hao-yuan/CoordFormer",
    "checked": true,
    "id": "27f740b8312b59b1c3344d020997f2d37a2a9a08",
    "semantic_title": "coordinate transformer: achieving single-stage multi-person mesh recovery from videos",
    "citation_count": 0,
    "authors": [
      "Haoyuan Li",
      "Haoye Dong",
      "Hanchao Jia",
      "Dong Huang",
      "Michael C. Kampffmeyer",
      "Liang Lin",
      "Xiaodan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_FLatten_Transformer_Vision_Transformer_using_Focused_Linear_Attention_ICCV_2023_paper.html": {
    "title": "FLatten Transformer: Vision Transformer using Focused Linear Attention",
    "volume": "main",
    "abstract": "The quadratic computation complexity of self-attention has been a persistent challenge when applying Transformer models to vision tasks. Linear attention, on the other hand, offers a much more efficient alternative with its linear complexity by approximating the Softmax operation through carefully designed mapping functions. However, current linear attention approaches either suffer from significant performance degradation or introduce additional computation overhead from the mapping functions. In this paper, we propose a novel Focused Linear Attention module to achieve both high efficiency and expressiveness. Specifically, we first analyze the factors contributing to the performance degradation of linear attention from two perspectives: the focus ability and feature diversity. To overcome these limitations, we introduce a simple yet effective mapping function and an efficient rank restoration module to enhance the expressiveness of self-attention while maintaining low computation complexity. Extensive experiments show that our linear attention module is applicable to a variety of advanced vision Transformers, and achieves consistently improved performances on multiple benchmarks. Code is available at https://github.com/LeapLabTHU/FLatten-Transformer",
    "checked": true,
    "id": "131ba9932572c92155874db93626cf299659254e",
    "semantic_title": "flatten transformer: vision transformer using focused linear attention",
    "citation_count": 2,
    "authors": [
      "Dongchen Han",
      "Xuran Pan",
      "Yizeng Han",
      "Shiji Song",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Q-Diffusion_Quantizing_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Q-Diffusion: Quantizing Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models have achieved great success in image synthesis through iterative noise estimation using deep neural networks. However, the slow inference, high memory consumption, and computation intensity of the noise estimation model hinder the efficient adoption of diffusion models. Although post-training quantization (PTQ) is considered a go-to compression method for other tasks, it does not work out-of-the-box on diffusion models. We propose a novel PTQ method specifically tailored towards the unique multi-timestep pipeline and model architecture of the diffusion models, which compresses the noise estimation network to accelerate the generation process. We identify the key difficulty of diffusion model quantization as the changing output distributions of noise estimation networks over multiple time steps and the bimodal activation distribution of the shortcut layers within the noise estimation network. We tackle these challenges with timestep-aware calibration and split shortcut quantization in this work. Experimental results show that our proposed method is able to quantize full-precision unconditional diffusion models into 4-bit while maintaining comparable performance (small FID change of at most 2.34 compared to >100 for traditional PTQ) in a training-free manner. Our approach can also be applied to text-guided image generation, where we can run stable diffusion in 4-bit weights with high generation quality for the first time",
    "checked": true,
    "id": "489ab1945feb21f17b3efbcf40726c8cbb52bb75",
    "semantic_title": "q-diffusion: quantizing diffusion models",
    "citation_count": 13,
    "authors": [
      "Xiuyu Li",
      "Yijiang Liu",
      "Long Lian",
      "Huanrui Yang",
      "Zhen Dong",
      "Daniel Kang",
      "Shanghang Zhang",
      "Kurt Keutzer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Robustifying_Token_Attention_for_Vision_Transformers_ICCV_2023_paper.html": {
    "title": "Robustifying Token Attention for Vision Transformers",
    "volume": "main",
    "abstract": "Despite the success of vision transformers (ViTs), they still suffer from significant drops in accuracy in the presence of common corruptions, such as noise or blur. Interestingly, we observe that the attention mechanism of ViTs tends to rely on few important tokens, a phenomenon we call token overfocusing. More critically, these tokens are not robust to corruptions, often leading to highly diverging attention patterns. In this paper, we intend to alleviate this overfocusing issue and make attention more stable through two general techniques: First, our Token-aware Average Pooling (TAP) module encourages the local neighborhood of each token to take part in the attention mechanism. Specifically, TAP learns average pooling schemes for each token such that the information of potentially important tokens in the neighborhood can adaptively be taken into account. Second, we force the output tokens to aggregate information from a diverse set of input tokens rather than focusing on just a few by using our Attention Diversification Loss (ADL). We achieve this by penalizing high cosine similarity between the attention vectors of different tokens. In experiments, we apply our methods to a wide range of transformer architectures and improve robustness significantly. For example, we improve corruption robustness on ImageNet-C by 2.4% while improving accuracy by 0.4% based on state-of-the-art robust architecture FAN. Also, when fine-tuning on semantic segmentation tasks, we improve robustness on CityScapes-C by 2.4% and ACDC by 3.0%. Our code is available at https://github.com/guoyongcs/TAPADL",
    "checked": true,
    "id": "f40b4d06f6d5836dfeb64a7139ba534d7e94bedf",
    "semantic_title": "robustifying token attention for vision transformers",
    "citation_count": 3,
    "authors": [
      "Yong Guo",
      "David Stutz",
      "Bernt Schiele"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Rachavarapu_Boosting_Positive_Segments_for_Weakly-Supervised_Audio-Visual_Video_Parsing_ICCV_2023_paper.html": {
    "title": "Boosting Positive Segments for Weakly-Supervised Audio-Visual Video Parsing",
    "volume": "main",
    "abstract": "In this paper, we address the problem of weakly supervised Audio-Visual Video Parsing (AVVP), where the goal is to temporally localize events that are audible or visible and simultaneously classify them into known event categories. This is a challenging task, as we only have access to the video-level event labels during training but need to predict event labels at the segment level during evaluation. Existing multiple-instance learning (MIL) based methods use a form of attentive pooling over segment-level predictions. These methods only optimize for a subset of most discriminative segments that satisfy the weak-supervision constraints, which miss identifying positive segments. To address this, we focus on improving the proportion of positive segments detected in a video. To this end, we model the number of positive segments in a video as a latent variable and show that it can be modeled as Poisson binomial distribution over segment-level predictions, which can be computed exactly. Given the absence of fine-grained supervision, we propose an Expectation-Maximization approach to learn the model parameters by maximizing the evidence lower bound (ELBO). We iteratively estimate the minimum positive segments in a video and refine them to capture more positive segments. We conducted extensive experiments on AVVP tasks to evaluate the effectiveness of our proposed approach, and the results clearly demonstrate that it increases the number of positive segments captured compared to existing methods. Additionally, our experiments on Temporal Action Localization (TAL) demonstrate the potential of our method for generalization to similar MIL tasks",
    "checked": false,
    "id": "2f5af52609c3da4616a43d08a1578ca656f6958e",
    "semantic_title": "dhhn: dual hierarchical hybrid network for weakly-supervised audio-visual video parsing",
    "citation_count": 8,
    "authors": [
      "Kranthi Kumar Rachavarapu",
      "Rajagopalan A. N."
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xiao_ADNet_Lane_Shape_Prediction_via_Anchor_Decomposition_ICCV_2023_paper.html": {
    "title": "ADNet: Lane Shape Prediction via Anchor Decomposition",
    "volume": "main",
    "abstract": "In this paper, we revisit the limitations of anchor-based lane detection methods, which have predominantly focused on fixed anchors that stem from the edges of the image, disregarding their versatility and quality. To overcome the inflexibility of anchors, we decompose them into learning the heat map of starting points and their associated directions. This decomposition removes the limitations on the starting point of anchors, making our algorithm adaptable to different lane types in various datasets. To enhance the quality of anchors, we introduce the Large Kernel Attention (LKA) for Feature Pyramid Network (FPN). This significantly increases the receptive field, which is crucial in capturing the sufficient context as lane lines typically run throughout the entire image. We have named our proposed system the Anchor Decomposition Network (ADNet). Additionally, we propose the General Lane IoU (GLIoU) loss, which significantly improves the performance of ADNet in complex scenarios. Experimental results on three widely used lane detection benchmarks, VIL-100, CULane, and TuSimple, demonstrate that our approach outperforms the state-of-the-art methods on VIL-100 and exhibits competitive accuracy on CULane and TuSimple. Code and models will be released on https://github.com/ Sephirex-X/ADNet",
    "checked": true,
    "id": "0e593637bfd92dcd99d329027243b873a2833980",
    "semantic_title": "adnet: lane shape prediction via anchor decomposition",
    "citation_count": 0,
    "authors": [
      "Lingyu Xiao",
      "Xiang Li",
      "Sen Yang",
      "Wankou Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_UniSeg_A_Unified_Multi-Modal_LiDAR_Segmentation_Network_and_the_OpenPCSeg_ICCV_2023_paper.html": {
    "title": "UniSeg: A Unified Multi-Modal LiDAR Segmentation Network and the OpenPCSeg Codebase",
    "volume": "main",
    "abstract": "Point-, voxel-, and range-views are three representative forms of point clouds. All of them have accurate 3D measurements but lack color and texture information. RGB images are a natural complement to these point cloud views and fully utilizing the comprehensive information of them benefits more robust perceptions. In this paper, we present a unified multi-modal LiDAR segmentation network, termed UniSeg, which leverages the information of RGB images and three views of the point cloud, and accomplishes semantic segmentation and panoptic segmentation simultaneously. Specifically, we first design the Learnable cross-Modal Association (LMA) module to automatically fuse voxel-view and range-view features with image features, which fully utilize the rich semantic information of images and are robust to calibration errors. Then, the enhanced voxel-view and range-view features are transformed to the point space, where three views of point cloud features are further fused adaptively by the Learnable cross-View Association module (LVA). Notably, UniSeg achieves promising results in three public benchmarks, i.e., SemanticKITTI, nuScenes, and Waymo Open Dataset (WOD); it ranks 1st on two challenges of two benchmarks, including the LiDAR semantic segmentation challenge of nuScenes and panoptic segmentation challenges of SemanticKITTI. Besides, we construct the OpenPCSeg codebase, which is the largest and most comprehensive outdoor LiDAR segmentation codebase. It contains most of the popular outdoor LiDAR segmentation algorithms and provides reproducible implementations. The OpenPCSeg codebase will be made publicly available at https://github.com/PJLab-ADG/PCSeg",
    "checked": true,
    "id": "7b6498faba13b4bca151dbeefdff608b09ce8152",
    "semantic_title": "uniseg: a unified multi-modal lidar segmentation network and the openpcseg codebase",
    "citation_count": 3,
    "authors": [
      "Youquan Liu",
      "Runnan Chen",
      "Xin Li",
      "Lingdong Kong",
      "Yuchen Yang",
      "Zhaoyang Xia",
      "Yeqi Bai",
      "Xinge Zhu",
      "Yuexin Ma",
      "Yikang Li",
      "Yu Qiao",
      "Yuenan Hou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yao_Sign_Language_Translation_with_Iterative_Prototype_ICCV_2023_paper.html": {
    "title": "Sign Language Translation with Iterative Prototype",
    "volume": "main",
    "abstract": "This paper presents IP-SLT, a simple yet effective framework for sign language translation (SLT). Our IP-SLT adopts a recurrent structure and enhances the semantic representation (prototype) of the input sign language video via an iterative refinement manner. Our idea mimics the behavior of human reading, where a sentence can be digested repeatedly, till reaching accurate understanding. Technically, IP-SLT consists of feature extraction, prototype initialization, and iterative prototype refinement. The initialization module generates the initial prototype based on the visual feature extracted by the feature extraction module. Then, the iterative refinement module leverages the cross-attention mechanism to polish the previous prototype by aggregating it with the original video feature. Through repeated refinement, the prototype finally converges to a more stable and accurate state, leading to a fluent and appropriate translation. In addition, to leverage the sequential dependence of prototypes, we further propose an iterative distillation loss to compress the knowledge of the final iteration into previous ones. As the autoregressive decoding process is executed only once in inference, our IP-SLT is ready to improve various SLT systems with acceptable overhead. Extensive experiments are conducted on public benchmarks to demonstrate the effectiveness of the IP-SLT",
    "checked": true,
    "id": "c34c074093fbbe2c6dcc96eba00c91415fba81c7",
    "semantic_title": "sign language translation with iterative prototype",
    "citation_count": 0,
    "authors": [
      "Huijie Yao",
      "Wengang Zhou",
      "Hao Feng",
      "Hezhen Hu",
      "Hao Zhou",
      "Houqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Pixel-Wise_Contrastive_Distillation_ICCV_2023_paper.html": {
    "title": "Pixel-Wise Contrastive Distillation",
    "volume": "main",
    "abstract": "We present a simple but effective pixel-level self-supervised distillation framework friendly to dense prediction tasks. Our method, called Pixel-Wise Contrastive Distillation (PCD), distills knowledge by attracting the corresponding pixels from student's and teacher's output feature maps. PCD includes a novel design called SpatialAdaptor which \"reshapes\" a part of the teacher network while preserving the distribution of its output features. Our ablation experiments suggest that this reshaping behavior enables more informative pixel-to-pixel distillation. Moreover, we utilize a plug-in multi-head self-attention module that explicitly relates the pixels of student's feature maps to enhance the effective receptive field, leading to a more competitive student. PCD outperforms previous self-supervised distillation methods on various dense prediction tasks. A backbone of ResNet-18-FPN distilled by PCD achieves 37.4 AP-bbox and 34.0 AP-mask on COCO dataset using the detector of Mask R-CNN. We hope our study will inspire future research on how to pre-train a small model friendly to dense prediction tasks in a self-supervised fashion",
    "checked": true,
    "id": "c43ad1aa0f5623ee5c3a7e57ce2fd85416ac2e91",
    "semantic_title": "pixel-wise contrastive distillation",
    "citation_count": 1,
    "authors": [
      "Junqiang Huang",
      "Zichao Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Efficient_Deep_Space_Filling_Curve_ICCV_2023_paper.html": {
    "title": "Efficient Deep Space Filling Curve",
    "volume": "main",
    "abstract": "Space-filling curves (SFCs) act as a linearization approach to map data in higher dimensional space to lower dimensional space, which is used comprehensively in computer vision, such as image/point cloud compression, hashing and etc. Currently, researchers formulate the problem of searching for an optimal SFC to the problem of finding a single Hamiltonian circuit on the image grid graph. Existing methods adopt graph neural networks (GNN) for SFC search. By modeling the pixel grid as a graph, they first adopt GNN to predict the edge weights and then generate a minimum spanning tree (MST) based on the predictions, which is further used to construct the SFC. However, GNN-based methods suffer from high computational costs and memory footprint usage. Besides, MST generation is un-differentiable, which is infeasible to optimize via gradient descent. To remedy these issues, we propose a GNN-based SFC-search framework with a tailored algorithm that largely reduces computational cost of GNN. Additionally, we propose a siamese network learning scheme to optimize DNN-based models in an end-to-end fashion. Extensive experiments show that our proposed method outperforms both DNN-based methods and traditional SFCs, e.g. Hilbert curve, by a large margin on various benchmarks",
    "checked": false,
    "id": "da13a4031cba951edee3b8157492970df858175b",
    "semantic_title": "a novel audio representation using space filling curves",
    "citation_count": 0,
    "authors": [
      "Wanli Chen",
      "Xufeng Yao",
      "Xinyun Zhang",
      "Bei Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qin_GlueGen_Plug_and_Play_Multi-modal_Encoders_for_X-to-image_Generation_ICCV_2023_paper.html": {
    "title": "GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation",
    "volume": "main",
    "abstract": "Text-to-image (T2I) models based on diffusion processes have achieved remarkable success in controllable image generation using user-provided captions. However, the tight coupling between the current text encoder and image decoder in T2I models makes it challenging to replace or upgrade. Such changes often require massive fine-tuning or even training from scratch with the prohibitive expense. To address this problem, we propose GlueGen, which applies a newly proposed GlueNet model to align features from single-modal or multi-modal encoders with the latent space of an existing T2I model. The approach introduces a new training objective that leverages parallel corpora to align the representation spaces of different encoders. Empirical results show that GlueNet can be trained efficiently and enables various capabilities beyond previous state-of-the-art models: 1) multilingual language models such as XLM-Roberta can be aligned with existing T2I models, allowing for the generation of high-quality images from captions beyond English; 2) GlueNet can align multi-modal encoders such as AudioCLIP with the Stable Diffusion model, enabling sound-to-image generation; 3) it can also upgrade the current text encoder of the latent diffusion model for challenging case generation. By the alignment of various feature representations, the GlueNet allows for flexible and efficient integration of new functionality into existing T2I models and sheds light on X-to-image (X2I) generation",
    "checked": true,
    "id": "32ff6ef526283742c0daafaa1fe454097ce88237",
    "semantic_title": "gluegen: plug and play multi-modal encoders for x-to-image generation",
    "citation_count": 5,
    "authors": [
      "Can Qin",
      "Ning Yu",
      "Chen Xing",
      "Shu Zhang",
      "Zeyuan Chen",
      "Stefano Ermon",
      "Yun Fu",
      "Caiming Xiong",
      "Ran Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Goel_Humans_in_4D_Reconstructing_and_Tracking_Humans_with_Transformers_ICCV_2023_paper.html": {
    "title": "Humans in 4D: Reconstructing and Tracking Humans with Transformers",
    "volume": "main",
    "abstract": "We present an approach to reconstruct humans and track them over time. At the core of our approach, we propose a fully \"transformerized\" version of a network for human mesh recovery. This network, HMR 2.0, advances the state of the art and shows the capability to analyze unusual poses that have in the past been difficult to reconstruct from single images. To analyze video, we use 3D reconstructions from HMR 2.0 as input to a tracking system that operates in 3D. This enables us to deal with multiple people and maintain identities through occlusion events. Our complete approach, 4DHumans, achieves state-of-the-art results for tracking people from monocular video. Furthermore, we demonstrate the effectiveness of HMR 2.0 on the downstream task of action recognition, achieving significant improvements over previous pose-based action recognition approaches. Our code and models are available on the project website: https://shubham-goel.github.io/4dhumans/",
    "checked": true,
    "id": "bec7eb3dcb597f7772a573d99f3be58e623e5a82",
    "semantic_title": "humans in 4d: reconstructing and tracking humans with transformers",
    "citation_count": 5,
    "authors": [
      "Shubham Goel",
      "Georgios Pavlakos",
      "Jathushan Rajasegaran",
      "Angjoo Kanazawa",
      "Jitendra Malik"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Ponder_Point_Cloud_Pre-training_via_Neural_Rendering_ICCV_2023_paper.html": {
    "title": "Ponder: Point Cloud Pre-training via Neural Rendering",
    "volume": "main",
    "abstract": "We propose a novel approach to self-supervised learning of point cloud representations by differentiable neural rendering. Motivated by the fact that informative point cloud features should be able to encode rich geometry and appearance cues and render realistic images, we train a point-cloud encoder within a devised point-based neural renderer by comparing the rendered images with real images on massive RGB-D data. The learned point-cloud encoder can be easily integrated into various downstream tasks, including not only high-level tasks like 3D detection and segmentation, but low-level tasks like 3D reconstruction and image synthesis. Extensive experiments on various tasks demonstrate the superiority of our approach compared to existing pre-training methods",
    "checked": true,
    "id": "c10e201728cbc639384abf598beeed5b01378740",
    "semantic_title": "ponder: point cloud pre-training via neural rendering",
    "citation_count": 7,
    "authors": [
      "Di Huang",
      "Sida Peng",
      "Tong He",
      "Honghui Yang",
      "Xiaowei Zhou",
      "Wanli Ouyang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Luo_Perpetual_Humanoid_Control_for_Real-time_Simulated_Avatars_ICCV_2023_paper.html": {
    "title": "Perpetual Humanoid Control for Real-time Simulated Avatars",
    "volume": "main",
    "abstract": "We present a physics-based humanoid controller that achieves high-fidelity motion imitation and fault-tolerant behavior in the presence of noisy input (e.g. pose estimates from video or generated from language) and unexpected falls. Our controller scales up to learning ten thousand motion clips without using any external stabilizing forces and learns to naturally recover from fail-state. Given reference motion, our controller can perpetually control simulated avatars without requiring resets. At its core, we propose the progressive multiplicative control policy (PMCP), which dynamically allocates new network capacity to learn harder and harder motion sequences. PMCP allows efficient scaling for learning from large-scale motion databases and adding new tasks, such as fail-state recovery, without catastrophic forgetting. We demonstrate the effectiveness of our controller by using it to imitate noisy poses from video-based pose estimators and language-based motion generators in a live and real-time multi-person avatar use case",
    "checked": true,
    "id": "1fbdd5a3ba7d17ab9c24c8c0e6ea93d7ccb90c16",
    "semantic_title": "perpetual humanoid control for real-time simulated avatars",
    "citation_count": 2,
    "authors": [
      "Zhengyi Luo",
      "Jinkun Cao",
      "AlexanderWinkler",
      "Kris Kitani",
      "Weipeng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xie_HollowNeRF_Pruning_Hashgrid-Based_NeRFs_with_Trainable_Collision_Mitigation_ICCV_2023_paper.html": {
    "title": "HollowNeRF: Pruning Hashgrid-Based NeRFs with Trainable Collision Mitigation",
    "volume": "main",
    "abstract": "Neural radiance fields (NeRF) have garnered significant attention, with recent works such as Instant-NGP accelerating NeRF training and evaluation through a combination of hashgrid-based positional encoding and neural networks. However, effectively leveraging the spatial sparsity of 3D scenes remains a challenge. To cull away unnecessary regions of the feature grid, existing solutions rely on prior knowledge of object shape or periodically estimate object shape during training by repeated model evaluations, which are costly and wasteful. To address this issue, we propose HollowNeRF, a novel compression solution for hashgrid-based NeRF which automatically sparsifies the feature grid during the training phase. Instead of directly compressing dense features, HollowNeRF trains a coarse 3D saliency mask that guides efficient feature pruning, and employs an alternating direction method of multipliers (ADMM) pruner to sparsify the 3D saliency mask during training. By exploiting the sparsity in the 3D scene to redistribute hash collisions, HollowNeRF improves rendering quality while using a fraction of the parameters of comparable state-of-the-art solutions, leading to a better cost-accuracy trade-off. Our method delivers comparable rendering quality to Instant-NGP, while utilizing just 31% of the parameters. In addition, our solution can achieve a PSNR accuracy gain of up to 1dB using only 56% of the parameters",
    "checked": true,
    "id": "40d0f7ba7db5e6ebe5d7b6237026bdae43384b99",
    "semantic_title": "hollownerf: pruning hashgrid-based nerfs with trainable collision mitigation",
    "citation_count": 0,
    "authors": [
      "Xiufeng Xie",
      "Riccardo Gherardi",
      "Zhihong Pan",
      "Stephen Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pandey_A_Complete_Recipe_for_Diffusion_Generative_Models_ICCV_2023_paper.html": {
    "title": "A Complete Recipe for Diffusion Generative Models",
    "volume": "main",
    "abstract": "Score-based Generative Models (SGMs) have demonstrated exceptional synthesis outcomes across various tasks. However, the current design landscape of the forward diffusion process remains largely untapped and often relies on physical heuristics or simplifying assumptions. Utilizing insights from the development of scalable Bayesian posterior samplers, we present a complete recipe for formulating forward processes in SGMs, ensuring convergence to the desired target distribution. Our approach reveals that several existing SGMs can be seen as specific manifestations of our framework. Building upon this method, we introduce Phase Space Langevin Diffusion (PSLD), which relies on score-based modeling within an augmented space enriched by auxiliary variables akin to physical phase space. Empirical results exhibit the superior sample quality and improved speed-quality trade-off of PSLD compared to various competing approaches on established image synthesis benchmarks. Remarkably, PSLD achieves sample quality akin to state-of-the-art SGMs (FID: 2.10 for unconditional CIFAR-10 generation). Lastly, we demonstrate the applicability of PSLD in conditional synthesis using pre-trained score networks, offering an appealing alternative as an SGM backbone for future advancements. Code and model checkpoints can be accessed at https://github.com/mandt-lab/PSLD",
    "checked": true,
    "id": "5ed3fe8d608e3b9b6a96b0c73bf084c0e8e700aa",
    "semantic_title": "a complete recipe for diffusion generative models",
    "citation_count": 1,
    "authors": [
      "Kushagra Pandey",
      "Stephan Mandt"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_The_Devil_is_in_the_Crack_Orientation_A_New_Perspective_ICCV_2023_paper.html": {
    "title": "The Devil is in the Crack Orientation: A New Perspective for Crack Detection",
    "volume": "main",
    "abstract": "Cracks are usually curve-like structures that are the focus of many computer-vision applications (e.g., road safety inspection and surface inspection of industrial facilities). The existing pixel-based crack segmentation methods rely on time-consuming and costly pixel-level annotations. And the object-based crack detection methods exploit the horizontal box to detect the crack without considering crack orientation, resulting in scale variation and intra-class variation. Considering this, we provide a new perspective for crack detection that models the cracks as a series of sub-cracks with the corresponding orientation. However, the vanilla adaptation of the existing oriented object detection methods to the crack detection tasks will result in limited performance, due to the boundary discontinuity issue and the ambiguities in sub-crack orientation. In this paper, we propose a first-of-its-kind oriented sub-crack detector, dubbed as CrackDet, which is derived from a novel piecewise angle definition, to ease the boundary discontinuity problem. And then, we propose a multi-branch angle regression loss for learning sub-crack orientation and variance together. Since there are no related benchmarks, we construct three fully annotated datasets, namely, ORC, ONPP, and OCCSD, which involve various cracks in road pavement and industrial facilities. Experiments show that our approach outperforms state-of-the-art crack detectors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuangzhuang Chen",
      "Jin Zhang",
      "Zhuonan Lai",
      "Guanming Zhu",
      "Zun Liu",
      "Jie Chen",
      "Jianqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_FedPD_Federated_Open_Set_Recognition_with_Parameter_Disentanglement_ICCV_2023_paper.html": {
    "title": "FedPD: Federated Open Set Recognition with Parameter Disentanglement",
    "volume": "main",
    "abstract": "Existing federated learning (FL) approaches are deployed under the unrealistic closed-set setting, with both training and testing classes belong to the same set, which makes the global model fail to identify the unseen classes as `unknown'. To this end, we aim to study a novel problem of federated open-set recognition (FedOSR), which learns an open-set recognition (OSR) model under federated paradigm such that it classifies seen classes while at the same time detects unknown classes. In this work, we propose a parameter disentanglement guided federated open-set recognition (FedPD) algorithm to address two core challenges of FedOSR: cross-client inter-set interference between learning closed-set and open-set knowledge and cross-client intra-set inconsistency by data heterogeneity. The proposed FedPD framework mainly leverages two modules, i.e., local parameter disentanglement (LPD) and global divide-and-conquer aggregation (GDCA), to first disentangle client OSR model into different subnetworks, then align the corresponding parts cross clients for matched model aggregation. Specifically, on the client side, LPD decouples an OSR model into a closed-set subnetwork and an open-set subnetwork by the task-related importance, thus preventing inter-set interference. On the server side, GDCA first partitions the two subnetworks into specific and shared parts, and subsequently aligns the corresponding parts through optimal transport to eliminate parameter misalignment. Extensive experiments on various datasets demonstrate the superior performance of our proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Yang",
      "Meilu Zhu",
      "Yifan Liu",
      "Yixuan Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lian_WaterMask_Instance_Segmentation_for_Underwater_Imagery_ICCV_2023_paper.html": {
    "title": "WaterMask: Instance Segmentation for Underwater Imagery",
    "volume": "main",
    "abstract": "Underwater image instance segmentation is a fundamental and critical step in underwater image analysis and understanding. However, the paucity of general multiclass instance segmentation datasets has impeded the development of instance segmentation studies for underwater images. In this paper, we propose the first underwater image instance segmentation dataset (UIIS), which provides 4628 images for 7 categories with pixel-level annotations. Meanwhile, we also design WaterMask for underwater image instance segmentation for the first time. In Water- Mask, we first devise Difference Similarity Graph Attention Module (DSGAT) to recover lost detailed information due to image quality degradation and downsampling to help the network prediction. Then, we propose Multi-level Feature Refinement Module (MFRM) to predict foreground masks and boundary masks separately by features at different scales, and guide the network through Boundary Mask Strategy (BMS) with boundary learning loss to provide finer prediction results. Extensive experimental results demonstrates that WaterMask can achieve significant gains of 2.9, 3.8 mAP over Mask R-CNN when using ResNet-50 and ResNet-101. Code and Dataset are available at https: //github.com/LiamLian0727/WaterMask",
    "checked": false,
    "id": "d90d89695da2a4648769f0e5090ce78edbfd5d56",
    "semantic_title": "synthetic data for semantic segmentation in underwater imagery",
    "citation_count": 1,
    "authors": [
      "Shijie Lian",
      "Hua Li",
      "Runmin Cong",
      "Suqi Li",
      "Wei Zhang",
      "Sam Kwong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_Score_Priors_Guided_Deep_Variational_Inference_for_Unsupervised_Real-World_Single_ICCV_2023_paper.html": {
    "title": "Score Priors Guided Deep Variational Inference for Unsupervised Real-World Single Image Denoising",
    "volume": "main",
    "abstract": "Real-world single image denoising is crucial and practical in computer vision. Bayesian inversions combined with score priors now have proven effective for single image denoising but are limited to white Gaussian noise. Moreover, applying existing score-based methods for real-world denoising requires not only the explicit train of score priors on the target domain but also the careful design of sampling procedures for posterior inference, which is complicated and impractical. To address these limitations, we propose a score priors-guided deep variational inference, namely ScoreDVI, for practical real-world denoising. By considering the deep variational image posterior with a Gaussian form, score priors are extracted based on easily accessible minimum MSE Non-i.i.d Gaussian denoisers and variational samples, which in turn facilitate optimizing the variational image posterior. Such a procedure adaptively applies cheap score priors to denoising. Additionally, we exploit a Non-i.i.d Gaussian mixture model and variational noise posterior to model the real-world noise. This scheme also enables the pixel-wise fusion of multiple image priors and variational image posteriors. Besides, we develop a noise-aware prior assignment strategy that dynamically adjusts the weight of image priors in the optimization. Our method outperforms other single image-based real-world denoising methods and achieves comparable performance to dataset-based unsupervised methods",
    "checked": true,
    "id": "d9d900b4a1c70829bbac1230374aca0bdd3c29fd",
    "semantic_title": "score priors guided deep variational inference for unsupervised real-world single image denoising",
    "citation_count": 0,
    "authors": [
      "Jun Cheng",
      "Tao Liu",
      "Shan Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Rehman_L-DAWA_Layer-wise_Divergence_Aware_Weight_Aggregation_in_Federated_Self-Supervised_Visual_ICCV_2023_paper.html": {
    "title": "L-DAWA: Layer-wise Divergence Aware Weight Aggregation in Federated Self-Supervised Visual Representation Learning",
    "volume": "main",
    "abstract": "The ubiquity of camera-enabled devices has led to large amounts of unlabeled image data being produced at the edge. The integration of self-supervised learning (SSL) and federated learning (FL) into one coherent system can potentially offer data privacy guarantees while also advancing the quality and robustness of the learned visual representations without needing to move data around. However, client bias and divergence during FL aggregation caused by data heterogeneity limits the performance of learned visual representations on downstream tasks. In this paper, we propose a new aggregation strategy termed Layer-wise Divergence Aware Weight Aggregation (L-DAWA) to mitigate the influence of client bias and divergence during FL aggregation. The proposed method aggregates weights at the layer-level according to the measure of angular divergence between the clients' model and the global model. Extensive experiments with cross-silo and cross-device settings on CIFAR-10/100 and Tiny ImageNet datasets demonstrate that our methods are effective and obtain new SOTA performance on both contrastive and non-contrastive SSL approaches",
    "checked": true,
    "id": "0b38470d17625bb5342d6c4d3fb43d914ed33a2d",
    "semantic_title": "l-dawa: layer-wise divergence aware weight aggregation in federated self-supervised visual representation learning",
    "citation_count": 0,
    "authors": [
      "Yasar Abbas Ur Rehman",
      "Yan Gao",
      "Pedro Porto Buarque de Gusmao",
      "Mina Alibeigi",
      "Jiajun Shen",
      "Nicholas D. Lane"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cao_Improving_Transformer-based_Image_Matching_by_Cascaded_Capturing_Spatially_Informative_Keypoints_ICCV_2023_paper.html": {
    "title": "Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints",
    "volume": "main",
    "abstract": "Learning robust local image feature matching is a fundamental low-level vision task, which has been widely explored in the past few years. Recently, detector-free local feature matchers based on transformers have shown promising results, which largely outperform pure Convolutional Neural Network (CNN) based ones. But correlations produced by transformer-based methods are spatially limited to the center of source views' coarse patches, because of the costly attention learning. In this work, we rethink this issue and find that such matching formulation degrades pose estimation, especially for low-resolution images. So we propose a transformer-based cascade matching model -- Cascade feature Matching TRansformer (CasMTR), to efficiently learn dense feature correlations, which allows us to choose more reliable matching pairs for the relative pose estimation. Instead of re-training a new detector, we use a simple yet effective Non-Maximum Suppression (NMS) post-process to filter keypoints through the confidence map, and largely improve the matching precision. CasMTR achieves state-of-the-art performance in indoor and outdoor pose estimation as well as visual localization. Moreover, thorough ablations show the efficacy of the proposed components and techniques",
    "checked": true,
    "id": "8f3ac6bd750e2aac5cd2e0f2bdb41f17f1313aa1",
    "semantic_title": "improving transformer-based image matching by cascaded capturing spatially informative keypoints",
    "citation_count": 0,
    "authors": [
      "Chenjie Cao",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Controllable_Guide-Space_for_Generalizable_Face_Forgery_Detection_ICCV_2023_paper.html": {
    "title": "Controllable Guide-Space for Generalizable Face Forgery Detection",
    "volume": "main",
    "abstract": "Recent studies on face forgery detection have shown satisfactory performance for methods involved in training datasets, but are not ideal enough for unknown domains. This motivates many works to improve the generalization, but forgery-irrelevant information, such as image background and identity, still exists in different domain features and causes unexpected clustering, limiting the generalization. In this paper, we propose a controllable guide-space (GS) method to enhance the discrimination of different forgery domains, so as to increase the forgery relevance of features and thereby improve the generalization. The well-designed guide-space can simultaneously achieve both the proper separation of forgery domains and the large distance between real-forgery domains in an explicit and controllable manner. Moreover, for better discrimination, we use a decoupling module to weaken the interference of forgery-irrelevant correlations between domains. Furthermore, we make adjustments to the decision boundary manifold according to the clustering degree of the same domain features within the neighborhood. Extensive experiments in multiple in-domain and cross-domain settings confirm that our method can achieve state-of-the-art generalization",
    "checked": true,
    "id": "cb3c861081cd3b112f7ff7670ac4735965e3ac24",
    "semantic_title": "controllable guide-space for generalizable face forgery detection",
    "citation_count": 1,
    "authors": [
      "Ying Guo",
      "Cheng Zhen",
      "Pengfei Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/LI_Calibrating_Uncertainty_for_Semi-Supervised_Crowd_Counting_ICCV_2023_paper.html": {
    "title": "Calibrating Uncertainty for Semi-Supervised Crowd Counting",
    "volume": "main",
    "abstract": "Semi-supervised crowd counting is an important yet challenging task. A popular approach is to iteratively generate pseudo-labels for unlabeled data and add them to the training set. The key is to use uncertainty to select reliable pseudo-labels. In this paper, we propose a novel method to calibrate model uncertainty for crowd counting. Our method takes a supervised uncertainty estimation strategy to train the model through a surrogate function. This ensures the uncertainty is well controlled throughout the training. We propose a matching-based patch-wise surrogate function to better approximate uncertainty for crowd counting tasks. The proposed method pays a sufficient amount of attention to details, while maintaining a proper granularity. Altogether our method is able to generate reliable uncertainty estimation, high quality pseudolabels, and achieve state-of-the-art performance in semisupervised crowd counting",
    "checked": true,
    "id": "67c695b05398ed80fd16e7aa00659910677d08d8",
    "semantic_title": "calibrating uncertainty for semi-supervised crowd counting",
    "citation_count": 3,
    "authors": [
      "Chen LI",
      "Xiaoling Hu",
      "Shahira Abousamra",
      "Chao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Silver_MosaiQ_Quantum_Generative_Adversarial_Networks_for_Image_Generation_on_NISQ_ICCV_2023_paper.html": {
    "title": "MosaiQ: Quantum Generative Adversarial Networks for Image Generation on NISQ Computers",
    "volume": "main",
    "abstract": "Quantum machine learning and vision have come to the fore recently, with hardware advances enabling rapid advancement in the capabilities of quantum machines. Recently, quantum image generation has been explored with many potential advantages over non-quantum techniques; however, previous techniques have suffered from poor quality and robustness. To address these problems, we introduce MosaiQ a high-quality quantum image generation GAN framework that can be executed on today's Near-term Intermediate Scale Quantum (NISQ) computers",
    "checked": true,
    "id": "e2364ba7d9ae1c1a5f0eb92a2fa84193f2a703b5",
    "semantic_title": "mosaiq: quantum generative adversarial networks for image generation on nisq computers",
    "citation_count": 0,
    "authors": [
      "Daniel Silver",
      "Tirthak Patel",
      "William Cutler",
      "Aditya Ranjan",
      "Harshitta Gandhi",
      "Devesh Tiwari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_DVIS_Decoupled_Video_Instance_Segmentation_Framework_ICCV_2023_paper.html": {
    "title": "DVIS: Decoupled Video Instance Segmentation Framework",
    "volume": "main",
    "abstract": "Video instance segmentation (VIS) is a critical task with diverse applications, including autonomous driving and video editing. Existing methods often underperform on complex and long videos in real world, primarily due to two factors. Firstly, offline methods are limited by the tightly-coupled modeling paradigm, which treats all frames equally and disregards the interdependencies between adjacent frames. Consequently, this leads to the introduction of excessive noise during long-term temporal alignment. Secondly, online methods suffer from inadequate utilization of temporal information. To tackle these challenges, we propose a decoupling strategy for VIS by dividing it into three independent sub-tasks: segmentation, tracking, and refinement. The efficacy of the decoupling strategy relies on two crucial elements: 1) attaining precise long-term alignment outcomes via frame-by-frame association during tracking, and 2) the effective utilization of temporal information predicated on the aforementioned accurate alignment outcomes during refinement. We introduce a novel referring tracker and temporal refiner to construct the Decoupled VIS framework (DVIS). DVIS achieves new SOTA performance in both VIS and VPS, surpassing the current SOTA methods by 7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the most challenging and realistic benchmarks. Moreover, thanks to the decoupling strategy, the referring tracker and temporal refiner are super light-weight (only 6% of the segmenter FLOPs), allowing for efficient training and inference on a single GPU with 11G memory. To promote reproducibility and facilitate further research, we will make the code publicly available",
    "checked": true,
    "id": "4008cdf1fce3310c4a6b338c1332e38e3753489f",
    "semantic_title": "dvis: decoupled video instance segmentation framework",
    "citation_count": 4,
    "authors": [
      "Tao Zhang",
      "Xingye Tian",
      "Yu Wu",
      "Shunping Ji",
      "Xuebo Wang",
      "Yuan Zhang",
      "Pengfei Wan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liao_Segmentation_of_Tubular_Structures_Using_Iterative_Training_with_Tailored_Samples_ICCV_2023_paper.html": {
    "title": "Segmentation of Tubular Structures Using Iterative Training with Tailored Samples",
    "volume": "main",
    "abstract": "We propose a minimal path method to simultaneously compute segmentation masks and extract centerlines of tubular structures with line-topology. Minimal path methods are commonly used for the segmentation of tubular structures in a wide variety of applications. Recent methods use features extracted by CNNs, and often outperform methods using hand-tuned features. However, for CNN-based methods, the samples used for training may be generated inappropriately, so that they can be very different from samples encountered during inference. We approach this discrepancy by introducing a novel iterative training scheme, which enables generating better training samples specifically tailored for the minimal path methods without changing existing annotations. In our method, segmentation masks and centerlines are not determined after one another by post-processing, but obtained using the same steps. Our method requires only very few annotated training images. Comparison with seven previous approaches on three public datasets, including satellite images and medical images, shows that our method achieves state-of-the-art results both for segmentation masks and centerlines",
    "checked": true,
    "id": "558a727349a890bbc47a2f186dd4dad68c514d59",
    "semantic_title": "segmentation of tubular structures using iterative training with tailored samples",
    "citation_count": 0,
    "authors": [
      "Wei Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Boundary-Aware_Divide_and_Conquer_A_Diffusion-Based_Solution_for_Unsupervised_Shadow_ICCV_2023_paper.html": {
    "title": "Boundary-Aware Divide and Conquer: A Diffusion-Based Solution for Unsupervised Shadow Removal",
    "volume": "main",
    "abstract": "Recent deep learning methods have achieved superior results in shadow removal. However, most of these supervised methods rely on training over a huge amount of shadow and shadow-free image pairs, which require laborious annotations and may end up with poor model generalization. Shadows, in fact, only form partial degradation in images, while their non-shadow regions provide rich structural information potentially for unsupervised learning. In this paper, we propose a novel diffusion-based solution for unsupervised shadow removal, which separately models the shadow, non-shadow, and their boundary regions. We employ a pretrained unconditional diffusion model fused with non-corrupted information to generate the natural shadow-free image. While the diffusion model can restore the clear structure in the boundary region by utilizing its adjacent non-corrupted contextual information, it fails to address the inner shadow area due to the isolation of the non-corrupted contexts. Thus we further propose a Shadow-Invariant Intrinsic Decomposition module to exploit the underlying reflectance in the shadow region to maintain structural consistency during the diffusive sampling. Extensive experiments on the publicly available shadow removal datasets show that the proposed method achieves a significant improvement compared to existing unsupervised methods, and even is comparable with some existing supervised methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lanqing Guo",
      "Chong Wang",
      "Wenhan Yang",
      "Yufei Wang",
      "Bihan Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qu_Towards_Nonlinear-Motion-Aware_and_Occlusion-Robust_Rolling_Shutter_Correction_ICCV_2023_paper.html": {
    "title": "Towards Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction",
    "volume": "main",
    "abstract": "This paper addresses the problem of rolling shutter correction in complex nonlinear and dynamic scenes with extreme occlusion. Existing methods suffer from two main drawbacks. Firstly, they face challenges in estimating the accurate correction field due to the uniform velocity assumption, leading to significant image correction errors under complex motion. Secondly, the drastic occlusion in dynamic scenes prevents current solutions from achieving better image quality because of the inherent difficulties in aligning and aggregating multiple frames. To tackle these challenges, we model the curvilinear trajectory of pixels analytically and propose a geometry-based Quadratic Rolling Shutter (QRS) motion solver, which precisely estimates the high-order correction field of individual pixels. Besides, to reconstruct high-quality occlusion frames in dynamic scenes, we present a 3D video architecture that effectively Aligns and Aggregates multi-frame context, namely, RSA2-Net. We evaluate our method across a broad range of cameras and video sequences, demonstrating its significant superiority. Specifically, our method surpasses the state-of-the-art by +4.98, +0.77, and +4.33 of PSNR on Carla-RS, Fastec-RS, and BS-RSC datasets, respectively. Code is available at https://github.com/DelinQu/qrsc",
    "checked": true,
    "id": "fb57150fdfb45811c8fbfbc8e84244b894fef3de",
    "semantic_title": "towards nonlinear-motion-aware and occlusion-robust rolling shutter correction",
    "citation_count": 0,
    "authors": [
      "Delin Qu",
      "Yizhen Lao",
      "Zhigang Wang",
      "Dong Wang",
      "Bin Zhao",
      "Xuelong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Surface_Extraction_from_Neural_Unsigned_Distance_Fields_ICCV_2023_paper.html": {
    "title": "Surface Extraction from Neural Unsigned Distance Fields",
    "volume": "main",
    "abstract": "We propose a method, named DualMesh-UDF, to extract a surface from unsigned distance functions (UDFs), encoded by neural networks, or neural UDFs. Neural UDFs are becoming increasingly popular for surface representation because of their versatility in presenting surfaces with arbitrary topologies, as opposed to the signed distance function that is limited to representing a closed surface. However, the applications of neural UDFs are hindered by the notorious difficulty in extracting the target surfaces they represent. Recent methods for surface extraction from a neural UDF suffer from significant geometric errors or topological artifacts due to two main difficulties: (1) A UDF does not exhibit sign changes; and (2) A neural UDF typically has substantial approximation errors. DualMesh-UDF addresses these two difficulties. Specifically, given a neural UDF encoding a target surface S to be recovered, we first estimate the tangent planes of S at a set of sample points close to S. Next, we organize these sample points into local clusters, and for each local cluster, solve a linear least squares problem to determine a final surface point. These surface points are then connected to create the output mesh surface, which approximates the target surface. The robust estimation of the tangent planes of the target surface and the subsequent minimization problem constitute our core strategy, which contributes to the favorable performance of DualMesh-UDF over other competing methods. To efficiently implement this strategy, we employ an adaptive Octree. Within this framework, we estimate the location of a surface point in each of the octree cells identified as containing part of the target surface. Extensive experiments show that our method outperforms existing methods in terms of surface reconstruction quality while maintaining comparable computational efficiency",
    "checked": true,
    "id": "1f2e372e9dda127e00e68be8853c07ca5ca7451c",
    "semantic_title": "surface extraction from neural unsigned distance fields",
    "citation_count": 1,
    "authors": [
      "Congyi Zhang",
      "Guying Lin",
      "Lei Yang",
      "Xin Li",
      "Taku Komura",
      "Scott Schaefer",
      "John Keyser",
      "Wenping Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_CBA_Improving_Online_Continual_Learning_via_Continual_Bias_Adaptor_ICCV_2023_paper.html": {
    "title": "CBA: Improving Online Continual Learning via Continual Bias Adaptor",
    "volume": "main",
    "abstract": "Online continual learning (CL) aims to learn new knowledge and consolidate previously learned knowledge from non-stationary data streams. Due to the time-varying training setting, the model learned from a changing distribution easily forgets the previously learned knowledge and biases towards the newly received task. To address this problem, we propose a Continual Bias Adaptor (CBA) module to augment the classifier network to adapt to catastrophic distribution change during training, such that the classifier network is able to learn a stable consolidation of previously learned tasks. In the testing stage, CBA can be removed which introduces no additional computation cost and memory overhead. We theoretically reveal the reason why the proposed method can effectively alleviate catastrophic distribution shifts, and empirically demonstrate its effectiveness through extensive experiments based on four rehearsal-based baselines and three public continual learning benchmarks",
    "checked": true,
    "id": "bfff05a19097b4c4a8654ea43a3d9283f86bb385",
    "semantic_title": "cba: improving online continual learning via continual bias adaptor",
    "citation_count": 0,
    "authors": [
      "Quanziang Wang",
      "Renzhen Wang",
      "Yichen Wu",
      "Xixi Jia",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_GraphEcho_Graph-Driven_Unsupervised_Domain_Adaptation_for_Echocardiogram_Video_Segmentation_ICCV_2023_paper.html": {
    "title": "GraphEcho: Graph-Driven Unsupervised Domain Adaptation for Echocardiogram Video Segmentation",
    "volume": "main",
    "abstract": "Echocardiogram video segmentation plays an important role in cardiac disease diagnosis. This paper studies the unsupervised domain adaption (UDA) for echocardiogram video segmentation, where the goal is to generalize the model trained on the source domain to other unlabeled target domains. Existing UDA segmentation methods are not suitable for this task because they do not model local information and the cyclical consistency of heartbeat. In this paper, we introduce a newly collected CardiacUDA dataset and a novel GraphEcho method for cardiac structure segmentation. Our GraphEcho comprises two innovative modules, the Spatial-wise Cross-domain Graph Matching (SCGM) and the Temporal Cycle Consistency (TCC) module, which utilize prior knowledge of echocardiogram videos, i.e., consistent cardiac structure across patients and centers and the heartbeat cyclical consistency, respectively. These two modules can better align global and local features from source and target domains, leading to improved UDA segmentation results. Experimental results showed that our GraphEcho outperforms existing state-of-the-art UDA segmentation methods. Our collected dataset and code will be publicly released upon acceptance. This work will lay a new and solid cornerstone for cardiac structure segmentation from echocardiogram videos",
    "checked": true,
    "id": "f91ea80b179d4f0a5d56000691d824f1a08dbd9a",
    "semantic_title": "graphecho: graph-driven unsupervised domain adaptation for echocardiogram video segmentation",
    "citation_count": 0,
    "authors": [
      "Jiewen Yang",
      "Xinpeng Ding",
      "Ziyang Zheng",
      "Xiaowei Xu",
      "Xiaomeng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qiao_Multi-view_Spectral_Polarization_Propagation_for_Video_Glass_Segmentation_ICCV_2023_paper.html": {
    "title": "Multi-view Spectral Polarization Propagation for Video Glass Segmentation",
    "volume": "main",
    "abstract": "In this paper, we present the first polarization-guided video glass segmentation propagation solution (PGVS-Net) that can robustly and coherently propagate glass segmentation in RGB-P video sequences. By leveraging spatiotemporal polarization and color information, our method combines multi-view polarization cues and thus can alleviate the view dependence of single-input intensity variations on glass objects. We demonstrate that our model can outperform glass segmentation on RGB-only video sequences as well as produce more robust segmentation than per-frame RGB-P single-image segmentation methods. To train and validate PGVS-Net, we introduce a novel RGB-P Glass Video dataset (PGV-117) containing 117 video sequences of scenes captured with different types of camera paths, lighting conditions, dynamics, and glass types",
    "checked": true,
    "id": "ed5951a81d2538acb4ec27bbff1cf399fe684e3d",
    "semantic_title": "multi-view spectral polarization propagation for video glass segmentation",
    "citation_count": 0,
    "authors": [
      "Yu Qiao",
      "Bo Dong",
      "Ao Jin",
      "Yu Fu",
      "Seung-Hwan Baek",
      "Felix Heide",
      "Pieter Peers",
      "Xiaopeng Wei",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fan_Rethinking_Amodal_Video_Segmentation_from_Learning_Supervised_Signals_with_Object-centric_ICCV_2023_paper.html": {
    "title": "Rethinking Amodal Video Segmentation from Learning Supervised Signals with Object-centric Representation",
    "volume": "main",
    "abstract": "Video amodal segmentation is a particularly challenging task in computer vision, which requires to deduce the full shape of an object from the visible parts of it. Recently, some studies have achieved promising performance by using motion flow to integrate information across frames under a self-supervised setting. However, motion flow has a clear limitation by the two factors of moving cameras and object deformation. This paper presents a rethinking to previous works. We particularly leverage the supervised signals with object-centric representation in real-world scenarios. The underlying idea is the supervision signal of the specific object and the features from different views can mutually benefit the deduction of the full mask in any specific frame. We thus propose an Efficient object-centric Representation amodal Segmentation (EoRaS). Specially, beyond solely relying on supervision signals, we design a translation module to project image features into the Bird's-Eye View (BEV), which introduces 3D information to improve current feature quality. Furthermore, we propose a multi-view fusion layer based temporal module which is equipped with a set of object slots and interacts with features from different views by attention mechanism to fulfill sufficient object representation completion. As a result, the full mask of the object can be decoded from image features updated by object slots. Extensive experiments on both real-world and synthetic benchmarks demonstrate the superiority of our proposed method, achieving state-of-the-art performance. Our code will be released at https://github.com/kfan21/EoRaS",
    "checked": true,
    "id": "b721cd7246f28894f65fdac1a51a4731aa6e858c",
    "semantic_title": "rethinking amodal video segmentation from learning supervised signals with object-centric representation",
    "citation_count": 0,
    "authors": [
      "Ke Fan",
      "Jingshi Lei",
      "Xuelin Qian",
      "Miaopeng Yu",
      "Tianjun Xiao",
      "Tong He",
      "Zheng Zhang",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Augmented_Box_Replay_Overcoming_Foreground_Shift_for_Incremental_Object_Detection_ICCV_2023_paper.html": {
    "title": "Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection",
    "volume": "main",
    "abstract": "In incremental learning, replaying stored samples from previous tasks together with current task samples is one of the most efficient approaches to address catastrophic forgetting. However, unlike incremental classification, image replay has not been successfully applied to incremental object detection (IOD). In this paper, we identify the overlooked problem of foreground shift as the main reason for this. Foreground shift only occurs when replaying images of previous tasks and refers to the fact that their background might contain foreground objects of the current task. To overcome this problem, a novel and efficient Augmented Box Replay (ABR) method is developed that only stores and replays foreground objects and thereby circumvents the foreground shift problem. In addition, we propose an innovative Attentive RoI Distillation loss that uses spatial attention from region-of-interest (RoI) features to constrain current model to focus on the most important information from old model. ABR significantly reduces forgetting of previous classes while maintaining high plasticity in current classes. Moreover, it considerably reduces the storage requirements when compared to standard image replay. Comprehensive experiments on Pascal-VOC and COCO datasets support the state-of-the-art performance of our model",
    "checked": true,
    "id": "4f78d2ef7f18907c04c7ab77b88172784e342f40",
    "semantic_title": "augmented box replay: overcoming foreground shift for incremental object detection",
    "citation_count": 1,
    "authors": [
      "Yuyang Liu",
      "Yang Cong",
      "Dipam Goswami",
      "Xialei Liu",
      "Joost van de Weijer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Distilled_Reverse_Attention_Network_for_Open-world_Compositional_Zero-Shot_Learning_ICCV_2023_paper.html": {
    "title": "Distilled Reverse Attention Network for Open-world Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "Open-World Compositional Zero-Shot Learning (OW-CZSL) aims to recognize new compositions of seen attributes and objects. In OW-CZSL, methods built on the conventional closed-world setting degrade severely due to the unconstrained OW test space. While previous works alleviate the issue by pruning compositions according to external knowledge or correlations in seen pairs, they introduce biases that harm the generalization. Some methods thus predict state and object with independently constructed and trained classifiers, ignoring that attributes are highly context-dependent and visually entangled with objects. In this paper, we propose a novel Distilled Reverse Attention Network to address the challenges. We also model attributes and objects separately but with different motivations, capturing contextuality and locality, respectively. We further design a reverse-and-distill strategy that learns disentangled representations of elementary components in training data supervised by reverse attention and knowledge distillation. We conduct experiments on three datasets and consistently achieve state-of-the-art (SOTA) performance",
    "checked": true,
    "id": "302fbc5f6dc359921eb3d2a52e85eec0a148e3b3",
    "semantic_title": "distilled reverse attention network for open-world compositional zero-shot learning",
    "citation_count": 0,
    "authors": [
      "Yun Li",
      "Zhe Liu",
      "Saurav Jha",
      "Lina Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_DandelionNet_Domain_Composition_with_Instance_Adaptive_Classification_for_Domain_Generalization_ICCV_2023_paper.html": {
    "title": "DandelionNet: Domain Composition with Instance Adaptive Classification for Domain Generalization",
    "volume": "main",
    "abstract": "Domain generalization (DG) attempts to learn a model on source domains that can well generalize to unseen but different domains. The multiple source domains are innately different in distribution but intrinsically related to each other, e.g., from the same label space. To achieve a generalizable feature, most existing methods attempt to reduce the domain discrepancy by either learning domain-invariant feature, or additionally mining domain-specific feature. In the space of these features, the multiple source domains are either tightly aligned or not aligned at all, which both cannot fully take the advantage of complementary information from multiple domains. In order to preserve more complementary information from multiple domains at the meantime of reducing their domain gap, we propose that the multiple domains should not be tightly aligned but composite together, where all domains are pulled closer but still preserve their individuality respectively. This is achieved by using instance-adaptive classifier specified for each instance's classification, where the instance-adaptive classifier is slightly deviated from a universal classifier shared by samples from all domains. This adaptive classifier deviation allows all instances from the same category but different domains to be dispersed around the class center rather than squeezed tightly, leading to better generalization for unseen domain samples. In result, the multiple domains are harmoniously composite centered on a universal core, like a dandelion, so this work is referred to as DandelionNet. Experiments on multiple DG benchmarks demonstrate that the proposed method can learn a model with better generalization and experiments on source free domain adaption also indicate the versatility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lanqing Hu",
      "Meina Kan",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cao_TexFusion_Synthesizing_3D_Textures_with_Text-Guided_Image_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion Models",
    "volume": "main",
    "abstract": "We present TexFusion(Texture Diffusion), a new method to synthesize textures for given 3D geometries, using only large-scale text-guided image diffusion models. In contrast to recent works that leverage 2D text-to-image diffusion models to distill 3D objects using a slow and fragile optimization process, TexFusion introduces a new 3D-consistent generation technique specifically designed for texture synthesis that employs regular diffusion model sampling on different 2D rendered views. Specifically, we leverage latent diffusion models, apply the diffusion model's denoiser on a set of 2D renders of the 3D object, and aggregate the different denoising predictions on a shared latent texture map. Final RGB output textures are produced by optimizing an intermediate neural color field on the decodings of 2D renders of the latent texture. We thoroughly validate TexFusion and show that we can efficiently generate diverse, high quality and globally coherent textures. We achieve state-of-the-art text-guided texture synthesis performance using only image diffusion models, while avoiding the pitfalls of previous distillation-based methods. The text-conditioning offers detailed control and we also do not rely on any ground truth 3D textures for training. This makes our method very versatile and applicable to a broad range of geometries and texture types. We hope that TexFusion will advance AI-based texturing of 3D assets for applications in virtual reality, game design, simulation, and more",
    "checked": true,
    "id": "c629d0ad20939623ef9cfc50a49ff634d42d1d43",
    "semantic_title": "texfusion: synthesizing 3d textures with text-guided image diffusion models",
    "citation_count": 0,
    "authors": [
      "Tianshi Cao",
      "Karsten Kreis",
      "Sanja Fidler",
      "Nicholas Sharp",
      "Kangxue Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_Shift_from_Texture-bias_to_Shape-bias_Edge_Deformation-based_Augmentation_for_Robust_ICCV_2023_paper.html": {
    "title": "Shift from Texture-bias to Shape-bias: Edge Deformation-based Augmentation for Robust Object Recognition",
    "volume": "main",
    "abstract": "Recent studies have shown the vulnerability of CNNs under perturbation noises, which is partially caused by the reason that the well-trained CNNs are too biased toward the object texture, i.e., they make predictions mainly based on texture cues. To reduce this texture-bias, current studies resort to learning augmented samples with heavily perturbed texture to make networks be more biased toward relatively stable shape cues. However, such methods usually fail to achieve real shape-biased networks due to the insufficient diversity of the shape cues. In this paper, we propose to augment the training dataset by generating semantically meaningful shapes and samples, via a shape deformation-based online augmentation, namely as SDbOA. The samples generated by our SDbOA have two main merits. First, the augmented samples with more diverse shape variations enable networks to learn the shape cues more elaborately, which encourages the network to be shape-biased. Second, semantic-meaningful shape-augmentation samples could be produced by jointly regularizing the generator with object texture and edge-guidance soft constraint, where the edges are represented more robustly with a self information guided map to better against the noises on them. Extensive experiments under various perturbation noises demonstrate the obvious superiority of our shape-bias-motivated model over the state of the arts in terms of robustness performance. Our code is appended in the supplementary material",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xilin He",
      "Qinliang Lin",
      "Cheng Luo",
      "Weicheng Xie",
      "Siyang Song",
      "Feng Liu",
      "Linlin Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jin_Lighting_Every_Darkness_in_Two_Pairs_A_Calibration-Free_Pipeline_for_ICCV_2023_paper.html": {
    "title": "Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising",
    "volume": "main",
    "abstract": "Calibration-based methods have dominated RAW image denoising under extremely low-light environments. However, these methods suffer from several main deficiencies: 1) the calibration procedure is laborious and time-consuming, 2) denoisers for different cameras are difficult to transfer, and 3) the discrepancy between synthetic noise and real noise is enlarged by high digital gain. To overcome the above shortcomings, we propose a calibration-free pipeline for Lighting Every Drakness (LED), regardless of the digital gain or camera sensor. Instead of calibrating the noise parameters and training repeatedly, our method could adapt to a target camera only with fewshot paired data and fine-tuning. In addition, well-designed structural modification during both stages alleviates the domain gap between synthetic noise and real noise without any extra computational cost. With 2 pairs for each additional digital gain (in total 6 pairs) and 0.5% iterations, our method achieves superior performance over other calibration-based methods",
    "checked": true,
    "id": "9760e379809ab555cde5a304c0437e7ff1b8a1a1",
    "semantic_title": "lighting every darkness in two pairs: a calibration-free pipeline for raw denoising",
    "citation_count": 2,
    "authors": [
      "Xin Jin",
      "Jia-Wen Xiao",
      "Ling-Hao Han",
      "Chunle Guo",
      "Ruixun Zhang",
      "Xialei Liu",
      "Chongyi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shao_Data-free_Knowledge_Distillation_for_Fine-grained_Visual_Categorization_ICCV_2023_paper.html": {
    "title": "Data-free Knowledge Distillation for Fine-grained Visual Categorization",
    "volume": "main",
    "abstract": "Data-free knowledge distillation (DFKD) is a promising approach for addressing issues related to model compression, security privacy, and transmission restrictions. Although the existing methods exploiting DFKD have achieved inspiring achievements in coarse-grained classification, in practical applications involving fine-grained classification tasks that require more detailed distinctions between similar categories, sub-optimal results are obtained. To address this issue, we propose an approach called DFKD-FGVC that extends DFKD to fine-grained vision categorization (FGVC) tasks. Our approach utilizes an adversarial distillation framework with attention generator, mixed high-order attention distillation, and semantic feature contrast learning. Specifically, we introduce a spatial-wise attention mechanism to the generator to synthesize fine-grained images with more details of discriminative parts. We also utilize the mixed high-order attention mechanism to capture complex interactions among parts and the subtle differences among discriminative features of the fine-grained categories, paying attention to both local features and semantic context relationships. Moreover, we leverage the teacher and student models of the distillation framework to contrast high-level semantic feature maps in the hyperspace, comparing variances of different categories. We evaluate our approach on three widely-used FGVC benchmarks (Aircraft, Cars196, and CUB200) and demonstrate its superior performance",
    "checked": true,
    "id": "3627b882d61affa667e125314adff5618a0fda24",
    "semantic_title": "data-free knowledge distillation for fine-grained visual categorization",
    "citation_count": 0,
    "authors": [
      "Renrong Shao",
      "Wei Zhang",
      "Jianhua Yin",
      "Jun Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_MotionBERT_A_Unified_Perspective_on_Learning_Human_Motion_Representations_ICCV_2023_paper.html": {
    "title": "MotionBERT: A Unified Perspective on Learning Human Motion Representations",
    "volume": "main",
    "abstract": "We present a unified perspective on tackling various human-centric video tasks by learning human motion representations from large-scale and heterogeneous data resources. Specifically, we propose a pretraining stage in which a motion encoder is trained to recover the underlying 3D motion from noisy partial 2D observations. The motion representations acquired in this way incorporate geometric, kinematic, and physical knowledge about human motion, which can be easily transferred to multiple downstream tasks. We implement the motion encoder with a Dual-stream Spatio-temporal Transformer (DSTformer) neural network. It could capture long-range spatio-temporal relationships among the skeletal joints comprehensively and adaptively, exemplified by the lowest 3D pose estimation error so far when trained from scratch. Furthermore, our proposed framework achieves state-of-the-art performance on all three downstream tasks by simply finetuning the pretrained motion encoder with a simple regression head (1-2 layers), which demonstrates the versatility of the learned motion representations. Code and models are available at https://motionbert.github.io/",
    "checked": true,
    "id": "c51eec0bdd986ba28c602e6fef20606edbe02182",
    "semantic_title": "motionbert: a unified perspective on learning human motion representations",
    "citation_count": 2,
    "authors": [
      "Wentao Zhu",
      "Xiaoxuan Ma",
      "Zhaoyang Liu",
      "Libin Liu",
      "Wayne Wu",
      "Yizhou Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chattopadhyay_PASTA_Proportional_Amplitude_Spectrum_Training_Augmentation_for_Syn-to-Real_Domain_Generalization_ICCV_2023_paper.html": {
    "title": "PASTA: Proportional Amplitude Spectrum Training Augmentation for Syn-to-Real Domain Generalization",
    "volume": "main",
    "abstract": "Synthetic data offers the promise of cheap and bountiful training data for settings where labeled real-world data is scarce. However, models trained on synthetic data significantly underperform when evaluated on real-world data. In this paper, we propose Proportional Amplitude Spectrum Training Augmentation (PASTA), a simple and effective augmentation strategy to improve out-of-the-box synthetic-to-real (syn-to-real) generalization performance. PASTA perturbs the amplitude spectra of synthetic images in the Fourier domain to generate augmented views. Specifically, with PASTA we propose a structured perturbation strategy where high-frequency components are perturbed relatively more than the low-frequency ones. For the tasks of semantic segmentation (GTAV-Real), object detection (Sim10K-Real), and object recognition (VisDA-C Syn-Real), across a total of 5 syn-to-real shifts, we find that PASTA outperforms more complex state-of-the-art generalization methods while being complementary to the same",
    "checked": true,
    "id": "98043e6d9d9fb203591842cef4bab440216db439",
    "semantic_title": "pasta: proportional amplitude spectrum training augmentation for syn-to-real domain generalization",
    "citation_count": 1,
    "authors": [
      "Prithvijit Chattopadhyay",
      "Kartik Sarangmath",
      "Vivek Vijaykumar",
      "Judy Hoffman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_EgoPCA_A_New_Framework_for_Egocentric_Hand-Object_Interaction_Understanding_ICCV_2023_paper.html": {
    "title": "EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding",
    "volume": "main",
    "abstract": "With the surge in attention to Egocentric Hand-Object Interaction (Ego-HOI), large-scale datasets such as Ego4D and EPIC-KITCHENS have been proposed. However, most current research is built on resources derived from third-person video action recognition. This inherent domain gap between first- and third-person action videos, which have not been adequately addressed before, makes current Ego-HOI suboptimal. This paper rethinks and proposes a new framework as an infrastructure to advance Ego-HOI recognition by Probing, Curation and Adaption (EgoPCA). We contribute comprehensive pre-train sets, balanced test sets and a new baseline, which are complete with a training-finetuning strategy. With our new framework, we not only achieve state-of-the-art performance on Ego-HOI benchmarks but also build several new and effective mechanisms and settings to advance further research. We believe our data and the findings will pave a new way for Ego-HOI understanding. Code and data are available at https://mvig-rhos.com/ego_pca",
    "checked": true,
    "id": "8fbce4563ef26c35362f009523c0bbc84a61b234",
    "semantic_title": "egopca: a new framework for egocentric hand-object interaction understanding",
    "citation_count": 0,
    "authors": [
      "Yue Xu",
      "Yong-Lu Li",
      "Zhemin Huang",
      "Michael Xu Liu",
      "Cewu Lu",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yin_Metric3D_Towards_Zero-shot_Metric_3D_Prediction_from_A_Single_Image_ICCV_2023_paper.html": {
    "title": "Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image",
    "volume": "main",
    "abstract": "Reconstructing accurate 3D scenes from images is a long-standing vision task. Due to the ill-posedness of the single-image reconstruction problem, most well-established methods are built upon multi-view geometry. State-of-the-art (SOTA) monocular metric depth estimation methods can only handle a single camera model and are unable to perform mixed-data training due to the metric ambiguity. Meanwhile, SOTA monocular methods trained on large mixed datasets achieve zero-shot generalization by learning affine-invariant depths, which cannot recover real-world metrics. In this work, we show that the key to a zero-shot single-view metric depth model lies in the combination of large-scale data training and resolving the metric ambiguity from various camera models. We propose a canonical camera space transformation module, which explicitly addresses the ambiguity problems and can be effortlessly plugged into existing monocular models. Equipped with our module, monocualr models can be stably trained over 8 millions of images with thousands of camera models, resulting in zero-shot generalization to in-the-wild images with unseen camera settings. Experiments demonstrate SOTA performance of our method on 7 zero-shot benchmarks. Our method can recover the metric 3D structure on randomly collected Internet images, enabling plausible single-image metrology. Downstream tasks can also be significantly improved by naively plug-in our model. E.g., our model relieves the scale drift issues of monocular-SLAM (Fig. 1), leading to metric scale high-quality dense mapping",
    "checked": true,
    "id": "bdd2972730730844d0366a5e5f596b1aeaa7c3ed",
    "semantic_title": "metric3d: towards zero-shot metric 3d prediction from a single image",
    "citation_count": 3,
    "authors": [
      "Wei Yin",
      "Chi Zhang",
      "Hao Chen",
      "Zhipeng Cai",
      "Gang Yu",
      "Kaixuan Wang",
      "Xiaozhi Chen",
      "Chunhua Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gu_I_Cant_Believe_Theres_No_Images_Learning_Visual_Tasks_Using_ICCV_2023_paper.html": {
    "title": "I Can't Believe There's No Images! Learning Visual Tasks Using only Language Supervision",
    "volume": "main",
    "abstract": "Many high-level skills that are required for computer vision tasks, such as parsing questions, comparing and contrasting semantics, and writing descriptions, are also required in other domains such as natural language processing. In this paper, we ask whether it is possible to learn those skills from text data and then transfer them to vision tasks without ever training on visual training data. Key to our approach is exploiting the joint embedding space of contrastively trained vision and language encoders. In practice, there can be systematic differences between embedding spaces for different modalities in contrastive models, and we analyze how these differences affect our approach and study strategies to mitigate this concern. We produce models using only text training data on four representative tasks: image captioning, visual entailment, visual question answering and visual news captioning, and evaluate them on standard benchmarks using images. We find these models perform close to models trained on images, while surpassing prior work for captioning and visual entailment in this text-only setting by over 9 points, and outperforming all prior work on visual news by over 30 points. We also showcase a variety of stylistic image captioning models that are trained using no image data and no human-curated language data, but instead using readily-available text data from books, the web, or language models",
    "checked": true,
    "id": "c7175ad54baf30168ce6c366350d21a08e17a91e",
    "semantic_title": "i can't believe there's no images! learning visual tasks using only language supervision",
    "citation_count": 0,
    "authors": [
      "Sophia Gu",
      "Christopher Clark",
      "Aniruddha Kembhavi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Lightweight_Image_Super-Resolution_with_Superpixel_Token_Interaction_ICCV_2023_paper.html": {
    "title": "Lightweight Image Super-Resolution with Superpixel Token Interaction",
    "volume": "main",
    "abstract": "Transformer-based methods have demonstrated impressive results on single-image super-resolution (SISR) task. However, self-attention mechanism is computationally expensive when applied to the entire image. As a result, current approaches divide low-resolution input images into small patches, which are processed separately and then fused to generate high-resolution images. Nevertheless, this conventional regular patch division is too coarse and lacks interpretability, resulting in artifacts and non-similar structure interference during attention operations. To address these challenges, we propose a novel super token interaction network (SPIN). Our method employs superpixels to cluster local similar pixels to form the explicable local regions and utilizes intra-superpixel attention to enable local information interaction. It is interpretable because only similar regions complement each other and dissimilar regions are excluded. Moreover, we design a superpixel cross-attention module to facilitate information propagation via the surrogation of superpixels. Extensive experiments demonstrate that the proposed SPIN model performs favorably against the state-of-the-art SR methods in terms of accuracy and lightweight. Code is available at https://github.com/ArcticHare105/SPIN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aiping Zhang",
      "Wenqi Ren",
      "Yi Liu",
      "Xiaochun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yan_Feature_Prediction_Diffusion_Model_for_Video_Anomaly_Detection_ICCV_2023_paper.html": {
    "title": "Feature Prediction Diffusion Model for Video Anomaly Detection",
    "volume": "main",
    "abstract": "Anomaly detection in the video is an important research area and a challenging task in real applications. Due to the unavailability of large-scale annotated anomaly events, most existing video anomaly detection (VAD) methods focus on learning the distribution of normal samples to detect the substantially deviated samples as anomalies. To well learn the distribution of normal motion and appearance, many auxiliary networks are employed to extract foreground object or action information. These high-level semantic features effectively filter the noise from the background to decrease its influence on detection models. However, the capability of these extra semantic models heavily affects the performance of the VAD methods. Motivated by the impressive generative and anti-noise capacity of diffusion model (DM), in this work, we introduce a novel DM-based method to predict the features of video frames for anomaly detection. We aim to learn the distribution of normal samples without any extra high-level semantic feature extraction models involved. To this end, we build two denoising diffusion implicit modules to predict and refine the features. The first module concentrates on feature motion learning, while the last focuses on feature appearance learning. To the best of our knowledge, it is the first DM-based method to predict frame features for VAD. The strong capacity of DMs also enables our method to more accurately predict the normal features than non-DM-based feature prediction-based VAD methods. Extensive experiments show that the proposed approach substantially outperforms state-of-the-art competing methods",
    "checked": false,
    "id": "30c3f6779c0161f102f21af8ad827729118a1be9",
    "semantic_title": "contrastive attention for video anomaly detection",
    "citation_count": 12,
    "authors": [
      "Cheng Yan",
      "Shiyu Zhang",
      "Yang Liu",
      "Guansong Pang",
      "Wenjun Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Iqbal_RANA_Relightable_Articulated_Neural_Avatars_ICCV_2023_paper.html": {
    "title": "RANA: Relightable Articulated Neural Avatars",
    "volume": "main",
    "abstract": "We propose RANA, a relightable and articulated neural avatar for the photorealistic synthesis of humans under arbitrary viewpoints, body poses, and lighting. We only require a short video clip of the person to create the avatar and assume no knowledge about the lighting environment. We present a novel framework to model humans while disentangling their geometry, texture, and also lighting environment from monocular RGB videos. To simplify this otherwise ill-posed task we first estimate the coarse geometry and texture of the person via SMPL+D model fitting and then learn an articulated neural representation for photorealistic image generation. RANA first generates the normal and albedo maps of the person in any given target body pose and then uses spherical harmonics lighting to generate the shaded image in the target lighting environment. We also propose to pretrain RANA using synthetic images and demonstrate that it leads to better disentanglement between geometry and texture while also improving robustness to novel body poses. Finally, we also present a new photorealistic synthetic dataset, Relighting Humans, to quantitatively evaluate the performance of the proposed approach",
    "checked": true,
    "id": "c188ec54ba8a4f6e53ca5554eadb1fefba72df27",
    "semantic_title": "rana: relightable articulated neural avatars",
    "citation_count": 2,
    "authors": [
      "Umar Iqbal",
      "Akin Caliskan",
      "Koki Nagano",
      "Sameh Khamis",
      "Pavlo Molchanov",
      "Jan Kautz"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zou_Iterative_Denoiser_and_Noise_Estimator_for_Self-Supervised_Image_Denoising_ICCV_2023_paper.html": {
    "title": "Iterative Denoiser and Noise Estimator for Self-Supervised Image Denoising",
    "volume": "main",
    "abstract": "With the emergence of powerful deep learning tools, more and more effective deep denoisers have advanced the field of image denoising. However, the huge progress made by these learning-based methods severely relies on large-scale and high-quality noisy/clean training pairs, which limits the practicality in real-world scenarios. To overcome this, researchers have been exploring self-supervised approaches that can denoise without paired data. However, the unavailable noise prior and inefficient feature extraction take these methods away from high practicality and precision. In this paper, we propose a Denoise-Corrupt-Denoise pipeline (DCD-Net) for self-supervised image denoising. Specifically, we design an iterative training strategy, which iteratively optimizes the denoiser and noise estimator, and gradually approaches high denoising performances using only single noisy images without any noise prior. The proposed self-supervised image denoising framework provides very competitive results compared with state-of-the-art methods on widely used synthetic and real-world image denoising benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Zou",
      "Chenggang Yan",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_MasQCLIP_for_Open-Vocabulary_Universal_Image_Segmentation_ICCV_2023_paper.html": {
    "title": "MasQCLIP for Open-Vocabulary Universal Image Segmentation",
    "volume": "main",
    "abstract": "We present a new method for open-vocabulary universal image segmentation, which is capable of performing instance, semantic, and panoptic segmentation under a unified framework. Our approach, called MasQCLIP, seamlessly integrates with a pre-trained CLIP model by utilizing its dense features, thereby circumventing the need for extensive parameter training. MasQCLIP emphasizes two new aspects when building an image segmentation method with a CLIP model: 1) a student-teacher module to deal with masks of the novel (unseen) classes by distilling information from the base (seen) classes; 2) a fine-tuning process to update model parameters for the queries Q within the CLIP model. Thanks to these two simple and intuitive designs, MasQCLIP is able to achieve state-of-the-art performances with a substantial gain over the competing methods by a large margin across all three tasks, including open-vocabulary instance, semantic, and panoptic segmentation. Project page is at https://masqclip.github.io/",
    "checked": true,
    "id": "37f9b34b8973a63606cfe20d1551d2c75ab6ede8",
    "semantic_title": "masqclip for open-vocabulary universal image segmentation",
    "citation_count": 0,
    "authors": [
      "Xin Xu",
      "Tianyi Xiong",
      "Zheng Ding",
      "Zhuowen Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Memory-and-Anticipation_Transformer_for_Online_Action_Understanding_ICCV_2023_paper.html": {
    "title": "Memory-and-Anticipation Transformer for Online Action Understanding",
    "volume": "main",
    "abstract": "Most existing forecasting systems are memory-based methods, which attempt to mimic human forecasting ability by employing various memory mechanisms and have progressed in temporal modeling for memory dependency. Nevertheless, an obvious weakness of this paradigm is that it can only model limited historical dependence and can not transcend the past. In this paper, we rethink the temporal dependence of event evolution and propose a novel memory-anticipation-based paradigm to model an entire temporal structure, including the past, present, and future. Based on this idea, we present Memory-and-Anticipation Transformer (MAT), a memory-anticipation-based approach, to address the online action detection and anticipation tasks. In addition, owing to the inherent superiority of MAT, it can process online action detection and anticipation tasks in a unified manner. The proposed MAT model is tested on four challenging benchmarks TVSeries, THUMOS'14, HDD, and EPIC-Kitchens-100, for online action detection and anticipation tasks, and it significantly outperforms all existing methods. Code is available at https://github.com/Echo0125/Memory-and-Anticipation-Transformer",
    "checked": true,
    "id": "6c43305ffb387b08b274332e52710acef280a19a",
    "semantic_title": "memory-and-anticipation transformer for online action understanding",
    "citation_count": 1,
    "authors": [
      "Jiahao Wang",
      "Guo Chen",
      "Yifei Huang",
      "Limin Wang",
      "Tong Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Self-similarity_Driven_Scale-invariant_Learning_for_Weakly_Supervised_Person_Search_ICCV_2023_paper.html": {
    "title": "Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search",
    "volume": "main",
    "abstract": "Weakly supervised person search aims to jointly detect and match persons with only bounding box annotations. Existing approaches typically focus on improving the features by exploring the relations of persons. However, scale variation problem is a more severe obstacle and under-studied that a person often owns images with different scales (resolutions). For one thing, small-scale images contain less information of a person, thus affecting the accuracy of the generated pseudo labels. For another, different similarities between cross-scale images of a person increase the difficulty of matching. In this paper, we address it by proposing a novel one-step framework, named Self-similarity driven Scale-invariant Learning (SSL). Scale invariance can be explored based on the self-similarity prior that it shows the same statistical properties of an image at different scales. To this end, we introduce a Multi-scale Exemplar Branch to guide the network in concentrating on the foreground and learning scale-invariant features by hard exemplars mining. To enhance the discriminative power of the learned features, we further introduce a dynamic pseudo label prediction that progressively seeks true labels for training. Experimental results on two standard benchmarks, i.e., PRW and CUHK-SYSU datasets, demonstrate that the proposed method can solve scale variation problem effectively and perform favorably against state-of-the-art methods. Code is available at https://github.com/Wangbenzhi/SSL.git",
    "checked": true,
    "id": "8a15d190e010dada2e3b1319df274b797f647537",
    "semantic_title": "self-similarity driven scale-invariant learning for weakly supervised person search",
    "citation_count": 1,
    "authors": [
      "Benzhi Wang",
      "Yang Yang",
      "Jinlin Wu",
      "Guo-jun Qi",
      "Zhen Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_MODA_Mapping-Once_Audio-driven_Portrait_Animation_with_Dual_Attentions_ICCV_2023_paper.html": {
    "title": "MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions",
    "volume": "main",
    "abstract": "Audio-driven portrait animation aims to synthesize portrait videos that are conditioned by given audio. Animating high-fidelity and multimodal video portraits has a variety of applications. Previous methods have attempted to capture different motion modes and generate high-fidelity portrait videos by training different models or sampling signals from given videos. However, lacking correlation learning between lip-sync and other movements (e.g., head pose/eye blinking) usually leads to unnatural results. In this paper, we propose a unified system for multi-person, diverse, and high-fidelity talking portrait generation. Our method contains three stages, i.e., 1) Mapping-Once network with Dual Attentions (MODA) generates talking representation from given audio. In MODA, we design a dual-attention module to encode accurate mouth movements and diverse modalities. 2) Facial composer network generates dense and detailed face landmarks, and 3) temporal-guided render syntheses stable videos. Extensive evaluations demonstrate that the proposed system produces more natural and realistic video portraits compared to previous methods",
    "checked": true,
    "id": "f44dba48520c7af65e5e349c40bc8041c235da6d",
    "semantic_title": "moda: mapping-once audio-driven portrait animation with dual attentions",
    "citation_count": 0,
    "authors": [
      "Yunfei Liu",
      "Lijian Lin",
      "Fei Yu",
      "Changyin Zhou",
      "Yu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Realistic_Full-Body_Tracking_from_Sparse_Observations_via_Joint-Level_Modeling_ICCV_2023_paper.html": {
    "title": "Realistic Full-Body Tracking from Sparse Observations via Joint-Level Modeling",
    "volume": "main",
    "abstract": "To bridge the physical and virtual worlds for rapidly developed VR/AR applications, the ability to realistically drive 3D full-body avatars is of great significance. Although real-time body tracking with only the head-mounted displays (HMDs) and hand controllers is heavily under-constrained, a carefully designed end-to-end neural network is of great potential to solve the problem by learning from large-scale motion data. To this end, we propose a two-stage framework that can obtain accurate and smooth full-body motions with the three tracking signals of head and hands only. Our framework explicitly models the joint-level features in the first stage and utilizes them as spatiotemporal tokens for alternating spatial and temporal transformer blocks to capture joint-level correlations in the second stage. Furthermore, we design a set of loss terms to constrain the task of a high degree of freedom, such that we can exploit the potential of our joint-level modeling. With extensive experiments on the AMASS motion dataset and real-captured data, we validate the effectiveness of our designs and show our proposed method can achieve more accurate and smooth motion compared to existing approaches",
    "checked": true,
    "id": "145ab9ba6a6788b129541a7eaf7d27a4e0fc9fcf",
    "semantic_title": "realistic full-body tracking from sparse observations via joint-level modeling",
    "citation_count": 0,
    "authors": [
      "Xiaozheng Zheng",
      "Zhuo Su",
      "Chao Wen",
      "Zhou Xue",
      "Xiaojie Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yin_MetaF2N_Blind_Image_Super-Resolution_by_Learning_Efficient_Model_Adaptation_from_ICCV_2023_paper.html": {
    "title": "MetaF2N: Blind Image Super-Resolution by Learning Efficient Model Adaptation from Faces",
    "volume": "main",
    "abstract": "Due to their highly structured characteristics, faces are easier to recover than natural scenes for blind image super-resolution. Therefore, we can extract the degradation representation of an image from the low-quality and recovered face pairs. Using the degradation representation, realistic low-quality images can then be synthesized to fine-tune the super-resolution model for the real-world low-quality image. However, such a procedure is time-consuming and laborious, and the gaps between recovered faces and the ground-truths further increase the optimization uncertainty. To facilitate efficient model adaptation towards image-specific degradations, we propose a method dubbed MetaF2N, which leverages the contained faces to fine-tune model parameters for adapting to the whole natural image in a meta-learning framework. The degradation extraction and low-quality image synthesis steps are thus circumvented in our MetaF2N, and it requires only one fine-tuning step to get decent performance. Considering the gaps between the recovered faces and ground-truths, we further deploy a MaskNet for adaptively predicting loss weights at different positions to reduce the impact of low-confidence areas. To evaluate our proposed MetaF2N, we have collected a real-world low-quality dataset with one or multiple faces in each image, and our MetaF2N achieves superior performance on both synthetic and realworld datasets. Source code, pre-trained models, and collected datasets are available at https://github.com/yinzhicun/MetaF2N",
    "checked": true,
    "id": "f8275426cdf947a93eb706453cfc712ac0692a13",
    "semantic_title": "metaf2n: blind image super-resolution by learning efficient model adaptation from faces",
    "citation_count": 0,
    "authors": [
      "Zhicun Yin",
      "Ming Liu",
      "Xiaoming Li",
      "Hui Yang",
      "Longan Xiao",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Lighting_up_NeRF_via_Unsupervised_Decomposition_and_Enhancement_ICCV_2023_paper.html": {
    "title": "Lighting up NeRF via Unsupervised Decomposition and Enhancement",
    "volume": "main",
    "abstract": "Neural Radiance Field (NeRF) is a promising approach for synthesizing novel views, given a set of images and the corresponding camera poses of a scene. However, images photographed from a low-light scene can hardly be used to train a NeRF model to produce high-quality results, due to their low pixel intensities, heavy noise, and color distortion. Combining existing low-light image enhancement methods with NeRF methods also does not work well due to the view inconsistency caused by the individual 2D enhancement process. In this paper, we propose a novel approach, called Low-Light NeRF (or LLNeRF), to enhance the scene representation and synthesize normal-light novel views directly from sRGB low-light images in an unsupervised manner. The core of our approach is a decomposition of radiance field learning, which allows us to enhance the illumination, reduce noise and correct the distorted colors jointly with the NeRF optimization process. Our method is able to produce novel view images with proper lighting and vivid colors and details, given a collection of camera-finished low dynamic range (8-bits/channel) images from a low-light scene. Experiments demonstrate that our method outperforms existing low-light enhancement methods and NeRF methods",
    "checked": true,
    "id": "20f3b47436d066f513743289cc285c6716cff70d",
    "semantic_title": "lighting up nerf via unsupervised decomposition and enhancement",
    "citation_count": 0,
    "authors": [
      "Haoyuan Wang",
      "Xiaogang Xu",
      "Ke Xu",
      "Rynson W.H. Lau"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Feng_ViM_Vision_Middleware_for_Unified_Downstream_Transferring_ICCV_2023_paper.html": {
    "title": "ViM: Vision Middleware for Unified Downstream Transferring",
    "volume": "main",
    "abstract": "Foundation models are pre-trained on massive data and transferred to downstream tasks via fine-tuning. This work presents Vision Middleware (ViM), a new learning paradigm that targets unified transferring from a single foundation model to a variety of downstream tasks. ViM consists of a zoo of lightweight plug-in modules, each of which is independently learned on a midstream dataset with a shared frozen backbone. Downstream tasks can then benefit from an adequate aggregation of the module zoo thanks to the rich knowledge inherited from midstream tasks. There are three major advantages of such a design. From the efficiency aspect, the upstream backbone can be trained only once and reused for all downstream tasks without tuning. From the scalability aspect, we can easily append additional modules to ViM with no influence on existing modules. From the performance aspect, ViM can include as many midstream tasks as possible, narrowing the task gap between upstream and downstream. Considering these benefits, we believe that ViM, which the community could maintain and develop together, would serve as a powerful tool to assist foundation models",
    "checked": true,
    "id": "530bee65ee844ed794d98b1120e4cf2738558316",
    "semantic_title": "vim: vision middleware for unified downstream transferring",
    "citation_count": 0,
    "authors": [
      "Yutong Feng",
      "Biao Gong",
      "Jianwen Jiang",
      "Yiliang Lv",
      "Yujun Shen",
      "Deli Zhao",
      "Jingren Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_DIRE_for_Diffusion-Generated_Image_Detection_ICCV_2023_paper.html": {
    "title": "DIRE for Diffusion-Generated Image Detection",
    "volume": "main",
    "abstract": "Diffusion models have shown remarkable success in visual synthesis, but have also raised concerns about potential abuse for malicious purposes. In this paper, we seek to build a detector for telling apart real images from diffusion-generated images. We find that existing detectors struggle to detect images generated by diffusion models, even if we include generated images from a specific diffusion model in their training data. To address this issue, we propose a novel image representation called DIffusion Reconstruction Error (DIRE), which measures the error between an input image and its reconstruction counterpart by a pre-trained diffusion model. We observe that diffusion-generated images can be approximately reconstructed by a diffusion model while real images cannot. It provides a hint that DIRE can serve as a bridge to distinguish generated and real images. DIRE provides an effective way to detect images generated by most diffusion models, and it is general for detecting generated images from unseen diffusion models and robust to various perturbations. Furthermore, we establish a comprehensive diffusion-generated benchmark including images generated by eight diffusion models to evaluate the performance of diffusion-generated image detectors. Extensive experiments on our collected benchmark demonstrate that DIRE exhibits superiority over previous generated-image detectors. The code, models, and dataset are available at https://github.com/ZhendongWang6/DIRE",
    "checked": true,
    "id": "d9d82eb6a8886226724ea230a7e5923d660a0bad",
    "semantic_title": "dire for diffusion-generated image detection",
    "citation_count": 8,
    "authors": [
      "Zhendong Wang",
      "Jianmin Bao",
      "Wengang Zhou",
      "Weilun Wang",
      "Hezhen Hu",
      "Hong Chen",
      "Houqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Ord2Seq_Regarding_Ordinal_Regression_as_Label_Sequence_Prediction_ICCV_2023_paper.html": {
    "title": "Ord2Seq: Regarding Ordinal Regression as Label Sequence Prediction",
    "volume": "main",
    "abstract": "Ordinal regression refers to classifying object instances into ordinal categories. It has been widely studied in many scenarios, such as medical disease grading and movie rating. Known methods focused only on learning inter-class ordinal relationships, but still incur limitations in distinguishing adjacent categories thus far. In this paper, we propose a simple sequence prediction framework for ordinal regression called Ord2Seq, which, for the first time, transforms each ordinal category label into a special label sequence and thus regards an ordinal regression task as a sequence prediction process. In this way, we decompose an ordinal regression task into a series of recursive binary classification steps, so as to subtly distinguish adjacent categories. Comprehensive experiments show the effectiveness of distinguishing adjacent categories for performance improvement and our new approach exceeds state-of-the-art performances in four different scenarios. Codes are available at https://github.com/wjh892521292/Ord2Seq",
    "checked": true,
    "id": "ae29578c017e6ee5a6f3d829e8929168b1844126",
    "semantic_title": "ord2seq: regarding ordinal regression as label sequence prediction",
    "citation_count": 0,
    "authors": [
      "Jinhong Wang",
      "Yi Cheng",
      "Jintai Chen",
      "TingTing Chen",
      "Danny Chen",
      "Jian Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Bring_Clipart_to_Life_ICCV_2023_paper.html": {
    "title": "Bring Clipart to Life",
    "volume": "main",
    "abstract": "The development of face editing has been boosted since the birth of StyleGAN. While previous works have explored different interactive methods, such as sketching and exemplar photos, they have been limited in terms of expressiveness and generality. In this paper, we propose a new interaction method by guiding the editing with abstract clipart, composed of a set of simple semantic parts, allowing users to control across face photos with simple clicks. However, this is a challenging task given the large domain gap between colorful face photos and abstract clipart with limited data. To solve this problem, we introduce a framework called ClipFaceShop built on top of StyleGAN. The key idea is to take advantage of W+ latent code encoded rich and disentangled visual features, and create a new lightweight selective feature adaptor to predict a modifiable path toward the target output photo. Since no pairwise labeled data exists for training, we design a set of losses to provide supervision signals for learning the modifiable path. Experimental results show that ClipFaceShop generates realistic and faithful face photos, sharing the same facial attributes as the reference clipart. We demonstrate that ClipFaceShop supports clipart in diverse styles, even in form of a free-hand sketch",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nanxuan Zhao",
      "Shengqi Dang",
      "Hexun Lin",
      "Yang Shi",
      "Nan Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/You_Co-Evolution_of_Pose_and_Mesh_for_3D_Human_Body_Estimation_ICCV_2023_paper.html": {
    "title": "Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video",
    "volume": "main",
    "abstract": "Despite significant progress in single image-based 3D human mesh recovery, accurately and smoothly recovering 3D human motion from a video remains challenging. Existing video-based methods generally recover human mesh by estimating the complex pose and shape parameters from coupled image features, whose high complexity and low representation ability often result in inconsistent pose motion and limited shape patterns. To alleviate this issue, we introduce 3D pose as the intermediary and propose a Pose and Mesh Co-Evolution network (PMCE) that decouples this task into two parts: 1) video-based 3D human pose estimation and 2) mesh vertices regression from the estimated 3D pose and temporal image feature. Specifically, we propose a two-stream encoder that estimates mid-frame 3D pose and extracts a temporal image feature from the input image sequence. In addition, we design a co-evolution decoder that performs pose and mesh interactions with the image-guided Adaptive Layer Normalization (AdaLN) to make pose and mesh fit the human body shape. Extensive experiments demonstrate that the proposed PMCE outperforms previous state-of-the-art methods in terms of both per-frame accuracy and temporal consistency on three benchmark datasets: 3DPW, Human3.6M, and MPI-INF-3DHP. Our code is available at https://github.com/kasvii/PMCE",
    "checked": true,
    "id": "748a58465e7a81fc64035c4388518beb7f180d4f",
    "semantic_title": "co-evolution of pose and mesh for 3d human body estimation from video",
    "citation_count": 0,
    "authors": [
      "Yingxuan You",
      "Hong Liu",
      "Ti Wang",
      "Wenhao Li",
      "Runwei Ding",
      "Xia Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Noise2Info_Noisy_Image_to_Information_of_Noise_for_Self-Supervised_Image_ICCV_2023_paper.html": {
    "title": "Noise2Info: Noisy Image to Information of Noise for Self-Supervised Image Denoising",
    "volume": "main",
    "abstract": "Unsupervised image denoising has been proposed to alleviate the widespread noise problem without requiring clean images. Existing works mainly follow the self-supervised way, which tries to reconstruct each pixel x of noisy images without the knowledge of x. More recently, some pioneer works further emphasize the importance of x and propose to weigh the information extracted from x and other pixels when recovering x. However, such a method is highly sensitive to the standard deviation \\sigma_n of noises injected to clean images, where \\sigma_n is inaccessible without knowing clean images. Thus, it is unrealistic to assume that \\sigma_n is known for pursuing high model performance. To alleviate this issue, we propose Noise2Info to extract the critical information, the standard deviation \\sigma_n of injected noise, only based on the noisy images. Specifically, we first theoretically provide an upper bound on \\sigma_n, while the bound requires clean images. Then, we propose a novel method to estimate the bound of \\sigma_n by only using noisy images. Besides, we prove that the difference between our estimation with the true deviation goes smaller as the model training. Empirical studies show that Noise2Info is effective and robust on benchmark data sets and closely estimates the standard deviation of noises during model training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachuan Wang",
      "Shimin Di",
      "Lei Chen",
      "Charles Wang Wai Ng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Controllable_Visual-Tactile_Synthesis_ICCV_2023_paper.html": {
    "title": "Controllable Visual-Tactile Synthesis",
    "volume": "main",
    "abstract": "Deep generative models have various content creation applications such as graphic design, e-commerce, and virtual try-on. However, current works mainly focus on synthesizing realistic visual outputs, often ignoring other sensory modalities, such as touch, which limits physical interaction with users. In this work, we leverage deep generative models to create a multi-sensory experience where users can touch and see the synthesized object when sliding their fingers on a haptic surface. The main challenges lie in the significant scale discrepancy between vision and touch sensing and the lack of explicit mapping from touch sensing data to a haptic rendering device. To bridge this gap, we collect high-resolution tactile data with a GelSight sensor and create a new visuotactile clothing dataset. We then develop a conditional generative model that synthesizes both visual and tactile outputs from a single sketch. We evaluate our method regarding image quality and tactile rendering accuracy. Finally, we introduce a pipeline to render high-quality visual and tactile outputs on an electroadhesion-based haptic device for an immersive experience, allowing for challenging materials and editable sketch inputs",
    "checked": true,
    "id": "93f364590f5bd062fefe40b61ddbc7a4b47c81c2",
    "semantic_title": "controllable visual-tactile synthesis",
    "citation_count": 0,
    "authors": [
      "Ruihan Gao",
      "Wenzhen Yuan",
      "Jun-Yan Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Psomas_Keep_It_SimPool_Who_Said_Supervised_Transformers_Suffer_from_Attention_ICCV_2023_paper.html": {
    "title": "Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?",
    "volume": "main",
    "abstract": "Convolutional networks and vision transformers have different forms of pairwise interactions, pooling across layers and pooling at the end of the network. Does the latter really need to be different? As a by-product of pooling, vision transformers provide spatial attention for free, but this is most often of low quality unless self-supervised, which is not well studied. Is supervision really the problem? In this work, we develop a generic pooling framework and then we formulate a number of existing methods as instantiations. By discussing the properties of each group of methods, we derive SimPool, a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. We find that, whether supervised or self-supervised, this improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases. One could thus call SimPool universal. To our knowledge, we are the first to obtain attention maps in supervised transformers of at least as good quality as self-supervised, without explicit losses or modifying the architecture. Code at: https://github.com/billpsomas/simpool",
    "checked": true,
    "id": "efdef5b804e9038cee1f3ad1b719bec2ac488d5d",
    "semantic_title": "keep it simpool: who said supervised transformers suffer from attention deficit?",
    "citation_count": 0,
    "authors": [
      "Bill Psomas",
      "Ioannis Kakogeorgiou",
      "Konstantinos Karantzalos",
      "Yannis Avrithis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_SynBody_Synthetic_Dataset_with_Layered_Human_Models_for_3D_Human_ICCV_2023_paper.html": {
    "title": "SynBody: Synthetic Dataset with Layered Human Models for 3D Human Perception and Modeling",
    "volume": "main",
    "abstract": "Synthetic data has emerged as a promising source for 3D human research as it offers low-cost access to large-scale human datasets. To advance the diversity and annotation quality of human models, we introduce a new synthetic dataset, SynBody, with three appealing features: 1) a clothed parametric human model that can generate a diverse range of subjects; 2) the layered human representation that naturally offers high-quality 3D annotations to support multiple tasks; 3) a scalable system for producing realistic data to facilitate real-world tasks. The dataset comprises 1.2M images with corresponding accurate 3D annotations, covering 10,000 human body models, 1,187 actions, and various viewpoints. The dataset includes two subsets for human pose and shape estimation as well as human neural rendering. Extensive experiments on SynBody indicate that it substantially enhances both SMPL and SMPL-X estimation. Furthermore, the incorporation of layered annotations offers a valuable training resource for investigating the Human Neural Radiance Fields(NeRF)",
    "checked": true,
    "id": "7ea5344715d210dad8171af1ca5018c665e4972a",
    "semantic_title": "synbody: synthetic dataset with layered human models for 3d human perception and modeling",
    "citation_count": 3,
    "authors": [
      "Zhitao Yang",
      "Zhongang Cai",
      "Haiyi Mei",
      "Shuai Liu",
      "Zhaoxi Chen",
      "Weiye Xiao",
      "Yukun Wei",
      "Zhongfei Qing",
      "Chen Wei",
      "Bo Dai",
      "Wayne Wu",
      "Chen Qian",
      "Dahua Lin",
      "Ziwei Liu",
      "Lei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Szymanowicz_Viewset_Diffusion_0-Image-Conditioned_3D_Generative_Models_from_2D_Data_ICCV_2023_paper.html": {
    "title": "Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data",
    "volume": "main",
    "abstract": "We present Viewset Diffusion, a diffusion-based generator that outputs 3D objects while only using multi-view 2D data for supervision. We note that there exists a one-to-one mapping between viewsets, i.e., collections of several 2D views of an object, and 3D models. Hence, we train a diffusion model to generate viewsets, but design the neural network generator to reconstruct internally corresponding 3D models, thus generating those too. We fit a diffusion model to a large number of viewsets for a given category of objects. The resulting generator can be conditioned on zero, one or more input views. Conditioned on a single view, it performs 3D reconstruction accounting for the ambiguity of the task and allowing to sample multiple solutions compatible with the input. The model performs reconstruction efficiently, in a feed-forward manner, and is trained using only rendering losses using as few as three views per viewset. Project page: szymanowiczs.github.io/viewset-diffusion",
    "checked": true,
    "id": "a59373155cd7f27de4687bcb97c02e2ed5926a9b",
    "semantic_title": "viewset diffusion: (0-)image-conditioned 3d generative models from 2d data",
    "citation_count": 5,
    "authors": [
      "Stanislaw Szymanowicz",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shi_LoGoPrompt_Synthetic_Text_Images_Can_Be_Good_Visual_Prompts_for_ICCV_2023_paper.html": {
    "title": "LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models",
    "volume": "main",
    "abstract": "Prompt engineering is a powerful tool used to enhance the performance of pre-trained models on downstream tasks. For example, providing the prompt \"Let's think step by step\" improved GPT-3's reasoning accuracy to 63% on MutiArith while prompting \"a photo of\" filled with a class name enables CLIP to achieve 80% zero-shot accuracy on ImageNet. While previous research has explored prompt learning for the visual modality, analyzing what constitutes a good visual prompt specifically for image recognition is limited. In addition, existing visual prompt tuning methods' generalization ability is worse than text-only prompting tuning. This paper explores our key insight: synthetic text images are good visual prompts for vision-language models! To achieve that, we propose our LoGoPrompt, which reformulates the classification objective to the visual prompt selection and addresses the chicken-and-egg challenge of first adding synthetic text images as class-wise visual prompts or predicting the class first. Without any trainable visual prompt parameters, experimental results on 16 datasets demonstrate that our method consistently outperforms state-of-the-art methods in few-shot learning, base-to-new generalization, and domain generalization. The code will be publicly available upon publication",
    "checked": true,
    "id": "da1df9dbbbfaa6031434f57d96be70d8fc0b0227",
    "semantic_title": "logoprompt: synthetic text images can be good visual prompts for vision-language models",
    "citation_count": 2,
    "authors": [
      "Cheng Shi",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_EP2P-Loc_End-to-End_3D_Point_to_2D_Pixel_Localization_for_Large-Scale_ICCV_2023_paper.html": {
    "title": "EP2P-Loc: End-to-End 3D Point to 2D Pixel Localization for Large-Scale Visual Localization",
    "volume": "main",
    "abstract": "Visual localization is the task of estimating a 6-DoF camera pose of a query image within a provided 3D reference map. Thanks to recent advances in various 3D sensors, 3D point clouds are becoming a more accurate and affordable option for building the reference map, but research to match the points of 3D point clouds with pixels in 2D images for visual localization remains challenging. Existing approaches that jointly learn 2D-3D feature matching suffer from low inliers due to representational differences between the two modalities, and the methods that bypass this problem into classification have an issue of poor refinement. In this work, we propose EP2P-Loc, a novel large-scale visual localization method that mitigates such appearance discrepancy and enables end-to-end training for pose estimation. To increase the number of inliers, we propose a simple algorithm to remove invisible 3D points in the image, and find all 2D-3D correspondences without keypoint detection. To reduce memory usage and search complexity, we take a coarse-to-fine approach where we extract patch-level features from 2D images, then perform 2D patch classification on each 3D point, and obtain the exact corresponding 2D pixel coordinates through positional encoding. Finally, for the first time in this task, we employ a differentiable PnP for end-to-end training. In the experiments on newly curated large-scale indoor and outdoor benchmarks based on 2D-3D-S and KITTI, we show that our method achieves the state-of-the-art performance compared to existing visual localization and image-to-point cloud registration methods",
    "checked": true,
    "id": "c5da5afc425c9120924ff8ca0a794c21d2d33e86",
    "semantic_title": "ep2p-loc: end-to-end 3d point to 2d pixel localization for large-scale visual localization",
    "citation_count": 0,
    "authors": [
      "Minjung Kim",
      "Junseo Koo",
      "Gunhee Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_SIRA-PCR_Sim-to-Real_Adaptation_for_3D_Point_Cloud_Registration_ICCV_2023_paper.html": {
    "title": "SIRA-PCR: Sim-to-Real Adaptation for 3D Point Cloud Registration",
    "volume": "main",
    "abstract": "Point cloud registration is essential for many applications. However, existing real datasets require extremely tedious and costly annotations, yet may not provide accurate camera poses. For the synthetic datasets, they are mainly object-level, so the trained models may not generalize well to real scenes. We design SIRA-PCR, a new approach to 3D point cloud registration. First, we build a synthetic scene-level 3D registration dataset, specifically designed with physically-based and random strategies to arrange diverse objects. Second, we account for variations in different sensing mechanisms and layout placements, then formulate a sim-to-real adaptation framework with an adaptive re-sample module to simulate patterns in real point clouds. To our best knowledge, this is the first work that explores sim-to-real adaptation for point cloud registration. Extensive experiments show the SOTA performance of SIRA-PCR on widely-used indoor and outdoor datasets. The code and dataset will be released on https://github.com/Chen-Suyi/SIRA_Pytorch",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suyi Chen",
      "Hao Xu",
      "Ru Li",
      "Guanghui Liu",
      "Chi-Wing Fu",
      "Shuaicheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hashmi_FeatEnHancer_Enhancing_Hierarchical_Features_for_Object_Detection_and_Beyond_Under_ICCV_2023_paper.html": {
    "title": "FeatEnHancer: Enhancing Hierarchical Features for Object Detection and Beyond Under Low-Light Vision",
    "volume": "main",
    "abstract": "Extracting useful visual cues for the downstream tasks is especially challenging under low-light vision. Prior works create enhanced representations by either correlating visual quality with machine perception or designing illumination-degrading transformation methods that require pre-training on synthetic datasets. We argue that optimizing enhanced image representation pertaining to the loss of the downstream task can result in more expressive representations. Therefore, in this work, we propose a novel module, FeatEnHancer, that hierarchically combines multiscale features using multiheaded attention guided by task-related loss function to create suitable representations. Furthermore, our intra-scale enhancement improves the quality of features extracted at each scale or level, as well as combines features from different scales in a way that reflects their relative importance for the task at hand. FeatEnHancer is a general-purpose plug-and-play module and can be incorporated into any low-light vision pipeline. We show with extensive experimentation that the enhanced representation produced with FeatEnHancer significantly and consistently improves results in several low-light vision tasks, including dark object detection (+5.7 mAP on ExDark), face detection (+1.5 mAP on DARK FACE), nighttime semantic segmentation (+5.1 mIoU on ACDC ), and video object detection (+1.8 mAP on DarkVision), highlighting the effectiveness of enhancing hierarchical features under low-light vision",
    "checked": true,
    "id": "524b2ded610a116d9242f9bd87851d96af4bcbfd",
    "semantic_title": "featenhancer: enhancing hierarchical features for object detection and beyond under low-light vision",
    "citation_count": 0,
    "authors": [
      "Khurram Azeem Hashmi",
      "Goutham Kallempudi",
      "Didier Stricker",
      "Muhammad Zeshan Afzal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_SOAR_Scene-debiasing_Open-set_Action_Recognition_ICCV_2023_paper.html": {
    "title": "SOAR: Scene-debiasing Open-set Action Recognition",
    "volume": "main",
    "abstract": "Deep models have the risk of utilizing spurious clues to make predictions, e.g., recognizing actions via classifying the background scene. This problem severely degrades the open-set action recognition performance when the testing samples exhibit scene distributions different from the training samples. To mitigate this scene bias, we propose a Scene-debiasing Open-set Action Recognition method (SOAR), which features an adversarial reconstruction module and an adaptive adversarial scene classification module. The former prevents a decoder from reconstructing the video background given video features, and thus helps reduce the background information in feature learning. The latter aims to confuse scene type classification given video features, and helps to learn scene-invariant information. In addition, we design an experiment to quantify the scene bias. The results suggest current open-set action recognizers are biased toward the scene, and our SOAR better mitigates such bias. Furthermore, extensive experiments show our method outperforms state-of-the-art methods, with ablation studies demonstrating the effectiveness of our proposed modules",
    "checked": true,
    "id": "66e9e2a9868d19cabacdd1702cd94dcafcca2fc5",
    "semantic_title": "soar: scene-debiasing open-set action recognition",
    "citation_count": 0,
    "authors": [
      "Yuanhao Zhai",
      "Ziyi Liu",
      "Zhenyu Wu",
      "Yi Wu",
      "Chunluan Zhou",
      "David Doermann",
      "Junsong Yuan",
      "Gang Hua"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Physics-Augmented_Autoencoder_for_3D_Skeleton-Based_Gait_Recognition_ICCV_2023_paper.html": {
    "title": "Physics-Augmented Autoencoder for 3D Skeleton-Based Gait Recognition",
    "volume": "main",
    "abstract": "In this paper, we introduce physics-augmented autoencoder (PAA), a framework for 3D skeleton-based human gait recognition. Specifically, we construct the autoencoder with a graph-convolution-based encoder and a physics-based decoder. The encoder takes the skeleton sequence as input and generates the generalized positions and forces of each joint, which are taken by the decoder to reconstruct the input skeleton based on the Lagrangian dynamics. In this way, the intermediate representations are physically plausible and discriminative. During the inference, the decoder is discared and a RNN-based classifier takes the output of the encoder for gait recognition. We evaluated our proposed method on three benchmark datasets including Gait3D, GREW, and KinectGait. Our method achieves state-of-the-art performance for 3D skeleton-based gait recognition. Furthermore, extensive ablation studies show that our method generalizes better and is more robust with small-scale training data by incorporating the physics knowledge. We also validated the physical plausibility of the intermediate representations by making force predictions on real data with physical annotations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongji Guo",
      "Qiang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Regularized_Primitive_Graph_Learning_for_Unified_Vector_Mapping_ICCV_2023_paper.html": {
    "title": "Regularized Primitive Graph Learning for Unified Vector Mapping",
    "volume": "main",
    "abstract": "Large-scale vector mapping is the foundation for transportation and urban planning. Most existing mapping methods are tailored to one specific mapping task, due to task-specific requirements on shape regularization and topology reconstruction. We propose GraphMapper, a unified framework for end-to-end vector map extraction from satellite images. Our key idea is using primitive graph as a unified representation of vector maps and formulating shape regularization and topology reconstruction as primitive graph reconstruction problems that can be solved in the same framework. Specifically, shape regularization is modeled as the consistency between primitive directions and their pairwise relationship. Based on the primitive graph, we design a learning approach to reconstruct primitive graphs in multiple stages. GraphMapper can fully explore primitive-wise and pairwise information for shape regularization and topology reconstruction, resulting improved primitive graph learning capabilities. We empirically demonstrate the effectiveness of GraphMapper on two challenging mapping tasks for building footprints and road networks. With the premise of sharing the majority design of the architecture and a few task-specific designs, our model outperforms state-of-the-art methods in both tasks on public benchmarks. Our code will be publicly available",
    "checked": false,
    "id": "32b1a22e64bc50a059295b6814ce210e7f34dc38",
    "semantic_title": "primitive graph learning for unified vector mapping",
    "citation_count": 0,
    "authors": [
      "Lei Wang",
      "Min Dai",
      "Jianan He",
      "Jingwei Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Saliency_Regularization_for_Self-Training_with_Partial_Annotations_ICCV_2023_paper.html": {
    "title": "Saliency Regularization for Self-Training with Partial Annotations",
    "volume": "main",
    "abstract": "Partially annotated images are easy to obtain in multi-label classification. However, unknown labels in partially annotated images exacerbate the positive-negative imbalance inherent in multi-label classification, which affects supervised learning of known labels. Most current methods require sufficient image annotations, and do not focus on the imbalance of the labels in the supervised training phase. In this paper, we propose saliency regularization (SR) for a novel self-training framework. In particular, we model saliency on the class-specific maps, and strengthen the saliency of object regions corresponding to the present labels. Besides, we introduce consistency regularization to mine unlabeled information to complement unknown labels with the help of SR. It is verified to alleviate the negative dominance caused by the imbalance, and achieve state-of-the-art performance on Pascal VOC 2007, MS-COCO, VG-200, and OpenImages V3",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shouwen Wang",
      "Qian Wan",
      "Xiang Xiang",
      "Zhigang Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Stabilizing_Visual_Reinforcement_Learning_via_Asymmetric_Interactive_Cooperation_ICCV_2023_paper.html": {
    "title": "Stabilizing Visual Reinforcement Learning via Asymmetric Interactive Cooperation",
    "volume": "main",
    "abstract": "Vision-based reinforcement learning (RL) depends on discriminative representation encoders to abstract the observation states. Despite the great success of increasing CNN parameters for many supervised computer vision tasks, reinforcement learning with temporal-difference (TD) losses cannot benefit from it in most complex environments. In this paper, we analyze that the training instability arises from the oscillating self-overfitting of the heavy-optimizable encoder. We argue that serious oscillation will occur to the parameters when enforced to fit the sensitive TD targets, causing uncertain drifting of the latent state space and thus transmitting these perturbations to the policy learning. To alleviate this phenomenon, we propose a novel asymmetric interactive cooperation approach with the interaction between a heavy-optimizable encoder and a supportive light-optimizable encoder, in which both their advantages are integrated including the highly discriminative capability as well as the training stability. We also present a greedy bootstrapping optimization to isolate the visual perturbations from policy learning, where representation and policy are trained sufficiently by turns. Finally, we demonstrate the effectiveness of our method in utilizing larger visual models by first-person highway driving task CARLA and Vizdoom environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunpeng Zhai",
      "Peixi Peng",
      "Yifan Zhao",
      "Yangru Huang",
      "Yonghong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Seo_FlipNeRF_Flipped_Reflection_Rays_for_Few-shot_Novel_View_Synthesis_ICCV_2023_paper.html": {
    "title": "FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis",
    "volume": "main",
    "abstract": "Neural Radiance Field (NeRF) has been a mainstream in novel view synthesis with its remarkable quality of rendered images and simple architecture. Although NeRF has been developed in various directions improving continuously its performance, the necessity of a dense set of multi-view images still exists as a stumbling block to progress for practical application. In this work, we propose FlipNeRF, a novel regularization method for few-shot novel view synthesis by utilizing our proposed flipped reflection rays. The flipped reflection rays are explicitly derived from the input ray directions and estimated normal vectors, and play a role of effective additional training rays while enabling to estimate more accurate surface normals and learn the 3D geometry effectively. Since the surface normal and the scene depth are both derived from the estimated densities along a ray, the accurate surface normal leads to more exact depth estimation, which is a key factor for few-shot novel view synthesis. Furthermore, with our proposed Uncertainty-aware Emptiness Loss and Bottleneck Feature Consistency Loss, FlipNeRF is able to estimate more reliable outputs with reducing floating artifacts effectively across the different scene structures, and enhance the feature-level consistency between the pair of the rays cast toward the photo-consistent pixels without any additional feature extractor, respectively. Our FlipNeRF achieves the SOTA performance on the multiple benchmarks across all the scenarios",
    "checked": true,
    "id": "6b88958e67ce08e8d77a19cf46d0a37d703f4e58",
    "semantic_title": "flipnerf: flipped reflection rays for few-shot novel view synthesis",
    "citation_count": 1,
    "authors": [
      "Seunghyeon Seo",
      "Yeonjin Chang",
      "Nojun Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Discovering_Spatio-Temporal_Rationales_for_Video_Question_Answering_ICCV_2023_paper.html": {
    "title": "Discovering Spatio-Temporal Rationales for Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicong Li",
      "Junbin Xiao",
      "Chun Feng",
      "Xiang Wang",
      "Tat-Seng Chua"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Iterative_Soft_Shrinkage_Learning_for_Efficient_Image_Super-Resolution_ICCV_2023_paper.html": {
    "title": "Iterative Soft Shrinkage Learning for Efficient Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiamian Wang",
      "Huan Wang",
      "Yulun Zhang",
      "Yun Fu",
      "Zhiqiang Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cui_Learning_Hierarchical_Features_with_Joint_Latent_Space_Energy-Based_Prior_ICCV_2023_paper.html": {
    "title": "Learning Hierarchical Features with Joint Latent Space Energy-Based Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiali Cui",
      "Ying Nian Wu",
      "Tian Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_UniFormerV2_Unlocking_the_Potential_of_Image_ViTs_for_Video_Understanding_ICCV_2023_paper.html": {
    "title": "UniFormerV2: Unlocking the Potential of Image ViTs for Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunchang Li",
      "Yali Wang",
      "Yinan He",
      "Yizhuo Li",
      "Yi Wang",
      "Limin Wang",
      "Yu Qiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_G2L_Semantically_Aligned_and_Uniform_Video_Grounding_via_Geodesic_and_ICCV_2023_paper.html": {
    "title": "G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and Game Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongxiang Li",
      "Meng Cao",
      "Xuxin Cheng",
      "Yaowei Li",
      "Zhihong Zhu",
      "Yuexian Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_TARGET_Federated_Class-Continual_Learning_via_Exemplar-Free_Distillation_ICCV_2023_paper.html": {
    "title": "TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Zhang",
      "Chen Chen",
      "Weiming Zhuang",
      "Lingjuan Lyu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pal_FashionNTM_Multi-turn_Fashion_Image_Retrieval_via_Cascaded_Memory_ICCV_2023_paper.html": {
    "title": "FashionNTM: Multi-turn Fashion Image Retrieval via Cascaded Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anwesan Pal",
      "Sahil Wadhwa",
      "Ayush Jaiswal",
      "Xu Zhang",
      "Yue Wu",
      "Rakesh Chada",
      "Pradeep Natarajan",
      "Henrik I. Christensen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Morin_MolGrapher_Graph-based_Visual_Recognition_of_Chemical_Structures_ICCV_2023_paper.html": {
    "title": "MolGrapher: Graph-based Visual Recognition of Chemical Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Morin",
      "Martin Danelljan",
      "Maria Isabel Agea",
      "Ahmed Nassar",
      "Valery Weber",
      "Ingmar Meijer",
      "Peter Staar",
      "Fisher Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_SAMPLING_Scene-adaptive_Hierarchical_Multiplane_Images_Representation_for_Novel_View_Synthesis_ICCV_2023_paper.html": {
    "title": "SAMPLING: Scene-adaptive Hierarchical Multiplane Images Representation for Novel View Synthesis from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Zhou",
      "Zhiwei Lin",
      "Xiaojun Shan",
      "Yongtao Wang",
      "Deqing Sun",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Choi_DiffV2S_Diffusion-Based_Video-to-Speech_Synthesis_with_Vision-Guided_Speaker_Embedding_ICCV_2023_paper.html": {
    "title": "DiffV2S: Diffusion-Based Video-to-Speech Synthesis with Vision-Guided Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongsoo Choi",
      "Joanna Hong",
      "Yong Man Ro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_PointOdyssey_A_Large-Scale_Synthetic_Dataset_for_Long-Term_Point_Tracking_ICCV_2023_paper.html": {
    "title": "PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zheng",
      "Adam W. Harley",
      "Bokui Shen",
      "Gordon Wetzstein",
      "Leonidas J. Guibas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Singh_The_Effectiveness_of_MAE_Pre-Pretraining_for_Billion-Scale_Pretraining_ICCV_2023_paper.html": {
    "title": "The Effectiveness of MAE Pre-Pretraining for Billion-Scale Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mannat Singh",
      "Quentin Duval",
      "Kalyan Vasudev Alwala",
      "Haoqi Fan",
      "Vaibhav Aggarwal",
      "Aaron Adcock",
      "Armand Joulin",
      "Piotr Dollar",
      "Christoph Feichtenhofer",
      "Ross Girshick",
      "Rohit Girdhar",
      "Ishan Misra"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Manivasagam_Towards_Zero_Domain_Gap_A_Comprehensive_Study_of_Realistic_LiDAR_ICCV_2023_paper.html": {
    "title": "Towards Zero Domain Gap: A Comprehensive Study of Realistic LiDAR Simulation for Autonomy Testing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sivabalan Manivasagam",
      "Ioan Andrei BÃ¢rsan",
      "Jingkang Wang",
      "Ze Yang",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_GPA-3D_Geometry-aware_Prototype_Alignment_for_Unsupervised_Domain_Adaptive_3D_Object_ICCV_2023_paper.html": {
    "title": "GPA-3D: Geometry-aware Prototype Alignment for Unsupervised Domain Adaptive 3D Object Detection from Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Li",
      "Jingming Guo",
      "Tongtong Cao",
      "Liu Bingbing",
      "Wankou Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pan_TransHuman_A_Transformer-based_Human_Representation_for_Generalizable_Neural_Human_Rendering_ICCV_2023_paper.html": {
    "title": "TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Pan",
      "Zongxin Yang",
      "Jianxin Ma",
      "Chang Zhou",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shao_LNPL-MIL_Learning_from_Noisy_Pseudo_Labels_for_Promoting_Multiple_Instance_ICCV_2023_paper.html": {
    "title": "LNPL-MIL: Learning from Noisy Pseudo Labels for Promoting Multiple Instance Learning in Whole Slide Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuchen Shao",
      "Yifeng Wang",
      "Yang Chen",
      "Hao Bian",
      "Shaohui Liu",
      "Haoqian Wang",
      "Yongbing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Few-Shot_Dataset_Distillation_via_Translative_Pre-Training_ICCV_2023_paper.html": {
    "title": "Few-Shot Dataset Distillation via Translative Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songhua Liu",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pan_Random_Sub-Samples_Generation_for_Self-Supervised_Real_Image_Denoising_ICCV_2023_paper.html": {
    "title": "Random Sub-Samples Generation for Self-Supervised Real Image Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhong Pan",
      "Xiao Liu",
      "Xiangyu Liao",
      "Yuanzhouhan Cao",
      "Chao Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Roth_Waffling_Around_for_Performance_Visual_Classification_with_Random_Words_and_ICCV_2023_paper.html": {
    "title": "Waffling Around for Performance: Visual Classification with Random Words and Broad Concepts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karsten Roth",
      "Jae Myung Kim",
      "A. Sophia Koepke",
      "Oriol Vinyals",
      "Cordelia Schmid",
      "Zeynep Akata"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Unsupervised_Surface_Anomaly_Detection_with_Diffusion_Probabilistic_Model_ICCV_2023_paper.html": {
    "title": "Unsupervised Surface Anomaly Detection with Diffusion Probabilistic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Zhang",
      "Naiqi Li",
      "Jiawei Li",
      "Tao Dai",
      "Yong Jiang",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_AutoAD_II_The_Sequel_-_Who_When_and_What_in_ICCV_2023_paper.html": {
    "title": "AutoAD II: The Sequel - Who, When, and What in Movie Audio Description",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tengda Han",
      "Max Bain",
      "Arsha Nagrani",
      "Gul Varol",
      "Weidi Xie",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_TinyCLIP_CLIP_Distillation_via_Affinity_Mimicking_and_Weight_Inheritance_ICCV_2023_paper.html": {
    "title": "TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kan Wu",
      "Houwen Peng",
      "Zhenghong Zhou",
      "Bin Xiao",
      "Mengchen Liu",
      "Lu Yuan",
      "Hong Xuan",
      "Michael Valenzuela",
      "Xi (Stephen) Chen",
      "Xinggang Wang",
      "Hongyang Chao",
      "Han Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Hyperbolic_Chamfer_Distance_for_Point_Cloud_Completion_ICCV_2023_paper.html": {
    "title": "Hyperbolic Chamfer Distance for Point Cloud Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangzhou Lin",
      "Yun Yue",
      "Songlin Hou",
      "Xuechu Yu",
      "Yajun Xu",
      "Kazunori D Yamada",
      "Ziming Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chowdhury_Democratising_2D_Sketch_to_3D_Shape_Retrieval_Through_Pivoting_ICCV_2023_paper.html": {
    "title": "Democratising 2D Sketch to 3D Shape Retrieval Through Pivoting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pinaki Nath Chowdhury",
      "Ayan Kumar Bhunia",
      "Aneeshan Sain",
      "Subhadeep Koley",
      "Tao Xiang",
      "Yi-Zhe Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Simoun_Synergizing_Interactive_Motion-appearance_Understanding_for_Vision-based_Reinforcement_Learning_ICCV_2023_paper.html": {
    "title": "Simoun: Synergizing Interactive Motion-appearance Understanding for Vision-based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangru Huang",
      "Peixi Peng",
      "Yifan Zhao",
      "Yunpeng Zhai",
      "Haoran Xu",
      "Yonghong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_AG3D_Learning_to_Generate_3D_Avatars_from_2D_Image_Collections_ICCV_2023_paper.html": {
    "title": "AG3D: Learning to Generate 3D Avatars from 2D Image Collections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Dong",
      "Xu Chen",
      "Jinlong Yang",
      "Michael J. Black",
      "Otmar Hilliges",
      "Andreas Geiger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Luo_KECOR_Kernel_Coding_Rate_Maximization_for_Active_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yadan Luo",
      "Zhuoxiao Chen",
      "Zhen Fang",
      "Zheng Zhang",
      "Mahsa Baktashmotlagh",
      "Zi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Learned_Image_Reasoning_Prior_Penetrates_Deep_Unfolding_Network_for_Panchromatic_ICCV_2023_paper.html": {
    "title": "Learned Image Reasoning Prior Penetrates Deep Unfolding Network for Panchromatic and Multi-spectral Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Man Zhou",
      "Jie Huang",
      "Naishan Zheng",
      "Chongyi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Representation_Disparity-aware_Distillation_for_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "Representation Disparity-aware Distillation for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanjing Li",
      "Sheng Xu",
      "Mingbao Lin",
      "Jihao Yin",
      "Baochang Zhang",
      "Xianbin Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_NCHO_Unsupervised_Learning_for_Neural_3D_Composition_of_Humans_and_ICCV_2023_paper.html": {
    "title": "NCHO: Unsupervised Learning for Neural 3D Composition of Humans and Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taeksoo Kim",
      "Shunsuke Saito",
      "Hanbyul Joo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qian_Breaking_The_Limits_of_Text-conditioned_3D_Motion_Synthesis_with_Elaborative_ICCV_2023_paper.html": {
    "title": "Breaking The Limits of Text-conditioned 3D Motion Synthesis with Elaborative Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijun Qian",
      "Jack Urbanek",
      "Alexander G. Hauptmann",
      "Jungdam Won"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_VL-PET_Vision-and-Language_Parameter-Efficient_Tuning_via_Granularity_Control_ICCV_2023_paper.html": {
    "title": "VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi-Yuan Hu",
      "Yanyang Li",
      "Michael R. Lyu",
      "Liwei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_ROME_Robustifying_Memory-Efficient_NAS_via_Topology_Disentanglement_and_Gradient_Accumulation_ICCV_2023_paper.html": {
    "title": "ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxing Wang",
      "Xiangxiang Chu",
      "Yuda Fan",
      "Zhexi Zhang",
      "Bo Zhang",
      "Xiaokang Yang",
      "Junchi Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Toward_Multi-Granularity_Decision-Making_Explicit_Visual_Reasoning_with_Hierarchical_Knowledge_ICCV_2023_paper.html": {
    "title": "Toward Multi-Granularity Decision-Making: Explicit Visual Reasoning with Hierarchical Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifeng Zhang",
      "Shi Chen",
      "Qi Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xiang_3D-aware_Image_Generation_using_2D_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "3D-aware Image Generation using 2D Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianfeng Xiang",
      "Jiaolong Yang",
      "Binbin Huang",
      "Xin Tong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fang_Locating_Noise_is_Halfway_Denoising_for_Semi-Supervised_Segmentation_ICCV_2023_paper.html": {
    "title": "Locating Noise is Halfway Denoising for Semi-Supervised Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Fang",
      "Feng Zhu",
      "Bowen Cheng",
      "Luoqi Liu",
      "Yao Zhao",
      "Yunchao Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liang_Learning_Non-Local_Spatial-Angular_Correlation_for_Light_Field_Image_Super-Resolution_ICCV_2023_paper.html": {
    "title": "Learning Non-Local Spatial-Angular Correlation for Light Field Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyu Liang",
      "Yingqian Wang",
      "Longguang Wang",
      "Jungang Yang",
      "Shilin Zhou",
      "Yulan Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_ICE-NeRF_Interactive_Color_Editing_of_NeRFs_via_Decomposition-Aware_Weight_Optimization_ICCV_2023_paper.html": {
    "title": "ICE-NeRF: Interactive Color Editing of NeRFs via Decomposition-Aware Weight Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae-Hyeok Lee",
      "Dae-Shik Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yun_SPANet_Frequency-balancing_Token_Mixer_using_Spectral_Pooling_Aggregation_Modulation_ICCV_2023_paper.html": {
    "title": "SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guhnoo Yun",
      "Juhan Yoo",
      "Kijung Kim",
      "Jeongho Lee",
      "Dong Hwan Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fu_ASAG_Building_Strong_One-Decoder-Layer_Sparse_Detectors_via_Adaptive_Sparse_Anchor_ICCV_2023_paper.html": {
    "title": "ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive Sparse Anchor Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghao Fu",
      "Junkai Yan",
      "Yipeng Gao",
      "Xiaohua Xie",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_MGMAE_Motion_Guided_Masking_for_Video_Masked_Autoencoding_ICCV_2023_paper.html": {
    "title": "MGMAE: Motion Guided Masking for Video Masked Autoencoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingkun Huang",
      "Zhiyu Zhao",
      "Guozhen Zhang",
      "Yu Qiao",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shejwalkar_The_Perils_of_Learning_From_Unlabeled_Data_Backdoor_Attacks_on_ICCV_2023_paper.html": {
    "title": "The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Virat Shejwalkar",
      "Lingjuan Lyu",
      "Amir Houmansadr"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fan_SSB_Simple_but_Strong_Baseline_for_Boosting_Performance_of_Open-Set_ICCV_2023_paper.html": {
    "title": "SSB: Simple but Strong Baseline for Boosting Performance of Open-Set Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Fan",
      "Anna Kukleva",
      "Dengxin Dai",
      "Bernt Schiele"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_StyleDiffusion_Controllable_Disentangled_Style_Transfer_via_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhizhong Wang",
      "Lei Zhao",
      "Wei Xing"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_AdvDiffuser_Natural_Adversarial_Example_Synthesis_with_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "AdvDiffuser: Natural Adversarial Example Synthesis with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinquan Chen",
      "Xitong Gao",
      "Juanjuan Zhao",
      "Kejiang Ye",
      "Cheng-Zhong Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_ViewRefer_Grasp_the_Multi-view_Knowledge_for_3D_Visual_Grounding_ICCV_2023_paper.html": {
    "title": "ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zoey Guo",
      "Yiwen Tang",
      "Ray Zhang",
      "Dong Wang",
      "Zhigang Wang",
      "Bin Zhao",
      "Xuelong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Su_CaPhy_Capturing_Physical_Properties_for_Animatable_Human_Avatars_ICCV_2023_paper.html": {
    "title": "CaPhy: Capturing Physical Properties for Animatable Human Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoqi Su",
      "Liangxiao Hu",
      "Siyou Lin",
      "Hongwen Zhang",
      "Shengping Zhang",
      "Justus Thies",
      "Yebin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Athwale_DarSwin_Distortion_Aware_Radial_Swin_Transformer_ICCV_2023_paper.html": {
    "title": "DarSwin: Distortion Aware Radial Swin Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshaya Athwale",
      "Arman Afrasiyabi",
      "Justin LagÃ¼e",
      "Ichrak Shili",
      "Ola Ahmad",
      "Jean-FranÃ§ois Lalonde"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Fine-grained_Unsupervised_Domain_Adaptation_for_Gait_Recognition_ICCV_2023_paper.html": {
    "title": "Fine-grained Unsupervised Domain Adaptation for Gait Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang Ma",
      "Ying Fu",
      "Dezhi Zheng",
      "Yunjie Peng",
      "Chunshui Cao",
      "Yongzhen Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Cross-Modal_Orthogonal_High-Rank_Augmentation_for_RGB-Event_Transformer-Trackers_ICCV_2023_paper.html": {
    "title": "Cross-Modal Orthogonal High-Rank Augmentation for RGB-Event Transformer-Trackers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Zhu",
      "Junhui Hou",
      "Dapeng Oliver Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Take-A-Photo_3D-to-2D_Generative_Pre-training_of_Point_Cloud_Models_ICCV_2023_paper.html": {
    "title": "Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Wang",
      "Xumin Yu",
      "Yongming Rao",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Open-vocabulary_Panoptic_Segmentation_with_Embedding_Modulation_ICCV_2023_paper.html": {
    "title": "Open-vocabulary Panoptic Segmentation with Embedding Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Chen",
      "Shuang Li",
      "Ser-Nam Lim",
      "Antonio Torralba",
      "Hengshuang Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jeon_Beyond_Single_Path_Integrated_Gradients_for_Reliable_Input_Attribution_via_ICCV_2023_paper.html": {
    "title": "Beyond Single Path Integrated Gradients for Reliable Input Attribution via Randomized Path Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giyoung Jeon",
      "Haedong Jeong",
      "Jaesik Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zha_Instance-aware_Dynamic_Prompt_Tuning_for_Pre-trained_Point_Cloud_Models_ICCV_2023_paper.html": {
    "title": "Instance-aware Dynamic Prompt Tuning for Pre-trained Point Cloud Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaohua Zha",
      "Jinpeng Wang",
      "Tao Dai",
      "Bin Chen",
      "Zhi Wang",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sevastopolskiy_How_to_Boost_Face_Recognition_with_StyleGAN_ICCV_2023_paper.html": {
    "title": "How to Boost Face Recognition with StyleGAN?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artem Sevastopolskiy",
      "Yury Malkov",
      "Nikita Durasov",
      "Luisa Verdoliva",
      "Matthias NieÃner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Text2Tex_Text-driven_Texture_Synthesis_via_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Text2Tex: Text-driven Texture Synthesis via Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dave Zhenyu Chen",
      "Yawar Siddiqui",
      "Hsin-Ying Lee",
      "Sergey Tulyakov",
      "Matthias NieÃner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_MUVA_A_New_Large-Scale_Benchmark_for_Multi-View_Amodal_Instance_Segmentation_ICCV_2023_paper.html": {
    "title": "MUVA: A New Large-Scale Benchmark for Multi-View Amodal Instance Segmentation in the Shopping Scenario",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixuan Li",
      "Weining Ye",
      "Juan Terven",
      "Zachary Bennett",
      "Ying Zheng",
      "Tingting Jiang",
      "Tiejun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dombrowski_Foreground-Background_Separation_through_Concept_Distillation_from_Generative_Image_Foundation_Models_ICCV_2023_paper.html": {
    "title": "Foreground-Background Separation through Concept Distillation from Generative Image Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mischa Dombrowski",
      "Hadrien Reynaud",
      "Matthew Baugh",
      "Bernhard Kainz"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liang_ENVIDR_Implicit_Differentiable_Renderer_with_Neural_Environment_Lighting_ICCV_2023_paper.html": {
    "title": "ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruofan Liang",
      "Huiting Chen",
      "Chunlin Li",
      "Fan Chen",
      "Selvakumar Panneer",
      "Nandita Vijaykumar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Not_All_Steps_are_Created_Equal_Selective_Diffusion_Distillation_for_ICCV_2023_paper.html": {
    "title": "Not All Steps are Created Equal: Selective Diffusion Distillation for Image Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luozhou Wang",
      "Shuai Yang",
      "Shu Liu",
      "Ying-cong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Park_SeiT_Storage-Efficient_Vision_Training_with_Tokens_Using_1_of_Pixel_ICCV_2023_paper.html": {
    "title": "SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel Storage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Park",
      "Sanghyuk Chun",
      "Byeongho Heo",
      "Wonjae Kim",
      "Sangdoo Yun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_ALIP_Adaptive_Language-Image_Pre-Training_with_Synthetic_Caption_ICCV_2023_paper.html": {
    "title": "ALIP: Adaptive Language-Image Pre-Training with Synthetic Caption",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaicheng Yang",
      "Jiankang Deng",
      "Xiang An",
      "Jiawei Li",
      "Ziyong Feng",
      "Jia Guo",
      "Jing Yang",
      "Tongliang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ren_GeoUDF_Surface_Reconstruction_from_3D_Point_Clouds_via_Geometry-guided_Distance_ICCV_2023_paper.html": {
    "title": "GeoUDF: Surface Reconstruction from 3D Point Clouds via Geometry-guided Distance Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyu Ren",
      "Junhui Hou",
      "Xiaodong Chen",
      "Ying He",
      "Wenping Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_LaPE_Layer-adaptive_Position_Embedding_for_Vision_Transformers_with_Independent_Layer_ICCV_2023_paper.html": {
    "title": "LaPE: Layer-adaptive Position Embedding for Vision Transformers with Independent Layer Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runyi Yu",
      "Zhennan Wang",
      "Yinhuai Wang",
      "Kehan Li",
      "Chang Liu",
      "Haoyi Duan",
      "Xiangyang Ji",
      "Jie Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_CLIP2Point_Transfer_CLIP_to_Point_Cloud_Classification_with_Image-Depth_Pre-Training_ICCV_2023_paper.html": {
    "title": "CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Huang",
      "Bowen Dong",
      "Yunhan Yang",
      "Xiaoshui Huang",
      "Rynson W.H. Lau",
      "Wanli Ouyang",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wen_Parametric_Classification_for_Generalized_Category_Discovery_A_Baseline_Study_ICCV_2023_paper.html": {
    "title": "Parametric Classification for Generalized Category Discovery: A Baseline Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Wen",
      "Bingchen Zhao",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_MeMOTR_Long-Term_Memory-Augmented_Transformer_for_Multi-Object_Tracking_ICCV_2023_paper.html": {
    "title": "MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruopeng Gao",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zou_RawHDR_High_Dynamic_Range_Image_Reconstruction_from_a_Single_Raw_ICCV_2023_paper.html": {
    "title": "RawHDR: High Dynamic Range Image Reconstruction from a Single Raw Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Zou",
      "Chenggang Yan",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xiang_Denoising_Diffusion_Autoencoders_are_Unified_Self-supervised_Learners_ICCV_2023_paper.html": {
    "title": "Denoising Diffusion Autoencoders are Unified Self-supervised Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weilai Xiang",
      "Hongyu Yang",
      "Di Huang",
      "Yunhong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cai_Robust_Object_Modeling_for_Visual_Tracking_ICCV_2023_paper.html": {
    "title": "Robust Object Modeling for Visual Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yidong Cai",
      "Jie Liu",
      "Jie Tang",
      "Gangshan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_FSI_Frequency_and_Spatial_Interactive_Learning_for_Image_Restoration_in_ICCV_2023_paper.html": {
    "title": "FSI: Frequency and Spatial Interactive Learning for Image Restoration in Under-Display Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengxu Liu",
      "Xuan Wang",
      "Shuai Li",
      "Yuzhi Wang",
      "Xueming Qian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_Cross-view_Topology_Based_Consistent_and_Complementary_Information_for_Deep_Multi-view_ICCV_2023_paper.html": {
    "title": "Cross-view Topology Based Consistent and Complementary Information for Deep Multi-view Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhibin Dong",
      "Siwei Wang",
      "Jiaqi Jin",
      "Xinwang Liu",
      "En Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Distribution-Consistent_Modal_Recovering_for_Incomplete_Multimodal_Learning_ICCV_2023_paper.html": {
    "title": "Distribution-Consistent Modal Recovering for Incomplete Multimodal Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanzhi Wang",
      "Zhen Cui",
      "Yong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_ContactGen_Generative_Contact_Modeling_for_Grasp_Generation_ICCV_2023_paper.html": {
    "title": "ContactGen: Generative Contact Modeling for Grasp Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaowei Liu",
      "Yang Zhou",
      "Jimei Yang",
      "Saurabh Gupta",
      "Shenlong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Temporal_Collection_and_Distribution_for_Referring_Video_Object_Segmentation_ICCV_2023_paper.html": {
    "title": "Temporal Collection and Distribution for Referring Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajin Tang",
      "Ge Zheng",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_SA-BEV_Generating_Semantic-Aware_Birds-Eye-View_Feature_for_Multi-view_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "SA-BEV: Generating Semantic-Aware Bird's-Eye-View Feature for Multi-view 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinqing Zhang",
      "Yanan Zhang",
      "Qingjie Liu",
      "Yunhong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Variational_Degeneration_to_Structural_Refinement_A_Unified_Framework_for_Superimposed_ICCV_2023_paper.html": {
    "title": "Variational Degeneration to Structural Refinement: A Unified Framework for Superimposed Image Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyu Li",
      "Yan Xu",
      "Yang Yang",
      "Haoran Ji",
      "Yue Lang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_Global_Knowledge_Calibration_for_Fast_Open-Vocabulary_Segmentation_ICCV_2023_paper.html": {
    "title": "Global Knowledge Calibration for Fast Open-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunyang Han",
      "Yong Liu",
      "Jun Hao Liew",
      "Henghui Ding",
      "Jiajun Liu",
      "Yitong Wang",
      "Yansong Tang",
      "Yujiu Yang",
      "Jiashi Feng",
      "Yao Zhao",
      "Yunchao Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Khirodkar_Ego-Humans_An_Ego-Centric_3D_Multi-Human_Benchmark_ICCV_2023_paper.html": {
    "title": "Ego-Humans: An Ego-Centric 3D Multi-Human Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rawal Khirodkar",
      "Aayush Bansal",
      "Lingni Ma",
      "Richard Newcombe",
      "Minh Vo",
      "Kris Kitani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cui_Focal_Network_for_Image_Restoration_ICCV_2023_paper.html": {
    "title": "Focal Network for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuning Cui",
      "Wenqi Ren",
      "Xiaochun Cao",
      "Alois Knoll"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dai_Indoor_Depth_Recovery_Based_on_Deep_Unfolding_with_Non-Local_Prior_ICCV_2023_paper.html": {
    "title": "Indoor Depth Recovery Based on Deep Unfolding with Non-Local Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Dai",
      "Junkang Zhang",
      "Faming Fang",
      "Guixu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bratelund_Compatibility_of_Fundamental_Matrices_for_Complete_Viewing_Graphs_ICCV_2023_paper.html": {
    "title": "Compatibility of Fundamental Matrices for Complete Viewing Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin BrÃ¥telund",
      "Felix Rydell"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Luo_GAFlow_Incorporating_Gaussian_Attention_into_Optical_Flow_ICCV_2023_paper.html": {
    "title": "GAFlow: Incorporating Gaussian Attention into Optical Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ao Luo",
      "Fan Yang",
      "Xin Li",
      "Lang Nie",
      "Chunyu Lin",
      "Haoqiang Fan",
      "Shuaicheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_MAtch_eXpand_and_Improve_Unsupervised_Finetuning_for_Zero-Shot_Action_Recognition_ICCV_2023_paper.html": {
    "title": "MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Lin",
      "Leonid Karlinsky",
      "Nina Shvetsova",
      "Horst Possegger",
      "Mateusz Kozinski",
      "Rameswar Panda",
      "Rogerio Feris",
      "Hilde Kuehne",
      "Horst Bischof"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Space_Engage_Collaborative_Space_Supervision_for_Contrastive-Based_Semi-Supervised_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Space Engage: Collaborative Space Supervision for Contrastive-Based Semi-Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changqi Wang",
      "Haoyu Xie",
      "Yuhui Yuan",
      "Chong Fu",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Delving_into_Motion-Aware_Matching_for_Monocular_3D_Object_Tracking_ICCV_2023_paper.html": {
    "title": "Delving into Motion-Aware Matching for Monocular 3D Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuan-Chih Huang",
      "Ming-Hsuan Yang",
      "Yi-Hsuan Tsai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sundar_SoDaCam_Software-defined_Cameras_via_Single-Photon_Imaging_ICCV_2023_paper.html": {
    "title": "SoDaCam: Software-defined Cameras via Single-Photon Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Varun Sundar",
      "Andrei Ardelean",
      "Tristan Swedish",
      "Claudio Bruschini",
      "Edoardo Charbon",
      "Mohit Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mirzaei_Reference-guided_Controllable_Inpainting_of_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "Reference-guided Controllable Inpainting of Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashkan Mirzaei",
      "Tristan Aumentado-Armstrong",
      "Marcus A. Brubaker",
      "Jonathan Kelly",
      "Alex Levinshtein",
      "Konstantinos G. Derpanis",
      "Igor Gilitschenski"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Diffusion-Guided_Reconstruction_of_Everyday_Hand-Object_Interaction_Clips_ICCV_2023_paper.html": {
    "title": "Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Ye",
      "Poorvi Hebbar",
      "Abhinav Gupta",
      "Shubham Tulsiani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ren_Decoupled_Iterative_Refinement_Framework_for_Interacting_Hands_Reconstruction_from_a_ICCV_2023_paper.html": {
    "title": "Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengfei Ren",
      "Chao Wen",
      "Xiaozheng Zheng",
      "Zhou Xue",
      "Haifeng Sun",
      "Qi Qi",
      "Jingyu Wang",
      "Jianxin Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Fast_Adversarial_Training_with_Smooth_Convergence_ICCV_2023_paper.html": {
    "title": "Fast Adversarial Training with Smooth Convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengnan Zhao",
      "Lihe Zhang",
      "Yuqiu Kong",
      "Baocai Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Goel_Who_Are_You_Referring_To_Coreference_Resolution_In_Image_Narrations_ICCV_2023_paper.html": {
    "title": "Who Are You Referring To? Coreference Resolution In Image Narrations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arushi Goel",
      "Basura Fernando",
      "Frank Keller",
      "Hakan Bilen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_DVGaze_Dual-View_Gaze_Estimation_ICCV_2023_paper.html": {
    "title": "DVGaze: Dual-View Gaze Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihua Cheng",
      "Feng Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Leng_Dynamic_Hyperbolic_Attention_Network_for_Fine_Hand-object_Reconstruction_ICCV_2023_paper.html": {
    "title": "Dynamic Hyperbolic Attention Network for Fine Hand-object Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiying Leng",
      "Shun-Cheng Wu",
      "Mahdi Saleh",
      "Antonio Montanaro",
      "Hao Yu",
      "Yin Wang",
      "Nassir Navab",
      "Xiaohui Liang",
      "Federico Tombari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Agarwal_A-STAR_Test-time_Attention_Segregation_and_Retention_for_Text-to-image_Synthesis_ICCV_2023_paper.html": {
    "title": "A-STAR: Test-time Attention Segregation and Retention for Text-to-image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aishwarya Agarwal",
      "Srikrishna Karanam",
      "K J Joseph",
      "Apoorv Saxena",
      "Koustava Goswami",
      "Balaji Vasan Srinivasan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Stier_LivePose_Online_3D_Reconstruction_from_Monocular_Video_with_Dynamic_Camera_ICCV_2023_paper.html": {
    "title": "LivePose: Online 3D Reconstruction from Monocular Video with Dynamic Camera Poses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noah Stier",
      "Baptiste Angles",
      "Liang Yang",
      "Yajie Yan",
      "Alex Colburn",
      "Ming Chuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Efficient_Joint_Optimization_of_Layer-Adaptive_Weight_Pruning_in_Deep_Neural_ICCV_2023_paper.html": {
    "title": "Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixin Xu",
      "Zhe Wang",
      "Xue Geng",
      "Min Wu",
      "Xiaoli Li",
      "Weisi Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Feature_Modulation_Transformer_Cross-Refinement_of_Global_Representation_via_High-Frequency_Prior_ICCV_2023_paper.html": {
    "title": "Feature Modulation Transformer: Cross-Refinement of Global Representation via High-Frequency Prior for Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ao Li",
      "Le Zhang",
      "Yun Liu",
      "Ce Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sudhakar_Exploring_the_Sim2Real_Gap_Using_Digital_Twins_ICCV_2023_paper.html": {
    "title": "Exploring the Sim2Real Gap Using Digital Twins",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sruthi Sudhakar",
      "Jon Hanzelka",
      "Josh Bobillot",
      "Tanmay Randhavane",
      "Neel Joshi",
      "Vibhav Vineet"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liang_MPI-Flow_Learning_Realistic_Optical_Flow_with_Multiplane_Images_ICCV_2023_paper.html": {
    "title": "MPI-Flow: Learning Realistic Optical Flow with Multiplane Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingping Liang",
      "Jiaming Liu",
      "Debing Zhang",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zorzi_RePolyWorld_-_A_Graph_Neural_Network_for_Polygonal_Scene_Parsing_ICCV_2023_paper.html": {
    "title": "Re:PolyWorld - A Graph Neural Network for Polygonal Scene Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Zorzi",
      "Friedrich Fraundorfer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hwang_FaceCLIPNeRF_Text-driven_3D_Face_Manipulation_using_Deformable_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungwon Hwang",
      "Junha Hyung",
      "Daejin Kim",
      "Min-Jung Kim",
      "Jaegul Choo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Video_State-Changing_Object_Segmentation_ICCV_2023_paper.html": {
    "title": "Video State-Changing Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangwei Yu",
      "Xiang Li",
      "Xinran Zhao",
      "Hongming Zhang",
      "Yu-Xiong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Learning_Shape_Primitives_via_Implicit_Convexity_Regularization_ICCV_2023_paper.html": {
    "title": "Learning Shape Primitives via Implicit Convexity Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyang Huang",
      "Yi Zhang",
      "Kai Chen",
      "Teng Li",
      "Wenjun Zhang",
      "Bingbing Ni"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tian_MonoNeRF_Learning_a_Generalizable_Dynamic_Radiance_Field_from_Monocular_Videos_ICCV_2023_paper.html": {
    "title": "MonoNeRF: Learning a Generalizable Dynamic Radiance Field from Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengrui Tian",
      "Shaoyi Du",
      "Yueqi Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Koo_PG-RCNN_Semantic_Surface_Point_Generation_for_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "PG-RCNN: Semantic Surface Point Generation for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inyong Koo",
      "Inyoung Lee",
      "Se-Ho Kim",
      "Hee-Seon Kim",
      "Woo-jin Jeon",
      "Changick Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_ITI-GEN_Inclusive_Text-to-Image_Generation_ICCV_2023_paper.html": {
    "title": "ITI-GEN: Inclusive Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Zhang",
      "Xuanbai Chen",
      "Siqi Chai",
      "Chen Henry Wu",
      "Dmitry Lagun",
      "Thabo Beeler",
      "Fernando De la Torre"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Costanzino_Learning_Depth_Estimation_for_Transparent_and_Mirror_Surfaces_ICCV_2023_paper.html": {
    "title": "Learning Depth Estimation for Transparent and Mirror Surfaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Costanzino",
      "Pierluigi Zama Ramirez",
      "Matteo Poggi",
      "Fabio Tosi",
      "Stefano Mattoccia",
      "Luigi Di Stefano"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Deng_Learning_Neural_Eigenfunctions_for_Unsupervised_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Learning Neural Eigenfunctions for Unsupervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijie Deng",
      "Yucen Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chassat_Shape_Analysis_of_Euclidean_Curves_under_Frenet-Serret_Framework_ICCV_2023_paper.html": {
    "title": "Shape Analysis of Euclidean Curves under Frenet-Serret Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Perrine Chassat",
      "Juhyun Park",
      "Nicolas Brunel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nakamura_Representation_Uncertainty_in_Self-Supervised_Learning_as_Variational_Inference_ICCV_2023_paper.html": {
    "title": "Representation Uncertainty in Self-Supervised Learning as Variational Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroki Nakamura",
      "Masashi Okada",
      "Tadahiro Taniguchi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hang_Efficient_Diffusion_Training_via_Min-SNR_Weighting_Strategy_ICCV_2023_paper.html": {
    "title": "Efficient Diffusion Training via Min-SNR Weighting Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiankai Hang",
      "Shuyang Gu",
      "Chen Li",
      "Jianmin Bao",
      "Dong Chen",
      "Han Hu",
      "Xin Geng",
      "Baining Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Bridging_Vision_and_Language_Encoders_Parameter-Efficient_Tuning_for_Referring_Image_ICCV_2023_paper.html": {
    "title": "Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zunnan Xu",
      "Zhihong Chen",
      "Yong Zhang",
      "Yibing Song",
      "Xiang Wan",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guizilini_Towards_Zero-Shot_Scale-Aware_Monocular_Depth_Estimation_ICCV_2023_paper.html": {
    "title": "Towards Zero-Shot Scale-Aware Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vitor Guizilini",
      "Igor Vasiljevic",
      "Dian Chen",
      "RareÈ AmbruÈ",
      "Adrien Gaidon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lorraine_ATT3D_Amortized_Text-to-3D_Object_Synthesis_ICCV_2023_paper.html": {
    "title": "ATT3D: Amortized Text-to-3D Object Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Lorraine",
      "Kevin Xie",
      "Xiaohui Zeng",
      "Chen-Hsuan Lin",
      "Towaki Takikawa",
      "Nicholas Sharp",
      "Tsung-Yi Lin",
      "Ming-Yu Liu",
      "Sanja Fidler",
      "James Lucas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Virtual_Try-On_with_Pose-Garment_Keypoints_Guided_Inpainting_ICCV_2023_paper.html": {
    "title": "Virtual Try-On with Pose-Garment Keypoints Guided Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Li",
      "Pengfei Wei",
      "Xiang Yin",
      "Zejun Ma",
      "Alex C. Kot"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shvetsova_Learning_by_Sorting_Self-supervised_Learning_with_Group_Ordering_Constraints_ICCV_2023_paper.html": {
    "title": "Learning by Sorting: Self-supervised Learning with Group Ordering Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina Shvetsova",
      "Felix Petersen",
      "Anna Kukleva",
      "Bernt Schiele",
      "Hilde Kuehne"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yan_Cross_Modal_Transformer_Towards_Fast_and_Robust_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "Cross Modal Transformer: Towards Fast and Robust 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Yan",
      "Yingfei Liu",
      "Jianjian Sun",
      "Fan Jia",
      "Shuailin Li",
      "Tiancai Wang",
      "Xiangyu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ranasinghe_Perceptual_Grouping_in_Contrastive_Vision-Language_Models_ICCV_2023_paper.html": {
    "title": "Perceptual Grouping in Contrastive Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kanchana Ranasinghe",
      "Brandon McKinzie",
      "Sachin Ravi",
      "Yinfei Yang",
      "Alexander Toshev",
      "Jonathon Shlens"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_Dynamic_Perceiver_for_Efficient_Visual_Recognition_ICCV_2023_paper.html": {
    "title": "Dynamic Perceiver for Efficient Visual Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizeng Han",
      "Dongchen Han",
      "Zeyu Liu",
      "Yulin Wang",
      "Xuran Pan",
      "Yifan Pu",
      "Chao Deng",
      "Junlan Feng",
      "Shiji Song",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_MoTIF_Learning_Motion_Trajectories_with_Local_Implicit_Neural_Functions_for_ICCV_2023_paper.html": {
    "title": "MoTIF: Learning Motion Trajectories with Local Implicit Neural Functions for Continuous Space-Time Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Hsin Chen",
      "Si-Cun Chen",
      "Yi-Hsin Chen",
      "Yen-Yu Lin",
      "Wen-Hsiao Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_CRN_Camera_Radar_Net_for_Accurate_Robust_Efficient_3D_Perception_ICCV_2023_paper.html": {
    "title": "CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngseok Kim",
      "Juyeb Shin",
      "Sanmin Kim",
      "In-Jae Lee",
      "Jun Won Choi",
      "Dongsuk Kum"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cho_PromptStyler_Prompt-driven_Style_Generation_for_Source-free_Domain_Generalization_ICCV_2023_paper.html": {
    "title": "PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhyeong Cho",
      "Gilhyun Nam",
      "Sungyeon Kim",
      "Hunmin Yang",
      "Suha Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.html": {
    "title": "Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teng Hu",
      "Jiangning Zhang",
      "Liang Liu",
      "Ran Yi",
      "Siqi Kou",
      "Haokun Zhu",
      "Xu Chen",
      "Yabiao Wang",
      "Chengjie Wang",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_SVQNet_Sparse_Voxel-Adjacent_Query_Network_for_4D_Spatio-Temporal_LiDAR_Semantic_ICCV_2023_paper.html": {
    "title": "SVQNet: Sparse Voxel-Adjacent Query Network for 4D Spatio-Temporal LiDAR Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuechao Chen",
      "Shuangjie Xu",
      "Xiaoyi Zou",
      "Tongyi Cao",
      "Dit-Yan Yeung",
      "Lu Fang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_HAL3D_Hierarchical_Active_Learning_for_Fine-Grained_3D_Part_Labeling_ICCV_2023_paper.html": {
    "title": "HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fenggen Yu",
      "Yiming Qian",
      "Francisca Gil-Ureta",
      "Brian Jackson",
      "Eric Bennett",
      "Hao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_MEFLUT_Unsupervised_1D_Lookup_Tables_for_Multi-exposure_Image_Fusion_ICCV_2023_paper.html": {
    "title": "MEFLUT: Unsupervised 1D Lookup Tables for Multi-exposure Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Jiang",
      "Chuan Wang",
      "Xinpeng Li",
      "Ru Li",
      "Haoqiang Fan",
      "Shuaicheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_FedPerfix_Towards_Partial_Model_Personalization_of_Vision_Transformers_in_Federated_ICCV_2023_paper.html": {
    "title": "FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyu Sun",
      "Matias Mendieta",
      "Jun Luo",
      "Shandong Wu",
      "Chen Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shum_Conditional_360-degree_Image_Synthesis_for_Immersive_Indoor_Scene_Decoration_ICCV_2023_paper.html": {
    "title": "Conditional 360-degree Image Synthesis for Immersive Indoor Scene Decoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ka Chun Shum",
      "Hong-Wing Pang",
      "Binh-Son Hua",
      "Duc Thanh Nguyen",
      "Sai-Kit Yeung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zara_The_Unreasonable_Effectiveness_of_Large_Language-Vision_Models_for_Source-Free_Video_ICCV_2023_paper.html": {
    "title": "The Unreasonable Effectiveness of Large Language-Vision Models for Source-Free Video Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giacomo Zara",
      "Alessandro Conti",
      "Subhankar Roy",
      "StÃ©phane LathuiliÃ¨re",
      "Paolo Rota",
      "Elisa Ricci"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Muaz_SIDGAN_High-Resolution_Dubbed_Video_Generation_via_Shift-Invariant_Learning_ICCV_2023_paper.html": {
    "title": "SIDGAN: High-Resolution Dubbed Video Generation via Shift-Invariant Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Urwa Muaz",
      "Wondong Jang",
      "Rohun Tripathi",
      "Santhosh Mani",
      "Wenbin Ouyang",
      "Ravi Teja Gadde",
      "Baris Gecer",
      "Sergio Elizondo",
      "Reza Madad",
      "Naveen Nair"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Meta-ZSDETR_Zero-shot_DETR_with_Meta-learning_ICCV_2023_paper.html": {
    "title": "Meta-ZSDETR: Zero-shot DETR with Meta-learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Zhang",
      "Chenbo Zhang",
      "Jiajia Zhao",
      "Jihong Guan",
      "Shuigeng Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ngo_GaPro_Box-Supervised_3D_Point_Cloud_Instance_Segmentation_Using_Gaussian_Processes_ICCV_2023_paper.html": {
    "title": "GaPro: Box-Supervised 3D Point Cloud Instance Segmentation Using Gaussian Processes as Pseudo Labelers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuan Duc Ngo",
      "Binh-Son Hua",
      "Khoi Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_STPrivacy_Spatio-Temporal_Privacy-Preserving_Action_Recognition_ICCV_2023_paper.html": {
    "title": "STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Li",
      "Xiangyu Xu",
      "Hehe Fan",
      "Pan Zhou",
      "Jun Liu",
      "Jia-Wei Liu",
      "Jiahe Li",
      "Jussi Keppo",
      "Mike Zheng Shou",
      "Shuicheng Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Get_the_Best_of_Both_Worlds_Improving_Accuracy_and_Transferability_ICCV_2023_paper.html": {
    "title": "Get the Best of Both Worlds: Improving Accuracy and Transferability by Grassmann Class Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoqi Wang",
      "Zhizhong Li",
      "Wayne Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Computationally-Efficient_Neural_Image_Compression_with_Shallow_Decoders_ICCV_2023_paper.html": {
    "title": "Computationally-Efficient Neural Image Compression with Shallow Decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibo Yang",
      "Stephan Mandt"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_ObjectSDF_Improved_Object-Compositional_Neural_Implicit_Surfaces_ICCV_2023_paper.html": {
    "title": "ObjectSDF++: Improved Object-Compositional Neural Implicit Surfaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianyi Wu",
      "Kaisiyuan Wang",
      "Kejie Li",
      "Jianmin Zheng",
      "Jianfei Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fang_Tracing_the_Origin_of_Adversarial_Attack_for_Forensic_Investigation_and_ICCV_2023_paper.html": {
    "title": "Tracing the Origin of Adversarial Attack for Forensic Investigation and Deterrence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Fang",
      "Jiyi Zhang",
      "Yupeng Qiu",
      "Jiayang Liu",
      "Ke Xu",
      "Chengfang Fang",
      "Ee-Chien Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Sketch_and_Text_Guided_Diffusion_Model_for_Colored_Point_Cloud_ICCV_2023_paper.html": {
    "title": "Sketch and Text Guided Diffusion Model for Colored Point Cloud Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Wu",
      "Yaonan Wang",
      "Mingtao Feng",
      "He Xie",
      "Ajmal Mian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Scenimefy_Learning_to_Craft_Anime_Scene_via_Semi-Supervised_Image-to-Image_Translation_ICCV_2023_paper.html": {
    "title": "Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Jiang",
      "Liming Jiang",
      "Shuai Yang",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Towards_Unsupervised_Domain_Generalization_for_Face_Anti-Spoofing_ICCV_2023_paper.html": {
    "title": "Towards Unsupervised Domain Generalization for Face Anti-Spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Liu",
      "Yabo Chen",
      "Mengran Gou",
      "Chun-Ting Huang",
      "Yaoming Wang",
      "Wenrui Dai",
      "Hongkai Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_DR-Tune_Improving_Fine-tuning_of_Pretrained_Visual_Models_by_Distribution_Regularization_ICCV_2023_paper.html": {
    "title": "DR-Tune: Improving Fine-tuning of Pretrained Visual Models by Distribution Regularization with Semantic Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Zhou",
      "Jiaxin Chen",
      "Di Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Parger_MotionDeltaCNN_Sparse_CNN_Inference_of_Frame_Differences_in_Moving_Camera_ICCV_2023_paper.html": {
    "title": "MotionDeltaCNN: Sparse CNN Inference of Frame Differences in Moving Camera Videos with Spherical Buffers and Padded Convolutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathias Parger",
      "Chengcheng Tang",
      "Thomas Neff",
      "Christopher D. Twigg",
      "Cem Keskin",
      "Robert Wang",
      "Markus Steinberger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_General_Image-to-Image_Translation_with_One-Shot_Image_Guidance_ICCV_2023_paper.html": {
    "title": "General Image-to-Image Translation with One-Shot Image Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Cheng",
      "Zuhao Liu",
      "Yunbo Peng",
      "Yue Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yun_Dense_2D-3D_Indoor_Prediction_with_Sound_via_Aligned_Cross-Modal_Distillation_ICCV_2023_paper.html": {
    "title": "Dense 2D-3D Indoor Prediction with Sound via Aligned Cross-Modal Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heeseung Yun",
      "Joonil Na",
      "Gunhee Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Leveraging_SE3_Equivariance_for_Learning_3D_Geometric_Shape_Assembly_ICCV_2023_paper.html": {
    "title": "Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihai Wu",
      "Chenrui Tie",
      "Yushi Du",
      "Yan Zhao",
      "Hao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_Adversarial_Bayesian_Augmentation_for_Single-Source_Domain_Generalization_ICCV_2023_paper.html": {
    "title": "Adversarial Bayesian Augmentation for Single-Source Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Cheng",
      "Tejas Gokhale",
      "Yezhou Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Robust_Geometry-Preserving_Depth_Estimation_Using_Differentiable_Rendering_ICCV_2023_paper.html": {
    "title": "Robust Geometry-Preserving Depth Estimation Using Differentiable Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Zhang",
      "Wei Yin",
      "Gang Yu",
      "Zhibin Wang",
      "Tao Chen",
      "Bin Fu",
      "Joey Tianyi Zhou",
      "Chunhua Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Khattak_Self-regulating_Prompts_Foundational_Model_Adaptation_without_Forgetting_ICCV_2023_paper.html": {
    "title": "Self-regulating Prompts: Foundational Model Adaptation without Forgetting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Uzair Khattak",
      "Syed Talal Wasim",
      "Muzammal Naseer",
      "Salman Khan",
      "Ming-Hsuan Yang",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_ASM_Adaptive_Skinning_Model_for_High-Quality_3D_Face_Modeling_ICCV_2023_paper.html": {
    "title": "ASM: Adaptive Skinning Model for High-Quality 3D Face Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Yang",
      "Hong Shang",
      "Tianyang Shi",
      "Xinghan Chen",
      "Jingkai Zhou",
      "Zhongqian Sun",
      "Wei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dastjerdi_EverLight_Indoor-Outdoor_Editable_HDR_Lighting_Estimation_ICCV_2023_paper.html": {
    "title": "EverLight: Indoor-Outdoor Editable HDR Lighting Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Reza Karimi Dastjerdi",
      "Jonathan Eisenmann",
      "Yannick Hold-Geoffroy",
      "Jean-FranÃ§ois Lalonde"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jo_MARS_Model-agnostic_Biased_Object_Removal_without_Additional_Supervision_for_Weakly-Supervised_ICCV_2023_paper.html": {
    "title": "MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyun Jo",
      "In-Jae Yu",
      "Kyungsu Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jung_CAFA_Class-Aware_Feature_Alignment_for_Test-Time_Adaptation_ICCV_2023_paper.html": {
    "title": "CAFA: Class-Aware Feature Alignment for Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghun Jung",
      "Jungsoo Lee",
      "Nanhee Kim",
      "Amirreza Shaban",
      "Byron Boots",
      "Jaegul Choo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Learning_Clothing_and_Pose_Invariant_3D_Shape_Representation_for_Long-Term_ICCV_2023_paper.html": {
    "title": "Learning Clothing and Pose Invariant 3D Shape Representation for Long-Term Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Liu",
      "Minchul Kim",
      "ZiAng Gu",
      "Anil Jain",
      "Xiaoming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Stretcu_Agile_Modeling_From_Concept_to_Classifier_in_Minutes_ICCV_2023_paper.html": {
    "title": "Agile Modeling: From Concept to Classifier in Minutes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Otilia Stretcu",
      "Edward Vendrow",
      "Kenji Hata",
      "Krishnamurthy Viswanathan",
      "Vittorio Ferrari",
      "Sasan Tavakkol",
      "Wenlei Zhou",
      "Aditya Avinash",
      "Emming Luo",
      "Neil Gordon Alldrin",
      "MohammadHossein Bateni",
      "Gabriel Berger",
      "Andrew Bunner",
      "Chun-Ta Lu",
      "Javier Rey",
      "Giulia DeSalvo",
      "Ranjay Krishna",
      "Ariel Fuxmanâ"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Improving_Lens_Flare_Removal_with_General-Purpose_Pipeline_and_Multiple_Light_ICCV_2023_paper.html": {
    "title": "Improving Lens Flare Removal with General-Purpose Pipeline and Multiple Light Sources Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyan Zhou",
      "Dong Liang",
      "Songcan Chen",
      "Sheng-Jun Huang",
      "Shuo Yang",
      "Chongyi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gustafson_FACET_Fairness_in_Computer_Vision_Evaluation_Benchmark_ICCV_2023_paper.html": {
    "title": "FACET: Fairness in Computer Vision Evaluation Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laura Gustafson",
      "Chloe Rolland",
      "Nikhila Ravi",
      "Quentin Duval",
      "Aaron Adcock",
      "Cheng-Yang Fu",
      "Melissa Hall",
      "Candace Ross"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Few-Shot_Physically-Aware_Articulated_Mesh_Generation_via_Hierarchical_Deformation_ICCV_2023_paper.html": {
    "title": "Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyi Liu",
      "Bin Wang",
      "He Wang",
      "Li Yi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Single-Stage_Diffusion_NeRF_A_Unified_Approach_to_3D_Generation_and_ICCV_2023_paper.html": {
    "title": "Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hansheng Chen",
      "Jiatao Gu",
      "Anpei Chen",
      "Wei Tian",
      "Zhuowen Tu",
      "Lingjie Liu",
      "Hao Su"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_DCPB_Deformable_Convolution_Based_on_the_Poincare_Ball_for_Top-view_ICCV_2023_paper.html": {
    "title": "DCPB: Deformable Convolution Based on the Poincare Ball for Top-view Fisheye Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Wei",
      "Zhidan Ran",
      "Xiaobo Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Integrating_Boxes_and_Masks_A_Multi-Object_Framework_for_Unified_Visual_ICCV_2023_paper.html": {
    "title": "Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyou Xu",
      "Zongxin Yang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_One-Shot_Generative_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "One-Shot Generative Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ceyuan Yang",
      "Yujun Shen",
      "Zhiyi Zhang",
      "Yinghao Xu",
      "Jiapeng Zhu",
      "Zhirong Wu",
      "Bolei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tian_Prototypes-oriented_Transductive_Few-shot_Learning_with_Conditional_Transport_ICCV_2023_paper.html": {
    "title": "Prototypes-oriented Transductive Few-shot Learning with Conditional Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Tian",
      "Jingyi Feng",
      "Xiaoqiang Chai",
      "Wenchao Chen",
      "Liming Wang",
      "Xiyang Liu",
      "Bo Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xie_SparseFusion_Fusing_Multi-Modal_Sparse_Representations_for_Multi-Sensor_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "SparseFusion: Fusing Multi-Modal Sparse Representations for Multi-Sensor 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Xie",
      "Chenfeng Xu",
      "Marie-Julie Rakotosaona",
      "Patrick Rim",
      "Federico Tombari",
      "Kurt Keutzer",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_DetermiNet_A_Large-Scale_Diagnostic_Dataset_for_Complex_Visually-Grounded_Referencing_using_ICCV_2023_paper.html": {
    "title": "DetermiNet: A Large-Scale Diagnostic Dataset for Complex Visually-Grounded Referencing using Determiners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clarence Lee",
      "M Ganesh Kumar",
      "Cheston Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ding_3DMOTFormer_Graph_Transformer_for_Online_3D_Multi-Object_Tracking_ICCV_2023_paper.html": {
    "title": "3DMOTFormer: Graph Transformer for Online 3D Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuxiao Ding",
      "Eike Rehder",
      "Lukas Schneider",
      "Marius Cordts",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bulat_ReGen_A_good_Generative_Zero-Shot_Video_Classifier_Should_be_Rewarded_ICCV_2023_paper.html": {
    "title": "ReGen: A good Generative Zero-Shot Video Classifier Should be Rewarded",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Bulat",
      "Enrique Sanchez",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cho_Complementary_Domain_Adaptation_and_Generalization_for_Unsupervised_Continual_Domain_Shift_ICCV_2023_paper.html": {
    "title": "Complementary Domain Adaptation and Generalization for Unsupervised Continual Domain Shift Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonguk Cho",
      "Jinha Park",
      "Taesup Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_RICO_Regularizing_the_Unobservable_for_Indoor_Compositional_Reconstruction_ICCV_2023_paper.html": {
    "title": "RICO: Regularizing the Unobservable for Indoor Compositional Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zizhang Li",
      "Xiaoyang Lyu",
      "Yuanyuan Ding",
      "Mengmeng Wang",
      "Yiyi Liao",
      "Yong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Agarwal_Ordered_Atomic_Activity_for_Fine-grained_Interactive_Traffic_Scenario_Understanding_ICCV_2023_paper.html": {
    "title": "Ordered Atomic Activity for Fine-grained Interactive Traffic Scenario Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nakul Agarwal",
      "Yi-Ting Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nakhli_CO-PILOT_Dynamic_Top-Down_Point_Cloud_with_Conditional_Neighborhood_Aggregation_for_ICCV_2023_paper.html": {
    "title": "CO-PILOT: Dynamic Top-Down Point Cloud with Conditional Neighborhood Aggregation for Multi-Gigapixel Histopathology Image Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramin Nakhli",
      "Allen Zhang",
      "Ali Mirabadi",
      "Katherine Rich",
      "Maryam Asadi",
      "Blake Gilks",
      "Hossein Farahani",
      "Ali Bashashati"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ou_Troubleshooting_Ethnic_Quality_Bias_with_Curriculum_Domain_Adaptation_for_Face_ICCV_2023_paper.html": {
    "title": "Troubleshooting Ethnic Quality Bias with Curriculum Domain Adaptation for Face Image Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu-Zhao Ou",
      "Baoliang Chen",
      "Chongyi Li",
      "Shiqi Wang",
      "Sam Kwong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yucel_HybridAugment_Unified_Frequency_Spectra_Perturbations_for_Model_Robustness_ICCV_2023_paper.html": {
    "title": "HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehmet Kerim Yucel",
      "Ramazan Gokberk Cinbis",
      "Pinar Duygulu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ge_CLR_Channel-wise_Lightweight_Reprogramming_for_Continual_Learning_ICCV_2023_paper.html": {
    "title": "CLR: Channel-wise Lightweight Reprogramming for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhao Ge",
      "Yuecheng Li",
      "Shuo Ni",
      "Jiaping Zhao",
      "Ming-Hsuan Yang",
      "Laurent Itti"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_IOMatch_Simplifying_Open-Set_Semi-Supervised_Learning_with_Joint_Inliers_and_Outliers_ICCV_2023_paper.html": {
    "title": "IOMatch: Simplifying Open-Set Semi-Supervised Learning with Joint Inliers and Outliers Utilization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekun Li",
      "Lei Qi",
      "Yinghuan Shi",
      "Yang Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Hierarchical_Point-based_Active_Learning_for_Semi-supervised_Point_Cloud_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongyi Xu",
      "Bo Yuan",
      "Shanshan Zhao",
      "Qianni Zhang",
      "Xinbo Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cai_Doppelgangers_Learning_to_Disambiguate_Images_of_Similar_Structures_ICCV_2023_paper.html": {
    "title": "Doppelgangers: Learning to Disambiguate Images of Similar Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruojin Cai",
      "Joseph Tung",
      "Qianqian Wang",
      "Hadar Averbuch-Elor",
      "Bharath Hariharan",
      "Noah Snavely"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_BEV-DG_Cross-Modal_Learning_under_Birds-Eye_View_for_Domain_Generalization_of_ICCV_2023_paper.html": {
    "title": "BEV-DG: Cross-Modal Learning under Bird's-Eye View for Domain Generalization of 3D Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miaoyu Li",
      "Yachao Zhang",
      "Xu Ma",
      "Yanyun Qu",
      "Yun Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cui_Grounded_Entity-Landmark_Adaptive_Pre-Training_for_Vision-and-Language_Navigation_ICCV_2023_paper.html": {
    "title": "Grounded Entity-Landmark Adaptive Pre-Training for Vision-and-Language Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibo Cui",
      "Liang Xie",
      "Yakun Zhang",
      "Meishan Zhang",
      "Ye Yan",
      "Erwei Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Lip_Reading_for_Low-resource_Languages_by_Learning_and_Combining_General_ICCV_2023_paper.html": {
    "title": "Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsu Kim",
      "Jeong Hun Yeo",
      "Jeongsoo Choi",
      "Yong Man Ro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Le_Quality-Agnostic_Deepfake_Detection_with_Intra-model_Collaborative_Learning_ICCV_2023_paper.html": {
    "title": "Quality-Agnostic Deepfake Detection with Intra-model Collaborative Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binh M. Le",
      "Simon S. Woo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Object-Centric_Multiple_Object_Tracking_ICCV_2023_paper.html": {
    "title": "Object-Centric Multiple Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixu Zhao",
      "Jiaze Wang",
      "Max Horn",
      "Yizhuo Ding",
      "Tong He",
      "Zechen Bai",
      "Dominik Zietlow",
      "Carl-Johann Simon-Gabriel",
      "Bing Shuai",
      "Zhuowen Tu",
      "Thomas Brox",
      "Bernt Schiele",
      "Yanwei Fu",
      "Francesco Locatello",
      "Zheng Zhang",
      "Tianjun Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hatem_Point-TTA_Test-Time_Adaptation_for_Point_Cloud_Registration_Using_Multitask_Meta-Auxiliary_ICCV_2023_paper.html": {
    "title": "Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Hatem",
      "Yiming Qian",
      "Yang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_HopFIR_Hop-wise_GraphFormer_with_Intragroup_Joint_Refinement_for_3D_Human_ICCV_2023_paper.html": {
    "title": "HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Zhai",
      "Qiang Nie",
      "Bo Ouyang",
      "Xiang Li",
      "Shanlin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Improving_Generalization_of_Adversarial_Training_via_Robust_Critical_Fine-Tuning_ICCV_2023_paper.html": {
    "title": "Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaijie Zhu",
      "Xixu Hu",
      "Jindong Wang",
      "Xing Xie",
      "Ge Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ding_Minimal_Solutions_to_Generalized_Three-View_Relative_Pose_Problem_ICCV_2023_paper.html": {
    "title": "Minimal Solutions to Generalized Three-View Relative Pose Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaqing Ding",
      "Chiang-Heng Chien",
      "Viktor Larsson",
      "Karl ÃstrÃ¶m",
      "Benjamin Kimia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shi_Trajectory_Unified_Transformer_for_Pedestrian_Trajectory_Prediction_ICCV_2023_paper.html": {
    "title": "Trajectory Unified Transformer for Pedestrian Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liushuai Shi",
      "Le Wang",
      "Sanping Zhou",
      "Gang Hua"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Park_Understanding_the_Feature_Norm_for_Out-of-Distribution_Detection_ICCV_2023_paper.html": {
    "title": "Understanding the Feature Norm for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewoo Park",
      "Jacky Chen Long Chai",
      "Jaeho Yoon",
      "Andrew Beng Jin Teoh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_MHEntropy_Entropy_Meets_Multiple_Hypotheses_for_Pose_and_Shape_Recovery_ICCV_2023_paper.html": {
    "title": "MHEntropy: Entropy Meets Multiple Hypotheses for Pose and Shape Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongyu Chen",
      "Linlin Yang",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ashesh_uSplit_Image_Decomposition_for_Fluorescence_Microscopy_ICCV_2023_paper.html": {
    "title": "uSplit: Image Decomposition for Fluorescence Microscopy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashesh Ashesh",
      "Alexander Krull",
      "Moises Di Sante",
      "Francesco Pasqualini",
      "Florian Jug"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Modeling_the_Relative_Visual_Tempo_for_Self-supervised_Skeleton-based_Action_Recognition_ICCV_2023_paper.html": {
    "title": "Modeling the Relative Visual Tempo for Self-supervised Skeleton-based Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisheng Zhu",
      "Hu Han",
      "Zhengtao Yu",
      "Guangcan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lindenberger_LightGlue_Local_Feature_Matching_at_Light_Speed_ICCV_2023_paper.html": {
    "title": "LightGlue: Local Feature Matching at Light Speed",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Lindenberger",
      "Paul-Edouard Sarlin",
      "Marc Pollefeys"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Masked_Autoencoders_are_Efficient_Class_Incremental_Learners_ICCV_2023_paper.html": {
    "title": "Masked Autoencoders are Efficient Class Incremental Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiang-Tian Zhai",
      "Xialei Liu",
      "Andrew D. Bagdanov",
      "Ke Li",
      "Ming-Ming Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Knowledge_Proxy_Intervention_for_Deconfounded_Video_Question_Answering_ICCV_2023_paper.html": {
    "title": "Knowledge Proxy Intervention for Deconfounded Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangtong Li",
      "Li Niu",
      "Liqing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Duan_Towards_Semi-supervised_Learning_with_Non-random_Missing_Labels_ICCV_2023_paper.html": {
    "title": "Towards Semi-supervised Learning with Non-random Missing Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Duan",
      "Zhen Zhao",
      "Lei Qi",
      "Luping Zhou",
      "Lei Wang",
      "Yinghuan Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_DetZero_Rethinking_Offboard_3D_Object_Detection_with_Long-term_Sequential_Point_ICCV_2023_paper.html": {
    "title": "DetZero: Rethinking Offboard 3D Object Detection with Long-term Sequential Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Ma",
      "Xuemeng Yang",
      "Hongbin Zhou",
      "Xin Li",
      "Botian Shi",
      "Junjie Liu",
      "Yuchen Yang",
      "Zhizheng Liu",
      "Liang He",
      "Yu Qiao",
      "Yikang Li",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_ImbSAM_A_Closer_Look_at_Sharpness-Aware_Minimization_in_Class-Imbalanced_Recognition_ICCV_2023_paper.html": {
    "title": "ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Zhou",
      "Yi Qu",
      "Xing Xu",
      "Hengtao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Learning_from_Noisy_Data_for_Semi-Supervised_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "Learning from Noisy Data for Semi-Supervised 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehui Chen",
      "Zhenyu Li",
      "Shuo Wang",
      "Dengpan Fu",
      "Feng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhan_NeRFrac_Neural_Radiance_Fields_through_Refractive_Surface_ICCV_2023_paper.html": {
    "title": "NeRFrac: Neural Radiance Fields through Refractive Surface",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhan",
      "Shohei Nobuhara",
      "Ko Nishino",
      "Yinqiang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_MonoDETR_Depth-guided_Transformer_for_Monocular_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renrui Zhang",
      "Han Qiu",
      "Tai Wang",
      "Ziyu Guo",
      "Ziteng Cui",
      "Yu Qiao",
      "Hongsheng Li",
      "Peng Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Towards_Authentic_Face_Restoration_with_Iterative_Diffusion_Models_and_Beyond_ICCV_2023_paper.html": {
    "title": "Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zhao",
      "Tingbo Hou",
      "Yu-Chuan Su",
      "Xuhui Jia",
      "Yandong Li",
      "Matthias Grundmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhi_LivelySpeaker_Towards_Semantic-Aware_Co-Speech_Gesture_Generation_ICCV_2023_paper.html": {
    "title": "LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihao Zhi",
      "Xiaodong Cun",
      "Xuelin Chen",
      "Xi Shen",
      "Wen Guo",
      "Shaoli Huang",
      "Shenghua Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Contrastive_Feature_Masking_Open-Vocabulary_Vision_Transformer_ICCV_2023_paper.html": {
    "title": "Contrastive Feature Masking Open-Vocabulary Vision Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dahun Kim",
      "Anelia Angelova",
      "Weicheng Kuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Group_DETR_Fast_DETR_Training_with_Group-Wise_One-to-Many_Assignment_ICCV_2023_paper.html": {
    "title": "Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Chen",
      "Xiaokang Chen",
      "Jian Wang",
      "Shan Zhang",
      "Kun Yao",
      "Haocheng Feng",
      "Junyu Han",
      "Errui Ding",
      "Gang Zeng",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Preventing_Zero-Shot_Transfer_Degradation_in_Continual_Learning_of_Vision-Language_Models_ICCV_2023_paper.html": {
    "title": "Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zangwei Zheng",
      "Mingyuan Ma",
      "Kai Wang",
      "Ziheng Qin",
      "Xiangyu Yue",
      "Yang You"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Personalized_Image_Generation_for_Color_Vision_Deficiency_Population_ICCV_2023_paper.html": {
    "title": "Personalized Image Generation for Color Vision Deficiency Population",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyi Jiang",
      "Daochang Liu",
      "Dingquan Li",
      "Chang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_EGC_Image_Generation_and_Classification_via_a_Diffusion_Energy-Based_Model_ICCV_2023_paper.html": {
    "title": "EGC: Image Generation and Classification via a Diffusion Energy-Based Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiushan Guo",
      "Chuofan Ma",
      "Yi Jiang",
      "Zehuan Yuan",
      "Yizhou Yu",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_OccFormer_Dual-path_Transformer_for_Vision-based_3D_Semantic_Occupancy_Prediction_ICCV_2023_paper.html": {
    "title": "OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunpeng Zhang",
      "Zheng Zhu",
      "Dalong Du"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Probabilistic_Triangulation_for_Uncalibrated_Multi-View_3D_Human_Pose_Estimation_ICCV_2023_paper.html": {
    "title": "Probabilistic Triangulation for Uncalibrated Multi-View 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyuan Jiang",
      "Lei Hu",
      "Shihong Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Weng_Joint_Metrics_Matter_A_Better_Standard_for_Trajectory_Forecasting_ICCV_2023_paper.html": {
    "title": "Joint Metrics Matter: A Better Standard for Trajectory Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erica Weng",
      "Hana Hoshino",
      "Deva Ramanan",
      "Kris Kitani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dou_TORE_Token_Reduction_for_Efficient_Human_Mesh_Recovery_with_Transformer_ICCV_2023_paper.html": {
    "title": "TORE: Token Reduction for Efficient Human Mesh Recovery with Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyang Dou",
      "Qingxuan Wu",
      "Cheng Lin",
      "Zeyu Cao",
      "Qiangqiang Wu",
      "Weilin Wan",
      "Taku Komura",
      "Wenping Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Roy_Test_Time_Adaptation_for_Blind_Image_Quality_Assessment_ICCV_2023_paper.html": {
    "title": "Test Time Adaptation for Blind Image Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhadeep Roy",
      "Shankhanil Mitra",
      "Soma Biswas",
      "Rajiv Soundararajan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_GeT_Generative_Target_Structure_Debiasing_for_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "GeT: Generative Target Structure Debiasing for Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can Zhang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_D3G_Exploring_Gaussian_Prior_for_Temporal_Sentence_Grounding_with_Glance_ICCV_2023_paper.html": {
    "title": "D3G: Exploring Gaussian Prior for Temporal Sentence Grounding with Glance Annotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanjun Li",
      "Xiujun Shu",
      "Sunan He",
      "Ruizhi Qiao",
      "Wei Wen",
      "Taian Guo",
      "Bei Gan",
      "Xing Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_GEDepth_Ground_Embedding_for_Monocular_Depth_Estimation_ICCV_2023_paper.html": {
    "title": "GEDepth: Ground Embedding for Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaodong Yang",
      "Zhuang Ma",
      "Zhiyu Ji",
      "Zhe Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zong_DETRs_with_Collaborative_Hybrid_Assignments_Training_ICCV_2023_paper.html": {
    "title": "DETRs with Collaborative Hybrid Assignments Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuofan Zong",
      "Guanglu Song",
      "Yu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Animal3D_A_Comprehensive_Dataset_of_3D_Animal_Pose_and_Shape_ICCV_2023_paper.html": {
    "title": "Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacong Xu",
      "Yi Zhang",
      "Jiawei Peng",
      "Wufei Ma",
      "Artur Jesslen",
      "Pengliang Ji",
      "Qixin Hu",
      "Jiehua Zhang",
      "Qihao Liu",
      "Jiahao Wang",
      "Wei Ji",
      "Chen Wang",
      "Xiaoding Yuan",
      "Prakhar Kaushik",
      "Guofeng Zhang",
      "Jie Liu",
      "Yushan Xie",
      "Yawen Cui",
      "Alan Yuille",
      "Adam Kortylewski"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ji_Rethinking_Video_Frame_Interpolation_from_Shutter_Mode_Induced_Degradation_ICCV_2023_paper.html": {
    "title": "Rethinking Video Frame Interpolation from Shutter Mode Induced Degradation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Ji",
      "Zhixiang Wang",
      "Zhihang Zhong",
      "Yinqiang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Multi-Modal_Neural_Radiance_Field_for_Monocular_Dense_SLAM_with_a_ICCV_2023_paper.html": {
    "title": "Multi-Modal Neural Radiance Field for Monocular Dense SLAM with a Light-Weight ToF Sensor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyang Liu",
      "Yijin Li",
      "Yanbin Teng",
      "Hujun Bao",
      "Guofeng Zhang",
      "Yinda Zhang",
      "Zhaopeng Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_MonoNeRD_NeRF-like_Representations_for_Monocular_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junkai Xu",
      "Liang Peng",
      "Haoran Cheng",
      "Hao Li",
      "Wei Qian",
      "Ke Li",
      "Wenxiao Wang",
      "Deng Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Monocular_3D_Object_Detection_with_Bounding_Box_Denoising_in_3D_ICCV_2023_paper.html": {
    "title": "Monocular 3D Object Detection with Bounding Box Denoising in 3D by Perceiver",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianpeng Liu",
      "Ce Zheng",
      "Kelvin B Cheng",
      "Nan Xue",
      "Guo-Jun Qi",
      "Tianfu Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sandstrom_Point-SLAM_Dense_Neural_Point_Cloud-based_SLAM_ICCV_2023_paper.html": {
    "title": "Point-SLAM: Dense Neural Point Cloud-based SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik SandstrÃ¶m",
      "Yue Li",
      "Luc Van Gool",
      "Martin R. Oswald"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_TrajectoryFormer_3D_Object_Tracking_Transformer_with_Predictive_Trajectory_Hypotheses_ICCV_2023_paper.html": {
    "title": "TrajectoryFormer: 3D Object Tracking Transformer with Predictive Trajectory Hypotheses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuesong Chen",
      "Shaoshuai Shi",
      "Chao Zhang",
      "Benjin Zhu",
      "Qiang Wang",
      "Ka Chun Cheung",
      "Simon See",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Semantic-Aware_Dynamic_Parameter_for_Video_Inpainting_Transformer_ICCV_2023_paper.html": {
    "title": "Semantic-Aware Dynamic Parameter for Video Inpainting Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunhye Lee",
      "Jinsu Yoo",
      "Yunjeong Yang",
      "Sungyong Baik",
      "Tae Hyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lu_See_More_and_Know_More_Zero-shot_Point_Cloud_Segmentation_via_ICCV_2023_paper.html": {
    "title": "See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Lu",
      "Qi Jiang",
      "Runnan Chen",
      "Yuenan Hou",
      "Xinge Zhu",
      "Yuexin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mikaeili_SKED_Sketch-guided_Text-based_3D_Editing_ICCV_2023_paper.html": {
    "title": "SKED: Sketch-guided Text-based 3D Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aryan Mikaeili",
      "Or Perel",
      "Mehdi Safaee",
      "Daniel Cohen-Or",
      "Ali Mahdavi-Amiri"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_WaveIPT_Joint_Attention_and_Flow_Alignment_in_the_Wavelet_domain_ICCV_2023_paper.html": {
    "title": "WaveIPT: Joint Attention and Flow Alignment in the Wavelet domain for Pose Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyuan Ma",
      "Tingwei Gao",
      "Haitian Jiang",
      "Haibin Shen",
      "Kejie Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Editable_Image_Geometric_Abstraction_via_Neural_Primitive_Assembly_ICCV_2023_paper.html": {
    "title": "Editable Image Geometric Abstraction via Neural Primitive Assembly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Chen",
      "Bingbing Ni",
      "Xuanhong Chen",
      "Zhangli Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Homeomorphism_Alignment_for_Unsupervised_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "Homeomorphism Alignment for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lihua Zhou",
      "Mao Ye",
      "Xiatian Zhu",
      "Siying Xiao",
      "Xu-Qian Fan",
      "Ferrante Neri"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_MBPTrack_Improving_3D_Point_Cloud_Tracking_with_Memory_Networks_and_ICCV_2023_paper.html": {
    "title": "MBPTrack: Improving 3D Point Cloud Tracking with Memory Networks and Box Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian-Xing Xu",
      "Yuan-Chen Guo",
      "Yu-Kun Lai",
      "Song-Hai Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qu_Novel-View_Synthesis_and_Pose_Estimation_for_Hand-Object_Interaction_from_Sparse_ICCV_2023_paper.html": {
    "title": "Novel-View Synthesis and Pose Estimation for Hand-Object Interaction from Sparse Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentian Qu",
      "Zhaopeng Cui",
      "Yinda Zhang",
      "Chenyu Meng",
      "Cuixia Ma",
      "Xiaoming Deng",
      "Hongan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_EmoSet_A_Large-scale_Visual_Emotion_Dataset_with_Rich_Attributes_ICCV_2023_paper.html": {
    "title": "EmoSet: A Large-scale Visual Emotion Dataset with Rich Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyuan Yang",
      "Qirui Huang",
      "Tingting Ding",
      "Dani Lischinski",
      "Danny Cohen-Or",
      "Hui Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Borup_Distilling_from_Similar_Tasks_for_Transfer_Learning_on_a_Budget_ICCV_2023_paper.html": {
    "title": "Distilling from Similar Tasks for Transfer Learning on a Budget",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenneth Borup",
      "Cheng Perng Phoo",
      "Bharath Hariharan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bhat_Self-Supervised_Burst_Super-Resolution_ICCV_2023_paper.html": {
    "title": "Self-Supervised Burst Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Goutam Bhat",
      "MichaÃ«l Gharbi",
      "Jiawen Chen",
      "Luc Van Gool",
      "Zhihao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gu_Class-relation_Knowledge_Distillation_for_Novel_Class_Discovery_ICCV_2023_paper.html": {
    "title": "Class-relation Knowledge Distillation for Novel Class Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiyan Gu",
      "Chuyu Zhang",
      "Ruijie Xu",
      "Xuming He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nie_PARTNER_Level_up_the_Polar_Representation_for_LiDAR_3D_Object_ICCV_2023_paper.html": {
    "title": "PARTNER: Level up the Polar Representation for LiDAR 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Nie",
      "Yujing Xue",
      "Chunwei Wang",
      "Chaoqiang Ye",
      "Hang Xu",
      "Xinge Zhu",
      "Qingqiu Huang",
      "Michael Bi Mi",
      "Xinchao Wang",
      "Li Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Aich_Data-Free_Class-Incremental_Hand_Gesture_Recognition_ICCV_2023_paper.html": {
    "title": "Data-Free Class-Incremental Hand Gesture Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubhra Aich",
      "Jesus Ruiz-Santaquiteria",
      "Zhenyu Lu",
      "Prachi Garg",
      "K J Joseph",
      "Alvaro Fernandez Garcia",
      "Vineeth N Balasubramanian",
      "Kenrick Kin",
      "Chengde Wan",
      "Necati Cihan Camgoz",
      "Shugao Ma",
      "Fernando De la Torre"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Srivastava_Corrupting_Neuron_Explanations_of_Deep_Visual_Features_ICCV_2023_paper.html": {
    "title": "Corrupting Neuron Explanations of Deep Visual Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divyansh Srivastava",
      "Tuomas Oikarinen",
      "Tsui-Wei Weng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bae_PNI__Industrial_Anomaly_Detection_using_Position_and_Neighborhood_Information_ICCV_2023_paper.html": {
    "title": "PNI : Industrial Anomaly Detection using Position and Neighborhood Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehyeok Bae",
      "Jae-Han Lee",
      "Seyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Park_PC-Adapter_Topology-Aware_Adapter_for_Efficient_Domain_Adaption_on_Point_Clouds_ICCV_2023_paper.html": {
    "title": "PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joonhyung Park",
      "Hyunjin Seo",
      "Eunho Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nam_Cyclic_Test-Time_Adaptation_on_Monocular_Video_for_3D_Human_Mesh_ICCV_2023_paper.html": {
    "title": "Cyclic Test-Time Adaptation on Monocular Video for 3D Human Mesh Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeongjin Nam",
      "Daniel Sungho Jung",
      "Yeonguk Oh",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_2D3D-MATR_2D-3D_Matching_Transformer_for_Detection-Free_Registration_Between_Images_and_ICCV_2023_paper.html": {
    "title": "2D3D-MATR: 2D-3D Matching Transformer for Detection-Free Registration Between Images and Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minhao Li",
      "Zheng Qin",
      "Zhirui Gao",
      "Renjiao Yi",
      "Chenyang Zhu",
      "Yulan Guo",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Mixed_Neural_Voxels_for_Fast_Multi-view_Video_Synthesis_ICCV_2023_paper.html": {
    "title": "Mixed Neural Voxels for Fast Multi-view Video Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Wang",
      "Sinan Tan",
      "Xinghang Li",
      "Zeyue Tian",
      "Yafei Song",
      "Huaping Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Bidirectionally_Deformable_Motion_Modulation_For_Video-based_Human_Pose_Transfer_ICCV_2023_paper.html": {
    "title": "Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wing-Yin Yu",
      "Lai-Man Po",
      "Ray C.C. Cheung",
      "Yuzhi Zhao",
      "Yu Xue",
      "Kun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Luo_Harvard_Glaucoma_Detection_and_Progression_A_Multimodal_Multitask_Dataset_and_ICCV_2023_paper.html": {
    "title": "Harvard Glaucoma Detection and Progression: A Multimodal Multitask Dataset and Generalization-Reinforced Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Luo",
      "Min Shi",
      "Yu Tian",
      "Tobias Elze",
      "Mengyu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Tracking_Everything_Everywhere_All_at_Once_ICCV_2023_paper.html": {
    "title": "Tracking Everything Everywhere All at Once",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianqian Wang",
      "Yen-Yu Chang",
      "Ruojin Cai",
      "Zhengqi Li",
      "Bharath Hariharan",
      "Aleksander Holynski",
      "Noah Snavely"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Group_Pose_A_Simple_Baseline_for_End-to-End_Multi-Person_Pose_Estimation_ICCV_2023_paper.html": {
    "title": "Group Pose: A Simple Baseline for End-to-End Multi-Person Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan Liu",
      "Qiang Chen",
      "Zichang Tan",
      "Jiang-Jiang Liu",
      "Jian Wang",
      "Xiangbo Su",
      "Xiaolong Li",
      "Kun Yao",
      "Junyu Han",
      "Errui Ding",
      "Yao Zhao",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Objects_Do_Not_Disappear_Video_Object_Detection_by_Single-Frame_Object_ICCV_2023_paper.html": {
    "title": "Objects Do Not Disappear: Video Object Detection by Single-Frame Object Location Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Liu",
      "Fatemeh Karimi Nejadasl",
      "Jan C. van Gemert",
      "Olaf Booij",
      "Silvia L. Pintea"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Miao_CauSSL_Causality-inspired_Semi-supervised_Learning_for_Medical_Image_Segmentation_ICCV_2023_paper.html": {
    "title": "CauSSL: Causality-inspired Semi-supervised Learning for Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juzheng Miao",
      "Cheng Chen",
      "Furui Liu",
      "Hao Wei",
      "Pheng-Ann Heng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_ChartReader_A_Unified_Framework_for_Chart_Derendering_and_Comprehension_without_ICCV_2023_paper.html": {
    "title": "ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi-Qi Cheng",
      "Qi Dai",
      "Alexander G. Hauptmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Learning_from_Semantic_Alignment_between_Unpaired_Multiviews_for_Egocentric_Video_ICCV_2023_paper.html": {
    "title": "Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qitong Wang",
      "Long Zhao",
      "Liangzhe Yuan",
      "Ting Liu",
      "Xi Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Neural_LiDAR_Fields_for_Novel_View_Synthesis_ICCV_2023_paper.html": {
    "title": "Neural LiDAR Fields for Novel View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyu Huang",
      "Zan Gojcic",
      "Zian Wang",
      "Francis Williams",
      "Yoni Kasten",
      "Sanja Fidler",
      "Konrad Schindler",
      "Or Litany"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/WU_Source-free_Depth_for_Object_Pop-out_ICCV_2023_paper.html": {
    "title": "Source-free Depth for Object Pop-out",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongwei WU",
      "Danda Pani Paudel",
      "Deng-Ping Fan",
      "Jingjing Wang",
      "Shuo Wang",
      "CÃ©dric Demonceaux",
      "Radu Timofte",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xiao_Token-Label_Alignment_for_Vision_Transformers_ICCV_2023_paper.html": {
    "title": "Token-Label Alignment for Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Xiao",
      "Wenzhao Zheng",
      "Zheng Zhu",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qian_Understanding_3D_Object_Interaction_from_a_Single_Image_ICCV_2023_paper.html": {
    "title": "Understanding 3D Object Interaction from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyi Qian",
      "David F. Fouhey"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Duan_SkeleTR_Towards_Skeleton-based_Action_Recognition_in_the_Wild_ICCV_2023_paper.html": {
    "title": "SkeleTR: Towards Skeleton-based Action Recognition in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haodong Duan",
      "Mingze Xu",
      "Bing Shuai",
      "Davide Modolo",
      "Zhuowen Tu",
      "Joseph Tighe",
      "Alessandro Bergamo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Learning_Gabor_Texture_Features_for_Fine-Grained_Recognition_ICCV_2023_paper.html": {
    "title": "Learning Gabor Texture Features for Fine-Grained Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lanyun Zhu",
      "Tianrun Chen",
      "Jianxiong Yin",
      "Simon See",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Weakly-Supervised_Action_Localization_by_Hierarchically-Structured_Latent_Attention_Modeling_ICCV_2023_paper.html": {
    "title": "Weakly-Supervised Action Localization by Hierarchically-Structured Latent Attention Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guiqin Wang",
      "Peng Zhao",
      "Cong Zhao",
      "Shusen Yang",
      "Jie Cheng",
      "Luziwei Leng",
      "Jianxing Liao",
      "Qinghai Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xiong_Get3DHuman_Lifting_StyleGAN-Human_into_a_3D_Generative_Model_Using_Pixel-Aligned_ICCV_2023_paper.html": {
    "title": "Get3DHuman: Lifting StyleGAN-Human into a 3D Generative Model Using Pixel-Aligned Reconstruction Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangyang Xiong",
      "Di Kang",
      "Derong Jin",
      "Weikai Chen",
      "Linchao Bao",
      "Shuguang Cui",
      "Xiaoguang Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Query6DoF_Learning_Sparse_Queries_as_Implicit_Shape_Prior_for_Category-Level_ICCV_2023_paper.html": {
    "title": "Query6DoF: Learning Sparse Queries as Implicit Shape Prior for Category-Level 6DoF Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Wang",
      "Xinggang Wang",
      "Te Li",
      "Rong Yang",
      "Minhong Wan",
      "Wenyu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fu_Towards_High-Quality_Specular_Highlight_Removal_by_Leveraging_Large-Scale_Synthetic_Data_ICCV_2023_paper.html": {
    "title": "Towards High-Quality Specular Highlight Removal by Leveraging Large-Scale Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang Fu",
      "Qing Zhang",
      "Lei Zhu",
      "Chunxia Xiao",
      "Ping Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_An_Embarrassingly_Simple_Backdoor_Attack_on_Self-supervised_Learning_ICCV_2023_paper.html": {
    "title": "An Embarrassingly Simple Backdoor Attack on Self-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changjiang Li",
      "Ren Pang",
      "Zhaohan Xi",
      "Tianyu Du",
      "Shouling Ji",
      "Yuan Yao",
      "Ting Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Cross-Modal_Translation_and_Alignment_for_Survival_Analysis_ICCV_2023_paper.html": {
    "title": "Cross-Modal Translation and Alignment for Survival Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengtao Zhou",
      "Hao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ong_Chaotic_World_A_Large_and_Challenging_Benchmark_for_Human_Behavior_ICCV_2023_paper.html": {
    "title": "Chaotic World: A Large and Challenging Benchmark for Human Behavior Understanding in Chaotic Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kian Eng Ong",
      "Xun Long Ng",
      "Yanchao Li",
      "Wenjie Ai",
      "Kuangyi Zhao",
      "Si Yong Yeo",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Najibi_Unsupervised_3D_Perception_with_2D_Vision-Language_Distillation_for_Autonomous_Driving_ICCV_2023_paper.html": {
    "title": "Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahyar Najibi",
      "Jingwei Ji",
      "Yin Zhou",
      "Charles R. Qi",
      "Xinchen Yan",
      "Scott Ettinger",
      "Dragomir Anguelov"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Towards_Grand_Unified_Representation_Learning_for_Unsupervised_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.html": {
    "title": "Towards Grand Unified Representation Learning for Unsupervised Visible-Infrared Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Yang",
      "Jun Chen",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bartolomei_Active_Stereo_Without_Pattern_Projector_ICCV_2023_paper.html": {
    "title": "Active Stereo Without Pattern Projector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Bartolomei",
      "Matteo Poggi",
      "Fabio Tosi",
      "Andrea Conti",
      "Stefano Mattoccia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Partition_Speeds_Up_Learning_Implicit_Neural_Representations_Based_on_Exponential-Increase_ICCV_2023_paper.html": {
    "title": "Partition Speeds Up Learning Implicit Neural Representations Based on Exponential-Increase Hypothesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Liu",
      "Feng Liu",
      "Haishuai Wang",
      "Ning Ma",
      "Jiajun Bu",
      "Bo Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jing_Uncertainty_Guided_Adaptive_Warping_for_Robust_and_Efficient_Stereo_Matching_ICCV_2023_paper.html": {
    "title": "Uncertainty Guided Adaptive Warping for Robust and Efficient Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junpeng Jing",
      "Jiankun Li",
      "Pengfei Xiong",
      "Jiangyu Liu",
      "Shuaicheng Liu",
      "Yichen Guo",
      "Xin Deng",
      "Mai Xu",
      "Lai Jiang",
      "Leonid Sigal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_ReFit_Recurrent_Fitting_Network_for_3D_Human_Recovery_ICCV_2023_paper.html": {
    "title": "ReFit: Recurrent Fitting Network for 3D Human Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufu Wang",
      "Kostas Daniilidis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Feng_Towards_Instance-adaptive_Inference_for_Federated_Learning_ICCV_2023_paper.html": {
    "title": "Towards Instance-adaptive Inference for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun-Mei Feng",
      "Kai Yu",
      "Nian Liu",
      "Xinxing Xu",
      "Salman Khan",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Reza_CGBA_Curvature-aware_Geometric_Black-box_Attack_ICCV_2023_paper.html": {
    "title": "CGBA: Curvature-aware Geometric Black-box Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Farhamdur Reza",
      "Ali Rahmati",
      "Tianfu Wu",
      "Huaiyu Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kicanaoglu_Unsupervised_Facial_Performance_Editing_via_Vector-Quantized_StyleGAN_Representations_ICCV_2023_paper.html": {
    "title": "Unsupervised Facial Performance Editing via Vector-Quantized StyleGAN Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Berkay Kicanaoglu",
      "Pablo Garrido",
      "Gaurav Bharaj"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Online_Clustered_Codebook_ICCV_2023_paper.html": {
    "title": "Online Clustered Codebook",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanxia Zheng",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Brinkmann_A_Multidimensional_Analysis_of_Social_Biases_in_Vision_Transformers_ICCV_2023_paper.html": {
    "title": "A Multidimensional Analysis of Social Biases in Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jannik Brinkmann",
      "Paul Swoboda",
      "Christian Bartelt"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Luo_PGFed_Personalize_Each_Clients_Global_Objective_for_Federated_Learning_ICCV_2023_paper.html": {
    "title": "PGFed: Personalize Each Client's Global Objective for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Luo",
      "Matias Mendieta",
      "Chen Chen",
      "Shandong Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Momeni_Verbs_in_Action_Improving_Verb_Understanding_in_Video-Language_Models_ICCV_2023_paper.html": {
    "title": "Verbs in Action: Improving Verb Understanding in Video-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liliane Momeni",
      "Mathilde Caron",
      "Arsha Nagrani",
      "Andrew Zisserman",
      "Cordelia Schmid"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Zero-Shot_Point_Cloud_Segmentation_by_Semantic-Visual_Aware_Synthesis_ICCV_2023_paper.html": {
    "title": "Zero-Shot Point Cloud Segmentation by Semantic-Visual Aware Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Yang",
      "Munawar Hayat",
      "Zhao Jin",
      "Hongyuan Zhu",
      "Yinjie Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Exploring_Predicate_Visual_Context_in_Detecting_of_Human-Object_Interactions_ICCV_2023_paper.html": {
    "title": "Exploring Predicate Visual Context in Detecting of Human-Object Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederic Z Zhang",
      "Yuhui Yuan",
      "Dylan Campbell",
      "Zhuoyao Zhong",
      "Stephen Gould"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kong_Robo3D_Towards_Robust_and_Reliable_3D_Perception_against_Corruptions_ICCV_2023_paper.html": {
    "title": "Robo3D: Towards Robust and Reliable 3D Perception against Corruptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingdong Kong",
      "Youquan Liu",
      "Xin Li",
      "Runnan Chen",
      "Wenwei Zhang",
      "Jiawei Ren",
      "Liang Pan",
      "Kai Chen",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Duan_Towards_Saner_Deep_Image_Registration_ICCV_2023_paper.html": {
    "title": "Towards Saner Deep Image Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Duan",
      "Ming Zhong",
      "Yan Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tian_Instance_and_Category_Supervision_are_Alternate_Learners_for_Continual_Learning_ICCV_2023_paper.html": {
    "title": "Instance and Category Supervision are Alternate Learners for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Tian",
      "Zhizhong Zhang",
      "Xin Tan",
      "Jun Liu",
      "Chengjie Wang",
      "Yanyun Qu",
      "Guannan Jiang",
      "Yuan Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Feng_Diverse_Data_Augmentation_with_Diffusions_for_Effective_Test-time_Prompt_Tuning_ICCV_2023_paper.html": {
    "title": "Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun-Mei Feng",
      "Kai Yu",
      "Yong Liu",
      "Salman Khan",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nakatani_Interaction-aware_Joint_Attention_Estimation_Using_People_Attributes_ICCV_2023_paper.html": {
    "title": "Interaction-aware Joint Attention Estimation Using People Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chihiro Nakatani",
      "Hiroaki Kawashima",
      "Norimichi Ukita"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Abdelsalam_GePSAn_Generative_Procedure_Step_Anticipation_in_Cooking_Videos_ICCV_2023_paper.html": {
    "title": "GePSAn: Generative Procedure Step Anticipation in Cooking Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed A. Abdelsalam",
      "Samrudhdhi B. Rangrej",
      "Isma Hadji",
      "Nikita Dvornik",
      "Konstantinos G. Derpanis",
      "Afsaneh Fazly"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Gradient-based_Sampling_for_Class_Imbalanced_Semi-supervised_Object_Detection_ICCV_2023_paper.html": {
    "title": "Gradient-based Sampling for Class Imbalanced Semi-supervised Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Li",
      "Xiangru Lin",
      "Wei Zhang",
      "Xiao Tan",
      "Yingying Li",
      "Junyu Han",
      "Errui Ding",
      "Jingdong Wang",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_SLCA_Slow_Learner_with_Classifier_Alignment_for_Continual_Learning_on_ICCV_2023_paper.html": {
    "title": "SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gengwei Zhang",
      "Liyuan Wang",
      "Guoliang Kang",
      "Ling Chen",
      "Yunchao Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tu_Implicit_Temporal_Modeling_with_Learnable_Alignment_for_Video_Recognition_ICCV_2023_paper.html": {
    "title": "Implicit Temporal Modeling with Learnable Alignment for Video Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyuan Tu",
      "Qi Dai",
      "Zuxuan Wu",
      "Zhi-Qi Cheng",
      "Han Hu",
      "Yu-Gang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cho_Non-Coaxial_Event-Guided_Motion_Deblurring_with_Spatial_Alignment_ICCV_2023_paper.html": {
    "title": "Non-Coaxial Event-Guided Motion Deblurring with Spatial Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoonhee Cho",
      "Yuhwan Jeong",
      "Taewoo Kim",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Quan_Fingerprinting_Deep_Image_Restoration_Models_ICCV_2023_paper.html": {
    "title": "Fingerprinting Deep Image Restoration Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Quan",
      "Huan Teng",
      "Ruotao Xu",
      "Jun Huang",
      "Hui Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_AutoDiffusion_Training-Free_Optimization_of_Time_Steps_and_Architectures_for_Automated_ICCV_2023_paper.html": {
    "title": "AutoDiffusion: Training-Free Optimization of Time Steps and Architectures for Automated Diffusion Model Acceleration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijiang Li",
      "Huixia Li",
      "Xiawu Zheng",
      "Jie Wu",
      "Xuefeng Xiao",
      "Rui Wang",
      "Min Zheng",
      "Xin Pan",
      "Fei Chao",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cui_SportsMOT_A_Large_Multi-Object_Tracking_Dataset_in_Multiple_Sports_Scenes_ICCV_2023_paper.html": {
    "title": "SportsMOT: A Large Multi-Object Tracking Dataset in Multiple Sports Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutao Cui",
      "Chenkai Zeng",
      "Xiaoyu Zhao",
      "Yichun Yang",
      "Gangshan Wu",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Barrios_Localizing_Moments_in_Long_Video_Via_Multimodal_Guidance_ICCV_2023_paper.html": {
    "title": "Localizing Moments in Long Video Via Multimodal Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wayner Barrios",
      "Mattia Soldan",
      "Alberto Mario Ceballos-Arroyo",
      "Fabian Caba Heilbron",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xie_Pixel-Aligned_Recurrent_Queries_for_Multi-View_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Xie",
      "Huaizu Jiang",
      "Georgia Gkioxari",
      "Julian Straub"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ypsilantis_Towards_Universal_Image_Embeddings_A_Large-Scale_Dataset_and_Challenge_for_ICCV_2023_paper.html": {
    "title": "Towards Universal Image Embeddings: A Large-Scale Dataset and Challenge for Generic Image Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolaos-Antonios Ypsilantis",
      "Kaifeng Chen",
      "Bingyi Cao",
      "MÃ¡rio LipovskÃ½",
      "Pelin Dogan-SchÃ¶nberger",
      "Grzegorz Makosa",
      "Boris Bluntschli",
      "Mojtaba Seyedhosseini",
      "OndÅej Chum",
      "AndrÃ© Araujo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_SemARFlow_Injecting_Semantics_into_Unsupervised_Optical_Flow_Estimation_for_Autonomous_ICCV_2023_paper.html": {
    "title": "SemARFlow: Injecting Semantics into Unsupervised Optical Flow Estimation for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Yuan",
      "Shuzhi Yu",
      "Hannah Kim",
      "Carlo Tomasi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kye_TiDAL_Learning_Training_Dynamics_for_Active_Learning_ICCV_2023_paper.html": {
    "title": "TiDAL: Learning Training Dynamics for Active Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seong Min Kye",
      "Kwanghee Choi",
      "Hyeongmin Byun",
      "Buru Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Uncertainty-aware_Unsupervised_Multi-Object_Tracking_ICCV_2023_paper.html": {
    "title": "Uncertainty-aware Unsupervised Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Liu",
      "Sheng Jin",
      "Zhihang Fu",
      "Ze Chen",
      "Rongxin Jiang",
      "Jieping Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tian_DPS-Net_Deep_Polarimetric_Stereo_Depth_Estimation_ICCV_2023_paper.html": {
    "title": "DPS-Net: Deep Polarimetric Stereo Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoran Tian",
      "Weihong Pan",
      "Zimo Wang",
      "Mao Mao",
      "Guofeng Zhang",
      "Hujun Bao",
      "Ping Tan",
      "Zhaopeng Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Designing_Phase_Masks_for_Under-Display_Cameras_ICCV_2023_paper.html": {
    "title": "Designing Phase Masks for Under-Display Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Yang",
      "Eunhee Kang",
      "Hyong-Euk Lee",
      "Aswin C. Sankaranarayanan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ng_Can_Language_Models_Learn_to_Listen_ICCV_2023_paper.html": {
    "title": "Can Language Models Learn to Listen?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evonne Ng",
      "Sanjay Subramanian",
      "Dan Klein",
      "Angjoo Kanazawa",
      "Trevor Darrell",
      "Shiry Ginosar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_SpaceEvo_Hardware-Friendly_Search_Space_Design_for_Efficient_INT8_Inference_ICCV_2023_paper.html": {
    "title": "SpaceEvo: Hardware-Friendly Search Space Design for Efficient INT8 Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Wang",
      "Li Lyna Zhang",
      "Jiahang Xu",
      "Quanlu Zhang",
      "Yujing Wang",
      "Yuqing Yang",
      "Ningxin Zheng",
      "Ting Cao",
      "Mao Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_How_Far_Pre-trained_Models_Are_from_Neural_Collapse_on_the_ICCV_2023_paper.html": {
    "title": "How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Wang",
      "Yadan Luo",
      "Liang Zheng",
      "Zi Huang",
      "Mahsa Baktashmotlagh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mani_SurfsUP_Learning_Fluid_Simulation_for_Novel_Surfaces_ICCV_2023_paper.html": {
    "title": "SurfsUP: Learning Fluid Simulation for Novel Surfaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arjun Mani",
      "Ishaan Preetam Chandratreya",
      "Elliot Creager",
      "Carl Vondrick",
      "Richard Zemel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kirchmeyer_Convolutional_Networks_with_Oriented_1D_Kernels_ICCV_2023_paper.html": {
    "title": "Convolutional Networks with Oriented 1D Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Kirchmeyer",
      "Jia Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Regularized_Mask_Tuning_Uncovering_Hidden_Knowledge_in_Pre-Trained_Vision-Language_Models_ICCV_2023_paper.html": {
    "title": "Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-Trained Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kecheng Zheng",
      "Wei Wu",
      "Ruili Feng",
      "Kai Zhu",
      "Jiawei Liu",
      "Deli Zhao",
      "Zheng-Jun Zha",
      "Wei Chen",
      "Yujun Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Skill_Transformer_A_Monolithic_Policy_for_Mobile_Manipulation_ICCV_2023_paper.html": {
    "title": "Skill Transformer: A Monolithic Policy for Mobile Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Huang",
      "Dhruv Batra",
      "Akshara Rai",
      "Andrew Szot"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Adaptive_and_Background-Aware_Vision_Transformer_for_Real-Time_UAV_Tracking_ICCV_2023_paper.html": {
    "title": "Adaptive and Background-Aware Vision Transformer for Real-Time UAV Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuiwang Li",
      "Yangxiang Yang",
      "Dan Zeng",
      "Xucheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Improving_Pixel-based_MIM_by_Reducing_Wasted_Modeling_Capability_ICCV_2023_paper.html": {
    "title": "Improving Pixel-based MIM by Reducing Wasted Modeling Capability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Liu",
      "Songyang Zhang",
      "Jiacheng Chen",
      "Zhaohui Yu",
      "Kai Chen",
      "Dahua Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Meng_Towards_Memory-_and_Time-Efficient_Backpropagation_for_Training_Spiking_Neural_Networks_ICCV_2023_paper.html": {
    "title": "Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyan Meng",
      "Mingqing Xiao",
      "Shen Yan",
      "Yisen Wang",
      "Zhouchen Lin",
      "Zhi-Quan Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tran_Persistent-Transient_Duality_A_Multi-Mechanism_Approach_for_Modeling_Human-Object_Interaction_ICCV_2023_paper.html": {
    "title": "Persistent-Transient Duality: A Multi-Mechanism Approach for Modeling Human-Object Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hung Tran",
      "Vuong Le",
      "Svetha Venkatesh",
      "Truyen Tran"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hou_When_to_Learn_What_Model-Adaptive_Data_Augmentation_Curriculum_ICCV_2023_paper.html": {
    "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengkai Hou",
      "Jieyu Zhang",
      "Tianyi Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Holmquist_DiffPose_Multi-hypothesis_Human_Pose_Estimation_using_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karl Holmquist",
      "Bastian Wandt"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hong_AesPA-Net_Aesthetic_Pattern-Aware_Style_Transfer_Networks_ICCV_2023_paper.html": {
    "title": "AesPA-Net: Aesthetic Pattern-Aware Style Transfer Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kibeom Hong",
      "Seogkyu Jeon",
      "Junsoo Lee",
      "Namhyuk Ahn",
      "Kunhee Kim",
      "Pilhyeon Lee",
      "Daesik Kim",
      "Youngjung Uh",
      "Hyeran Byun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pan_COPILOT_Human-Environment_Collision_Prediction_and_Localization_from_Egocentric_Videos_ICCV_2023_paper.html": {
    "title": "COPILOT: Human-Environment Collision Prediction and Localization from Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boxiao Pan",
      "Bokui Shen",
      "Davis Rempe",
      "Despoina Paschalidou",
      "Kaichun Mo",
      "Yanchao Yang",
      "Leonidas J. Guibas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yun_EGformer_Equirectangular_Geometry-biased_Transformer_for_360_Depth_Estimation_ICCV_2023_paper.html": {
    "title": "EGformer: Equirectangular Geometry-biased Transformer for 360 Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilwi Yun",
      "Chanyong Shin",
      "Hyunku Lee",
      "Hyuk-Jae Lee",
      "Chae Eun Rhee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Size_Does_Matter_Size-aware_Virtual_Try-on_via_Clothing-oriented_Transformation_Try-on_ICCV_2023_paper.html": {
    "title": "Size Does Matter: Size-aware Virtual Try-on via Clothing-oriented Transformation Try-on Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chieh-Yun Chen",
      "Yi-Chung Chen",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Generating_Realistic_Images_from_In-the-wild_Sounds_ICCV_2023_paper.html": {
    "title": "Generating Realistic Images from In-the-wild Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taegyeong Lee",
      "Jeonghun Kang",
      "Hyeonyu Kim",
      "Taehwan Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Miao_DDS2M_Self-Supervised_Denoising_Diffusion_Spatio-Spectral_Model_for_Hyperspectral_Image_Restoration_ICCV_2023_paper.html": {
    "title": "DDS2M: Self-Supervised Denoising Diffusion Spatio-Spectral Model for Hyperspectral Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchun Miao",
      "Lefei Zhang",
      "Liangpei Zhang",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_Candidate-aware_Selective_Disambiguation_Based_On_Normalized_Entropy_for_Instance-dependent_Partial-label_ICCV_2023_paper.html": {
    "title": "Candidate-aware Selective Disambiguation Based On Normalized Entropy for Instance-dependent Partial-label Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo He",
      "Guowu Yang",
      "Lei Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ko_Open-vocabulary_Video_Question_Answering_A_New_Benchmark_for_Evaluating_the_ICCV_2023_paper.html": {
    "title": "Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dohwan Ko",
      "Ji Soo Lee",
      "Miso Choi",
      "Jaewon Chu",
      "Jihwan Park",
      "Hyunwoo J. Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Puy_Using_a_Waffle_Iron_for_Automotive_Point_Cloud_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gilles Puy",
      "Alexandre Boulch",
      "Renaud Marlet"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Peng_AutoReP_Automatic_ReLU_Replacement_for_Fast_Private_Network_Inference_ICCV_2023_paper.html": {
    "title": "AutoReP: Automatic ReLU Replacement for Fast Private Network Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongwu Peng",
      "Shaoyi Huang",
      "Tong Zhou",
      "Yukui Luo",
      "Chenghong Wang",
      "Zigeng Wang",
      "Jiahui Zhao",
      "Xi Xie",
      "Ang Li",
      "Tony Geng",
      "Kaleel Mahmood",
      "Wujie Wen",
      "Xiaolin Xu",
      "Caiwen Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Seff_MotionLM_Multi-Agent_Motion_Forecasting_as_Language_Modeling_ICCV_2023_paper.html": {
    "title": "MotionLM: Multi-Agent Motion Forecasting as Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ari Seff",
      "Brian Cera",
      "Dian Chen",
      "Mason Ng",
      "Aurick Zhou",
      "Nigamaa Nayakanti",
      "Khaled S. Refaat",
      "Rami Al-Rfou",
      "Benjamin Sapp"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.html": {
    "title": "Black Box Few-Shot Adaptation for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yassine Ouali",
      "Adrian Bulat",
      "Brais Matinez",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Center-Based_Decoupled_Point-cloud_Registration_for_6D_Object_Pose_Estimation_ICCV_2023_paper.html": {
    "title": "Center-Based Decoupled Point-cloud Registration for 6D Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobo Jiang",
      "Zheng Dang",
      "Shuo Gu",
      "Jin Xie",
      "Mathieu Salzmann",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Self-Ordering_Point_Clouds_ICCV_2023_paper.html": {
    "title": "Self-Ordering Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengwan Yang",
      "Cees G. M. Snoek",
      "Yuki M. Asano"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ji_Continual_Segment_Towards_a_Single_Unified_and_Non-forgetting_Continual_Segmentation_ICCV_2023_paper.html": {
    "title": "Continual Segment: Towards a Single, Unified and Non-forgetting Continual Segmentation Model of 143 Whole-body Organs in CT Scans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanghexuan Ji",
      "Dazhou Guo",
      "Puyang Wang",
      "Ke Yan",
      "Le Lu",
      "Minfeng Xu",
      "Qifeng Wang",
      "Jia Ge",
      "Mingchen Gao",
      "Xianghua Ye",
      "Dakai Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Konwer_Enhancing_Modality-Agnostic_Representations_via_Meta-Learning_for_Brain_Tumor_Segmentation_ICCV_2023_paper.html": {
    "title": "Enhancing Modality-Agnostic Representations via Meta-Learning for Brain Tumor Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aishik Konwer",
      "Xiaoling Hu",
      "Joseph Bae",
      "Xuan Xu",
      "Chao Chen",
      "Prateek Prasanna"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Zero-1-to-3_Zero-shot_One_Image_to_3D_Object_ICCV_2023_paper.html": {
    "title": "Zero-1-to-3: Zero-shot One Image to 3D Object",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoshi Liu",
      "Rundi Wu",
      "Basile Van Hoorick",
      "Pavel Tokmakov",
      "Sergey Zakharov",
      "Carl Vondrick"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shi_3D_Distillation_Improving_Self-Supervised_Monocular_Depth_Estimation_on_Reflective_Surfaces_ICCV_2023_paper.html": {
    "title": "3D Distillation: Improving Self-Supervised Monocular Depth Estimation on Reflective Surfaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuepeng Shi",
      "Georgi Dikov",
      "Gerhard Reitmayr",
      "Tae-Kyun Kim",
      "Mohsen Ghafoorian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xie_GAIT_Generating_Aesthetic_Indoor_Tours_with_Deep_Reinforcement_Learning_ICCV_2023_paper.html": {
    "title": "GAIT: Generating Aesthetic Indoor Tours with Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Desai Xie",
      "Ping Hu",
      "Xin Sun",
      "Soren Pirk",
      "Jianming Zhang",
      "Radomir Mech",
      "Arie E. Kaufman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Low-Light_Image_Enhancement_with_Multi-Stage_Residue_Quantization_and_Brightness-Aware_Attention_ICCV_2023_paper.html": {
    "title": "Low-Light Image Enhancement with Multi-Stage Residue Quantization and Brightness-Aware Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlong Liu",
      "Tao Huang",
      "Weisheng Dong",
      "Fangfang Wu",
      "Xin Li",
      "Guangming Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Hierarchically_Decomposed_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_ICCV_2023_paper.html": {
    "title": "Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungho Lee",
      "Minhyeok Lee",
      "Dogyoon Lee",
      "Sangyoun Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Arshad_LIST_Learning_Implicitly_from_Spatial_Transformers_for_Single-View_3D_Reconstruction_ICCV_2023_paper.html": {
    "title": "LIST: Learning Implicitly from Spatial Transformers for Single-View 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Samiul Arshad",
      "William J. Beksi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Rethinking_Mobile_Block_for_Efficient_Attention-based_Models_ICCV_2023_paper.html": {
    "title": "Rethinking Mobile Block for Efficient Attention-based Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangning Zhang",
      "Xiangtai Li",
      "Jian Li",
      "Liang Liu",
      "Zhucun Xue",
      "Boshen Zhang",
      "Zhengkai Jiang",
      "Tianxin Huang",
      "Yabiao Wang",
      "Chengjie Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hingun_REAP_A_Large-Scale_Realistic_Adversarial_Patch_Benchmark_ICCV_2023_paper.html": {
    "title": "REAP: A Large-Scale Realistic Adversarial Patch Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nabeel Hingun",
      "Chawin Sitawarin",
      "Jerry Li",
      "David Wagner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_LRRU_Long-short_Range_Recurrent_Updating_Networks_for_Depth_Completion_ICCV_2023_paper.html": {
    "title": "LRRU: Long-short Range Recurrent Updating Networks for Depth Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Wang",
      "Bo Li",
      "Ge Zhang",
      "Qi Liu",
      "Tao Gao",
      "Yuchao Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ge_MetaBEV_Solving_Sensor_Failures_for_3D_Detection_and_Map_Segmentation_ICCV_2023_paper.html": {
    "title": "MetaBEV: Solving Sensor Failures for 3D Detection and Map Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongjian Ge",
      "Junsong Chen",
      "Enze Xie",
      "Zhongdao Wang",
      "Lanqing Hong",
      "Huchuan Lu",
      "Zhenguo Li",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_DNA-Rendering_A_Diverse_Neural_Actor_Repository_for_High-Fidelity_Human-Centric_Rendering_ICCV_2023_paper.html": {
    "title": "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-Centric Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Cheng",
      "Ruixiang Chen",
      "Siming Fan",
      "Wanqi Yin",
      "Keyu Chen",
      "Zhongang Cai",
      "Jingbo Wang",
      "Yang Gao",
      "Zhengming Yu",
      "Zhengyu Lin",
      "Daxuan Ren",
      "Lei Yang",
      "Ziwei Liu",
      "Chen Change Loy",
      "Chen Qian",
      "Wayne Wu",
      "Dahua Lin",
      "Bo Dai",
      "Kwan-Yee Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Exploring_Temporal_Concurrency_for_Video-Language_Representation_Learning_ICCV_2023_paper.html": {
    "title": "Exploring Temporal Concurrency for Video-Language Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Zhang",
      "Daqing Liu",
      "Zezhong Lv",
      "Bing Su",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_StegaNeRF_Embedding_Invisible_Information_within_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "StegaNeRF: Embedding Invisible Information within Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxin Li",
      "Brandon Y. Feng",
      "Zhiwen Fan",
      "Panwang Pan",
      "Zhangyang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yoshimura_DynamicISP_Dynamically_Controlled_Image_Signal_Processor_for_Image_Recognition_ICCV_2023_paper.html": {
    "title": "DynamicISP: Dynamically Controlled Image Signal Processor for Image Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masakazu Yoshimura",
      "Junji Otsuka",
      "Atsushi Irie",
      "Takeshi Ohashi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Choi_R-Pred_Two-Stage_Motion_Prediction_Via_Tube-Query_Attention-Based_Trajectory_Refinement_ICCV_2023_paper.html": {
    "title": "R-Pred: Two-Stage Motion Prediction Via Tube-Query Attention-Based Trajectory Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sehwan Choi",
      "Jungho Kim",
      "Junyong Yun",
      "Jun Won Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pintea_A_step_towards_understanding_why_classification_helps_regression_ICCV_2023_paper.html": {
    "title": "A step towards understanding why classification helps regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Silvia L. Pintea",
      "Yancong Lin",
      "Jouke Dijkstra",
      "Jan C. van Gemert"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Robust_Evaluation_of_Diffusion-Based_Adversarial_Purification_ICCV_2023_paper.html": {
    "title": "Robust Evaluation of Diffusion-Based Adversarial Purification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minjong Lee",
      "Dongwoo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hong_Hyperbolic_Audio-visual_Zero-shot_Learning_ICCV_2023_paper.html": {
    "title": "Hyperbolic Audio-visual Zero-shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Hong",
      "Zeeshan Hayder",
      "Junlin Han",
      "Pengfei Fang",
      "Mehrtash Harandi",
      "Lars Petersson"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_CTPTowards_Vision-Language_Continual_Pretraining_via_Compatible_Momentum_Contrast_and_Topology_ICCV_2023_paper.html": {
    "title": "CTP:Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongguang Zhu",
      "Yunchao Wei",
      "Xiaodan Liang",
      "Chunjie Zhang",
      "Yao Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Aggregating_Feature_Point_Cloud_for_Depth_Completion_ICCV_2023_paper.html": {
    "title": "Aggregating Feature Point Cloud for Depth Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhu Yu",
      "Zehua Sheng",
      "Zili Zhou",
      "Lun Luo",
      "Si-Yuan Cao",
      "Hong Gu",
      "Huaqi Zhang",
      "Hui-Liang Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Srivatsan_FLIP_Cross-domain_Face_Anti-spoofing_with_Language_Guidance_ICCV_2023_paper.html": {
    "title": "FLIP: Cross-domain Face Anti-spoofing with Language Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Koushik Srivatsan",
      "Muzammal Naseer",
      "Karthik Nandakumar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Distribution_Shift_Matters_for_Knowledge_Distillation_with_Webly_Collected_Images_ICCV_2023_paper.html": {
    "title": "Distribution Shift Matters for Knowledge Distillation with Webly Collected Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialiang Tang",
      "Shuo Chen",
      "Gang Niu",
      "Masashi Sugiyama",
      "Chen Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Reconstructed_Convolution_Module_Based_Look-Up_Tables_for_Efficient_Image_Super-Resolution_ICCV_2023_paper.html": {
    "title": "Reconstructed Convolution Module Based Look-Up Tables for Efficient Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guandu Liu",
      "Yukang Ding",
      "Mading Li",
      "Ming Sun",
      "Xing Wen",
      "Bin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shao_Action_Sensitivity_Learning_for_Temporal_Action_Localization_ICCV_2023_paper.html": {
    "title": "Action Sensitivity Learning for Temporal Action Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Shao",
      "Xiaohan Wang",
      "Ruijie Quan",
      "Junjun Zheng",
      "Jiang Yang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qiu_Gram-based_Attentive_Neural_Ordinary_Differential_Equations_Network_for_Video_Nystagmography_ICCV_2023_paper.html": {
    "title": "Gram-based Attentive Neural Ordinary Differential Equations Network for Video Nystagmography Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xihe Qiu",
      "Shaojie Shi",
      "Xiaoyu Tan",
      "Chao Qu",
      "Zhijun Fang",
      "Hailing Wang",
      "Yongbin Gao",
      "Peixia Wu",
      "Huawei Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_PEANUT_Predicting_and_Navigating_to_Unseen_Targets_ICCV_2023_paper.html": {
    "title": "PEANUT: Predicting and Navigating to Unseen Targets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Albert J. Zhai",
      "Shenlong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Pluralistic_Aging_Diffusion_Autoencoder_ICCV_2023_paper.html": {
    "title": "Pluralistic Aging Diffusion Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peipei Li",
      "Rui Wang",
      "Huaibo Huang",
      "Ran He",
      "Zhaofeng He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Song_ModelGiF_Gradient_Fields_for_Model_Functional_Distance_ICCV_2023_paper.html": {
    "title": "ModelGiF: Gradient Fields for Model Functional Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Song",
      "Zhengqi Xu",
      "Sai Wu",
      "Gang Chen",
      "Mingli Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_PoseDiffusion_Solving_Pose_Estimation_via_Diffusion-aided_Bundle_Adjustment_ICCV_2023_paper.html": {
    "title": "PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyuan Wang",
      "Christian Rupprecht",
      "David Novotny"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_TIFA_Accurate_and_Interpretable_Text-to-Image_Faithfulness_Evaluation_with_Question_Answering_ICCV_2023_paper.html": {
    "title": "TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushi Hu",
      "Benlin Liu",
      "Jungo Kasai",
      "Yizhong Wang",
      "Mari Ostendorf",
      "Ranjay Krishna",
      "Noah A. Smith"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_SIGMA_Scale-Invariant_Global_Sparse_Shape_Matching_ICCV_2023_paper.html": {
    "title": "SIGMA: Scale-Invariant Global Sparse Shape Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maolin Gao",
      "Paul Roetzer",
      "Marvin Eisenberger",
      "Zorah LÃ¤hner",
      "Michael Moeller",
      "Daniel Cremers",
      "Florian Bernard"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_CORE_Cooperative_Reconstruction_for_Multi-Agent_Perception_ICCV_2023_paper.html": {
    "title": "CORE: Cooperative Reconstruction for Multi-Agent Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binglu Wang",
      "Lei Zhang",
      "Zhaozhong Wang",
      "Yongqiang Zhao",
      "Tianfei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ali_VidStyleODE_Disentangled_Video_Editing_via_StyleGAN_and_NeuralODEs_ICCV_2023_paper.html": {
    "title": "VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moayed Haji Ali",
      "Andrew Bond",
      "Tolga Birdal",
      "Duygu Ceylan",
      "Levent Karacan",
      "Erkut Erdem",
      "Aykut Erdem"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_SEFD_Learning_to_Distill_Complex_Pose_and_Occlusion_ICCV_2023_paper.html": {
    "title": "SEFD: Learning to Distill Complex Pose and Occlusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ChangHee Yang",
      "Kyeongbo Kong",
      "SungJun Min",
      "Dongyoon Wee",
      "Ho-Deok Jang",
      "Geonho Cha",
      "SukJu Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_CiT_Curation_in_Training_for_Effective_Vision-Language_Data_ICCV_2023_paper.html": {
    "title": "CiT: Curation in Training for Effective Vision-Language Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hu Xu",
      "Saining Xie",
      "Po-Yao Huang",
      "Licheng Yu",
      "Russell Howes",
      "Gargi Ghosh",
      "Luke Zettlemoyer",
      "Christoph Feichtenhofer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_SparseNeRF_Distilling_Depth_Ranking_for_Few-shot_Novel_View_Synthesis_ICCV_2023_paper.html": {
    "title": "SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangcong Wang",
      "Zhaoxi Chen",
      "Chen Change Loy",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ganz_Towards_Models_that_Can_See_and_Read_ICCV_2023_paper.html": {
    "title": "Towards Models that Can See and Read",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roy Ganz",
      "Oren Nuriel",
      "Aviad Aberdam",
      "Yair Kittenplon",
      "Shai Mazor",
      "Ron Litman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_ProPainter_Improving_Propagation_and_Transformer_for_Video_Inpainting_ICCV_2023_paper.html": {
    "title": "ProPainter: Improving Propagation and Transformer for Video Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangchen Zhou",
      "Chongyi Li",
      "Kelvin C.K. Chan",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lu_Query_Refinement_Transformer_for_3D_Instance_Segmentation_ICCV_2023_paper.html": {
    "title": "Query Refinement Transformer for 3D Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Lu",
      "Jiacheng Deng",
      "Chuxin Wang",
      "Jianfeng He",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Root_Pose_Decomposition_Towards_Generic_Non-rigid_3D_Reconstruction_with_Monocular_ICCV_2023_paper.html": {
    "title": "Root Pose Decomposition Towards Generic Non-rigid 3D Reconstruction with Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikai Wang",
      "Yinpeng Dong",
      "Fuchun Sun",
      "Xiao Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_3DHumanGAN_3D-Aware_Human_Image_Generation_with_3D_Pose_Mapping_ICCV_2023_paper.html": {
    "title": "3DHumanGAN: 3D-Aware Human Image Generation with 3D Pose Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoqian Yang",
      "Shikai Li",
      "Wayne Wu",
      "Bo Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_LeaF_Learning_Frames_for_4D_Point_Cloud_Sequence_Understanding_ICCV_2023_paper.html": {
    "title": "LeaF: Learning Frames for 4D Point Cloud Sequence Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunze Liu",
      "Junyu Chen",
      "Zekai Zhang",
      "Jingwei Huang",
      "Li Yi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_GLA-GCN_Global-local_Adaptive_Graph_Convolutional_Network_for_3D_Human_Pose_ICCV_2023_paper.html": {
    "title": "GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bruce X.B. Yu",
      "Zhi Zhang",
      "Yongxu Liu",
      "Sheng-hua Zhong",
      "Yan Liu",
      "Chang Wen Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Snow_Removal_in_Video_A_New_Dataset_and_A_Novel_ICCV_2023_paper.html": {
    "title": "Snow Removal in Video: A New Dataset and A Novel Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Chen",
      "Jingjing Ren",
      "Jinjin Gu",
      "Hongtao Wu",
      "Xuequan Lu",
      "Haoming Cai",
      "Lei Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_Degradation-Resistant_Unfolding_Network_for_Heterogeneous_Image_Fusion_ICCV_2023_paper.html": {
    "title": "Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunming He",
      "Kai Li",
      "Guoxia Xu",
      "Yulun Zhang",
      "Runze Hu",
      "Zhenhua Guo",
      "Xiu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kong_Priority-Centric_Human_Motion_Generation_in_Discrete_Latent_Space_ICCV_2023_paper.html": {
    "title": "Priority-Centric Human Motion Generation in Discrete Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Kong",
      "Kehong Gong",
      "Dongze Lian",
      "Michael Bi Mi",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sanyal_Domain-Specificity_Inducing_Transformers_for_Source-Free_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "Domain-Specificity Inducing Transformers for Source-Free Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunandini Sanyal",
      "Ashish Ramayee Asokan",
      "Suvaansh Bhambri",
      "Akshay Kulkarni",
      "Jogendra Nath Kundu",
      "R Venkatesh Babu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Balasubramanian_Towards_Improved_Input_Masking_for_Convolutional_Neural_Networks_ICCV_2023_paper.html": {
    "title": "Towards Improved Input Masking for Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sriram Balasubramanian",
      "Soheil Feizi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tao_3DHacker_Spectrum-based_Decision_Boundary_Generation_for_Hard-label_3D_Point_Cloud_ICCV_2023_paper.html": {
    "title": "3DHacker: Spectrum-based Decision Boundary Generation for Hard-label 3D Point Cloud Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunbo Tao",
      "Daizong Liu",
      "Pan Zhou",
      "Yulai Xie",
      "Wei Du",
      "Wei Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kang_Exploring_Lightweight_Hierarchical_Vision_Transformers_for_Efficient_Visual_Tracking_ICCV_2023_paper.html": {
    "title": "Exploring Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Kang",
      "Xin Chen",
      "Dong Wang",
      "Houwen Peng",
      "Huchuan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/An_MiniROAD_Minimal_RNN_Framework_for_Online_Action_Detection_ICCV_2023_paper.html": {
    "title": "MiniROAD: Minimal RNN Framework for Online Action Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joungbin An",
      "Hyolim Kang",
      "Su Ho Han",
      "Ming-Hsuan Yang",
      "Seon Joo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gan_Efficient_Emotional_Adaptation_for_Audio-Driven_Talking-Head_Generation_ICCV_2023_paper.html": {
    "title": "Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Gan",
      "Zongxin Yang",
      "Xihang Yue",
      "Lingyun Sun",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tonini_Object-aware_Gaze_Target_Detection_ICCV_2023_paper.html": {
    "title": "Object-aware Gaze Target Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Tonini",
      "Nicola Dall'Asen",
      "Cigdem Beyan",
      "Elisa Ricci"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ryu_Gramian_Attention_Heads_are_Strong_yet_Efficient_Vision_Learners_ICCV_2023_paper.html": {
    "title": "Gramian Attention Heads are Strong yet Efficient Vision Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongbin Ryu",
      "Dongyoon Han",
      "Jongwoo Lim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Black_VADER_Video_Alignment_Differencing_and_Retrieval_ICCV_2023_paper.html": {
    "title": "VADER: Video Alignment Differencing and Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Black",
      "Simon Jenni",
      "Tu Bui",
      "Md. Mehrab Tanjim",
      "Stefano Petrangeli",
      "Ritwik Sinha",
      "Viswanathan Swaminathan",
      "John Collomosse"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sargsyan_MI-GAN_A_Simple_Baseline_for_Image_Inpainting_on_Mobile_Devices_ICCV_2023_paper.html": {
    "title": "MI-GAN: A Simple Baseline for Image Inpainting on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andranik Sargsyan",
      "Shant Navasardyan",
      "Xingqian Xu",
      "Humphrey Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_HiLo_Exploiting_High_Low_Frequency_Relations_for_Unbiased_Panoptic_Scene_ICCV_2023_paper.html": {
    "title": "HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Zhou",
      "Miaojing Shi",
      "Holger Caesar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Saini_Chop__Learn_Recognizing_and_Generating_Object-State_Compositions_ICCV_2023_paper.html": {
    "title": "Chop & Learn: Recognizing and Generating Object-State Compositions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nirat Saini",
      "Hanyu Wang",
      "Archana Swaminathan",
      "Vinoj Jayasundara",
      "Bo He",
      "Kamal Gupta",
      "Abhinav Shrivastava"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xiao_Automatic_Animation_of_Hair_Blowing_in_Still_Portrait_Photos_ICCV_2023_paper.html": {
    "title": "Automatic Animation of Hair Blowing in Still Portrait Photos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenpeng Xiao",
      "Wentao Liu",
      "Yitong Wang",
      "Bernard Ghanem",
      "Bing Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lu_A_Large-Scale_Outdoor_Multi-Modal_Dataset_and_Benchmark_for_Novel_View_ICCV_2023_paper.html": {
    "title": "A Large-Scale Outdoor Multi-Modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongshan Lu",
      "Fukun Yin",
      "Xin Chen",
      "Wen Liu",
      "Tao Chen",
      "Gang Yu",
      "Jiayuan Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_4D_Panoptic_Segmentation_as_Invariant_and_Equivariant_Field_Prediction_ICCV_2023_paper.html": {
    "title": "4D Panoptic Segmentation as Invariant and Equivariant Field Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghan Zhu",
      "Shizhong Han",
      "Hong Cai",
      "Shubhankar Borse",
      "Maani Ghaffari",
      "Fatih Porikli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fang_Unleashing_Vanilla_Vision_Transformer_with_Masked_Image_Modeling_for_Object_ICCV_2023_paper.html": {
    "title": "Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Fang",
      "Shusheng Yang",
      "Shijie Wang",
      "Yixiao Ge",
      "Ying Shan",
      "Xinggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yao_NDC-Scene_Boost_Monocular_3D_Semantic_Scene_Completion_in_Normalized_Device_ICCV_2023_paper.html": {
    "title": "NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Yao",
      "Chuming Li",
      "Keqiang Sun",
      "Yingjie Cai",
      "Hao Li",
      "Wanli Ouyang",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sameni_Spatio-Temporal_Crop_Aggregation_for_Video_Representation_Learning_ICCV_2023_paper.html": {
    "title": "Spatio-Temporal Crop Aggregation for Video Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sepehr Sameni",
      "Simon Jenni",
      "Paolo Favaro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Barron_Zip-NeRF_Anti-Aliased_Grid-Based_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan T. Barron",
      "Ben Mildenhall",
      "Dor Verbin",
      "Pratul P. Srinivasan",
      "Peter Hedman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Neural-PBIR_Reconstruction_of_Shape_Material_and_Illumination_ICCV_2023_paper.html": {
    "title": "Neural-PBIR Reconstruction of Shape, Material, and Illumination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Sun",
      "Guangyan Cai",
      "Zhengqin Li",
      "Kai Yan",
      "Cheng Zhang",
      "Carl Marshall",
      "Jia-Bin Huang",
      "Shuang Zhao",
      "Zhao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Fg-T2M_Fine-Grained_Text-Driven_Human_Motion_Generation_via_Diffusion_Model_ICCV_2023_paper.html": {
    "title": "Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yin Wang",
      "Zhiying Leng",
      "Frederick W. B. Li",
      "Shun-Cheng Wu",
      "Xiaohui Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jeong_BlindHarmony_Blind_Harmonization_for_MR_Images_via_Flow_Model_ICCV_2023_paper.html": {
    "title": "BlindHarmony: \"Blind\" Harmonization for MR Images via Flow Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hwihun Jeong",
      "Heejoon Byun",
      "Dong Un Kang",
      "Jongho Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Rewatbowornwong_Zero-guidance_Segmentation_Using_Zero_Segment_Labels_ICCV_2023_paper.html": {
    "title": "Zero-guidance Segmentation Using Zero Segment Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pitchaporn Rewatbowornwong",
      "Nattanat Chatthee",
      "Ekapol Chuangsuwanich",
      "Supasorn Suwajanakorn"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hui_Efficient_LiDAR_Point_Cloud_Oversegmentation_Network_ICCV_2023_paper.html": {
    "title": "Efficient LiDAR Point Cloud Oversegmentation Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Hui",
      "Linghua Tang",
      "Yuchao Dai",
      "Jin Xie",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Communication-efficient_Federated_Learning_with_Single-Step_Synthetic_Features_Compressor_for_Faster_ICCV_2023_paper.html": {
    "title": "Communication-efficient Federated Learning with Single-Step Synthetic Features Compressor for Faster Convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Zhou",
      "Mingjia Shi",
      "Yuanxi Li",
      "Yanan Sun",
      "Qing Ye",
      "Jiancheng Lv"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_SVDFormer_Complementing_Point_Cloud_via_Self-view_Augmentation_and_Self-structure_Dual-generator_ICCV_2023_paper.html": {
    "title": "SVDFormer: Complementing Point Cloud via Self-view Augmentation and Self-structure Dual-generator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Zhu",
      "Honghua Chen",
      "Xing He",
      "Weiming Wang",
      "Jing Qin",
      "Mingqiang Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xia_Few-Shot_Video_Classification_via_Representation_Fusion_and_Promotion_Learning_ICCV_2023_paper.html": {
    "title": "Few-Shot Video Classification via Representation Fusion and Promotion Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haifeng Xia",
      "Kai Li",
      "Martin Renqiang Min",
      "Zhengming Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_E3Sym_Leveraging_E3_Invariance_for_Unsupervised_3D_Planar_Reflective_Symmetry_ICCV_2023_paper.html": {
    "title": "E3Sym: Leveraging E(3) Invariance for Unsupervised 3D Planar Reflective Symmetry Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ren-Wu Li",
      "Ling-Xiao Zhang",
      "Chunpeng Li",
      "Yu-Kun Lai",
      "Lin Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ying_CTVIS_Consistent_Training_for_Online_Video_Instance_Segmentation_ICCV_2023_paper.html": {
    "title": "CTVIS: Consistent Training for Online Video Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaining Ying",
      "Qing Zhong",
      "Weian Mao",
      "Zhenhua Wang",
      "Hao Chen",
      "Lin Yuanbo Wu",
      "Yifan Liu",
      "Chengxiang Fan",
      "Yunzhi Zhuge",
      "Chunhua Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Su_Unsupervised_Video_Object_Segmentation_with_Online_Adversarial_Self-Tuning_ICCV_2023_paper.html": {
    "title": "Unsupervised Video Object Segmentation with Online Adversarial Self-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiankang Su",
      "Huihui Song",
      "Dong Liu",
      "Bo Liu",
      "Qingshan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Hallucination_Improves_the_Performance_of_Unsupervised_Visual_Representation_Learning_ICCV_2023_paper.html": {
    "title": "Hallucination Improves the Performance of Unsupervised Visual Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wu",
      "Jennifer Hobbs",
      "Naira Hovakimyan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xie_S3IM_Stochastic_Structural_SIMilarity_and_Its_Unreasonable_Effectiveness_for_Neural_ICCV_2023_paper.html": {
    "title": "S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeke Xie",
      "Xindi Yang",
      "Yujie Yang",
      "Qi Sun",
      "Yixiang Jiang",
      "Haoran Wang",
      "Yunfeng Cai",
      "Mingming Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_GlobalMapper_Arbitrary-Shaped_Urban_Layout_Generation_ICCV_2023_paper.html": {
    "title": "GlobalMapper: Arbitrary-Shaped Urban Layout Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liu He",
      "Daniel Aliaga"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Membrane_Potential_Batch_Normalization_for_Spiking_Neural_Networks_ICCV_2023_paper.html": {
    "title": "Membrane Potential Batch Normalization for Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Guo",
      "Yuhan Zhang",
      "Yuanpei Chen",
      "Weihang Peng",
      "Xiaode Liu",
      "Liwen Zhang",
      "Xuhui Huang",
      "Zhe Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gui_Enhancing_Sample_Utilization_through_Sample_Adaptive_Augmentation_in_Semi-Supervised_Learning_ICCV_2023_paper.html": {
    "title": "Enhancing Sample Utilization through Sample Adaptive Augmentation in Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guan Gui",
      "Zhen Zhao",
      "Lei Qi",
      "Luping Zhou",
      "Lei Wang",
      "Yinghuan Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Thambiraja_Imitator_Personalized_Speech-driven_3D_Facial_Animation_ICCV_2023_paper.html": {
    "title": "Imitator: Personalized Speech-driven 3D Facial Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Balamurugan Thambiraja",
      "Ikhsanul Habibie",
      "Sadegh Aliakbarian",
      "Darren Cosker",
      "Christian Theobalt",
      "Justus Thies"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Unified_Coarse-to-Fine_Alignment_for_Video-Text_Retrieval_ICCV_2023_paper.html": {
    "title": "Unified Coarse-to-Fine Alignment for Video-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Wang",
      "Yi-Lin Sung",
      "Feng Cheng",
      "Gedas Bertasius",
      "Mohit Bansal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Seeing_Beyond_the_Patch_Scale-Adaptive_Semantic_Segmentation_of_High-resolution_Remote_ICCV_2023_paper.html": {
    "title": "Seeing Beyond the Patch: Scale-Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery based on Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinhe Liu",
      "Sunan Shi",
      "Junjue Wang",
      "Yanfei Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Gradient-Regulated_Meta-Prompt_Learning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.html": {
    "title": "Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juncheng Li",
      "Minghe Gao",
      "Longhui Wei",
      "Siliang Tang",
      "Wenqiao Zhang",
      "Mengze Li",
      "Wei Ji",
      "Qi Tian",
      "Tat-Seng Chua",
      "Yueting Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Baldrati_Zero-Shot_Composed_Image_Retrieval_with_Textual_Inversion_ICCV_2023_paper.html": {
    "title": "Zero-Shot Composed Image Retrieval with Textual Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alberto Baldrati",
      "Lorenzo Agnolucci",
      "Marco Bertini",
      "Alberto Del Bimbo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_MUter_Machine_Unlearning_on_Adversarially_Trained_Models_ICCV_2023_paper.html": {
    "title": "MUter: Machine Unlearning on Adversarially Trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junxu Liu",
      "Mingsheng Xue",
      "Jian Lou",
      "Xiaoyu Zhang",
      "Li Xiong",
      "Zhan Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Le_Moing_WALDO_Future_Video_Synthesis_Using_Object_Layer_Decomposition_and_Parametric_ICCV_2023_paper.html": {
    "title": "WALDO: Future Video Synthesis Using Object Layer Decomposition and Parametric Flow Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Le Moing",
      "Jean Ponce",
      "Cordelia Schmid"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_ParCNetV2_Oversized_Kernel_with_Enhanced_Attention_ICCV_2023_paper.html": {
    "title": "ParCNetV2: Oversized Kernel with Enhanced Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihan Xu",
      "Haokui Zhang",
      "Wenze Hu",
      "Shiliang Zhang",
      "Xiaoyu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_BiFF_Bi-level_Future_Fusion_with_Polyline-based_Coordinate_for_Interactive_Trajectory_ICCV_2023_paper.html": {
    "title": "BiFF: Bi-level Future Fusion with Polyline-based Coordinate for Interactive Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyao Zhu",
      "Di Luan",
      "Shaojie Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_RealGraph_A_Multiview_Dataset_for_4D_Real-world_Context_Graph_Generation_ICCV_2023_paper.html": {
    "title": "RealGraph: A Multiview Dataset for 4D Real-world Context Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Lin",
      "Zequn Chen",
      "Jinzhi Zhang",
      "Bing Bai",
      "Yu Wang",
      "Ruqi Huang",
      "Lu Fang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ladune_COOL-CHIC_Coordinate-based_Low_Complexity_Hierarchical_Image_Codec_ICCV_2023_paper.html": {
    "title": "COOL-CHIC: Coordinate-based Low Complexity Hierarchical Image Codec",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ThÃ©o Ladune",
      "Pierrick Philippe",
      "FÃ©lix Henry",
      "Gordon Clare",
      "Thomas Leguay"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hirschorn_Normalizing_Flows_for_Human_Pose_Anomaly_Detection_ICCV_2023_paper.html": {
    "title": "Normalizing Flows for Human Pose Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Or Hirschorn",
      "Shai Avidan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Reconstructing_Groups_of_People_with_Hypergraph_Relational_Reasoning_ICCV_2023_paper.html": {
    "title": "Reconstructing Groups of People with Hypergraph Relational Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Buzhen Huang",
      "Jingyi Ju",
      "Zhihao Li",
      "Yangang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ding_PivotNet_Vectorized_Pivot_Learning_for_End-to-end_HD_Map_Construction_ICCV_2023_paper.html": {
    "title": "PivotNet: Vectorized Pivot Learning for End-to-end HD Map Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Ding",
      "Limeng Qiao",
      "Xi Qiu",
      "Chi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Universal_Domain_Adaptation_via_Compressive_Attention_Matching_ICCV_2023_paper.html": {
    "title": "Universal Domain Adaptation via Compressive Attention Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Didi Zhu",
      "Yinchuan Li",
      "Junkun Yuan",
      "Zexi Li",
      "Kun Kuang",
      "Chao Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Contactless_Pulse_Estimation_Leveraging_Pseudo_Labels_and_Self-Supervision_ICCV_2023_paper.html": {
    "title": "Contactless Pulse Estimation Leveraging Pseudo Labels and Self-Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihua Li",
      "Lijun Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Haque_Instruct-NeRF2NeRF_Editing_3D_Scenes_with_Instructions_ICCV_2023_paper.html": {
    "title": "Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayaan Haque",
      "Matthew Tancik",
      "Alexei A. Efros",
      "Aleksander Holynski",
      "Angjoo Kanazawa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Point2Mask_Point-supervised_Panoptic_Segmentation_via_Optimal_Transport_ICCV_2023_paper.html": {
    "title": "Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentong Li",
      "Yuqian Yuan",
      "Song Wang",
      "Jianke Zhu",
      "Jianshu Li",
      "Jian Liu",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Multi-Task_Learning_with_Knowledge_Distillation_for_Dense_Prediction_ICCV_2023_paper.html": {
    "title": "Multi-Task Learning with Knowledge Distillation for Dense Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyang Xu",
      "Yibo Yang",
      "Lefei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pratt_What_Does_a_Platypus_Look_Like_Generating_Customized_Prompts_for_ICCV_2023_paper.html": {
    "title": "What Does a Platypus Look Like? Generating Customized Prompts for Zero-Shot Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Pratt",
      "Ian Covert",
      "Rosanne Liu",
      "Ali Farhadi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tong_Scene_as_Occupancy_ICCV_2023_paper.html": {
    "title": "Scene as Occupancy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenwen Tong",
      "Chonghao Sima",
      "Tai Wang",
      "Li Chen",
      "Silei Wu",
      "Hanming Deng",
      "Yi Gu",
      "Lewei Lu",
      "Ping Luo",
      "Dahua Lin",
      "Hongyang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Di_U-RED_Unsupervised_3D_Shape_Retrieval_and_Deformation_for_Partial_Point_ICCV_2023_paper.html": {
    "title": "U-RED: Unsupervised 3D Shape Retrieval and Deformation for Partial Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Di",
      "Chenyangguang Zhang",
      "Ruida Zhang",
      "Fabian Manhardt",
      "Yongzhi Su",
      "Jason Rambach",
      "Didier Stricker",
      "Xiangyang Ji",
      "Federico Tombari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_RFLA_A_Stealthy_Reflected_Light_Adversarial_Attack_in_the_Physical_ICCV_2023_paper.html": {
    "title": "RFLA: A Stealthy Reflected Light Adversarial Attack in the Physical World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghua Wang",
      "Wen Yao",
      "Tingsong Jiang",
      "Chao Li",
      "Xiaoqian Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Park_Nearest_Neighbor_Guidance_for_Out-of-Distribution_Detection_ICCV_2023_paper.html": {
    "title": "Nearest Neighbor Guidance for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewoo Park",
      "Yoon Gyo Jung",
      "Andrew Beng Jin Teoh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_PatchCT_Aligning_Patch_Set_and_Label_Set_with_Conditional_Transport_ICCV_2023_paper.html": {
    "title": "PatchCT: Aligning Patch Set and Label Set with Conditional Transport for Multi-Label Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miaoge Li",
      "Dongsheng Wang",
      "Xinyang Liu",
      "Zequn Zeng",
      "Ruiying Lu",
      "Bo Chen",
      "Mingyuan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_VI-Net_Boosting_Category-level_6D_Object_Pose_Estimation_via_Learning_Decoupled_ICCV_2023_paper.html": {
    "title": "VI-Net: Boosting Category-level 6D Object Pose Estimation via Learning Decoupled Rotations on the Spherical Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiehong Lin",
      "Zewei Wei",
      "Yabin Zhang",
      "Kui Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_ICD-Face_Intra-class_Compactness_Distillation_for_Face_Recognition_ICCV_2023_paper.html": {
    "title": "ICD-Face: Intra-class Compactness Distillation for Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Yu",
      "Jiaheng Liu",
      "Haoyu Qin",
      "Yichao Wu",
      "Kun Hu",
      "Jiayi Tian",
      "Ding Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chou_Diffusion-SDF_Conditional_Generative_Modeling_of_Signed_Distance_Functions_ICCV_2023_paper.html": {
    "title": "Diffusion-SDF: Conditional Generative Modeling of Signed Distance Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gene Chou",
      "Yuval Bahat",
      "Felix Heide"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Open-Vocabulary_Object_Detection_With_an_Open_Corpus_ICCV_2023_paper.html": {
    "title": "Open-Vocabulary Object Detection With an Open Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiong Wang",
      "Huiming Zhang",
      "Haiwen Hong",
      "Xuan Jin",
      "Yuan He",
      "Hui Xue",
      "Zhou Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Argaw_Long-range_Multimodal_Pretraining_for_Movie_Understanding_ICCV_2023_paper.html": {
    "title": "Long-range Multimodal Pretraining for Movie Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dawit Mureja Argaw",
      "Joon-Young Lee",
      "Markus Woodson",
      "In So Kweon",
      "Fabian Caba Heilbron"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_MRM_Masked_Relation_Modeling_for_Medical_Image_Pre-Training_with_Genetics_ICCV_2023_paper.html": {
    "title": "MRM: Masked Relation Modeling for Medical Image Pre-Training with Genetics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiushi Yang",
      "Wuyang Li",
      "Baopu Li",
      "Yixuan Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Adverse_Weather_Removal_with_Codebook_Priors_ICCV_2023_paper.html": {
    "title": "Adverse Weather Removal with Codebook Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Ye",
      "Sixiang Chen",
      "Jinbin Bai",
      "Jun Shi",
      "Chenghao Xue",
      "Jingxia Jiang",
      "Junjie Yin",
      "Erkang Chen",
      "Yun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Miao_Spectrum-guided_Multi-granularity_Referring_Video_Object_Segmentation_ICCV_2023_paper.html": {
    "title": "Spectrum-guided Multi-granularity Referring Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Miao",
      "Mohammed Bennamoun",
      "Yongsheng Gao",
      "Ajmal Mian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Senocak_Sound_Source_Localization_is_All_about_Cross-Modal_Alignment_ICCV_2023_paper.html": {
    "title": "Sound Source Localization is All about Cross-Modal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arda Senocak",
      "Hyeonggon Ryu",
      "Junsik Kim",
      "Tae-Hyun Oh",
      "Hanspeter Pfister",
      "Joon Son Chung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_MAP_Towards_Balanced_Generalization_of_IID_and_OOD_through_Model-Agnostic_ICCV_2023_paper.html": {
    "title": "MAP: Towards Balanced Generalization of IID and OOD through Model-Agnostic Adapters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Zhang",
      "Junkun Yuan",
      "Yue He",
      "Wenbin Li",
      "Zhengyu Chen",
      "Kun Kuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Exploring_Group_Video_Captioning_with_Efficient_Relational_Approximation_ICCV_2023_paper.html": {
    "title": "Exploring Group Video Captioning with Efficient Relational Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Lin",
      "Tao Jin",
      "Ye Wang",
      "Wenwen Pan",
      "Linjun Li",
      "Xize Cheng",
      "Zhou Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Aydemir_ADAPT_Efficient_Multi-Agent_Trajectory_Prediction_with_Adaptation_ICCV_2023_paper.html": {
    "title": "ADAPT: Efficient Multi-Agent Trajectory Prediction with Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "GÃ¶rkay Aydemir",
      "Adil Kaan Akan",
      "Fatma GÃ¼ney"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye_TaskExpert_Dynamically_Assembling_Multi-Task_Representations_with_Memorial_Mixture-of-Experts_ICCV_2023_paper.html": {
    "title": "TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanrong Ye",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Meta_OOD_Learning_For_Continuously_Adaptive_OOD_Detection_ICCV_2023_paper.html": {
    "title": "Meta OOD Learning For Continuously Adaptive OOD Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinheng Wu",
      "Jie Lu",
      "Zhen Fang",
      "Guangquan Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_MAPConNet_Self-supervised_3D_Pose_Transfer_with_Mesh_and_Point_Contrastive_ICCV_2023_paper.html": {
    "title": "MAPConNet: Self-supervised 3D Pose Transfer with Mesh and Point Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaze Sun",
      "Zhixiang Chen",
      "Tae-Kyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shiohara_BlendFace_Re-designing_Identity_Encoders_for_Face-Swapping_ICCV_2023_paper.html": {
    "title": "BlendFace: Re-designing Identity Encoders for Face-Swapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaede Shiohara",
      "Xingchao Yang",
      "Takafumi Taketomi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cui_Test-time_Personalizable_Forecasting_of_3D_Human_Poses_ICCV_2023_paper.html": {
    "title": "Test-time Personalizable Forecasting of 3D Human Poses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiongjie Cui",
      "Huaijiang Sun",
      "Jianfeng Lu",
      "Weiqing Li",
      "Bin Li",
      "Hongwei Yi",
      "Haofan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gu_Few-shot_Continual_Infomax_Learning_ICCV_2023_paper.html": {
    "title": "Few-shot Continual Infomax Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Gu",
      "Chunyan Xu",
      "Jian Yang",
      "Zhen Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_A_Parse-Then-Place_Approach_for_Generating_Graphic_Layouts_from_Textual_Descriptions_ICCV_2023_paper.html": {
    "title": "A Parse-Then-Place Approach for Generating Graphic Layouts from Textual Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Lin",
      "Jiaqi Guo",
      "Shizhao Sun",
      "Weijiang Xu",
      "Ting Liu",
      "Jian-Guang Lou",
      "Dongmei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Raj_DreamBooth3D_Subject-Driven_Text-to-3D_Generation_ICCV_2023_paper.html": {
    "title": "DreamBooth3D: Subject-Driven Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amit Raj",
      "Srinivas Kaza",
      "Ben Poole",
      "Michael Niemeyer",
      "Nataniel Ruiz",
      "Ben Mildenhall",
      "Shiran Zada",
      "Kfir Aberman",
      "Michael Rubinstein",
      "Jonathan Barron",
      "Yuanzhen Li",
      "Varun Jampani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Segu_DARTH_Holistic_Test-time_Adaptation_for_Multiple_Object_Tracking_ICCV_2023_paper.html": {
    "title": "DARTH: Holistic Test-time Adaptation for Multiple Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mattia Segu",
      "Bernt Schiele",
      "Fisher Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Multi-interactive_Feature_Learning_and_a_Full-time_Multi-modality_Benchmark_for_Image_ICCV_2023_paper.html": {
    "title": "Multi-interactive Feature Learning and a Full-time Multi-modality Benchmark for Image Fusion and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyuan Liu",
      "Zhu Liu",
      "Guanyao Wu",
      "Long Ma",
      "Risheng Liu",
      "Wei Zhong",
      "Zhongxuan Luo",
      "Xin Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hartman_BaRe-ESA_A_Riemannian_Framework_for_Unregistered_Human_Body_Shapes_ICCV_2023_paper.html": {
    "title": "BaRe-ESA: A Riemannian Framework for Unregistered Human Body Shapes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emmanuel Hartman",
      "Emery Pierson",
      "Martin Bauer",
      "Nicolas Charon",
      "Mohamed Daoudi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Skip-Plan_Procedure_Planning_in_Instructional_Videos_via_Condensed_Action_Space_ICCV_2023_paper.html": {
    "title": "Skip-Plan: Procedure Planning in Instructional Videos via Condensed Action Space Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiheng Li",
      "Wenjia Geng",
      "Muheng Li",
      "Lei Chen",
      "Yansong Tang",
      "Jiwen Lu",
      "Jie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_A_Retrospect_to_Multi-prompt_Learning_across_Vision_and_Language_ICCV_2023_paper.html": {
    "title": "A Retrospect to Multi-prompt Learning across Vision and Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziliang Chen",
      "Xin Huang",
      "Quanlong Guan",
      "Liang Lin",
      "Weiqi Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_Sparse_Instance_Conditioned_Multimodal_Trajectory_Prediction_ICCV_2023_paper.html": {
    "title": "Sparse Instance Conditioned Multimodal Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yonghao Dong",
      "Le Wang",
      "Sanping Zhou",
      "Gang Hua"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Park_Label_Shift_Adapter_for_Test-Time_Adaptation_under_Covariate_and_Label_ICCV_2023_paper.html": {
    "title": "Label Shift Adapter for Test-Time Adaptation under Covariate and Label Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunghyun Park",
      "Seunghan Yang",
      "Jaegul Choo",
      "Sungrack Yun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Malepathirana_NAPA-VQ_Neighborhood-Aware_Prototype_Augmentation_with_Vector_Quantization_for_Continual_Learning_ICCV_2023_paper.html": {
    "title": "NAPA-VQ: Neighborhood-Aware Prototype Augmentation with Vector Quantization for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tamasha Malepathirana",
      "Damith Senanayake",
      "Saman Halgamuge"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qi_Dynamic_Snake_Convolution_Based_on_Topological_Geometric_Constraints_for_Tubular_ICCV_2023_paper.html": {
    "title": "Dynamic Snake Convolution Based on Topological Geometric Constraints for Tubular Structure Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaolei Qi",
      "Yuting He",
      "Xiaoming Qi",
      "Yuan Zhang",
      "Guanyu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fan_Unsupervised_Open-Vocabulary_Object_Localization_in_Videos_ICCV_2023_paper.html": {
    "title": "Unsupervised Open-Vocabulary Object Localization in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Fan",
      "Zechen Bai",
      "Tianjun Xiao",
      "Dominik Zietlow",
      "Max Horn",
      "Zixu Zhao",
      "Carl-Johann Simon-Gabriel",
      "Mike Zheng Shou",
      "Francesco Locatello",
      "Bernt Schiele",
      "Thomas Brox",
      "Zheng Zhang",
      "Yanwei Fu",
      "Tong He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Dataset_Quantization_ICCV_2023_paper.html": {
    "title": "Dataset Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daquan Zhou",
      "Kai Wang",
      "Jianyang Gu",
      "Xiangyu Peng",
      "Dongze Lian",
      "Yifan Zhang",
      "Yang You",
      "Jiashi Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Unsupervised_Video_Deraining_with_An_Event_Camera_ICCV_2023_paper.html": {
    "title": "Unsupervised Video Deraining with An Event Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Wang",
      "Wenming Weng",
      "Yueyi Zhang",
      "Zhiwei Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Overcoming_Forgetting_Catastrophe_in_Quantization-Aware_Training_ICCV_2023_paper.html": {
    "title": "Overcoming Forgetting Catastrophe in Quantization-Aware Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting-An Chen",
      "De-Nian Yang",
      "Ming-Syan Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_DIME-FM__DIstilling_Multimodal_and_Efficient_Foundation_Models_ICCV_2023_paper.html": {
    "title": "DIME-FM : DIstilling Multimodal and Efficient Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ximeng Sun",
      "Pengchuan Zhang",
      "Peizhao Zhang",
      "Hardik Shah",
      "Kate Saenko",
      "Xide Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Boosting_Single_Image_Super-Resolution_via_Partial_Channel_Shifting_ICCV_2023_paper.html": {
    "title": "Boosting Single Image Super-Resolution via Partial Channel Shifting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoming Zhang",
      "Tianrui Li",
      "Xiaole Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Learning_to_Upsample_by_Learning_to_Sample_ICCV_2023_paper.html": {
    "title": "Learning to Upsample by Learning to Sample",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenze Liu",
      "Hao Lu",
      "Hongtao Fu",
      "Zhiguo Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_LayoutDiffusion_Improving_Graphic_Layout_Generation_by_Discrete_Diffusion_Probabilistic_Models_ICCV_2023_paper.html": {
    "title": "LayoutDiffusion: Improving Graphic Layout Generation by Discrete Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Zhang",
      "Jiaqi Guo",
      "Shizhao Sun",
      "Jian-Guang Lou",
      "Dongmei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jain_Efficiently_Robustify_Pre-Trained_Models_ICCV_2023_paper.html": {
    "title": "Efficiently Robustify Pre-Trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nishant Jain",
      "Harkirat Behl",
      "Yogesh Singh Rawat",
      "Vibhav Vineet"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Davtyan_Efficient_Video_Prediction_via_Sparsely_Conditioned_Flow_Matching_ICCV_2023_paper.html": {
    "title": "Efficient Video Prediction via Sparsely Conditioned Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aram Davtyan",
      "Sepehr Sameni",
      "Paolo Favaro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Popovic_Surface_Normal_Clustering_for_Implicit_Representation_of_Manhattan_Scenes_ICCV_2023_paper.html": {
    "title": "Surface Normal Clustering for Implicit Representation of Manhattan Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikola Popovic",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Schmalfuss_Distracting_Downpour_Adversarial_Weather_Attacks_for_Motion_Estimation_ICCV_2023_paper.html": {
    "title": "Distracting Downpour: Adversarial Weather Attacks for Motion Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jenny Schmalfuss",
      "Lukas Mehl",
      "AndrÃ©s Bruhn"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lebailly_Adaptive_Similarity_Bootstrapping_for_Self-Distillation_Based_Representation_Learning_ICCV_2023_paper.html": {
    "title": "Adaptive Similarity Bootstrapping for Self-Distillation Based Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Lebailly",
      "Thomas StegmÃ¼ller",
      "Behzad Bozorgtabar",
      "Jean-Philippe Thiran",
      "Tinne Tuytelaars"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Generalized_Differentiable_RANSAC_ICCV_2023_paper.html": {
    "title": "Generalized Differentiable RANSAC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Wei",
      "Yash Patel",
      "Alexander Shekhovtsov",
      "Jiri Matas",
      "Daniel Barath"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Unfolding_Framework_with_Prior_of_Convolution-Transformer_Mixture_and_Uncertainty_Estimation_ICCV_2023_paper.html": {
    "title": "Unfolding Framework with Prior of Convolution-Transformer Mixture and Uncertainty Estimation for Video Snapshot Compressive Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siming Zheng",
      "Xin Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tian_Non-Semantics_Suppressed_Mask_Learning_for_Unsupervised_Video_Semantic_Compression_ICCV_2023_paper.html": {
    "title": "Non-Semantics Suppressed Mask Learning for Unsupervised Video Semantic Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Tian",
      "Guo Lu",
      "Guangtao Zhai",
      "Zhiyong Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Abati_ResQ_Residual_Quantization_for_Video_Perception_ICCV_2023_paper.html": {
    "title": "ResQ: Residual Quantization for Video Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Abati",
      "Haitam Ben Yahia",
      "Markus Nagel",
      "Amirhossein Habibian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Inverse_Compositional_Learning_for_Weakly-supervised_Relation_Grounding_ICCV_2023_paper.html": {
    "title": "Inverse Compositional Learning for Weakly-supervised Relation Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan Li",
      "Ping Wei",
      "Zeyu Ma",
      "Nanning Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bekuzarov_XMem_Production-level_Video_Segmentation_From_Few_Annotated_Frames_ICCV_2023_paper.html": {
    "title": "XMem++: Production-level Video Segmentation From Few Annotated Frames",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maksym Bekuzarov",
      "Ariana Bermudez",
      "Joon-Young Lee",
      "Hao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_MHCN_A_Hyperbolic_Neural_Network_Model_for_Multi-view_Hierarchical_Clustering_ICCV_2023_paper.html": {
    "title": "MHCN: A Hyperbolic Neural Network Model for Multi-view Hierarchical Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangfei Lin",
      "Bing Bai",
      "Yiwen Guo",
      "Hao Chen",
      "Yazhou Ren",
      "Zenglin Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wallace_End-to-End_Diffusion_Latent_Optimization_Improves_Classifier_Guidance_ICCV_2023_paper.html": {
    "title": "End-to-End Diffusion Latent Optimization Improves Classifier Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bram Wallace",
      "Akash Gokul",
      "Stefano Ermon",
      "Nikhil Naik"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Stier_FineRecon_Depth-aware_Feed-forward_Network_for_Detailed_3D_Reconstruction_ICCV_2023_paper.html": {
    "title": "FineRecon: Depth-aware Feed-forward Network for Detailed 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noah Stier",
      "Anurag Ranjan",
      "Alex Colburn",
      "Yajie Yan",
      "Liang Yang",
      "Fangchang Ma",
      "Baptiste Angles"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Krantz_Navigating_to_Objects_Specified_by_Images_ICCV_2023_paper.html": {
    "title": "Navigating to Objects Specified by Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Krantz",
      "Theophile Gervet",
      "Karmesh Yadav",
      "Austin Wang",
      "Chris Paxton",
      "Roozbeh Mottaghi",
      "Dhruv Batra",
      "Jitendra Malik",
      "Stefan Lee",
      "Devendra Singh Chaplot"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_TRM-UAP_Enhancing_the_Transferability_of_Data-Free_Universal_Adversarial_Perturbation_via_ICCV_2023_paper.html": {
    "title": "TRM-UAP: Enhancing the Transferability of Data-Free Universal Adversarial Perturbation via Truncated Ratio Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiran Liu",
      "Xin Feng",
      "Yunlong Wang",
      "Wu Yang",
      "Di Ming"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Luo_LATR_3D_Lane_Detection_from_Monocular_Images_with_Transformer_ICCV_2023_paper.html": {
    "title": "LATR: 3D Lane Detection from Monocular Images with Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueru Luo",
      "Chaoda Zheng",
      "Xu Yan",
      "Tang Kun",
      "Chao Zheng",
      "Shuguang Cui",
      "Zhen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hyeon-Woo_Scratching_Visual_Transformers_Back_with_Uniform_Attention_ICCV_2023_paper.html": {
    "title": "Scratching Visual Transformer's Back with Uniform Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nam Hyeon-Woo",
      "Kim Yu-Ji",
      "Byeongho Heo",
      "Dongyoon Han",
      "Seong Joon Oh",
      "Tae-Hyun Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Tune-A-Video_One-Shot_Tuning_of_Image_Diffusion_Models_for_Text-to-Video_Generation_ICCV_2023_paper.html": {
    "title": "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jay Zhangjie Wu",
      "Yixiao Ge",
      "Xintao Wang",
      "Stan Weixian Lei",
      "Yuchao Gu",
      "Yufei Shi",
      "Wynne Hsu",
      "Ying Shan",
      "Xiaohu Qie",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lv_Anchor-Intermediate_Detector_Decoupling_and_Coupling_Bounding_Boxes_for_Accurate_Object_ICCV_2023_paper.html": {
    "title": "Anchor-Intermediate Detector: Decoupling and Coupling Bounding Boxes for Accurate Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilong Lv",
      "Min Li",
      "Yujie He",
      "Shaopeng Li",
      "Zhuzhen He",
      "Aitao Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Min_Environment-Invariant_Curriculum_Relation_Learning_for_Fine-Grained_Scene_Graph_Generation_ICCV_2023_paper.html": {
    "title": "Environment-Invariant Curriculum Relation Learning for Fine-Grained Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukuan Min",
      "Aming Wu",
      "Cheng Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Extensible_and_Efficient_Proxy_for_Neural_Architecture_Search_ICCV_2023_paper.html": {
    "title": "Extensible and Efficient Proxy for Neural Architecture Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhong Li",
      "Jiajie Li",
      "Cong Hao",
      "Pan Li",
      "Jinjun Xiong",
      "Deming Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Alibeigi_Zenseact_Open_Dataset_A_Large-Scale_and_Diverse_Multimodal_Dataset_for_ICCV_2023_paper.html": {
    "title": "Zenseact Open Dataset: A Large-Scale and Diverse Multimodal Dataset for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mina Alibeigi",
      "William Ljungbergh",
      "Adam Tonderski",
      "Georg Hess",
      "Adam Lilja",
      "Carl LindstrÃ¶m",
      "Daria Motorniuk",
      "Junsheng Fu",
      "Jenny Widahl",
      "Christoffer Petersson"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liang_MAAL_Multimodality-Aware_Autoencoder-Based_Affordance_Learning_for_3D_Articulated_Objects_ICCV_2023_paper.html": {
    "title": "MAAL: Multimodality-Aware Autoencoder-Based Affordance Learning for 3D Articulated Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanzhi Liang",
      "Xiaohan Wang",
      "Linchao Zhu",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Generalizable_Decision_Boundaries_Dualistic_Meta-Learning_for_Open_Set_Domain_Generalization_ICCV_2023_paper.html": {
    "title": "Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiran Wang",
      "Jian Zhang",
      "Lei Qi",
      "Yinghuan Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ji_Benchmarking_and_Analyzing_Robust_Point_Cloud_Recognition_Bag_of_Tricks_ICCV_2023_paper.html": {
    "title": "Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiufan Ji",
      "Lin Wang",
      "Cong Shi",
      "Shengshan Hu",
      "Yingying Chen",
      "Lichao Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Weakly_Supervised_Referring_Image_Segmentation_with_Intra-Chunk_and_Inter-Chunk_Consistency_ICCV_2023_paper.html": {
    "title": "Weakly Supervised Referring Image Segmentation with Intra-Chunk and Inter-Chunk Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungbeom Lee",
      "Sungjin Lee",
      "Jinseok Nam",
      "Seunghak Yu",
      "Jaeyoung Do",
      "Tara Taghavi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/van_Spengler_Poincare_ResNet_ICCV_2023_paper.html": {
    "title": "Poincare ResNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max van Spengler",
      "Erwin Berkhout",
      "Pascal Mettes"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zeng_Parameterized_Cost_Volume_for_Stereo_Matching_ICCV_2023_paper.html": {
    "title": "Parameterized Cost Volume for Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxi Zeng",
      "Chengtang Yao",
      "Lidong Yu",
      "Yuwei Wu",
      "Yunde Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wilson_SAFE_Sensitivity-Aware_Features_for_Out-of-Distribution_Object_Detection_ICCV_2023_paper.html": {
    "title": "SAFE: Sensitivity-Aware Features for Out-of-Distribution Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Wilson",
      "Tobias Fischer",
      "Feras Dayoub",
      "Dimity Miller",
      "Niko SÃ¼nderhauf"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Feng_SimFIR_A_Simple_Framework_for_Fisheye_Image_Rectification_with_Self-supervised_ICCV_2023_paper.html": {
    "title": "SimFIR: A Simple Framework for Fisheye Image Rectification with Self-supervised Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Feng",
      "Wendi Wang",
      "Jiajun Deng",
      "Wengang Zhou",
      "Li Li",
      "Houqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hou_Subclass-balancing_Contrastive_Learning_for_Long-tailed_Recognition_ICCV_2023_paper.html": {
    "title": "Subclass-balancing Contrastive Learning for Long-tailed Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengkai Hou",
      "Jieyu Zhang",
      "Haonan Wang",
      "Tianyi Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yao_Generalized_Lightness_Adaptation_with_Channel_Selective_Normalization_ICCV_2023_paper.html": {
    "title": "Generalized Lightness Adaptation with Channel Selective Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingde Yao",
      "Jie Huang",
      "Xin Jin",
      "Ruikang Xu",
      "Shenglong Zhou",
      "Man Zhou",
      "Zhiwei Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Omnidirectional_Information_Gathering_for_Knowledge_Transfer-Based_Audio-Visual_Navigation_ICCV_2023_paper.html": {
    "title": "Omnidirectional Information Gathering for Knowledge Transfer-Based Audio-Visual Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyu Chen",
      "Wenguan Wang",
      "Si Liu",
      "Hongsheng Li",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_Multi-Scale_Bidirectional_Recurrent_Network_with_Hybrid_Correlation_for_Point_Cloud_ICCV_2023_paper.html": {
    "title": "Multi-Scale Bidirectional Recurrent Network with Hybrid Correlation for Point Cloud Based Scene Flow Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wencan Cheng",
      "Jong Hwan Ko"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qiao_Dynamic_Mesh-Aware_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "Dynamic Mesh-Aware Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Ling Qiao",
      "Alexander Gao",
      "Yiran Xu",
      "Yue Feng",
      "Jia-Bin Huang",
      "Ming C. Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Learning_Support_and_Trivial_Prototypes_for_Interpretable_Image_Classification_ICCV_2023_paper.html": {
    "title": "Learning Support and Trivial Prototypes for Interpretable Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Wang",
      "Yuyuan Liu",
      "Yuanhong Chen",
      "Fengbei Liu",
      "Yu Tian",
      "Davis McCarthy",
      "Helen Frazer",
      "Gustavo Carneiro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Decoupled_DETR_Spatially_Disentangling_Localization_and_Classification_for_Improved_End-to-End_ICCV_2023_paper.html": {
    "title": "Decoupled DETR: Spatially Disentangling Localization and Classification for Improved End-to-End Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manyuan Zhang",
      "Guanglu Song",
      "Yu Liu",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fang_GIFD_A_Generative_Gradient_Inversion_Method_with_Feature_Domain_Optimization_ICCV_2023_paper.html": {
    "title": "GIFD: A Generative Gradient Inversion Method with Feature Domain Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Fang",
      "Bin Chen",
      "Xuan Wang",
      "Zhi Wang",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qiao_VLN-PETL_Parameter-Efficient_Transfer_Learning_for_Vision-and-Language_Navigation_ICCV_2023_paper.html": {
    "title": "VLN-PETL: Parameter-Efficient Transfer Learning for Vision-and-Language Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyuan Qiao",
      "Zheng Yu",
      "Qi Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gurbuz_Generalized_Sum_Pooling_for_Metric_Learning_ICCV_2023_paper.html": {
    "title": "Generalized Sum Pooling for Metric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeti Z. GÃ¼rbÃ¼z",
      "Ozan Sener",
      "A. Aydin Alatan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_AlignDet_Aligning_Pre-training_and_Fine-tuning_in_Object_Detection_ICCV_2023_paper.html": {
    "title": "AlignDet: Aligning Pre-training and Fine-tuning in Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Li",
      "Jie Wu",
      "Xionghui Wang",
      "Chen Chen",
      "Jie Qin",
      "Xuefeng Xiao",
      "Rui Wang",
      "Min Zheng",
      "Xin Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Learning_Continuous_Exposure_Value_Representations_for_Single-Image_HDR_Reconstruction_ICCV_2023_paper.html": {
    "title": "Learning Continuous Exposure Value Representations for Single-Image HDR Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Su-Kai Chen",
      "Hung-Lin Yen",
      "Yu-Lun Liu",
      "Min-Hung Chen",
      "Hou-Ning Hu",
      "Wen-Hsiao Peng",
      "Yen-Yu Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_DREAM_Efficient_Dataset_Distillation_by_Representative_Matching_ICCV_2023_paper.html": {
    "title": "DREAM: Efficient Dataset Distillation by Representative Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanqing Liu",
      "Jianyang Gu",
      "Kai Wang",
      "Zheng Zhu",
      "Wei Jiang",
      "Yang You"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_MixSynthFormer_A_Transformer_Encoder-like_Structure_with_Mixed_Synthetic_Self-attention_for_ICCV_2023_paper.html": {
    "title": "MixSynthFormer: A Transformer Encoder-like Structure with Mixed Synthetic Self-attention for Efficient Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuran Sun",
      "Alan William Dougherty",
      "Zhuoying Zhang",
      "Yi King Choi",
      "Chuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huo_Focus_on_Your_Target_A_Dual_Teacher-Student_Framework_for_Domain-Adaptive_ICCV_2023_paper.html": {
    "title": "Focus on Your Target: A Dual Teacher-Student Framework for Domain-Adaptive Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyue Huo",
      "Lingxi Xie",
      "Wengang Zhou",
      "Houqiang Li",
      "Qi Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Taraday_Enhanced_Meta_Label_Correction_for_Coping_with_Label_Corruption_ICCV_2023_paper.html": {
    "title": "Enhanced Meta Label Correction for Coping with Label Corruption",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mitchell Keren Taraday",
      "Chaim Baskin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Dense_Text-to-Image_Generation_with_Attention_Modulation_ICCV_2023_paper.html": {
    "title": "Dense Text-to-Image Generation with Attention Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunji Kim",
      "Jiyoung Lee",
      "Jin-Hwa Kim",
      "Jung-Woo Ha",
      "Jun-Yan Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_HumanMAC_Masked_Motion_Completion_for_Human_Motion_Prediction_ICCV_2023_paper.html": {
    "title": "HumanMAC: Masked Motion Completion for Human Motion Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling-Hao Chen",
      "JiaWei Zhang",
      "Yewen Li",
      "Yiren Pang",
      "Xiaobo Xia",
      "Tongliang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hataya_Will_Large-scale_Generative_Models_Corrupt_Future_Datasets_ICCV_2023_paper.html": {
    "title": "Will Large-scale Generative Models Corrupt Future Datasets?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryuichiro Hataya",
      "Han Bao",
      "Hiromi Arai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Girish_SHACIRA_Scalable_HAsh-grid_Compression_for_Implicit_Neural_Representations_ICCV_2023_paper.html": {
    "title": "SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharath Girish",
      "Abhinav Shrivastava",
      "Kamal Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Deng_Prompt_Switch_Efficient_CLIP_Adaptation_for_Text-Video_Retrieval_ICCV_2023_paper.html": {
    "title": "Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaorui Deng",
      "Qi Chen",
      "Pengda Qin",
      "Da Chen",
      "Qi Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Video_Action_Recognition_with_Attentive_Semantic_Units_ICCV_2023_paper.html": {
    "title": "Video Action Recognition with Attentive Semantic Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Chen",
      "Dapeng Chen",
      "Ruijin Liu",
      "Hao Li",
      "Wei Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Khoshsirat_Sentence_Attention_Blocks_for_Answer_Grounding_ICCV_2023_paper.html": {
    "title": "Sentence Attention Blocks for Answer Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seyedalireza Khoshsirat",
      "Chandra Kambhamettu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pan_Scanning_Only_Once_An_End-to-end_Framework_for_Fast_Temporal_Grounding_ICCV_2023_paper.html": {
    "title": "Scanning Only Once: An End-to-end Framework for Fast Temporal Grounding in Long Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulin Pan",
      "Xiangteng He",
      "Biao Gong",
      "Yiliang Lv",
      "Yujun Shen",
      "Yuxin Peng",
      "Deli Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dukic_A_Low-Shot_Object_Counting_Network_With_Iterative_Prototype_Adaptation_ICCV_2023_paper.html": {
    "title": "A Low-Shot Object Counting Network With Iterative Prototype Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikola ÄukiÄ",
      "Alan LukeÅ¾iÄ",
      "Vitjan Zavrtanik",
      "Matej Kristan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Towards_Fairness-aware_Adversarial_Network_Pruning_ICCV_2023_paper.html": {
    "title": "Towards Fairness-aware Adversarial Network Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Zhang",
      "Zhibo Wang",
      "Xiaowei Dong",
      "Yunhe Feng",
      "Xiaoyi Pang",
      "Zhifei Zhang",
      "Kui Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Maruani_VoroMesh_Learning_Watertight_Surface_Meshes_with_Voronoi_Diagrams_ICCV_2023_paper.html": {
    "title": "VoroMesh: Learning Watertight Surface Meshes with Voronoi Diagrams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nissim Maruani",
      "Roman Klokov",
      "Maks Ovsjanikov",
      "Pierre Alliez",
      "Mathieu Desbrun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Breaking_Temporal_Consistency_Generating_Video_Universal_Adversarial_Perturbations_Using_Image_ICCV_2023_paper.html": {
    "title": "Breaking Temporal Consistency: Generating Video Universal Adversarial Perturbations Using Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hee-Seon Kim",
      "Minji Son",
      "Minbeom Kim",
      "Myung-Joon Kwon",
      "Changick Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sushko_Smoothness_Similarity_Regularization_for_Few-Shot_GAN_Adaptation_ICCV_2023_paper.html": {
    "title": "Smoothness Similarity Regularization for Few-Shot GAN Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vadim Sushko",
      "Ruyu Wang",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Distilling_Coarse-to-Fine_Semantic_Matching_Knowledge_for_Weakly_Supervised_3D_Visual_ICCV_2023_paper.html": {
    "title": "Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehan Wang",
      "Haifeng Huang",
      "Yang Zhao",
      "Linjun Li",
      "Xize Cheng",
      "Yichen Zhu",
      "Aoxiong Yin",
      "Zhou Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shtedritski_What_does_CLIP_know_about_a_red_circle_Visual_prompt_ICCV_2023_paper.html": {
    "title": "What does CLIP know about a red circle? Visual prompt engineering for VLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandar Shtedritski",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sadoughi_MEGA_Multimodal_Alignment_Aggregation_and_Distillation_For_Cinematic_Video_Segmentation_ICCV_2023_paper.html": {
    "title": "MEGA: Multimodal Alignment Aggregation and Distillation For Cinematic Video Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Najmeh Sadoughi",
      "Xinyu Li",
      "Avijit Vajpayee",
      "David Fan",
      "Bing Shuai",
      "Hector Santos-Villalobos",
      "Vimal Bhat",
      "Rohith MV"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_DiffRate__Differentiable_Compression_Rate_for_Efficient_Vision_Transformers_ICCV_2023_paper.html": {
    "title": "DiffRate : Differentiable Compression Rate for Efficient Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengzhao Chen",
      "Wenqi Shao",
      "Peng Xu",
      "Mingbao Lin",
      "Kaipeng Zhang",
      "Fei Chao",
      "Rongrong Ji",
      "Yu Qiao",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ghodsi_zPROBE_Zero_Peek_Robustness_Checks_for_Federated_Learning_ICCV_2023_paper.html": {
    "title": "zPROBE: Zero Peek Robustness Checks for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zahra Ghodsi",
      "Mojan Javaheripi",
      "Nojan Sheybani",
      "Xinqiao Zhang",
      "Ke Huang",
      "Farinaz Koushanfar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_LoLep_Single-View_View_Synthesis_with_Locally-Learned_Planes_and_Self-Attention_Occlusion_ICCV_2023_paper.html": {
    "title": "LoLep: Single-View View Synthesis with Locally-Learned Planes and Self-Attention Occlusion Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Wang",
      "Yu-Ping Wang",
      "Dinesh Manocha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cao_Multi-Modal_Continual_Test-Time_Adaptation_for_3D_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Multi-Modal Continual Test-Time Adaptation for 3D Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhi Cao",
      "Yuecong Xu",
      "Jianfei Yang",
      "Pengyu Yin",
      "Shenghai Yuan",
      "Lihua Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Choi_Exploring_Positional_Characteristics_of_Dual-Pixel_Data_for_Camera_Autofocus_ICCV_2023_paper.html": {
    "title": "Exploring Positional Characteristics of Dual-Pixel Data for Camera Autofocus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myungsub Choi",
      "Hana Lee",
      "Hyong-euk Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_Heterogeneous_Forgetting_Compensation_for_Class-Incremental_Learning_ICCV_2023_paper.html": {
    "title": "Heterogeneous Forgetting Compensation for Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahua Dong",
      "Wenqi Liang",
      "Yang Cong",
      "Gan Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tu_FemtoDet_An_Object_Detection_Baseline_for_Energy_Versus_Performance_Tradeoffs_ICCV_2023_paper.html": {
    "title": "FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Tu",
      "Xu Xie",
      "Guo Ai",
      "Yuexiang Li",
      "Yawen Huang",
      "Yefeng Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Generative_Prompt_Model_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.html": {
    "title": "Generative Prompt Model for Weakly Supervised Object Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhong Zhao",
      "Qixiang Ye",
      "Weijia Wu",
      "Chunhua Shen",
      "Fang Wan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_ActFormer_A_GAN-based_Transformer_towards_General_Action-Conditioned_3D_Human_Motion_ICCV_2023_paper.html": {
    "title": "ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D Human Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Xu",
      "Ziyang Song",
      "Dongliang Wang",
      "Jing Su",
      "Zhicheng Fang",
      "Chenjing Ding",
      "Weihao Gan",
      "Yichao Yan",
      "Xin Jin",
      "Xiaokang Yang",
      "Wenjun Zeng",
      "Wei Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Su_Hiding_Visual_Information_via_Obfuscating_Adversarial_Perturbations_ICCV_2023_paper.html": {
    "title": "Hiding Visual Information via Obfuscating Adversarial Perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhigang Su",
      "Dawei Zhou",
      "Nannan Wang",
      "Decheng Liu",
      "Zhen Wang",
      "Xinbo Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Category-aware_Allocation_Transformer_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.html": {
    "title": "Category-aware Allocation Transformer for Weakly Supervised Object Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Chen",
      "Jinren Ding",
      "Liujuan Cao",
      "Yunhang Shen",
      "Shengchuan Zhang",
      "Guannan Jiang",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Domain_Specified_Optimization_for_Deployment_Authorization_ICCV_2023_paper.html": {
    "title": "Domain Specified Optimization for Deployment Authorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Wang",
      "Haoang Chi",
      "Wenjing Yang",
      "Zhipeng Lin",
      "Mingyang Geng",
      "Long Lan",
      "Jing Zhang",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liang_Iterative_Prompt_Learning_for_Unsupervised_Backlit_Image_Enhancement_ICCV_2023_paper.html": {
    "title": "Iterative Prompt Learning for Unsupervised Backlit Image Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhexin Liang",
      "Chongyi Li",
      "Shangchen Zhou",
      "Ruicheng Feng",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_UMIFormer_Mining_the_Correlations_between_Similar_Tokens_for_Multi-View_3D_ICCV_2023_paper.html": {
    "title": "UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenwei Zhu",
      "Liying Yang",
      "Ning Li",
      "Chaohao Jiang",
      "Yanyan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ngo_Improved_Knowledge_Transfer_for_Semi-Supervised_Domain_Adaptation_via_Trico_Training_ICCV_2023_paper.html": {
    "title": "Improved Knowledge Transfer for Semi-Supervised Domain Adaptation via Trico Training Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ba Hung Ngo",
      "Yeon Jeong Chae",
      "Jung Eun Kwon",
      "Jae Hyeon Park",
      "Sung In Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pang_Locally_Stylized_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "Locally Stylized Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong-Wing Pang",
      "Binh-Son Hua",
      "Sai-Kit Yeung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_InterFormer_Real-time_Interactive_Image_Segmentation_ICCV_2023_paper.html": {
    "title": "InterFormer: Real-time Interactive Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "You Huang",
      "Hao Yang",
      "Ke Sun",
      "Shengchuan Zhang",
      "Liujuan Cao",
      "Guannan Jiang",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Confidence-aware_Pseudo-label_Learning_for_Weakly_Supervised_Visual_Grounding_ICCV_2023_paper.html": {
    "title": "Confidence-aware Pseudo-label Learning for Weakly Supervised Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Liu",
      "Jiahua Zhang",
      "Qingchao Chen",
      "Yuxin Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Baek_Luminance-aware_Color_Transform_for_Multiple_Exposure_Correction_ICCV_2023_paper.html": {
    "title": "Luminance-aware Color Transform for Multiple Exposure Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jong-Hyeon Baek",
      "DaeHyun Kim",
      "Su-Min Choi",
      "Hyo-jun Lee",
      "Hanul Kim",
      "Yeong Jun Koh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_A_Simple_Framework_for_Open-Vocabulary_Segmentation_and_Detection_ICCV_2023_paper.html": {
    "title": "A Simple Framework for Open-Vocabulary Segmentation and Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhang",
      "Feng Li",
      "Xueyan Zou",
      "Shilong Liu",
      "Chunyuan Li",
      "Jianwei Yang",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Alignment_Before_Aggregation_Trajectory_Memory_Retrieval_Network_for_Video_Object_ICCV_2023_paper.html": {
    "title": "Alignment Before Aggregation: Trajectory Memory Retrieval Network for Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Sun",
      "Yuan Wang",
      "Huayu Mai",
      "Tianzhu Zhang",
      "Feng Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fang_UATVR_Uncertainty-Adaptive_Text-Video_Retrieval_ICCV_2023_paper.html": {
    "title": "UATVR: Uncertainty-Adaptive Text-Video Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Fang",
      "Wenhao Wu",
      "Chang Liu",
      "Yu Zhou",
      "Yuxin Song",
      "Weiping Wang",
      "Xiangbo Shu",
      "Xiangyang Ji",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Su_Deep_Directly-Trained_Spiking_Neural_Networks_for_Object_Detection_ICCV_2023_paper.html": {
    "title": "Deep Directly-Trained Spiking Neural Networks for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiaoyi Su",
      "Yuhong Chou",
      "Yifan Hu",
      "Jianing Li",
      "Shijie Mei",
      "Ziyang Zhang",
      "Guoqi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Online_Prototype_Learning_for_Online_Continual_Learning_ICCV_2023_paper.html": {
    "title": "Online Prototype Learning for Online Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Wei",
      "Jiaxin Ye",
      "Zhizhong Huang",
      "Junping Zhang",
      "Hongming Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Low_Robust_e-NeRF_NeRF_from_Sparse__Noisy_Events_under_Non-Uniform_ICCV_2023_paper.html": {
    "title": "Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weng Fei Low",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mu_ActorsNeRF_Animatable_Few-shot_Human_Rendering_with_Generalizable_NeRFs_ICCV_2023_paper.html": {
    "title": "ActorsNeRF: Animatable Few-shot Human Rendering with Generalizable NeRFs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiteng Mu",
      "Shen Sang",
      "Nuno Vasconcelos",
      "Xiaolong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Koo_SALAD_Part-Level_Latent_Diffusion_for_3D_Shape_Generation_and_Manipulation_ICCV_2023_paper.html": {
    "title": "SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juil Koo",
      "Seungwoo Yoo",
      "Minh Hieu Nguyen",
      "Minhyuk Sung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Park_COMPASS_High-Efficiency_Deep_Image_Compression_with_Arbitrary-scale_Spatial_Scalability_ICCV_2023_paper.html": {
    "title": "COMPASS: High-Efficiency Deep Image Compression with Arbitrary-scale Spatial Scalability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongmin Park",
      "Jooyoung Lee",
      "Munchurl Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lao_Masked_Autoencoders_Are_Stronger_Knowledge_Distillers_ICCV_2023_paper.html": {
    "title": "Masked Autoencoders Are Stronger Knowledge Distillers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanshan Lao",
      "Guanglu Song",
      "Boxiao Liu",
      "Yu Liu",
      "Yujiu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Feng_Score-Based_Diffusion_Models_as_Principled_Priors_for_Inverse_Imaging_ICCV_2023_paper.html": {
    "title": "Score-Based Diffusion Models as Principled Priors for Inverse Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Berthy T. Feng",
      "Jamie Smith",
      "Michael Rubinstein",
      "Huiwen Chang",
      "Katherine L. Bouman",
      "William T. Freeman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ren_Multiscale_Structure_Guided_Diffusion_for_Image_Deblurring_ICCV_2023_paper.html": {
    "title": "Multiscale Structure Guided Diffusion for Image Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengwei Ren",
      "Mauricio Delbracio",
      "Hossein Talebi",
      "Guido Gerig",
      "Peyman Milanfar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Multiple_Planar_Object_Tracking_ICCV_2023_paper.html": {
    "title": "Multiple Planar Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhicheng Zhang",
      "Shengzhe Liu",
      "Jufeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lian_CheckerPose_Progressive_Dense_Keypoint_Localization_for_Object_Pose_Estimation_with_ICCV_2023_paper.html": {
    "title": "CheckerPose: Progressive Dense Keypoint Localization for Object Pose Estimation with Graph Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruyi Lian",
      "Haibin Ling"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gupta_ASIC_Aligning_Sparse_in-the-wild_Image_Collections_ICCV_2023_paper.html": {
    "title": "ASIC: Aligning Sparse in-the-wild Image Collections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kamal Gupta",
      "Varun Jampani",
      "Carlos Esteves",
      "Abhinav Shrivastava",
      "Ameesh Makadia",
      "Noah Snavely",
      "Abhishek Kar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Residual_Pattern_Learning_for_Pixel-Wise_Out-of-Distribution_Detection_in_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Residual Pattern Learning for Pixel-Wise Out-of-Distribution Detection in Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyuan Liu",
      "Choubo Ding",
      "Yu Tian",
      "Guansong Pang",
      "Vasileios Belagiannis",
      "Ian Reid",
      "Gustavo Carneiro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Hierarchical_Visual_Primitive_Experts_for_Compositional_Zero-Shot_Learning_ICCV_2023_paper.html": {
    "title": "Hierarchical Visual Primitive Experts for Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanjae Kim",
      "Jiyoung Lee",
      "Seongheon Park",
      "Kwanghoon Sohn"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Event_Camera_Data_Pre-training_ICCV_2023_paper.html": {
    "title": "Event Camera Data Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Yang",
      "Liyuan Pan",
      "Liu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Segment_Every_Reference_Object_in_Spatial_and_Temporal_Spaces_ICCV_2023_paper.html": {
    "title": "Segment Every Reference Object in Spatial and Temporal Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiannan Wu",
      "Yi Jiang",
      "Bin Yan",
      "Huchuan Lu",
      "Zehuan Yuan",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Averly_Unified_Out-Of-Distribution_Detection_A_Model-Specific_Perspective_ICCV_2023_paper.html": {
    "title": "Unified Out-Of-Distribution Detection: A Model-Specific Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Averly",
      "Wei-Lun Chao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_One-shot_Implicit_Animatable_Avatars_with_Model-based_Priors_ICCV_2023_paper.html": {
    "title": "One-shot Implicit Animatable Avatars with Model-based Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyi Huang",
      "Hongwei Yi",
      "Weiyang Liu",
      "Haofan Wang",
      "Boxi Wu",
      "Wenxiao Wang",
      "Binbin Lin",
      "Debing Zhang",
      "Deng Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Unsupervised_Feature_Representation_Learning_for_Domain-generalized_Cross-domain_Image_Retrieval_ICCV_2023_paper.html": {
    "title": "Unsupervised Feature Representation Learning for Domain-generalized Cross-domain Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Conghui Hu",
      "Can Zhang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_RankMatch_Fostering_Confidence_and_Consistency_in_Learning_with_Noisy_Labels_ICCV_2023_paper.html": {
    "title": "RankMatch: Fostering Confidence and Consistency in Learning with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Zhang",
      "Weikai Chen",
      "Chaowei Fang",
      "Zhen Li",
      "Lechao Chen",
      "Liang Lin",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shen_Dec-Adapter_Exploring_Efficient_Decoder-Side_Adapter_for_Bridging_Screen_Content_and_ICCV_2023_paper.html": {
    "title": "Dec-Adapter: Exploring Efficient Decoder-Side Adapter for Bridging Screen Content and Natural Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Shen",
      "Huanjing Yue",
      "Jingyu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cai_MixReorg_Cross-Modal_Mixed_Patch_Reorganization_is_a_Good_Mask_Learner_ICCV_2023_paper.html": {
    "title": "MixReorg: Cross-Modal Mixed Patch Reorganization is a Good Mask Learner for Open-World Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixin Cai",
      "Pengzhen Ren",
      "Yi Zhu",
      "Hang Xu",
      "Jianzhuang Liu",
      "Changlin Li",
      "Guangrun Wang",
      "Xiaodan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Buhler_Preface_A_Data-driven_Volumetric_Prior_for_Few-shot_Ultra_High-resolution_Face_ICCV_2023_paper.html": {
    "title": "Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcel C. BÃ¼hler",
      "Kripasindhu Sarkar",
      "Tanmay Shah",
      "Gengyan Li",
      "Daoye Wang",
      "Leonhard Helminger",
      "Sergio Orts-Escolano",
      "Dmitry Lagun",
      "Otmar Hilliges",
      "Thabo Beeler",
      "Abhimitra Meka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Label-Guided_Knowledge_Distillation_for_Continual_Semantic_Segmentation_on_2D_Images_ICCV_2023_paper.html": {
    "title": "Label-Guided Knowledge Distillation for Continual Semantic Segmentation on 2D Images and 3D Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ze Yang",
      "Ruibo Li",
      "Evan Ling",
      "Chi Zhang",
      "Yiming Wang",
      "Dezhao Huang",
      "Keng Teck Ma",
      "Minhoe Hur",
      "Guosheng Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Song_Under-Display_Camera_Image_Restoration_with_Scattering_Effect_ICCV_2023_paper.html": {
    "title": "Under-Display Camera Image Restoration with Scattering Effect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binbin Song",
      "Xiangyu Chen",
      "Shuning Xu",
      "Jiantao Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nooralinejad_PRANC_Pseudo_RAndom_Networks_for_Compacting_Deep_Models_ICCV_2023_paper.html": {
    "title": "PRANC: Pseudo RAndom Networks for Compacting Deep Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parsa Nooralinejad",
      "Ali Abbasi",
      "Soroush Abbasi Koohpayegani",
      "Kossar Pourahmadi Meibodi",
      "Rana Muhammad Shahroz Khan",
      "Soheil Kolouri",
      "Hamed Pirsiavash"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Rymarczyk_ICICLE_Interpretable_Class_Incremental_Continual_Learning_ICCV_2023_paper.html": {
    "title": "ICICLE: Interpretable Class Incremental Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dawid Rymarczyk",
      "Joost van de Weijer",
      "Bartosz ZieliÅski",
      "Bartlomiej Twardowski"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Clutter_Detection_and_Removal_in_3D_Scenes_with_View-Consistent_Inpainting_ICCV_2023_paper.html": {
    "title": "Clutter Detection and Removal in 3D Scenes with View-Consistent Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangyin Wei",
      "Thomas Funkhouser",
      "Szymon Rusinkiewicz"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.html": {
    "title": "PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyang Zhu",
      "Renrui Zhang",
      "Bowei He",
      "Ziyu Guo",
      "Ziyao Zeng",
      "Zipeng Qin",
      "Shanghang Zhang",
      "Peng Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shi_VideoFlow_Exploiting_Temporal_Cues_for_Multi-frame_Optical_Flow_Estimation_ICCV_2023_paper.html": {
    "title": "VideoFlow: Exploiting Temporal Cues for Multi-frame Optical Flow Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Shi",
      "Zhaoyang Huang",
      "Weikang Bian",
      "Dasong Li",
      "Manyuan Zhang",
      "Ka Chun Cheung",
      "Simon See",
      "Hongwei Qin",
      "Jifeng Dai",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_3DMiner_Discovering_Shapes_from_Large-Scale_Unannotated_Image_Datasets_ICCV_2023_paper.html": {
    "title": "3DMiner: Discovering Shapes from Large-Scale Unannotated Image Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ta-Ying Cheng",
      "Matheus Gadelha",
      "SÃ¶ren Pirk",
      "Thibault Groueix",
      "RadomÃ­r MÄch",
      "Andrew Markham",
      "Niki Trigoni"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Metzen_Identification_of_Systematic_Errors_of_Image_Classifiers_on_Rare_Subgroups_ICCV_2023_paper.html": {
    "title": "Identification of Systematic Errors of Image Classifiers on Rare Subgroups",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Hendrik Metzen",
      "Robin Hutmacher",
      "N. Grace Hua",
      "Valentyn Boreiko",
      "Dan Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Hierarchical_Spatio-Temporal_Representation_Learning_for_Gait_Recognition_ICCV_2023_paper.html": {
    "title": "Hierarchical Spatio-Temporal Representation Learning for Gait Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Wang",
      "Bo Liu",
      "Fangfang Liang",
      "Bincheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Order-Prompted_Tag_Sequence_Generation_for_Video_Tagging_ICCV_2023_paper.html": {
    "title": "Order-Prompted Tag Sequence Generation for Video Tagging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongyang Ma",
      "Ziqi Zhang",
      "Yuxin Chen",
      "Zhongang Qi",
      "Yingmin Luo",
      "Zekun Li",
      "Chunfeng Yuan",
      "Bing Li",
      "Xiaohu Qie",
      "Ying Shan",
      "Weiming Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lai_XVO_Generalized_Visual_Odometry_via_Cross-Modal_Self-Training_ICCV_2023_paper.html": {
    "title": "XVO: Generalized Visual Odometry via Cross-Modal Self-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Lai",
      "Zhongkai Shangguan",
      "Jimuyang Zhang",
      "Eshed Ohn-Bar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Weakly_Supervised_Learning_of_Semantic_Correspondence_through_Cascaded_Online_Correspondence_ICCV_2023_paper.html": {
    "title": "Weakly Supervised Learning of Semantic Correspondence through Cascaded Online Correspondence Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwen Huang",
      "Yixuan Sun",
      "Chenghang Lai",
      "Qing Xu",
      "Xiaomei Wang",
      "Xuli Shen",
      "Weifeng Ge"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pei_Clusterformer_Cluster-based_Transformer_for_3D_Object_Detection_in_Point_Clouds_ICCV_2023_paper.html": {
    "title": "Clusterformer: Cluster-based Transformer for 3D Object Detection in Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Pei",
      "Xian Zhao",
      "Hao Li",
      "Jingyuan Ma",
      "Jingwei Zhang",
      "Shiliang Pu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Aliakbarian_HMD-NeMo_Online_3D_Avatar_Motion_Generation_From_Sparse_Observations_ICCV_2023_paper.html": {
    "title": "HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sadegh Aliakbarian",
      "Fatemeh Saleh",
      "David Collier",
      "Pashmina Cameron",
      "Darren Cosker"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xie_NaviNeRF_NeRF-based_3D_Representation_Disentanglement_by_Latent_Semantic_Navigation_ICCV_2023_paper.html": {
    "title": "NaviNeRF: NeRF-based 3D Representation Disentanglement by Latent Semantic Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baao Xie",
      "Bohan Li",
      "Zequn Zhang",
      "Junting Dong",
      "Xin Jin",
      "Jingyu Yang",
      "Wenjun Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Adaptive_Illumination_Mapping_for_Shadow_Detection_in_Raw_Images_ICCV_2023_paper.html": {
    "title": "Adaptive Illumination Mapping for Shadow Detection in Raw Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayu Sun",
      "Ke Xu",
      "Youwei Pang",
      "Lihe Zhang",
      "Huchuan Lu",
      "Gerhard Hancke",
      "Rynson Lau"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Abdelfattah_CDUL_CLIP-Driven_Unsupervised_Learning_for_Multi-Label_Image_Classification_ICCV_2023_paper.html": {
    "title": "CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rabab Abdelfattah",
      "Qing Guo",
      "Xiaoguang Li",
      "Xiaofeng Wang",
      "Song Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Your_Diffusion_Model_is_Secretly_a_Zero-Shot_Classifier_ICCV_2023_paper.html": {
    "title": "Your Diffusion Model is Secretly a Zero-Shot Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander C. Li",
      "Mihir Prabhudesai",
      "Shivam Duggal",
      "Ellis Brown",
      "Deepak Pathak"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Backpropagation_Path_Search_On_Adversarial_Transferability_ICCV_2023_paper.html": {
    "title": "Backpropagation Path Search On Adversarial Transferability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoer Xu",
      "Zhangxuan Gu",
      "Jianping Zhang",
      "Shiwen Cui",
      "Changhua Meng",
      "Weiqiang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Boosting_Adversarial_Transferability_via_Gradient_Relevance_Attack_ICCV_2023_paper.html": {
    "title": "Boosting Adversarial Transferability via Gradient Relevance Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hegui Zhu",
      "Yuchen Ren",
      "Xiaoyan Sui",
      "Lianping Yang",
      "Wuming Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Christensen_Image-Free_Classifier_Injection_for_Zero-Shot_Classification_ICCV_2023_paper.html": {
    "title": "Image-Free Classifier Injection for Zero-Shot Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anders Christensen",
      "Massimiliano Mancini",
      "A. Sophia Koepke",
      "Ole Winther",
      "Zeynep Akata"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_CLIPN_for_Zero-Shot_OOD_Detection_Teaching_CLIP_to_Say_No_ICCV_2023_paper.html": {
    "title": "CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hualiang Wang",
      "Yi Li",
      "Huifeng Yao",
      "Xiaomeng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xie_CO-Net_Learning_Multiple_Point_Cloud_Tasks_at_Once_with_A_ICCV_2023_paper.html": {
    "title": "CO-Net: Learning Multiple Point Cloud Tasks at Once with A Cohesive Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Xie",
      "Ke Wang",
      "Siyi Lu",
      "Yukun Zhang",
      "Kun Dai",
      "Xiaoyu Li",
      "Jie Xu",
      "Li Wang",
      "Lijun Zhao",
      "Xinyu Zhang",
      "Ruifeng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chavhan_Quality_Diversity_for_Visual_Pre-Training_ICCV_2023_paper.html": {
    "title": "Quality Diversity for Visual Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruchika Chavhan",
      "Henry Gouk",
      "Da Li",
      "Timothy Hospedales"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wan_UniDexGrasp_Improving_Dexterous_Grasping_Policy_Learning_via_Geometry-Aware_Curriculum_and_ICCV_2023_paper.html": {
    "title": "UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-Aware Curriculum and Iterative Generalist-Specialist Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weikang Wan",
      "Haoran Geng",
      "Yun Liu",
      "Zikang Shan",
      "Yaodong Yang",
      "Li Yi",
      "He Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_Multi-Scale_Residual_Low-Pass_Filter_Network_for_Image_Deblurring_ICCV_2023_paper.html": {
    "title": "Multi-Scale Residual Low-Pass Filter Network for Image Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangxin Dong",
      "Jinshan Pan",
      "Zhongbao Yang",
      "Jinhui Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shen_FerKD_Surgical_Label_Adaptation_for_Efficient_Distillation_ICCV_2023_paper.html": {
    "title": "FerKD: Surgical Label Adaptation for Efficient Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqiang Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shandilya_Neural_Fields_for_Structured_Lighting_ICCV_2023_paper.html": {
    "title": "Neural Fields for Structured Lighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aarrushi Shandilya",
      "Benjamin Attal",
      "Christian Richardt",
      "James Tompkin",
      "Matthew O'toole"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.html": {
    "title": "ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqiang Xu",
      "Wenxin Du",
      "Han Xue",
      "Yutong Li",
      "Ruolin Ye",
      "Yan-Feng Wang",
      "Cewu Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Feng_Semantically_Structured_Image_Compression_via_Irregular_Group-Based_Decoupling_ICCV_2023_paper.html": {
    "title": "Semantically Structured Image Compression via Irregular Group-Based Decoupling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoyu Feng",
      "Yixin Gao",
      "Xin Jin",
      "Runsen Feng",
      "Zhibo Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shi_PhaseMP_Robust_3D_Pose_Estimation_via_Phase-conditioned_Human_Motion_Prior_ICCV_2023_paper.html": {
    "title": "PhaseMP: Robust 3D Pose Estimation via Phase-conditioned Human Motion Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyi Shi",
      "Sebastian Starke",
      "Yuting Ye",
      "Taku Komura",
      "Jungdam Won"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fujimura_NLOS-NeuS_Non-line-of-sight_Neural_Implicit_Surface_ICCV_2023_paper.html": {
    "title": "NLOS-NeuS: Non-line-of-sight Neural Implicit Surface",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Fujimura",
      "Takahiro Kushida",
      "Takuya Funatomi",
      "Yasuhiro Mukaigawa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Song_Unsupervised_Object_Localization_with_Representer_Point_Selection_ICCV_2023_paper.html": {
    "title": "Unsupervised Object Localization with Representer Point Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeonghwan Song",
      "Seokwoo Jang",
      "Dina Katabi",
      "Jeany Son"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ravindran_SEMPART_Self-supervised_Multi-resolution_Partitioning_of_Image_Semantics_ICCV_2023_paper.html": {
    "title": "SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sriram Ravindran",
      "Debraj Basu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Flatness-Aware_Minimization_for_Domain_Generalization_ICCV_2023_paper.html": {
    "title": "Flatness-Aware Minimization for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingxuan Zhang",
      "Renzhe Xu",
      "Han Yu",
      "Yancheng Dong",
      "Pengfei Tian",
      "Peng Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_ProtoFL_Unsupervised_Federated_Learning_via_Prototypical_Distillation_ICCV_2023_paper.html": {
    "title": "ProtoFL: Unsupervised Federated Learning via Prototypical Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hansol Kim",
      "Youngjun Kwak",
      "Minyoung Jung",
      "Jinho Shin",
      "Youngsung Kim",
      "Changick Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Augmenting_and_Aligning_Snippets_for_Few-Shot_Video_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "Augmenting and Aligning Snippets for Few-Shot Video Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuecong Xu",
      "Jianfei Yang",
      "Yunjiao Zhou",
      "Zhenghua Chen",
      "Min Wu",
      "Xiaoli Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Self-Organizing_Pathway_Expansion_for_Non-Exemplar_Class-Incremental_Learning_ICCV_2023_paper.html": {
    "title": "Self-Organizing Pathway Expansion for Non-Exemplar Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Zhu",
      "Kecheng Zheng",
      "Ruili Feng",
      "Deli Zhao",
      "Yang Cao",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_Preserving_Tumor_Volumes_for_Unsupervised_Medical_Image_Registration_ICCV_2023_paper.html": {
    "title": "Preserving Tumor Volumes for Unsupervised Medical Image Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihua Dong",
      "Hao Du",
      "Ying Song",
      "Yan Xu",
      "Jing Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mur-Labadia_Multi-label_Affordance_Mapping_from_Egocentric_Vision_ICCV_2023_paper.html": {
    "title": "Multi-label Affordance Mapping from Egocentric Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Mur-Labadia",
      "Jose J. Guerrero",
      "Ruben Martinez-Cantin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Towards_Real-World_Burst_Image_Super-Resolution_Benchmark_and_Method_ICCV_2023_paper.html": {
    "title": "Towards Real-World Burst Image Super-Resolution: Benchmark and Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengxu Wei",
      "Yujing Sun",
      "Xingbei Guo",
      "Chang Liu",
      "Guanbin Li",
      "Jie Chen",
      "Xiangyang Ji",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Unified_Adversarial_Patch_for_Cross-Modal_Attacks_in_the_Physical_World_ICCV_2023_paper.html": {
    "title": "Unified Adversarial Patch for Cross-Modal Attacks in the Physical World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingxing Wei",
      "Yao Huang",
      "Yitong Sun",
      "Jie Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Unsupervised_Accuracy_Estimation_of_Deep_Visual_Models_using_Domain-Adaptive_Adversarial_ICCV_2023_paper.html": {
    "title": "Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JoonHo Lee",
      "Jae Oh Woo",
      "Hankyu Moon",
      "Kwonho Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Misalign_Contrast_then_Distill_Rethinking_Misalignments_in_Language-Image_Pre-training_ICCV_2023_paper.html": {
    "title": "Misalign, Contrast then Distill: Rethinking Misalignments in Language-Image Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bumsoo Kim",
      "Yeonsik Jo",
      "Jinhyung Kim",
      "Seunghwan Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gou_SYENet_A_Simple_Yet_Effective_Network_for_Multiple_Low-Level_Vision_ICCV_2023_paper.html": {
    "title": "SYENet: A Simple Yet Effective Network for Multiple Low-Level Vision Tasks with Real-Time Performance on Mobile Device",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiran Gou",
      "Ziyao Yi",
      "Yan Xiang",
      "Shaoqing Li",
      "Zibin Liu",
      "Dehui Kong",
      "Ke Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mirza_MATE_Masked_Autoencoders_are_Online_3D_Test-Time_Learners_ICCV_2023_paper.html": {
    "title": "MATE: Masked Autoencoders are Online 3D Test-Time Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "M. Jehanzeb Mirza",
      "Inkyu Shin",
      "Wei Lin",
      "Andreas Schriebl",
      "Kunyang Sun",
      "Jaesung Choe",
      "Mateusz Kozinski",
      "Horst Possegger",
      "In So Kweon",
      "Kuk-Jin Yoon",
      "Horst Bischof"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shi_EdaDet_Open-Vocabulary_Object_Detection_Using_Early_Dense_Alignment_ICCV_2023_paper.html": {
    "title": "EdaDet: Open-Vocabulary Object Detection Using Early Dense Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Shi",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chu_MixPath_A_Unified_Approach_for_One-shot_Neural_Architecture_Search_ICCV_2023_paper.html": {
    "title": "MixPath: A Unified Approach for One-shot Neural Architecture Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangxiang Chu",
      "Shun Lu",
      "Xudong Li",
      "Bo Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cong_Enhancing_NeRF_akin_to_Enhancing_LLMs_Generalizable_NeRF_Transformer_with_ICCV_2023_paper.html": {
    "title": "Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyan Cong",
      "Hanxue Liang",
      "Peihao Wang",
      "Zhiwen Fan",
      "Tianlong Chen",
      "Mukund Varma",
      "Yi Wang",
      "Zhangyang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Task-aware_Adaptive_Learning_for_Cross-domain_Few-shot_Learning_ICCV_2023_paper.html": {
    "title": "Task-aware Adaptive Learning for Cross-domain Few-shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yurong Guo",
      "Ruoyi Du",
      "Yuan Dong",
      "Timothy Hospedales",
      "Yi-Zhe Song",
      "Zhanyu Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gu_Two_Birds_One_Stone_A_Unified_Framework_for_Joint_Learning_ICCV_2023_paper.html": {
    "title": "Two Birds, One Stone: A Unified Framework for Joint Learning of Image and Video Style Transfers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohai Gu",
      "Heng Fan",
      "Libo Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Revisiting_Domain-Adaptive_3D_Object_Detection_by_Reliable_Diverse_and_Class-balanced_ICCV_2023_paper.html": {
    "title": "Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoxiao Chen",
      "Yadan Luo",
      "Zheng Wang",
      "Mahsa Baktashmotlagh",
      "Zi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Long_Task-Oriented_Multi-Modal_Mutual_Leaning_for_Vision-Language_Models_ICCV_2023_paper.html": {
    "title": "Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sifan Long",
      "Zhen Zhao",
      "Junkun Yuan",
      "Zichang Tan",
      "Jiangjiang Liu",
      "Luping Zhou",
      "Shengsheng Wang",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lei_Efficient_Adaptive_Human-Object_Interaction_Detection_with_Concept-guided_Memory_ICCV_2023_paper.html": {
    "title": "Efficient Adaptive Human-Object Interaction Detection with Concept-guided Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Lei",
      "Fabian Caba",
      "Qingchao Chen",
      "Hailin Jin",
      "Yuxin Peng",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_NeMF_Inverse_Volume_Rendering_with_Neural_Microflake_Field_ICCV_2023_paper.html": {
    "title": "NeMF: Inverse Volume Rendering with Neural Microflake Field",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youjia Zhang",
      "Teng Xu",
      "Junqing Yu",
      "Yuteng Ye",
      "Yanqing Jing",
      "Junle Wang",
      "Jingyi Yu",
      "Wei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Attentive_Mask_CLIP_ICCV_2023_paper.html": {
    "title": "Attentive Mask CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Yang",
      "Weiquan Huang",
      "Yixuan Wei",
      "Houwen Peng",
      "Xinyang Jiang",
      "Huiqiang Jiang",
      "Fangyun Wei",
      "Yin Wang",
      "Han Hu",
      "Lili Qiu",
      "Yuqing Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_DOLCE_A_Model-Based_Probabilistic_Diffusion_Framework_for_Limited-Angle_CT_Reconstruction_ICCV_2023_paper.html": {
    "title": "DOLCE: A Model-Based Probabilistic Diffusion Framework for Limited-Angle CT Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Liu",
      "Rushil Anirudh",
      "Jayaraman J. Thiagarajan",
      "Stewart He",
      "K Aditya Mohan",
      "Ulugbek S. Kamilov",
      "Hyojin Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Beyond_Image_Borders_Learning_Feature_Extrapolation_for_Unbounded_Image_Composition_ICCV_2023_paper.html": {
    "title": "Beyond Image Borders: Learning Feature Extrapolation for Unbounded Image Composition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Liu",
      "Ming Liu",
      "Junyi Li",
      "Shuai Liu",
      "Xiaotao Wang",
      "Lei Lei",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cao_MasaCtrl_Tuning-Free_Mutual_Self-Attention_Control_for_Consistent_Image_Synthesis_and_ICCV_2023_paper.html": {
    "title": "MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingdeng Cao",
      "Xintao Wang",
      "Zhongang Qi",
      "Ying Shan",
      "Xiaohu Qie",
      "Yinqiang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hemati_Understanding_Hessian_Alignment_for_Domain_Generalization_ICCV_2023_paper.html": {
    "title": "Understanding Hessian Alignment for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sobhan Hemati",
      "Guojun Zhang",
      "Amir Estiri",
      "Xi Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_DeepChange_A_Long-Term_Person_Re-Identification_Benchmark_with_Clothes_Change_ICCV_2023_paper.html": {
    "title": "DeepChange: A Long-Term Person Re-Identification Benchmark with Clothes Change",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Xu",
      "Xiatian Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ge_Preserve_Your_Own_Correlation_A_Noise_Prior_for_Video_Diffusion_ICCV_2023_paper.html": {
    "title": "Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songwei Ge",
      "Seungjun Nah",
      "Guilin Liu",
      "Tyler Poon",
      "Andrew Tao",
      "Bryan Catanzaro",
      "David Jacobs",
      "Jia-Bin Huang",
      "Ming-Yu Liu",
      "Yogesh Balaji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zou_Discrepant_and_Multi-Instance_Proxies_for_Unsupervised_Person_Re-Identification_ICCV_2023_paper.html": {
    "title": "Discrepant and Multi-Instance Proxies for Unsupervised Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Zou",
      "Zeqi Chen",
      "Zhichao Cui",
      "Yuehu Liu",
      "Chi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Joint-Relation_Transformer_for_Multi-Person_Motion_Prediction_ICCV_2023_paper.html": {
    "title": "Joint-Relation Transformer for Multi-Person Motion Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyao Xu",
      "Weibo Mao",
      "Jingze Gong",
      "Chenxin Xu",
      "Siheng Chen",
      "Weidi Xie",
      "Ya Zhang",
      "Yanfeng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chang_Revisiting_Vision_Transformer_from_the_View_of_Path_Ensemble_ICCV_2023_paper.html": {
    "title": "Revisiting Vision Transformer from the View of Path Ensemble",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuning Chang",
      "Pichao Wang",
      "Hao Luo",
      "Fan Wang",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kulhanek_Tetra-NeRF_Representing_Neural_Radiance_Fields_Using_Tetrahedra_ICCV_2023_paper.html": {
    "title": "Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Kulhanek",
      "Torsten Sattler"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_TMA_Temporal_Motion_Aggregation_for_Event-based_Optical_Flow_ICCV_2023_paper.html": {
    "title": "TMA: Temporal Motion Aggregation for Event-based Optical Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Liu",
      "Guang Chen",
      "Sanqing Qu",
      "Yanping Zhang",
      "Zhijun Li",
      "Alois Knoll",
      "Changjun Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kumari_Ablating_Concepts_in_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Ablating Concepts in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nupur Kumari",
      "Bingliang Zhang",
      "Sheng-Yu Wang",
      "Eli Shechtman",
      "Richard Zhang",
      "Jun-Yan Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fan_Motion-Guided_Masking_for_Spatiotemporal_Representation_Learning_ICCV_2023_paper.html": {
    "title": "Motion-Guided Masking for Spatiotemporal Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Fan",
      "Jue Wang",
      "Shuai Liao",
      "Yi Zhu",
      "Vimal Bhat",
      "Hector Santos-Villalobos",
      "Rohith MV",
      "Xinyu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bernhard_MapFormer_Boosting_Change_Detection_by_Using_Pre-change_Information_ICCV_2023_paper.html": {
    "title": "MapFormer: Boosting Change Detection by Using Pre-change Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Bernhard",
      "Niklas StrauÃ",
      "Matthias Schubert"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Masked_Diffusion_Transformer_is_a_Strong_Image_Synthesizer_ICCV_2023_paper.html": {
    "title": "Masked Diffusion Transformer is a Strong Image Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanghua Gao",
      "Pan Zhou",
      "Ming-Ming Cheng",
      "Shuicheng Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Rodriguez-Puigvert_LightDepth_Single-View_Depth_Self-Supervision_from_Illumination_Decline_ICCV_2023_paper.html": {
    "title": "LightDepth: Single-View Depth Self-Supervision from Illumination Decline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javier RodrÃ­guez-Puigvert",
      "VÃ­ctor M. Batlle",
      "J.M.M. Montiel",
      "Ruben Martinez-Cantin",
      "Pascal Fua",
      "Juan D. TardÃ³s",
      "Javier Civera"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lu_Urban_Radiance_Field_Representation_with_Deformable_Neural_Mesh_Primitives_ICCV_2023_paper.html": {
    "title": "Urban Radiance Field Representation with Deformable Neural Mesh Primitives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Lu",
      "Yan Xu",
      "Guang Chen",
      "Hongsheng Li",
      "Kwan-Yee Lin",
      "Changjun Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Adaptive_Frequency_Filters_As_Efficient_Global_Token_Mixers_ICCV_2023_paper.html": {
    "title": "Adaptive Frequency Filters As Efficient Global Token Mixers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Huang",
      "Zhizheng Zhang",
      "Cuiling Lan",
      "Zheng-Jun Zha",
      "Yan Lu",
      "Baining Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Referring_Image_Segmentation_Using_Text_Supervision_ICCV_2023_paper.html": {
    "title": "Referring Image Segmentation Using Text Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Liu",
      "Yuhao Liu",
      "Yuqiu Kong",
      "Ke Xu",
      "Lihe Zhang",
      "Baocai Yin",
      "Gerhard Hancke",
      "Rynson Lau"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Zolly_Zoom_Focal_Length_Correctly_for_Perspective-Distorted_Human_Mesh_Reconstruction_ICCV_2023_paper.html": {
    "title": "Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjia Wang",
      "Yongtao Ge",
      "Haiyi Mei",
      "Zhongang Cai",
      "Qingping Sun",
      "Yanjun Wang",
      "Chunhua Shen",
      "Lei Yang",
      "Taku Komura"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fan_Once_Detected_Never_Lost_Surpassing_Human_Performance_in_Offline_LiDAR_ICCV_2023_paper.html": {
    "title": "Once Detected, Never Lost: Surpassing Human Performance in Offline LiDAR based 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lue Fan",
      "Yuxue Yang",
      "Yiming Mao",
      "Feng Wang",
      "Yuntao Chen",
      "Naiyan Wang",
      "Zhaoxiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/B_Building_a_Winning_Team_Selecting_Source_Model_Ensembles_using_a_ICCV_2023_paper.html": {
    "title": "Building a Winning Team: Selecting Source Model Ensembles using a Submodular Transferability Estimation Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vimal K B",
      "Saketh Bachu",
      "Tanmay Garg",
      "Niveditha Lakshmi Narasimhan",
      "Raghavan Konuru",
      "Vineeth N Balasubramanian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dutson_Eventful_Transformers_Leveraging_Temporal_Redundancy_in_Vision_Transformers_ICCV_2023_paper.html": {
    "title": "Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Dutson",
      "Yin Li",
      "Mohit Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bramlage_Plausible_Uncertainties_for_Human_Pose_Regression_ICCV_2023_paper.html": {
    "title": "Plausible Uncertainties for Human Pose Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lennart Bramlage",
      "Michelle Karg",
      "CristÃ³bal Curio"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Beyond_One-to-One_Rethinking_the_Referring_Image_Segmentation_ICCV_2023_paper.html": {
    "title": "Beyond One-to-One: Rethinking the Referring Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutao Hu",
      "Qixiong Wang",
      "Wenqi Shao",
      "Enze Xie",
      "Zhenguo Li",
      "Jungong Han",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Robust_Referring_Video_Object_Segmentation_with_Cyclic_Structural_Consensus_ICCV_2023_paper.html": {
    "title": "Robust Referring Video Object Segmentation with Cyclic Structural Consensus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Li",
      "Jinglu Wang",
      "Xiaohao Xu",
      "Xiao Li",
      "Bhiksha Raj",
      "Yan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xia_DiffIR_Efficient_Diffusion_Model_for_Image_Restoration_ICCV_2023_paper.html": {
    "title": "DiffIR: Efficient Diffusion Model for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Xia",
      "Yulun Zhang",
      "Shiyin Wang",
      "Yitong Wang",
      "Xinglong Wu",
      "Yapeng Tian",
      "Wenming Yang",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_MoreauGrad_Sparse_and_Robust_Interpretation_of_Neural_Networks_via_Moreau_ICCV_2023_paper.html": {
    "title": "MoreauGrad: Sparse and Robust Interpretation of Neural Networks via Moreau Envelope",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwei Zhang",
      "Farzan Farnia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shao_Building_Bridge_Across_the_Time_Disruption_and_Restoration_of_Murals_ICCV_2023_paper.html": {
    "title": "Building Bridge Across the Time: Disruption and Restoration of Murals In the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiyang Shao",
      "Qianqian Xu",
      "Peisong Wen",
      "Peifeng Gao",
      "Zhiyong Yang",
      "Qingming Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mo_Class-Incremental_Grouping_Network_for_Continual_Audio-Visual_Learning_ICCV_2023_paper.html": {
    "title": "Class-Incremental Grouping Network for Continual Audio-Visual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shentong Mo",
      "Weiguo Pian",
      "Yapeng Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sklyarova_Neural_Haircut_Prior-Guided_Strand-Based_Hair_Reconstruction_ICCV_2023_paper.html": {
    "title": "Neural Haircut: Prior-Guided Strand-Based Hair Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vanessa Sklyarova",
      "Jenya Chelishev",
      "Andreea Dogaru",
      "Igor Medvedev",
      "Victor Lempitsky",
      "Egor Zakharov"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hong_Improving_Sample_Quality_of_Diffusion_Models_Using_Self-Attention_Guidance_ICCV_2023_paper.html": {
    "title": "Improving Sample Quality of Diffusion Models Using Self-Attention Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Susung Hong",
      "Gyuseong Lee",
      "Wooseok Jang",
      "Seungryong Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Evaluating_Data_Attribution_for_Text-to-Image_Models_ICCV_2023_paper.html": {
    "title": "Evaluating Data Attribution for Text-to-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng-Yu Wang",
      "Alexei A. Efros",
      "Jun-Yan Zhu",
      "Richard Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hertz_Delta_Denoising_Score_ICCV_2023_paper.html": {
    "title": "Delta Denoising Score",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Hertz",
      "Kfir Aberman",
      "Daniel Cohen-Or"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ren_Hierarchical_Prior_Mining_for_Non-local_Multi-View_Stereo_ICCV_2023_paper.html": {
    "title": "Hierarchical Prior Mining for Non-local Multi-View Stereo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunlin Ren",
      "Qingshan Xu",
      "Shikun Zhang",
      "Jiaqi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kumar_Generative_Multiplane_Neural_Radiance_for_3D-Aware_Image_Generation_ICCV_2023_paper.html": {
    "title": "Generative Multiplane Neural Radiance for 3D-Aware Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amandeep Kumar",
      "Ankan Kumar Bhunia",
      "Sanath Narayan",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer",
      "Salman Khan",
      "Ming-Hsuan Yang",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ju_DG-Recon_Depth-Guided_Neural_3D_Scene_Reconstruction_ICCV_2023_paper.html": {
    "title": "DG-Recon: Depth-Guided Neural 3D Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihong Ju",
      "Ching Wei Tseng",
      "Oleksandr Bailo",
      "Georgi Dikov",
      "Mohsen Ghafoorian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liang_Simple_Baselines_for_Interactive_Video_Retrieval_with_Questions_and_Answers_ICCV_2023_paper.html": {
    "title": "Simple Baselines for Interactive Video Retrieval with Questions and Answers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiqu Liang",
      "Samuel Albanie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fernandez_The_Stable_Signature_Rooting_Watermarks_in_Latent_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "The Stable Signature: Rooting Watermarks in Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Fernandez",
      "Guillaume Couairon",
      "HervÃ© JÃ©gou",
      "Matthijs Douze",
      "Teddy Furon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Boosting_Semantic_Segmentation_from_the_Perspective_of_Explicit_Class_Embeddings_ICCV_2023_paper.html": {
    "title": "Boosting Semantic Segmentation from the Perspective of Explicit Class Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhe Liu",
      "Chuanjian Liu",
      "Kai Han",
      "Quan Tang",
      "Zengchang Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Going_Denser_with_Open-Vocabulary_Part_Segmentation_ICCV_2023_paper.html": {
    "title": "Going Denser with Open-Vocabulary Part Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peize Sun",
      "Shoufa Chen",
      "Chenchen Zhu",
      "Fanyi Xiao",
      "Ping Luo",
      "Saining Xie",
      "Zhicheng Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Learning_to_Identify_Critical_States_for_Reinforcement_Learning_from_Videos_ICCV_2023_paper.html": {
    "title": "Learning to Identify Critical States for Reinforcement Learning from Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Liu",
      "Mingchen Zhuge",
      "Bing Li",
      "Yuhui Wang",
      "Francesco Faccio",
      "Bernard Ghanem",
      "JÃ¼rgen Schmidhuber"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Orgad_Editing_Implicit_Assumptions_in_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Editing Implicit Assumptions in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadas Orgad",
      "Bahjat Kawar",
      "Yonatan Belinkov"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_OCHID-Fi_Occlusion-Robust_Hand_Pose_Estimation_in_3D_via_RF-Vision_ICCV_2023_paper.html": {
    "title": "OCHID-Fi: Occlusion-Robust Hand Pose Estimation in 3D via RF-Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shujie Zhang",
      "Tianyue Zheng",
      "Zhe Chen",
      "Jingzhi Hu",
      "Abdelwahed Khamis",
      "Jiajun Liu",
      "Jun Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ozkan_Conceptual_and_Hierarchical_Latent_Space_Decomposition_for_Face_Editing_ICCV_2023_paper.html": {
    "title": "Conceptual and Hierarchical Latent Space Decomposition for Face Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Savas Ozkan",
      "Mete Ozay",
      "Tom Robinson"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bi_VL-Match_Enhancing_Vision-Language_Pretraining_with_Token-Level_and_Instance-Level_Matching_ICCV_2023_paper.html": {
    "title": "VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Bi",
      "Daixuan Cheng",
      "Ping Yao",
      "Bochen Pang",
      "Yuefeng Zhan",
      "Chuanguang Yang",
      "Yujing Wang",
      "Hao Sun",
      "Weiwei Deng",
      "Qi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zuo_Reconstructing_Interacting_Hands_with_Interaction_Prior_from_Monocular_Images_ICCV_2023_paper.html": {
    "title": "Reconstructing Interacting Hands with Interaction Prior from Monocular Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binghui Zuo",
      "Zimeng Zhao",
      "Wenqian Sun",
      "Wei Xie",
      "Zhou Xue",
      "Yangang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chavan_Towards_Realistic_Evaluation_of_Industrial_Continual_Learning_Scenarios_with_an_ICCV_2023_paper.html": {
    "title": "Towards Realistic Evaluation of Industrial Continual Learning Scenarios with an Emphasis on Energy Consumption and Computational Footprint",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivek Chavan",
      "Paul Koch",
      "Marian SchlÃ¼ter",
      "Clemens Briese"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lu_Translating_Images_to_Road_Network_A_Non-Autoregressive_Sequence-to-Sequence_Approach_ICCV_2023_paper.html": {
    "title": "Translating Images to Road Network: A Non-Autoregressive Sequence-to-Sequence Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Lu",
      "Renyuan Peng",
      "Xinyue Cai",
      "Hang Xu",
      "Hongyang Li",
      "Feng Wen",
      "Wei Zhang",
      "Li Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bahrami_How_Much_Temporal_Long-Term_Context_is_Needed_for_Action_Segmentation_ICCV_2023_paper.html": {
    "title": "How Much Temporal Long-Term Context is Needed for Action Segmentation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emad Bahrami",
      "Gianpiero Francesca",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Luo_3D_VR_Sketch_Guided_3D_Shape_Prototyping_and_Exploration_ICCV_2023_paper.html": {
    "title": "3D VR Sketch Guided 3D Shape Prototyping and Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling Luo",
      "Pinaki Nath Chowdhury",
      "Tao Xiang",
      "Yi-Zhe Song",
      "Yulia Gryaditskaya"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Seal-3D_Interactive_Pixel-Level_Editing_for_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Wang",
      "Jingsen Zhu",
      "Qi Ye",
      "Yuchi Huo",
      "Yunlong Ran",
      "Zhihua Zhong",
      "Jiming Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chan_Generative_Novel_View_Synthesis_with_3D-Aware_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Generative Novel View Synthesis with 3D-Aware Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric R. Chan",
      "Koki Nagano",
      "Matthew A. Chan",
      "Alexander W. Bergman",
      "Jeong Joon Park",
      "Axel Levy",
      "Miika Aittala",
      "Shalini De Mello",
      "Tero Karras",
      "Gordon Wetzstein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_MDCS_More_Diverse_Experts_with_Consistency_Self-distillation_for_Long-tailed_Recognition_ICCV_2023_paper.html": {
    "title": "MDCS: More Diverse Experts with Consistency Self-distillation for Long-tailed Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihao Zhao",
      "Chen Jiang",
      "Wei Hu",
      "Fan Zhang",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Luo_Similarity_Min-Max_Zero-Shot_Day-Night_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "Similarity Min-Max: Zero-Shot Day-Night Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rundong Luo",
      "Wenjing Wang",
      "Wenhan Yang",
      "Jiaying Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mohwald_Dark_Side_Augmentation_Generating_Diverse_Night_Examples_for_Metric_Learning_ICCV_2023_paper.html": {
    "title": "Dark Side Augmentation: Generating Diverse Night Examples for Metric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Albert Mohwald",
      "Tomas Jenicek",
      "OndÅej Chum"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_NeRF-MS_Neural_Radiance_Fields_with_Multi-Sequence_ICCV_2023_paper.html": {
    "title": "NeRF-MS: Neural Radiance Fields with Multi-Sequence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peihao Li",
      "Shaohui Wang",
      "Chen Yang",
      "Bingbing Liu",
      "Weichao Qiu",
      "Haoqian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hong_LVOS_A_Benchmark_for_Long-term_Video_Object_Segmentation_ICCV_2023_paper.html": {
    "title": "LVOS: A Benchmark for Long-term Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingyi Hong",
      "Wenchao Chen",
      "Zhongying Liu",
      "Wei Zhang",
      "Pinxue Guo",
      "Zhaoyu Chen",
      "Wenqiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Diffusion_Model_as_Representation_Learner_ICCV_2023_paper.html": {
    "title": "Diffusion Model as Representation Learner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyi Yang",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Warburg_Nerfbusters_Removing_Ghostly_Artifacts_from_Casually_Captured_NeRFs_ICCV_2023_paper.html": {
    "title": "Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederik Warburg",
      "Ethan Weber",
      "Matthew Tancik",
      "Aleksander Holynski",
      "Angjoo Kanazawa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Van_Landeghem_Document_Understanding_Dataset_and_Evaluation_DUDE_ICCV_2023_paper.html": {
    "title": "Document Understanding Dataset and Evaluation (DUDE)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jordy Van Landeghem",
      "RubÃ¨n Tito",
      "Åukasz Borchmann",
      "MichaÅ Pietruszka",
      "Pawel Joziak",
      "Rafal Powalski",
      "Dawid Jurkiewicz",
      "Mickael Coustaty",
      "Bertrand Anckaert",
      "Ernest Valveny",
      "Matthew Blaschko",
      "Sien Moens",
      "Tomasz Stanislawek"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_ALWOD_Active_Learning_for_Weakly-Supervised_Object_Detection_ICCV_2023_paper.html": {
    "title": "ALWOD: Active Learning for Weakly-Supervised Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuting Wang",
      "Velibor Ilic",
      "Jiatong Li",
      "Branislav KisaÄanin",
      "Vladimir Pavlovic"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Prototypical_Kernel_Learning_and_Open-set_Foreground_Perception_for_Generalized_Few-shot_ICCV_2023_paper.html": {
    "title": "Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Huang",
      "Feigege Wang",
      "Ye Xi",
      "Yutao Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Noh_Simple_and_Effective_Out-of-Distribution_Detection_via_Cosine-based_Softmax_Loss_ICCV_2023_paper.html": {
    "title": "Simple and Effective Out-of-Distribution Detection via Cosine-based Softmax Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SoonCheol Noh",
      "DongEon Jeong",
      "Jee-Hyong Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_CFCG_Semi-Supervised_Semantic_Segmentation_via_Cross-Fusion_and_Contour_Guidance_Supervision_ICCV_2023_paper.html": {
    "title": "CFCG: Semi-Supervised Semantic Segmentation via Cross-Fusion and Contour Guidance Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Li",
      "Yue He",
      "Weiming Zhang",
      "Wei Zhang",
      "Xiao Tan",
      "Junyu Han",
      "Errui Ding",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_CHAMPAGNE_Learning_Real-world_Conversation_from_Large-Scale_Web_Videos_ICCV_2023_paper.html": {
    "title": "CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungju Han",
      "Jack Hessel",
      "Nouha Dziri",
      "Yejin Choi",
      "Youngjae Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_SLAN_Self-Locator_Aided_Network_for_Vision-Language_Understanding_ICCV_2023_paper.html": {
    "title": "SLAN: Self-Locator Aided Network for Vision-Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiang-Tian Zhai",
      "Qi Zhang",
      "Tong Wu",
      "Xing-Yu Chen",
      "Jiang-Jiang Liu",
      "Ming-Ming Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_S-VolSDF_Sparse_Multi-View_Stereo_Regularization_of_Neural_Implicit_Surfaces_ICCV_2023_paper.html": {
    "title": "S-VolSDF: Sparse Multi-View Stereo Regularization of Neural Implicit Surfaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Wu",
      "Alexandros Graikos",
      "Dimitris Samaras"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shin_Anomaly_Detection_using_Score-based_Perturbation_Resilience_ICCV_2023_paper.html": {
    "title": "Anomaly Detection using Score-based Perturbation Resilience",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woosang Shin",
      "Jonghyeon Lee",
      "Taehan Lee",
      "Sangmoon Lee",
      "Jong Pil Yun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Generating_Visual_Scenes_from_Touch_ICCV_2023_paper.html": {
    "title": "Generating Visual Scenes from Touch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyu Yang",
      "Jiacheng Zhang",
      "Andrew Owens"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_DeformToon3D_Deformable_Neural_Radiance_Fields_for_3D_Toonification_ICCV_2023_paper.html": {
    "title": "DeformToon3D: Deformable Neural Radiance Fields for 3D Toonification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junzhe Zhang",
      "Yushi Lan",
      "Shuai Yang",
      "Fangzhou Hong",
      "Quan Wang",
      "Chai Kiat Yeo",
      "Ziwei Liu",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bastani_SatlasPretrain_A_Large-Scale_Dataset_for_Remote_Sensing_Image_Understanding_ICCV_2023_paper.html": {
    "title": "SatlasPretrain: A Large-Scale Dataset for Remote Sensing Image Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Favyen Bastani",
      "Piper Wolters",
      "Ritwik Gupta",
      "Joe Ferdinando",
      "Aniruddha Kembhavi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Empowering_Low-Light_Image_Enhancer_through_Customized_Learnable_Priors_ICCV_2023_paper.html": {
    "title": "Empowering Low-Light Image Enhancer through Customized Learnable Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naishan Zheng",
      "Man Zhou",
      "Yanmeng Dong",
      "Xiangyu Rui",
      "Jie Huang",
      "Chongyi Li",
      "Feng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye-Bin_TextManiA_Enriching_Visual_Feature_by_Text-driven_Manifold_Augmentation_ICCV_2023_paper.html": {
    "title": "TextManiA: Enriching Visual Feature by Text-driven Manifold Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moon Ye-Bin",
      "Jisoo Kim",
      "Hongyeob Kim",
      "Kilho Son",
      "Tae-Hyun Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kornblith_Guiding_Image_Captioning_Models_Toward_More_Specific_Captions_ICCV_2023_paper.html": {
    "title": "Guiding Image Captioning Models Toward More Specific Captions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Kornblith",
      "Lala Li",
      "Zirui Wang",
      "Thao Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bitton-Guetta_Breaking_Common_Sense_WHOOPS_A_Vision-and-Language_Benchmark_of_Synthetic_and_ICCV_2023_paper.html": {
    "title": "Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nitzan Bitton-Guetta",
      "Yonatan Bitton",
      "Jack Hessel",
      "Ludwig Schmidt",
      "Yuval Elovici",
      "Gabriel Stanovsky",
      "Roy Schwartz"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cai_Consistent_Depth_Prediction_for_Transparent_Object_Reconstruction_from_RGB-D_Camera_ICCV_2023_paper.html": {
    "title": "Consistent Depth Prediction for Transparent Object Reconstruction from RGB-D Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Cai",
      "Yifan Zhu",
      "Haiwei Zhang",
      "Bo Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_DReg-NeRF_Deep_Registration_for_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "DReg-NeRF: Deep Registration for Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Chen",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_DETR_Does_Not_Need_Multi-Scale_or_Locality_Design_ICCV_2023_paper.html": {
    "title": "DETR Does Not Need Multi-Scale or Locality Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Lin",
      "Yuhui Yuan",
      "Zheng Zhang",
      "Chen Li",
      "Nanning Zheng",
      "Han Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Towards_Effective_Instance_Discrimination_Contrastive_Loss_for_Unsupervised_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "Towards Effective Instance Discrimination Contrastive Loss for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Zhang",
      "Zilei Wang",
      "Junjie Li",
      "Jiafan Zhuang",
      "Zihan Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hakim_ClusT3_Information_Invariant_Test-Time_Training_ICCV_2023_paper.html": {
    "title": "ClusT3: Information Invariant Test-Time Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gustavo A. Vargas Hakim",
      "David Osowiechi",
      "Mehrdad Noori",
      "Milad Cheraghalikhani",
      "Ali Bahri",
      "Ismail Ben Ayed",
      "Christian Desrosiers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_FrozenRecon_Pose-free_3D_Scene_Reconstruction_with_Frozen_Depth_Models_ICCV_2023_paper.html": {
    "title": "FrozenRecon: Pose-free 3D Scene Reconstruction with Frozen Depth Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangkai Xu",
      "Wei Yin",
      "Hao Chen",
      "Chunhua Shen",
      "Kai Cheng",
      "Feng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Weng_Affective_Image_Filter_Reflecting_Emotions_from_Text_to_Images_ICCV_2023_paper.html": {
    "title": "Affective Image Filter: Reflecting Emotions from Text to Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuchen Weng",
      "Peixuan Zhang",
      "Zheng Chang",
      "Xinlong Wang",
      "Si Li",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Park_Content-Aware_Local_GAN_for_Photo-Realistic_Super-Resolution_ICCV_2023_paper.html": {
    "title": "Content-Aware Local GAN for Photo-Realistic Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JoonKyu Park",
      "Sanghyun Son",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Structure-Aware_Surface_Reconstruction_via_Primitive_Assembly_ICCV_2023_paper.html": {
    "title": "Structure-Aware Surface Reconstruction via Primitive Assembly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingen Jiang",
      "Mingyang Zhao",
      "Shiqing Xin",
      "Yanchao Yang",
      "Hanxiao Wang",
      "Xiaohong Jia",
      "Dong-Ming Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_FineDance_A_Fine-grained_Choreography_Dataset_for_3D_Full_Body_Dance_ICCV_2023_paper.html": {
    "title": "FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronghui Li",
      "Junfan Zhao",
      "Yachao Zhang",
      "Mingyang Su",
      "Zeping Ren",
      "Han Zhang",
      "Yansong Tang",
      "Xiu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xiangli_AssetField_Assets_Mining_and_Reconfiguration_in_Ground_Feature_Plane_Representation_ICCV_2023_paper.html": {
    "title": "AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanbo Xiangli",
      "Linning Xu",
      "Xingang Pan",
      "Nanxuan Zhao",
      "Bo Dai",
      "Dahua Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Can_Improving_Online_Lane_Graph_Extraction_by_Object-Lane_Clustering_ICCV_2023_paper.html": {
    "title": "Improving Online Lane Graph Extraction by Object-Lane Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yigit Baran Can",
      "Alexander Liniger",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Stolik_SAGA_Spectral_Adversarial_Geometric_Attack_on_3D_Meshes_ICCV_2023_paper.html": {
    "title": "SAGA: Spectral Adversarial Geometric Attack on 3D Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomer Stolik",
      "Itai Lang",
      "Shai Avidan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ning_All_in_Tokens_Unifying_Output_Space_of_Visual_Tasks_via_ICCV_2023_paper.html": {
    "title": "All in Tokens: Unifying Output Space of Visual Tasks via Soft Token",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia Ning",
      "Chen Li",
      "Zheng Zhang",
      "Chunyu Wang",
      "Zigang Geng",
      "Qi Dai",
      "Kun He",
      "Han Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hong_Learning_Navigational_Visual_Representations_with_Semantic_Map_Supervision_ICCV_2023_paper.html": {
    "title": "Learning Navigational Visual Representations with Semantic Map Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicong Hong",
      "Yang Zhou",
      "Ruiyi Zhang",
      "Franck Dernoncourt",
      "Trung Bui",
      "Stephen Gould",
      "Hao Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_LDL_Line_Distance_Functions_for_Panoramic_Localization_ICCV_2023_paper.html": {
    "title": "LDL: Line Distance Functions for Panoramic Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junho Kim",
      "Changwoon Choi",
      "Hojun Jang",
      "Young Min Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_TransTIC_Transferring_Transformer-based_Image_Compression_from_Human_Perception_to_Machine_ICCV_2023_paper.html": {
    "title": "TransTIC: Transferring Transformer-based Image Compression from Human Perception to Machine Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Hsin Chen",
      "Ying-Chieh Weng",
      "Chia-Hao Kao",
      "Cheng Chien",
      "Wei-Chen Chiu",
      "Wen-Hsiao Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_CHORUS__Learning_Canonicalized_3D_Human-Object_Spatial_Relations_from_Unbounded_ICCV_2023_paper.html": {
    "title": "CHORUS : Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sookwan Han",
      "Hanbyul Joo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chung_Shortcut-V2V_Compression_Framework_for_Video-to-Video_Translation_Based_on_Temporal_Redundancy_ICCV_2023_paper.html": {
    "title": "Shortcut-V2V: Compression Framework for Video-to-Video Translation Based on Temporal Redundancy Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaeyeon Chung",
      "Yeojeong Park",
      "Seunghwan Choi",
      "Munkhsoyol Ganbat",
      "Jaegul Choo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Varma_ViLLA_Fine-Grained_Vision-Language_Representation_Learning_from_Real-World_Data_ICCV_2023_paper.html": {
    "title": "ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maya Varma",
      "Jean-Benoit Delbrouck",
      "Sarah Hooper",
      "Akshay Chaudhari",
      "Curtis Langlotz"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ren_SG-Former_Self-guided_Transformer_with_Evolving_Token_Reallocation_ICCV_2023_paper.html": {
    "title": "SG-Former: Self-guided Transformer with Evolving Token Reallocation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sucheng Ren",
      "Xingyi Yang",
      "Songhua Liu",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Towards_Unifying_Medical_Vision-and-Language_Pre-Training_via_Soft_Prompts_ICCV_2023_paper.html": {
    "title": "Towards Unifying Medical Vision-and-Language Pre-Training via Soft Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihong Chen",
      "Shizhe Diao",
      "Benyou Wang",
      "Guanbin Li",
      "Xiang Wan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Deng_A_Large-scale_Study_of_Spatiotemporal_Representation_Learning_with_a_New_ICCV_2023_paper.html": {
    "title": "A Large-scale Study of Spatiotemporal Representation Learning with a New Benchmark on Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andong Deng",
      "Taojiannan Yang",
      "Chen Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhuo_Video_Background_Music_Generation_Dataset_Method_and_Evaluation_ICCV_2023_paper.html": {
    "title": "Video Background Music Generation: Dataset, Method and Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Zhuo",
      "Zhaokai Wang",
      "Baisen Wang",
      "Yue Liao",
      "Chenxi Bao",
      "Stanley Peng",
      "Songhao Han",
      "Aixi Zhang",
      "Fei Fang",
      "Si Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Karnewar_HoloFusion_Towards_Photo-realistic_3D_Generative_Modeling_ICCV_2023_paper.html": {
    "title": "HoloFusion: Towards Photo-realistic 3D Generative Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Animesh Karnewar",
      "Niloy J. Mitra",
      "Andrea Vedaldi",
      "David Novotny"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tang_ProtoTransfer_Cross-Modal_Prototype_Transfer_for_Point_Cloud_Segmentation_ICCV_2023_paper.html": {
    "title": "ProtoTransfer: Cross-Modal Prototype Transfer for Point Cloud Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pin Tang",
      "Hai-Ming Xu",
      "Chao Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Improving_Continuous_Sign_Language_Recognition_with_Cross-Lingual_Signs_ICCV_2023_paper.html": {
    "title": "Improving Continuous Sign Language Recognition with Cross-Lingual Signs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangyun Wei",
      "Yutong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Aziere_Markov_Game_Video_Augmentation_for_Action_Segmentation_ICCV_2023_paper.html": {
    "title": "Markov Game Video Augmentation for Action Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Aziere",
      "Sinisa Todorovic"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Niu_Deep_Image_Harmonization_with_Globally_Guided_Feature_Transformation_and_Relation_ICCV_2023_paper.html": {
    "title": "Deep Image Harmonization with Globally Guided Feature Transformation and Relation Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Niu",
      "Linfeng Tan",
      "Xinhao Tao",
      "Junyan Cao",
      "Fengjun Guo",
      "Teng Long",
      "Liqing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_TransIFF_An_Instance-Level_Feature_Fusion_Framework_for_Vehicle-Infrastructure_Cooperative_3D_ICCV_2023_paper.html": {
    "title": "TransIFF: An Instance-Level Feature Fusion Framework for Vehicle-Infrastructure Cooperative 3D Detection with Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziming Chen",
      "Yifeng Shi",
      "Jinrang Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_RegFormer_An_Efficient_Projection-Aware_Transformer_Network_for_Large-Scale_Point_Cloud_ICCV_2023_paper.html": {
    "title": "RegFormer: An Efficient Projection-Aware Transformer Network for Large-Scale Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuming Liu",
      "Guangming Wang",
      "Zhe Liu",
      "Chaokang Jiang",
      "Marc Pollefeys",
      "Hesheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Masked_Retraining_Teacher-Student_Framework_for_Domain_Adaptive_Object_Detection_ICCV_2023_paper.html": {
    "title": "Masked Retraining Teacher-Student Framework for Domain Adaptive Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijing Zhao",
      "Sitong Wei",
      "Qingchao Chen",
      "Dehui Li",
      "Yifan Yang",
      "Yuxin Peng",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ding_Prune_Spatio-temporal_Tokens_by_Semantic-aware_Temporal_Accumulation_ICCV_2023_paper.html": {
    "title": "Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuangrui Ding",
      "Peisen Zhao",
      "Xiaopeng Zhang",
      "Rui Qian",
      "Hongkai Xiong",
      "Qi Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sargent_VQ3D_Learning_a_3D-Aware_Generative_Model_on_ImageNet_ICCV_2023_paper.html": {
    "title": "VQ3D: Learning a 3D-Aware Generative Model on ImageNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyle Sargent",
      "Jing Yu Koh",
      "Han Zhang",
      "Huiwen Chang",
      "Charles Herrmann",
      "Pratul Srinivasan",
      "Jiajun Wu",
      "Deqing Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jin_Growing_a_Brain_with_Sparsity-Inducing_Generation_for_Continual_Learning_ICCV_2023_paper.html": {
    "title": "Growing a Brain with Sparsity-Inducing Generation for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyundong Jin",
      "Gyeong-hyeon Kim",
      "Chanho Ahn",
      "Eunwoo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Cross-Ray_Neural_Radiance_Fields_for_Novel-View_Synthesis_from_Unconstrained_Image_ICCV_2023_paper.html": {
    "title": "Cross-Ray Neural Radiance Fields for Novel-View Synthesis from Unconstrained Image Collections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Yang",
      "Shuhai Zhang",
      "Zixiong Huang",
      "Yubing Zhang",
      "Mingkui Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Seo_Graphics2RAW_Mapping_Computer_Graphics_Images_to_Sensor_RAW_Images_ICCV_2023_paper.html": {
    "title": "Graphics2RAW: Mapping Computer Graphics Images to Sensor RAW Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghwan Seo",
      "Abhijith Punnappurath",
      "Luxi Zhao",
      "Abdelrahman Abdelhamed",
      "Sai Kiran Tedla",
      "Sanguk Park",
      "Jihwan Choe",
      "Michael S. Brown"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gururani_SPACE_Speech-driven_Portrait_Animation_with_Controllable_Expression_ICCV_2023_paper.html": {
    "title": "SPACE: Speech-driven Portrait Animation with Controllable Expression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddharth Gururani",
      "Arun Mallya",
      "Ting-Chun Wang",
      "Rafael Valle",
      "Ming-Yu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_2D-3D_Interlaced_Transformer_for_Point_Cloud_Segmentation_with_Scene-Level_Supervision_ICCV_2023_paper.html": {
    "title": "2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-Kun Yang",
      "Min-Hung Chen",
      "Yung-Yu Chuang",
      "Yen-Yu Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Collecting_The_Puzzle_Pieces_Disentangled_Self-Driven_Human_Pose_Transfer_by_ICCV_2023_paper.html": {
    "title": "Collecting The Puzzle Pieces: Disentangled Self-Driven Human Pose Transfer by Permuting Textures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nannan Li",
      "Kevin J Shih",
      "Bryan A. Plummer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_VAD_Vectorized_Scene_Representation_for_Efficient_Autonomous_Driving_ICCV_2023_paper.html": {
    "title": "VAD: Vectorized Scene Representation for Efficient Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Jiang",
      "Shaoyu Chen",
      "Qing Xu",
      "Bencheng Liao",
      "Jiajie Chen",
      "Helong Zhou",
      "Qian Zhang",
      "Wenyu Liu",
      "Chang Huang",
      "Xinggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_End-to-end_3D_Tracking_with_Decoupled_Queries_ICCV_2023_paper.html": {
    "title": "End-to-end 3D Tracking with Decoupled Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanwei Li",
      "Zhiding Yu",
      "Jonah Philion",
      "Anima Anandkumar",
      "Sanja Fidler",
      "Jiaya Jia",
      "Jose Alvarez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Sound_Localization_from_Motion_Jointly_Learning_Sound_Direction_and_Camera_ICCV_2023_paper.html": {
    "title": "Sound Localization from Motion: Jointly Learning Sound Direction and Camera Rotation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Chen",
      "Shengyi Qian",
      "Andrew Owens"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Batch-based_Model_Registration_for_Fast_3D_Sherd_Reconstruction_ICCV_2023_paper.html": {
    "title": "Batch-based Model Registration for Fast 3D Sherd Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiepeng Wang",
      "Congyi Zhang",
      "Peng Wang",
      "Xin Li",
      "Peter J. Cobb",
      "Christian Theobalt",
      "Wenping Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chai_HiFace_High-Fidelity_3D_Face_Reconstruction_by_Learning_Static_and_Dynamic_ICCV_2023_paper.html": {
    "title": "HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and Dynamic Details",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zenghao Chai",
      "Tianke Zhang",
      "Tianyu He",
      "Xu Tan",
      "Tadas Baltrusaitis",
      "HsiangTao Wu",
      "Runnan Li",
      "Sheng Zhao",
      "Chun Yuan",
      "Jiang Bian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Fast_and_Accurate_Transferability_Measurement_by_Evaluating_Intra-class_Feature_Variance_ICCV_2023_paper.html": {
    "title": "Fast and Accurate Transferability Measurement by Evaluating Intra-class Feature Variance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiwen Xu",
      "U Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Deformable_Model-Driven_Neural_Rendering_for_High-Fidelity_3D_Reconstruction_of_Human_ICCV_2023_paper.html": {
    "title": "Deformable Model-Driven Neural Rendering for High-Fidelity 3D Reconstruction of Human Heads Under Low-View Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baixin Xu",
      "Jiarui Zhang",
      "Kwan-Yee Lin",
      "Chen Qian",
      "Ying He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Algebraically_Rigorous_Quaternion_Framework_for_the_Neural_Network_Pose_Estimation_ICCV_2023_paper.html": {
    "title": "Algebraically Rigorous Quaternion Framework for the Neural Network Pose Estimation Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Lin",
      "Andrew J. Hanson",
      "Sonya M. Hanson"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_Prompt_Tuning_Inversion_for_Text-driven_Image_Editing_Using_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Prompt Tuning Inversion for Text-driven Image Editing Using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenkai Dong",
      "Song Xue",
      "Xiaoyue Duan",
      "Shumin Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_CVSformer_Cross-View_Synthesis_Transformer_for_Semantic_Scene_Completion_ICCV_2023_paper.html": {
    "title": "CVSformer: Cross-View Synthesis Transformer for Semantic Scene Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Dong",
      "Enhui Ma",
      "Lubo Wang",
      "Miaohui Wang",
      "Wuyuan Xie",
      "Qing Guo",
      "Ping Li",
      "Lingyu Liang",
      "Kairui Yang",
      "Di Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_UrbanGIRAFFE_Representing_Urban_Scenes_as_Compositional_Generative_Neural_Feature_Fields_ICCV_2023_paper.html": {
    "title": "UrbanGIRAFFE: Representing Urban Scenes as Compositional Generative Neural Feature Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanbo Yang",
      "Yifei Yang",
      "Hanlei Guo",
      "Rong Xiong",
      "Yue Wang",
      "Yiyi Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fu_UnitedHuman_Harnessing_Multi-Source_Data_for_High-Resolution_Human_Generation_ICCV_2023_paper.html": {
    "title": "UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianglin Fu",
      "Shikai Li",
      "Yuming Jiang",
      "Kwan-Yee Lin",
      "Wayne Wu",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yan_Active_Neural_Mapping_ICCV_2023_paper.html": {
    "title": "Active Neural Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zike Yan",
      "Haoxiang Yang",
      "Hongbin Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Density-invariant_Features_for_Distant_Point_Cloud_Registration_ICCV_2023_paper.html": {
    "title": "Density-invariant Features for Distant Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan Liu",
      "Hongzi Zhu",
      "Yunsong Zhou",
      "Hongyang Li",
      "Shan Chang",
      "Minyi Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Butoi_UniverSeg_Universal_Medical_Image_Segmentation_ICCV_2023_paper.html": {
    "title": "UniverSeg: Universal Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Ion Butoi",
      "Jose Javier Gonzalez Ortiz",
      "Tianyu Ma",
      "Mert R. Sabuncu",
      "John Guttag",
      "Adrian V. Dalca"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liao_RecRecNet_Rectangling_Rectified_Wide-Angle_Images_by_Thin-Plate_Spline_Model_and_ICCV_2023_paper.html": {
    "title": "RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang Liao",
      "Lang Nie",
      "Chunyu Lin",
      "Zishuo Zheng",
      "Yao Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mai_Neural_Microfacet_Fields_for_Inverse_Rendering_ICCV_2023_paper.html": {
    "title": "Neural Microfacet Fields for Inverse Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Mai",
      "Dor Verbin",
      "Falko Kuester",
      "Sara Fridovich-Keil"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Understanding_Self-attention_Mechanism_via_Dynamical_System_Perspective_ICCV_2023_paper.html": {
    "title": "Understanding Self-attention Mechanism via Dynamical System Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongzhan Huang",
      "Mingfu Liang",
      "Jinghui Qin",
      "Shanshan Zhong",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Luo_Learning_Versatile_3D_Shape_Generation_with_Improved_Auto-regressive_Models_ICCV_2023_paper.html": {
    "title": "Learning Versatile 3D Shape Generation with Improved Auto-regressive Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simian Luo",
      "Xuelin Qian",
      "Yanwei Fu",
      "Yinda Zhang",
      "Ying Tai",
      "Zhenyu Zhang",
      "Chengjie Wang",
      "Xiangyang Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_DETA_Denoised_Task_Adaptation_for_Few-Shot_Learning_ICCV_2023_paper.html": {
    "title": "DETA: Denoised Task Adaptation for Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Zhang",
      "Lianli Gao",
      "Xu Luo",
      "Hengtao Shen",
      "Jingkuan Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tang_DDG-Net_Discriminability-Driven_Graph_Network_for_Weakly-supervised_Temporal_Action_Localization_ICCV_2023_paper.html": {
    "title": "DDG-Net: Discriminability-Driven Graph Network for Weakly-supervised Temporal Action Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojun Tang",
      "Junsong Fan",
      "Chuanchen Luo",
      "Zhaoxiang Zhang",
      "Man Zhang",
      "Zongyuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Diffusion_Models_as_Masked_Autoencoders_ICCV_2023_paper.html": {
    "title": "Diffusion Models as Masked Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Wei",
      "Karttikeya Mangalam",
      "Po-Yao Huang",
      "Yanghao Li",
      "Haoqi Fan",
      "Hu Xu",
      "Huiyu Wang",
      "Cihang Xie",
      "Alan Yuille",
      "Christoph Feichtenhofer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Delattre_Robust_Frame-to-Frame_Camera_Rotation_Estimation_in_Crowded_Scenes_ICCV_2023_paper.html": {
    "title": "Robust Frame-to-Frame Camera Rotation Estimation in Crowded Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabien Delattre",
      "David Dirnfeld",
      "Phat Nguyen",
      "Stephen K Scarano",
      "Michael J Jones",
      "Pedro Miraldo",
      "Erik Learned-Miller"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Derakhshani_Bayesian_Prompt_Learning_for_Image-Language_Model_Generalization_ICCV_2023_paper.html": {
    "title": "Bayesian Prompt Learning for Image-Language Model Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Mahdi Derakhshani",
      "Enrique Sanchez",
      "Adrian Bulat",
      "Victor G. Turrisi da Costa",
      "Cees G.M. Snoek",
      "Georgios Tzimiropoulos",
      "Brais Martinez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Drehwald_One-Shot_Recognition_of_Any_Material_Anywhere_Using_Contrastive_Learning_with_ICCV_2023_paper.html": {
    "title": "One-Shot Recognition of Any Material Anywhere Using Contrastive Learning with Physics-Based Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel S. Drehwald",
      "Sagi Eppel",
      "Jolina Li",
      "Han Hao",
      "Alan Aspuru-Guzik"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_DiLiGenT-Pi_Photometric_Stereo_for_Planar_Surfaces_with_Rich_Details_-_ICCV_2023_paper.html": {
    "title": "DiLiGenT-Pi: Photometric Stereo for Planar Surfaces with Rich Details - Benchmark Dataset and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feishi Wang",
      "Jieji Ren",
      "Heng Guo",
      "Mingjun Ren",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Rethinking_Data_Distillation_Do_Not_Overlook_Calibration_ICCV_2023_paper.html": {
    "title": "Rethinking Data Distillation: Do Not Overlook Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyao Zhu",
      "Bowen Lei",
      "Jie Zhang",
      "Yanbo Fang",
      "Yiqun Xie",
      "Ruqi Zhang",
      "Dongkuan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shen_Accurate_and_Fast_Compressed_Video_Captioning_ICCV_2023_paper.html": {
    "title": "Accurate and Fast Compressed Video Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaojie Shen",
      "Xin Gu",
      "Kai Xu",
      "Heng Fan",
      "Longyin Wen",
      "Libo Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Building_Vision_Transformers_with_Hierarchy_Aware_Feature_Aggregation_ICCV_2023_paper.html": {
    "title": "Building Vision Transformers with Hierarchy Aware Feature Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongjie Chen",
      "Hongmin Liu",
      "Haoran Yin",
      "Bin Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fang_Visible-Infrared_Person_Re-Identification_via_Semantic_Alignment_and_Affinity_Inference_ICCV_2023_paper.html": {
    "title": "Visible-Infrared Person Re-Identification via Semantic Alignment and Affinity Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingye Fang",
      "Yang Yang",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_SAL-ViT_Towards_Latency_Efficient_Private_Inference_on_ViT_using_Selective_ICCV_2023_paper.html": {
    "title": "SAL-ViT: Towards Latency Efficient Private Inference on ViT using Selective Attention Search with a Learnable Softmax Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuke Zhang",
      "Dake Chen",
      "Souvik Kundu",
      "Chenghao Li",
      "Peter A. Beerel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sur_TIJO_Trigger_Inversion_with_Joint_Optimization_for_Defending_Multimodal_Backdoored_ICCV_2023_paper.html": {
    "title": "TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Indranil Sur",
      "Karan Sikka",
      "Matthew Walmer",
      "Kaushik Koneripalli",
      "Anirban Roy",
      "Xiao Lin",
      "Ajay Divakaran",
      "Susmit Jha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zuo_DG3D_Generating_High_Quality_3D_Textured_Shapes_by_Learning_to_ICCV_2023_paper.html": {
    "title": "DG3D: Generating High Quality 3D Textured Shapes by Learning to Discriminate Multi-Modal Diffusion-Renderings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zuo",
      "Yafei Song",
      "Jianfang Li",
      "Lin Liu",
      "Liefeng Bo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Improving_Adversarial_Robustness_of_Masked_Autoencoders_via_Test-time_Frequency-domain_Prompting_ICCV_2023_paper.html": {
    "title": "Improving Adversarial Robustness of Masked Autoencoders via Test-time Frequency-domain Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qidong Huang",
      "Xiaoyi Dong",
      "Dongdong Chen",
      "Yinpeng Chen",
      "Lu Yuan",
      "Gang Hua",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_HairCLIPv2_Unifying_Hair_Editing_via_Proxy_Feature_Blending_ICCV_2023_paper.html": {
    "title": "HairCLIPv2: Unifying Hair Editing via Proxy Feature Blending",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Wei",
      "Dongdong Chen",
      "Wenbo Zhou",
      "Jing Liao",
      "Weiming Zhang",
      "Gang Hua",
      "Nenghai Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Slyman_VLSlice_Interactive_Vision-and-Language_Slice_Discovery_ICCV_2023_paper.html": {
    "title": "VLSlice: Interactive Vision-and-Language Slice Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Slyman",
      "Minsuk Kahng",
      "Stefan Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mavroudi_Learning_to_Ground_Instructional_Articles_in_Videos_through_Narrations_ICCV_2023_paper.html": {
    "title": "Learning to Ground Instructional Articles in Videos through Narrations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Effrosyni Mavroudi",
      "Triantafyllos Afouras",
      "Lorenzo Torresani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liao_DocTr_Document_Transformer_for_Structured_Information_Extraction_in_Documents_ICCV_2023_paper.html": {
    "title": "DocTr: Document Transformer for Structured Information Extraction in Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haofu Liao",
      "Aruni RoyChowdhury",
      "Weijian Li",
      "Ankan Bansal",
      "Yuting Zhang",
      "Zhuowen Tu",
      "Ravi Kumar Satzoda",
      "R. Manmatha",
      "Vijay Mahadevan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lamdouar_The_Making_and_Breaking_of_Camouflage_ICCV_2023_paper.html": {
    "title": "The Making and Breaking of Camouflage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hala Lamdouar",
      "Weidi Xie",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tanaka_Role-Aware_Interaction_Generation_from_Textual_Description_ICCV_2023_paper.html": {
    "title": "Role-Aware Interaction Generation from Textual Description",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikihiro Tanaka",
      "Kent Fujiwara"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yasarla_MAMo_Leveraging_Memory_and_Attention_for_Monocular_Video_Depth_Estimation_ICCV_2023_paper.html": {
    "title": "MAMo: Leveraging Memory and Attention for Monocular Video Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajeev Yasarla",
      "Hong Cai",
      "Jisoo Jeong",
      "Yunxiao Shi",
      "Risheek Garrepalli",
      "Fatih Porikli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ahuja_Continual_Learning_for_Personalized_Co-speech_Gesture_Generation_ICCV_2023_paper.html": {
    "title": "Continual Learning for Personalized Co-speech Gesture Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaitanya Ahuja",
      "Pratik Joshi",
      "Ryo Ishii",
      "Louis-Philippe Morency"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Object_as_Query_Lifting_Any_2D_Object_Detector_to_3D_ICCV_2023_paper.html": {
    "title": "Object as Query: Lifting Any 2D Object Detector to 3D Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitian Wang",
      "Zehao Huang",
      "Jiahui Fu",
      "Naiyan Wang",
      "Si Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xing_HDG-ODE_A_Hierarchical_Continuous-Time_Model_for_Human_Pose_Forecasting_ICCV_2023_paper.html": {
    "title": "HDG-ODE: A Hierarchical Continuous-Time Model for Human Pose Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucheng Xing",
      "Xin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Versatile_Diffusion_Text_Images_and_Variations_All_in_One_Diffusion_ICCV_2023_paper.html": {
    "title": "Versatile Diffusion: Text, Images and Variations All in One Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingqian Xu",
      "Zhangyang Wang",
      "Gong Zhang",
      "Kai Wang",
      "Humphrey Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_DreamTeacher_Pretraining_Image_Backbones_with_Deep_Generative_Models_ICCV_2023_paper.html": {
    "title": "DreamTeacher: Pretraining Image Backbones with Deep Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daiqing Li",
      "Huan Ling",
      "Amlan Kar",
      "David Acuna",
      "Seung Wook Kim",
      "Karsten Kreis",
      "Antonio Torralba",
      "Sanja Fidler"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lei_Decomposition-Based_Variational_Network_for_Multi-Contrast_MRI_Super-Resolution_and_Reconstruction_ICCV_2023_paper.html": {
    "title": "Decomposition-Based Variational Network for Multi-Contrast MRI Super-Resolution and Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengcheng Lei",
      "Faming Fang",
      "Guixu Zhang",
      "Tieyong Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Varghese_Self-supervised_Monocular_Underwater_Depth_Recovery_Image_Restoration_and_a_Real-sea_ICCV_2023_paper.html": {
    "title": "Self-supervised Monocular Underwater Depth Recovery, Image Restoration, and a Real-sea Video Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nisha Varghese",
      "Ashish Kumar",
      "A. N. Rajagopalan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Geometrized_Transformer_for_Self-Supervised_Homography_Estimation_ICCV_2023_paper.html": {
    "title": "Geometrized Transformer for Self-Supervised Homography Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazhen Liu",
      "Xirong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qian_Sat2Density_Faithful_Density_Learning_from_Satellite-Ground_Image_Pairs_ICCV_2023_paper.html": {
    "title": "Sat2Density: Faithful Density Learning from Satellite-Ground Image Pairs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Qian",
      "Jincheng Xiong",
      "Gui-Song Xia",
      "Nan Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shah_TiDy-PSFs_Computational_Imaging_with_Time-Averaged_Dynamic_Point-Spread-Functions_ICCV_2023_paper.html": {
    "title": "TiDy-PSFs: Computational Imaging with Time-Averaged Dynamic Point-Spread-Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sachin Shah",
      "Sakshum Kulshrestha",
      "Christopher A. Metzler"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ge_Expressive_Text-to-Image_Generation_with_Rich_Text_ICCV_2023_paper.html": {
    "title": "Expressive Text-to-Image Generation with Rich Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songwei Ge",
      "Taesung Park",
      "Jun-Yan Zhu",
      "Jia-Bin Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Learning_Fine-Grained_Features_for_Pixel-Wise_Video_Correspondences_ICCV_2023_paper.html": {
    "title": "Learning Fine-Grained Features for Pixel-Wise Video Correspondences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Li",
      "Shenglong Zhou",
      "Dong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bulat_FS-DETR_Few-Shot_DEtection_TRansformer_with_Prompting_and_without_Re-Training_ICCV_2023_paper.html": {
    "title": "FS-DETR: Few-Shot DEtection TRansformer with Prompting and without Re-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Bulat",
      "Ricardo Guerrero",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Semi-supervised_Speech-driven_3D_Facial_Animation_via_Cross-modal_Encoding_ICCV_2023_paper.html": {
    "title": "Semi-supervised Speech-driven 3D Facial Animation via Cross-modal Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiji Yang",
      "Huawei Wei",
      "Yicheng Zhong",
      "Zhisheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Singh_Learning_to_Learn_How_to_Continuously_Teach_Humans_and_Machines_ICCV_2023_paper.html": {
    "title": "Learning to Learn: How to Continuously Teach Humans and Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parantak Singh",
      "You Li",
      "Ankur Sikarwar",
      "Stan Weixian Lei",
      "Difei Gao",
      "Morgan B. Talbot",
      "Ying Sun",
      "Mike Zheng Shou",
      "Gabriel Kreiman",
      "Mengmi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Text-Driven_Generative_Domain_Adaptation_with_Spectral_Consistency_Regularization_ICCV_2023_paper.html": {
    "title": "Text-Driven Generative Domain Adaptation with Spectral Consistency Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenhuan Liu",
      "Liang Li",
      "Jiayu Xiao",
      "Zheng-Jun Zha",
      "Qingming Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_A_5-Point_Minimal_Solver_for_Event_Camera_Relative_Motion_Estimation_ICCV_2023_paper.html": {
    "title": "A 5-Point Minimal Solver for Event Camera Relative Motion Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling Gao",
      "Hang Su",
      "Daniel Gehrig",
      "Marco Cannici",
      "Davide Scaramuzza",
      "Laurent Kneip"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gong_TM2D_Bimodality_Driven_3D_Dance_Generation_via_Music-Text_Integration_ICCV_2023_paper.html": {
    "title": "TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kehong Gong",
      "Dongze Lian",
      "Heng Chang",
      "Chuan Guo",
      "Zihang Jiang",
      "Xinxin Zuo",
      "Michael Bi Mi",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Bootstrap_Motion_Forecasting_With_Self-Consistent_Constraints_ICCV_2023_paper.html": {
    "title": "Bootstrap Motion Forecasting With Self-Consistent Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maosheng Ye",
      "Jiamiao Xu",
      "Xunnong Xu",
      "Tengfei Wang",
      "Tongyi Cao",
      "Qifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_CDAC_Cross-domain_Attention_Consistency_in_Transformer_for_Domain_Adaptive_Semantic_ICCV_2023_paper.html": {
    "title": "CDAC: Cross-domain Attention Consistency in Transformer for Domain Adaptive Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaihong Wang",
      "Donghyun Kim",
      "Rogerio Feris",
      "Margrit Betke"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_WaveNeRF_Wavelet-based_Generalizable_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "WaveNeRF: Wavelet-based Generalizable Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muyu Xu",
      "Fangneng Zhan",
      "Jiahui Zhang",
      "Yingchen Yu",
      "Xiaoqin Zhang",
      "Christian Theobalt",
      "Ling Shao",
      "Shijian Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kloepfer_LoCUS_Learning_Multiscale_3D-consistent_Features_from_Posed_Images_ICCV_2023_paper.html": {
    "title": "LoCUS: Learning Multiscale 3D-consistent Features from Posed Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik A. Kloepfer",
      "Dylan Campbell",
      "JoÃ£o F. Henriques"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Neural_Reconstruction_of_Relightable_Human_Model_from_Monocular_Video_ICCV_2023_paper.html": {
    "title": "Neural Reconstruction of Relightable Human Model from Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhang Sun",
      "Yunlong Che",
      "Han Huang",
      "Yandong Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_FB-BEV_BEV_Representation_from_Forward-Backward_View_Transformations_ICCV_2023_paper.html": {
    "title": "FB-BEV: BEV Representation from Forward-Backward View Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqi Li",
      "Zhiding Yu",
      "Wenhai Wang",
      "Anima Anandkumar",
      "Tong Lu",
      "Jose M. Alvarez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_BoxSnake_Polygonal_Instance_Segmentation_with_Box_Supervision_ICCV_2023_paper.html": {
    "title": "BoxSnake: Polygonal Instance Segmentation with Box Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Yang",
      "Lin Song",
      "Yixiao Ge",
      "Xiu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xiong_Confidence-based_Visual_Dispersal_for_Few-shot_Unsupervised_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "Confidence-based Visual Dispersal for Few-shot Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhe Xiong",
      "Hui Chen",
      "Zijia Lin",
      "Sicheng Zhao",
      "Guiguang Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Event-Guided_Procedure_Planning_from_Instructional_Videos_with_Text_Supervision_ICCV_2023_paper.html": {
    "title": "Event-Guided Procedure Planning from Instructional Videos with Text Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An-Lan Wang",
      "Kun-Yu Lin",
      "Jia-Run Du",
      "Jingke Meng",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Foreground_Object_Search_by_Distilling_Composite_Image_Feature_ICCV_2023_paper.html": {
    "title": "Foreground Object Search by Distilling Composite Image Feature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Zhang",
      "Jiacheng Sui",
      "Li Niu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Flaborea_Multimodal_Motion_Conditioned_Diffusion_Model_for_Skeleton-based_Video_Anomaly_Detection_ICCV_2023_paper.html": {
    "title": "Multimodal Motion Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro Flaborea",
      "Luca Collorone",
      "Guido Maria D'Amely di Melendugno",
      "Stefano D'Arrigo",
      "Bardh Prenkaj",
      "Fabio Galasso"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_ClimateNeRF_Extreme_Weather_Synthesis_in_Neural_Radiance_Field_ICCV_2023_paper.html": {
    "title": "ClimateNeRF: Extreme Weather Synthesis in Neural Radiance Field",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Li",
      "Zhi-Hao Lin",
      "David Forsyth",
      "Jia-Bin Huang",
      "Shenlong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Samarasinghe_CDFSL-V_Cross-Domain_Few-Shot_Learning_for_Videos_ICCV_2023_paper.html": {
    "title": "CDFSL-V: Cross-Domain Few-Shot Learning for Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarinda Samarasinghe",
      "Mamshad Nayeem Rizve",
      "Navid Kardan",
      "Mubarak Shah"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Generalized_Few-Shot_Point_Cloud_Segmentation_via_Geometric_Words_ICCV_2023_paper.html": {
    "title": "Generalized Few-Shot Point Cloud Segmentation via Geometric Words",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yating Xu",
      "Conghui Hu",
      "Na Zhao",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Monte_Carlo_Linear_Clustering_with_Single-Point_Supervision_is_Enough_for_ICCV_2023_paper.html": {
    "title": "Monte Carlo Linear Clustering with Single-Point Supervision is Enough for Infrared Small Target Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyang Li",
      "Yingqian Wang",
      "Longguang Wang",
      "Fei Zhang",
      "Ting Liu",
      "Zaiping Lin",
      "Wei An",
      "Yulan Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ko_Practical_Membership_Inference_Attacks_Against_Large-Scale_Multi-Modal_Models_A_Pilot_ICCV_2023_paper.html": {
    "title": "Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models: A Pilot Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myeongseob Ko",
      "Ming Jin",
      "Chenguang Wang",
      "Ruoxi Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_TCOVIS_Temporally_Consistent_Online_Video_Instance_Segmentation_ICCV_2023_paper.html": {
    "title": "TCOVIS: Temporally Consistent Online Video Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junlong Li",
      "Bingyao Yu",
      "Yongming Rao",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Klinghoffer_Towards_Viewpoint_Robustness_in_Birds_Eye_View_Segmentation_ICCV_2023_paper.html": {
    "title": "Towards Viewpoint Robustness in Bird's Eye View Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tzofi Klinghoffer",
      "Jonah Philion",
      "Wenzheng Chen",
      "Or Litany",
      "Zan Gojcic",
      "Jungseock Joo",
      "Ramesh Raskar",
      "Sanja Fidler",
      "Jose M. Alvarez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Long-Term_Photometric_Consistent_Novel_View_Synthesis_with_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Long-Term Photometric Consistent Novel View Synthesis with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jason J. Yu",
      "Fereshteh Forghani",
      "Konstantinos G. Derpanis",
      "Marcus A. Brubaker"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Plizzari_What_Can_a_Cook_in_Italy_Teach_a_Mechanic_in_ICCV_2023_paper.html": {
    "title": "What Can a Cook in Italy Teach a Mechanic in India? Action Recognition Generalisation Over Scenarios and Locations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiara Plizzari",
      "Toby Perrett",
      "Barbara Caputo",
      "Dima Damen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kaufmann_EMDB_The_Electromagnetic_Database_of_Global_3D_Human_Pose_and_ICCV_2023_paper.html": {
    "title": "EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Kaufmann",
      "Jie Song",
      "Chen Guo",
      "Kaiyue Shen",
      "Tianjian Jiang",
      "Chengcheng Tang",
      "Juan JosÃ© ZÃ¡rate",
      "Otmar Hilliges"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_STEERER_Resolving_Scale_Variations_for_Counting_and_Localization_via_Selective_ICCV_2023_paper.html": {
    "title": "STEERER: Resolving Scale Variations for Counting and Localization via Selective Inheritance Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Han",
      "Lei Bai",
      "Lingbo Liu",
      "Wanli Ouyang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liang_Benchmarking_Algorithmic_Bias_in_Face_Recognition_An_Experimental_Approach_Using_ICCV_2023_paper.html": {
    "title": "Benchmarking Algorithmic Bias in Face Recognition: An Experimental Approach Using Synthetic Faces and Human Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Liang",
      "Pietro Perona",
      "Guha Balakrishnan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Spatial-Aware_Token_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.html": {
    "title": "Spatial-Aware Token for Weakly Supervised Object Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingyu Wu",
      "Wei Zhai",
      "Yang Cao",
      "Jiebo Luo",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Harnessing_the_Spatial-Temporal_Attention_of_Diffusion_Models_for_High-Fidelity_Text-to-Image_ICCV_2023_paper.html": {
    "title": "Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiucheng Wu",
      "Yujian Liu",
      "Handong Zhao",
      "Trung Bui",
      "Zhe Lin",
      "Yang Zhang",
      "Shiyu Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Song_GraphAlign_Enhancing_Accurate_Feature_Alignment_by_Graph_matching_for_Multi-Modal_ICCV_2023_paper.html": {
    "title": "GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for Multi-Modal 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziying Song",
      "Haiyue Wei",
      "Lin Bai",
      "Lei Yang",
      "Caiyan Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ghoddoosian_Weakly-Supervised_Action_Segmentation_and_Unseen_Error_Detection_in_Anomalous_Instructional_ICCV_2023_paper.html": {
    "title": "Weakly-Supervised Action Segmentation and Unseen Error Detection in Anomalous Instructional Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Ghoddoosian",
      "Isht Dwivedi",
      "Nakul Agarwal",
      "Behzad Dariush"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_NEMTO_Neural_Environment_Matting_for_Novel_View_and_Relighting_Synthesis_ICCV_2023_paper.html": {
    "title": "NEMTO: Neural Environment Matting for Novel View and Relighting Synthesis of Transparent Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongqing Wang",
      "Tong Zhang",
      "Sabine SÃ¼sstrunk"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Min_Geometric_Viewpoint_Learning_with_Hyper-Rays_and_Harmonics_Encoding_ICCV_2023_paper.html": {
    "title": "Geometric Viewpoint Learning with Hyper-Rays and Harmonics Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixiang Min",
      "Juan Carlos Dibene",
      "Enrique Dunn"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_C2F2NeUS_Cascade_Cost_Frustum_Fusion_for_High_Fidelity_and_Generalizable_ICCV_2023_paper.html": {
    "title": "C2F2NeUS: Cascade Cost Frustum Fusion for High Fidelity and Generalizable Neural Surface Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luoyuan Xu",
      "Tao Guan",
      "Yuesong Wang",
      "Wenkai Liu",
      "Zhaojie Zeng",
      "Junle Wang",
      "Wei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bokhovkin_Mesh2Tex_Generating_Mesh_Textures_from_Image_Queries_ICCV_2023_paper.html": {
    "title": "Mesh2Tex: Generating Mesh Textures from Image Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexey Bokhovkin",
      "Shubham Tulsiani",
      "Angela Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Peng_USAGE_A_Unified_Seed_Area_Generation_Paradigm_for_Weakly_Supervised_ICCV_2023_paper.html": {
    "title": "USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Peng",
      "Guanchun Wang",
      "Lingxi Xie",
      "Dongsheng Jiang",
      "Wei Shen",
      "Qi Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_NeuS2_Fast_Learning_of_Neural_Implicit_Surfaces_for_Multi-view_Reconstruction_ICCV_2023_paper.html": {
    "title": "NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Wang",
      "Qin Han",
      "Marc Habermann",
      "Kostas Daniilidis",
      "Christian Theobalt",
      "Lingjie Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Deep_Feature_Deblurring_Diffusion_for_Detecting_Out-of-Distribution_Objects_ICCV_2023_paper.html": {
    "title": "Deep Feature Deblurring Diffusion for Detecting Out-of-Distribution Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aming Wu",
      "Da Chen",
      "Cheng Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Fast_Full-frame_Video_Stabilization_with_Iterative_Optimization_ICCV_2023_paper.html": {
    "title": "Fast Full-frame Video Stabilization with Iterative Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyue Zhao",
      "Xin Li",
      "Zhan Peng",
      "Xianrui Luo",
      "Xinyi Ye",
      "Hao Lu",
      "Zhiguo Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Meister_Gender_Artifacts_in_Visual_Datasets_ICCV_2023_paper.html": {
    "title": "Gender Artifacts in Visual Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicole Meister",
      "Dora Zhao",
      "Angelina Wang",
      "Vikram V. Ramaswamy",
      "Ruth Fong",
      "Olga Russakovsky"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Learning_Semi-supervised_Gaussian_Mixture_Models_for_Generalized_Category_Discovery_ICCV_2023_paper.html": {
    "title": "Learning Semi-supervised Gaussian Mixture Models for Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingchen Zhao",
      "Xin Wen",
      "Kai Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Udandarao_SuS-X_Training-Free_Name-Only_Transfer_of_Vision-Language_Models_ICCV_2023_paper.html": {
    "title": "SuS-X: Training-Free Name-Only Transfer of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishaal Udandarao",
      "Ankush Gupta",
      "Samuel Albanie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Rethinking_Point_Cloud_Registration_as_Masking_and_Reconstruction_ICCV_2023_paper.html": {
    "title": "Rethinking Point Cloud Registration as Masking and Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyan Chen",
      "Meiling Wang",
      "Li Yuan",
      "Yi Yang",
      "Yufeng Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Beating_Backdoor_Attack_at_Its_Own_Game_ICCV_2023_paper.html": {
    "title": "Beating Backdoor Attack at Its Own Game",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Liu",
      "Alberto Sangiovanni-Vincentelli",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Khan_Introducing_Language_Guidance_in_Prompt-based_Continual_Learning_ICCV_2023_paper.html": {
    "title": "Introducing Language Guidance in Prompt-based Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Gul Zain Ali Khan",
      "Muhammad Ferjad Naeem",
      "Luc Van Gool",
      "Didier Stricker",
      "Federico Tombari",
      "Muhammad Zeshan Afzal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yi_Invariant_Training_2D-3D_Joint_Hard_Samples_for_Few-Shot_Point_Cloud_ICCV_2023_paper.html": {
    "title": "Invariant Training 2D-3D Joint Hard Samples for Few-Shot Point Cloud Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanyu Yi",
      "Jiajun Deng",
      "Qianru Sun",
      "Xian-Sheng Hua",
      "Joo-Hwee Lim",
      "Hanwang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Berton_EigenPlaces_Training_Viewpoint_Robust_Models_for_Visual_Place_Recognition_ICCV_2023_paper.html": {
    "title": "EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriele Berton",
      "Gabriele Trivigno",
      "Barbara Caputo",
      "Carlo Masone"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Do_DALL-E_and_Flamingo_Understand_Each_Other_ICCV_2023_paper.html": {
    "title": "Do DALL-E and Flamingo Understand Each Other?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Li",
      "Jindong Gu",
      "Rajat Koner",
      "Sahand Sharifzadeh",
      "Volker Tresp"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_CIRI_Curricular_Inactivation_for_Residue-aware_One-shot_Video_Inpainting_ICCV_2023_paper.html": {
    "title": "CIRI: Curricular Inactivation for Residue-aware One-shot Video Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiying Zheng",
      "Cheng Xu",
      "Xuemiao Xu",
      "Wenxi Liu",
      "Shengfeng He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/van_Noord_Protoype-based_Dataset_Comparison_ICCV_2023_paper.html": {
    "title": "Prototype-based Dataset Comparison",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nanne van Noord"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shi_FreeCOS_Self-Supervised_Learning_from_Fractals_and_Unlabeled_Images_for_Curvilinear_ICCV_2023_paper.html": {
    "title": "FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Shi",
      "Xiaohuan Ding",
      "Liang Zhang",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Generating_Dynamic_Kernels_via_Transformers_for_Lane_Detection_ICCV_2023_paper.html": {
    "title": "Generating Dynamic Kernels via Transformers for Lane Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziye Chen",
      "Yu Liu",
      "Mingming Gong",
      "Bo Du",
      "Guoqi Qian",
      "Kate Smith-Miles"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ouyang_RSFNet_A_White-Box_Image_Retouching_Approach_using_Region-Specific_Color_Filters_ICCV_2023_paper.html": {
    "title": "RSFNet: A White-Box Image Retouching Approach using Region-Specific Color Filters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqi Ouyang",
      "Yi Dong",
      "Xiaoyang Kang",
      "Peiran Ren",
      "Xin Xu",
      "Xuansong Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Tem-Adapter_Adapting_Image-Text_Pretraining_for_Video_Question_Answer_ICCV_2023_paper.html": {
    "title": "Tem-Adapter: Adapting Image-Text Pretraining for Video Question Answer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyi Chen",
      "Xiao Liu",
      "Guangrun Wang",
      "Kun Zhang",
      "Philip H.S. Torr",
      "Xiao-Ping Zhang",
      "Yansong Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_Boosting_Long-tailed_Object_Detection_via_Step-wise_Learning_on_Smooth-tail_Data_ICCV_2023_paper.html": {
    "title": "Boosting Long-tailed Object Detection via Step-wise Learning on Smooth-tail Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Na Dong",
      "Yongqiang Zhang",
      "Mingli Ding",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Talking_Head_Generation_with_Probabilistic_Audio-to-Visual_Diffusion_Priors_ICCV_2023_paper.html": {
    "title": "Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhentao Yu",
      "Zixin Yin",
      "Deyu Zhou",
      "Duomin Wang",
      "Finn Wong",
      "Baoyuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Learning_Cross-Modal_Affinity_for_Referring_Video_Object_Segmentation_Targeting_Limited_ICCV_2023_paper.html": {
    "title": "Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanghui Li",
      "Mingqi Gao",
      "Heng Liu",
      "Xiantong Zhen",
      "Feng Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Human_Part-wise_3D_Motion_Context_Learning_for_Sign_Language_Recognition_ICCV_2023_paper.html": {
    "title": "Human Part-wise 3D Motion Context Learning for Sign Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taeryung Lee",
      "Yeonguk Oh",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gu_Remembering_Normality_Memory-guided_Knowledge_Distillation_for_Unsupervised_Anomaly_Detection_ICCV_2023_paper.html": {
    "title": "Remembering Normality: Memory-guided Knowledge Distillation for Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Gu",
      "Liang Liu",
      "Xu Chen",
      "Ran Yi",
      "Jiangning Zhang",
      "Yabiao Wang",
      "Chengjie Wang",
      "Annan Shu",
      "Guannan Jiang",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Coordinate_Quantized_Neural_Implicit_Representations_for_Multi-view_Reconstruction_ICCV_2023_paper.html": {
    "title": "Coordinate Quantized Neural Implicit Representations for Multi-view Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijia Jiang",
      "Jing Hua",
      "Zhizhong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Unleashing_the_Potential_of_Spiking_Neural_Networks_with_Dynamic_Confidence_ICCV_2023_paper.html": {
    "title": "Unleashing the Potential of Spiking Neural Networks with Dynamic Confidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Li",
      "Edward G Jones",
      "Steve Furber"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fioresi_TeD-SPAD_Temporal_Distinctiveness_for_Self-Supervised_Privacy-Preservation_for_Video_Anomaly_Detection_ICCV_2023_paper.html": {
    "title": "TeD-SPAD: Temporal Distinctiveness for Self-Supervised Privacy-Preservation for Video Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Fioresi",
      "Ishan Rajendrakumar Dave",
      "Mubarak Shah"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhuang_MAS_Towards_Resource-Efficient_Federated_Multiple-Task_Learning_ICCV_2023_paper.html": {
    "title": "MAS: Towards Resource-Efficient Federated Multiple-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiming Zhuang",
      "Yonggang Wen",
      "Lingjuan Lyu",
      "Shuai Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Bridging_Cross-task_Protocol_Inconsistency_for_Distillation_in_Dense_Object_Detection_ICCV_2023_paper.html": {
    "title": "Bridging Cross-task Protocol Inconsistency for Distillation in Dense Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longrong Yang",
      "Xianpan Zhou",
      "Xuewei Li",
      "Liang Qiao",
      "Zheyang Li",
      "Ziwei Yang",
      "Gaoang Wang",
      "Xi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wen_Divide_and_Conquer_a_Two-Step_Method_for_High_Quality_Face_ICCV_2023_paper.html": {
    "title": "Divide and Conquer: a Two-Step Method for High Quality Face De-identification with Model Explainability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunqian Wen",
      "Bo Liu",
      "Jingyi Cao",
      "Rong Xie",
      "Li Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye_HiTeA_Hierarchical_Temporal-Aware_Video-Language_Pre-training_ICCV_2023_paper.html": {
    "title": "HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinghao Ye",
      "Guohai Xu",
      "Ming Yan",
      "Haiyang Xu",
      "Qi Qian",
      "Ji Zhang",
      "Fei Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fu_VAPCNet_Viewpoint-Aware_3D_Point_Cloud_Completion_ICCV_2023_paper.html": {
    "title": "VAPCNet: Viewpoint-Aware 3D Point Cloud Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiheng Fu",
      "Longguang Wang",
      "Lian Xu",
      "Zhiyong Wang",
      "Hamid Laga",
      "Yulan Guo",
      "Farid Boussaid",
      "Mohammed Bennamoun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lu_Set-level_Guidance_Attack_Boosting_Adversarial_Transferability_of_Vision-Language_Pre-training_Models_ICCV_2023_paper.html": {
    "title": "Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Lu",
      "Zhiqiang Wang",
      "Teng Wang",
      "Weili Guan",
      "Hongchang Gao",
      "Feng Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dang_AutoSynth_Learning_to_Generate_3D_Training_Data_for_Object_Point_ICCV_2023_paper.html": {
    "title": "AutoSynth: Learning to Generate 3D Training Data for Object Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Dang",
      "Mathieu Salzmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Radevski_Multimodal_Distillation_for_Egocentric_Action_Recognition_ICCV_2023_paper.html": {
    "title": "Multimodal Distillation for Egocentric Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gorjan Radevski",
      "Dusan Grujicic",
      "Matthew Blaschko",
      "Marie-Francine Moens",
      "Tinne Tuytelaars"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Self-supervised_Learning_of_Implicit_Shape_Representation_with_Dense_Correspondence_for_ICCV_2023_paper.html": {
    "title": "Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baowen Zhang",
      "Jiahe Li",
      "Xiaoming Deng",
      "Yinda Zhang",
      "Cuixia Ma",
      "Hongan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Perceptual_Artifacts_Localization_for_Image_Synthesis_Tasks_ICCV_2023_paper.html": {
    "title": "Perceptual Artifacts Localization for Image Synthesis Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingzhi Zhang",
      "Zhengjie Xu",
      "Connelly Barnes",
      "Yuqian Zhou",
      "Qing Liu",
      "He Zhang",
      "Sohrab Amirghodsi",
      "Zhe Lin",
      "Eli Shechtman",
      "Jianbo Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xuan_Narrator_Towards_Natural_Control_of_Human-Scene_Interaction_Generation_via_Relationship_ICCV_2023_paper.html": {
    "title": "Narrator: Towards Natural Control of Human-Scene Interaction Generation via Relationship Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haibiao Xuan",
      "Xiongzheng Li",
      "Jinsong Zhang",
      "Hongwen Zhang",
      "Yebin Liu",
      "Kun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sudhakaran_Vision_Relation_Transformer_for_Unbiased_Scene_Graph_Generation_ICCV_2023_paper.html": {
    "title": "Vision Relation Transformer for Unbiased Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gopika Sudhakaran",
      "Devendra Singh Dhami",
      "Kristian Kersting",
      "Stefan Roth"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Scaling_Data_Generation_in_Vision-and-Language_Navigation_ICCV_2023_paper.html": {
    "title": "Scaling Data Generation in Vision-and-Language Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zun Wang",
      "Jialu Li",
      "Yicong Hong",
      "Yi Wang",
      "Qi Wu",
      "Mohit Bansal",
      "Stephen Gould",
      "Hao Tan",
      "Yu Qiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chiu_Better_May_Not_Be_Fairer_A_Study_on_Subgroup_Discrepancy_ICCV_2023_paper.html": {
    "title": "Better May Not Be Fairer: A Study on Subgroup Discrepancy in Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming-Chang Chiu",
      "Pin-Yu Chen",
      "Xuezhe Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhong_3D_Implicit_Transporter_for_Temporally_Consistent_Keypoint_Discovery_ICCV_2023_paper.html": {
    "title": "3D Implicit Transporter for Temporally Consistent Keypoint Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengliang Zhong",
      "Yuhang Zheng",
      "Yupeng Zheng",
      "Hao Zhao",
      "Li Yi",
      "Xiaodong Mu",
      "Ling Wang",
      "Pengfei Li",
      "Guyue Zhou",
      "Chao Yang",
      "Xinliang Zhang",
      "Jian Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pu_Adaptive_Rotated_Convolution_for_Rotated_Object_Detection_ICCV_2023_paper.html": {
    "title": "Adaptive Rotated Convolution for Rotated Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Pu",
      "Yiru Wang",
      "Zhuofan Xia",
      "Yizeng Han",
      "Yulin Wang",
      "Weihao Gan",
      "Zidong Wang",
      "Shiji Song",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guan_Revisit_PCA-based_Technique_for_Out-of-Distribution_Detection_ICCV_2023_paper.html": {
    "title": "Revisit PCA-based Technique for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyuan Guan",
      "Zhouwu Liu",
      "Wei-Shi Zheng",
      "Yuren Zhou",
      "Ruixuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Visually-Prompted_Language_Model_for_Fine-Grained_Scene_Graph_Generation_in_an_ICCV_2023_paper.html": {
    "title": "Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qifan Yu",
      "Juncheng Li",
      "Yu Wu",
      "Siliang Tang",
      "Wei Ji",
      "Yueting Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Khan_FishNet_A_Large-scale_Dataset_and_Benchmark_for_Fish_Recognition_Detection_ICCV_2023_paper.html": {
    "title": "FishNet: A Large-scale Dataset and Benchmark for Fish Recognition, Detection, and Functional Trait Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Faizan Farooq Khan",
      "Xiang Li",
      "Andrew J. Temple",
      "Mohamed Elhoseiny"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_Dual_Learning_with_Dynamic_Knowledge_Distillation_for_Partially_Relevant_Video_ICCV_2023_paper.html": {
    "title": "Dual Learning with Dynamic Knowledge Distillation for Partially Relevant Video Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianfeng Dong",
      "Minsong Zhang",
      "Zheng Zhang",
      "Xianke Chen",
      "Daizong Liu",
      "Xiaoye Qu",
      "Xun Wang",
      "Baolong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_UniVTG_Towards_Unified_Video-Language_Temporal_Grounding_ICCV_2023_paper.html": {
    "title": "UniVTG: Towards Unified Video-Language Temporal Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Qinghong Lin",
      "Pengchuan Zhang",
      "Joya Chen",
      "Shraman Pramanick",
      "Difei Gao",
      "Alex Jinpeng Wang",
      "Rui Yan",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Koh_Disposable_Transfer_Learning_for_Selective_Source_Task_Unlearning_ICCV_2023_paper.html": {
    "title": "Disposable Transfer Learning for Selective Source Task Unlearning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghee Koh",
      "Hyounguk Shon",
      "Janghyeon Lee",
      "Hyeong Gwon Hong",
      "Junmo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Grounding_3D_Object_Affordance_from_2D_Interactions_in_Images_ICCV_2023_paper.html": {
    "title": "Grounding 3D Object Affordance from 2D Interactions in Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Yang",
      "Wei Zhai",
      "Hongchen Luo",
      "Yang Cao",
      "Jiebo Luo",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hajder_Fast_Globally_Optimal_Surface_Normal_Estimation_from_an_Affine_Correspondence_ICCV_2023_paper.html": {
    "title": "Fast Globally Optimal Surface Normal Estimation from an Affine Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Levente Hajder",
      "Lajos LÃ³czi",
      "Daniel Barath"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shen_Masked_Spatio-Temporal_Structure_Prediction_for_Self-supervised_Learning_on_Point_Cloud_ICCV_2023_paper.html": {
    "title": "Masked Spatio-Temporal Structure Prediction for Self-supervised Learning on Point Cloud Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqiang Shen",
      "Xiaoxiao Sheng",
      "Hehe Fan",
      "Longguang Wang",
      "Yulan Guo",
      "Qiong Liu",
      "Hao Wen",
      "Xi Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Frequency-aware_GAN_for_Adversarial_Manipulation_Generation_ICCV_2023_paper.html": {
    "title": "Frequency-aware GAN for Adversarial Manipulation Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peifei Zhu",
      "Genki Osada",
      "Hirokatsu Kataoka",
      "Tsubasa Takahashi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Karras_DreamPose_Fashion_Video_Synthesis_with_Stable_Diffusion_ICCV_2023_paper.html": {
    "title": "DreamPose: Fashion Video Synthesis with Stable Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johanna Karras",
      "Aleksander Holynski",
      "Ting-Chun Wang",
      "Ira Kemelmacher-Shlizerman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Tube-Link_A_Flexible_Cross_Tube_Framework_for_Universal_Video_Segmentation_ICCV_2023_paper.html": {
    "title": "Tube-Link: A Flexible Cross Tube Framework for Universal Video Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangtai Li",
      "Haobo Yuan",
      "Wenwei Zhang",
      "Guangliang Cheng",
      "Jiangmiao Pang",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lai_Hybrid_Spectral_Denoising_Transformer_with_Guided_Attention_ICCV_2023_paper.html": {
    "title": "Hybrid Spectral Denoising Transformer with Guided Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqiang Lai",
      "Chenggang Yan",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shao_HiVLP_Hierarchical_Interactive_Video-Language_Pre-Training_ICCV_2023_paper.html": {
    "title": "HiVLP: Hierarchical Interactive Video-Language Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Shao",
      "Jianzhuang Liu",
      "Renjing Pei",
      "Songcen Xu",
      "Peng Dai",
      "Juwei Lu",
      "Weimian Li",
      "Youliang Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Learning_Concordant_Attention_via_Target-aware_Alignment_for_Visible-Infrared_Person_Re-identification_ICCV_2023_paper.html": {
    "title": "Learning Concordant Attention via Target-aware Alignment for Visible-Infrared Person Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianbing Wu",
      "Hong Liu",
      "Yuxin Su",
      "Wei Shi",
      "Hao Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_PhysDiff_Physics-Guided_Human_Motion_Diffusion_Model_ICCV_2023_paper.html": {
    "title": "PhysDiff: Physics-Guided Human Motion Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Yuan",
      "Jiaming Song",
      "Umar Iqbal",
      "Arash Vahdat",
      "Jan Kautz"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mao_Masked_Motion_Predictors_are_Strong_3D_Action_Representation_Learners_ICCV_2023_paper.html": {
    "title": "Masked Motion Predictors are Strong 3D Action Representation Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunyao Mao",
      "Jiajun Deng",
      "Wengang Zhou",
      "Yao Fang",
      "Wanli Ouyang",
      "Houqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Template-guided_Hierarchical_Feature_Restoration_for_Anomaly_Detection_ICCV_2023_paper.html": {
    "title": "Template-guided Hierarchical Feature Restoration for Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hewei Guo",
      "Liping Ren",
      "Jingjing Fu",
      "Yuwang Wang",
      "Zhizheng Zhang",
      "Cuiling Lan",
      "Haoqian Wang",
      "Xinwen Hou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shaker_SwiftFormer_Efficient_Additive_Attention_for_Transformer-based_Real-time_Mobile_Vision_Applications_ICCV_2023_paper.html": {
    "title": "SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdelrahman Shaker",
      "Muhammad Maaz",
      "Hanoona Rasheed",
      "Salman Khan",
      "Ming-Hsuan Yang",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hwang_UpCycling_Semi-supervised_3D_Object_Detection_without_Sharing_Raw-level_Unlabeled_Scenes_ICCV_2023_paper.html": {
    "title": "UpCycling: Semi-supervised 3D Object Detection without Sharing Raw-level Unlabeled Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunwook Hwang",
      "Youngseok Kim",
      "Seongwon Kim",
      "Saewoong Bahk",
      "Hyung-Sin Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_RIGID_Recurrent_GAN_Inversion_and_Editing_of_Real_Face_Videos_ICCV_2023_paper.html": {
    "title": "RIGID: Recurrent GAN Inversion and Editing of Real Face Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyang Xu",
      "Shengfeng He",
      "Kwan-Yee K. Wong",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_PourIt_Weakly-Supervised_Liquid_Perception_from_a_Single_Image_for_Visual_ICCV_2023_paper.html": {
    "title": "PourIt!: Weakly-Supervised Liquid Perception from a Single Image for Visual Closed-Loop Robotic Pouring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitao Lin",
      "Yanwei Fu",
      "Xiangyang Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_CSDA_Learning_Category-Scale_Joint_Feature_for_Domain_Adaptive_Object_Detection_ICCV_2023_paper.html": {
    "title": "CSDA: Learning Category-Scale Joint Feature for Domain Adaptive Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changlong Gao",
      "Chengxu Liu",
      "Yujie Dun",
      "Xueming Qian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_A_Latent_Space_of_Stochastic_Diffusion_Models_for_Zero-Shot_Image_ICCV_2023_paper.html": {
    "title": "A Latent Space of Stochastic Diffusion Models for Zero-Shot Image Editing and Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Henry Wu",
      "Fernando De la Torre"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Quan_Single_Image_Defocus_Deblurring_via_Implicit_Neural_Inverse_Kernels_ICCV_2023_paper.html": {
    "title": "Single Image Defocus Deblurring via Implicit Neural Inverse Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Quan",
      "Xin Yao",
      "Hui Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xi_Open_Set_Video_HOI_detection_from_Action-Centric_Chain-of-Look_Prompting_ICCV_2023_paper.html": {
    "title": "Open Set Video HOI detection from Action-Centric Chain-of-Look Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Xi",
      "Jingjing Meng",
      "Junsong Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Robust_Mixture-of-Expert_Training_for_Convolutional_Neural_Networks_ICCV_2023_paper.html": {
    "title": "Robust Mixture-of-Expert Training for Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihua Zhang",
      "Ruisi Cai",
      "Tianlong Chen",
      "Guanhua Zhang",
      "Huan Zhang",
      "Pin-Yu Chen",
      "Shiyu Chang",
      "Zhangyang Wang",
      "Sijia Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_AvatarCraft_Transforming_Text_into_Neural_Human_Avatars_with_Parameterized_Shape_ICCV_2023_paper.html": {
    "title": "AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruixiang Jiang",
      "Can Wang",
      "Jingbo Zhang",
      "Menglei Chai",
      "Mingming He",
      "Dongdong Chen",
      "Jing Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Du_s-Adaptive_Decoupled_Prototype_for_Few-Shot_Object_Detection_ICCV_2023_paper.html": {
    "title": "s-Adaptive Decoupled Prototype for Few-Shot Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhao Du",
      "Shan Zhang",
      "Qiang Chen",
      "Haifeng Le",
      "Yanpeng Sun",
      "Yao Ni",
      "Jian Wang",
      "Bin He",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Why_Is_Prompt_Tuning_for_Vision-Language_Models_Robust_to_Noisy_ICCV_2023_paper.html": {
    "title": "Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-En Wu",
      "Yu Tian",
      "Haichao Yu",
      "Heng Wang",
      "Pedro Morgado",
      "Yu Hen Hu",
      "Linjie Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shao_Unified_Pre-Training_with_Pseudo_Texts_for_Text-To-Image_Person_Re-Identification_ICCV_2023_paper.html": {
    "title": "Unified Pre-Training with Pseudo Texts for Text-To-Image Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyin Shao",
      "Xinyu Zhang",
      "Changxing Ding",
      "Jian Wang",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qian_Semantics_Meets_Temporal_Correspondence_Self-supervised_Object-centric_Learning_in_Videos_ICCV_2023_paper.html": {
    "title": "Semantics Meets Temporal Correspondence: Self-supervised Object-centric Learning in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Qian",
      "Shuangrui Ding",
      "Xian Liu",
      "Dahua Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_UniTR_A_Unified_and_Efficient_Multi-Modal_Transformer_for_Birds-Eye-View_Representation_ICCV_2023_paper.html": {
    "title": "UniTR: A Unified and Efficient Multi-Modal Transformer for Bird's-Eye-View Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Wang",
      "Hao Tang",
      "Shaoshuai Shi",
      "Aoxue Li",
      "Zhenguo Li",
      "Bernt Schiele",
      "Liwei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Traj-MAE_Masked_Autoencoders_for_Trajectory_Prediction_ICCV_2023_paper.html": {
    "title": "Traj-MAE: Masked Autoencoders for Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Chen",
      "Jiaze Wang",
      "Kun Shao",
      "Furui Liu",
      "Jianye Hao",
      "Chenyong Guan",
      "Guangyong Chen",
      "Pheng-Ann Heng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Panos_First_Session_Adaptation_A_Strong_Replay-Free_Baseline_for_Class-Incremental_Learning_ICCV_2023_paper.html": {
    "title": "First Session Adaptation: A Strong Replay-Free Baseline for Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aristeidis Panos",
      "Yuriko Kobe",
      "Daniel Olmeda Reino",
      "Rahaf Aljundi",
      "Richard E. Turner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Ada3D__Exploiting_the_Spatial_Redundancy_with_Adaptive_Inference_for_ICCV_2023_paper.html": {
    "title": "Ada3D : Exploiting the Spatial Redundancy with Adaptive Inference for Efficient 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianchen Zhao",
      "Xuefei Ning",
      "Ke Hong",
      "Zhongyuan Qiu",
      "Pu Lu",
      "Yali Zhao",
      "Linfeng Zhang",
      "Lipu Zhou",
      "Guohao Dai",
      "Huazhong Yang",
      "Yu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Schmied_R3D3_Dense_3D_Reconstruction_of_Dynamic_Scenes_from_Multiple_Cameras_ICCV_2023_paper.html": {
    "title": "R3D3: Dense 3D Reconstruction of Dynamic Scenes from Multiple Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aron Schmied",
      "Tobias Fischer",
      "Martin Danelljan",
      "Marc Pollefeys",
      "Fisher Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qin_UniFusion_Unified_Multi-View_Fusion_Transformer_for_Spatial-Temporal_Representation_in_Birds-Eye-View_ICCV_2023_paper.html": {
    "title": "UniFusion: Unified Multi-View Fusion Transformer for Spatial-Temporal Representation in Bird's-Eye-View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zequn Qin",
      "Jingyu Chen",
      "Chao Chen",
      "Xiaozhi Chen",
      "Xi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sheng_Point_Contrastive_Prediction_with_Semantic_Clustering_for_Self-Supervised_Learning_on_ICCV_2023_paper.html": {
    "title": "Point Contrastive Prediction with Semantic Clustering for Self-Supervised Learning on Point Cloud Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxiao Sheng",
      "Zhiqiang Shen",
      "Gang Xiao",
      "Longguang Wang",
      "Yulan Guo",
      "Hehe Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Swetha_Preserving_Modality_Structure_Improves_Multi-Modal_Learning_ICCV_2023_paper.html": {
    "title": "Preserving Modality Structure Improves Multi-Modal Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sirnam Swetha",
      "Mamshad Nayeem Rizve",
      "Nina Shvetsova",
      "Hilde Kuehne",
      "Mubarak Shah"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yao_Focus_the_Discrepancy_Intra-_and_Inter-Correlation_Learning_for_Image_Anomaly_ICCV_2023_paper.html": {
    "title": "Focus the Discrepancy: Intra- and Inter-Correlation Learning for Image Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xincheng Yao",
      "Ruoqi Li",
      "Zefeng Qian",
      "Yan Luo",
      "Chongyang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nakamura_Pre-training_Vision_Transformers_with_Very_Limited_Synthesized_Images_ICCV_2023_paper.html": {
    "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryo Nakamura",
      "Hirokatsu Kataoka",
      "Sora Takashima",
      "Edgar Josafat Martinez Noriega",
      "Rio Yokota",
      "Nakamasa Inoue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Sample-adaptive_Augmentation_for_Point_Cloud_Recognition_Against_Real-world_Corruptions_ICCV_2023_paper.html": {
    "title": "Sample-adaptive Augmentation for Point Cloud Recognition Against Real-world Corruptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Wang",
      "Lihe Ding",
      "Tingfa Xu",
      "Shaocong Dong",
      "Xinli Xu",
      "Long Bai",
      "Jianan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_Make_Encoder_Great_Again_in_3D_GAN_Inversion_through_Geometry_ICCV_2023_paper.html": {
    "title": "Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Yuan",
      "Yiming Zhu",
      "Yu Li",
      "Hongyu Liu",
      "Chun Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Modality_Unifying_Network_for_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.html": {
    "title": "Modality Unifying Network for Visible-Infrared Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Yu",
      "Xu Cheng",
      "Wei Peng",
      "Weihao Liu",
      "Guoying Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Levi_DLT_Conditioned_layout_generation_with_Joint_Discrete-Continuous_Diffusion_Layout_Transformer_ICCV_2023_paper.html": {
    "title": "DLT: Conditioned layout generation with Joint Discrete-Continuous Diffusion Layout Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elad Levi",
      "Eli Brosh",
      "Mykola Mykhailych",
      "Meir Perez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_PADDLES_Phase-Amplitude_Spectrum_Disentangled_Early_Stopping_for_Learning_with_Noisy_ICCV_2023_paper.html": {
    "title": "PADDLES: Phase-Amplitude Spectrum Disentangled Early Stopping for Learning with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huaxi Huang",
      "Hui Kang",
      "Sheng Liu",
      "Olivier Salvado",
      "Thierry Rakotoarivelo",
      "Dadong Wang",
      "Tongliang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Paredes-Valles_Taming_Contrast_Maximization_for_Learning_Sequential_Low-latency_Event-based_Optical_Flow_ICCV_2023_paper.html": {
    "title": "Taming Contrast Maximization for Learning Sequential, Low-latency, Event-based Optical Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Paredes-VallÃ©s",
      "Kirk Y. W. Scheper",
      "Christophe De Wagter",
      "Guido C. H. E. de Croon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shen_CLIP-Cluster_CLIP-Guided_Attribute_Hallucination_for_Face_Clustering_ICCV_2023_paper.html": {
    "title": "CLIP-Cluster: CLIP-Guided Attribute Hallucination for Face Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Shen",
      "Wanhua Li",
      "Xiaobing Wang",
      "Dafeng Zhang",
      "Zhezhu Jin",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xia_CASSPR_Cross_Attention_Single_Scan_Place_Recognition_ICCV_2023_paper.html": {
    "title": "CASSPR: Cross Attention Single Scan Place Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Xia",
      "Mariia Gladkova",
      "Rui Wang",
      "Qianyun Li",
      "Uwe Stilla",
      "JoÃ£o F Henriques",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_DDFM_Denoising_Diffusion_Model_for_Multi-Modality_Image_Fusion_ICCV_2023_paper.html": {
    "title": "DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixiang Zhao",
      "Haowen Bai",
      "Yuanzhi Zhu",
      "Jiangshe Zhang",
      "Shuang Xu",
      "Yulun Zhang",
      "Kai Zhang",
      "Deyu Meng",
      "Radu Timofte",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_A_Unified_Continual_Learning_Framework_with_General_Parameter-Efficient_Tuning_ICCV_2023_paper.html": {
    "title": "A Unified Continual Learning Framework with General Parameter-Efficient Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiankun Gao",
      "Chen Zhao",
      "Yifan Sun",
      "Teng Xi",
      "Gang Zhang",
      "Bernard Ghanem",
      "Jian Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pi_Hierarchical_Generation_of_Human-Object_Interactions_with_Diffusion_Probabilistic_Models_ICCV_2023_paper.html": {
    "title": "Hierarchical Generation of Human-Object Interactions with Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huaijin Pi",
      "Sida Peng",
      "Minghui Yang",
      "Xiaowei Zhou",
      "Hujun Bao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tuo_Learning_Data-Driven_Vector-Quantized_Degradation_Model_for_Animation_Video_Super-Resolution_ICCV_2023_paper.html": {
    "title": "Learning Data-Driven Vector-Quantized Degradation Model for Animation Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixi Tuo",
      "Huan Yang",
      "Jianlong Fu",
      "Yujie Dun",
      "Xueming Qian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Compositional_Feature_Augmentation_for_Unbiased_Scene_Graph_Generation_ICCV_2023_paper.html": {
    "title": "Compositional Feature Augmentation for Unbiased Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Li",
      "Guikun Chen",
      "Jun Xiao",
      "Yi Yang",
      "Chunping Wang",
      "Long Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Foreground_and_Text-lines_Aware_Document_Image_Rectification_ICCV_2023_paper.html": {
    "title": "Foreground and Text-lines Aware Document Image Rectification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Li",
      "Xiangping Wu",
      "Qingcai Chen",
      "Qianjin Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_Open-Vocabulary_Semantic_Segmentation_with_Decoupled_One-Pass_Network_ICCV_2023_paper.html": {
    "title": "Open-Vocabulary Semantic Segmentation with Decoupled One-Pass Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Han",
      "Yujie Zhong",
      "Dengjie Li",
      "Kai Han",
      "Lin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_INSTA-BNN_Binary_Neural_Network_with_INSTAnce-aware_Threshold_ICCV_2023_paper.html": {
    "title": "INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changhun Lee",
      "Hyungjun Kim",
      "Eunhyeok Park",
      "Jae-Joon Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Human-Inspired_Facial_Sketch_Synthesis_with_Dynamic_Adaptation_ICCV_2023_paper.html": {
    "title": "Human-Inspired Facial Sketch Synthesis with Dynamic Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Gao",
      "Yifan Zhu",
      "Chang Jiang",
      "Nannan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_When_Epipolar_Constraint_Meets_Non-Local_Operators_in_Multi-View_Stereo_ICCV_2023_paper.html": {
    "title": "When Epipolar Constraint Meets Non-Local Operators in Multi-View Stereo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianqi Liu",
      "Xinyi Ye",
      "Weiyue Zhao",
      "Zhiyu Pan",
      "Min Shi",
      "Zhiguo Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_LU-NeRF_Scene_and_Pose_Estimation_by_Synchronizing_Local_Unposed_NeRFs_ICCV_2023_paper.html": {
    "title": "LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zezhou Cheng",
      "Carlos Esteves",
      "Varun Jampani",
      "Abhishek Kar",
      "Subhransu Maji",
      "Ameesh Makadia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Calibrating_Panoramic_Depth_Estimation_for_Practical_Localization_and_Mapping_ICCV_2023_paper.html": {
    "title": "Calibrating Panoramic Depth Estimation for Practical Localization and Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junho Kim",
      "Eun Sun Lee",
      "Young Min Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_DiffDis_Empowering_Generative_Diffusion_Model_with_Cross-Modal_Discrimination_Capability_ICCV_2023_paper.html": {
    "title": "DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runhui Huang",
      "Jianhua Han",
      "Guansong Lu",
      "Xiaodan Liang",
      "Yihan Zeng",
      "Wei Zhang",
      "Hang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tanveer_DS-Fusion_Artistic_Typography_via_Discriminated_and_Stylized_Diffusion_ICCV_2023_paper.html": {
    "title": "DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maham Tanveer",
      "Yizhi Wang",
      "Ali Mahdavi-Amiri",
      "Hao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Distilling_DETR_with_Visual-Linguistic_Knowledge_for_Open-Vocabulary_Object_Detection_ICCV_2023_paper.html": {
    "title": "Distilling DETR with Visual-Linguistic Knowledge for Open-Vocabulary Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangqi Li",
      "Jiaxu Miao",
      "Dahu Shi",
      "Wenming Tan",
      "Ye Ren",
      "Yi Yang",
      "Shiliang Pu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Reed_Scale-MAE_A_Scale-Aware_Masked_Autoencoder_for_Multiscale_Geospatial_Representation_Learning_ICCV_2023_paper.html": {
    "title": "Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Colorado J Reed",
      "Ritwik Gupta",
      "Shufan Li",
      "Sarah Brockman",
      "Christopher Funk",
      "Brian Clipp",
      "Kurt Keutzer",
      "Salvatore Candido",
      "Matt Uyttendaele",
      "Trevor Darrell"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_View_Consistent_Purification_for_Accurate_Cross-View_Localization_ICCV_2023_paper.html": {
    "title": "View Consistent Purification for Accurate Cross-View Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shan Wang",
      "Yanhao Zhang",
      "Akhil Perincherry",
      "Ankit Vora",
      "Hongdong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jeon_A_Unified_Framework_for_Robustness_on_Diverse_Sampling_Errors_ICCV_2023_paper.html": {
    "title": "A Unified Framework for Robustness on Diverse Sampling Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myeongho Jeon",
      "Myungjoo Kang",
      "Joonseok Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Efficient_Video_Action_Detection_with_Token_Dropout_and_Context_Refinement_ICCV_2023_paper.html": {
    "title": "Efficient Video Action Detection with Token Dropout and Context Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Chen",
      "Zhan Tong",
      "Yibing Song",
      "Gangshan Wu",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Deng_Explicit_Motion_Disentangling_for_Efficient_Optical_Flow_Estimation_ICCV_2023_paper.html": {
    "title": "Explicit Motion Disentangling for Efficient Optical Flow Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changxing Deng",
      "Ao Luo",
      "Haibin Huang",
      "Shaodan Ma",
      "Jiangyu Liu",
      "Shuaicheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_LiDAR-Camera_Panoptic_Segmentation_via_Geometry-Consistent_and_Semantic-Aware_Alignment_ICCV_2023_paper.html": {
    "title": "LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Zhang",
      "Zhizhong Zhang",
      "Qian Yu",
      "Ran Yi",
      "Yuan Xie",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Deng_GrowCLIP_Data-Aware_Automatic_Model_Growing_for_Large-scale_Contrastive_Language-Image_Pre-Training_ICCV_2023_paper.html": {
    "title": "GrowCLIP: Data-Aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinchi Deng",
      "Han Shi",
      "Runhui Huang",
      "Changlin Li",
      "Hang Xu",
      "Jianhua Han",
      "James Kwok",
      "Shen Zhao",
      "Wei Zhang",
      "Xiaodan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zubic_From_Chaos_Comes_Order_Ordering_Event_Representations_for_Object_Recognition_ICCV_2023_paper.html": {
    "title": "From Chaos Comes Order: Ordering Event Representations for Object Recognition and Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikola ZubiÄ",
      "Daniel Gehrig",
      "Mathias Gehrig",
      "Davide Scaramuzza"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_LA-Net_Landmark-Aware_Learning_for_Reliable_Facial_Expression_Recognition_under_Label_ICCV_2023_paper.html": {
    "title": "LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Wu",
      "Jinshi Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Deng_Identity-Consistent_Aggregation_for_Video_Object_Detection_ICCV_2023_paper.html": {
    "title": "Identity-Consistent Aggregation for Video Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaorui Deng",
      "Da Chen",
      "Qi Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Scene-Aware_Label_Graph_Learning_for_Multi-Label_Image_Classification_ICCV_2023_paper.html": {
    "title": "Scene-Aware Label Graph Learning for Multi-Label Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuelin Zhu",
      "Jian Liu",
      "Weijia Liu",
      "Jiawei Ge",
      "Bo Liu",
      "Jiuxin Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Papantoniou_Relightify_Relightable_3D_Faces_from_a_Single_Image_via_Diffusion_ICCV_2023_paper.html": {
    "title": "Relightify: Relightable 3D Faces from a Single Image via Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Foivos Paraperas Papantoniou",
      "Alexandros Lattas",
      "Stylianos Moschoglou",
      "Stefanos Zafeiriou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Fcaformer_Forward_Cross_Attention_in_Hybrid_Vision_Transformer_ICCV_2023_paper.html": {
    "title": "Fcaformer: Forward Cross Attention in Hybrid Vision Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haokui Zhang",
      "Wenze Hu",
      "Xiaoyu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Progressive_Spatio-Temporal_Prototype_Matching_for_Text-Video_Retrieval_ICCV_2023_paper.html": {
    "title": "Progressive Spatio-Temporal Prototype Matching for Text-Video Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pandeng Li",
      "Chen-Wei Xie",
      "Liming Zhao",
      "Hongtao Xie",
      "Jiannan Ge",
      "Yun Zheng",
      "Deli Zhao",
      "Yongdong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Leveraging_Spatio-Temporal_Dependency_for_Skeleton-Based_Action_Recognition_ICCV_2023_paper.html": {
    "title": "Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungho Lee",
      "Minhyeok Lee",
      "Suhwan Cho",
      "Sungmin Woo",
      "Sungjun Jang",
      "Sangyoun Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Data_Augmented_Flatness-aware_Gradient_Projection_for_Continual_Learning_ICCV_2023_paper.html": {
    "title": "Data Augmented Flatness-aware Gradient Projection for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enneng Yang",
      "Li Shen",
      "Zhenyi Wang",
      "Shiwei Liu",
      "Guibing Guo",
      "Xingwei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Camera-Driven_Representation_Learning_for_Unsupervised_Domain_Adaptive_Person_Re-identification_ICCV_2023_paper.html": {
    "title": "Camera-Driven Representation Learning for Unsupervised Domain Adaptive Person Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geon Lee",
      "Sanghoon Lee",
      "Dohyung Kim",
      "Younghoon Shin",
      "Yongsang Yoon",
      "Bumsub Ham"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ahn_Sample-wise_Label_Confidence_Incorporation_for_Learning_with_Noisy_Labels_ICCV_2023_paper.html": {
    "title": "Sample-wise Label Confidence Incorporation for Learning with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chanho Ahn",
      "Kikyung Kim",
      "Ji-won Baek",
      "Jongin Lim",
      "Seungju Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gupta_CLIPTrans_Transferring_Visual_Knowledge_with_Pre-trained_Models_for_Multimodal_Machine_ICCV_2023_paper.html": {
    "title": "CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Devaansh Gupta",
      "Siddhant Kharbanda",
      "Jiawei Zhou",
      "Wanhua Li",
      "Hanspeter Pfister",
      "Donglai Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sarkar_SGAligner_3D_Scene_Alignment_with_Scene_Graphs_ICCV_2023_paper.html": {
    "title": "SGAligner: 3D Scene Alignment with Scene Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayan Deb Sarkar",
      "Ondrej Miksik",
      "Marc Pollefeys",
      "Daniel Barath",
      "Iro Armeni"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Su_Name_Your_Colour_For_the_Task_Artificially_Discover_Colour_Naming_ICCV_2023_paper.html": {
    "title": "Name Your Colour For the Task: Artificially Discover Colour Naming via Colour Quantisation Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghan Su",
      "Lin Gu",
      "Yue Yang",
      "Zenghui Zhang",
      "Tatsuya Harada"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_FSAR_Federated_Skeleton-based_Action_Recognition_with_Adaptive_Topology_Structure_and_ICCV_2023_paper.html": {
    "title": "FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwen Guo",
      "Hong Liu",
      "Shitong Sun",
      "Tianyu Guo",
      "Min Zhang",
      "Chenyang Si"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Video_Adverse-Weather-Component_Suppression_Network_via_Weather_Messenger_and_Adversarial_Backpropagation_ICCV_2023_paper.html": {
    "title": "Video Adverse-Weather-Component Suppression Network via Weather Messenger and Adversarial Backpropagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijun Yang",
      "Angelica I. Aviles-Rivero",
      "Huazhu Fu",
      "Ye Liu",
      "Weiming Wang",
      "Lei Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Barkan_Efficient_Discovery_and_Effective_Evaluation_of_Visual_Perceptual_Similarity_A_ICCV_2023_paper.html": {
    "title": "Efficient Discovery and Effective Evaluation of Visual Perceptual Similarity: A Benchmark and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oren Barkan",
      "Tal Reiss",
      "Jonathan Weill",
      "Ori Katz",
      "Roy Hirsch",
      "Itzik Malkiel",
      "Noam Koenigstein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Ego-Only_Egocentric_Action_Detection_without_Exocentric_Transferring_ICCV_2023_paper.html": {
    "title": "Ego-Only: Egocentric Action Detection without Exocentric Transferring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiyu Wang",
      "Mitesh Kumar Singh",
      "Lorenzo Torresani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_CoinSeg_Contrast_Inter-_and_Intra-_Class_Representations_for_Incremental_Segmentation_ICCV_2023_paper.html": {
    "title": "CoinSeg: Contrast Inter- and Intra- Class Representations for Incremental Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekang Zhang",
      "Guangyu Gao",
      "Jianbo Jiao",
      "Chi Harold Liu",
      "Yunchao Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Du_Multi-View_Active_Fine-Grained_Visual_Recognition_ICCV_2023_paper.html": {
    "title": "Multi-View Active Fine-Grained Visual Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoyi Du",
      "Wenqing Yu",
      "Heqing Wang",
      "Ting-En Lin",
      "Dongliang Chang",
      "Zhanyu Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ni_Part-Aware_Transformer_for_Generalizable_Person_Re-identification_ICCV_2023_paper.html": {
    "title": "Part-Aware Transformer for Generalizable Person Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Ni",
      "Yuke Li",
      "Lianli Gao",
      "Heng Tao Shen",
      "Jingkuan Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xue_Variational_Causal_Inference_Network_for_Explanatory_Visual_Question_Answering_ICCV_2023_paper.html": {
    "title": "Variational Causal Inference Network for Explanatory Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dizhan Xue",
      "Shengsheng Qian",
      "Changsheng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Improving_Representation_Learning_for_Histopathologic_Images_with_Cluster_Constraints_ICCV_2023_paper.html": {
    "title": "Improving Representation Learning for Histopathologic Images with Cluster Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyi Wu",
      "Chongyang Gao",
      "Joseph DiPalma",
      "Soroush Vosoughi",
      "Saeed Hassanpour"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Song_Blending-NeRF_Text-Driven_Localized_Editing_in_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonseop Song",
      "Seokhun Choi",
      "Hoseok Do",
      "Chul Lee",
      "Taehyeong Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jungerman_Panoramas_from_Photons_ICCV_2023_paper.html": {
    "title": "Panoramas from Photons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sacha Jungerman",
      "Atul Ingle",
      "Mohit Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chai_Global_Adaptation_Meets_Local_Generalization_Unsupervised_Domain_Adaptation_for_3D_ICCV_2023_paper.html": {
    "title": "Global Adaptation Meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Chai",
      "Zhongyu Jiang",
      "Jenq-Neng Hwang",
      "Gaoang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Learning_Neural_Implicit_Surfaces_with_Object-Aware_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "Learning Neural Implicit Surfaces with Object-Aware Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiheng Zhang",
      "Zhaofan Qiu",
      "Yingwei Pan",
      "Ting Yao",
      "Tao Mei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lai_PADCLIP_Pseudo-labeling_with_Adaptive_Debiasing_in_CLIP_for_Unsupervised_Domain_ICCV_2023_paper.html": {
    "title": "PADCLIP: Pseudo-labeling with Adaptive Debiasing in CLIP for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengfeng Lai",
      "Noranart Vesdapunt",
      "Ning Zhou",
      "Jun Wu",
      "Cong Phuoc Huynh",
      "Xuelu Li",
      "Kah Kuen Fu",
      "Chen-Nee Chuah"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shang_Causal-DFQ_Causality_Guided_Data-Free_Network_Quantization_ICCV_2023_paper.html": {
    "title": "Causal-DFQ: Causality Guided Data-Free Network Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhang Shang",
      "Bingxin Xu",
      "Gaowen Liu",
      "Ramana Rao Kompella",
      "Yan Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Enhancing_Generalization_of_Universal_Adversarial_Perturbation_through_Gradient_Aggregation_ICCV_2023_paper.html": {
    "title": "Enhancing Generalization of Universal Adversarial Perturbation through Gradient Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuannan Liu",
      "Yaoyao Zhong",
      "Yuhang Zhang",
      "Lixiong Qin",
      "Weihong Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_CancerUniT_Towards_a_Single_Unified_Model_for_Effective_Detection_Segmentation_ICCV_2023_paper.html": {
    "title": "CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieneng Chen",
      "Yingda Xia",
      "Jiawen Yao",
      "Ke Yan",
      "Jianpeng Zhang",
      "Le Lu",
      "Fakai Wang",
      "Bo Zhou",
      "Mingyan Qiu",
      "Qihang Yu",
      "Mingze Yuan",
      "Wei Fang",
      "Yuxing Tang",
      "Minfeng Xu",
      "Jian Zhou",
      "Yuqian Zhao",
      "Qifeng Wang",
      "Xianghua Ye",
      "Xiaoli Yin",
      "Yu Shi",
      "Xin Chen",
      "Jingren Zhou",
      "Alan Yuille",
      "Zaiyi Liu",
      "Ling Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Dual_Meta-Learning_with_Longitudinally_Consistent_Regularization_for_One-Shot_Brain_Tissue_ICCV_2023_paper.html": {
    "title": "Dual Meta-Learning with Longitudinally Consistent Regularization for One-Shot Brain Tissue Segmentation Across the Human Lifespan",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongheng Sun",
      "Fan Wang",
      "Jun Shu",
      "Haifeng Wang",
      "Li Wang",
      "Deyu Meng",
      "Chunfeng Lian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_DeFormer_Integrating_Transformers_with_Deformable_Models_for_3D_Shape_Abstraction_ICCV_2023_paper.html": {
    "title": "DeFormer: Integrating Transformers with Deformable Models for 3D Shape Abstraction from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Liu",
      "Xiang Yu",
      "Meng Ye",
      "Qilong Zhangli",
      "Zhuowei Li",
      "Zhixing Zhang",
      "Dimitris N. Metaxas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Parallel_Attention_Interaction_Network_for_Few-Shot_Skeleton-Based_Action_Recognition_ICCV_2023_paper.html": {
    "title": "Parallel Attention Interaction Network for Few-Shot Skeleton-Based Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Liu",
      "Sanping Zhou",
      "Le Wang",
      "Gang Hua"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Cross-view_Semantic_Alignment_for_Livestreaming_Product_Recognition_ICCV_2023_paper.html": {
    "title": "Cross-view Semantic Alignment for Livestreaming Product Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Yang",
      "Yiyi Chen",
      "Yan Li",
      "Yanhua Cheng",
      "Xudong Liu",
      "Quan Chen",
      "Han Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ko_Continuously_Masked_Transformer_for_Image_Inpainting_ICCV_2023_paper.html": {
    "title": "Continuously Masked Transformer for Image Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keunsoo Ko",
      "Chang-Su Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pautrat_Vanishing_Point_Estimation_in_Uncalibrated_Images_with_Prior_Gravity_Direction_ICCV_2023_paper.html": {
    "title": "Vanishing Point Estimation in Uncalibrated Images with Prior Gravity Direction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "RÃ©mi Pautrat",
      "Shaohui Liu",
      "Petr Hruby",
      "Marc Pollefeys",
      "Daniel Barath"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pourkeshavarz_Learn_TAROT_with_MENTOR_A_Meta-Learned_Self-Supervised_Approach_for_Trajectory_ICCV_2023_paper.html": {
    "title": "Learn TAROT with MENTOR: A Meta-Learned Self-Supervised Approach for Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mozhgan Pourkeshavarz",
      "Changhe Chen",
      "Amir Rasouli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_MatrixVT_Efficient_Multi-Camera_to_BEV_Transformation_for_3D_Perception_ICCV_2023_paper.html": {
    "title": "MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Zhou",
      "Zheng Ge",
      "Zeming Li",
      "Xiangyu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tao_Local_and_Global_Logit_Adjustments_for_Long-Tailed_Learning_ICCV_2023_paper.html": {
    "title": "Local and Global Logit Adjustments for Long-Tailed Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingfan Tao",
      "Jingna Sun",
      "Hao Yang",
      "Li Chen",
      "Xu Wang",
      "Wenming Yang",
      "Daniel Du",
      "Min Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cabannes_Active_Self-Supervised_Learning_A_Few_Low-Cost_Relationships_Are_All_You_ICCV_2023_paper.html": {
    "title": "Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivien Cabannes",
      "Leon Bottou",
      "Yann Lecun",
      "Randall Balestriero"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Wasserstein_Expansible_Variational_Autoencoder_for_Discriminative_and_Generative_Continual_Learning_ICCV_2023_paper.html": {
    "title": "Wasserstein Expansible Variational Autoencoder for Discriminative and Generative Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Ye",
      "Adrian G. Bors"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_Sensitivity-Aware_Visual_Parameter-Efficient_Fine-Tuning_ICCV_2023_paper.html": {
    "title": "Sensitivity-Aware Visual Parameter-Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu He",
      "Jianfei Cai",
      "Jing Zhang",
      "Dacheng Tao",
      "Bohan Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cho_Label-Free_Event-based_Object_Recognition_via_Joint_Learning_with_Image_Reconstruction_ICCV_2023_paper.html": {
    "title": "Label-Free Event-based Object Recognition via Joint Learning with Image Reconstruction from Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoonhee Cho",
      "Hyeonseong Kim",
      "Yujeong Chae",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Gloss-Free_Sign_Language_Translation_Improving_from_Visual-Language_Pretraining_ICCV_2023_paper.html": {
    "title": "Gloss-Free Sign Language Translation: Improving from Visual-Language Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjia Zhou",
      "Zhigang Chen",
      "Albert ClapÃ©s",
      "Jun Wan",
      "Yanyan Liang",
      "Sergio Escalera",
      "Zhen Lei",
      "Du Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Weakly-supervised_3D_Pose_Transfer_with_Keypoints_ICCV_2023_paper.html": {
    "title": "Weakly-supervised 3D Pose Transfer with Keypoints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinnan Chen",
      "Chen Li",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Not_All_Features_Matter_Enhancing_Few-shot_CLIP_with_Adaptive_Prior_ICCV_2023_paper.html": {
    "title": "Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyang Zhu",
      "Renrui Zhang",
      "Bowei He",
      "Aojun Zhou",
      "Dong Wang",
      "Bin Zhao",
      "Peng Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pramanick_EgoVLPv2_Egocentric_Video-Language_Pre-training_with_Fusion_in_the_Backbone_ICCV_2023_paper.html": {
    "title": "EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shraman Pramanick",
      "Yale Song",
      "Sayan Nag",
      "Kevin Qinghong Lin",
      "Hardik Shah",
      "Mike Zheng Shou",
      "Rama Chellappa",
      "Pengchuan Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Luo_On_the_Effectiveness_of_Spectral_Discriminators_for_Perceptual_Quality_Improvement_ICCV_2023_paper.html": {
    "title": "On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Luo",
      "Yunan Zhu",
      "Shunxin Xu",
      "Dong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Shrinking_Class_Space_for_Enhanced_Certainty_in_Semi-Supervised_Learning_ICCV_2023_paper.html": {
    "title": "Shrinking Class Space for Enhanced Certainty in Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lihe Yang",
      "Zhen Zhao",
      "Lei Qi",
      "Yu Qiao",
      "Yinghuan Shi",
      "Hengshuang Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Deep_Equilibrium_Object_Detection_ICCV_2023_paper.html": {
    "title": "Deep Equilibrium Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Wang",
      "Yao Teng",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shan_Diffusion-Based_3D_Human_Pose_Estimation_with_Multi-Hypothesis_Aggregation_ICCV_2023_paper.html": {
    "title": "Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenkang Shan",
      "Zhenhua Liu",
      "Xinfeng Zhang",
      "Zhao Wang",
      "Kai Han",
      "Shanshe Wang",
      "Siwei Ma",
      "Wen Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wan_RPEFlow_Multimodal_Fusion_of_RGB-PointCloud-Event_for_Joint_Optical_Flow_and_ICCV_2023_paper.html": {
    "title": "RPEFlow: Multimodal Fusion of RGB-PointCloud-Event for Joint Optical Flow and Scene Flow Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhexiong Wan",
      "Yuxin Mao",
      "Jing Zhang",
      "Yuchao Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_SMAUG_Sparse_Masked_Autoencoder_for_Efficient_Video-Language_Pre-Training_ICCV_2023_paper.html": {
    "title": "SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanze Lin",
      "Chen Wei",
      "Huiyu Wang",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shukor_eP-ALM_Efficient_Perceptual_Augmentation_of_Language_Models_ICCV_2023_paper.html": {
    "title": "eP-ALM: Efficient Perceptual Augmentation of Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mustafa Shukor",
      "Corentin Dancette",
      "Matthieu Cord"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Multimodal_Optimal_Transport-based_Co-Attention_Transformer_with_Global_Structure_Consistency_for_ICCV_2023_paper.html": {
    "title": "Multimodal Optimal Transport-based Co-Attention Transformer with Global Structure Consistency for Survival Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingxue Xu",
      "Hao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Communication-Efficient_Vertical_Federated_Learning_with_Limited_Overlapping_Samples_ICCV_2023_paper.html": {
    "title": "Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwei Sun",
      "Ziyue Xu",
      "Dong Yang",
      "Vishwesh Nath",
      "Wenqi Li",
      "Can Zhao",
      "Daguang Xu",
      "Yiran Chen",
      "Holger R. Roth"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Niu_On_the_Audio-visual_Synchronization_for_Lip-to-Speech_Synthesis_ICCV_2023_paper.html": {
    "title": "On the Audio-visual Synchronization for Lip-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Niu",
      "Brian Mak"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Oorloff_Robust_One-Shot_Face_Video_Re-enactment_using_Hybrid_Latent_Spaces_of_ICCV_2023_paper.html": {
    "title": "Robust One-Shot Face Video Re-enactment using Hybrid Latent Spaces of StyleGAN2",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trevine Oorloff",
      "Yaser Yacoob"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shin_BallGAN_3D-aware_Image_Synthesis_with_a_Spherical_Background_ICCV_2023_paper.html": {
    "title": "BallGAN: 3D-aware Image Synthesis with a Spherical Background",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minjung Shin",
      "Yunji Seo",
      "Jeongmin Bae",
      "Young Sun Choi",
      "Hyunsu Kim",
      "Hyeran Byun",
      "Youngjung Uh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shen_RPG-Palm_Realistic_Pseudo-data_Generation_for_Palmprint_Recognition_ICCV_2023_paper.html": {
    "title": "RPG-Palm: Realistic Pseudo-data Generation for Palmprint Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Shen",
      "Jianlong Jin",
      "Ruixin Zhang",
      "Huaen Li",
      "Kai Zhao",
      "Yingyi Zhang",
      "Jingyun Zhang",
      "Shouhong Ding",
      "Yang Zhao",
      "Wei Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Lecture_Presentations_Multimodal_Dataset_Towards_Understanding_Multimodality_in_Educational_Videos_ICCV_2023_paper.html": {
    "title": "Lecture Presentations Multimodal Dataset: Towards Understanding Multimodality in Educational Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Won Lee",
      "Chaitanya Ahuja",
      "Paul Pu Liang",
      "Sanika Natu",
      "Louis-Philippe Morency"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xia_Window-Based_Early-Exit_Cascades_for_Uncertainty_Estimation_When_Deep_Ensembles_are_ICCV_2023_paper.html": {
    "title": "Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoxuan Xia",
      "Christos-Savvas Bouganis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhong_AttT2M_Text-Driven_Human_Motion_Generation_with_Multi-Perspective_Attention_Mechanism_ICCV_2023_paper.html": {
    "title": "AttT2M: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongyang Zhong",
      "Lei Hu",
      "Zihao Zhang",
      "Shihong Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mehta_A_Theory_of_Topological_Derivatives_for_Inverse_Rendering_of_Geometry_ICCV_2023_paper.html": {
    "title": "A Theory of Topological Derivatives for Inverse Rendering of Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ishit Mehta",
      "Manmohan Chandraker",
      "Ravi Ramamoorthi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yi_Canonical_Factors_for_Hybrid_Neural_Fields_ICCV_2023_paper.html": {
    "title": "Canonical Factors for Hybrid Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brent Yi",
      "Weijia Zeng",
      "Sam Buchanan",
      "Yi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_XNet_Wavelet-Based_Low_and_High_Frequency_Fusion_Networks_for_Fully-_ICCV_2023_paper.html": {
    "title": "XNet: Wavelet-Based Low and High Frequency Fusion Networks for Fully- and Semi-Supervised Semantic Segmentation of Biomedical Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanfeng Zhou",
      "Jiaxing Huang",
      "Chenlong Wang",
      "Le Song",
      "Ge Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Betrayed_by_Captions_Joint_Caption_Grounding_and_Generation_for_Open_ICCV_2023_paper.html": {
    "title": "Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianzong Wu",
      "Xiangtai Li",
      "Henghui Ding",
      "Xia Li",
      "Guangliang Cheng",
      "Yunhai Tong",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_StyleGANEX_StyleGAN-Based_Manipulation_Beyond_Cropped_Aligned_Faces_ICCV_2023_paper.html": {
    "title": "StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Yang",
      "Liming Jiang",
      "Ziwei Liu",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_HandR2N2_Iterative_3D_Hand_Pose_Estimation_Using_a_Residual_Recurrent_ICCV_2023_paper.html": {
    "title": "HandR2N2: Iterative 3D Hand Pose Estimation Using a Residual Recurrent Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wencan Cheng",
      "Jong Hwan Ko"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Peng_GET_Group_Event_Transformer_for_Event-Based_Vision_ICCV_2023_paper.html": {
    "title": "GET: Group Event Transformer for Event-Based Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yansong Peng",
      "Yueyi Zhang",
      "Zhiwei Xiong",
      "Xiaoyan Sun",
      "Feng Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wolf_Unsupervised_Learning_of_Object-Centric_Embeddings_for_Cell_Instance_Segmentation_in_ICCV_2023_paper.html": {
    "title": "Unsupervised Learning of Object-Centric Embeddings for Cell Instance Segmentation in Microscopy Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steffen Wolf",
      "Manan Lalit",
      "Katie McDole",
      "Jan Funke"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_DyGait_Exploiting_Dynamic_Representations_for_High-performance_Gait_Recognition_ICCV_2023_paper.html": {
    "title": "DyGait: Exploiting Dynamic Representations for High-performance Gait Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Wang",
      "Xianda Guo",
      "Beibei Lin",
      "Tian Yang",
      "Zheng Zhu",
      "Lincheng Li",
      "Shunli Zhang",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Vahidian_When_Do_Curricula_Work_in_Federated_Learning_ICCV_2023_paper.html": {
    "title": "When Do Curricula Work in Federated Learning?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeed Vahidian",
      "Sreevatsank Kadaveru",
      "Woonjoon Baek",
      "Weijia Wang",
      "Vyacheslav Kungurtsev",
      "Chen Chen",
      "Mubarak Shah",
      "Bill Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ancilotto_XiNet_Efficient_Neural_Networks_for_tinyML_ICCV_2023_paper.html": {
    "title": "XiNet: Efficient Neural Networks for tinyML",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alberto Ancilotto",
      "Francesco Paissan",
      "Elisabetta Farella"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_GridPull_Towards_Scalability_in_Learning_Implicit_Representations_from_3D_Point_ICCV_2023_paper.html": {
    "title": "GridPull: Towards Scalability in Learning Implicit Representations from 3D Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Chen",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pian_Audio-Visual_Class-Incremental_Learning_ICCV_2023_paper.html": {
    "title": "Audio-Visual Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiguo Pian",
      "Shentong Mo",
      "Yunhui Guo",
      "Yapeng Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_GeoMIM_Towards_Better_3D_Knowledge_Transfer_via_Masked_Image_Modeling_ICCV_2023_paper.html": {
    "title": "GeoMIM: Towards Better 3D Knowledge Transfer via Masked Image Modeling for Multi-view 3D Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihao Liu",
      "Tai Wang",
      "Boxiao Liu",
      "Qihang Zhang",
      "Yu Liu",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ruan_Towards_Viewpoint-Invariant_Visual_Recognition_via_Adversarial_Training_ICCV_2023_paper.html": {
    "title": "Towards Viewpoint-Invariant Visual Recognition via Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shouwei Ruan",
      "Yinpeng Dong",
      "Hang Su",
      "Jianteng Peng",
      "Ning Chen",
      "Xingxing Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Helping_Hands_An_Object-Aware_Ego-Centric_Video_Recognition_Model_ICCV_2023_paper.html": {
    "title": "Helping Hands: An Object-Aware Ego-Centric Video Recognition Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuhan Zhang",
      "Ankush Gupta",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_RenderIH_A_Large-Scale_Synthetic_Dataset_for_3D_Interacting_Hand_Pose_ICCV_2023_paper.html": {
    "title": "RenderIH: A Large-Scale Synthetic Dataset for 3D Interacting Hand Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijun Li",
      "Linrui Tian",
      "Xindi Zhang",
      "Qi Wang",
      "Bang Zhang",
      "Liefeng Bo",
      "Mengyuan Liu",
      "Chen Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Multi-Metrics_Adaptively_Identifies_Backdoors_in_Federated_Learning_ICCV_2023_paper.html": {
    "title": "Multi-Metrics Adaptively Identifies Backdoors in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siquan Huang",
      "Yijiang Li",
      "Chong Chen",
      "Leyu Shi",
      "Ying Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chan_SpinCam_High-Speed_Imaging_via_a_Rotating_Point-Spread_Function_ICCV_2023_paper.html": {
    "title": "SpinCam: High-Speed Imaging via a Rotating Point-Spread Function",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dorian Chan",
      "Mark Sheinin",
      "Matthew O'Toole"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_FPR_False_Positive_Rectification_for_Weakly_Supervised_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "FPR: False Positive Rectification for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyi Chen",
      "Chenyang Lei",
      "Ruihuang Li",
      "Shuai Li",
      "Zhaoxiang Zhang",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Long_Cross-modal_Scalable_Hierarchical_Clustering_in_Hyperbolic_space_ICCV_2023_paper.html": {
    "title": "Cross-modal Scalable Hyperbolic Hierarchical Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teng Long",
      "Nanne van Noord"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chang_DETRDistill_A_Universal_Knowledge_Distillation_Framework_for_DETR-families_ICCV_2023_paper.html": {
    "title": "DETRDistill: A Universal Knowledge Distillation Framework for DETR-families",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Chang",
      "Shuo Wang",
      "Hai-Ming Xu",
      "Zehui Chen",
      "Chenhongyi Yang",
      "Feng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_FF_Attack_Adversarial_Attack_against_Multiple_Object_Trackers_by_Inducing_ICCV_2023_paper.html": {
    "title": "F&F Attack: Adversarial Attack against Multiple Object Trackers by Inducing False Negatives and False Positives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Zhou",
      "Qi Ye",
      "Wenhan Luo",
      "Kaihao Zhang",
      "Zhiguo Shi",
      "Jiming Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fei_Transferable_Decoding_with_Visual_Entities_for_Zero-Shot_Image_Captioning_ICCV_2023_paper.html": {
    "title": "Transferable Decoding with Visual Entities for Zero-Shot Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Fei",
      "Teng Wang",
      "Jinrui Zhang",
      "Zhenyu He",
      "Chengjie Wang",
      "Feng Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_ReMoDiffuse_Retrieval-Augmented_Motion_Diffusion_Model_ICCV_2023_paper.html": {
    "title": "ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyuan Zhang",
      "Xinying Guo",
      "Liang Pan",
      "Zhongang Cai",
      "Fangzhou Hong",
      "Huirong Li",
      "Lei Yang",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pautrat_GlueStick_Robust_Image_Matching_by_Sticking_Points_and_Lines_Together_ICCV_2023_paper.html": {
    "title": "GlueStick: Robust Image Matching by Sticking Points and Lines Together",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "RÃ©mi Pautrat",
      "Iago SuÃ¡rez",
      "Yifan Yu",
      "Marc Pollefeys",
      "Viktor Larsson"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Klotz_Computational_3D_Imaging_with_Position_Sensors_ICCV_2023_paper.html": {
    "title": "Computational 3D Imaging with Position Sensors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Klotz",
      "Mohit Gupta",
      "Aswin C. Sankaranarayanan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_PointMBF_A_Multi-scale_Bidirectional_Fusion_Network_for_Unsupervised_RGB-D_Point_ICCV_2023_paper.html": {
    "title": "PointMBF: A Multi-scale Bidirectional Fusion Network for Unsupervised RGB-D Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingzhi Yuan",
      "Kexue Fu",
      "Zhihao Li",
      "Yucong Meng",
      "Manning Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shao_Towards_Multi-Layered_3D_Garments_Animation_ICCV_2023_paper.html": {
    "title": "Towards Multi-Layered 3D Garments Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yidi Shao",
      "Chen Change Loy",
      "Bo Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mundra_LiveHand_Real-time_and_Photorealistic_Neural_Hand_Rendering_ICCV_2023_paper.html": {
    "title": "LiveHand: Real-time and Photorealistic Neural Hand Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Mundra",
      "Mallikarjun B R",
      "Jiayi Wang",
      "Marc Habermann",
      "Christian Theobalt",
      "Mohamed Elgharib"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Advancing_Referring_Expression_Segmentation_Beyond_Single_Image_ICCV_2023_paper.html": {
    "title": "Advancing Referring Expression Segmentation Beyond Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Wu",
      "Zhao Zhang",
      "Chi Xie",
      "Feng Zhu",
      "Rui Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Learning_Image_Harmonization_in_the_Linear_Color_Space_ICCV_2023_paper.html": {
    "title": "Learning Image Harmonization in the Linear Color Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Xu",
      "Gerhard Petrus Hancke",
      "Rynson W.H. Lau"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Heinrich_Chasing_Clouds_Differentiable_Volumetric_Rasterisation_of_Point_Clouds_as_a_ICCV_2023_paper.html": {
    "title": "Chasing Clouds: Differentiable Volumetric Rasterisation of Point Clouds as a Highly Efficient and Accurate Loss for Large-Scale Deformable 3D Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mattias P. Heinrich",
      "Alexander Bigalke",
      "Christoph GroÃbrÃ¶hmer",
      "Lasse Hansen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fu_TripLe_Revisiting_Pretrained_Model_Reuse_and_Progressive_Learning_for_Efficient_ICCV_2023_paper.html": {
    "title": "TripLe: Revisiting Pretrained Model Reuse and Progressive Learning for Efficient Vision Transformer Scaling and Searching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Fu",
      "Hanxian Huang",
      "Zixuan Jiang",
      "Yun Ni",
      "Lifeng Nai",
      "Gang Wu",
      "Liqun Cheng",
      "Yanqi Zhou",
      "Sheng Li",
      "Andrew Li",
      "Jishen Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_LogicSeg_Parsing_Visual_Semantics_with_Neural_Logic_Learning_and_Reasoning_ICCV_2023_paper.html": {
    "title": "LogicSeg: Parsing Visual Semantics with Neural Logic Learning and Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liulei Li",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_The_Devil_is_in_the_Upsampling_Architectural_Decisions_Made_Simpler_ICCV_2023_paper.html": {
    "title": "The Devil is in the Upsampling: Architectural Decisions Made Simpler for Denoising with Deep Image Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilin Liu",
      "Jiang Li",
      "Yunkui Pang",
      "Dong Nie",
      "Pew-Thian Yap"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yoo_Video_Object_Segmentation-aware_Video_Frame_Interpolation_ICCV_2023_paper.html": {
    "title": "Video Object Segmentation-aware Video Frame Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Sang Yoo",
      "Hongjae Lee",
      "Seung-Won Jung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liang_Coherent_Event_Guided_Low-Light_Video_Enhancement_ICCV_2023_paper.html": {
    "title": "Coherent Event Guided Low-Light Video Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxiu Liang",
      "Yixin Yang",
      "Boyu Li",
      "Peiqi Duan",
      "Yong Xu",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Texture_Learning_Domain_Randomization_for_Domain_Generalized_Segmentation_ICCV_2023_paper.html": {
    "title": "Texture Learning Domain Randomization for Domain Generalized Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunghwan Kim",
      "Dae-hwan Kim",
      "Hoseong Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yadav_FCCNs_Fully_Complex-valued_Convolutional_Networks_using_Complex-valued_Color_Model_and_ICCV_2023_paper.html": {
    "title": "FCCNs: Fully Complex-valued Convolutional Networks using Complex-valued Color Model and Loss Function",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saurabh Yadav",
      "Koteswar Rao Jerripothula"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yan_Learning_Concise_and_Descriptive_Attributes_for_Visual_Recognition_ICCV_2023_paper.html": {
    "title": "Learning Concise and Descriptive Attributes for Visual Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An Yan",
      "Yu Wang",
      "Yiwu Zhong",
      "Chengyu Dong",
      "Zexue He",
      "Yujie Lu",
      "William Yang Wang",
      "Jingbo Shang",
      "Julian McAuley"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Learning_Unified_Decompositional_and_Compositional_NeRF_for_Editable_Novel_View_ICCV_2023_paper.html": {
    "title": "Learning Unified Decompositional and Compositional NeRF for Editable Novel View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Wang",
      "Wayne Wu",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lu_Label-Noise_Learning_with_Intrinsically_Long-Tailed_Data_ICCV_2023_paper.html": {
    "title": "Label-Noise Learning with Intrinsically Long-Tailed Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Lu",
      "Yiliang Zhang",
      "Bo Han",
      "Yiu-ming Cheung",
      "Hanzi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Larue_SeeABLE_Soft_Discrepancies_and_Bounded_Contrastive_Learning_for_Exposing_Deepfakes_ICCV_2023_paper.html": {
    "title": "SeeABLE: Soft Discrepancies and Bounded Contrastive Learning for Exposing Deepfakes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Larue",
      "Ngoc-Son Vu",
      "Vitomir Struc",
      "Peter Peer",
      "Vassilis Christophides"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Du_Semi-Supervised_Learning_via_Weight-Aware_Distillation_under_Class_Distribution_Mismatch_ICCV_2023_paper.html": {
    "title": "Semi-Supervised Learning via Weight-Aware Distillation under Class Distribution Mismatch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pan Du",
      "Suyun Zhao",
      "Zisen Sheng",
      "Cuiping Li",
      "Hong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lou_ELFNet_Evidential_Local-global_Fusion_for_Stereo_Matching_ICCV_2023_paper.html": {
    "title": "ELFNet: Evidential Local-global Fusion for Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieming Lou",
      "Weide Liu",
      "Zhuo Chen",
      "Fayao Liu",
      "Jun Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_SimpleClick_Interactive_Image_Segmentation_with_Simple_Vision_Transformers_ICCV_2023_paper.html": {
    "title": "SimpleClick: Interactive Image Segmentation with Simple Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qin Liu",
      "Zhenlin Xu",
      "Gedas Bertasius",
      "Marc Niethammer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/An_Towards_Content-based_Pixel_Retrieval_in_Revisited_Oxford_and_Paris_ICCV_2023_paper.html": {
    "title": "Towards Content-based Pixel Retrieval in Revisited Oxford and Paris",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoyuan An",
      "Woo Jae Kim",
      "Saelyne Yang",
      "Rong Li",
      "Yuchi Huo",
      "Sun-Eui Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Santellani_S-TREK_Sequential_Translation_and_Rotation_Equivariant_Keypoints_for_Local_Feature_ICCV_2023_paper.html": {
    "title": "S-TREK: Sequential Translation and Rotation Equivariant Keypoints for Local Feature Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuele Santellani",
      "Christian Sormann",
      "Mattia Rossi",
      "Andreas Kuhn",
      "Friedrich Fraundorfer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xiang_Retro-FPN_Retrospective_Feature_Pyramid_Network_for_Point_Cloud_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Retro-FPN: Retrospective Feature Pyramid Network for Point Cloud Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Xiang",
      "Xin Wen",
      "Yu-Shen Liu",
      "Hui Zhang",
      "Yi Fang",
      "Zhizhong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kong_Rethinking_Range_View_Representation_for_LiDAR_Segmentation_ICCV_2023_paper.html": {
    "title": "Rethinking Range View Representation for LiDAR Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingdong Kong",
      "Youquan Liu",
      "Runnan Chen",
      "Yuexin Ma",
      "Xinge Zhu",
      "Yikang Li",
      "Yuenan Hou",
      "Yu Qiao",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Divide_and_Conquer_3D_Point_Cloud_Instance_Segmentation_With_Point-Wise_ICCV_2023_paper.html": {
    "title": "Divide and Conquer: 3D Point Cloud Instance Segmentation With Point-Wise Binarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiguang Zhao",
      "Yuyao Yan",
      "Chaolong Yang",
      "Jianan Ye",
      "Xi Yang",
      "Kaizhu Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Piedade_BANSAC_A_Dynamic_BAyesian_Network_for_Adaptive_SAmple_Consensus_ICCV_2023_paper.html": {
    "title": "BANSAC: A Dynamic BAyesian Network for Adaptive SAmple Consensus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valter Piedade",
      "Pedro Miraldo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tian_ShapeScaffolder_Structure-Aware_3D_Shape_Generation_from_Text_ICCV_2023_paper.html": {
    "title": "ShapeScaffolder: Structure-Aware 3D Shape Generation from Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Tian",
      "Yong-Liang Yang",
      "Qi Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Read-only_Prompt_Optimization_for_Vision-Language_Few-shot_Learning_ICCV_2023_paper.html": {
    "title": "Read-only Prompt Optimization for Vision-Language Few-shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjun Lee",
      "Seokwon Song",
      "Jihee Suh",
      "Joonmyeong Choi",
      "Sanghyeok Lee",
      "Hyunwoo J. Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mao_COCO-O_A_Benchmark_for_Object_Detectors_under_Natural_Distribution_Shifts_ICCV_2023_paper.html": {
    "title": "COCO-O: A Benchmark for Object Detectors under Natural Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofeng Mao",
      "Yuefeng Chen",
      "Yao Zhu",
      "Da Chen",
      "Hang Su",
      "Rong Zhang",
      "Hui Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qi_E2NeRF_Event_Enhanced_Neural_Radiance_Fields_from_Blurry_Images_ICCV_2023_paper.html": {
    "title": "E2NeRF: Event Enhanced Neural Radiance Fields from Blurry Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunshan Qi",
      "Lin Zhu",
      "Yu Zhang",
      "Jia Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hazra_EgoTV_Egocentric_Task_Verification_from_Natural_Language_Task_Descriptions_ICCV_2023_paper.html": {
    "title": "EgoTV: Egocentric Task Verification from Natural Language Task Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishi Hazra",
      "Brian Chen",
      "Akshara Rai",
      "Nitin Kamra",
      "Ruta Desai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Singh_Benchmarking_Low-Shot_Robustness_to_Natural_Distribution_Shifts_ICCV_2023_paper.html": {
    "title": "Benchmarking Low-Shot Robustness to Natural Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaditya Singh",
      "Kartik Sarangmath",
      "Prithvijit Chattopadhyay",
      "Judy Hoffman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Teng_StageInteractor_Query-based_Object_Detector_with_Cross-stage_Interaction_ICCV_2023_paper.html": {
    "title": "StageInteractor: Query-based Object Detector with Cross-stage Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Teng",
      "Haisong Liu",
      "Sheng Guo",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guizilini_DeLiRa_Self-Supervised_Depth_Light_and_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "DeLiRa: Self-Supervised Depth, Light, and Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vitor Guizilini",
      "Igor Vasiljevic",
      "Jiading Fang",
      "Rares Ambrus",
      "Sergey Zakharov",
      "Vincent Sitzmann",
      "Adrien Gaidon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Croitoru_Moment_Detection_in_Long_Tutorial_Videos_ICCV_2023_paper.html": {
    "title": "Moment Detection in Long Tutorial Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioana Croitoru",
      "Simion-Vlad Bogolin",
      "Samuel Albanie",
      "Yang Liu",
      "Zhaowen Wang",
      "Seunghyun Yoon",
      "Franck Dernoncourt",
      "Hailin Jin",
      "Trung Bui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qian_Stable_Cluster_Discrimination_for_Deep_Clustering_ICCV_2023_paper.html": {
    "title": "Stable Cluster Discrimination for Deep Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Qian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ceylan_Pix2Video_Video_Editing_using_Image_Diffusion_ICCV_2023_paper.html": {
    "title": "Pix2Video: Video Editing using Image Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duygu Ceylan",
      "Chun-Hao P. Huang",
      "Niloy J. Mitra"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_DFA3D_3D_Deformable_Attention_For_2D-to-3D_Feature_Lifting_ICCV_2023_paper.html": {
    "title": "DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyang Li",
      "Hao Zhang",
      "Zhaoyang Zeng",
      "Shilong Liu",
      "Feng Li",
      "Tianhe Ren",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lu_Holistic_Geometric_Feature_Learning_for_Structured_Reconstruction_ICCV_2023_paper.html": {
    "title": "Holistic Geometric Feature Learning for Structured Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqiong Lu",
      "Linxi Huan",
      "Qiyuan Ma",
      "Xianwei Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/QI_FateZero_Fusing_Attentions_for_Zero-shot_Text-based_Video_Editing_ICCV_2023_paper.html": {
    "title": "FateZero: Fusing Attentions for Zero-shot Text-based Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyang QI",
      "Xiaodong Cun",
      "Yong Zhang",
      "Chenyang Lei",
      "Xintao Wang",
      "Ying Shan",
      "Qifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ji_Uncertainty-guided_Learning_for_Improving_Image_Manipulation_Detection_ICCV_2023_paper.html": {
    "title": "Uncertainty-guided Learning for Improving Image Manipulation Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixiang Ji",
      "Feng Chen",
      "Xin Guo",
      "Yadong Xu",
      "Jian Wang",
      "Jingdong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_LMR_A_Large-Scale_Multi-Reference_Dataset_for_Reference-Based_Super-Resolution_ICCV_2023_paper.html": {
    "title": "LMR: A Large-Scale Multi-Reference Dataset for Reference-Based Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Zhang",
      "Xin Li",
      "Dongliang He",
      "Fu Li",
      "Errui Ding",
      "Zhaoxiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Novello_Neural_Implicit_Surface_Evolution_ICCV_2023_paper.html": {
    "title": "Neural Implicit Surface Evolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiago Novello",
      "Vinicius da Silva",
      "Guilherme Schardong",
      "Luiz Schirmer",
      "Helio Lopes",
      "Luiz Velho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Foo_Distribution-Aligned_Diffusion_for_Human_Mesh_Recovery_ICCV_2023_paper.html": {
    "title": "Distribution-Aligned Diffusion for Human Mesh Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Geng Foo",
      "Jia Gong",
      "Hossein Rahmani",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dravid_Rosetta_Neurons_Mining_the_Common_Units_in_a_Model_Zoo_ICCV_2023_paper.html": {
    "title": "Rosetta Neurons: Mining the Common Units in a Model Zoo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amil Dravid",
      "Yossi Gandelsman",
      "Alexei A. Efros",
      "Assaf Shocher"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Semi-Supervised_Semantic_Segmentation_under_Label_Noise_via_Diverse_Learning_Groups_ICCV_2023_paper.html": {
    "title": "Semi-Supervised Semantic Segmentation under Label Noise via Diverse Learning Groups",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peixia Li",
      "Pulak Purkait",
      "Thalaiyasingam Ajanthan",
      "Majid Abdolshah",
      "Ravi Garg",
      "Hisham Husain",
      "Chenchen Xu",
      "Stephen Gould",
      "Wanli Ouyang",
      "Anton van den Hengel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_AdaMV-MoE_Adaptive_Multi-Task_Vision_Mixture-of-Experts_ICCV_2023_paper.html": {
    "title": "AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianlong Chen",
      "Xuxi Chen",
      "Xianzhi Du",
      "Abdullah Rashwan",
      "Fan Yang",
      "Huizhong Chen",
      "Zhangyang Wang",
      "Yeqing Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Hierarchical_Visual_Categories_Modeling_A_Joint_Representation_Learning_and_Density_ICCV_2023_paper.html": {
    "title": "Hierarchical Visual Categories Modeling: A Joint Representation Learning and Density Estimation Framework for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinglun Li",
      "Xinyu Zhou",
      "Pinxue Guo",
      "Yixuan Sun",
      "Yiwen Huang",
      "Weifeng Ge",
      "Wenqiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Diffuse3D_Wide-Angle_3D_Photography_via_Bilateral_Diffusion_ICCV_2023_paper.html": {
    "title": "Diffuse3D: Wide-Angle 3D Photography via Bilateral Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutao Jiang",
      "Yang Zhou",
      "Yuan Liang",
      "Wenxi Liu",
      "Jianbo Jiao",
      "Yuhui Quan",
      "Shengfeng He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_ReNeRF_Relightable_Neural_Radiance_Fields_with_Nearfield_Lighting_ICCV_2023_paper.html": {
    "title": "ReNeRF: Relightable Neural Radiance Fields with Nearfield Lighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingyan Xu",
      "Gaspard Zoss",
      "Prashanth Chandran",
      "Markus Gross",
      "Derek Bradley",
      "Paulo Gotardo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html": {
    "title": "Segment Anything",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Kirillov",
      "Eric Mintun",
      "Nikhila Ravi",
      "Hanzi Mao",
      "Chloe Rolland",
      "Laura Gustafson",
      "Tete Xiao",
      "Spencer Whitehead",
      "Alexander C. Berg",
      "Wan-Yen Lo",
      "Piotr Dollar",
      "Ross Girshick"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_Unsupervised_Prompt_Tuning_for_Text-Driven_Object_Detection_ICCV_2023_paper.html": {
    "title": "Unsupervised Prompt Tuning for Text-Driven Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhen He",
      "Weijie Chen",
      "Binbin Chen",
      "Shicai Yang",
      "Di Xie",
      "Luojun Lin",
      "Donglian Qi",
      "Yueting Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Thoker_Tubelet-Contrastive_Self-Supervision_for_Video-Efficient_Generalization_ICCV_2023_paper.html": {
    "title": "Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fida Mohammad Thoker",
      "Hazel Doughty",
      "Cees G. M. Snoek"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Rojas_Re-ReND_Real-Time_Rendering_of_NeRFs_across_Devices_ICCV_2023_paper.html": {
    "title": "Re-ReND: Real-Time Rendering of NeRFs across Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Rojas",
      "Jesus Zarzar",
      "Juan C. PÃ©rez",
      "Artsiom Sanakoyeu",
      "Ali Thabet",
      "Albert Pumarola",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_360VOT_A_New_Benchmark_Dataset_for_Omnidirectional_Visual_Object_Tracking_ICCV_2023_paper.html": {
    "title": "360VOT: A New Benchmark Dataset for Omnidirectional Visual Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huajian Huang",
      "Yinzhe Xu",
      "Yingshu Chen",
      "Sai-Kit Yeung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Is_Imitation_All_You_Need_Generalized_Decision-Making_with_Dual-Phase_Training_ICCV_2023_paper.html": {
    "title": "Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Wei",
      "Yanchao Sun",
      "Ruijie Zheng",
      "Sai Vemprala",
      "Rogerio Bonatti",
      "Shuhang Chen",
      "Ratnesh Madaan",
      "Zhongjie Ba",
      "Ashish Kapoor",
      "Shuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Generalizing_Event-Based_Motion_Deblurring_in_Real-World_Scenarios_ICCV_2023_paper.html": {
    "title": "Generalizing Event-Based Motion Deblurring in Real-World Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Zhang",
      "Lei Yu",
      "Wen Yang",
      "Jianzhuang Liu",
      "Gui-Song Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gholamian_Handwritten_and_Printed_Text_Segmentation_A_Signature_Case_Study_ICCV_2023_paper.html": {
    "title": "Handwritten and Printed Text Segmentation: A Signature Case Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sina Gholamian",
      "Ali Vahdat"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kerr_LERF_Language_Embedded_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "LERF: Language Embedded Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Kerr",
      "Chung Min Kim",
      "Ken Goldberg",
      "Angjoo Kanazawa",
      "Matthew Tancik"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_DomainAdaptor_A_Novel_Approach_to_Test-time_Adaptation_ICCV_2023_paper.html": {
    "title": "DomainAdaptor: A Novel Approach to Test-time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Zhang",
      "Lei Qi",
      "Yinghuan Shi",
      "Yang Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fan_RCA-NOC_Relative_Contrastive_Alignment_for_Novel_Object_Captioning_ICCV_2023_paper.html": {
    "title": "RCA-NOC: Relative Contrastive Alignment for Novel Object Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashuo Fan",
      "Yaoyuan Liang",
      "Leyao Liu",
      "Shaolun Huang",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Mitigating_and_Evaluating_Static_Bias_of_Action_Representations_in_the_ICCV_2023_paper.html": {
    "title": "Mitigating and Evaluating Static Bias of Action Representations in the Background and the Foreground",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxin Li",
      "Yuan Liu",
      "Hanwang Zhang",
      "Boyang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nayal_RbA_Segmenting_Unknown_Regions_Rejected_by_All_ICCV_2023_paper.html": {
    "title": "RbA: Segmenting Unknown Regions Rejected by All",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nazir Nayal",
      "Misra Yavuz",
      "JoÃ£o F. Henriques",
      "Fatma GÃ¼ney"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_CuNeRF_Cube-Based_Neural_Radiance_Field_for_Zero-Shot_Medical_Image_Arbitrary-Scale_ICCV_2023_paper.html": {
    "title": "CuNeRF: Cube-Based Neural Radiance Field for Zero-Shot Medical Image Arbitrary-Scale Super Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Chen",
      "Lingxiao Yang",
      "Jian-Huang Lai",
      "Xiaohua Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Beyond_Object_Recognition_A_New_Benchmark_towards_Object_Concept_Learning_ICCV_2023_paper.html": {
    "title": "Beyond Object Recognition: A New Benchmark towards Object Concept Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong-Lu Li",
      "Yue Xu",
      "Xinyu Xu",
      "Xiaohan Mao",
      "Yuan Yao",
      "Siqi Liu",
      "Cewu Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Towards_Open-Vocabulary_Video_Instance_Segmentation_ICCV_2023_paper.html": {
    "title": "Towards Open-Vocabulary Video Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Wang",
      "Cilin Yan",
      "Shuai Wang",
      "Xiaolong Jiang",
      "Xu Tang",
      "Yao Hu",
      "Weidi Xie",
      "Efstratios Gavves"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Unleashing_the_Power_of_Gradient_Signal-to-Noise_Ratio_for_Zero-Shot_NAS_ICCV_2023_paper.html": {
    "title": "Unleashing the Power of Gradient Signal-to-Noise Ratio for Zero-Shot NAS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Sun",
      "Yu Sun",
      "Longxing Yang",
      "Shun Lu",
      "Jilin Mei",
      "Wenxiao Zhao",
      "Yu Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_EgoObjects_A_Large-Scale_Egocentric_Dataset_for_Fine-Grained_Object_Understanding_ICCV_2023_paper.html": {
    "title": "EgoObjects: A Large-Scale Egocentric Dataset for Fine-Grained Object Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenchen Zhu",
      "Fanyi Xiao",
      "Andres Alvarado",
      "Yasmine Babaei",
      "Jiabo Hu",
      "Hichem El-Mohri",
      "Sean Culatana",
      "Roshan Sumbaly",
      "Zhicheng Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_What_Can_Simple_Arithmetic_Operations_Do_for_Temporal_Modeling_ICCV_2023_paper.html": {
    "title": "What Can Simple Arithmetic Operations Do for Temporal Modeling?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Wu",
      "Yuxin Song",
      "Zhun Sun",
      "Jingdong Wang",
      "Chang Xu",
      "Wanli Ouyang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Pixel_Adaptive_Deep_Unfolding_Transformer_for_Hyperspectral_Image_Reconstruction_ICCV_2023_paper.html": {
    "title": "Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miaoyu Li",
      "Ying Fu",
      "Ji Liu",
      "Yulun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_BiViT_Extremely_Compressed_Binary_Vision_Transformers_ICCV_2023_paper.html": {
    "title": "BiViT: Extremely Compressed Binary Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yefei He",
      "Zhenyu Lou",
      "Luoming Zhang",
      "Jing Liu",
      "Weijia Wu",
      "Hong Zhou",
      "Bohan Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bai_Dynamic_PlenOctree_for_Adaptive_Sampling_Refinement_in_Explicit_NeRF_ICCV_2023_paper.html": {
    "title": "Dynamic PlenOctree for Adaptive Sampling Refinement in Explicit NeRF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Bai",
      "Yiqi Lin",
      "Yize Chen",
      "Lin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Scene_Matters_Model-based_Deep_Video_Compression_ICCV_2023_paper.html": {
    "title": "Scene Matters: Model-based Deep Video Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lv Tang",
      "Xinfeng Zhang",
      "Gai Zhang",
      "Xiaoqi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Geng_Tree-Structured_Shading_Decomposition_ICCV_2023_paper.html": {
    "title": "Tree-Structured Shading Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Geng",
      "Hong-Xing Yu",
      "Sharon Zhang",
      "Maneesh Agrawala",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_EfficientTrain_Exploring_Generalized_Curriculum_Learning_for_Training_Visual_Backbones_ICCV_2023_paper.html": {
    "title": "EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulin Wang",
      "Yang Yue",
      "Rui Lu",
      "Tianjiao Liu",
      "Zhao Zhong",
      "Shiji Song",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fan_Simulating_Fluids_in_Real-World_Still_Images_ICCV_2023_paper.html": {
    "title": "Simulating Fluids in Real-World Still Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siming Fan",
      "Jingtan Piao",
      "Chen Qian",
      "Hongsheng Li",
      "Kwan-Yee Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zohaib_SC3K_Self-supervised_and_Coherent_3D_Keypoints_Estimation_from_Rotated_Noisy_ICCV_2023_paper.html": {
    "title": "SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Zohaib",
      "Alessio Del Bue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye_IntrinsicNeRF_Learning_Intrinsic_Neural_Radiance_Fields_for_Editable_Novel_View_ICCV_2023_paper.html": {
    "title": "IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weicai Ye",
      "Shuo Chen",
      "Chong Bao",
      "Hujun Bao",
      "Marc Pollefeys",
      "Zhaopeng Cui",
      "Guofeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gasperini_Segmenting_Known_Objects_and_Unseen_Unknowns_without_Prior_Knowledge_ICCV_2023_paper.html": {
    "title": "Segmenting Known Objects and Unseen Unknowns without Prior Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Gasperini",
      "Alvaro Marcos-Ramiro",
      "Michael Schmidt",
      "Nassir Navab",
      "Benjamin Busam",
      "Federico Tombari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_A_Good_Student_is_Cooperative_and_Reliable_CNN-Transformer_Collaborative_Learning_ICCV_2023_paper.html": {
    "title": "A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinjing Zhu",
      "Yunhao Luo",
      "Xu Zheng",
      "Hao Wang",
      "Lin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xia_CMDA_Cross-Modality_Domain_Adaptation_for_Nighttime_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihao Xia",
      "Chaoqiang Zhao",
      "Meng Zheng",
      "Ziyan Wu",
      "Qiyu Sun",
      "Yang Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yan_Learning_with_Diversity_Self-Expanded_Equalization_for_Better_Generalized_Deep_Metric_ICCV_2023_paper.html": {
    "title": "Learning with Diversity: Self-Expanded Equalization for Better Generalized Deep Metric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiexi Yan",
      "Zhihui Yin",
      "Erkun Yang",
      "Yanhua Yang",
      "Heng Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Fan-Beam_Binarization_Difference_Projection_FB-BDP_A_Novel_Local_Object_Descriptor_ICCV_2023_paper.html": {
    "title": "Fan-Beam Binarization Difference Projection (FB-BDP): A Novel Local Object Descriptor for Fine-Grained Leaf Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Chen",
      "Bin Wang",
      "Yongsheng Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Dynamic_Residual_Classifier_for_Class_Incremental_Learning_ICCV_2023_paper.html": {
    "title": "Dynamic Residual Classifier for Class Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuwei Chen",
      "Xiaobin Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Optimizing_the_Placement_of_Roadside_LiDARs_for_Autonomous_Driving_ICCV_2023_paper.html": {
    "title": "Optimizing the Placement of Roadside LiDARs for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Jiang",
      "Hao Xiang",
      "Xinyu Cai",
      "Runsheng Xu",
      "Jiaqi Ma",
      "Yikang Li",
      "Gim Hee Lee",
      "Si Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yildirim_Diverse_Inpainting_and_Editing_with_GAN_Inversion_ICCV_2023_paper.html": {
    "title": "Diverse Inpainting and Editing with GAN Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmet Burak Yildirim",
      "Hamza Pehlivan",
      "Bahri Batuhan Bilecen",
      "Aysegul Dundar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_InterDiff_Generating_3D_Human-Object_Interactions_with_Physics-Informed_Diffusion_ICCV_2023_paper.html": {
    "title": "InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sirui Xu",
      "Zhengyuan Li",
      "Yu-Xiong Wang",
      "Liang-Yan Gui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ponglertnapakorn_DiFaReli_Diffusion_Face_Relighting_ICCV_2023_paper.html": {
    "title": "DiFaReli: Diffusion Face Relighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Puntawat Ponglertnapakorn",
      "Nontawat Tritrong",
      "Supasorn Suwajanakorn"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_IST-Net_Prior-Free_Category-Level_Pose_Estimation_with_Implicit_Space_Transformation_ICCV_2023_paper.html": {
    "title": "IST-Net: Prior-Free Category-Level Pose Estimation with Implicit Space Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhui Liu",
      "Yukang Chen",
      "Xiaoqing Ye",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Building3D_A_Urban-Scale_Dataset_and_Benchmarks_for_Learning_Roof_Structures_ICCV_2023_paper.html": {
    "title": "Building3D: A Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruisheng Wang",
      "Shangfeng Huang",
      "Hongxin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Safadoust_Multi-Object_Discovery_by_Low-Dimensional_Object_Motion_ICCV_2023_paper.html": {
    "title": "Multi-Object Discovery by Low-Dimensional Object Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sadra Safadoust",
      "Fatma GÃ¼ney"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Patashnik_Localizing_Object-Level_Shape_Variations_with_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Localizing Object-Level Shape Variations with Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Or Patashnik",
      "Daniel Garibi",
      "Idan Azuri",
      "Hadar Averbuch-Elor",
      "Daniel Cohen-Or"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiao_CoSign_Exploring_Co-occurrence_Signals_in_Skeleton-based_Continuous_Sign_Language_Recognition_ICCV_2023_paper.html": {
    "title": "CoSign: Exploring Co-occurrence Signals in Skeleton-based Continuous Sign Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiqi Jiao",
      "Yuecong Min",
      "Yanan Li",
      "Xiaotao Wang",
      "Lei Lei",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Schinagl_GACE_Geometry_Aware_Confidence_Enhancement_for_Black-Box_3D_Object_Detectors_ICCV_2023_paper.html": {
    "title": "GACE: Geometry Aware Confidence Enhancement for Black-Box 3D Object Detectors on LiDAR-Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Schinagl",
      "Georg Krispel",
      "Christian Fruhwirth-Reisinger",
      "Horst Possegger",
      "Horst Bischof"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Saratchandran_Curvature-Aware_Training_for_Coordinate_Networks_ICCV_2023_paper.html": {
    "title": "Curvature-Aware Training for Coordinate Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hemanth Saratchandran",
      "Shin-Fang Chng",
      "Sameera Ramasinghe",
      "Lachlan MacDonald",
      "Simon Lucey"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Disentangle_then_Parse_Night-time_Semantic_Segmentation_with_Illumination_Disentanglement_ICCV_2023_paper.html": {
    "title": "Disentangle then Parse: Night-time Semantic Segmentation with Illumination Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixiang Wei",
      "Lin Chen",
      "Tao Tu",
      "Pengyang Ling",
      "Huaian Chen",
      "Yi Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_Large-Scale_Land_Cover_Mapping_with_Fine-Grained_Classes_via_Class-Aware_Semi-Supervised_ICCV_2023_paper.html": {
    "title": "Large-Scale Land Cover Mapping with Fine-Grained Classes via Class-Aware Semi-Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runmin Dong",
      "Lichao Mou",
      "Mengxuan Chen",
      "Weijia Li",
      "Xin-Yi Tong",
      "Shuai Yuan",
      "Lixian Zhang",
      "Juepeng Zheng",
      "Xiaoxiang Zhu",
      "Haohuan Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gong_ToonTalker_Cross-Domain_Face_Reenactment_ICCV_2023_paper.html": {
    "title": "ToonTalker: Cross-Domain Face Reenactment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Gong",
      "Yong Zhang",
      "Xiaodong Cun",
      "Fei Yin",
      "Yanbo Fan",
      "Xuan Wang",
      "Baoyuan Wu",
      "Yujiu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_LISTER_Neighbor_Decoding_for_Length-Insensitive_Scene_Text_Recognition_ICCV_2023_paper.html": {
    "title": "LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changxu Cheng",
      "Peng Wang",
      "Cheng Da",
      "Qi Zheng",
      "Cong Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Proxy_Anchor-based_Unsupervised_Learning_for_Continuous_Generalized_Category_Discovery_ICCV_2023_paper.html": {
    "title": "Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyungmin Kim",
      "Sungho Suh",
      "Daehwan Kim",
      "Daun Jeong",
      "Hansang Cho",
      "Junmo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cho_Distribution-Aware_Prompt_Tuning_for_Vision-Language_Models_ICCV_2023_paper.html": {
    "title": "Distribution-Aware Prompt Tuning for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eulrang Cho",
      "Jooyeon Kim",
      "Hyunwoo J Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Learning_Rain_Location_Prior_for_Nighttime_Deraining_ICCV_2023_paper.html": {
    "title": "Learning Rain Location Prior for Nighttime Deraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Zhang",
      "Shaodi You",
      "Yu Li",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_FBLNet_FeedBack_Loop_Network_for_Driver_Attention_Prediction_ICCV_2023_paper.html": {
    "title": "FBLNet: FeedBack Loop Network for Driver Attention Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilong Chen",
      "Zhixiong Nan",
      "Tao Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Peng_Source-free_Domain_Adaptive_Human_Pose_Estimation_ICCV_2023_paper.html": {
    "title": "Source-free Domain Adaptive Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qucheng Peng",
      "Ce Zheng",
      "Chen Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shi_Video_Anomaly_Detection_via_Sequentially_Learning_Multiple_Pretext_Tasks_ICCV_2023_paper.html": {
    "title": "Video Anomaly Detection via Sequentially Learning Multiple Pretext Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenrui Shi",
      "Che Sun",
      "Yuwei Wu",
      "Yunde Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_SlaBins_Fisheye_Depth_Estimation_using_Slanted_Bins_on_Road_Environments_ICCV_2023_paper.html": {
    "title": "SlaBins: Fisheye Depth Estimation using Slanted Bins on Road Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongsung Lee",
      "Gyeongsu Cho",
      "Jeongin Park",
      "Kyongjun Kim",
      "Seongoh Lee",
      "Jung-Hee Kim",
      "Seong-Gyun Jeong",
      "Kyungdon Joo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_DOT_A_Distillation-Oriented_Trainer_ICCV_2023_paper.html": {
    "title": "DOT: A Distillation-Oriented Trainer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Borui Zhao",
      "Quan Cui",
      "Renjie Song",
      "Jiajun Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Neural_Collage_Transfer_Artistic_Reconstruction_via_Material_Manipulation_ICCV_2023_paper.html": {
    "title": "Neural Collage Transfer: Artistic Reconstruction via Material Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ganghun Lee",
      "Minji Kim",
      "Yunsu Lee",
      "Minsu Lee",
      "Byoung-Tak Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Fantasia3D_Disentangling_Geometry_and_Appearance_for_High-quality_Text-to-3D_Content_Creation_ICCV_2023_paper.html": {
    "title": "Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Chen",
      "Yongwei Chen",
      "Ningxin Jiao",
      "Kui Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_MagicFusion_Boosting_Text-to-Image_Generation_Performance_by_Fusing_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Zhao",
      "Heliang Zheng",
      "Chaoyue Wang",
      "Long Lan",
      "Wenjing Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yan_UCF_Uncovering_Common_Features_for_Generalizable_Deepfake_Detection_ICCV_2023_paper.html": {
    "title": "UCF: Uncovering Common Features for Generalizable Deepfake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Yan",
      "Yong Zhang",
      "Yanbo Fan",
      "Baoyuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qiao_March_in_Chat_Interactive_Prompting_for_Remote_Embodied_Referring_Expression_ICCV_2023_paper.html": {
    "title": "March in Chat: Interactive Prompting for Remote Embodied Referring Expression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyuan Qiao",
      "Yuankai Qi",
      "Zheng Yu",
      "Jing Liu",
      "Qi Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Deuser_Sample4Geo_Hard_Negative_Sampling_For_Cross-View_Geo-Localisation_ICCV_2023_paper.html": {
    "title": "Sample4Geo: Hard Negative Sampling For Cross-View Geo-Localisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian Deuser",
      "Konrad Habel",
      "Norbert Oswald"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Novel_Scenes__Classes_Towards_Adaptive_Open-set_Object_Detection_ICCV_2023_paper.html": {
    "title": "Novel Scenes & Classes: Towards Adaptive Open-set Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wuyang Li",
      "Xiaoqing Guo",
      "Yixuan Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dawidowicz_LIMITR_Leveraging_Local_Information_for_Medical_Image-Text_Representation_ICCV_2023_paper.html": {
    "title": "LIMITR: Leveraging Local Information for Medical Image-Text Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gefen Dawidowicz",
      "Elad Hirsch",
      "Ayellet Tal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Multi-task_View_Synthesis_with_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "Multi-task View Synthesis with Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhong Zheng",
      "Zhipeng Bao",
      "Martial Hebert",
      "Yu-Xiong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Informative_Data_Mining_for_One-Shot_Cross-Domain_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Informative Data Mining for One-Shot Cross-Domain Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxi Wang",
      "Jian Liang",
      "Jun Xiao",
      "Shuqi Mei",
      "Yuran Yang",
      "Zhaoxiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Efficient_Unified_Demosaicing_for_Bayer_and_Non-Bayer_Patterned_Image_Sensors_ICCV_2023_paper.html": {
    "title": "Efficient Unified Demosaicing for Bayer and Non-Bayer Patterned Image Sensors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haechang Lee",
      "Dongwon Park",
      "Wongi Jeong",
      "Kijeong Kim",
      "Hyunwoo Je",
      "Dongil Ryu",
      "Se Young Chun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Visual_Traffic_Knowledge_Graph_Generation_from_Scene_Images_ICCV_2023_paper.html": {
    "title": "Visual Traffic Knowledge Graph Generation from Scene Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfei Guo",
      "Fei Yin",
      "Xiao-hui Li",
      "Xudong Yan",
      "Tao Xue",
      "Shuqi Mei",
      "Cheng-Lin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Song_Householder_Projector_for_Unsupervised_Latent_Semantics_Discovery_ICCV_2023_paper.html": {
    "title": "Householder Projector for Unsupervised Latent Semantics Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Song",
      "Jichao Zhang",
      "Nicu Sebe",
      "Wei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Spatially-Adaptive_Feature_Modulation_for_Efficient_Image_Super-Resolution_ICCV_2023_paper.html": {
    "title": "Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Sun",
      "Jiangxin Dong",
      "Jinhui Tang",
      "Jinshan Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Unsupervised_Image_Denoising_in_Real-World_Scenarios_via_Self-Collaboration_Parallel_Generative_ICCV_2023_paper.html": {
    "title": "Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Lin",
      "Chao Ren",
      "Xiao Liu",
      "Jie Huang",
      "Yinjie Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Bayesian_Optimization_Meets_Self-Distillation_ICCV_2023_paper.html": {
    "title": "Bayesian Optimization Meets Self-Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HyunJae Lee",
      "Heon Song",
      "Hyeonsoo Lee",
      "Gi-hyeon Lee",
      "Suyeong Park",
      "Donggeun Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_No_Fear_of_Classifier_Biases_Neural_Collapse_Inspired_Federated_Learning_ICCV_2023_paper.html": {
    "title": "No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zexi Li",
      "Xinyi Shang",
      "Rui He",
      "Tao Lin",
      "Chao Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_MemorySeg_Online_LiDAR_Semantic_Segmentation_with_a_Latent_Memory_ICCV_2023_paper.html": {
    "title": "MemorySeg: Online LiDAR Semantic Segmentation with a Latent Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enxu Li",
      "Sergio Casas",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chan_Hashing_Neural_Video_Decomposition_with_Multiplicative_Residuals_in_Space-Time_ICCV_2023_paper.html": {
    "title": "Hashing Neural Video Decomposition with Multiplicative Residuals in Space-Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-Hung Chan",
      "Cheng-Yang Yuan",
      "Cheng Sun",
      "Hwann-Tzong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mao_Multimodal_Variational_Auto-encoder_based_Audio-Visual_Segmentation_ICCV_2023_paper.html": {
    "title": "Multimodal Variational Auto-encoder based Audio-Visual Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Mao",
      "Jing Zhang",
      "Mochu Xiang",
      "Yiran Zhong",
      "Yuchao Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Rana_DynaMITe_Dynamic_Query_Bootstrapping_for_Multi-object_Interactive_Segmentation_Transformer_ICCV_2023_paper.html": {
    "title": "DynaMITe: Dynamic Query Bootstrapping for Multi-object Interactive Segmentation Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amit Kumar Rana",
      "Sabarinath Mahadevan",
      "Alexander Hermans",
      "Bastian Leibe"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_FRAug_Tackling_Federated_Learning_with_Non-IID_Features_via_Representation_Augmentation_ICCV_2023_paper.html": {
    "title": "FRAug: Tackling Federated Learning with Non-IID Features via Representation Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haokun Chen",
      "Ahmed Frikha",
      "Denis Krompass",
      "Jindong Gu",
      "Volker Tresp"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Homography_Guided_Temporal_Fusion_for_Road_Line_and_Marking_Segmentation_ICCV_2023_paper.html": {
    "title": "Homography Guided Temporal Fusion for Road Line and Marking Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shan Wang",
      "Chuong Nguyen",
      "Jiawei Liu",
      "Kaihao Zhang",
      "Wenhan Luo",
      "Yanhao Zhang",
      "Sundaram Muthu",
      "Fahira Afzal Maken",
      "Hongdong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_NeuRBF_A_Neural_Fields_Representation_with_Adaptive_Radial_Basis_Functions_ICCV_2023_paper.html": {
    "title": "NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhang Chen",
      "Zhong Li",
      "Liangchen Song",
      "Lele Chen",
      "Jingyi Yu",
      "Junsong Yuan",
      "Yi Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_OmnimatteRF_Robust_Omnimatte_with_3D_Background_Modeling_ICCV_2023_paper.html": {
    "title": "OmnimatteRF: Robust Omnimatte with 3D Background Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geng Lin",
      "Chen Gao",
      "Jia-Bin Huang",
      "Changil Kim",
      "Yipeng Wang",
      "Matthias Zwicker",
      "Ayush Saraf"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jang_Self-supervised_Image_Denoising_with_Downsampled_Invariance_Loss_and_Conditional_Blind-Spot_ICCV_2023_paper.html": {
    "title": "Self-supervised Image Denoising with Downsampled Invariance Loss and Conditional Blind-Spot Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeong Il Jang",
      "Keuntek Lee",
      "Gu Yong Park",
      "Seyun Kim",
      "Nam Ik Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Multi-granularity_Interaction_Simulation_for_Unsupervised_Interactive_Segmentation_ICCV_2023_paper.html": {
    "title": "Multi-granularity Interaction Simulation for Unsupervised Interactive Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kehan Li",
      "Yian Zhao",
      "Zhennan Wang",
      "Zesen Cheng",
      "Peng Jin",
      "Xiangyang Ji",
      "Li Yuan",
      "Chang Liu",
      "Jie Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_RecursiveDet_End-to-End_Region-Based_Recursive_Object_Detection_ICCV_2023_paper.html": {
    "title": "RecursiveDet: End-to-End Region-Based Recursive Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Zhao",
      "Li Sun",
      "Qingli Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Bold_but_Cautious_Unlocking_the_Potential_of_Personalized_Federated_Learning_ICCV_2023_paper.html": {
    "title": "Bold but Cautious: Unlocking the Potential of Personalized Federated Learning through Cautiously Aggressive Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinghao Wu",
      "Xuefeng Liu",
      "Jianwei Niu",
      "Guogang Zhu",
      "Shaojie Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_ESSAformer_Efficient_Transformer_for_Hyperspectral_Image_Super-resolution_ICCV_2023_paper.html": {
    "title": "ESSAformer: Efficient Transformer for Hyperspectral Image Super-resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjin Zhang",
      "Chi Zhang",
      "Qiming Zhang",
      "Jie Guo",
      "Xinbo Gao",
      "Jing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xiang_Generative_Action_Description_Prompts_for_Skeleton-based_Action_Recognition_ICCV_2023_paper.html": {
    "title": "Generative Action Description Prompts for Skeleton-based Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wangmeng Xiang",
      "Chao Li",
      "Yuxuan Zhou",
      "Biao Wang",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Structure_Invariant_Transformation_for_better_Adversarial_Transferability_ICCV_2023_paper.html": {
    "title": "Structure Invariant Transformation for better Adversarial Transferability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaosen Wang",
      "Zeliang Zhang",
      "Jianping Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_Thinking_Image_Color_Aesthetics_Assessment_Models_Datasets_and_Benchmarks_ICCV_2023_paper.html": {
    "title": "Thinking Image Color Aesthetics Assessment: Models, Datasets and Benchmarks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai He",
      "Anlong Ming",
      "Yaqi Li",
      "Jinyuan Sun",
      "ShunTian Zheng",
      "Huadong Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dal_Cin_Multi-body_Depth_and_Camera_Pose_Estimation_from_Multiple_Views_ICCV_2023_paper.html": {
    "title": "Multi-body Depth and Camera Pose Estimation from Multiple Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Porfiri Dal Cin",
      "Giacomo Boracchi",
      "Luca Magri"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Klinghoffer_DISeR_Designing_Imaging_Systems_with_Reinforcement_Learning_ICCV_2023_paper.html": {
    "title": "DISeR: Designing Imaging Systems with Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tzofi Klinghoffer",
      "Kushagra Tiwary",
      "Nikhil Behari",
      "Bhavya Agrawalla",
      "Ramesh Raskar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_The_Euclidean_Space_is_Evil_Hyperbolic_Attribute_Editing_for_Few-shot_ICCV_2023_paper.html": {
    "title": "The Euclidean Space is Evil: Hyperbolic Attribute Editing for Few-shot Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingxiao Li",
      "Yi Zhang",
      "Shuhui Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_FULLER_Unified_Multi-modality_Multi-task_3D_Perception_via_Multi-level_Gradient_Calibration_ICCV_2023_paper.html": {
    "title": "FULLER: Unified Multi-modality Multi-task 3D Perception via Multi-level Gradient Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijian Huang",
      "Sihao Lin",
      "Guiyu Liu",
      "Mukun Luo",
      "Chaoqiang Ye",
      "Hang Xu",
      "Xiaojun Chang",
      "Xiaodan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shao_Transparent_Shape_from_a_Single_View_Polarization_Image_ICCV_2023_paper.html": {
    "title": "Transparent Shape from a Single View Polarization Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingqi Shao",
      "Chongkun Xia",
      "Zhendong Yang",
      "Junnan Huang",
      "Xueqian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Invariant_Feature_Regularization_for_Fair_Face_Recognition_ICCV_2023_paper.html": {
    "title": "Invariant Feature Regularization for Fair Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiali Ma",
      "Zhongqi Yue",
      "Kagaya Tomoyuki",
      "Suzuki Tomoki",
      "Karlekar Jayashree",
      "Sugiri Pranata",
      "Hanwang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bai_Cross-Domain_Product_Representation_Learning_for_Rich-Content_E-Commerce_ICCV_2023_paper.html": {
    "title": "Cross-Domain Product Representation Learning for Rich-Content E-Commerce",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuehan Bai",
      "Yan Li",
      "Yanhua Cheng",
      "Wenjie Yang",
      "Quan Chen",
      "Han Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jia_DriveAdapter_Breaking_the_Coupling_Barrier_of_Perception_and_Planning_in_ICCV_2023_paper.html": {
    "title": "DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaosong Jia",
      "Yulu Gao",
      "Li Chen",
      "Junchi Yan",
      "Patrick Langechuan Liu",
      "Hongyang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dibene_General_Planar_Motion_from_a_Pair_of_3D_Correspondences_ICCV_2023_paper.html": {
    "title": "General Planar Motion from a Pair of 3D Correspondences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan Carlos Dibene",
      "Zhixiang Min",
      "Enrique Dunn"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Single_Depth-image_3D_Reflection_Symmetry_and_Shape_Prediction_ICCV_2023_paper.html": {
    "title": "Single Depth-image 3D Reflection Symmetry and Shape Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoxuan Zhang",
      "Bo Dong",
      "Tong Li",
      "Felix Heide",
      "Pieter Peers",
      "Baocai Yin",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Local_Context-Aware_Active_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "Local Context-Aware Active Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Sun",
      "Cheng Lu",
      "Haibin Ling"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ni_Deep_Incubation_Training_Large_Models_by_Divide-and-Conquering_ICCV_2023_paper.html": {
    "title": "Deep Incubation: Training Large Models by Divide-and-Conquering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zanlin Ni",
      "Yulin Wang",
      "Jiangwei Yu",
      "Haojun Jiang",
      "Yue Cao",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Downscaled_Representation_Matters_Improving_Image_Rescaling_with_Collaborative_Downscaled_Images_ICCV_2023_paper.html": {
    "title": "Downscaled Representation Matters: Improving Image Rescaling with Collaborative Downscaled Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingna Xu",
      "Yong Guo",
      "Luoqian Jiang",
      "Mianjie Yu",
      "Jian Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Detection_Transformer_with_Stable_Matching_ICCV_2023_paper.html": {
    "title": "Detection Transformer with Stable Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shilong Liu",
      "Tianhe Ren",
      "Jiayu Chen",
      "Zhaoyang Zeng",
      "Hao Zhang",
      "Feng Li",
      "Hongyang Li",
      "Jun Huang",
      "Hang Su",
      "Jun Zhu",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Be_Everywhere_-_Hear_Everything_BEE_Audio_Scene_Reconstruction_by_ICCV_2023_paper.html": {
    "title": "Be Everywhere - Hear Everything (BEE): Audio Scene Reconstruction by Sparse Audio-Visual Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingfei Chen",
      "Kun Su",
      "Eli Shlizerman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_iVS-Net_Learning_Human_View_Synthesis_from_Internet_Videos_ICCV_2023_paper.html": {
    "title": "iVS-Net: Learning Human View Synthesis from Internet Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junting Dong",
      "Qi Fang",
      "Tianshuo Yang",
      "Qing Shuai",
      "Chengyu Qiao",
      "Sida Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ahn_Story_Visualization_by_Online_Text_Augmentation_with_Context_Memory_ICCV_2023_paper.html": {
    "title": "Story Visualization by Online Text Augmentation with Context Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daechul Ahn",
      "Daneul Kim",
      "Gwangmo Song",
      "Seung Hwan Kim",
      "Honglak Lee",
      "Dongyeop Kang",
      "Jonghyun Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hong_Attention_Discriminant_Sampling_for_Point_Clouds_ICCV_2023_paper.html": {
    "title": "Attention Discriminant Sampling for Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-Yao Hong",
      "Yu-Ying Chou",
      "Tyng-Luh Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zeng_Global_Balanced_Experts_for_Federated_Long-Tailed_Learning_ICCV_2023_paper.html": {
    "title": "Global Balanced Experts for Federated Long-Tailed Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaopei Zeng",
      "Lei Liu",
      "Li Liu",
      "Li Shen",
      "Shaoguo Liu",
      "Baoyuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Estepa_All4One_Symbiotic_Neighbour_Contrastive_Learning_via_Self-Attention_and_Redundancy_Reduction_ICCV_2023_paper.html": {
    "title": "All4One: Symbiotic Neighbour Contrastive Learning via Self-Attention and Redundancy Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Imanol G. Estepa",
      "Ignacio Sarasua",
      "Bhalaji Nagarajan",
      "Petia Radeva"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Contrastive_Pseudo_Learning_for_Open-World_DeepFake_Attribution_ICCV_2023_paper.html": {
    "title": "Contrastive Pseudo Learning for Open-World DeepFake Attribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhimin Sun",
      "Shen Chen",
      "Taiping Yao",
      "Bangjie Yin",
      "Ran Yi",
      "Shouhong Ding",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_ICL-D3IE_In-Context_Learning_with_Diverse_Demonstrations_Updating_for_Document_Information_ICCV_2023_paper.html": {
    "title": "ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiabang He",
      "Lei Wang",
      "Yi Hu",
      "Ning Liu",
      "Hui Liu",
      "Xing Xu",
      "Heng Tao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_IHNet_Iterative_Hierarchical_Network_Guided_by_High-Resolution_Estimated_Information_for_ICCV_2023_paper.html": {
    "title": "IHNet: Iterative Hierarchical Network Guided by High-Resolution Estimated Information for Scene Flow Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Wang",
      "Cheng Chi",
      "Min Lin",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wewer_SimNP_Learning_Self-Similarity_Priors_Between_Neural_Points_ICCV_2023_paper.html": {
    "title": "SimNP: Learning Self-Similarity Priors Between Neural Points",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Wewer",
      "Eddy Ilg",
      "Bernt Schiele",
      "Jan Eric Lenssen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Beyond_the_Limitation_of_Monocular_3D_Detector_via_Knowledge_Distillation_ICCV_2023_paper.html": {
    "title": "Beyond the Limitation of Monocular 3D Detector via Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiran Yang",
      "Dongshuo Yin",
      "Xuee Rong",
      "Xian Sun",
      "Wenhui Diao",
      "Xinming Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Cascade-DETR_Delving_into_High-Quality_Universal_Object_Detection_ICCV_2023_paper.html": {
    "title": "Cascade-DETR: Delving into High-Quality Universal Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingqiao Ye",
      "Lei Ke",
      "Siyuan Li",
      "Yu-Wing Tai",
      "Chi-Keung Tang",
      "Martin Danelljan",
      "Fisher Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Park_ACLS_Adaptive_and_Conditional_Label_Smoothing_for_Network_Calibration_ICCV_2023_paper.html": {
    "title": "ACLS: Adaptive and Conditional Label Smoothing for Network Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyekang Park",
      "Jongyoun Noh",
      "Youngmin Oh",
      "Donghyeon Baek",
      "Bumsub Ham"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_EMR-MSF_Self-Supervised_Recurrent_Monocular_Scene_Flow_Exploiting_Ego-Motion_Rigidity_ICCV_2023_paper.html": {
    "title": "EMR-MSF: Self-Supervised Recurrent Monocular Scene Flow Exploiting Ego-Motion Rigidity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Jiang",
      "Masatoshi Okutomi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Evaluation_and_Improvement_of_Interpretability_for_Self-Explainable_Part-Prototype_Networks_ICCV_2023_paper.html": {
    "title": "Evaluation and Improvement of Interpretability for Self-Explainable Part-Prototype Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihan Huang",
      "Mengqi Xue",
      "Wenqi Huang",
      "Haofei Zhang",
      "Jie Song",
      "Yongcheng Jing",
      "Mingli Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Temporal-Coded_Spiking_Neural_Networks_with_Dynamic_Firing_Threshold_Learning_with_ICCV_2023_paper.html": {
    "title": "Temporal-Coded Spiking Neural Networks with Dynamic Firing Threshold: Learning with Event-Driven Backpropagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Wei",
      "Malu Zhang",
      "Hong Qu",
      "Ammar Belatreche",
      "Jian Zhang",
      "Hong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Mitigating_Adversarial_Vulnerability_through_Causal_Parameter_Estimation_by_Adversarial_Double_ICCV_2023_paper.html": {
    "title": "Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byung-Kwan Lee",
      "Junho Kim",
      "Yong Man Ro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Dynamic_Token_Pruning_in_Plain_Vision_Transformers_for_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan Tang",
      "Bowen Zhang",
      "Jiajun Liu",
      "Fagui Liu",
      "Yifan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_Shape_Anchor_Guided_Holistic_Indoor_Scene_Understanding_ICCV_2023_paper.html": {
    "title": "Shape Anchor Guided Holistic Indoor Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyue Dong",
      "Linxi Huan",
      "Hanjiang Xiong",
      "Shuhan Shen",
      "Xianwei Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cao_Knowledge-Aware_Federated_Active_Learning_with_Non-IID_Data_ICCV_2023_paper.html": {
    "title": "Knowledge-Aware Federated Active Learning with Non-IID Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Tong Cao",
      "Ye Shi",
      "Baosheng Yu",
      "Jingya Wang",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_PlankAssembly_Robust_3D_Reconstruction_from_Three_Orthographic_Views_with_Learnt_ICCV_2023_paper.html": {
    "title": "PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Hu",
      "Jia Zheng",
      "Zixin Zhang",
      "Xiaojun Yuan",
      "Jian Yin",
      "Zihan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_PODIA-3D_Domain_Adaptation_of_3D_Generative_Model_Across_Large_Domain_ICCV_2023_paper.html": {
    "title": "PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gwanghyun Kim",
      "Ji Ha Jang",
      "Se Young Chun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nair_Steered_Diffusion_A_Generalized_Framework_for_Plug-and-Play_Conditional_Image_Synthesis_ICCV_2023_paper.html": {
    "title": "Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nithin Gopalakrishnan Nair",
      "Anoop Cherian",
      "Suhas Lohit",
      "Ye Wang",
      "Toshiaki Koike-Akino",
      "Vishal M. Patel",
      "Tim K. Marks"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xie_DiffFit_Unlocking_Transferability_of_Large_Diffusion_Models_via_Simple_Parameter-efficient_ICCV_2023_paper.html": {
    "title": "DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-efficient Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enze Xie",
      "Lewei Yao",
      "Han Shi",
      "Zhili Liu",
      "Daquan Zhou",
      "Zhaoqiang Liu",
      "Jiawei Li",
      "Zhenguo Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Irshad_NeO_360_Neural_Fields_for_Sparse_View_Synthesis_of_Outdoor_ICCV_2023_paper.html": {
    "title": "NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Zubair Irshad",
      "Sergey Zakharov",
      "Katherine Liu",
      "Vitor Guizilini",
      "Thomas Kollar",
      "Adrien Gaidon",
      "Zsolt Kira",
      "Rares Ambrus"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yan_UnLoc_A_Unified_Framework_for_Video_Localization_Tasks_ICCV_2023_paper.html": {
    "title": "UnLoc: A Unified Framework for Video Localization Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shen Yan",
      "Xuehan Xiong",
      "Arsha Nagrani",
      "Anurag Arnab",
      "Zhonghao Wang",
      "Weina Ge",
      "David Ross",
      "Cordelia Schmid"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_QD-BEV__Quantization-aware_View-guided_Distillation_for_Multi-view_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "QD-BEV : Quantization-aware View-guided Distillation for Multi-view 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhang",
      "Zhen Dong",
      "Huanrui Yang",
      "Ming Lu",
      "Cheng-Ching Tseng",
      "Yuan Du",
      "Kurt Keutzer",
      "Li Du",
      "Shanghang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Maeda_Fast_Inference_and_Update_of_Probabilistic_Density_Estimation_on_Trajectory_ICCV_2023_paper.html": {
    "title": "Fast Inference and Update of Probabilistic Density Estimation on Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takahiro Maeda",
      "Norimichi Ukita"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Vinker_CLIPascene_Scene_Sketching_with_Different_Types_and_Levels_of_Abstraction_ICCV_2023_paper.html": {
    "title": "CLIPascene: Scene Sketching with Different Types and Levels of Abstraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yael Vinker",
      "Yuval Alaluf",
      "Daniel Cohen-Or",
      "Ariel Shamir"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Da_Vision_Grid_Transformer_for_Document_Layout_Analysis_ICCV_2023_paper.html": {
    "title": "Vision Grid Transformer for Document Layout Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Da",
      "Chuwei Luo",
      "Qi Zheng",
      "Cong Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Naveh_Multi-Directional_Subspace_Editing_in_Style-Space_ICCV_2023_paper.html": {
    "title": "Multi-Directional Subspace Editing in Style-Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Naveh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Adaptive_Superpixel_for_Active_Learning_in_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Adaptive Superpixel for Active Learning in Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoyoung Kim",
      "Minhyeon Oh",
      "Sehyun Hwang",
      "Suha Kwak",
      "Jungseul Ok"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Babiloni_Adaptive_Spiral_Layers_for_Efficient_3D_Representation_Learning_on_Meshes_ICCV_2023_paper.html": {
    "title": "Adaptive Spiral Layers for Efficient 3D Representation Learning on Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesca Babiloni",
      "Matteo Maggioni",
      "Thomas Tanay",
      "Jiankang Deng",
      "Ales Leonardis",
      "Stefanos Zafeiriou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chiaroni_Parametric_Information_Maximization_for_Generalized_Category_Discovery_ICCV_2023_paper.html": {
    "title": "Parametric Information Maximization for Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florent Chiaroni",
      "Jose Dolz",
      "Ziko Imtiaz Masud",
      "Amar Mitiche",
      "Ismail Ben Ayed"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Vavilala_Convex_Decomposition_of_Indoor_Scenes_ICCV_2023_paper.html": {
    "title": "Convex Decomposition of Indoor Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaibhav Vavilala",
      "David Forsyth"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Toward_Unsupervised_Realistic_Visual_Question_Answering_ICCV_2023_paper.html": {
    "title": "Toward Unsupervised Realistic Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Zhang",
      "Chih-Hui Ho",
      "Nuno Vasconcelos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_A_Generalist_Framework_for_Panoptic_Segmentation_of_Images_and_Videos_ICCV_2023_paper.html": {
    "title": "A Generalist Framework for Panoptic Segmentation of Images and Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Chen",
      "Lala Li",
      "Saurabh Saxena",
      "Geoffrey Hinton",
      "David J. Fleet"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cho_DALL-Eval_Probing_the_Reasoning_Skills_and_Social_Biases_of_Text-to-Image_ICCV_2023_paper.html": {
    "title": "DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaemin Cho",
      "Abhay Zala",
      "Mohit Bansal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Heigold_Video_OWL-ViT_Temporally-consistent_Open-world_Localization_in_Video_ICCV_2023_paper.html": {
    "title": "Video OWL-ViT: Temporally-consistent Open-world Localization in Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georg Heigold",
      "Matthias Minderer",
      "Alexey Gritsenko",
      "Alex Bewley",
      "Daniel Keysers",
      "Mario LuÄiÄ",
      "Fisher Yu",
      "Thomas Kipf"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pan_Few_Shot_Font_Generation_Via_Transferring_Similarity_Guided_Global_Style_ICCV_2023_paper.html": {
    "title": "Few Shot Font Generation Via Transferring Similarity Guided Global Style and Quantization Local Style",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Pan",
      "Anna Zhu",
      "Xinyu Zhou",
      "Brian Kenji Iwana",
      "Shilin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Differentiable_Transportation_Pruning_ICCV_2023_paper.html": {
    "title": "Differentiable Transportation Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunqiang Li",
      "Jan C. van Gemert",
      "Torsten Hoefler",
      "Bert Moons",
      "Evangelos Eleftheriou",
      "Bram-Ernst Verhoef"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jaiswal_Physics-Driven_Turbulence_Image_Restoration_with_Stochastic_Refinement_ICCV_2023_paper.html": {
    "title": "Physics-Driven Turbulence Image Restoration with Stochastic Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ajay Jaiswal",
      "Xingguang Zhang",
      "Stanley H. Chan",
      "Zhangyang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Enhancing_Non-line-of-sight_Imaging_via_Learnable_Inverse_Kernel_and_Attention_Mechanisms_ICCV_2023_paper.html": {
    "title": "Enhancing Non-line-of-sight Imaging via Learnable Inverse Kernel and Attention Mechanisms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanhua Yu",
      "Siyuan Shen",
      "Zi Wang",
      "Binbin Huang",
      "Yuehan Wang",
      "Xingyue Peng",
      "Suan Xia",
      "Ping Liu",
      "Ruiqian Li",
      "Shiying Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tripathi_DECO_Dense_Estimation_of_3D_Human-Scene_Contact_In_The_Wild_ICCV_2023_paper.html": {
    "title": "DECO: Dense Estimation of 3D Human-Scene Contact In The Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashank Tripathi",
      "Agniv Chatterjee",
      "Jean-Claude Passy",
      "Hongwei Yi",
      "Dimitrios Tzionas",
      "Michael J. Black"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Scale-Aware_Modulation_Meet_Transformer_ICCV_2023_paper.html": {
    "title": "Scale-Aware Modulation Meet Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weifeng Lin",
      "Ziheng Wu",
      "Jiayu Chen",
      "Jun Huang",
      "Lianwen Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Large_Selective_Kernel_Network_for_Remote_Sensing_Object_Detection_ICCV_2023_paper.html": {
    "title": "Large Selective Kernel Network for Remote Sensing Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Li",
      "Qibin Hou",
      "Zhaohui Zheng",
      "Ming-Ming Cheng",
      "Jian Yang",
      "Xiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shi_PlaneRecTR_Unified_Query_Learning_for_3D_Plane_Recovery_from_a_ICCV_2023_paper.html": {
    "title": "PlaneRecTR: Unified Query Learning for 3D Plane Recovery from a Single View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingjia Shi",
      "Shuaifeng Zhi",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bae_EigenTrajectory_Low-Rank_Descriptors_for_Multi-Modal_Trajectory_Forecasting_ICCV_2023_paper.html": {
    "title": "EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inhwan Bae",
      "Jean Oh",
      "Hae-Gon Jeon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_I-ViT_Integer-only_Quantization_for_Efficient_Vision_Transformer_Inference_ICCV_2023_paper.html": {
    "title": "I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhikai Li",
      "Qingyi Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Simons_SUMMIT_Source-Free_Adaptation_of_Uni-Modal_Models_to_Multi-Modal_Targets_ICCV_2023_paper.html": {
    "title": "SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cody Simons",
      "Dripta S. Raychaudhuri",
      "Sk Miraj Ahmed",
      "Suya You",
      "Konstantinos Karydis",
      "Amit K. Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Learning_a_More_Continuous_Zero_Level_Set_in_Unsigned_Distance_ICCV_2023_paper.html": {
    "title": "Learning a More Continuous Zero Level Set in Unsigned Distance Fields through Level Set Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junsheng Zhou",
      "Baorui Ma",
      "Shujuan Li",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wasim_Video-FocalNets_Spatio-Temporal_Focal_Modulation_for_Video_Action_Recognition_ICCV_2023_paper.html": {
    "title": "Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Syed Talal Wasim",
      "Muhammad Uzair Khattak",
      "Muzammal Naseer",
      "Salman Khan",
      "Mubarak Shah",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Colomer_To_Adapt_or_Not_to_Adapt_Real-Time_Adaptation_for_Semantic_ICCV_2023_paper.html": {
    "title": "To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Botet Colomer",
      "Pier Luigi Dovesi",
      "Theodoros Panagiotakopoulos",
      "Joao Frederico Carvalho",
      "Linus HÃ¤renstam-Nielsen",
      "Hossein Azizpour",
      "Hedvig KjellstrÃ¶m",
      "Daniel Cremers",
      "Matteo Poggi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jaeger_Hidden_Biases_of_End-to-End_Driving_Models_ICCV_2023_paper.html": {
    "title": "Hidden Biases of End-to-End Driving Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bernhard Jaeger",
      "Kashyap Chitta",
      "Andreas Geiger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chang_HairNeRF_Geometry-Aware_Image_Synthesis_for_Hairstyle_Transfer_ICCV_2023_paper.html": {
    "title": "HairNeRF: Geometry-Aware Image Synthesis for Hairstyle Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunggyu Chang",
      "Gihoon Kim",
      "Hayeon Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Strivec_Sparse_Tri-Vector_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "Strivec: Sparse Tri-Vector Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quankai Gao",
      "Qiangeng Xu",
      "Hao Su",
      "Ulrich Neumann",
      "Zexiang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Multiscale_Representation_for_Real-Time_Anti-Aliasing_Neural_Rendering_ICCV_2023_paper.html": {
    "title": "Multiscale Representation for Real-Time Anti-Aliasing Neural Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongting Hu",
      "Zhenkai Zhang",
      "Tingbo Hou",
      "Tongliang Liu",
      "Huan Fu",
      "Mingming Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Borrowing_Knowledge_From_Pre-trained_Language_Model_A_New_Data-efficient_Visual_ICCV_2023_paper.html": {
    "title": "Borrowing Knowledge From Pre-trained Language Model: A New Data-efficient Visual Learning Paradigm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Ma",
      "Shuang Li",
      "JinMing Zhang",
      "Chi Harold Liu",
      "Jingxuan Kang",
      "Yulin Wang",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_GETAvatar_Generative_Textured_Meshes_for_Animatable_Human_Avatars_ICCV_2023_paper.html": {
    "title": "GETAvatar: Generative Textured Meshes for Animatable Human Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanmeng Zhang",
      "Jianfeng Zhang",
      "Rohan Chacko",
      "Hongyi Xu",
      "Guoxian Song",
      "Yi Yang",
      "Jiashi Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Meng_Tracking_without_Label_Unsupervised_Multiple_Object_Tracking_via_Contrastive_Similarity_ICCV_2023_paper.html": {
    "title": "Tracking without Label: Unsupervised Multiple Object Tracking via Contrastive Similarity Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sha Meng",
      "Dian Shao",
      "Jiacheng Guo",
      "Shan Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guan_PIDRo_Parallel_Isomeric_Attention_with_Dynamic_Routing_for_Text-Video_Retrieval_ICCV_2023_paper.html": {
    "title": "PIDRo: Parallel Isomeric Attention with Dynamic Routing for Text-Video Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiyan Guan",
      "Renjing Pei",
      "Bin Shao",
      "Jianzhuang Liu",
      "Weimian Li",
      "Jiaxi Gu",
      "Hang Xu",
      "Songcen Xu",
      "Youliang Yan",
      "Edmund Y. Lam"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cao_Re-mine_Learn_and_Reason_Exploring_the_Cross-modal_Semantic_Correlations_for_ICCV_2023_paper.html": {
    "title": "Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichao Cao",
      "Qingfei Tang",
      "Feng Yang",
      "Xiu Su",
      "Shan You",
      "Xiaobo Lu",
      "Chang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dhiman_Strata-NeRF__Neural_Radiance_Fields_for_Stratified_Scenes_ICCV_2023_paper.html": {
    "title": "Strata-NeRF : Neural Radiance Fields for Stratified Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankit Dhiman",
      "R Srinath",
      "Harsh Rangwani",
      "Rishubh Parihar",
      "Lokesh R Boregowda",
      "Srinath Sridhar",
      "R Venkatesh Babu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_StylerDALLE_Language-Guided_Style_Transfer_Using_a_Vector-Quantized_Tokenizer_of_a_ICCV_2023_paper.html": {
    "title": "StylerDALLE: Language-Guided Style Transfer Using a Vector-Quantized Tokenizer of a Large-Scale Generative Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zipeng Xu",
      "Enver Sangineto",
      "Nicu Sebe"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_3D-aware_Blending_with_Generative_NeRFs_ICCV_2023_paper.html": {
    "title": "3D-aware Blending with Generative NeRFs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunsu Kim",
      "Gayoung Lee",
      "Yunjey Choi",
      "Jin-Hwa Kim",
      "Jun-Yan Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cao_Multi-Modal_Gated_Mixture_of_Local-to-Global_Experts_for_Dynamic_Image_Fusion_ICCV_2023_paper.html": {
    "title": "Multi-Modal Gated Mixture of Local-to-Global Experts for Dynamic Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bing Cao",
      "Yiming Sun",
      "Pengfei Zhu",
      "Qinghua Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Niu_Deep_Image_Harmonization_with_Learnable_Augmentation_ICCV_2023_paper.html": {
    "title": "Deep Image Harmonization with Learnable Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Niu",
      "Junyan Cao",
      "Wenyan Cong",
      "Liqing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Peng_DELFlow_Dense_Efficient_Learning_of_Scene_Flow_for_Large-Scale_Point_ICCV_2023_paper.html": {
    "title": "DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chensheng Peng",
      "Guangming Wang",
      "Xian Wan Lo",
      "Xinrui Wu",
      "Chenfeng Xu",
      "Masayoshi Tomizuka",
      "Wei Zhan",
      "Hesheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_RFD-ECNet_Extreme_Underwater_Image_Compression_with_Reference_to_Feature_Dictionary_ICCV_2023_paper.html": {
    "title": "RFD-ECNet: Extreme Underwater Image Compression with Reference to Feature Dictionary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyao Li",
      "Liquan Shen",
      "Peng Ye",
      "Guorui Feng",
      "Zheyin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_E2VPT_An_Effective_and_Efficient_Approach_for_Visual_Prompt_Tuning_ICCV_2023_paper.html": {
    "title": "E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Han",
      "Qifan Wang",
      "Yiming Cui",
      "Zhiwen Cao",
      "Wenguan Wang",
      "Siyuan Qi",
      "Dongfang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_High-Resolution_Document_Shadow_Removal_via_A_Large-Scale_Real-World_Dataset_and_ICCV_2023_paper.html": {
    "title": "High-Resolution Document Shadow Removal via A Large-Scale Real-World Dataset and A Frequency-Aware Shadow Erasing Net",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zinuo Li",
      "Xuhang Chen",
      "Chi-Man Pun",
      "Xiaodong Cun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html": {
    "title": "Scalable Diffusion Models with Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Peebles",
      "Saining Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_MMST-ViT_Climate_Change-aware_Crop_Yield_Prediction_via_Multi-Modal_Spatial-Temporal_Vision_ICCV_2023_paper.html": {
    "title": "MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fudong Lin",
      "Summer Crawford",
      "Kaleb Guillot",
      "Yihe Zhang",
      "Yan Chen",
      "Xu Yuan",
      "Li Chen",
      "Shelby Williams",
      "Robert Minvielle",
      "Xiangming Xiao",
      "Drew Gholson",
      "Nicolas Ashwell",
      "Tri Setiyono",
      "Brenda Tubana",
      "Lu Peng",
      "Magdy Bayoumi",
      "Nian-Feng Tzeng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_From_Knowledge_Distillation_to_Self-Knowledge_Distillation_A_Unified_Approach_with_ICCV_2023_paper.html": {
    "title": "From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhendong Yang",
      "Ailing Zeng",
      "Zhe Li",
      "Tianke Zhang",
      "Chun Yuan",
      "Yu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_SILT_Shadow-Aware_Iterative_Label_Tuning_for_Learning_to_Detect_Shadows_ICCV_2023_paper.html": {
    "title": "SILT: Shadow-Aware Iterative Label Tuning for Learning to Detect Shadows from Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Yang",
      "Tianyu Wang",
      "Xiaowei Hu",
      "Chi-Wing Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yan_Implicit_Autoencoder_for_Point-Cloud_Self-Supervised_Representation_Learning_ICCV_2023_paper.html": {
    "title": "Implicit Autoencoder for Point-Cloud Self-Supervised Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siming Yan",
      "Zhenpei Yang",
      "Haoxiang Li",
      "Chen Song",
      "Li Guan",
      "Hao Kang",
      "Gang Hua",
      "Qixing Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Grounded_Image_Text_Matching_with_Mismatched_Relation_Reasoning_ICCV_2023_paper.html": {
    "title": "Grounded Image Text Matching with Mismatched Relation Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Wu",
      "Yana Wei",
      "Haozhe Wang",
      "Yongfei Liu",
      "Sibei Yang",
      "Xuming He"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lao_UniKD_Universal_Knowledge_Distillation_for_Mimicking_Homogeneous_or_Heterogeneous_Object_ICCV_2023_paper.html": {
    "title": "UniKD: Universal Knowledge Distillation for Mimicking Homogeneous or Heterogeneous Object Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanshan Lao",
      "Guanglu Song",
      "Boxiao Liu",
      "Yu Liu",
      "Yujiu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_Speech4Mesh_Speech-Assisted_Monocular_3D_Facial_Reconstruction_for_Speech-Driven_3D_Facial_ICCV_2023_paper.html": {
    "title": "Speech4Mesh: Speech-Assisted Monocular 3D Facial Reconstruction for Speech-Driven 3D Facial Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shan He",
      "Haonan He",
      "Shuo Yang",
      "Xiaoyan Wu",
      "Pengcheng Xia",
      "Bing Yin",
      "Cong Liu",
      "Lirong Dai",
      "Chang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xie_BoxDiff_Text-to-Image_Synthesis_with_Training-Free_Box-Constrained_Diffusion_ICCV_2023_paper.html": {
    "title": "BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinheng Xie",
      "Yuexiang Li",
      "Yawen Huang",
      "Haozhe Liu",
      "Wentian Zhang",
      "Yefeng Zheng",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Feng_Generalizing_Neural_Human_Fitting_to_Unseen_Poses_With_Articulated_SE3_ICCV_2023_paper.html": {
    "title": "Generalizing Neural Human Fitting to Unseen Poses With Articulated SE(3) Equivariance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiwen Feng",
      "Peter Kulits",
      "Shichen Liu",
      "Michael J. Black",
      "Victoria Fernandez Abrevaya"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yeo_Rapid_Network_Adaptation_Learning_to_Adapt_Neural_Networks_Using_Test-Time_ICCV_2023_paper.html": {
    "title": "Rapid Network Adaptation: Learning to Adapt Neural Networks Using Test-Time Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teresa Yeo",
      "OÄuzhan Fatih Kar",
      "Zahra Sodagar",
      "Amir Zamir"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Rydell_Theoretical_and_Numerical_Analysis_of_3D_Reconstruction_Using_Point_and_ICCV_2023_paper.html": {
    "title": "Theoretical and Numerical Analysis of 3D Reconstruction Using Point and Line Incidences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Rydell",
      "Elima Shehu",
      "AngÃ©lica Torres"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jin_Explaining_Adversarial_Robustness_of_Neural_Networks_from_Clustering_Effect_Perspective_ICCV_2023_paper.html": {
    "title": "Explaining Adversarial Robustness of Neural Networks from Clustering Effect Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulin Jin",
      "Xiaoyu Zhang",
      "Jian Lou",
      "Xu Ma",
      "Zilong Wang",
      "Xiaofeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Stergiou_Leaping_Into_Memories_Space-Time_Deep_Feature_Synthesis_ICCV_2023_paper.html": {
    "title": "Leaping Into Memories: Space-Time Deep Feature Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandros Stergiou",
      "Nikos Deligiannis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Improving_Generalization_in_Visual_Reinforcement_Learning_via_Conflict-aware_Gradient_Agreement_ICCV_2023_paper.html": {
    "title": "Improving Generalization in Visual Reinforcement Learning via Conflict-aware Gradient Agreement Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siao Liu",
      "Zhaoyu Chen",
      "Yang Liu",
      "Yuzheng Wang",
      "Dingkang Yang",
      "Zhile Zhao",
      "Ziqing Zhou",
      "Xie Yi",
      "Wei Li",
      "Wenqiang Zhang",
      "Zhongxue Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Graph_Matching_with_Bi-level_Noisy_Correspondence_ICCV_2023_paper.html": {
    "title": "Graph Matching with Bi-level Noisy Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijie Lin",
      "Mouxing Yang",
      "Jun Yu",
      "Peng Hu",
      "Changqing Zhang",
      "Xi Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xia_Learning_from_Noisy_Pseudo_Labels_for_Semi-Supervised_Temporal_Action_Localization_ICCV_2023_paper.html": {
    "title": "Learning from Noisy Pseudo Labels for Semi-Supervised Temporal Action Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Xia",
      "Le Wang",
      "Sanping Zhou",
      "Gang Hua",
      "Wei Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_InfiniCity_Infinite-Scale_City_Synthesis_ICCV_2023_paper.html": {
    "title": "InfiniCity: Infinite-Scale City Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chieh Hubert Lin",
      "Hsin-Ying Lee",
      "Willi Menapace",
      "Menglei Chai",
      "Aliaksandr Siarohin",
      "Ming-Hsuan Yang",
      "Sergey Tulyakov"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_OpenOccupancy_A_Large_Scale_Benchmark_for_Surrounding_Semantic_Occupancy_Perception_ICCV_2023_paper.html": {
    "title": "OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Wenbo Xu",
      "Yunpeng Zhang",
      "Yi Wei",
      "Xu Chi",
      "Yun Ye",
      "Dalong Du",
      "Jiwen Lu",
      "Xingang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Weakly-Supervised_Text-Driven_Contrastive_Learning_for_Facial_Behavior_Understanding_ICCV_2023_paper.html": {
    "title": "Weakly-Supervised Text-Driven Contrastive Learning for Facial Behavior Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Zhang",
      "Taoyue Wang",
      "Xiaotian Li",
      "Huiyuan Yang",
      "Lijun Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gomel_Box-based_Refinement_for_Weakly_Supervised_and_Unsupervised_Localization_Tasks_ICCV_2023_paper.html": {
    "title": "Box-based Refinement for Weakly Supervised and Unsupervised Localization Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eyal Gomel",
      "Tal Shaharbany",
      "Lior Wolf"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Activate_and_Reject_Towards_Safe_Domain_Generalization_under_Category_Shift_ICCV_2023_paper.html": {
    "title": "Activate and Reject: Towards Safe Domain Generalization under Category Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoqi Chen",
      "Luyao Tang",
      "Leitian Tao",
      "Hong-Yu Zhou",
      "Yue Huang",
      "Xiaoguang Han",
      "Yizhou Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_PRIOR_Prototype_Representation_Joint_Learning_from_Medical_Images_and_Reports_ICCV_2023_paper.html": {
    "title": "PRIOR: Prototype Representation Joint Learning from Medical Images and Reports",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pujin Cheng",
      "Li Lin",
      "Junyan Lyu",
      "Yijin Huang",
      "Wenhan Luo",
      "Xiaoying Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jang_Dynamic_Mesh_Recovery_from_Partial_Point_Cloud_Sequence_ICCV_2023_paper.html": {
    "title": "Dynamic Mesh Recovery from Partial Point Cloud Sequence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hojun Jang",
      "Minkwan Kim",
      "Jinseok Bae",
      "Young Min Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_WDiscOOD_Out-of-Distribution_Detection_via_Whitened_Linear_Discriminant_Analysis_ICCV_2023_paper.html": {
    "title": "WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiye Chen",
      "Yunzhi Lin",
      "Ruinian Xu",
      "Patricio A. Vela"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xing_Boosting_Few-shot_Action_Recognition_with_Graph-guided_Hybrid_Matching_ICCV_2023_paper.html": {
    "title": "Boosting Few-shot Action Recognition with Graph-guided Hybrid Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazheng Xing",
      "Mengmeng Wang",
      "Yudi Ruan",
      "Bofan Chen",
      "Yaowei Guo",
      "Boyu Mu",
      "Guang Dai",
      "Jingdong Wang",
      "Yong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Neural_Deformable_Models_for_3D_Bi-Ventricular_Heart_Shape_Reconstruction_and_ICCV_2023_paper.html": {
    "title": "Neural Deformable Models for 3D Bi-Ventricular Heart Shape Reconstruction and Modeling from 2D Sparse Cardiac Magnetic Resonance Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Ye",
      "Dong Yang",
      "Mikael Kanski",
      "Leon Axel",
      "Dimitris Metaxas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_Vision_HGNN_An_Image_is_More_than_a_Graph_of_ICCV_2023_paper.html": {
    "title": "Vision HGNN: An Image is More than a Graph of Nodes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Han",
      "Peihao Wang",
      "Souvik Kundu",
      "Ying Ding",
      "Zhangyang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xie_Nonrigid_Object_Contact_Estimation_With_Regional_Unwrapping_Transformer_ICCV_2023_paper.html": {
    "title": "Nonrigid Object Contact Estimation With Regional Unwrapping Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Xie",
      "Zimeng Zhao",
      "Shiying Li",
      "Binghui Zuo",
      "Yangang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Everaert_Diffusion_in_Style_ICCV_2023_paper.html": {
    "title": "Diffusion in Style",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Nicolas Everaert",
      "Marco Bocchio",
      "Sami Arpa",
      "Sabine SÃ¼sstrunk",
      "Radhakrishna Achanta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hesse_FunnyBirds_A_Synthetic_Vision_Dataset_for_a_Part-Based_Analysis_of_ICCV_2023_paper.html": {
    "title": "FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robin Hesse",
      "Simone Schaub-Meyer",
      "Stefan Roth"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Deformable_Neural_Radiance_Fields_using_RGB_and_Event_Cameras_ICCV_2023_paper.html": {
    "title": "Deformable Neural Radiance Fields using RGB and Event Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Ma",
      "Danda Pani Paudel",
      "Ajad Chhatkuli",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Barquero_BeLFusion_Latent_Diffusion_for_Behavior-Driven_Human_Motion_Prediction_ICCV_2023_paper.html": {
    "title": "BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "German Barquero",
      "Sergio Escalera",
      "Cristina Palmero"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiao_Semi-supervised_Semantics-guided_Adversarial_Training_for_Robust_Trajectory_Prediction_ICCV_2023_paper.html": {
    "title": "Semi-supervised Semantics-guided Adversarial Training for Robust Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruochen Jiao",
      "Xiangguo Liu",
      "Takami Sato",
      "Qi Alfred Chen",
      "Qi Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Linear-Covariance_Loss_for_End-to-End_Learning_of_6D_Pose_Estimation_ICCV_2023_paper.html": {
    "title": "Linear-Covariance Loss for End-to-End Learning of 6D Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fulin Liu",
      "Yinlin Hu",
      "Mathieu Salzmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nie_RLSAC_Reinforcement_Learning_Enhanced_Sample_Consensus_for_End-to-End_Robust_Estimation_ICCV_2023_paper.html": {
    "title": "RLSAC: Reinforcement Learning Enhanced Sample Consensus for End-to-End Robust Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Nie",
      "Guangming Wang",
      "Zhe Liu",
      "Luca Cavalli",
      "Marc Pollefeys",
      "Hesheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bansal_CleanCLIP_Mitigating_Data_Poisoning_Attacks_in_Multimodal_Contrastive_Learning_ICCV_2023_paper.html": {
    "title": "CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hritik Bansal",
      "Nishad Singhi",
      "Yu Yang",
      "Fan Yin",
      "Aditya Grover",
      "Kai-Wei Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Multi-Frequency_Representation_Enhancement_with_Privilege_Information_for_Video_Super-Resolution_ICCV_2023_paper.html": {
    "title": "Multi-Frequency Representation Enhancement with Privilege Information for Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Li",
      "Linfeng Zhang",
      "Zikun Liu",
      "Juan Lei",
      "Zhenbo Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Self-supervised_Pre-training_for_Mirror_Detection_ICCV_2023_paper.html": {
    "title": "Self-supervised Pre-training for Mirror Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaying Lin",
      "Rynson W.H. Lau"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_GlowGAN_Unsupervised_Learning_of_HDR_Images_from_LDR_Images_in_ICCV_2023_paper.html": {
    "title": "GlowGAN: Unsupervised Learning of HDR Images from LDR Images in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Wang",
      "Ana Serrano",
      "Xingang Pan",
      "Bin Chen",
      "Karol Myszkowski",
      "Hans-Peter Seidel",
      "Christian Theobalt",
      "Thomas LeimkÃ¼hler"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Cumulative_Spatial_Knowledge_Distillation_for_Vision_Transformers_ICCV_2023_paper.html": {
    "title": "Cumulative Spatial Knowledge Distillation for Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Borui Zhao",
      "Renjie Song",
      "Jiajun Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shi_Dual_Pseudo-Labels_Interactive_Self-Training_for_Semi-Supervised_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.html": {
    "title": "Dual Pseudo-Labels Interactive Self-Training for Semi-Supervised Visible-Infrared Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangming Shi",
      "Yachao Zhang",
      "Xiangbo Yin",
      "Yuan Xie",
      "Zhizhong Zhang",
      "Jianping Fan",
      "Zhongchao Shi",
      "Yanyun Qu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Less_is_More_Focus_Attention_for_Efficient_DETR_ICCV_2023_paper.html": {
    "title": "Less is More: Focus Attention for Efficient DETR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dehua Zheng",
      "Wenhui Dong",
      "Hailin Hu",
      "Xinghao Chen",
      "Yunhe Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Aich_Efficient_Controllable_Multi-Task_Architectures_ICCV_2023_paper.html": {
    "title": "Efficient Controllable Multi-Task Architectures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhishek Aich",
      "Samuel Schulter",
      "Amit K. Roy-Chowdhury",
      "Manmohan Chandraker",
      "Yumin Suh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ju_HumanSD_A_Native_Skeleton-Guided_Diffusion_Model_for_Human_Image_Generation_ICCV_2023_paper.html": {
    "title": "HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Ju",
      "Ailing Zeng",
      "Chenchen Zhao",
      "Jianan Wang",
      "Lei Zhang",
      "Qiang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Piche-Meunier_Lens_Parameter_Estimation_for_Realistic_Depth_of_Field_Modeling_ICCV_2023_paper.html": {
    "title": "Lens Parameter Estimation for Realistic Depth of Field Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominique PichÃ©-Meunier",
      "Yannick Hold-Geoffroy",
      "Jianming Zhang",
      "Jean-FranÃ§ois Lalonde"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gutierrez-Barragan_Learned_Compressive_Representations_for_Single-Photon_3D_Imaging_ICCV_2023_paper.html": {
    "title": "Learned Compressive Representations for Single-Photon 3D Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felipe Gutierrez-Barragan",
      "Fangzhou Mu",
      "Andrei Ardelean",
      "Atul Ingle",
      "Claudio Bruschini",
      "Edoardo Charbon",
      "Yin Li",
      "Mohit Gupta",
      "Andreas Velten"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tel_Alignment-free_HDR_Deghosting_with_Semantics_Consistent_Transformer_ICCV_2023_paper.html": {
    "title": "Alignment-free HDR Deghosting with Semantics Consistent Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steven Tel",
      "Zongwei Wu",
      "Yulun Zhang",
      "BarthÃ©lÃ©my Heyrman",
      "CÃ©dric Demonceaux",
      "Radu Timofte",
      "Dominique Ginhac"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Semantic-Aware_Implicit_Template_Learning_via_Part_Deformation_Consistency_ICCV_2023_paper.html": {
    "title": "Semantic-Aware Implicit Template Learning via Part Deformation Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihyeon Kim",
      "Minseok Joo",
      "Jaewon Lee",
      "Juyeon Ko",
      "Juhan Cha",
      "Hyunwoo J. Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bakr_HRS-Bench_Holistic_Reliable_and_Scalable_Benchmark_for_Text-to-Image_Models_ICCV_2023_paper.html": {
    "title": "HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eslam Mohamed Bakr",
      "Pengzhan Sun",
      "Xiaogian Shen",
      "Faizan Farooq Khan",
      "Li Erran Li",
      "Mohamed Elhoseiny"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Multi3DRefer_Grounding_Text_Description_to_Multiple_3D_Objects_ICCV_2023_paper.html": {
    "title": "Multi3DRefer: Grounding Text Description to Multiple 3D Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Zhang",
      "ZeMing Gong",
      "Angel X. Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tedla_Examining_Autoexposure_for_Challenging_Scenes_ICCV_2023_paper.html": {
    "title": "Examining Autoexposure for Challenging Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SaiKiran Tedla",
      "Beixuan Yang",
      "Michael S. Brown"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_DiffCloth_Diffusion_Based_Garment_Synthesis_and_Manipulation_via_Structural_Cross-modal_ICCV_2023_paper.html": {
    "title": "DiffCloth: Diffusion Based Garment Synthesis and Manipulation via Structural Cross-modal Semantic Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xujie Zhang",
      "Binbin Yang",
      "Michael C. Kampffmeyer",
      "Wenqing Zhang",
      "Shiyue Zhang",
      "Guansong Lu",
      "Liang Lin",
      "Hang Xu",
      "Xiaodan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Improved_Visual_Fine-tuning_with_Natural_Language_Supervision_ICCV_2023_paper.html": {
    "title": "Improved Visual Fine-tuning with Natural Language Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyang Wang",
      "Yuanhong Xu",
      "Juhua Hu",
      "Ming Yan",
      "Jitao Sang",
      "Qi Qian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ahmad_Person_Re-Identification_without_Identification_via_Event_anonymization_ICCV_2023_paper.html": {
    "title": "Person Re-Identification without Identification via Event anonymization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shafiq Ahmad",
      "Pietro Morerio",
      "Alessio Del Bue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xiang_GRAM-HD_3D-Consistent_Image_Generation_at_High_Resolution_with_Generative_Radiance_ICCV_2023_paper.html": {
    "title": "GRAM-HD: 3D-Consistent Image Generation at High Resolution with Generative Radiance Manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianfeng Xiang",
      "Jiaolong Yang",
      "Yu Deng",
      "Xin Tong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_Small_Object_Detection_via_Coarse-to-fine_Proposal_Generation_and_Imitation_Learning_ICCV_2023_paper.html": {
    "title": "Small Object Detection via Coarse-to-fine Proposal Generation and Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Yuan",
      "Gong Cheng",
      "Kebing Yan",
      "Qinghua Zeng",
      "Junwei Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cao_Anomaly_Detection_Under_Distribution_Shift_ICCV_2023_paper.html": {
    "title": "Anomaly Detection Under Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tri Cao",
      "Jiawen Zhu",
      "Guansong Pang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Class_Prior-Free_Positive-Unlabeled_Learning_with_Taylor_Variational_Loss_for_Hyperspectral_ICCV_2023_paper.html": {
    "title": "Class Prior-Free Positive-Unlabeled Learning with Taylor Variational Loss for Hyperspectral Remote Sensing Imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengwei Zhao",
      "Xinyu Wang",
      "Jingtao Li",
      "Yanfei Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_HoloAssist_an_Egocentric_Human_Interaction_Dataset_for_Interactive_AI_Assistants_ICCV_2023_paper.html": {
    "title": "HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Wang",
      "Taein Kwon",
      "Mahdi Rad",
      "Bowen Pan",
      "Ishani Chakraborty",
      "Sean Andrist",
      "Dan Bohus",
      "Ashley Feniello",
      "Bugra Tekin",
      "Felipe Vieira Frujeri",
      "Neel Joshi",
      "Marc Pollefeys"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Self-Feedback_DETR_for_Temporal_Action_Detection_ICCV_2023_paper.html": {
    "title": "Self-Feedback DETR for Temporal Action Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihwan Kim",
      "Miso Lee",
      "Jae-Pil Heo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chai_StableVideo_Text-driven_Consistency-aware_Diffusion_Video_Editing_ICCV_2023_paper.html": {
    "title": "StableVideo: Text-driven Consistency-aware Diffusion Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Chai",
      "Xun Guo",
      "Gaoang Wang",
      "Yan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Deng_PIRNet_Privacy-Preserving_Image_Restoration_Network_via_Wavelet_Lifting_ICCV_2023_paper.html": {
    "title": "PIRNet: Privacy-Preserving Image Restoration Network via Wavelet Lifting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Deng",
      "Chao Gao",
      "Mai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_LAW-Diffusion_Complex_Scene_Generation_by_Diffusion_with_Layouts_ICCV_2023_paper.html": {
    "title": "LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binbin Yang",
      "Yi Luo",
      "Ziliang Chen",
      "Guangrun Wang",
      "Xiaodan Liang",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Multi-Label_Knowledge_Distillation_ICCV_2023_paper.html": {
    "title": "Multi-Label Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Penghui Yang",
      "Ming-Kun Xie",
      "Chen-Chen Zong",
      "Lei Feng",
      "Gang Niu",
      "Masashi Sugiyama",
      "Sheng-Jun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mendieta_Towards_Geospatial_Foundation_Models_via_Continual_Pretraining_ICCV_2023_paper.html": {
    "title": "Towards Geospatial Foundation Models via Continual Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MatÃ­as Mendieta",
      "Boran Han",
      "Xingjian Shi",
      "Yi Zhu",
      "Chen Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_ConSlide_Asynchronous_Hierarchical_Interaction_Transformer_with_Breakup-Reorganize_Rehearsal_for_Continual_ICCV_2023_paper.html": {
    "title": "ConSlide: Asynchronous Hierarchical Interaction Transformer with Breakup-Reorganize Rehearsal for Continual Whole Slide Image Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyan Huang",
      "Weiqin Zhao",
      "Shujun Wang",
      "Yu Fu",
      "Yuming Jiang",
      "Lequan Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_RepQ-ViT_Scale_Reparameterization_for_Post-Training_Quantization_of_Vision_Transformers_ICCV_2023_paper.html": {
    "title": "RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhikai Li",
      "Junrui Xiao",
      "Lianwei Yang",
      "Qingyi Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wan_Enhancing_Privacy_Preservation_in_Federated_Learning_via_Learning_Rate_Perturbation_ICCV_2023_paper.html": {
    "title": "Enhancing Privacy Preservation in Federated Learning via Learning Rate Perturbation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangnian Wan",
      "Haitao Du",
      "Xuejing Yuan",
      "Jun Yang",
      "Meiling Chen",
      "Jie Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_UMC_A_Unified_Bandwidth-efficient_and_Multi-resolution_based_Collaborative_Perception_Framework_ICCV_2023_paper.html": {
    "title": "UMC: A Unified Bandwidth-efficient and Multi-resolution based Collaborative Perception Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhang Wang",
      "Guang Chen",
      "Kai Chen",
      "Zhengfa Liu",
      "Bo Zhang",
      "Alois Knoll",
      "Changjun Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Arrigoni_Viewing_Graph_Solvability_in_Practice_ICCV_2023_paper.html": {
    "title": "Viewing Graph Solvability in Practice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federica Arrigoni",
      "Tomas Pajdla",
      "Andrea Fusiello"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Abdelreheem_SATR_Zero-Shot_Semantic_Segmentation_of_3D_Shapes_ICCV_2023_paper.html": {
    "title": "SATR: Zero-Shot Semantic Segmentation of 3D Shapes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Abdelreheem",
      "Ivan Skorokhodov",
      "Maks Ovsjanikov",
      "Peter Wonka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_ReactioNet_Learning_High-Order_Facial_Behavior_from_Universal_Stimulus-Reaction_by_Dyadic_ICCV_2023_paper.html": {
    "title": "ReactioNet: Learning High-Order Facial Behavior from Universal Stimulus-Reaction by Dyadic Relation Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotian Li",
      "Taoyue Wang",
      "Geran Zhao",
      "Xiang Zhang",
      "Xi Kang",
      "Lijun Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hai_Pseudo_Flow_Consistency_for_Self-Supervised_6D_Object_Pose_Estimation_ICCV_2023_paper.html": {
    "title": "Pseudo Flow Consistency for Self-Supervised 6D Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Hai",
      "Rui Song",
      "Jiaojiao Li",
      "David Ferstl",
      "Yinlin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Song_Emotional_Listener_Portrait_Neural_Listener_Head_Generation_with_Emotion_ICCV_2023_paper.html": {
    "title": "Emotional Listener Portrait: Neural Listener Head Generation with Emotion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luchuan Song",
      "Guojun Yin",
      "Zhenchao Jin",
      "Xiaoyi Dong",
      "Chenliang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jian_Unsupervised_Domain_Adaptation_for_Training_Event-Based_Networks_Using_Contrastive_Learning_ICCV_2023_paper.html": {
    "title": "Unsupervised Domain Adaptation for Training Event-Based Networks Using Contrastive Learning and Uncorrelated Conditioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dayuan Jian",
      "Mohammad Rostami"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Probabilistic_Human_Mesh_Recovery_in_3D_Scenes_from_Egocentric_Views_ICCV_2023_paper.html": {
    "title": "Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siwei Zhang",
      "Qianli Ma",
      "Yan Zhang",
      "Sadegh Aliakbarian",
      "Darren Cosker",
      "Siyu Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tu_ImGeoNet_Image-induced_Geometry-aware_Voxel_Representation_for_Multi-view_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "ImGeoNet: Image-induced Geometry-aware Voxel Representation for Multi-view 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Tu",
      "Shun-Po Chuang",
      "Yu-Lun Liu",
      "Cheng Sun",
      "Ke Zhang",
      "Donna Roy",
      "Cheng-Hao Kuo",
      "Min Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_DRAW_Defending_Camera-shooted_RAW_Against_Image_Manipulation_ICCV_2023_paper.html": {
    "title": "DRAW: Defending Camera-shooted RAW Against Image Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxiao Hu",
      "Qichao Ying",
      "Zhenxing Qian",
      "Sheng Li",
      "Xinpeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_Controllable_Person_Image_Synthesis_with_Pose-Constrained_Latent_Diffusion_ICCV_2023_paper.html": {
    "title": "Controllable Person Image Synthesis with Pose-Constrained Latent Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Han",
      "Xiatian Zhu",
      "Jiankang Deng",
      "Yi-Zhe Song",
      "Tao Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Peng_Diffusion-based_Image_Translation_with_Label_Guidance_for_Domain_Adaptive_Semantic_ICCV_2023_paper.html": {
    "title": "Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duo Peng",
      "Ping Hu",
      "Qiuhong Ke",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_TopoSeg_Topology-Aware_Nuclear_Instance_Segmentation_ICCV_2023_paper.html": {
    "title": "TopoSeg: Topology-Aware Nuclear Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongliang He",
      "Jun Wang",
      "Pengxu Wei",
      "Fan Xu",
      "Xiangyang Ji",
      "Chang Liu",
      "Jie Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cao_SceneRF_Self-Supervised_Monocular_3D_Scene_Reconstruction_with_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anh-Quan Cao",
      "Raoul de Charette"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_Isomer_Isomerous_Transformer_for_Zero-shot_Video_Object_Segmentation_ICCV_2023_paper.html": {
    "title": "Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Yuan",
      "Yifan Wang",
      "Lijun Wang",
      "Xiaoqi Zhao",
      "Huchuan Lu",
      "Yu Wang",
      "Weibo Su",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_CPCM_Contextual_Point_Cloud_Modeling_for_Weakly-supervised_Point_Cloud_Semantic_ICCV_2023_paper.html": {
    "title": "CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lizhao Liu",
      "Zhuangwei Zhuang",
      "Shangxin Huang",
      "Xunlong Xiao",
      "Tianhang Xiang",
      "Cen Chen",
      "Jingdong Wang",
      "Mingkui Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Motamed_PATMAT_Person_Aware_Tuning_of_Mask-Aware_Transformer_for_Face_Inpainting_ICCV_2023_paper.html": {
    "title": "PATMAT: Person Aware Tuning of Mask-Aware Transformer for Face Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saman Motamed",
      "Jianjin Xu",
      "Chen Henry Wu",
      "Christian HÃ¤ne",
      "Jean-Charles Bazin",
      "Fernando De la Torre"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Adaptive_Nonlinear_Latent_Transformation_for_Conditional_Face_Editing_ICCV_2023_paper.html": {
    "title": "Adaptive Nonlinear Latent Transformation for Conditional Face Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhizhong Huang",
      "Siteng Ma",
      "Junping Zhang",
      "Hongming Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Tiny_Updater_Towards_Efficient_Neural_Network-Driven_Software_Updating_ICCV_2023_paper.html": {
    "title": "Tiny Updater: Towards Efficient Neural Network-Driven Software Updating",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linfeng Zhang",
      "Kaisheng Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yan_INT2_Interactive_Trajectory_Prediction_at_Intersections_ICCV_2023_paper.html": {
    "title": "INT2: Interactive Trajectory Prediction at Intersections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijie Yan",
      "Pengfei Li",
      "Zheng Fu",
      "Shaocong Xu",
      "Yongliang Shi",
      "Xiaoxue Chen",
      "Yuhang Zheng",
      "Yang Li",
      "Tianyu Liu",
      "Chuxuan Li",
      "Nairui Luo",
      "Xu Gao",
      "Yilun Chen",
      "Zuoxu Wang",
      "Yifeng Shi",
      "Pengfei Huang",
      "Zhengxiao Han",
      "Jirui Yuan",
      "Jiangtao Gong",
      "Guyue Zhou",
      "Hang Zhao",
      "Hao Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_MapPrior_Birds-Eye_View_Map_Layout_Estimation_with_Generative_Models_ICCV_2023_paper.html": {
    "title": "MapPrior: Bird's-Eye View Map Layout Estimation with Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyue Zhu",
      "Vlas Zyrianov",
      "Zhijian Liu",
      "Shenlong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Maninis_CAD-Estate_Large-scale_CAD_Model_Annotation_in_RGB_Videos_ICCV_2023_paper.html": {
    "title": "CAD-Estate: Large-scale CAD Model Annotation in RGB Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevis-Kokitsi Maninis",
      "Stefan Popov",
      "Matthias NieÃner",
      "Vittorio Ferrari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Song_Conditional_Cross_Attention_Network_for_Multi-Space_Embedding_without_Entanglement_in_ICCV_2023_paper.html": {
    "title": "Conditional Cross Attention Network for Multi-Space Embedding without Entanglement in Only a SINGLE Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chull Hwan Song",
      "Taebaek Hwang",
      "Jooyoung Yoon",
      "Shunghyun Choi",
      "Yeong Hyeon Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qiu_MB-TaylorFormer_Multi-Branch_Efficient_Transformer_Expanded_by_Taylor_Formula_for_Image_ICCV_2023_paper.html": {
    "title": "MB-TaylorFormer: Multi-Branch Efficient Transformer Expanded by Taylor Formula for Image Dehazing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Qiu",
      "Kaihao Zhang",
      "Chenxi Wang",
      "Wenhan Luo",
      "Hongdong Li",
      "Zhi Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_X-Mesh_Towards_Fast_and_Accurate_Text-driven_3D_Stylization_via_Dynamic_ICCV_2023_paper.html": {
    "title": "X-Mesh: Towards Fast and Accurate Text-driven 3D Stylization via Dynamic Textual Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Ma",
      "Xiaoqing Zhang",
      "Xiaoshuai Sun",
      "Jiayi Ji",
      "Haowei Wang",
      "Guannan Jiang",
      "Weilin Zhuang",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chiquier_Muscles_in_Action_ICCV_2023_paper.html": {
    "title": "Muscles in Action",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mia Chiquier",
      "Carl Vondrick"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Large-Scale_Person_Detection_and_Localization_Using_Overhead_Fisheye_Cameras_ICCV_2023_paper.html": {
    "title": "Large-Scale Person Detection and Localization Using Overhead Fisheye Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Yang",
      "Liulei Li",
      "Xueshi Xin",
      "Yifan Sun",
      "Qing Song",
      "Wenguan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_ViLTA_Enhancing_Vision-Language_Pre-training_through_Textual_Augmentation_ICCV_2023_paper.html": {
    "title": "ViLTA: Enhancing Vision-Language Pre-training through Textual Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihan Wang",
      "Zhen Yang",
      "Bin Xu",
      "Juanzi Li",
      "Yankui Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_All-to-Key_Attention_for_Arbitrary_Style_Transfer_ICCV_2023_paper.html": {
    "title": "All-to-Key Attention for Arbitrary Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingrui Zhu",
      "Xiao He",
      "Nannan Wang",
      "Xiaoyu Wang",
      "Xinbo Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Learning_to_Distill_Global_Representation_for_Sparse-View_CT_ICCV_2023_paper.html": {
    "title": "Learning to Distill Global Representation for Sparse-View CT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilong Li",
      "Chenglong Ma",
      "Jie Chen",
      "Junping Zhang",
      "Hongming Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_FocalFormer3D_Focusing_on_Hard_Instance_for_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "FocalFormer3D: Focusing on Hard Instance for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Chen",
      "Zhiding Yu",
      "Yukang Chen",
      "Shiyi Lan",
      "Anima Anandkumar",
      "Jiaya Jia",
      "Jose M. Alvarez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Not_Every_Side_Is_Equal_Localization_Uncertainty_Estimation_for_Semi-Supervised_ICCV_2023_paper.html": {
    "title": "Not Every Side Is Equal: Localization Uncertainty Estimation for Semi-Supervised 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuxin Wang",
      "Wenfei Yang",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Paiss_Teaching_CLIP_to_Count_to_Ten_ICCV_2023_paper.html": {
    "title": "Teaching CLIP to Count to Ten",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roni Paiss",
      "Ariel Ephrat",
      "Omer Tov",
      "Shiran Zada",
      "Inbar Mosseri",
      "Michal Irani",
      "Tali Dekel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Choudhury_TEMPO_Efficient_Multi-View_Pose_Estimation_Tracking_and_Forecasting_ICCV_2023_paper.html": {
    "title": "TEMPO: Efficient Multi-View Pose Estimation, Tracking, and Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohan Choudhury",
      "Kris M. Kitani",
      "LÃ¡szlÃ³ A. Jeni"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_SparseMAE_Sparse_Training_Meets_Masked_Autoencoders_ICCV_2023_paper.html": {
    "title": "SparseMAE: Sparse Training Meets Masked Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aojun Zhou",
      "Yang Li",
      "Zipeng Qin",
      "Jianbo Liu",
      "Junting Pan",
      "Renrui Zhang",
      "Rui Zhao",
      "Peng Gao",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Feng_DiffPose_SpatioTemporal_Diffusion_Model_for_Video-Based_Human_Pose_Estimation_ICCV_2023_paper.html": {
    "title": "DiffPose: SpatioTemporal Diffusion Model for Video-Based Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runyang Feng",
      "Yixing Gao",
      "Tze Ho Elden Tse",
      "Xueqing Ma",
      "Hyung Jin Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_ELITE_Encoding_Visual_Concepts_into_Textual_Embeddings_for_Customized_Text-to-Image_ICCV_2023_paper.html": {
    "title": "ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Wei",
      "Yabo Zhang",
      "Zhilong Ji",
      "Jinfeng Bai",
      "Lei Zhang",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Text2Performer_Text-Driven_Human_Video_Generation_ICCV_2023_paper.html": {
    "title": "Text2Performer: Text-Driven Human Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuming Jiang",
      "Shuai Yang",
      "Tong Liang Koh",
      "Wayne Wu",
      "Chen Change Loy",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cetin_A_Simple_Recipe_to_Meta-Learn_Forward_and_Backward_Transfer_ICCV_2023_paper.html": {
    "title": "A Simple Recipe to Meta-Learn Forward and Backward Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edoardo Cetin",
      "Antonio Carta",
      "Oya Celiktutan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_4D_Myocardium_Reconstruction_with_Decoupled_Motion_and_Shape_Model_ICCV_2023_paper.html": {
    "title": "4D Myocardium Reconstruction with Decoupled Motion and Shape Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Yuan",
      "Cong Liu",
      "Yangang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_IntentQA_Context-aware_Video_Intent_Reasoning_ICCV_2023_paper.html": {
    "title": "IntentQA: Context-aware Video Intent Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiapeng Li",
      "Ping Wei",
      "Wenjuan Han",
      "Lifeng Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shaban_LiDAR-UDA_Self-ensembling_Through_Time_for_Unsupervised_LiDAR_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "LiDAR-UDA: Self-ensembling Through Time for Unsupervised LiDAR Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirreza Shaban",
      "JoonHo Lee",
      "Sanghun Jung",
      "Xiangyun Meng",
      "Byron Boots"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gasperini_Robust_Monocular_Depth_Estimation_under_Challenging_Conditions_ICCV_2023_paper.html": {
    "title": "Robust Monocular Depth Estimation under Challenging Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Gasperini",
      "Nils Morbitzer",
      "HyunJun Jung",
      "Nassir Navab",
      "Federico Tombari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Parametric_Depth_Based_Feature_Representation_Learning_for_Object_Detection_and_ICCV_2023_paper.html": {
    "title": "Parametric Depth Based Feature Representation Learning for Object Detection and Segmentation in Bird's-Eye View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayu Yang",
      "Enze Xie",
      "Miaomiao Liu",
      "Jose M. Alvarez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Moon_MSI_Maximize_Support-Set_Information_for_Few-Shot_Segmentation_ICCV_2023_paper.html": {
    "title": "MSI: Maximize Support-Set Information for Few-Shot Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonghyeon Moon",
      "Samuel S. Sohn",
      "Honglu Zhou",
      "Sejong Yoon",
      "Vladimir Pavlovic",
      "Muhammad Haris Khan",
      "Mubbasir Kapadia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shao_Global_Features_are_All_You_Need_for_Image_Retrieval_and_ICCV_2023_paper.html": {
    "title": "Global Features are All You Need for Image Retrieval and Reranking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihao Shao",
      "Kaifeng Chen",
      "Arjun Karpur",
      "Qinghua Cui",
      "AndrÃ© Araujo",
      "Bingyi Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shuai_DPF-Net_Combining_Explicit_Shape_Priors_in_Deformable_Primitive_Field_for_ICCV_2023_paper.html": {
    "title": "DPF-Net: Combining Explicit Shape Priors in Deformable Primitive Field for Unsupervised Structural Reconstruction of 3D Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyao Shuai",
      "Chi Zhang",
      "Kaizhi Yang",
      "Xuejin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_CORE_Co-planarity_Regularized_Monocular_Geometry_Estimation_with_Weak_Supervision_ICCV_2023_paper.html": {
    "title": "CORE: Co-planarity Regularized Monocular Geometry Estimation with Weak Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuguang Li",
      "Kai Wang",
      "Hui Li",
      "Seon-Min Rhee",
      "Seungju Han",
      "Jihye Kim",
      "Min Yang",
      "Ran Yang",
      "Feng Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_A_Sentence_Speaks_a_Thousand_Images_Domain_Generalization_through_Distilling_ICCV_2023_paper.html": {
    "title": "A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyi Huang",
      "Andy Zhou",
      "Zijian Ling",
      "Mu Cai",
      "Haohan Wang",
      "Yong Jae Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_H3WB_Human3.6M_3D_WholeBody_Dataset_and_Benchmark_ICCV_2023_paper.html": {
    "title": "H3WB: Human3.6M 3D WholeBody Dataset and Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhu",
      "Nermin Samet",
      "David Picard"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Aiger_Yes_we_CANN_Constrained_Approximate_Nearest_Neighbors_for_Local_Feature-Based_ICCV_2023_paper.html": {
    "title": "Yes, we CANN: Constrained Approximate Nearest Neighbors for Local Feature-Based Visual Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dror Aiger",
      "Andre Araujo",
      "Simon Lynen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Marza_Multi-Object_Navigation_with_Dynamically_Learned_Neural_Implicit_Representations_ICCV_2023_paper.html": {
    "title": "Multi-Object Navigation with Dynamically Learned Neural Implicit Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Marza",
      "Laetitia Matignon",
      "Olivier Simonin",
      "Christian Wolf"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Su_NPC_Neural_Point_Characters_from_Video_ICCV_2023_paper.html": {
    "title": "NPC: Neural Point Characters from Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shih-Yang Su",
      "Timur Bagautdinov",
      "Helge Rhodin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pittaluga_LDP-Feat_Image_Features_with_Local_Differential_Privacy_ICCV_2023_paper.html": {
    "title": "LDP-Feat: Image Features with Local Differential Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Pittaluga",
      "Bingbing Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Pre-Training-Free_Image_Manipulation_Localization_through_Non-Mutually_Exclusive_Contrastive_Learning_ICCV_2023_paper.html": {
    "title": "Pre-Training-Free Image Manipulation Localization through Non-Mutually Exclusive Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jizhe Zhou",
      "Xiaochen Ma",
      "Xia Du",
      "Ahmed Y. Alhammadi",
      "Wentao Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_MRN_Multiplexed_Routing_Network_for_Incremental_Multilingual_Text_Recognition_ICCV_2023_paper.html": {
    "title": "MRN: Multiplexed Routing Network for Incremental Multilingual Text Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianlun Zheng",
      "Zhineng Chen",
      "Bingchen Huang",
      "Wei Zhang",
      "Yu-Gang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Michalkiewicz_Domain_Generalization_Guided_by_Gradient_Signal_to_Noise_Ratio_of_ICCV_2023_paper.html": {
    "title": "Domain Generalization Guided by Gradient Signal to Noise Ratio of Parameters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mateusz Michalkiewicz",
      "Masoud Faraki",
      "Xiang Yu",
      "Manmohan Chandraker",
      "Mahsa Baktashmotlagh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Counterfactual-based_Saliency_Map_Towards_Visual_Contrastive_Explanations_for_Neural_Networks_ICCV_2023_paper.html": {
    "title": "Counterfactual-based Saliency Map: Towards Visual Contrastive Explanations for Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xue Wang",
      "Zhibo Wang",
      "Haiqin Weng",
      "Hengchang Guo",
      "Zhifei Zhang",
      "Lu Jin",
      "Tao Wei",
      "Kui Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Vo_MST-compression_Compressing_and_Accelerating_Binary_Neural_Networks_with_Minimum_Spanning_ICCV_2023_paper.html": {
    "title": "MST-compression: Compressing and Accelerating Binary Neural Networks with Minimum Spanning Tree",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quang Hieu Vo",
      "Linh-Tam Tran",
      "Sung-Ho Bae",
      "Lok-Won Kim",
      "Choong Seon Hong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Rambhatla_MOST_Multiple_Object_Localization_with_Self-Supervised_Transformers_for_Object_Discovery_ICCV_2023_paper.html": {
    "title": "MOST: Multiple Object Localization with Self-Supervised Transformers for Object Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Saketh Rambhatla",
      "Ishan Misra",
      "Rama Chellappa",
      "Abhinav Shrivastava"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cai_IIEU_Rethinking_Neural_Feature_Activation_from_Decision-Making_ICCV_2023_paper.html": {
    "title": "IIEU: Rethinking Neural Feature Activation from Decision-Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudong Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Integrally_Migrating_Pre-trained_Transformer_Encoder-decoders_for_Visual_Object_Detection_ICCV_2023_paper.html": {
    "title": "Integrally Migrating Pre-trained Transformer Encoder-decoders for Visual Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Liu",
      "Xiaosong Zhang",
      "Zhiliang Peng",
      "Zonghao Guo",
      "Fang Wan",
      "Xiangyang Ji",
      "Qixiang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Burgdorfer_V-FUSE_Volumetric_Depth_Map_Fusion_with_Long-Range_Constraints_ICCV_2023_paper.html": {
    "title": "V-FUSE: Volumetric Depth Map Fusion with Long-Range Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathaniel Burgdorfer",
      "Philippos Mordohai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guan_CrossLoc3D_Aerial-Ground_Cross-Source_3D_Place_Recognition_ICCV_2023_paper.html": {
    "title": "CrossLoc3D: Aerial-Ground Cross-Source 3D Place Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianrui Guan",
      "Aswath Muthuselvam",
      "Montana Hoover",
      "Xijun Wang",
      "Jing Liang",
      "Adarsh Jagan Sathyamoorthy",
      "Damon Conover",
      "Dinesh Manocha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jin_Recursive_Video_Lane_Detection_ICCV_2023_paper.html": {
    "title": "Recursive Video Lane Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongkwon Jin",
      "Dahyun Kim",
      "Chang-Su Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tyszkiewicz_GECCO_Geometrically-Conditioned_Point_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "GECCO: Geometrically-Conditioned Point Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MichaÅ J Tyszkiewicz",
      "Pascal Fua",
      "Eduard Trulls"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Unsupervised_Self-Driving_Attention_Prediction_via_Uncertainty_Mining_and_Knowledge_Embedding_ICCV_2023_paper.html": {
    "title": "Unsupervised Self-Driving Attention Prediction via Uncertainty Mining and Knowledge Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengfei Zhu",
      "Mengshi Qi",
      "Xia Li",
      "Weijian Li",
      "Huadong Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_PETRv2_A_Unified_Framework_for_3D_Perception_from_Multi-Camera_Images_ICCV_2023_paper.html": {
    "title": "PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingfei Liu",
      "Junjie Yan",
      "Fan Jia",
      "Shuailin Li",
      "Aqi Gao",
      "Tiancai Wang",
      "Xiangyu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Out-of-Domain_GAN_Inversion_via_Invertibility_Decomposition_for_Photo-Realistic_Human_Face_ICCV_2023_paper.html": {
    "title": "Out-of-Domain GAN Inversion via Invertibility Decomposition for Photo-Realistic Human Face Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Yang",
      "Xiaogang XU",
      "Yingcong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dukler_SAFE_Machine_Unlearning_With_Shard_Graphs_ICCV_2023_paper.html": {
    "title": "SAFE: Machine Unlearning With Shard Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yonatan Dukler",
      "Benjamin Bowman",
      "Alessandro Achille",
      "Aditya Golatkar",
      "Ashwin Swaminathan",
      "Stefano Soatto"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Learning_Trajectory-Word_Alignments_for_Video-Language_Tasks_ICCV_2023_paper.html": {
    "title": "Learning Trajectory-Word Alignments for Video-Language Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Yang",
      "Zhangzikang Li",
      "Haiyang Xu",
      "Hanwang Zhang",
      "Qinghao Ye",
      "Chenliang Li",
      "Ming Yan",
      "Yu Zhang",
      "Fei Huang",
      "Songfang Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_OrthoPlanes_A_Novel_Representation_for_Better_3D-Awareness_of_GANs_ICCV_2023_paper.html": {
    "title": "OrthoPlanes: A Novel Representation for Better 3D-Awareness of GANs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honglin He",
      "Zhuoqian Yang",
      "Shikai Li",
      "Bo Dai",
      "Wayne Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yin_Geometry-guided_Feature_Learning_and_Fusion_for_Indoor_Scene_Reconstruction_ICCV_2023_paper.html": {
    "title": "Geometry-guided Feature Learning and Fusion for Indoor Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihong Yin",
      "Sezer Karaoglu",
      "Theo Gevers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Atmospheric_Transmission_and_Thermal_Inertia_Induced_Blind_Road_Segmentation_with_ICCV_2023_paper.html": {
    "title": "Atmospheric Transmission and Thermal Inertia Induced Blind Road Segmentation with a Large-Scale Dataset TBRSD",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junzhang Chen",
      "Xiangzhi Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_NeTONeural_Reconstruction_of_Transparent_Objects_with_Self-Occlusion_Aware_Refraction-Tracing_ICCV_2023_paper.html": {
    "title": "NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongcheng Li",
      "Xiaoxiao Long",
      "Yusen Wang",
      "Tuo Cao",
      "Wenping Wang",
      "Fei Luo",
      "Chunxia Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shi_Boosting_3-DoF_Ground-to-Satellite_Camera_Localization_Accuracy_via_Geometry-Guided_Cross-View_Transformer_ICCV_2023_paper.html": {
    "title": "Boosting 3-DoF Ground-to-Satellite Camera Localization Accuracy via Geometry-Guided Cross-View Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujiao Shi",
      "Fei Wu",
      "Akhil Perincherry",
      "Ankit Vora",
      "Hongdong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cao_Efficient-VQGAN_Towards_High-Resolution_Image_Generation_with_Efficient_Vision_Transformers_ICCV_2023_paper.html": {
    "title": "Efficient-VQGAN: Towards High-Resolution Image Generation with Efficient Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyue Cao",
      "Yueqin Yin",
      "Lianghua Huang",
      "Yu Liu",
      "Xin Zhao",
      "Deli Zhao",
      "Kaigi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_DLGSANet_Lightweight_Dynamic_Local_and_Global_Self-Attention_Networks_for_Image_ICCV_2023_paper.html": {
    "title": "DLGSANet: Lightweight Dynamic Local and Global Self-Attention Networks for Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Li",
      "Jiangxin Dong",
      "Jinhui Tang",
      "Jinshan Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Adaptive_Reordering_Sampler_with_Neurally_Guided_MAGSAC_ICCV_2023_paper.html": {
    "title": "Adaptive Reordering Sampler with Neurally Guided MAGSAC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Wei",
      "Jiri Matas",
      "Daniel Barath"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Learning_Cross-Representation_Affinity_Consistency_for_Sparsely_Supervised_Biomedical_Instance_Segmentation_ICCV_2023_paper.html": {
    "title": "Learning Cross-Representation Affinity Consistency for Sparsely Supervised Biomedical Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Liu",
      "Wei Huang",
      "Zhiwei Xiong",
      "Shenglong Zhou",
      "Yueyi Zhang",
      "Xuejin Chen",
      "Zheng-Jun Zha",
      "Feng Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Black-Box_Unsupervised_Domain_Adaptation_with_Bi-Directional_Atkinson-Shiffrin_Memory_ICCV_2023_paper.html": {
    "title": "Black-Box Unsupervised Domain Adaptation with Bi-Directional Atkinson-Shiffrin Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Zhang",
      "Jiaxing Huang",
      "Xueying Jiang",
      "Shijian Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Towards_Fair_and_Comprehensive_Comparisons_for_Image-Based_3D_Object_Detection_ICCV_2023_paper.html": {
    "title": "Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinzhu Ma",
      "Yongtao Wang",
      "Yinmin Zhang",
      "Zhiyi Xia",
      "Yuan Meng",
      "Zhihui Wang",
      "Haojie Li",
      "Wanli Ouyang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qing_Disentangling_Spatial_and_Temporal_Learning_for_Efficient_Image-to-Video_Transfer_Learning_ICCV_2023_paper.html": {
    "title": "Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwu Qing",
      "Shiwei Zhang",
      "Ziyuan Huang",
      "Yingya Zhang",
      "Changxin Gao",
      "Deli Zhao",
      "Nong Sang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Menten_A_Skeletonization_Algorithm_for_Gradient-Based_Optimization_ICCV_2023_paper.html": {
    "title": "A Skeletonization Algorithm for Gradient-Based Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin J. Menten",
      "Johannes C. Paetzold",
      "Veronika A. Zimmer",
      "Suprosanna Shit",
      "Ivan Ezhov",
      "Robbie Holland",
      "Monika Probst",
      "Julia A. Schnabel",
      "Daniel Rueckert"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_V3Det_Vast_Vocabulary_Visual_Detection_Dataset_ICCV_2023_paper.html": {
    "title": "V3Det: Vast Vocabulary Visual Detection Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Wang",
      "Pan Zhang",
      "Tao Chu",
      "Yuhang Cao",
      "Yujie Zhou",
      "Tong Wu",
      "Bin Wang",
      "Conghui He",
      "Dahua Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Coarse-to-Fine_Learning_Compact_Discriminative_Representation_for_Single-Stage_Image_Retrieval_ICCV_2023_paper.html": {
    "title": "Coarse-to-Fine: Learning Compact Discriminative Representation for Single-Stage Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunquan Zhu",
      "Xinkai Gao",
      "Bo Ke",
      "Ruizhi Qiao",
      "Xing Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Patil_Multi-weather_Image_Restoration_via_Domain_Translation_ICCV_2023_paper.html": {
    "title": "Multi-weather Image Restoration via Domain Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prashant W. Patil",
      "Sunil Gupta",
      "Santu Rana",
      "Svetha Venkatesh",
      "Subrahmanyam Murala"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Deep_Fusion_Transformer_Network_with_Weighted_Vector-Wise_Keypoints_Voting_for_ICCV_2023_paper.html": {
    "title": "Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting for Robust 6D Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Zhou",
      "Kai Chen",
      "Linlin Xu",
      "Qi Dou",
      "Jing Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_BT2_Backward-compatible_Training_with_Basis_Transformation_ICCV_2023_paper.html": {
    "title": "BT^2: Backward-compatible Training with Basis Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Zhou",
      "Zilu Li",
      "Abhinav Shrivastava",
      "Hengshuang Zhao",
      "Antonio Torralba",
      "Taipeng Tian",
      "Ser-Nam Lim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Suris_ViperGPT_Visual_Inference_via_Python_Execution_for_Reasoning_ICCV_2023_paper.html": {
    "title": "ViperGPT: Visual Inference via Python Execution for Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "DÃ­dac SurÃ­s",
      "Sachit Menon",
      "Carl Vondrick"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ganeshan_Improving_Unsupervised_Visual_Program_Inference_with_Code_Rewriting_Families_ICCV_2023_paper.html": {
    "title": "Improving Unsupervised Visual Program Inference with Code Rewriting Families",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Ganeshan",
      "R. Kenny Jones",
      "Daniel Ritchie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Karimian_Essential_Matrix_Estimation_using_Convex_Relaxations_in_Orthogonal_Space_ICCV_2023_paper.html": {
    "title": "Essential Matrix Estimation using Convex Relaxations in Orthogonal Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arman Karimian",
      "Roberto Tron"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Concept-wise_Fine-tuning_Matters_in_Preventing_Negative_Transfer_ICCV_2023_paper.html": {
    "title": "Concept-wise Fine-tuning Matters in Preventing Negative Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunqiao Yang",
      "Long-Kai Huang",
      "Ying Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Learning_Human_Dynamics_in_Autonomous_Driving_Scenarios_ICCV_2023_paper.html": {
    "title": "Learning Human Dynamics in Autonomous Driving Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingbo Wang",
      "Ye Yuan",
      "Zhengyi Luo",
      "Kevin Xie",
      "Dahua Lin",
      "Umar Iqbal",
      "Sanja Fidler",
      "Sameh Khamis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Niu_Fine-grained_Visible_Watermark_Removal_ICCV_2023_paper.html": {
    "title": "Fine-grained Visible Watermark Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Niu",
      "Xing Zhao",
      "Bo Zhang",
      "Liqing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ji_DDP_Diffusion_Model_for_Dense_Visual_Prediction_ICCV_2023_paper.html": {
    "title": "DDP: Diffusion Model for Dense Visual Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanfeng Ji",
      "Zhe Chen",
      "Enze Xie",
      "Lanqing Hong",
      "Xihui Liu",
      "Zhaoqiang Liu",
      "Tong Lu",
      "Zhenguo Li",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Song_Semantics-Consistent_Feature_Search_for_Self-Supervised_Visual_Representation_Learning_ICCV_2023_paper.html": {
    "title": "Semantics-Consistent Feature Search for Self-Supervised Visual Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiyou Song",
      "Shan Zhang",
      "Zimeng Luo",
      "Tong Wang",
      "Jin Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_GridMM_Grid_Memory_Map_for_Vision-and-Language_Navigation_ICCV_2023_paper.html": {
    "title": "GridMM: Grid Memory Map for Vision-and-Language Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Wang",
      "Xiangyang Li",
      "Jiahao Yang",
      "Yeqi Liu",
      "Shuqiang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Schmidt_Probabilistic_Modeling_of_Inter-_and_Intra-observer_Variability_in_Medical_Image_ICCV_2023_paper.html": {
    "title": "Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arne Schmidt",
      "Pablo Morales-Ãlvarez",
      "Rafael Molina"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_LAC_-_Latent_Action_Composition_for_Skeleton-based_Action_Segmentation_ICCV_2023_paper.html": {
    "title": "LAC - Latent Action Composition for Skeleton-based Action Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Yang",
      "Yaohui Wang",
      "Antitza Dantcheva",
      "Quan Kong",
      "Lorenzo Garattoni",
      "Gianpiero Francesca",
      "Francois Bremond"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Learning_Vision-and-Language_Navigation_from_YouTube_Videos_ICCV_2023_paper.html": {
    "title": "Learning Vision-and-Language Navigation from YouTube Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunyang Lin",
      "Peihao Chen",
      "Diwei Huang",
      "Thomas H. Li",
      "Mingkui Tan",
      "Chuang Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Song_Total-Recon_Deformable_Scene_Reconstruction_for_Embodied_View_Synthesis_ICCV_2023_paper.html": {
    "title": "Total-Recon: Deformable Scene Reconstruction for Embodied View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chonghyuk Song",
      "Gengshan Yang",
      "Kangle Deng",
      "Jun-Yan Zhu",
      "Deva Ramanan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tao_AdaNIC_Towards_Practical_Neural_Image_Compression_via_Dynamic_Transform_Routing_ICCV_2023_paper.html": {
    "title": "AdaNIC: Towards Practical Neural Image Compression via Dynamic Transform Routing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lvfang Tao",
      "Wei Gao",
      "Ge Li",
      "Chenhao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bao_Uncertainty-aware_State_Space_Transformer_for_Egocentric_3D_Hand_Trajectory_Forecasting_ICCV_2023_paper.html": {
    "title": "Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Bao",
      "Lele Chen",
      "Libing Zeng",
      "Zhong Li",
      "Yi Xu",
      "Junsong Yuan",
      "Yu Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Patel_Pretrained_Language_Models_as_Visual_Planners_for_Human_Assistance_ICCV_2023_paper.html": {
    "title": "Pretrained Language Models as Visual Planners for Human Assistance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhruvesh Patel",
      "Hamid Eghbalzadeh",
      "Nitin Kamra",
      "Michael Louis Iuzzolino",
      "Unnat Jain",
      "Ruta Desai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Prokudin_Dynamic_Point_Fields_ICCV_2023_paper.html": {
    "title": "Dynamic Point Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergey Prokudin",
      "Qianli Ma",
      "Maxime Raafat",
      "Julien Valentin",
      "Siyu Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Djilali_Lip2Vec_Efficient_and_Robust_Visual_Speech_Recognition_via_Latent-to-Latent_Visual_ICCV_2023_paper.html": {
    "title": "Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasser Abdelaziz Dahou Djilali",
      "Sanath Narayan",
      "Haithem Boussaid",
      "Ebtessam Almazrouei",
      "Merouane Debbah"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pan_Privacy_Preserving_Localization_via_Coordinate_Permutations_ICCV_2023_paper.html": {
    "title": "Privacy Preserving Localization via Coordinate Permutations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linfei Pan",
      "Johannes L. SchÃ¶nberger",
      "Viktor Larsson",
      "Marc Pollefeys"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Random_Boxes_Are_Open-world_Object_Detectors_ICCV_2023_paper.html": {
    "title": "Random Boxes Are Open-world Object Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanghao Wang",
      "Zhongqi Yue",
      "Xian-Sheng Hua",
      "Hanwang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cai_DiffDreamer_Towards_Consistent_Unsupervised_Single-view_Scene_Extrapolation_with_Conditional_Diffusion_ICCV_2023_paper.html": {
    "title": "DiffDreamer: Towards Consistent Unsupervised Single-view Scene Extrapolation with Conditional Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengqu Cai",
      "Eric Ryan Chan",
      "Songyou Peng",
      "Mohamad Shahbazi",
      "Anton Obukhov",
      "Luc Van Gool",
      "Gordon Wetzstein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tse_Spectral_Graphormer_Spectral_Graph-Based_Transformer_for_Egocentric_Two-Hand_Reconstruction_using_ICCV_2023_paper.html": {
    "title": "Spectral Graphormer: Spectral Graph-Based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tze Ho Elden Tse",
      "Franziska Mueller",
      "Zhengyang Shen",
      "Danhang Tang",
      "Thabo Beeler",
      "Mingsong Dou",
      "Yinda Zhang",
      "Sasa Petrovic",
      "Hyung Jin Chang",
      "Jonathan Taylor",
      "Bardia Doosti"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_SMMix_Self-Motivated_Image_Mixing_for_Vision_Transformers_ICCV_2023_paper.html": {
    "title": "SMMix: Self-Motivated Image Mixing for Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengzhao Chen",
      "Mingbao Lin",
      "Zhihang Lin",
      "Yuxin Zhang",
      "Fei Chao",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Enhancing_Adversarial_Robustness_in_Low-Label_Regime_via_Adaptively_Weighted_Regularization_ICCV_2023_paper.html": {
    "title": "Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyoon Yang",
      "Insung Kong",
      "Yongdai Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Recovering_a_Molecules_3D_Dynamics_from_Liquid-phase_Electron_Microscopy_Movies_ICCV_2023_paper.html": {
    "title": "Recovering a Molecule's 3D Dynamics from Liquid-phase Electron Microscopy Movies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enze Ye",
      "Yuhang Wang",
      "Hong Zhang",
      "Yiqin Gao",
      "Huan Wang",
      "He Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Reconciling_Object-Level_and_Global-Level_Objectives_for_Long-Tail_Detection_ICCV_2023_paper.html": {
    "title": "Reconciling Object-Level and Global-Level Objectives for Long-Tail Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoyu Zhang",
      "Chen Chen",
      "Silong Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shvetsova_In-Style_Bridging_Text_and_Uncurated_Videos_with_Style_Transfer_for_ICCV_2023_paper.html": {
    "title": "In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina Shvetsova",
      "Anna Kukleva",
      "Bernt Schiele",
      "Hilde Kuehne"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kaneko_MIMO-NeRF_Fast_Neural_Rendering_with_Multi-input_Multi-output_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "MIMO-NeRF: Fast Neural Rendering with Multi-input Multi-output Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takuhiro Kaneko"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Instance_Neural_Radiance_Field_ICCV_2023_paper.html": {
    "title": "Instance Neural Radiance Field",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Liu",
      "Benran Hu",
      "Junkai Huang",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_One-bit_Flip_is_All_You_Need_When_Bit-flip_Attack_Meets_ICCV_2023_paper.html": {
    "title": "One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianshuo Dong",
      "Han Qiu",
      "Yiming Li",
      "Tianwei Zhang",
      "Yuanjie Li",
      "Zeqi Lai",
      "Chao Zhang",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Aberdam_CLIPTER_Looking_at_the_Bigger_Picture_in_Scene_Text_Recognition_ICCV_2023_paper.html": {
    "title": "CLIPTER: Looking at the Bigger Picture in Scene Text Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aviad Aberdam",
      "David Bensaid",
      "Alona Golts",
      "Roy Ganz",
      "Oren Nuriel",
      "Royee Tichauer",
      "Shai Mazor",
      "Ron Litman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Revisiting_Scene_Text_Recognition_A_Data_Perspective_ICCV_2023_paper.html": {
    "title": "Revisiting Scene Text Recognition: A Data Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Jiang",
      "Jiapeng Wang",
      "Dezhi Peng",
      "Chongyu Liu",
      "Lianwen Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Improving_CLIP_Fine-tuning_Performance_ICCV_2023_paper.html": {
    "title": "Improving CLIP Fine-tuning Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Wei",
      "Han Hu",
      "Zhenda Xie",
      "Ze Liu",
      "Zheng Zhang",
      "Yue Cao",
      "Jianmin Bao",
      "Dong Chen",
      "Baining Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jeong_The_Power_of_Sound_TPoS_Audio_Reactive_Video_Generation_with_ICCV_2023_paper.html": {
    "title": "The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujin Jeong",
      "Wonjeong Ryoo",
      "Seunghyun Lee",
      "Dabin Seo",
      "Wonmin Byeon",
      "Sangpil Kim",
      "Jinkyu Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wan_SOCS_Semantically-Aware_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_ICCV_2023_paper.html": {
    "title": "SOCS: Semantically-Aware Object Coordinate Space for Category-Level 6D Object Pose Estimation under Large Shape Variations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyan Wan",
      "Yifei Shi",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Deng_NeRF-LOAM_Neural_Implicit_Representation_for_Large-Scale_Incremental_LiDAR_Odometry_and_ICCV_2023_paper.html": {
    "title": "NeRF-LOAM: Neural Implicit Representation for Large-Scale Incremental LiDAR Odometry and Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyuan Deng",
      "Qi Wu",
      "Xieyuanli Chen",
      "Songpengcheng Xia",
      "Zhen Sun",
      "Guoqing Liu",
      "Wenxian Yu",
      "Ling Pei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Svitov_DINAR_Diffusion_Inpainting_of_Neural_Textures_for_One-Shot_Human_Avatars_ICCV_2023_paper.html": {
    "title": "DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Svitov",
      "Dmitrii Gudkov",
      "Renat Bashirov",
      "Victor Lempitsky"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_DPM-OT_A_New_Diffusion_Probabilistic_Model_Based_on_Optimal_Transport_ICCV_2023_paper.html": {
    "title": "DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zezeng Li",
      "Shenghao Li",
      "Zhanpeng Wang",
      "Na Lei",
      "Zhongxuan Luo",
      "David Xianfeng Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tang_ElasticViT_Conflict-aware_Supernet_Training_for_Deploying_Fast_Vision_Transformer_on_ICCV_2023_paper.html": {
    "title": "ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Tang",
      "Li Lyna Zhang",
      "Huiqiang Jiang",
      "Jiahang Xu",
      "Ting Cao",
      "Quanlu Zhang",
      "Yuqing Yang",
      "Zhi Wang",
      "Mao Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Schulter_OmniLabel_A_Challenging_Benchmark_for_Language-Based_Object_Detection_ICCV_2023_paper.html": {
    "title": "OmniLabel: A Challenging Benchmark for Language-Based Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Schulter",
      "Vijay Kumar B G",
      "Yumin Suh",
      "Konstantinos M. Dafnis",
      "Zhixing Zhang",
      "Shiyu Zhao",
      "Dimitris Metaxas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kang_Noise-Aware_Learning_from_Web-Crawled_Image-Text_Data_for_Image_Captioning_ICCV_2023_paper.html": {
    "title": "Noise-Aware Learning from Web-Crawled Image-Text Data for Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wooyoung Kang",
      "Jonghwan Mun",
      "Sungjun Lee",
      "Byungseok Roh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Trivigno_DivideClassify_Fine-Grained_Classification_for_City-Wide_Visual_Geo-Localization_ICCV_2023_paper.html": {
    "title": "Divide&Classify: Fine-Grained Classification for City-Wide Visual Geo-Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriele Trivigno",
      "Gabriele Berton",
      "Juan Aragon",
      "Barbara Caputo",
      "Carlo Masone"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_3D_Semantic_Subspace_Traverser_Empowering_3D_Generative_Model_with_Shape_ICCV_2023_paper.html": {
    "title": "3D Semantic Subspace Traverser: Empowering 3D Generative Model with Shape Editing Capability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruowei Wang",
      "Yu Liu",
      "Pei Su",
      "Jianwei Zhang",
      "Qijun Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yao_Inherent_Redundancy_in_Spiking_Neural_Networks_ICCV_2023_paper.html": {
    "title": "Inherent Redundancy in Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Man Yao",
      "Jiakui Hu",
      "Guangshe Zhao",
      "Yaoyuan Wang",
      "Ziyang Zhang",
      "Bo Xu",
      "Guoqi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hollein_Text2Room_Extracting_Textured_3D_Meshes_from_2D_Text-to-Image_Models_ICCV_2023_paper.html": {
    "title": "Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas HÃ¶llein",
      "Ang Cao",
      "Andrew Owens",
      "Justin Johnson",
      "Matthias NieÃner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hong_On_the_Robustness_of_Normalizing_Flows_for_Inverse_Problems_in_ICCV_2023_paper.html": {
    "title": "On the Robustness of Normalizing Flows for Inverse Problems in Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongmin Hong",
      "Inbum Park",
      "Se Young Chun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fang_FastRecon_Few-shot_Industrial_Anomaly_Detection_via_Fast_Feature_Reconstruction_ICCV_2023_paper.html": {
    "title": "FastRecon: Few-shot Industrial Anomaly Detection via Fast Feature Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Fang",
      "Xiaoyang Wang",
      "Haocheng Li",
      "Jiejie Liu",
      "Qiugui Hu",
      "Jimin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cho_Local_or_Global_Selective_Knowledge_Assimilation_for_Federated_Learning_with_ICCV_2023_paper.html": {
    "title": "Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yae Jee Cho",
      "Gauri Joshi",
      "Dimitrios Dimitriadis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_DistillBEV_Boosting_Multi-Camera_3D_Object_Detection_with_Cross-Modal_Knowledge_Distillation_ICCV_2023_paper.html": {
    "title": "DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Wang",
      "Dingwen Li",
      "Chenxu Luo",
      "Cihang Xie",
      "Xiaodong Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Delmas_PoseFix_Correcting_3D_Human_Poses_with_Natural_Language_ICCV_2023_paper.html": {
    "title": "PoseFix: Correcting 3D Human Poses with Natural Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ginger Delmas",
      "Philippe Weinzaepfel",
      "Francesc Moreno-Noguer",
      "GrÃ©gory Rogez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Doersch_TAPIR_Tracking_Any_Point_with_Per-Frame_Initialization_and_Temporal_Refinement_ICCV_2023_paper.html": {
    "title": "TAPIR: Tracking Any Point with Per-Frame Initialization and Temporal Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carl Doersch",
      "Yi Yang",
      "Mel Vecerik",
      "Dilara Gokay",
      "Ankush Gupta",
      "Yusuf Aytar",
      "Joao Carreira",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tang_SwinLSTM_Improving_Spatiotemporal_Prediction_Accuracy_using_Swin_Transformer_and_LSTM_ICCV_2023_paper.html": {
    "title": "SwinLSTM: Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Tang",
      "Chuang Li",
      "Pu Zhang",
      "RongNian Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bhowmik_Detecting_Objects_with_Context-Likelihood_Graphs_and_Graph_Refinement_ICCV_2023_paper.html": {
    "title": "Detecting Objects with Context-Likelihood Graphs and Graph Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aritra Bhowmik",
      "Yu Wang",
      "Nora Baka",
      "Martin R. Oswald",
      "Cees G. M. Snoek"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Coarse-to-Fine_Amodal_Segmentation_with_Shape_Prior_ICCV_2023_paper.html": {
    "title": "Coarse-to-Fine Amodal Segmentation with Shape Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianxiong Gao",
      "Xuelin Qian",
      "Yikai Wang",
      "Tianjun Xiao",
      "Tong He",
      "Zheng Zhang",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Baranchuk_DEDRIFT_Robust_Similarity_Search_under_Content_Drift_ICCV_2023_paper.html": {
    "title": "DEDRIFT: Robust Similarity Search under Content Drift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitry Baranchuk",
      "Matthijs Douze",
      "Yash Upadhyay",
      "I. Zeki Yalniz"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Learning_Pseudo-Relations_for_Cross-domain_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Learning Pseudo-Relations for Cross-domain Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Zhao",
      "Shuang Wang",
      "Qi Zang",
      "Dou Quan",
      "Xiutiao Ye",
      "Rui Yang",
      "Licheng Jiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chowdhury_AdVerb_Visually_Guided_Audio_Dereverberation_ICCV_2023_paper.html": {
    "title": "AdVerb: Visually Guided Audio Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjoy Chowdhury",
      "Sreyan Ghosh",
      "Subhrajyoti Dasgupta",
      "Anton Ratnarajah",
      "Utkarsh Tyagi",
      "Dinesh Manocha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ibrahimi_Audio-Enhanced_Text-to-Video_Retrieval_using_Text-Conditioned_Feature_Alignment_ICCV_2023_paper.html": {
    "title": "Audio-Enhanced Text-to-Video Retrieval using Text-Conditioned Feature Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Ibrahimi",
      "Xiaohang Sun",
      "Pichao Wang",
      "Amanmeet Garg",
      "Ashutosh Sanan",
      "Mohamed Omar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Open-vocabulary_Object_Segmentation_with_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Open-vocabulary Object Segmentation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Li",
      "Qinye Zhou",
      "Xiaoyun Zhang",
      "Ya Zhang",
      "Yanfeng Wang",
      "Weidi Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Human-centric_Scene_Understanding_for_3D_Large-scale_Scenarios_ICCV_2023_paper.html": {
    "title": "Human-centric Scene Understanding for 3D Large-scale Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiteng Xu",
      "Peishan Cong",
      "Yichen Yao",
      "Runnan Chen",
      "Yuenan Hou",
      "Xinge Zhu",
      "Xuming He",
      "Jingyi Yu",
      "Yuexin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Barraco_With_a_Little_Help_from_Your_Own_Past_Prototypical_Memory_ICCV_2023_paper.html": {
    "title": "With a Little Help from Your Own Past: Prototypical Memory Networks for Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuele Barraco",
      "Sara Sarto",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_SimMatchV2_Semi-Supervised_Learning_with_Graph_Consistency_ICCV_2023_paper.html": {
    "title": "SimMatchV2: Semi-Supervised Learning with Graph Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingkai Zheng",
      "Shan You",
      "Lang Huang",
      "Chen Luo",
      "Fei Wang",
      "Chen Qian",
      "Chang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ren_Reinforced_Disentanglement_for_Face_Swapping_without_Skip_Connection_ICCV_2023_paper.html": {
    "title": "Reinforced Disentanglement for Face Swapping without Skip Connection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohang Ren",
      "Xingyu Chen",
      "Pengfei Yao",
      "Heung-Yeung Shum",
      "Baoyuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/van_der_Klis_PDiscoNet_Semantically_consistent_part_discovery_for_fine-grained_recognition_ICCV_2023_paper.html": {
    "title": "PDiscoNet: Semantically consistent part discovery for fine-grained recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert van der Klis",
      "Stephan Alaniz",
      "Massimiliano Mancini",
      "Cassio F. Dantas",
      "Dino Ienco",
      "Zeynep Akata",
      "Diego Marcos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mi_Privacy-Preserving_Face_Recognition_Using_Random_Frequency_Components_ICCV_2023_paper.html": {
    "title": "Privacy-Preserving Face Recognition Using Random Frequency Components",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxi Mi",
      "Yuge Huang",
      "Jiazhen Ji",
      "Minyi Zhao",
      "Jiaxiang Wu",
      "Xingkun Xu",
      "Shouhong Ding",
      "Shuigeng Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bhattacharjee_Vision_Transformer_Adapters_for_Generalizable_Multitask_Learning_ICCV_2023_paper.html": {
    "title": "Vision Transformer Adapters for Generalizable Multitask Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deblina Bhattacharjee",
      "Sabine SÃ¼sstrunk",
      "Mathieu Salzmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Maho_How_to_Choose_your_Best_Allies_for_a_Transferable_Attack_ICCV_2023_paper.html": {
    "title": "How to Choose your Best Allies for a Transferable Attack?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibault Maho",
      "Seyed-Mohsen Moosavi-Dezfooli",
      "Teddy Furon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Feng_CVRecon_Rethinking_3D_Geometric_Feature_Learning_For_Neural_Reconstruction_ICCV_2023_paper.html": {
    "title": "CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyue Feng",
      "Liang Yang",
      "Pengsheng Guo",
      "Bing Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Akiva_Self-Supervised_Object_Detection_from_Egocentric_Videos_ICCV_2023_paper.html": {
    "title": "Self-Supervised Object Detection from Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peri Akiva",
      "Jing Huang",
      "Kevin J Liang",
      "Rama Kovvuri",
      "Xingyu Chen",
      "Matt Feiszli",
      "Kristin Dana",
      "Tal Hassner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Raychaudhuri_Prior-guided_Source-free_Domain_Adaptation_for_Human_Pose_Estimation_ICCV_2023_paper.html": {
    "title": "Prior-guided Source-free Domain Adaptation for Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dripta S. Raychaudhuri",
      "Calvin-Khang Ta",
      "Arindam Dutta",
      "Rohit Lal",
      "Amit K. Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_ClothesNet_An_Information-Rich_3D_Garment_Model_Repository_with_Simulated_Clothes_ICCV_2023_paper.html": {
    "title": "ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingyang Zhou",
      "Haoyu Zhou",
      "Tianhai Liang",
      "Qiaojun Yu",
      "Siheng Zhao",
      "Yuwei Zeng",
      "Jun Lv",
      "Siyuan Luo",
      "Qiancai Wang",
      "Xinyuan Yu",
      "Haonan Chen",
      "Cewu Lu",
      "Lin Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Auxiliary_Tasks_Benefit_3D_Skeleton-based_Human_Motion_Prediction_ICCV_2023_paper.html": {
    "title": "Auxiliary Tasks Benefit 3D Skeleton-based Human Motion Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxin Xu",
      "Robby T. Tan",
      "Yuhong Tan",
      "Siheng Chen",
      "Xinchao Wang",
      "Yanfeng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lyu_Measuring_Asymmetric_Gradient_Discrepancy_in_Parallel_Continual_Learning_ICCV_2023_paper.html": {
    "title": "Measuring Asymmetric Gradient Discrepancy in Parallel Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Lyu",
      "Qing Sun",
      "Fanhua Shang",
      "Liang Wan",
      "Wei Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ki_StyleLipSync_Style-based_Personalized_Lip-sync_Video_Generation_ICCV_2023_paper.html": {
    "title": "StyleLipSync: Style-based Personalized Lip-sync Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taekyung Ki",
      "Dongchan Min"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Cross_Contrasting_Feature_Perturbation_for_Domain_Generalization_ICCV_2023_paper.html": {
    "title": "Cross Contrasting Feature Perturbation for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenming Li",
      "Daoan Zhang",
      "Wenjian Huang",
      "Jianguo Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jin_DiffusionRet_Generative_Text-Video_Retrieval_with_Diffusion_Model_ICCV_2023_paper.html": {
    "title": "DiffusionRet: Generative Text-Video Retrieval with Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Jin",
      "Hao Li",
      "Zesen Cheng",
      "Kehan Li",
      "Xiangyang Ji",
      "Chang Liu",
      "Li Yuan",
      "Jie Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Robert_Efficient_3D_Semantic_Segmentation_with_Superpoint_Transformer_ICCV_2023_paper.html": {
    "title": "Efficient 3D Semantic Segmentation with Superpoint Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damien Robert",
      "Hugo Raguet",
      "Loic Landrieu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Suzuki_Adversarial_Finetuning_with_Latent_Representation_Constraint_to_Mitigate_Accuracy-Robustness_Tradeoff_ICCV_2023_paper.html": {
    "title": "Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Satoshi Suzuki",
      "Shin'ya Yamaguchi",
      "Shoichiro Takeda",
      "Sekitoshi Kanai",
      "Naoki Makishima",
      "Atsushi Ando",
      "Ryo Masumura"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Erkoc_HyperDiffusion_Generating_Implicit_Neural_Fields_with_Weight-Space_Diffusion_ICCV_2023_paper.html": {
    "title": "HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziya ErkoÃ§",
      "Fangchang Ma",
      "Qi Shan",
      "Matthias NieÃner",
      "Angela Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cai_Retinexformer_One-stage_Retinex-based_Transformer_for_Low-light_Image_Enhancement_ICCV_2023_paper.html": {
    "title": "Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Cai",
      "Hao Bian",
      "Jing Lin",
      "Haoqian Wang",
      "Radu Timofte",
      "Yulun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Minimum_Latency_Deep_Online_Video_Stabilization_ICCV_2023_paper.html": {
    "title": "Minimum Latency Deep Online Video Stabilization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuofan Zhang",
      "Zhen Liu",
      "Ping Tan",
      "Bing Zeng",
      "Shuaicheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Speech2Lip_High-fidelity_Speech_to_Lip_Generation_by_Learning_from_a_ICCV_2023_paper.html": {
    "title": "Speech2Lip: High-fidelity Speech to Lip Generation by Learning from a Short Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuzhe Wu",
      "Pengfei Hu",
      "Yang Wu",
      "Xiaoyang Lyu",
      "Yan-Pei Cao",
      "Ying Shan",
      "Wenming Yang",
      "Zhongqian Sun",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_UHDNeRF_Ultra-High-Definition_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "UHDNeRF: Ultra-High-Definition Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quewei Li",
      "Feichao Li",
      "Jie Guo",
      "Yanwen Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Trager_Linear_Spaces_of_Meanings_Compositional_Structures_in_Vision-Language_Models_ICCV_2023_paper.html": {
    "title": "Linear Spaces of Meanings: Compositional Structures in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Trager",
      "Pramuditha Perera",
      "Luca Zancato",
      "Alessandro Achille",
      "Parminder Bhatia",
      "Stefano Soatto"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tu_MULLER_Multilayer_Laplacian_Resizer_for_Vision_ICCV_2023_paper.html": {
    "title": "MULLER: Multilayer Laplacian Resizer for Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengzhong Tu",
      "Peyman Milanfar",
      "Hossein Talebi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dai_X-VoE_Measuring_eXplanatory_Violation_of_Expectation_in_Physical_Events_ICCV_2023_paper.html": {
    "title": "X-VoE: Measuring eXplanatory Violation of Expectation in Physical Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Dai",
      "Linge Wang",
      "Baoxiong Jia",
      "Zeyu Zhang",
      "Song-Chun Zhu",
      "Chi Zhang",
      "Yixin Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Tracking_by_Natural_Language_Specification_with_Long_Short-term_Context_Decoupling_ICCV_2023_paper.html": {
    "title": "Tracking by Natural Language Specification with Long Short-term Context Decoupling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ding Ma",
      "Xiangqian Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_COOP_Decoupling_and_Coupling_of_Whole-Body_Grasping_Pose_Generation_ICCV_2023_paper.html": {
    "title": "COOP: Decoupling and Coupling of Whole-Body Grasping Pose Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanzhao Zheng",
      "Yunzhou Shi",
      "Yuhao Cui",
      "Zhongzhou Zhao",
      "Zhiling Luo",
      "Wei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/He_Pyramid_Dual_Domain_Injection_Network_for_Pan-sharpening_ICCV_2023_paper.html": {
    "title": "Pyramid Dual Domain Injection Network for Pan-sharpening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanhua He",
      "Keyu Yan",
      "Rui Li",
      "Chengjun Xie",
      "Jie Zhang",
      "Man Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Why_do_networks_have_inhibitorynegative_connections_ICCV_2023_paper.html": {
    "title": "Why do networks have inhibitory/negative connections?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyang Wang",
      "Mike A. Powell",
      "Ali Geisa",
      "Eric Bridgeford",
      "Carey E. Priebe",
      "Joshua T. Vogelstein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wen_Ordinal_Label_Distribution_Learning_ICCV_2023_paper.html": {
    "title": "Ordinal Label Distribution Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changsong Wen",
      "Xin Zhang",
      "Xingxu Yao",
      "Jufeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Model_Calibration_in_Dense_Classification_with_Adaptive_Label_Perturbation_ICCV_2023_paper.html": {
    "title": "Model Calibration in Dense Classification with Adaptive Label Perturbation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Liu",
      "Changkun Ye",
      "Shan Wang",
      "Ruikai Cui",
      "Jing Zhang",
      "Kaihao Zhang",
      "Nick Barnes"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Boosting_Multi-modal_Model_Performance_with_Adaptive_Gradient_Modulation_ICCV_2023_paper.html": {
    "title": "Boosting Multi-modal Model Performance with Adaptive Gradient Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong Li",
      "Xingyu Li",
      "Pengbo Hu",
      "Yinuo Lei",
      "Chunxiao Li",
      "Yi Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Quan_Semantic_Information_in_Contrastive_Learning_ICCV_2023_paper.html": {
    "title": "Semantic Information in Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjiang Quan",
      "Masahiro Hirano",
      "Yuji Yamakawa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Structure and Content-Guided Video Synthesis with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Esser",
      "Johnathan Chiu",
      "Parmida Atighehchian",
      "Jonathan Granskog",
      "Anastasis Germanidis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pakulev_NeSS-ST_Detecting_Good_and_Stable_Keypoints_with_a_Neural_Stability_ICCV_2023_paper.html": {
    "title": "NeSS-ST: Detecting Good and Stable Keypoints with a Neural Stability Score and the Shi-Tomasi detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantin Pakulev",
      "Alexander Vakhitov",
      "Gonzalo Ferrer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Thong_Beyond_Skin_Tone_A_Multidimensional_Measure_of_Apparent_Skin_Color_ICCV_2023_paper.html": {
    "title": "Beyond Skin Tone: A Multidimensional Measure of Apparent Skin Color",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Thong",
      "Przemyslaw Joniak",
      "Alice Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fahes_PODA_Prompt-driven_Zero-shot_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "PODA: Prompt-driven Zero-shot Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Fahes",
      "Tuan-Hung Vu",
      "Andrei Bursuc",
      "Patrick PÃ©rez",
      "Raoul de Charette"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Video_Action_Segmentation_via_Contextually_Refined_Temporal_Keypoints_ICCV_2023_paper.html": {
    "title": "Video Action Segmentation via Contextually Refined Temporal Keypoints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Borui Jiang",
      "Yang Jin",
      "Zhentao Tan",
      "Yadong Mu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Shatter_and_Gather_Learning_Referring_Image_Segmentation_with_Text_Supervision_ICCV_2023_paper.html": {
    "title": "Shatter and Gather: Learning Referring Image Segmentation with Text Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongwon Kim",
      "Namyup Kim",
      "Cuiling Lan",
      "Suha Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Two-in-One_Depth_Bridging_the_Gap_Between_Monocular_and_Binocular_Self-Supervised_ICCV_2023_paper.html": {
    "title": "Two-in-One Depth: Bridging the Gap Between Monocular and Binocular Self-Supervised Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengming Zhou",
      "Qiulei Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sun_SAFL-Net_Semantic-Agnostic_Feature_Learning_Network_with_Auxiliary_Plugins_for_Image_ICCV_2023_paper.html": {
    "title": "SAFL-Net: Semantic-Agnostic Feature Learning Network with Auxiliary Plugins for Image Manipulation Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Sun",
      "Haoran Jiang",
      "Danding Wang",
      "Xirong Li",
      "Juan Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Sajedi_DataDAM_Efficient_Dataset_Distillation_with_Attention_Matching_ICCV_2023_paper.html": {
    "title": "DataDAM: Efficient Dataset Distillation with Attention Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Sajedi",
      "Samir Khaki",
      "Ehsan Amjadian",
      "Lucy Z. Liu",
      "Yuri A. Lawryshyn",
      "Konstantinos N. Plataniotis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Rethinking_Pose_Estimation_in_Crowds_Overcoming_the_Detection_Information_Bottleneck_ICCV_2023_paper.html": {
    "title": "Rethinking Pose Estimation in Crowds: Overcoming the Detection Information Bottleneck and Ambiguity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mu Zhou",
      "Lucas Stoffl",
      "Mackenzie Weygandt Mathis",
      "Alexander Mathis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tanke_Social_Diffusion_Long-term_Multiple_Human_Motion_Anticipation_ICCV_2023_paper.html": {
    "title": "Social Diffusion: Long-term Multiple Human Motion Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Tanke",
      "Linguang Zhang",
      "Amy Zhao",
      "Chengcheng Tang",
      "Yujun Cai",
      "Lezi Wang",
      "Po-Chen Wu",
      "Juergen Gall",
      "Cem Keskin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Synchronize_Feature_Extracting_and_Matching_A_Single_Branch_Framework_for_ICCV_2023_paper.html": {
    "title": "Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teli Ma",
      "Mengmeng Wang",
      "Jimin Xiao",
      "Huifeng Wu",
      "Yong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Leveraging_Intrinsic_Properties_for_Non-Rigid_Garment_Alignment_ICCV_2023_paper.html": {
    "title": "Leveraging Intrinsic Properties for Non-Rigid Garment Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyou Lin",
      "Boyao Zhou",
      "Zerong Zheng",
      "Hongwen Zhang",
      "Yebin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_NeILF_Inter-Reflectable_Light_Fields_for_Geometry_and_Material_Estimation_ICCV_2023_paper.html": {
    "title": "NeILF++: Inter-Reflectable Light Fields for Geometry and Material Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyang Zhang",
      "Yao Yao",
      "Shiwei Li",
      "Jingbo Liu",
      "Tian Fang",
      "David McKinnon",
      "Yanghai Tsin",
      "Long Quan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_MAGI_Multi-Annotated_Explanation-Guided_Learning_ICCV_2023_paper.html": {
    "title": "MAGI: Multi-Annotated Explanation-Guided Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Zhang",
      "Siyi Gu",
      "Yuyang Gao",
      "Bo Pan",
      "Xiaofeng Yang",
      "Liang Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Adaptive_Positional_Encoding_for_Bundle-Adjusting_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "Adaptive Positional Encoding for Bundle-Adjusting Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Gao",
      "Weichen Dai",
      "Yu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liang_Inducing_Neural_Collapse_to_a_Fixed_Hierarchy-Aware_Frame_for_Reducing_ICCV_2023_paper.html": {
    "title": "Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Liang",
      "Jim Davis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_PlanarTrack_A_Large-scale_Challenging_Benchmark_for_Planar_Object_Tracking_ICCV_2023_paper.html": {
    "title": "PlanarTrack: A Large-scale Challenging Benchmark for Planar Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinran Liu",
      "Xiaoqiong Liu",
      "Ziruo Yi",
      "Xin Zhou",
      "Thanh Le",
      "Libo Zhang",
      "Yan Huang",
      "Qing Yang",
      "Heng Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Factorized_Inverse_Path_Tracing_for_Efficient_and_Accurate_Material-Lighting_Estimation_ICCV_2023_paper.html": {
    "title": "Factorized Inverse Path Tracing for Efficient and Accurate Material-Lighting Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liwen Wu",
      "Rui Zhu",
      "Mustafa B. Yaldiz",
      "Yinhao Zhu",
      "Hong Cai",
      "Janarbek Matai",
      "Fatih Porikli",
      "Tzu-Mao Li",
      "Manmohan Chandraker",
      "Ravi Ramamoorthi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cui_P2C_Self-Supervised_Point_Cloud_Completion_from_Single_Partial_Clouds_ICCV_2023_paper.html": {
    "title": "P2C: Self-Supervised Point Cloud Completion from Single Partial Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruikai Cui",
      "Shi Qiu",
      "Saeed Anwar",
      "Jiawei Liu",
      "Chaoyue Xing",
      "Jing Zhang",
      "Nick Barnes"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Overwriting_Pretrained_Bias_with_Finetuning_Data_ICCV_2023_paper.html": {
    "title": "Overwriting Pretrained Bias with Finetuning Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angelina Wang",
      "Olga Russakovsky"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Van_Le_Anti-DreamBooth_Protecting_Users_from_Personalized_Text-to-image_Synthesis_ICCV_2023_paper.html": {
    "title": "Anti-DreamBooth: Protecting Users from Personalized Text-to-image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thanh Van Le",
      "Hao Phung",
      "Thuan Hoang Nguyen",
      "Quan Dao",
      "Ngoc N. Tran",
      "Anh Tran"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_Contrastive_Continuity_on_Augmentation_Stability_Rehearsal_for_Continual_Self-Supervised_Learning_ICCV_2023_paper.html": {
    "title": "Contrastive Continuity on Augmentation Stability Rehearsal for Continual Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang Cheng",
      "Haitao Wen",
      "Xiaoliang Zhang",
      "Heqian Qiu",
      "Lanxiao Wang",
      "Hongliang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Treating_Pseudo-labels_Generation_as_Image_Matting_for_Weakly_Supervised_Semantic_ICCV_2023_paper.html": {
    "title": "Treating Pseudo-labels Generation as Image Matting for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changwei Wang",
      "Rongtao Xu",
      "Shibiao Xu",
      "Weiliang Meng",
      "Xiaopeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Structural_Alignment_for_Network_Pruning_through_Partial_Regularization_ICCV_2023_paper.html": {
    "title": "Structural Alignment for Network Pruning through Partial Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangqian Gao",
      "Zeyu Zhang",
      "Yanfu Zhang",
      "Feihu Huang",
      "Heng Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Learning_Long-Range_Information_with_Dual-Scale_Transformers_for_Indoor_Scene_Completion_ICCV_2023_paper.html": {
    "title": "Learning Long-Range Information with Dual-Scale Transformers for Indoor Scene Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Wang",
      "Fei Luo",
      "Xiaoxiao Long",
      "Wenxiao Zhang",
      "Chunxia Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Belder_A_Game_of_Bundle_Adjustment_-_Learning_Efficient_Convergence_ICCV_2023_paper.html": {
    "title": "A Game of Bundle Adjustment - Learning Efficient Convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Belder",
      "Refael Vivanti",
      "Ayellet Tal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Learning_Correction_Filter_via_Degradation-Adaptive_Regression_for_Blind_Single_Image_ICCV_2023_paper.html": {
    "title": "Learning Correction Filter via Degradation-Adaptive Regression for Blind Single Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyang Zhou",
      "Xiaobin Zhu",
      "Jianqing Zhu",
      "Zheng Han",
      "Shi-Xue Zhang",
      "Jingyan Qin",
      "Xu-Cheng Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jain_UMFuse_Unified_Multi_View_Fusion_for_Human_Editing_Applications_ICCV_2023_paper.html": {
    "title": "UMFuse: Unified Multi View Fusion for Human Editing Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishabh Jain",
      "Mayur Hemani",
      "Duygu Ceylan",
      "Krishna Kumar Singh",
      "Jingwan Lu",
      "Mausoom Sarkar",
      "Balaji Krishnamurthy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Moreau_CROSSFIRE_Camera_Relocalization_On_Self-Supervised_Features_from_an_Implicit_Representation_ICCV_2023_paper.html": {
    "title": "CROSSFIRE: Camera Relocalization On Self-Supervised Features from an Implicit Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arthur Moreau",
      "Nathan Piasco",
      "Moussab Bennehar",
      "Dzmitry Tsishkou",
      "Bogdan Stanciulescu",
      "Arnaud de La Fortelle"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Schwartz_Discriminative_Class_Tokens_for_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Discriminative Class Tokens for Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Idan Schwartz",
      "VÃ©steinn SnÃ¦bjarnarson",
      "Hila Chefer",
      "Serge Belongie",
      "Lior Wolf",
      "Sagie Benaim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Athanasiou_SINC_Spatial_Composition_of_3D_Human_Motions_for_Simultaneous_Action_ICCV_2023_paper.html": {
    "title": "SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikos Athanasiou",
      "Mathis Petrovich",
      "Michael J. Black",
      "GÃ¼l Varol"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Choi_ORC_Network_Group-based_Knowledge_Distillation_using_Online_Role_Change_ICCV_2023_paper.html": {
    "title": "ORC: Network Group-based Knowledge Distillation using Online Role Change",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyong Choi",
      "Hyeon Cho",
      "Seokhwa Cheung",
      "Wonjun Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Georgescu_Audiovisual_Masked_Autoencoders_ICCV_2023_paper.html": {
    "title": "Audiovisual Masked Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mariana-Iuliana Georgescu",
      "Eduardo Fonseca",
      "Radu Tudor Ionescu",
      "Mario Lucic",
      "Cordelia Schmid",
      "Anurag Arnab"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_MV-DeepSDF_Implicit_Modeling_with_Multi-Sweep_Point_Clouds_for_3D_Vehicle_ICCV_2023_paper.html": {
    "title": "MV-DeepSDF: Implicit Modeling with Multi-Sweep Point Clouds for 3D Vehicle Reconstruction in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibo Liu",
      "Kelly Zhu",
      "Guile Wu",
      "Yuan Ren",
      "Bingbing Liu",
      "Yang Liu",
      "Jinjun Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_CHORD_Category-level_Hand-held_Object_Reconstruction_via_Shape_Deformation_ICCV_2023_paper.html": {
    "title": "CHORD: Category-level Hand-held Object Reconstruction via Shape Deformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kailin Li",
      "Lixin Yang",
      "Haoyu Zhen",
      "Zenan Lin",
      "Xinyu Zhan",
      "Licheng Zhong",
      "Jian Xu",
      "Kejian Wu",
      "Cewu Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nandan_Unmasking_Anomalies_in_Road-Scene_Segmentation_ICCV_2023_paper.html": {
    "title": "Unmasking Anomalies in Road-Scene Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shyam Nandan Rai",
      "Fabio Cermelli",
      "Dario Fontanel",
      "Carlo Masone",
      "Barbara Caputo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_DomainDrop_Suppressing_Domain-Sensitive_Channels_for_Domain_Generalization_ICCV_2023_paper.html": {
    "title": "DomainDrop: Suppressing Domain-Sensitive Channels for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintao Guo",
      "Lei Qi",
      "Yinghuan Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Towards_Universal_LiDAR-Based_3D_Object_Detection_by_Multi-Domain_Knowledge_Transfer_ICCV_2023_paper.html": {
    "title": "Towards Universal LiDAR-Based 3D Object Detection by Multi-Domain Knowledge Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guile Wu",
      "Tongtong Cao",
      "Bingbing Liu",
      "Xingxin Chen",
      "Yuan Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_StyleInV_A_Temporal_Style_Modulated_Inversion_Network_for_Unconditional_Video_ICCV_2023_paper.html": {
    "title": "StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Wang",
      "Liming Jiang",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Self-Calibrated_Cross_Attention_Network_for_Few-Shot_Segmentation_ICCV_2023_paper.html": {
    "title": "Self-Calibrated Cross Attention Network for Few-Shot Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianxiong Xu",
      "Wenting Zhao",
      "Guosheng Lin",
      "Cheng Long"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Anatomical_Invariance_Modeling_and_Semantic_Alignment_for_Self-supervised_Learning_in_ICCV_2023_paper.html": {
    "title": "Anatomical Invariance Modeling and Semantic Alignment for Self-supervised Learning in 3D Medical Image Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yankai Jiang",
      "Mingze Sun",
      "Heng Guo",
      "Xiaoyu Bai",
      "Ke Yan",
      "Le Lu",
      "Minfeng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Towards_High-Fidelity_Text-Guided_3D_Face_Generation_and_Manipulation_Using_only_ICCV_2023_paper.html": {
    "title": "Towards High-Fidelity Text-Guided 3D Face Generation and Manipulation Using only Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cuican Yu",
      "Guansong Lu",
      "Yihan Zeng",
      "Jian Sun",
      "Xiaodan Liang",
      "Huibin Li",
      "Zongben Xu",
      "Songcen Xu",
      "Wei Zhang",
      "Hang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ahmed_SSDA_Secure_Source-Free_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "SSDA: Secure Source-Free Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sabbir Ahmed",
      "Abdullah Al Arafat",
      "Mamshad Nayeem Rizve",
      "Rahim Hossain",
      "Zhishan Guo",
      "Adnan Siraj Rakin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kotar_ENTL_Embodied_Navigation_Trajectory_Learner_ICCV_2023_paper.html": {
    "title": "ENTL: Embodied Navigation Trajectory Learner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Klemen Kotar",
      "Aaron Walsman",
      "Roozbeh Mottaghi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_AGG-Net_Attention_Guided_Gated-Convolutional_Network_for_Depth_Image_Completion_ICCV_2023_paper.html": {
    "title": "AGG-Net: Attention Guided Gated-Convolutional Network for Depth Image Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyue Chen",
      "Tingxuan Huang",
      "Zhimin Song",
      "Shizhuo Deng",
      "Tong Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shen_Learning_Global-aware_Kernel_for_Image_Harmonization_ICCV_2023_paper.html": {
    "title": "Learning Global-aware Kernel for Image Harmonization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xintian Shen",
      "Jiangning Zhang",
      "Jun Chen",
      "Shipeng Bai",
      "Yue Han",
      "Yabiao Wang",
      "Chengjie Wang",
      "Yong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Real-Time_Neural_Rasterization_for_Large_Scenes_ICCV_2023_paper.html": {
    "title": "Real-Time Neural Rasterization for Large Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeffrey Yunfan Liu",
      "Yun Chen",
      "Ze Yang",
      "Jingkang Wang",
      "Sivabalan Manivasagam",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_ESTextSpotter_Towards_Better_Scene_Text_Spotting_with_Explicit_Synergy_in_ICCV_2023_paper.html": {
    "title": "ESTextSpotter: Towards Better Scene Text Spotting with Explicit Synergy in Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingxin Huang",
      "Jiaxin Zhang",
      "Dezhi Peng",
      "Hao Lu",
      "Can Huang",
      "Yuliang Liu",
      "Xiang Bai",
      "Lianwen Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ren_UGC_Unified_GAN_Compression_for_Efficient_Image-to-Image_Translation_ICCV_2023_paper.html": {
    "title": "UGC: Unified GAN Compression for Efficient Image-to-Image Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxi Ren",
      "Jie Wu",
      "Peng Zhang",
      "Manlin Zhang",
      "Xuefeng Xiao",
      "Qian He",
      "Rui Wang",
      "Min Zheng",
      "Xin Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Efficient_View_Synthesis_with_Neural_Radiance_Distribution_Field_ICCV_2023_paper.html": {
    "title": "Efficient View Synthesis with Neural Radiance Distribution Field",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushuang Wu",
      "Xiao Li",
      "Jinglu Wang",
      "Xiaoguang Han",
      "Shuguang Cui",
      "Yan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_MixSpeech_Cross-Modality_Self-Learning_with_Audio-Visual_Stream_Mixup_for_Visual_Speech_ICCV_2023_paper.html": {
    "title": "MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual Speech Translation and Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xize Cheng",
      "Tao Jin",
      "Rongjie Huang",
      "Linjun Li",
      "Wang Lin",
      "Zehan Wang",
      "Ye Wang",
      "Huadai Liu",
      "Aoxiong Yin",
      "Zhou Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mankovich_Chordal_Averaging_on_Flag_Manifolds_and_Its_Applications_ICCV_2023_paper.html": {
    "title": "Chordal Averaging on Flag Manifolds and Its Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Mankovich",
      "Tolga Birdal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bu_Towards_Building_More_Robust_Models_with_Frequency_Bias_ICCV_2023_paper.html": {
    "title": "Towards Building More Robust Models with Frequency Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingwen Bu",
      "Dong Huang",
      "Heming Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_SparseBEV_High-Performance_Sparse_3D_Object_Detection_from_Multi-Camera_Videos_ICCV_2023_paper.html": {
    "title": "SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haisong Liu",
      "Yao Teng",
      "Tao Lu",
      "Haiguang Wang",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qu_Boosting_Whole_Slide_Image_Classification_from_the_Perspectives_of_Distribution_ICCV_2023_paper.html": {
    "title": "Boosting Whole Slide Image Classification from the Perspectives of Distribution, Correlation and Magnification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linhao Qu",
      "Zhiwei Yang",
      "Minghong Duan",
      "Yingfan Ma",
      "Shuo Wang",
      "Manning Wang",
      "Zhijian Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_PolicyCleanse_Backdoor_Detection_and_Mitigation_for_Competitive_Reinforcement_Learning_ICCV_2023_paper.html": {
    "title": "PolicyCleanse: Backdoor Detection and Mitigation for Competitive Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfeng Guo",
      "Ang Li",
      "Lixu Wang",
      "Cong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ge_Ref-NeuS_Ambiguity-Reduced_Neural_Implicit_Surface_Learning_for_Multi-View_Reconstruction_with_ICCV_2023_paper.html": {
    "title": "Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhang Ge",
      "Tao Hu",
      "Haoyu Zhao",
      "Shu Liu",
      "Ying-Cong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Innovating_Real_Fisheye_Image_Correction_with_Dual_Diffusion_Architecture_ICCV_2023_paper.html": {
    "title": "Innovating Real Fisheye Image Correction with Dual Diffusion Architecture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangrong Yang",
      "Chunyu Lin",
      "Kang Liao",
      "Yao Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tai_Global_Perception_Based_Autoregressive_Neural_Processes_ICCV_2023_paper.html": {
    "title": "Global Perception Based Autoregressive Neural Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyang Tai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hsieh_Class-incremental_Continual_Learning_for_Instance_Segmentation_with_Image-level_Weak_Supervision_ICCV_2023_paper.html": {
    "title": "Class-incremental Continual Learning for Instance Segmentation with Image-level Weak Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Hsing Hsieh",
      "Guan-Sheng Chen",
      "Shun-Xian Cai",
      "Ting-Yun Wei",
      "Huei-Fang Yang",
      "Chu-Song Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tang_When_Prompt-based_Incremental_Learning_Does_Not_Meet_Strong_Pretraining_ICCV_2023_paper.html": {
    "title": "When Prompt-based Incremental Learning Does Not Meet Strong Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Ming Tang",
      "Yi-Xing Peng",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Multimodal_High-order_Relation_Transformer_for_Scene_Boundary_Detection_ICCV_2023_paper.html": {
    "title": "Multimodal High-order Relation Transformer for Scene Boundary Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Wei",
      "Zhangxiang Shi",
      "Tianzhu Zhang",
      "Xiaoyuan Yu",
      "Lei Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Tri-MipRF_Tri-Mip_Representation_for_Efficient_Anti-Aliasing_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Hu",
      "Yuling Wang",
      "Lin Ma",
      "Bangbang Yang",
      "Lin Gao",
      "Xiao Liu",
      "Yuewen Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zust_LaRS_A_Diverse_Panoptic_Maritime_Obstacle_Detection_Dataset_and_Benchmark_ICCV_2023_paper.html": {
    "title": "LaRS: A Diverse Panoptic Maritime Obstacle Detection Dataset and Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lojze Å½ust",
      "Janez PerÅ¡",
      "Matej Kristan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Exploring_Transformers_for_Open-world_Instance_Segmentation_ICCV_2023_paper.html": {
    "title": "Exploring Transformers for Open-world Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiannan Wu",
      "Yi Jiang",
      "Bin Yan",
      "Huchuan Lu",
      "Zehuan Yuan",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_VQA_Therapy_Exploring_Answer_Differences_by_Visually_Grounding_Answers_ICCV_2023_paper.html": {
    "title": "VQA Therapy: Exploring Answer Differences by Visually Grounding Answers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongyan Chen",
      "Samreen Anjum",
      "Danna Gurari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Herath_Energy-based_Self-Training_and_Normalization_for_Unsupervised_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "Energy-based Self-Training and Normalization for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samitha Herath",
      "Basura Fernando",
      "Ehsan Abbasnejad",
      "Munawar Hayat",
      "Shahram Khadivi",
      "Mehrtash Harandi",
      "Hamid Rezatofighi",
      "Gholamreza Haffari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Self-Evolved_Dynamic_Expansion_Model_for_Task-Free_Continual_Learning_ICCV_2023_paper.html": {
    "title": "Self-Evolved Dynamic Expansion Model for Task-Free Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Ye",
      "Adrian G. Bors"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pan_Adaptive_Template_Transformer_for_Mitochondria_Segmentation_in_Electron_Microscopy_Images_ICCV_2023_paper.html": {
    "title": "Adaptive Template Transformer for Mitochondria Segmentation in Electron Microscopy Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwen Pan",
      "Naisong Luo",
      "Rui Sun",
      "Meng Meng",
      "Tianzhu Zhang",
      "Zhiwei Xiong",
      "Yongdong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Collaborative_Tracking_Learning_for_Frame-Rate-Insensitive_Multi-Object_Tracking_ICCV_2023_paper.html": {
    "title": "Collaborative Tracking Learning for Frame-Rate-Insensitive Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiheng Liu",
      "Junta Wu",
      "Yi Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Tangent_Model_Composition_for_Ensembling_and_Continual_Fine-tuning_ICCV_2023_paper.html": {
    "title": "Tangent Model Composition for Ensembling and Continual Fine-tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Yu Liu",
      "Stefano Soatto"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Knowledge-Spreader_Learning_Semi-Supervised_Facial_Action_Dynamics_by_Consistifying_Knowledge_Granularity_ICCV_2023_paper.html": {
    "title": "Knowledge-Spreader: Learning Semi-Supervised Facial Action Dynamics by Consistifying Knowledge Granularity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotian Li",
      "Xiang Zhang",
      "Taoyue Wang",
      "Lijun Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_SSF_Accelerating_Training_of_Spiking_Neural_Networks_with_Stabilized_Spiking_ICCV_2023_paper.html": {
    "title": "SSF: Accelerating Training of Spiking Neural Networks with Stabilized Spiking Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingtao Wang",
      "Zengjie Song",
      "Yuxi Wang",
      "Jun Xiao",
      "Yuran Yang",
      "Shuqi Mei",
      "Zhaoxiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Manipulate_by_Seeing_Creating_Manipulation_Controllers_from_Pre-Trained_Representations_ICCV_2023_paper.html": {
    "title": "Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianren Wang",
      "Sudeep Dasari",
      "Mohan Kumar Srirama",
      "Shubham Tulsiani",
      "Abhinav Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Alper_Learning_Human-Human_Interactions_in_Images_from_Weak_Textual_Supervision_ICCV_2023_paper.html": {
    "title": "Learning Human-Human Interactions in Images from Weak Textual Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Morris Alper",
      "Hadar Averbuch-Elor"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Prompt-aligned_Gradient_for_Prompt_Tuning_ICCV_2023_paper.html": {
    "title": "Prompt-aligned Gradient for Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beier Zhu",
      "Yulei Niu",
      "Yucheng Han",
      "Yue Wu",
      "Hanwang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lv_Aperture_Diffraction_for_Compact_Snapshot_Spectral_Imaging_ICCV_2023_paper.html": {
    "title": "Aperture Diffraction for Compact Snapshot Spectral Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Lv",
      "Hao Ye",
      "Quan Yuan",
      "Zhan Shi",
      "Yibo Wang",
      "Shuming Wang",
      "Xun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Diffusion_Action_Segmentation_ICCV_2023_paper.html": {
    "title": "Diffusion Action Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daochang Liu",
      "Qiyue Li",
      "Anh-Dung Dinh",
      "Tingting Jiang",
      "Mubarak Shah",
      "Chang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shi_Prototype_Reminiscence_and_Augmented_Asymmetric_Knowledge_Aggregation_for_Non-Exemplar_Class-Incremental_ICCV_2023_paper.html": {
    "title": "Prototype Reminiscence and Augmented Asymmetric Knowledge Aggregation for Non-Exemplar Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wuxuan Shi",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Roy_Exemplar-Free_Continual_Transformer_with_Convolutions_ICCV_2023_paper.html": {
    "title": "Exemplar-Free Continual Transformer with Convolutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anurag Roy",
      "Vinay K. Verma",
      "Sravan Voonna",
      "Kripabandhu Ghosh",
      "Saptarshi Ghosh",
      "Abir Das"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Scalable_Video_Object_Segmentation_with_Simplified_Framework_ICCV_2023_paper.html": {
    "title": "Scalable Video Object Segmentation with Simplified Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiangqiang Wu",
      "Tianyu Yang",
      "Wei Wu",
      "Antoni B. Chan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cai_Rehearsal-Free_Domain_Continual_Face_Anti-Spoofing_Generalize_More_and_Forget_Less_ICCV_2023_paper.html": {
    "title": "Rehearsal-Free Domain Continual Face Anti-Spoofing: Generalize More and Forget Less",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rizhao Cai",
      "Yawen Cui",
      "Zhi Li",
      "Zitong Yu",
      "Haoliang Li",
      "Yongjian Hu",
      "Alex Kot"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Efficient_Decision-based_Black-box_Patch_Attacks_on_Video_Recognition_ICCV_2023_paper.html": {
    "title": "Efficient Decision-based Black-box Patch Attacks on Video Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixun Jiang",
      "Zhaoyu Chen",
      "Hao Huang",
      "Jiafeng Wang",
      "Dingkang Yang",
      "Bo Li",
      "Yan Wang",
      "Wenqiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Spencer_Kick_Back__Relax_Learning_to_Reconstruct_the_World_by_ICCV_2023_paper.html": {
    "title": "Kick Back & Relax: Learning to Reconstruct the World by Watching SlowTV",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaime Spencer",
      "Chris Russell",
      "Simon Hadfield",
      "Richard Bowden"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_MetaGCD_Learning_to_Continually_Learn_in_Generalized_Category_Discovery_ICCV_2023_paper.html": {
    "title": "MetaGCD: Learning to Continually Learn in Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanan Wu",
      "Zhixiang Chi",
      "Yang Wang",
      "Songhe Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cao_Strip-MLP_Efficient_Token_Interaction_for_Vision_MLP_ICCV_2023_paper.html": {
    "title": "Strip-MLP: Efficient Token Interaction for Vision MLP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guiping Cao",
      "Shengda Luo",
      "Wenjian Huang",
      "Xiangyuan Lan",
      "Dongmei Jiang",
      "Yaowei Wang",
      "Jianguo Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_SAFARI_Versatile_and_Efficient_Evaluations_for_Robustness_of_Interpretability_ICCV_2023_paper.html": {
    "title": "SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Huang",
      "Xingyu Zhao",
      "Gaojie Jin",
      "Xiaowei Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tafasca_ChildPlay_A_New_Benchmark_for_Understanding_Childrens_Gaze_Behaviour_ICCV_2023_paper.html": {
    "title": "ChildPlay: A New Benchmark for Understanding Children's Gaze Behaviour",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samy Tafasca",
      "Anshul Gupta",
      "Jean-Marc Odobez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Towards_General_Low-Light_Raw_Noise_Synthesis_and_Modeling_ICCV_2023_paper.html": {
    "title": "Towards General Low-Light Raw Noise Synthesis and Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Zhang",
      "Bin Xu",
      "Zhiqiang Li",
      "Xinran Liu",
      "Qingbo Lu",
      "Changxin Gao",
      "Nong Sang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xia_Combating_Noisy_Labels_with_Sample_Selection_by_Mining_High-Discrepancy_Examples_ICCV_2023_paper.html": {
    "title": "Combating Noisy Labels with Sample Selection by Mining High-Discrepancy Examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobo Xia",
      "Bo Han",
      "Yibing Zhan",
      "Jun Yu",
      "Mingming Gong",
      "Chen Gong",
      "Tongliang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bolduc_Beyond_the_Pixel_a_Photometrically_Calibrated_HDR_Dataset_for_Luminance_ICCV_2023_paper.html": {
    "title": "Beyond the Pixel: a Photometrically Calibrated HDR Dataset for Luminance and Color Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christophe Bolduc",
      "Justine Giroux",
      "Marc HÃ©bert",
      "Claude Demers",
      "Jean-FranÃ§ois Lalonde"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_What_can_Discriminator_do_Towards_Box-free_Ownership_Verification_of_Generative_ICCV_2023_paper.html": {
    "title": "What can Discriminator do? Towards Box-free Ownership Verification of Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziheng Huang",
      "Boheng Li",
      "Yan Cai",
      "Run Wang",
      "Shangwei Guo",
      "Liming Fang",
      "Jing Chen",
      "Lina Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_When_Noisy_Labels_Meet_Long_Tail_Dilemmas_A_Representation_Calibration_ICCV_2023_paper.html": {
    "title": "When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manyi Zhang",
      "Xuyang Zhao",
      "Jun Yao",
      "Chun Yuan",
      "Weiran Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Faghri_Reinforce_Data_Multiply_Impact_Improved_Model_Accuracy_and_Robustness_with_ICCV_2023_paper.html": {
    "title": "Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fartash Faghri",
      "Hadi Pouransari",
      "Sachin Mehta",
      "Mehrdad Farajtabar",
      "Ali Farhadi",
      "Mohammad Rastegari",
      "Oncel Tuzel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_An_Adaptive_Model_Ensemble_Adversarial_Attack_for_Boosting_Adversarial_Transferability_ICCV_2023_paper.html": {
    "title": "An Adaptive Model Ensemble Adversarial Attack for Boosting Adversarial Transferability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Chen",
      "Jiali Yin",
      "Shukai Chen",
      "Bohao Chen",
      "Ximeng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Incremental_Generalized_Category_Discovery_ICCV_2023_paper.html": {
    "title": "Incremental Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingchen Zhao",
      "Oisin Mac Aodha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Prototypical_Mixing_and_Retrieval-Based_Refinement_for_Label_Noise-Resistant_Image_Retrieval_ICCV_2023_paper.html": {
    "title": "Prototypical Mixing and Retrieval-Based Refinement for Label Noise-Resistant Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinlong Yang",
      "Haixin Wang",
      "Jinan Sun",
      "Shikun Zhang",
      "Chong Chen",
      "Xian-Sheng Hua",
      "Xiao Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_AccFlow_Backward_Accumulation_for_Long-Range_Optical_Flow_ICCV_2023_paper.html": {
    "title": "AccFlow: Backward Accumulation for Long-Range Optical Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyang Wu",
      "Xiaohong Liu",
      "Kunming Luo",
      "Xi Liu",
      "Qingqing Zheng",
      "Shuaicheng Liu",
      "Xinyang Jiang",
      "Guangtao Zhai",
      "Wenyi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Guiding_Local_Feature_Matching_with_Surface_Curvature_ICCV_2023_paper.html": {
    "title": "Guiding Local Feature Matching with Surface Curvature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuzhe Wang",
      "Juho Kannala",
      "Marc Pollefeys",
      "Daniel Barath"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_3D-VisTA_Pre-trained_Transformer_for_3D_Vision_and_Text_Alignment_ICCV_2023_paper.html": {
    "title": "3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Zhu",
      "Xiaojian Ma",
      "Yixin Chen",
      "Zhidong Deng",
      "Siyuan Huang",
      "Qing Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Constraining_Depth_Map_Geometry_for_Multi-View_Stereo_A_Dual-Depth_Approach_ICCV_2023_paper.html": {
    "title": "Constraining Depth Map Geometry for Multi-View Stereo: A Dual-Depth Approach with Saddle-shaped Depth Cells",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Ye",
      "Weiyue Zhao",
      "Tianqi Liu",
      "Zihao Huang",
      "Zhiguo Cao",
      "Xin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Suri_SparseDet_Improving_Sparsely_Annotated_Object_Detection_with_Pseudo-positive_Mining_ICCV_2023_paper.html": {
    "title": "SparseDet: Improving Sparsely Annotated Object Detection with Pseudo-positive Mining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saksham Suri",
      "Saketh Rambhatla",
      "Rama Chellappa",
      "Abhinav Shrivastava"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Among_Us_Adversarially_Robust_Collaborative_Perception_by_Consensus_ICCV_2023_paper.html": {
    "title": "Among Us: Adversarially Robust Collaborative Perception by Consensus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Li",
      "Qi Fang",
      "Jiamu Bai",
      "Siheng Chen",
      "Felix Juefei-Xu",
      "Chen Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_BUS_Efficient_and_Effective_Vision-Language_Pre-Training_with_Bottom-Up_Patch_Summarization._ICCV_2023_paper.html": {
    "title": "BUS: Efficient and Effective Vision-Language Pre-Training with Bottom-Up Patch Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoya Jiang",
      "Haiyang Xu",
      "Wei Ye",
      "Qinghao Ye",
      "Chenliang Li",
      "Ming Yan",
      "Bin Bi",
      "Shikun Zhang",
      "Fei Huang",
      "Songfang Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_DiffusionDet_Diffusion_Model_for_Object_Detection_ICCV_2023_paper.html": {
    "title": "DiffusionDet: Diffusion Model for Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoufa Chen",
      "Peize Sun",
      "Yibing Song",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Forward_Flow_for_Novel_View_Synthesis_of_Dynamic_Scenes_ICCV_2023_paper.html": {
    "title": "Forward Flow for Novel View Synthesis of Dynamic Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Guo",
      "Jiadai Sun",
      "Yuchao Dai",
      "Guanying Chen",
      "Xiaoqing Ye",
      "Xiao Tan",
      "Errui Ding",
      "Yumeng Zhang",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Luo_CopyRNeRF_Protecting_the_CopyRight_of_Neural_Radiance_Fields_ICCV_2023_paper.html": {
    "title": "CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyuan Luo",
      "Qing Guo",
      "Ka Chun Cheung",
      "Simon See",
      "Renjie Wan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bruggemann_Contrastive_Model_Adaptation_for_Cross-Condition_Robustness_in_Semantic_Segmentation_ICCV_2023_paper.html": {
    "title": "Contrastive Model Adaptation for Cross-Condition Robustness in Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David BrÃ¼ggemann",
      "Christos Sakaridis",
      "Tim Broedermann",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shinoda_SegRCDB_Semantic_Segmentation_via_Formula-Driven_Supervised_Learning_ICCV_2023_paper.html": {
    "title": "SegRCDB: Semantic Segmentation via Formula-Driven Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Risa Shinoda",
      "Ryo Hayamizu",
      "Kodai Nakashima",
      "Nakamasa Inoue",
      "Rio Yokota",
      "Hirokatsu Kataoka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Creative_Birds_Self-Supervised_Single-View_3D_Style_Transfer_ICCV_2023_paper.html": {
    "title": "Creative Birds: Self-Supervised Single-View 3D Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renke Wang",
      "Guimin Que",
      "Shuo Chen",
      "Xiang Li",
      "Jun Li",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_LoTE-Animal_A_Long_Time-span_Dataset_for_Endangered_Animal_Behavior_Understanding_ICCV_2023_paper.html": {
    "title": "LoTE-Animal: A Long Time-span Dataset for Endangered Animal Behavior Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Liu",
      "Jin Hou",
      "Shaoli Huang",
      "Jing Liu",
      "Yuxin He",
      "Bochuan Zheng",
      "Jifeng Ning",
      "Jingdong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gao_DQS3D_Densely-matched_Quantization-aware_Semi-supervised_3D_Detection_ICCV_2023_paper.html": {
    "title": "DQS3D: Densely-matched Quantization-aware Semi-supervised 3D Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan-ang Gao",
      "Beiwen Tian",
      "Pengfei Li",
      "Hao Zhao",
      "Guyue Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Deng_Towards_Inadequately_Pre-trained_Models_in_Transfer_Learning_ICCV_2023_paper.html": {
    "title": "Towards Inadequately Pre-trained Models in Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andong Deng",
      "Xingjian Li",
      "Di Hu",
      "Tianyang Wang",
      "Haoyi Xiong",
      "Cheng-Zhong Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zang_Boosting_Novel_Category_Discovery_Over_Domains_with_Soft_Contrastive_Learning_ICCV_2023_paper.html": {
    "title": "Boosting Novel Category Discovery Over Domains with Soft Contrastive Learning and All in One Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Zang",
      "Lei Shang",
      "Senqiao Yang",
      "Fei Wang",
      "Baigui Sun",
      "Xuansong Xie",
      "Stan Z. Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hao_Class-Aware_Patch_Embedding_Adaptation_for_Few-Shot_Image_Classification_ICCV_2023_paper.html": {
    "title": "Class-Aware Patch Embedding Adaptation for Few-Shot Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fusheng Hao",
      "Fengxiang He",
      "Liu Liu",
      "Fuxiang Wu",
      "Dacheng Tao",
      "Jun Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_SegPrompt_Boosting_Open-World_Segmentation_via_Category-Level_Prompt_Learning_ICCV_2023_paper.html": {
    "title": "SegPrompt: Boosting Open-World Segmentation via Category-Level Prompt Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muzhi Zhu",
      "Hengtao Li",
      "Hao Chen",
      "Chengxiang Fan",
      "Weian Mao",
      "Chenchen Jing",
      "Yifan Liu",
      "Chunhua Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dang_Search_for_or_Navigate_to_Dual_Adaptive_Thinking_for_Object_ICCV_2023_paper.html": {
    "title": "Search for or Navigate to? Dual Adaptive Thinking for Object Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronghao Dang",
      "Liuyi Wang",
      "Zongtao He",
      "Shuai Su",
      "Jiagui Tang",
      "Chengju Liu",
      "Qijun Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xiong_CL-MVSNet_Unsupervised_Multi-View_Stereo_with_Dual-Level_Contrastive_Learning_ICCV_2023_paper.html": {
    "title": "CL-MVSNet: Unsupervised Multi-View Stereo with Dual-Level Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiqiang Xiong",
      "Rui Peng",
      "Zhe Zhang",
      "Tianxing Feng",
      "Jianbo Jiao",
      "Feng Gao",
      "Ronggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Federated_Learning_Over_Images_Vertical_Decompositions_and_Pre-Trained_Backbones_Are_ICCV_2023_paper.html": {
    "title": "Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erdong Hu",
      "Yuxin Tang",
      "Anastasios Kyrillidis",
      "Chris Jermaine"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_HOSNeRF_Dynamic_Human-Object-Scene_Neural_Radiance_Fields_from_a_Single_Video_ICCV_2023_paper.html": {
    "title": "HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia-Wei Liu",
      "Yan-Pei Cao",
      "Tianyuan Yang",
      "Zhongcong Xu",
      "Jussi Keppo",
      "Ying Shan",
      "Xiaohu Qie",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cao_OmniZoomer_Learning_to_Move_and_Zoom_in_on_Sphere_at_ICCV_2023_paper.html": {
    "title": "OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zidong Cao",
      "Hao Ai",
      "Yan-Pei Cao",
      "Ying Shan",
      "Xiaohu Qie",
      "Lin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jang_Knowing_Where_to_Focus_Event-aware_Transformer_for_Video_Grounding_ICCV_2023_paper.html": {
    "title": "Knowing Where to Focus: Event-aware Transformer for Video Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhyun Jang",
      "Jungin Park",
      "Jin Kim",
      "Hyeongjun Kwon",
      "Kwanghoon Sohn"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lu_TF-ICON_Diffusion-Based_Training-Free_Cross-Domain_Image_Composition_ICCV_2023_paper.html": {
    "title": "TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shilin Lu",
      "Yanzhu Liu",
      "Adams Wai-Kin Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Landscape_Learning_for_Neural_Network_Inversion_ICCV_2023_paper.html": {
    "title": "Landscape Learning for Neural Network Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoshi Liu",
      "Chengzhi Mao",
      "Purva Tendulkar",
      "Hao Wang",
      "Carl Vondrick"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Movement_Enhancement_toward_Multi-Scale_Video_Feature_Representation_for_Temporal_Action_ICCV_2023_paper.html": {
    "title": "Movement Enhancement toward Multi-Scale Video Feature Representation for Temporal Action Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Zhao",
      "Dongqi Wang",
      "Xu Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dong_Collaborative_Propagation_on_Multiple_Instance_Graphs_for_3D_Instance_Segmentation_ICCV_2023_paper.html": {
    "title": "Collaborative Propagation on Multiple Instance Graphs for 3D Instance Segmentation with Single-point Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shichao Dong",
      "Ruibo Li",
      "Jiacheng Wei",
      "Fayao Liu",
      "Guosheng Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_PPR_Physically_Plausible_Reconstruction_from_Monocular_Videos_ICCV_2023_paper.html": {
    "title": "PPR: Physically Plausible Reconstruction from Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gengshan Yang",
      "Shuo Yang",
      "John Z. Zhang",
      "Zachary Manchester",
      "Deva Ramanan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ji_Single_Image_Deblurring_with_Row-dependent_Blur_Magnitude_ICCV_2023_paper.html": {
    "title": "Single Image Deblurring with Row-dependent Blur Magnitude",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Ji",
      "Zhixiang Wang",
      "Shin'ichi Satoh",
      "Yinqiang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fang_Robust_Heterogeneous_Federated_Learning_under_Data_Corruption_ICCV_2023_paper.html": {
    "title": "Robust Heterogeneous Federated Learning under Data Corruption",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuwen Fang",
      "Mang Ye",
      "Xiyuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_RMP-Loss_Regularizing_Membrane_Potential_Distribution_for_Spiking_Neural_Networks_ICCV_2023_paper.html": {
    "title": "RMP-Loss: Regularizing Membrane Potential Distribution for Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Guo",
      "Xiaode Liu",
      "Yuanpei Chen",
      "Liwen Zhang",
      "Weihang Peng",
      "Yuhan Zhang",
      "Xuhui Huang",
      "Zhe Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yin_Cyclic-Bootstrap_Labeling_for_Weakly_Supervised_Object_Detection_ICCV_2023_paper.html": {
    "title": "Cyclic-Bootstrap Labeling for Weakly Supervised Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Yin",
      "Jiajun Deng",
      "Wengang Zhou",
      "Li Li",
      "Houqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Deep_Active_Contours_for_Real-time_6-DoF_Object_Tracking_ICCV_2023_paper.html": {
    "title": "Deep Active Contours for Real-time 6-DoF Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Wang",
      "Shen Yan",
      "Jianan Zhen",
      "Yu Liu",
      "Maojun Zhang",
      "Guofeng Zhang",
      "Xiaowei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Terekhov_Tangent_Sampson_Error_Fast_Approximate_Two-view_Reprojection_Error_for_Central_ICCV_2023_paper.html": {
    "title": "Tangent Sampson Error: Fast Approximate Two-view Reprojection Error for Central Camera Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikhail Terekhov",
      "Viktor Larsson"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Multi-grained_Temporal_Prototype_Learning_for_Few-shot_Video_Object_Segmentation_ICCV_2023_paper.html": {
    "title": "Multi-grained Temporal Prototype Learning for Few-shot Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nian Liu",
      "Kepan Nan",
      "Wangbo Zhao",
      "Yuanwei Liu",
      "Xiwen Yao",
      "Salman Khan",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer",
      "Junwei Han",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Improving_3D_Imaging_with_Pre-Trained_Perpendicular_2D_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suhyeon Lee",
      "Hyungjin Chung",
      "Minyoung Park",
      "Jonghyuk Park",
      "Wi-Sun Ryu",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Salehi_Time_Does_Tell_Self-Supervised_Time-Tuning_of_Dense_Image_Representations_ICCV_2023_paper.html": {
    "title": "Time Does Tell: Self-Supervised Time-Tuning of Dense Image Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammadreza Salehi",
      "Efstratios Gavves",
      "Cees G.M. Snoek",
      "Yuki M. Asano"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Weinzaepfel_CroCo_v2_Improved_Cross-view_Completion_Pre-training_for_Stereo_Matching_and_ICCV_2023_paper.html": {
    "title": "CroCo v2: Improved Cross-view Completion Pre-training for Stereo Matching and Optical Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philippe Weinzaepfel",
      "Thomas Lucas",
      "Vincent Leroy",
      "Yohann Cabon",
      "Vaibhav Arora",
      "Romain BrÃ©gier",
      "Gabriela Csurka",
      "Leonid Antsfeld",
      "Boris Chidlovskii",
      "Jerome Revaud"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Lee_ExBluRF_Efficient_Radiance_Fields_for_Extreme_Motion_Blurred_Images_ICCV_2023_paper.html": {
    "title": "ExBluRF: Efficient Radiance Fields for Extreme Motion Blurred Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongwoo Lee",
      "Jeongtaek Oh",
      "Jaesung Rim",
      "Sunghyun Cho",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zeng_MPCViT_Searching_for_Accurate_and_Efficient_MPC-Friendly_Vision_Transformer_with_ICCV_2023_paper.html": {
    "title": "MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision Transformer with Heterogeneous Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Zeng",
      "Meng Li",
      "Wenjie Xiong",
      "Tong Tong",
      "Wen-jie Lu",
      "Jin Tan",
      "Runsheng Wang",
      "Ru Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Moon_Online_Class_Incremental_Learning_on_Stochastic_Blurry_Task_Boundary_via_ICCV_2023_paper.html": {
    "title": "Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Yeong Moon",
      "Keon-Hee Park",
      "Jung Uk Kim",
      "Gyeong-Moon Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Khachatryan_Text2Video-Zero_Text-to-Image_Diffusion_Models_are_Zero-Shot_Video_Generators_ICCV_2023_paper.html": {
    "title": "Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Levon Khachatryan",
      "Andranik Movsisyan",
      "Vahram Tadevosyan",
      "Roberto Henschel",
      "Zhangyang Wang",
      "Shant Navasardyan",
      "Humphrey Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Masked_Spiking_Transformer_ICCV_2023_paper.html": {
    "title": "Masked Spiking Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqing Wang",
      "Yuetong Fang",
      "Jiahang Cao",
      "Qiang Zhang",
      "Zhongrui Wang",
      "Renjing Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Exploring_Video_Quality_Assessment_on_User_Generated_Contents_from_Aesthetic_ICCV_2023_paper.html": {
    "title": "Exploring Video Quality Assessment on User Generated Contents from Aesthetic and Technical Perspectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoning Wu",
      "Erli Zhang",
      "Liang Liao",
      "Chaofeng Chen",
      "Jingwen Hou",
      "Annan Wang",
      "Wenxiu Sun",
      "Qiong Yan",
      "Weisi Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Distributed_Bundle_Adjustment_with_Block-Based_Sparse_Matrix_Compression_for_Super_ICCV_2023_paper.html": {
    "title": "Distributed Bundle Adjustment with Block-Based Sparse Matrix Compression for Super Large Scale Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maoteng Zheng",
      "Nengcheng Chen",
      "Junfeng Zhu",
      "Xiaoru Zeng",
      "Huanbin Qiu",
      "Yuyao Jiang",
      "Xingyue Lu",
      "Hao Qu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yoon_SCANet_Scene_Complexity_Aware_Network_for_Weakly-Supervised_Video_Moment_Retrieval_ICCV_2023_paper.html": {
    "title": "SCANet: Scene Complexity Aware Network for Weakly-Supervised Video Moment Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunjae Yoon",
      "Gwanhyeong Koo",
      "Dahyun Kim",
      "Chang D. Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Neural_Interactive_Keypoint_Detection_ICCV_2023_paper.html": {
    "title": "Neural Interactive Keypoint Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Yang",
      "Ailing Zeng",
      "Feng Li",
      "Shilong Liu",
      "Ruimao Zhang",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Joint_Implicit_Neural_Representation_for_High-fidelity_and_Compact_Vector_Fonts_ICCV_2023_paper.html": {
    "title": "Joint Implicit Neural Representation for High-fidelity and Compact Vector Fonts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chia-Hao Chen",
      "Ying-Tian Liu",
      "Zhifei Zhang",
      "Yuan-Chen Guo",
      "Song-Hai Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Neuhaus_Spurious_Features_Everywhere_-_Large-Scale_Detection_of_Harmful_Spurious_Features_ICCV_2023_paper.html": {
    "title": "Spurious Features Everywhere - Large-Scale Detection of Harmful Spurious Features in ImageNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yannic Neuhaus",
      "Maximilian Augustin",
      "Valentyn Boreiko",
      "Matthias Hein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.html": {
    "title": "Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baoshuo Kan",
      "Teng Wang",
      "Wenpeng Lu",
      "Xiantong Zhen",
      "Weili Guan",
      "Feng Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Delicate_Textured_Mesh_Recovery_from_NeRF_via_Adaptive_Surface_Refinement_ICCV_2023_paper.html": {
    "title": "Delicate Textured Mesh Recovery from NeRF via Adaptive Surface Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxiang Tang",
      "Hang Zhou",
      "Xiaokang Chen",
      "Tianshu Hu",
      "Errui Ding",
      "Jingdong Wang",
      "Gang Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Leveraging_Inpainting_for_Single-Image_Shadow_Removal_ICCV_2023_paper.html": {
    "title": "Leveraging Inpainting for Single-Image Shadow Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoguang Li",
      "Qing Guo",
      "Rabab Abdelfattah",
      "Di Lin",
      "Wei Feng",
      "Ivor Tsang",
      "Song Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Neural_Characteristic_Function_Learning_for_Conditional_Image_Generation_ICCV_2023_paper.html": {
    "title": "Neural Characteristic Function Learning for Conditional Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengxi Li",
      "Jialu Zhang",
      "Yifei Li",
      "Mai Xu",
      "Xin Deng",
      "Li Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Accurate_3D_Face_Reconstruction_with_Facial_Component_Tokens_ICCV_2023_paper.html": {
    "title": "Accurate 3D Face Reconstruction with Facial Component Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianke Zhang",
      "Xuangeng Chu",
      "Yunfei Liu",
      "Lijian Lin",
      "Zhendong Yang",
      "Zhengzhuo Xu",
      "Chengkun Cao",
      "Fei Yu",
      "Changyin Zhou",
      "Chun Yuan",
      "Yu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xia_Holistic_Label_Correction_for_Noisy_Multi-Label_Classification_ICCV_2023_paper.html": {
    "title": "Holistic Label Correction for Noisy Multi-Label Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobo Xia",
      "Jiankang Deng",
      "Wei Bao",
      "Yuxuan Du",
      "Bo Han",
      "Shiguang Shan",
      "Tongliang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Park_Probabilistic_Precision_and_Recall_Towards_Reliable_Evaluation_of_Generative_Models_ICCV_2023_paper.html": {
    "title": "Probabilistic Precision and Recall Towards Reliable Evaluation of Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dogyun Park",
      "Suhyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shi_Deep_Multitask_Learning_with_Progressive_Parameter_Sharing_ICCV_2023_paper.html": {
    "title": "Deep Multitask Learning with Progressive Parameter Sharing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haosen Shi",
      "Shen Ren",
      "Tianwei Zhang",
      "Sinno Jialin Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xia_Personalized_Semantics_Excitation_for_Federated_Image_Classification_ICCV_2023_paper.html": {
    "title": "Personalized Semantics Excitation for Federated Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haifeng Xia",
      "Kai Li",
      "Zhengming Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Bai_Unified_Data-Free_Compression_Pruning_and_Quantization_without_Fine-Tuning_ICCV_2023_paper.html": {
    "title": "Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shipeng Bai",
      "Jun Chen",
      "Xintian Shen",
      "Yixuan Qian",
      "Yong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wei_SurroundOcc_Multi-camera_3D_Occupancy_Prediction_for_Autonomous_Driving_ICCV_2023_paper.html": {
    "title": "SurroundOcc: Multi-camera 3D Occupancy Prediction for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Wei",
      "Linqing Zhao",
      "Wenzhao Zheng",
      "Zheng Zhu",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zong_Temporal_Enhanced_Training_of_Multi-view_3D_Object_Detector_via_Historical_ICCV_2023_paper.html": {
    "title": "Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuofan Zong",
      "Dongzhi Jiang",
      "Guanglu Song",
      "Zeyue Xue",
      "Jingyong Su",
      "Hongsheng Li",
      "Yu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_PARIS_Part-level_Reconstruction_and_Motion_Analysis_for_Articulated_Objects_ICCV_2023_paper.html": {
    "title": "PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Liu",
      "Ali Mahdavi-Amiri",
      "Manolis Savva"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_OnlineRefer_A_Simple_Online_Baseline_for_Referring_Video_Object_Segmentation_ICCV_2023_paper.html": {
    "title": "OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongming Wu",
      "Tiancai Wang",
      "Yuang Zhang",
      "Xiangyu Zhang",
      "Jianbing Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Implicit_Neural_Representation_for_Cooperative_Low-light_Image_Enhancement_ICCV_2023_paper.html": {
    "title": "Implicit Neural Representation for Cooperative Low-light Image Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuzhou Yang",
      "Moxuan Ding",
      "Yanmin Wu",
      "Zihan Li",
      "Jian Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Choi_Environment_Agnostic_Representation_for_Visual_Reinforcement_Learning_ICCV_2023_paper.html": {
    "title": "Environment Agnostic Representation for Visual Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyesong Choi",
      "Hunsang Lee",
      "Seongwon Jeong",
      "Dongbo Min"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Deep_Multiview_Clustering_by_Contrasting_Cluster_Assignments_ICCV_2023_paper.html": {
    "title": "Deep Multiview Clustering by Contrasting Cluster Assignments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Chen",
      "Hua Mao",
      "Wai Lok Woo",
      "Xi Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Mimic3D_Thriving_3D-Aware_GANs_via_3D-to-2D_Imitation_ICCV_2023_paper.html": {
    "title": "Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Chen",
      "Yu Deng",
      "Baoyuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Look_at_the_Neighbor_Distortion-aware_Unsupervised_Domain_Adaptation_for_Panoramic_ICCV_2023_paper.html": {
    "title": "Look at the Neighbor: Distortion-aware Unsupervised Domain Adaptation for Panoramic Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Zheng",
      "Tianbo Pan",
      "Yunhao Luo",
      "Lin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Rethinking_Safe_Semi-supervised_Learning_Transferring_the_Open-set_Problem_to_A_ICCV_2023_paper.html": {
    "title": "Rethinking Safe Semi-supervised Learning: Transferring the Open-set Problem to A Close-set One",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiankun Ma",
      "Jiyao Gao",
      "Bo Zhan",
      "Yunpeng Guo",
      "Jiliu Zhou",
      "Yan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Does_Physical_Adversarial_Example_Really_Matter_to_Autonomous_Driving_Towards_ICCV_2023_paper.html": {
    "title": "Does Physical Adversarial Example Really Matter to Autonomous Driving? Towards System-Level Effect of Adversarial Object Evasion Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ningfei Wang",
      "Yunpeng Luo",
      "Takami Sato",
      "Kaidi Xu",
      "Qi Alfred Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chan_ReLeaPS__Reinforcement_Learning-based_Illumination_Planning_for_Generalized_Photometric_Stereo_ICCV_2023_paper.html": {
    "title": "ReLeaPS : Reinforcement Learning-based Illumination Planning for Generalized Photometric Stereo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Hoong Chan",
      "Bohan Yu",
      "Heng Guo",
      "Jieji Ren",
      "Zongqing Lu",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Learning_Foresightful_Dense_Visual_Affordance_for_Deformable_Object_Manipulation_ICCV_2023_paper.html": {
    "title": "Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihai Wu",
      "Chuanruo Ning",
      "Hao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gu_Generalizable_Neural_Fields_as_Partially_Observed_Neural_Processes_ICCV_2023_paper.html": {
    "title": "Generalizable Neural Fields as Partially Observed Neural Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeffrey Gu",
      "Kuan-Chieh Wang",
      "Serena Yeung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_CiteTracker_Correlating_Image_and_Text_for_Visual_Tracking_ICCV_2023_paper.html": {
    "title": "CiteTracker: Correlating Image and Text for Visual Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Li",
      "Yuqing Huang",
      "Zhenyu He",
      "Yaowei Wang",
      "Huchuan Lu",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html": {
    "title": "Adding Conditional Control to Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lvmin Zhang",
      "Anyi Rao",
      "Maneesh Agrawala"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Al_Khatib_3D_Instance_Segmentation_via_Enhanced_Spatial_and_Semantic_Supervision_ICCV_2023_paper.html": {
    "title": "3D Instance Segmentation via Enhanced Spatial and Semantic Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salwa Al Khatib",
      "Mohamed El Amine Boudjoghra",
      "Jean Lahoud",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Unleashing_Text-to-Image_Diffusion_Models_for_Visual_Perception_ICCV_2023_paper.html": {
    "title": "Unleashing Text-to-Image Diffusion Models for Visual Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenliang Zhao",
      "Yongming Rao",
      "Zuyan Liu",
      "Benlin Liu",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Alaniz_Iterative_Superquadric_Recomposition_of_3D_Objects_from_Multiple_Views_ICCV_2023_paper.html": {
    "title": "Iterative Superquadric Recomposition of 3D Objects from Multiple Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephan Alaniz",
      "Massimiliano Mancini",
      "Zeynep Akata"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_PHRIT_Parametric_Hand_Representation_with_Implicit_Template_ICCV_2023_paper.html": {
    "title": "PHRIT: Parametric Hand Representation with Implicit Template",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhisheng Huang",
      "Yujin Chen",
      "Di Kang",
      "Jinlu Zhang",
      "Zhigang Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Luo_BEVPlace_Learning_LiDAR-based_Place_Recognition_using_Birds_Eye_View_Images_ICCV_2023_paper.html": {
    "title": "BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lun Luo",
      "Shuhang Zheng",
      "Yixuan Li",
      "Yongzhi Fan",
      "Beinan Yu",
      "Si-Yuan Cao",
      "Junwei Li",
      "Hui-Liang Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Transferable_Adversarial_Attack_for_Both_Vision_Transformers_and_Convolutional_Networks_ICCV_2023_paper.html": {
    "title": "Transferable Adversarial Attack for Both Vision Transformers and Convolutional Networks via Momentum Integrated Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenshuo Ma",
      "Yidong Li",
      "Xiaofeng Jia",
      "Wei Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_TrajPAC_Towards_Robustness_Verification_of_Pedestrian_Trajectory_Prediction_Models_ICCV_2023_paper.html": {
    "title": "TrajPAC: Towards Robustness Verification of Pedestrian Trajectory Prediction Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Zhang",
      "Nathaniel Xu",
      "Pengfei Yang",
      "Gaojie Jin",
      "Cheng-Chao Huang",
      "Lijun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shvai_Adaptive_Image_Anonymization_in_the_Context_of_Image_Classification_with_ICCV_2023_paper.html": {
    "title": "Adaptive Image Anonymization in the Context of Image Classification with Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nadiya Shvai",
      "Arcadi Llanza Carmona",
      "Amir Nakib"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gleize_SiLK_Simple_Learned_Keypoints_ICCV_2023_paper.html": {
    "title": "SiLK: Simple Learned Keypoints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Gleize",
      "Weiyao Wang",
      "Matt Feiszli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cai_EfficientViT_Lightweight_Multi-Scale_Attention_for_High-Resolution_Dense_Prediction_ICCV_2023_paper.html": {
    "title": "EfficientViT: Lightweight Multi-Scale Attention for High-Resolution Dense Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Cai",
      "Junyan Li",
      "Muyan Hu",
      "Chuang Gan",
      "Song Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Mercier_Efficient_Neural_Supersampling_on_a_Novel_Gaming_Dataset_ICCV_2023_paper.html": {
    "title": "Efficient Neural Supersampling on a Novel Gaming Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Mercier",
      "Ruan Erasmus",
      "Yashesh Savani",
      "Manik Dhingra",
      "Fatih Porikli",
      "Guillaume Berger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Al_Kader_Hammoud_Rapid_Adaptation_in_Online_Continual_Learning_Are_We_Evaluating_It_ICCV_2023_paper.html": {
    "title": "Rapid Adaptation in Online Continual Learning: Are We Evaluating It Right?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hasan Abed Al Kader Hammoud",
      "Ameya Prabhu",
      "Ser-Nam Lim",
      "Philip H.S. Torr",
      "Adel Bibi",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Label-Efficient_Online_Continual_Object_Detection_in_Streaming_Video_ICCV_2023_paper.html": {
    "title": "Label-Efficient Online Continual Object Detection in Streaming Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jay Zhangjie Wu",
      "David Junhao Zhang",
      "Wynne Hsu",
      "Mengmi Zhang",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Learning_Point_Cloud_Completion_without_Complete_Point_Clouds_A_Pose-Aware_ICCV_2023_paper.html": {
    "title": "Learning Point Cloud Completion without Complete Point Clouds: A Pose-Aware Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihun Kim",
      "Hyeokjun Kweon",
      "Yunseo Yang",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_Frequency_Guidance_Matters_in_Few-Shot_Learning_ICCV_2023_paper.html": {
    "title": "Frequency Guidance Matters in Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Cheng",
      "Siyuan Yang",
      "Joey Tianyi Zhou",
      "Lanqing Guo",
      "Bihan Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Saltori_Walking_Your_LiDOG_A_Journey_Through_Multiple_Domains_for_LiDAR_ICCV_2023_paper.html": {
    "title": "Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cristiano Saltori",
      "Aljosa Osep",
      "Elisa Ricci",
      "Laura Leal-TaixÃ©"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Diverse_Cotraining_Makes_Strong_Semi-Supervised_Segmentor_ICCV_2023_paper.html": {
    "title": "Diverse Cotraining Makes Strong Semi-Supervised Segmentor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijiang Li",
      "Xinjiang Wang",
      "Lihe Yang",
      "Litong Feng",
      "Wayne Zhang",
      "Ying Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Spherical_Space_Feature_Decomposition_for_Guided_Depth_Map_Super-Resolution_ICCV_2023_paper.html": {
    "title": "Spherical Space Feature Decomposition for Guided Depth Map Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixiang Zhao",
      "Jiangshe Zhang",
      "Xiang Gu",
      "Chengli Tan",
      "Shuang Xu",
      "Yulun Zhang",
      "Radu Timofte",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Khan_Tiled_Multiplane_Images_for_Practical_3D_Photography_ICCV_2023_paper.html": {
    "title": "Tiled Multiplane Images for Practical 3D Photography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Numair Khan",
      "Lei Xiao",
      "Douglas Lanman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Wang_VQA-GNN_Reasoning_with_Multimodal_Knowledge_via_Graph_Neural_Networks_for_ICCV_2023_paper.html": {
    "title": "VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks for Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanan Wang",
      "Michihiro Yasunaga",
      "Hongyu Ren",
      "Shinya Wada",
      "Jure Leskovec"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Unmasked_Teacher_Towards_Training-Efficient_Video_Foundation_Models_ICCV_2023_paper.html": {
    "title": "Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunchang Li",
      "Yali Wang",
      "Yizhuo Li",
      "Yi Wang",
      "Yinan He",
      "Limin Wang",
      "Yu Qiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Explore_and_Tell_Embodied_Visual_Captioning_in_3D_Environments_ICCV_2023_paper.html": {
    "title": "Explore and Tell: Embodied Visual Captioning in 3D Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anwen Hu",
      "Shizhe Chen",
      "Liang Zhang",
      "Qin Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Vasu_FastViT_A_Fast_Hybrid_Vision_Transformer_Using_Structural_Reparameterization_ICCV_2023_paper.html": {
    "title": "FastViT: A Fast Hybrid Vision Transformer Using Structural Reparameterization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavan Kumar Anasosalu Vasu",
      "James Gabriel",
      "Jeff Zhu",
      "Oncel Tuzel",
      "Anurag Ranjan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xie_OFVL-MS_Once_for_Visual_Localization_across_Multiple_Indoor_Scenes_ICCV_2023_paper.html": {
    "title": "OFVL-MS: Once for Visual Localization across Multiple Indoor Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Xie",
      "Kun Dai",
      "Siyi Lu",
      "Ke Wang",
      "Zhiqiang Jiang",
      "Jinghan Gao",
      "Dedong Liu",
      "Jie Xu",
      "Lijun Zhao",
      "Ruifeng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_HTML_Hybrid_Temporal-scale_Multimodal_Learning_Framework_for_Referring_Video_Object_ICCV_2023_paper.html": {
    "title": "HTML: Hybrid Temporal-scale Multimodal Learning Framework for Referring Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingfei Han",
      "Yali Wang",
      "Zhihui Li",
      "Lina Yao",
      "Xiaojun Chang",
      "Yu Qiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Fang_SQAD_Automatic_Smartphone_Camera_Quality_Assessment_and_Benchmarking_ICCV_2023_paper.html": {
    "title": "SQAD: Automatic Smartphone Camera Quality Assessment and Benchmarking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilin Fang",
      "Andrey Ignatov",
      "Eduard Zamfir",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_PointDC_Unsupervised_Semantic_Segmentation_of_3D_Point_Clouds_via_Cross-Modal_ICCV_2023_paper.html": {
    "title": "PointDC: Unsupervised Semantic Segmentation of 3D Point Clouds via Cross-Modal Distillation and Super-Voxel Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zisheng Chen",
      "Hongbin Xu",
      "Weitao Chen",
      "Zhipeng Zhou",
      "Haihong Xiao",
      "Baigui Sun",
      "Xuansong Xie",
      "Wenxiong kang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xie_MV-Map_Offboard_HD-Map_Generation_with_Multi-view_Consistency_ICCV_2023_paper.html": {
    "title": "MV-Map: Offboard HD-Map Generation with Multi-view Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Xie",
      "Ziqi Pang",
      "Yu-Xiong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Multi-view_Self-supervised_Disentanglement_for_General_Image_Denoising_ICCV_2023_paper.html": {
    "title": "Multi-view Self-supervised Disentanglement for General Image Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Chen",
      "Chenyuan Qu",
      "Yu Zhang",
      "Chen Chen",
      "Jianbo Jiao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/McIntosh_Inter-Realization_Channels_Unsupervised_Anomaly_Detection_Beyond_One-Class_Classification_ICCV_2023_paper.html": {
    "title": "Inter-Realization Channels: Unsupervised Anomaly Detection Beyond One-Class Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Declan McIntosh",
      "Alexandra Branzan Albu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Multi-Event_Video-Text_Retrieval_ICCV_2023_paper.html": {
    "title": "Multi-Event Video-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gengyuan Zhang",
      "Jisen Ren",
      "Jindong Gu",
      "Volker Tresp"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hu_SHERF_Generalizable_Human_NeRF_from_a_Single_Image_ICCV_2023_paper.html": {
    "title": "SHERF: Generalizable Human NeRF from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoukang Hu",
      "Fangzhou Hong",
      "Liang Pan",
      "Haiyi Mei",
      "Lei Yang",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_MVPSNet_Fast_Generalizable_Multi-view_Photometric_Stereo_ICCV_2023_paper.html": {
    "title": "MVPSNet: Fast Generalizable Multi-view Photometric Stereo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongxu Zhao",
      "Daniel Lichy",
      "Pierre-Nicolas Perrin",
      "Jan-Michael Frahm",
      "Soumyadip Sengupta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Qi_High_Quality_Entity_Segmentation_ICCV_2023_paper.html": {
    "title": "High Quality Entity Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Qi",
      "Jason Kuen",
      "Tiancheng Shen",
      "Jiuxiang Gu",
      "Wenbo Li",
      "Weidong Guo",
      "Jiaya Jia",
      "Zhe Lin",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Tang_CoTDet_Affordance_Knowledge_Prompting_for_Task_Driven_Object_Detection_ICCV_2023_paper.html": {
    "title": "CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajin Tang",
      "Ge Zheng",
      "Jingyi Yu",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Samet_You_Never_Get_a_Second_Chance_To_Make_a_Good_ICCV_2023_paper.html": {
    "title": "You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nermin Samet",
      "Oriane SimÃ©oni",
      "Gilles Puy",
      "Georgy Ponimatkin",
      "Renaud Marlet",
      "Vincent Lepetit"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Scalable_Multi-Temporal_Remote_Sensing_Change_Data_Generation_via_Simulating_Stochastic_ICCV_2023_paper.html": {
    "title": "Scalable Multi-Temporal Remote Sensing Change Data Generation via Simulating Stochastic Change Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuo Zheng",
      "Shiqi Tian",
      "Ailong Ma",
      "Liangpei Zhang",
      "Yanfei Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Human_from_Blur_Human_Pose_Tracking_from_Blurry_Images_ICCV_2023_paper.html": {
    "title": "Human from Blur: Human Pose Tracking from Blurry Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Zhao",
      "Denys Rozumnyi",
      "Jie Song",
      "Otmar Hilliges",
      "Marc Pollefeys",
      "Martin R. Oswald"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_NerfAcc_Efficient_Sampling_Accelerates_NeRFs_ICCV_2023_paper.html": {
    "title": "NerfAcc: Efficient Sampling Accelerates NeRFs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruilong Li",
      "Hang Gao",
      "Matthew Tancik",
      "Angjoo Kanazawa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Colbert_A2Q_Accumulator-Aware_Quantization_with_Guaranteed_Overflow_Avoidance_ICCV_2023_paper.html": {
    "title": "A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian Colbert",
      "Alessandro Pappalardo",
      "Jakoba Petri-Koenig"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Uni-3D_A_Universal_Model_for_Panoptic_3D_Scene_Reconstruction_ICCV_2023_paper.html": {
    "title": "Uni-3D: A Universal Model for Panoptic 3D Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Zhang",
      "Zeyuan Chen",
      "Fangyin Wei",
      "Zhuowen Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Gong_ARNOLD_A_Benchmark_for_Language-Grounded_Task_Learning_with_Continuous_States_ICCV_2023_paper.html": {
    "title": "ARNOLD: A Benchmark for Language-Grounded Task Learning with Continuous States in Realistic 3D Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ran Gong",
      "Jiangyong Huang",
      "Yizhou Zhao",
      "Haoran Geng",
      "Xiaofeng Gao",
      "Qingyang Wu",
      "Wensi Ai",
      "Ziheng Zhou",
      "Demetri Terzopoulos",
      "Song-Chun Zhu",
      "Baoxiong Jia",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Full-Body_Articulated_Human-Object_Interaction_ICCV_2023_paper.html": {
    "title": "Full-Body Articulated Human-Object Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Jiang",
      "Tengyu Liu",
      "Zhexuan Cao",
      "Jieming Cui",
      "Zhiyuan Zhang",
      "Yixin Chen",
      "He Wang",
      "Yixin Zhu",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye_FeatureNeRF_Learning_Generalizable_NeRFs_by_Distilling_Foundation_Models_ICCV_2023_paper.html": {
    "title": "FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianglong Ye",
      "Naiyan Wang",
      "Xiaolong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_SRFormer_Permuted_Self-Attention_for_Single_Image_Super-Resolution_ICCV_2023_paper.html": {
    "title": "SRFormer: Permuted Self-Attention for Single Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yupeng Zhou",
      "Zhen Li",
      "Chun-Le Guo",
      "Song Bai",
      "Ming-Ming Cheng",
      "Qibin Hou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yan_Deep_Homography_Mixture_for_Single_Image_Rolling_Shutter_Correction_ICCV_2023_paper.html": {
    "title": "Deep Homography Mixture for Single Image Rolling Shutter Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weilong Yan",
      "Robby T. Tan",
      "Bing Zeng",
      "Shuaicheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Nugroho_Audio-Visual_Glance_Network_for_Efficient_Video_Recognition_ICCV_2023_paper.html": {
    "title": "Audio-Visual Glance Network for Efficient Video Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Adi Nugroho",
      "Sangmin Woo",
      "Sumin Lee",
      "Changick Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cai_CLNeRF_Continual_Learning_Meets_NeRF_ICCV_2023_paper.html": {
    "title": "CLNeRF: Continual Learning Meets NeRF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Cai",
      "Matthias MÃ¼ller"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Xiang_Rendering_Humans_from_Object-Occluded_Monocular_Videos_ICCV_2023_paper.html": {
    "title": "Rendering Humans from Object-Occluded Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiange Xiang",
      "Adam Sun",
      "Jiajun Wu",
      "Ehsan Adeli",
      "Li Fei-Fei"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yin_CrossMatch_Source-Free_Domain_Adaptive_Semantic_Segmentation_via_Cross-Modal_Consistency_Training_ICCV_2023_paper.html": {
    "title": "CrossMatch: Source-Free Domain Adaptive Semantic Segmentation via Cross-Modal Consistency Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifang Yin",
      "Wenmiao Hu",
      "Zhenguang Liu",
      "Guanfeng Wang",
      "Shili Xiang",
      "Roger Zimmermann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Hornauer_Out-of-Distribution_Detection_for_Monocular_Depth_Estimation_ICCV_2023_paper.html": {
    "title": "Out-of-Distribution Detection for Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julia Hornauer",
      "Adrian Holzbock",
      "Vasileios Belagiannis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Shah_STEPs_Self-Supervised_Key_Step_Extraction_and_Localization_from_Unlabeled_Procedural_ICCV_2023_paper.html": {
    "title": "STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anshul Shah",
      "Benjamin Lundell",
      "Harpreet Sawhney",
      "Rama Chellappa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhong_Improving_Equivariance_in_State-of-the-Art_Supervised_Depth_and_Normal_Predictors_ICCV_2023_paper.html": {
    "title": "Improving Equivariance in State-of-the-Art Supervised Depth and Normal Predictors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyi Zhong",
      "Anand Bhattad",
      "Yu-Xiong Wang",
      "David Forsyth"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Park_Towards_Robust_and_Smooth_3D_Multi-Person_Pose_Estimation_from_Monocular_ICCV_2023_paper.html": {
    "title": "Towards Robust and Smooth 3D Multi-Person Pose Estimation from Monocular Videos in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungchan Park",
      "Eunyi You",
      "Inhoe Lee",
      "Joonseok Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Do_Reducing_Training_Time_in_Cross-Silo_Federated_Learning_Using_Multigraph_Topology_ICCV_2023_paper.html": {
    "title": "Reducing Training Time in Cross-Silo Federated Learning Using Multigraph Topology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuong Do",
      "Binh X. Nguyen",
      "Vuong Pham",
      "Toan Tran",
      "Erman Tjiputra",
      "Quang D. Tran",
      "Anh Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Counting_Crowds_in_Bad_Weather_ICCV_2023_paper.html": {
    "title": "Counting Crowds in Bad Weather",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi-Kai Huang",
      "Wei-Ting Chen",
      "Yuan-Chun Chiang",
      "Sy-Yen Kuo",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yu_FreeDoM_Training-Free_Energy-Guided_Conditional_Diffusion_Model_ICCV_2023_paper.html": {
    "title": "FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiwen Yu",
      "Yinhuai Wang",
      "Chen Zhao",
      "Bernard Ghanem",
      "Jian Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Chen_UniT3D_A_Unified_Transformer_for_3D_Dense_Captioning_and_Visual_ICCV_2023_paper.html": {
    "title": "UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Chen",
      "Ronghang Hu",
      "Xinlei Chen",
      "Matthias NieÃner",
      "Angel X. Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_SKiT_a_Fast_Key_Information_Video_Transformer_for_Online_Surgical_ICCV_2023_paper.html": {
    "title": "SKiT: a Fast Key Information Video Transformer for Online Surgical Phase Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Liu",
      "Jiayu Huo",
      "Jingjing Peng",
      "Rachel Sparks",
      "Prokar Dasgupta",
      "Alejandro Granados",
      "Sebastien Ourselin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Feng_Clustering_based_Point_Cloud_Representation_Learning_for_3D_Analysis_ICCV_2023_paper.html": {
    "title": "Clustering based Point Cloud Representation Learning for 3D Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuo Feng",
      "Wenguan Wang",
      "Xiaohan Wang",
      "Yi Yang",
      "Qinghua Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Automatic_Network_Pruning_via_Hilbert-Schmidt_Independence_Criterion_Lasso_under_Information_ICCV_2023_paper.html": {
    "title": "Automatic Network Pruning via Hilbert-Schmidt Independence Criterion Lasso under Information Bottleneck Principle",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Guo",
      "Lei Zhang",
      "Xiawu Zheng",
      "Yan Wang",
      "Yuchao Li",
      "Fei Chao",
      "Chenglin Wu",
      "Shengchuan Zhang",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_Forecast-MAE_Self-supervised_Pre-training_for_Motion_Forecasting_with_Masked_Autoencoders_ICCV_2023_paper.html": {
    "title": "Forecast-MAE: Self-supervised Pre-training for Motion Forecasting with Masked Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Cheng",
      "Xiaodong Mei",
      "Ming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Efficient_Transformer-based_3D_Object_Detection_with_Dynamic_Token_Halting_ICCV_2023_paper.html": {
    "title": "Efficient Transformer-based 3D Object Detection with Dynamic Token Halting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mao Ye",
      "Gregory P. Meyer",
      "Yuning Chai",
      "Qiang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Han_Neglected_Free_Lunch_-_Learning_Image_Classifiers_Using_Annotation_Byproducts_ICCV_2023_paper.html": {
    "title": "Neglected Free Lunch - Learning Image Classifiers Using Annotation Byproducts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyoon Han",
      "Junsuk Choe",
      "Seonghyeok Chun",
      "John Joon Young Chung",
      "Minsuk Chang",
      "Sangdoo Yun",
      "Jean Y. Song",
      "Seong Joon Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Rethinking_the_Role_of_Pre-Trained_Networks_in_Source-Free_Domain_Adaptation_ICCV_2023_paper.html": {
    "title": "Rethinking the Role of Pre-Trained Networks in Source-Free Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyu Zhang",
      "Li Shen",
      "Chuan-Sheng Foo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_RLIPv2_Fast_Scaling_of_Relational_Language-Image_Pre-Training_ICCV_2023_paper.html": {
    "title": "RLIPv2: Fast Scaling of Relational Language-Image Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hangjie Yuan",
      "Shiwei Zhang",
      "Xiang Wang",
      "Samuel Albanie",
      "Yining Pan",
      "Tao Feng",
      "Jianwen Jiang",
      "Dong Ni",
      "Yingya Zhang",
      "Deli Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Dan_TransFace_Calibrating_Transformer_Training_for_Face_Recognition_from_a_Data-Centric_ICCV_2023_paper.html": {
    "title": "TransFace: Calibrating Transformer Training for Face Recognition from a Data-Centric Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Dan",
      "Yang Liu",
      "Haoyu Xie",
      "Jiankang Deng",
      "Haoran Xie",
      "Xuansong Xie",
      "Baigui Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Song_LLM-Planner_Few-Shot_Grounded_Planning_for_Embodied_Agents_with_Large_Language_ICCV_2023_paper.html": {
    "title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chan Hee Song",
      "Jiaman Wu",
      "Clayton Washington",
      "Brian M Sadler",
      "Wei-Lun Chao",
      "Yu Su"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Li_Exploring_Model_Transferability_through_the_Lens_of_Potential_Energy_ICCV_2023_paper.html": {
    "title": "Exploring Model Transferability through the Lens of Potential Energy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotong Li",
      "Zixuan Hu",
      "Yixiao Ge",
      "Ying Shan",
      "Ling-Yu Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Video_Task_Decathlon_Unifying_Image_and_Video_Tasks_in_Autonomous_ICCV_2023_paper.html": {
    "title": "Video Task Decathlon: Unifying Image and Video Tasks in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas E. Huang",
      "Yifan Liu",
      "Luc Van Gool",
      "Fisher Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Pan_Aria_Digital_Twin_A_New_Benchmark_Dataset_for_Egocentric_3D_ICCV_2023_paper.html": {
    "title": "Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaqing Pan",
      "Nicholas Charron",
      "Yongqian Yang",
      "Scott Peters",
      "Thomas Whelan",
      "Chen Kong",
      "Omkar Parkhi",
      "Richard Newcombe",
      "Yuheng (Carl) Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023/html/Kil_PreSTU_Pre-Training_for_Scene-Text_Understanding_ICCV_2023_paper.html": {
    "title": "PreSTU: Pre-Training for Scene-Text Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihyung Kil",
      "Soravit Changpinyo",
      "Xi Chen",
      "Hexiang Hu",
      "Sebastian Goodman",
      "Wei-Lun Chao",
      "Radu Soricut"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/html/Aflalo_DeepCut_Unsupervised_Segmentation_Using_Graph_Neural_Networks_Clustering_ICCVW_2023_paper.html": {
    "title": "DeepCut: Unsupervised Segmentation Using Graph Neural Networks Clustering",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amit Aflalo",
      "Shai Bagon",
      "Tamar Kashti",
      "Yonina Eldar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/html/Ulger_Relational_Prior_Knowledge_Graphs_for_Detection_and_Instance_Segmentation_ICCVW_2023_paper.html": {
    "title": "Relational Prior Knowledge Graphs for Detection and Instance Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Osman Ãlger",
      "Yu Wang",
      "Ysbrand Galama",
      "Sezer Karaoglu",
      "Theo Gevers",
      "Martin R. Oswald"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/html/Neau_Fine-Grained_is_Too_Coarse_A_Novel_Data-Centric_Approach_for_Efficient_ICCVW_2023_paper.html": {
    "title": "Fine-Grained is Too Coarse: A Novel Data-Centric Approach for Efficient Scene Graph Generation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MaÃ«lic Neau",
      "Paulo E. Santos",
      "Anne-Gwenn Bosser",
      "CÃ©dric Buche"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/html/Gillsjo_Polygon_Detection_for_Room_Layout_Estimation_using_Heterogeneous_Graphs_andWireframes_ICCVW_2023_paper.html": {
    "title": "Polygon Detection for Room Layout Estimation using Heterogeneous Graphs andWireframes",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David GillsjÃ¶",
      "Gabrielle Flood",
      "Kalle ÃstrÃ¶m"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/html/Farshad_SceneGenie_Scene_Graph_Guided_Diffusion_Models_for_Image_Synthesis_ICCVW_2023_paper.html": {
    "title": "SceneGenie: Scene Graph Guided Diffusion Models for Image Synthesis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Azade Farshad",
      "Yousef Yeganeh",
      "Yu Chi",
      "Chengzhi Shen",
      "BÃ¶jrn Ommer",
      "Nassir Navab"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/html/Thauvin_Knowledge_Informed_Sequential_Scene_Graph_Verification_Using_VQA_ICCVW_2023_paper.html": {
    "title": "Knowledge Informed Sequential Scene Graph Verification Using VQA",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dao Thauvin",
      "StÃ©phane Herbin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/html/Holm_Dynamic_Scene_Graph_Representation_for_Surgical_Video_ICCVW_2023_paper.html": {
    "title": "Dynamic Scene Graph Representation for Surgical Video",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Holm",
      "Ghazal Ghazaei",
      "Tobias Czempiel",
      "Ege Ãzsoy",
      "Stefan Saur",
      "Nassir Navab"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/html/Lorenz_Haystack_A_Panoptic_Scene_Graph_Dataset_to_Evaluate_Rare_Predicate_ICCVW_2023_paper.html": {
    "title": "Haystack: A Panoptic Scene Graph Dataset to Evaluate Rare Predicate Classes",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Lorenz",
      "Florian Barthel",
      "Daniel Kienzle",
      "Rainer Lienhart"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/html/Mlodzian_nuScenes_Knowledge_Graph_-_A_Comprehensive_Semantic_Representation_of_Traffic_ICCVW_2023_paper.html": {
    "title": "nuScenes Knowledge Graph - A Comprehensive Semantic Representation of Traffic Scenes for Trajectory Prediction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leon Mlodzian",
      "Zhigang Sun",
      "Hendrik Berkemeyer",
      "Sebastian Monka",
      "Zixu Wang",
      "Stefan Dietze",
      "Lavdim Halilaj",
      "Juergen Luettin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/html/Sun_Exploring_the_Road_Graph_in_Trajectory_Forecasting_for_Autonomous_Driving_ICCVW_2023_paper.html": {
    "title": "Exploring the Road Graph in Trajectory Forecasting for Autonomous Driving",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "RÃ©my Sun",
      "Diane Lingrand",
      "FrÃ©dÃ©ric Precioso"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Edixhoven_Using_and_Abusing_Equivariance_ICCVW_2023_paper.html": {
    "title": "Using and Abusing Equivariance",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Edixhoven",
      "Attila Lengyel",
      "Jan C. van Gemert"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Strafforello_Video_BagNet_Short_Temporal_Receptive_Fields_Increase_Robustness_in_Long-Term_ICCVW_2023_paper.html": {
    "title": "Video BagNet: Short Temporal Receptive Fields Increase Robustness in Long-Term Action Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ombretta Strafforello",
      "Xin Liu",
      "Klamer Schutte",
      "Jan van Gemert"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Daroya_COSE_A_Consistency-Sensitivity_Metric_for_Saliency_on_Image_Classification_ICCVW_2023_paper.html": {
    "title": "COSE: A Consistency-Sensitivity Metric for Saliency on Image Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rangel Daroya",
      "Aaron Sun",
      "Subhransu Maji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Wang_DFM-X_Augmentation_by_Leveraging_Prior_Knowledge_of_Shortcut_Learning_ICCVW_2023_paper.html": {
    "title": "DFM-X: Augmentation by Leveraging Prior Knowledge of Shortcut Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunxin Wang",
      "Christoph Brune",
      "Raymond Veldhuis",
      "Nicola Strisciuglio"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Estepa_Good_Fences_Make_Good_Neighbours_ICCVW_2023_paper.html": {
    "title": "Good Fences Make Good Neighbours",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Imanol G. Estepa",
      "JesÃºs RodrÃ­guez-de-Vera",
      "Bhalaji Nagarajan",
      "Petia Radeva"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Shyam_Data_Efficient_Single_Image_Dehazing_via_Adversarial_Auto-Augmentation_and_Extended_ICCVW_2023_paper.html": {
    "title": "Data Efficient Single Image Dehazing via Adversarial Auto-Augmentation and Extended Atmospheric Scattering Model",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranjay Shyam",
      "HyunJin Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Radwan_Distilling_Part-Whole_Hierarchical_Knowledge_from_a_Huge_Pretrained_Class_Agnostic_ICCVW_2023_paper.html": {
    "title": "Distilling Part-Whole Hierarchical Knowledge from a Huge Pretrained Class Agnostic Segmentation Framework",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Radwan",
      "Mohamed S. Shehata"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Garcia-Gasulla_Padding_Aware_Neurons_ICCVW_2023_paper.html": {
    "title": "Padding Aware Neurons",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dario Garcia-Gasulla",
      "Victor Gimenez-Abalos",
      "Pablo Martin-Torres"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Ganatra_Logarithm-Transform_Aided_Gaussian_Sampling_for_Few-Shot_Learning_ICCVW_2023_paper.html": {
    "title": "Logarithm-Transform Aided Gaussian Sampling for Few-Shot Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaibhav Ganatra"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Mazumder_DeepVAT_A_Self-Supervised_Technique_for_Cluster_Assessment_in_Image_Datasets_ICCVW_2023_paper.html": {
    "title": "DeepVAT: A Self-Supervised Technique for Cluster Assessment in Image Datasets",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alokendu Mazumder",
      "Tirthajit Baruah",
      "Akash Kumar Singh",
      "Pagadala Krishna Murthy",
      "Vishwajeet Pattanaik",
      "Punit Rathore"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Brigato_No_Data_Augmentation_Alternative_Regularizations_for_Effective_Training_on_Small_ICCVW_2023_paper.html": {
    "title": "No Data Augmentation? Alternative Regularizations for Effective Training on Small Datasets",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Brigato",
      "Stavroula Mougiakakou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Nicodemou_RV-VAE_Integrating_Random_Variable_Algebra_into_Variational_Autoencoders_ICCVW_2023_paper.html": {
    "title": "RV-VAE: Integrating Random Variable Algebra into Variational Autoencoders",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vassilis C. Nicodemou",
      "Iason Oikonomidis",
      "Antonis Argyros"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Saha_PARTICLE_Part_Discovery_and_Contrastive_Learning_for_Fine-Grained_Recognition_ICCVW_2023_paper.html": {
    "title": "PARTICLE: Part Discovery and Contrastive Learning for Fine-Grained Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oindrila Saha",
      "Subhransu Maji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Silva_Self-Supervised_Learning_of_Contextualized_Local_Visual_Embeddings_ICCVW_2023_paper.html": {
    "title": "Self-Supervised Learning of Contextualized Local Visual Embeddings",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thalles Silva",
      "Helio Pedrini",
      "AdÃ­n RamÃ­rez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Thopalli_InterAug_A_Tuning-Free_Augmentation_Policy_for_Data-Efficient_and_Robust_Object_ICCVW_2023_paper.html": {
    "title": "InterAug: A Tuning-Free Augmentation Policy for Data-Efficient and Robust Object Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kowshik Thopalli",
      "Devi S",
      "Jayaraman J. Thiagarajan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Cosma_Geometric_Superpixel_Representations_for_Efficient_Image_Classification_with_Graph_Neural_ICCVW_2023_paper.html": {
    "title": "Geometric Superpixel Representations for Efficient Image Classification with Graph Neural Networks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Radu A. Cosma",
      "Lukas Knobel",
      "Putri van der Linden",
      "David M. Knigge",
      "Erik J. Bekkers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VIPriors/html/Koishekenov_Geometric_Contrastive_Learning_ICCVW_2023_paper.html": {
    "title": "Geometric Contrastive Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeskendir Koishekenov",
      "Sharvaree Vadgama",
      "Riccardo Valperga",
      "Erik J. Bekkers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/MMFM/html/Nuthalapati_Coarse_to_Fine_Frame_Selection_for_Online_Open-Ended_Video_Question_ICCVW_2023_paper.html": {
    "title": "Coarse to Fine Frame Selection for Online Open-Ended Video Question Answering",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vidyaranya Nuthalapati",
      "Anirudh Tunga"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/MMFM/html/Pan_Retrieving-to-Answer_Zero-Shot_Video_Question_Answering_with_Frozen_Large_Language_Models_ICCVW_2023_paper.html": {
    "title": "Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junting Pan",
      "Ziyi Lin",
      "Yuying Ge",
      "Xiatian Zhu",
      "Renrui Zhang",
      "Yi Wang",
      "Yu Qiao",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/MMFM/html/Zonneveld_Video-and-Language_VidL_models_and_their_cognitive_relevance_ICCVW_2023_paper.html": {
    "title": "Video-and-Language (VidL) models and their cognitive relevance",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anne Zonneveld",
      "Albert Gatt",
      "Iacer Calixto"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/MMFM/html/Wang_Video_Attribute_Prototype_Network_A_New_Perspective_for_Zero-Shot_Video_ICCVW_2023_paper.html": {
    "title": "Video Attribute Prototype Network: A New Perspective for Zero-Shot Video Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Wang",
      "Kaili Zhao",
      "Hongyang Zhao",
      "Shi Pu",
      "Bo Xiao",
      "Jun Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/MMFM/html/Huang_Interaction-Aware_Prompting_for_Zero-Shot_Spatio-Temporal_Action_Detection_ICCVW_2023_paper.html": {
    "title": "Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Jhe Huang",
      "Jheng-Hsien Yeh",
      "Min-Hung Chen",
      "Gueter Josmy Faure",
      "Shang-Hong Lai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/MMFM/html/Zhong_ClipCrop_Conditioned_Cropping_Driven_by_Vision-Language_Model_ICCVW_2023_paper.html": {
    "title": "ClipCrop: Conditioned Cropping Driven by Vision-Language Model",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihang Zhong",
      "Mingxi Cheng",
      "Zhirong Wu",
      "Yuhui Yuan",
      "Yinqiang Zheng",
      "Ji Li",
      "Han Hu",
      "Stephen Lin",
      "Yoichi Sato",
      "Imari Sato"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/MMFM/html/Salin_Towards_an_Exhaustive_Evaluation_of_Vision-Language_Foundation_Models_ICCVW_2023_paper.html": {
    "title": "Towards an Exhaustive Evaluation of Vision-Language Foundation Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emmanuelle Salin",
      "StÃ©phane Ayache",
      "Benoit Favre"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/MMFM/html/Maniparambil_Enhancing_CLIP_with_GPT-4_Harnessing_Visual_Descriptions_as_Prompts_ICCVW_2023_paper.html": {
    "title": "Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mayug Maniparambil",
      "Chris Vorster",
      "Derek Molloy",
      "Noel Murphy",
      "Kevin McGuinness",
      "Noel E. O'Connor"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/MMFM/html/Pourreza_Painter_Teaching_Auto-Regressive_Language_Models_to_Draw_Sketches_ICCVW_2023_paper.html": {
    "title": "Painter: Teaching Auto-Regressive Language Models to Draw Sketches",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Pourreza",
      "Apratim Bhattacharyya",
      "Sunny Panchal",
      "Mingu Lee",
      "Pulkit Madan",
      "Roland Memisevic"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/DFAD/html/Lorenz_Detecting_Images_Generated_by_Deep_Diffusion_Models_Using_Their_Local_ICCVW_2023_paper.html": {
    "title": "Detecting Images Generated by Deep Diffusion Models Using Their Local Intrinsic Dimensionality",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Lorenz",
      "Ricard L. Durall",
      "Janis Keuper"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/DFAD/html/Balaji_Attending_Generalizability_in_Course_of_Deep_Fake_Detection_by_Exploring_ICCVW_2023_paper.html": {
    "title": "Attending Generalizability in Course of Deep Fake Detection by Exploring Multi-Task Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranav Balaji",
      "Abhijit Das",
      "Srijan Das",
      "Antitza Dantcheva"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/DFAD/html/Nandi_TrainFors_A_Large_Benchmark_Training_Dataset_for_Image_Manipulation_Detection_ICCVW_2023_paper.html": {
    "title": "TrainFors: A Large Benchmark Training Dataset for Image Manipulation Detection and Localization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumyaroop Nandi",
      "Prem Natarajan",
      "Wael Abd-Almageed"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/DFAD/html/Rosberg_FIVA_Facial_Image_and_Video_Anonymization_and_Anonymization_Defense_ICCVW_2023_paper.html": {
    "title": "FIVA: Facial Image and Video Anonymization and Anonymization Defense",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Rosberg",
      "Eren Erdal Aksoy",
      "Cristofer Englund",
      "Fernando Alonso-Fernandez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/DFAD/html/Aghasanli_Interpretable-Through-Prototypes_Deepfake_Detection_for_Diffusion_Models_ICCVW_2023_paper.html": {
    "title": "Interpretable-Through-Prototypes Deepfake Detection for Diffusion Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Agil Aghasanli",
      "Dmitry Kangin",
      "Plamen Angelov"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/DFAD/html/Das_Learning_Interpretable_Forensic_Representations_via_Local_Window_Modulation_ICCVW_2023_paper.html": {
    "title": "Learning Interpretable Forensic Representations via Local Window Modulation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sowmen Das",
      "Md. Ruhul Amin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/DFAD/html/Kamat_Revisiting_Generalizability_in_Deepfake_Detection_Improving_Metrics_and_Stabilizing_Transfer_ICCVW_2023_paper.html": {
    "title": "Revisiting Generalizability in Deepfake Detection: Improving Metrics and Stabilizing Transfer",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarthak Kamat",
      "Shruti Agarwal",
      "Trevor Darrell",
      "Anna Rohrbach"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/DFAD/html/Beuve_WaterLo_Protect_Images_from_Deepfakes_Using_Localized_Semi-Fragile_Watermark_ICCVW_2023_paper.html": {
    "title": "WaterLo: Protect Images from Deepfakes Using Localized Semi-Fragile Watermark",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Beuve",
      "Wassim Hamidouche",
      "Olivier DÃ©forges"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/DFAD/html/Epstein_Online_Detection_of_AI-Generated_Images__ICCVW_2023_paper.html": {
    "title": "Online Detection of AI-Generated Images",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David C. Epstein",
      "Ishan Jain",
      "Oliver Wang",
      "Richard Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/DFAD/html/Husseini_A_Comprehensive_Framework_for_Evaluating_Deepfake_Generators_Dataset_Metrics_Performance_ICCVW_2023_paper.html": {
    "title": "A Comprehensive Framework for Evaluating Deepfake Generators: Dataset, Metrics Performance, and Comparative Analysis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahar Husseini",
      "Jean-Luc Dugelay"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/DFAD/html/Saha_Undercover_Deepfakes_Detecting_Fake_Segments_in_Videos_ICCVW_2023_paper.html": {
    "title": "Undercover Deepfakes: Detecting Fake Segments in Videos",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjay Saha",
      "Rashindrie Perera",
      "Sachith Seneviratne",
      "Tamasha Malepathirana",
      "Sanka Rasnayaka",
      "Deshani Geethika",
      "Terence Sim",
      "Saman Halgamuge"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/DFAD/html/Hamadene_Deepfakes_Signatures_Detection_in_the_Handcrafted_Features_Space_ICCVW_2023_paper.html": {
    "title": "Deepfakes Signatures Detection in the Handcrafted Features Space",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Assia Hamadene",
      "Abdeldjalil Ouahabi",
      "Abdenour Hadid"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Penzel_Analyzing_the_Behavior_of_Cauliflower_Harvest-Readiness_Models_by_Investigating_Feature_ICCVW_2023_paper.html": {
    "title": "Analyzing the Behavior of Cauliflower Harvest-Readiness Models by Investigating Feature Relevances",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niklas Penzel",
      "Jana Kierdorf",
      "Ribana Roscher",
      "Joachim Denzler"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Schauer_Towards_Automated_Regulation_of_Jacobaea_Vulgaris_in_Grassland_Using_Deep_ICCVW_2023_paper.html": {
    "title": "Towards Automated Regulation of Jacobaea Vulgaris in Grassland Using Deep Neural Networks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moritz Schauer",
      "Renke Hohl",
      "Dennis Vaupel",
      "Diethelm Bienhaus",
      "Seyed Eghbal Ghobadi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Casado-Garcia_Estimation_of_Crop_Production_by_Fusing_Images_and_Crop_Features_ICCVW_2023_paper.html": {
    "title": "Estimation of Crop Production by Fusing Images and Crop Features",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ãngela Casado-GarcÃ­a",
      "JÃ³nathan Heras",
      "Jon Miranda-Apodaca",
      "Xabier Simon MartÃ­nez-GoÃ±i",
      "Usue PÃ©rez-LÃ³pez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Lejeune_An_Interpretable_Framework_to_Characterize_Compound_Treatments_on_Filamentous_Fungi_ICCVW_2023_paper.html": {
    "title": "An Interpretable Framework to Characterize Compound Treatments on Filamentous Fungi Using Cell Painting and Deep Metric Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laurent Lejeune",
      "Morgane Roussin",
      "Bruno Leggio",
      "Aurelia Vernay"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Rustia_Rapid_Tomato_DUS_Trait_Analysis_Using_an_Optimized_Mobile-Based_Coarse-to-Fine_ICCVW_2023_paper.html": {
    "title": "Rapid Tomato DUS Trait Analysis Using an Optimized Mobile-Based Coarse-to-Fine Instance Segmentation Algorithm",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Jeric Arcega Rustia",
      "Guido Alexander Jansen",
      "Selwin Hageraats",
      "Joseph Peller",
      "Rick van de Zedde",
      "CÃ©cile Marchennay",
      "Wim Sangster",
      "Gosia Blokker"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Jol_Non-Destructive_Infield_Quality_Estimation_of_Strawberries_Using_Deep_Architectures_ICCVW_2023_paper.html": {
    "title": "Non-Destructive Infield Quality Estimation of Strawberries Using Deep Architectures",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cees Jol",
      "Junhan Wen",
      "Jan van Gemert"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Villalpando_Reinforcement_Learning_with_Space_Carving_for_Plant_Scanning_ICCVW_2023_paper.html": {
    "title": "Reinforcement Learning with Space Carving for Plant Scanning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonio Pico Villalpando",
      "Matthias Kubisch",
      "David Colliaux",
      "Peter Hanappe",
      "Verena V. Hafner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Cherepashkin_Deep_Learning_Based_3d_Reconstruction_for_Phenotyping_of_Wheat_Seeds_ICCVW_2023_paper.html": {
    "title": "Deep Learning Based 3d Reconstruction for Phenotyping of Wheat Seeds: a Dataset, Challenge, and Baseline Method",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vsevolod Cherepashkin",
      "Erenus Yildiz",
      "Andreas Fischbach",
      "Leif Kobbelt",
      "Hanno Scharr"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Celikkan_Semantic_Segmentation_of_Crops_andWeeds_with_Probabilistic_Modeling_and_Uncertainty_ICCVW_2023_paper.html": {
    "title": "Semantic Segmentation of Crops and Weeds with Probabilistic Modeling and Uncertainty Quantification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ekin Celikkan",
      "Mohammadmehdi Saberioon",
      "Martin Herold",
      "Nadja Klein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Sama_A_new_Large_Dataset_and_a_Transfer_Learning_Methodology_for_ICCVW_2023_paper.html": {
    "title": "A new Large Dataset and a Transfer Learning Methodology for Plant Phenotyping in Vertical Farms",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nico Sama",
      "Etienne David",
      "Simone Rossetti",
      "Alessandro Antona",
      "Benjamin Franchetti",
      "Fiora Pirri"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Tausch_Pollinators_as_Data_Collectors_Estimating_Floral_Diversity_with_Bees_and_ICCVW_2023_paper.html": {
    "title": "Pollinators as Data Collectors: Estimating Floral Diversity with Bees and Computer Vision",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederic Tausch",
      "Jan Wagner",
      "Simon Klaus"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Farag_Inductive_Conformal_Prediction_for_Harvest-Readiness_Classification_of_Cauliflower_Plants_A_ICCVW_2023_paper.html": {
    "title": "Inductive Conformal Prediction for Harvest-Readiness Classification of Cauliflower Plants: A Comparative Study of Uncertainty Quantification Methods",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Farag",
      "Jana Kierdorf",
      "Ribana Roscher"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Melki_Group-Conditional_Conformal_Prediction_via_Quantile_Regression_Calibration_for_Crop_and_ICCVW_2023_paper.html": {
    "title": "Group-Conditional Conformal Prediction via Quantile Regression Calibration for Crop and Weed Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Melki",
      "Lionel Bombrun",
      "Boubacar Diallo",
      "JÃ©rÃ´me Dias",
      "Jean-Pierre Da Costa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Wagner_Vision-Based_Monitoring_of_the_Short-Term_Dynamic_Behaviour_of_Plants_for_ICCVW_2023_paper.html": {
    "title": "Vision-Based Monitoring of the Short-Term Dynamic Behaviour of Plants for Automated Phenotyping",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolaus Wagner",
      "Grzegorz Cielniak"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Engstrom_Improving_Deep_Learning_on_Hyperspectral_Images_of_Grain_by_Incorporating_ICCVW_2023_paper.html": {
    "title": "Improving Deep Learning on Hyperspectral Images of Grain by Incorporating Domain Knowledge from Chemometrics",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ole-Christian Galbo EngstrÃ¸m",
      "Erik Schou Dreier",
      "Birthe MÃ¸ller Jespersen",
      "Kim Steenstrup Pedersen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Chen_Adapting_Vision_Foundation_Models_for_Plant_Phenotyping_ICCVW_2023_paper.html": {
    "title": "Adapting Vision Foundation Models for Plant Phenotyping",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Chen",
      "Mario Valerio Giuffrida",
      "Sotirios A. Tsaftaris"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Amine_Embedded_Plant_Recognition_A_Benchmark_for_low_Footprint_Deep_Neural_ICCVW_2023_paper.html": {
    "title": "Embedded Plant Recognition: A Benchmark for low Footprint Deep Neural Networks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sehaba Mohammed El Amine",
      "Crispim-Junior Carlos",
      "Tougne Rodet Laure"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Tempelaere_Deep_Learning_for_Apple_Fruit_Quality_Inspection_Using_X-Ray_Imaging_ICCVW_2023_paper.html": {
    "title": "Deep Learning for Apple Fruit Quality Inspection Using X-Ray Imaging",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Astrid Tempelaere",
      "Leen Van Doorselaer",
      "Jiaqi He",
      "Pieter Verboven",
      "Tinne Tuytelaars",
      "Bart Nicolai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Najafian_Detection_of_Fusarium_Damaged_Kernels_in_Wheat_Using_Deep_Semi-Supervised_ICCVW_2023_paper.html": {
    "title": "Detection of Fusarium Damaged Kernels in Wheat Using Deep Semi-Supervised Learning on a Novel WheatSeedBelt Dataset",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keyhan Najafian",
      "Lingling Jin",
      "H. Randy Kutcher",
      "Mackenzie Hladun",
      "Samuel Horovatin",
      "Maria Alejandra Oviedo-Ludena",
      "Sheila Maria Pereira de Andrade",
      "Lipu Wang",
      "Ian Stavness"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Korschens_Unified_Automatic_Plant_Cover_and_Phenology_Prediction_ICCVW_2023_paper.html": {
    "title": "Unified Automatic Plant Cover and Phenology Prediction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthias KÃ¶rschens",
      "Solveig Franziska Bucher",
      "Christine RÃ¶mermann",
      "Joachim Denzler"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Wang_Weed_Mapping_with_Convolutional_Neural_Networks_on_High_Resolution_Whole-Field_ICCVW_2023_paper.html": {
    "title": "Weed Mapping with Convolutional Neural Networks on High Resolution Whole-Field Images",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuemin Wang",
      "Thuan Ha",
      "Kathryn Aldridge",
      "Hema Duddu",
      "Steve Shirtliffe",
      "Ian Stavness"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Hartley_Unlocking_Comparative_Plant_Scoring_with_Siamese_Neural_Networks_and_Pairwise_ICCVW_2023_paper.html": {
    "title": "Unlocking Comparative Plant Scoring with Siamese Neural Networks and Pairwise Pseudo Labelling",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zane K. J. Hartley",
      "Rob J. Lind",
      "Nicholas Smith",
      "Bob Collison",
      "Andrew P. French"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Page-Fortin_Class-Incremental_Learning_of_Plant_and_Disease_Detection_Growing_Branches_with_ICCVW_2023_paper.html": {
    "title": "Class-Incremental Learning of Plant and Disease Detection: Growing Branches with Knowledge Distillation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathieu PagÃ©-Fortin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Gentilhomme_Efficient_Grapevine_Structure_Estimation_in_Vineyards_Conditions_ICCVW_2023_paper.html": {
    "title": "Efficient Grapevine Structure Estimation in Vineyards Conditions",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ThÃ©ophile Gentilhomme",
      "Michael Villamizar",
      "Jerome Corre",
      "Jean-Marc Odobez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Song_Plant_Root_Occlusion_Inpainting_with_Generative_Adversarial_Network_ICCVW_2023_paper.html": {
    "title": "Plant Root Occlusion Inpainting with Generative Adversarial Network",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Song",
      "Karim Panjvani",
      "Zhigang Liu",
      "Huzaifa Amar",
      "Leon Kochian",
      "Shengjian Ye",
      "Xuan Yang",
      "J. Allan Feurtado",
      "Krunal Chavda",
      "Karina Angela Chimbo Huatatoca",
      "Mark Eramian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Englebert_Explaining_Through_Transformer_Input_Sampling_ICCVW_2023_paper.html": {
    "title": "Explaining Through Transformer Input Sampling",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Englebert",
      "SÃ©drick Stassin",
      "GÃ©raldin Nanfack",
      "Sidi Ahmed Mahmoudi",
      "Xavier Siebert",
      "Olivier Cornu",
      "Christophe De Vleeschouwer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Mondal_Actor-Agnostic_Multi-Label_Action_Recognition_with_Multi-Modal_Query_ICCVW_2023_paper.html": {
    "title": "Actor-Agnostic Multi-Label Action Recognition with Multi-Modal Query",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anindya Mondal",
      "Sauradip Nag",
      "Joaquin M Prada",
      "Xiatian Zhu",
      "Anjan Dutta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Sun_All-pairs_Consistency_Learning_forWeakly_Supervised_Semantic_Segmentation_ICCVW_2023_paper.html": {
    "title": "All-pairs Consistency Learning forWeakly Supervised Semantic Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixuan Sun",
      "Yanhao Zhang",
      "Zhen Qin",
      "Zheyuan Liu",
      "Lin Cheng",
      "Fanyi Wang",
      "Yiran Zhong",
      "Nick Barnes"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Wang_Dual-Contrastive_Dual-Consistency_Dual-Transformer_A_Semi-Supervised_Approach_to_Medical_Image_Segmentation_ICCVW_2023_paper.html": {
    "title": "Dual-Contrastive Dual-Consistency Dual-Transformer: A Semi-Supervised Approach to Medical Image Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Wang",
      "Congying Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Djenouri_A_Hybrid_Visual_Transformer_for_Efficient_Deep_Human_Activity_Recognition_ICCVW_2023_paper.html": {
    "title": "A Hybrid Visual Transformer for Efficient Deep Human Activity Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youcef Djenouri",
      "Ahmed Nabil Belbachir"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Haurum_Which_Tokens_to_Use_Investigating_Token_Reduction_in_Vision_Transformers_ICCVW_2023_paper.html": {
    "title": "Which Tokens to Use? Investigating Token Reduction in Vision Transformers",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joakim Bruslund Haurum",
      "Sergio Escalera",
      "Graham W. Taylor",
      "Thomas B. Moeslund"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Yoo_Hierarchical_Spatiotemporal_Transformers_for_Video_Object_Segmentation_ICCVW_2023_paper.html": {
    "title": "Hierarchical Spatiotemporal Transformers for Video Object Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Sang Yoo",
      "Hongjae Lee",
      "Seung-Won Jung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Das_IDTransformer_Transformer_for_Intrinsic_Image_Decomposition_ICCVW_2023_paper.html": {
    "title": "IDTransformer: Transformer for Intrinsic Image Decomposition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Partha Das",
      "Maxime Gevers",
      "Sezer Karaoglu",
      "Theo Gevers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Havtorn_MSViT_Dynamic_Mixed-Scale_Tokenization_for_Vision_Transformers_ICCVW_2023_paper.html": {
    "title": "MSViT: Dynamic Mixed-Scale Tokenization for Vision Transformers",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jakob Drachmann Havtorn",
      "AmÃ©lie Royer",
      "Tijmen Blankevoort",
      "Babak Ehteshami Bejnordi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Hertlein_Template-Guided_Illumination_Correction_for_Document_Images_with_Imperfect_Geometric_Reconstruction_ICCVW_2023_paper.html": {
    "title": "Template-Guided Illumination Correction for Document Images with Imperfect Geometric Reconstruction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Hertlein",
      "Alexander Naumann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Diba_Spatio-Temporal_Convolution-Attention_Video_Network_ICCVW_2023_paper.html": {
    "title": "Spatio-Temporal Convolution-Attention Video Network",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Diba",
      "Vivek Sharma",
      "Mohammad.M Arzani",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Sekhar_TSOSVNet_Teacher-Student_Collaborative_Knowledge_Distillation_for_Online_Signature_Verification_ICCVW_2023_paper.html": {
    "title": "TSOSVNet: Teacher-Student Collaborative Knowledge Distillation for Online Signature Verification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chandra Sekhar V",
      "Avinash Gautam",
      "Viswanath P",
      "Sreeja SR",
      "Rama Krishna Sai G"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Jain_SeMask_Semantically_Masked_Transformers_for_Semantic_Segmentation_ICCVW_2023_paper.html": {
    "title": "SeMask: Semantically Masked Transformers for Semantic Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jitesh Jain",
      "Anukriti Singh",
      "Nikita Orlov",
      "Zilong Huang",
      "Jiachen Li",
      "Steven Walton",
      "Humphrey Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Shamsolmoali_TransInpaint_Transformer-Based_Image_Inpainting_with_Context_Adaptation_ICCVW_2023_paper.html": {
    "title": "TransInpaint: Transformer-Based Image Inpainting with Context Adaptation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pourya Shamsolmoali",
      "Masoumeh Zareapoor",
      "Eric Granger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Li_Interactive_Image_Segmentation_with_Cross-Modality_Vision_Transformers_ICCVW_2023_paper.html": {
    "title": "Interactive Image Segmentation with Cross-Modality Vision Transformers",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Li",
      "George Vosselman",
      "Michael Ying Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Ganugula_MOSAIC_Multi-Object_Segmented_Arbitrary_Stylization_Using_CLIP_ICCVW_2023_paper.html": {
    "title": "MOSAIC: Multi-Object Segmented Arbitrary Stylization Using CLIP",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prajwal Ganugula",
      "Y S S S Santosh Kumar",
      "N K Sagar Reddy",
      "Prabhath Chellingi",
      "Avinash Thakur",
      "Neeraj Kasera",
      "C Shyam Anand"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Homeyer_On_Moving_Object_Segmentation_from_Monocular_Video_with_Transformers_ICCVW_2023_paper.html": {
    "title": "On Moving Object Segmentation from Monocular Video with Transformers",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Homeyer",
      "Christoph SchnÃ¶rr"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Wang_SCSC_Spatial_Cross-Scale_Convolution_Module_to_Strengthen_Both_CNNs_and_ICCVW_2023_paper.html": {
    "title": "SCSC: Spatial Cross-Scale Convolution Module to Strengthen Both CNNs and Transformers",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xijun Wang",
      "Xiaojie Chu",
      "Chunrui Han",
      "Xiangyu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Lim_Image_Guided_Inpainting_with_Parameter_Efficient_Learning_ICCVW_2023_paper.html": {
    "title": "Image Guided Inpainting with Parameter Efficient Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangbeom Lim",
      "Seungryong Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Park_Augmenting_Features_via_Contrastive_Learning-Based_Generative_Model_for_Long-Tailed_Classification_ICCVW_2023_paper.html": {
    "title": "Augmenting Features via Contrastive Learning-Based Generative Model for Long-Tailed Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minho Park",
      "Hyung-Il Kim",
      "Hwa Jeon Song",
      "Dong-oh Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Kender_G2L_A_High-Dimensional_Geometric_Approach_for_Automatic_Generation_of_Highly_ICCVW_2023_paper.html": {
    "title": "G2L: A High-Dimensional Geometric Approach for Automatic Generation of Highly Accurate Pseudo-Labels",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John R. Kender",
      "Parijat Dube",
      "Zhengyang Han",
      "Bishwaranjan Bhattacharjee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Marcu_Self-Supervised_Hypergraphs_for_Learning_Multiple_World_Interpretations_ICCVW_2023_paper.html": {
    "title": "Self-Supervised Hypergraphs for Learning Multiple World Interpretations",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alina Marcu",
      "Mihai Pirvu",
      "Dragos Costea",
      "Emanuela Haller",
      "Emil Slusanschi",
      "Ahmed Nabil Belbachir",
      "Rahul Sukthankar",
      "Marius Leordeanu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Kwarciak_Deep_Generative_Networks_for_Heterogeneous_Augmentation_of_Cranial_Defects_ICCVW_2023_paper.html": {
    "title": "Deep Generative Networks for Heterogeneous Augmentation of Cranial Defects",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kamil Kwarciak",
      "Marek WodziÅski"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Reichardt_360deg_from_a_Single_Camera_A_Few-Shot_Approach_for_LiDAR_ICCVW_2023_paper.html": {
    "title": "360deg from a Single Camera: A Few-Shot Approach for LiDAR Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laurenz Reichardt",
      "Nikolas Ebert",
      "Oliver WasenmÃ¼ller"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Vandeghen_Adaptive_Self-Training_for_Object_Detection_ICCVW_2023_paper.html": {
    "title": "Adaptive Self-Training for Object Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renaud Vandeghen",
      "Gilles Louppe",
      "Marc Van Droogenbroeck"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Lee_Recognition-Friendly_Industrial_Image_Generation_for_Defect_Recognition_Under_Limited_Data_ICCVW_2023_paper.html": {
    "title": "Recognition-Friendly Industrial Image Generation for Defect Recognition Under Limited Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Younkwan Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Psaltis_FedLID_Self-Supervised_Federated_Learning_for_Leveraging_Limited_Image_Data_ICCVW_2023_paper.html": {
    "title": "FedLID: Self-Supervised Federated Learning for Leveraging Limited Image Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Athanasios Psaltis",
      "Anestis Kastellos",
      "Charalampos Z. Patrikakis",
      "Petros Daras"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Sosa_A_Horse_with_no_Labels_Self-Supervised_Horse_Pose_Estimation_from_ICCVW_2023_paper.html": {
    "title": "A Horse with no Labels: Self-Supervised Horse Pose Estimation from Unlabelled Images and Synthetic Prior",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jose Sosa",
      "David Hogg"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Nguyen_Boosting_Semi-Supervised_Learning_by_Bridging_high_and_low-Confidence_Predictions_ICCVW_2023_paper.html": {
    "title": "Boosting Semi-Supervised Learning by Bridging high and low-Confidence Predictions",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khanh-Binh Nguyen",
      "Joon-Sung Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Dawoud_SelectNAdapt_Support_Set_Selection_for_Few-Shot_Domain_Adaptation_ICCVW_2023_paper.html": {
    "title": "SelectNAdapt: Support Set Selection for Few-Shot Domain Adaptation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youssef Dawoud",
      "Gustavo Carneiro",
      "Vasileios Belagiannis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Bao_MIAD_A_Maintenance_Inspection_Dataset_for_Unsupervised_Anomaly_Detection_ICCVW_2023_paper.html": {
    "title": "MIAD: A Maintenance Inspection Dataset for Unsupervised Anomaly Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianpeng Bao",
      "Jiadong Chen",
      "Wei Li",
      "Xiang Wang",
      "Jingjing Fei",
      "Liwei Wu",
      "Rui Zhao",
      "Ye Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Hong_Enhancing_Classification_Accuracy_on_Limited_Data_via_Unconditional_GAN_ICCVW_2023_paper.html": {
    "title": "Enhancing Classification Accuracy on Limited Data via Unconditional GAN",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunsan Hong",
      "Byunghee Cha",
      "Bohyung Kim",
      "Tae-Hyun Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Le_Self-Training_and_Multi-Task_Learning_for_Limited_Data_Evaluation_Study_on_ICCVW_2023_paper.html": {
    "title": "Self-Training and Multi-Task Learning for Limited Data: Evaluation Study on Object Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HoÃ ng-Ãn LÃª",
      "Minh-Tan Pham"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Bicsi_JEDI_Joint_Expert_Distillation_in_a_Semi-Supervised_Multi-Dataset_Student-Teacher_Scenario_ICCVW_2023_paper.html": {
    "title": "JEDI: Joint Expert Distillation in a Semi-Supervised Multi-Dataset Student-Teacher Scenario for Video Action Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucian Bicsi",
      "Bogdan Alexe",
      "Radu Tudor Ionescu",
      "Marius Leordeanu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Li_Semantic_RGB-D_Image_Synthesis_ICCVW_2023_paper.html": {
    "title": "Semantic RGB-D Image Synthesis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Li",
      "Rong Li",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Shtedritski_Learning_Universal_Semantic_Correspondences_with_No_Supervision_and_Automatic_Data_ICCVW_2023_paper.html": {
    "title": "Learning Universal Semantic Correspondences with No Supervision and Automatic Data Curation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandar Shtedritski",
      "Andrea Vedaldi",
      "Christian Rupprecht"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Takenaka_Guiding_Video_Prediction_with_Explicit_Procedural_Knowledge_ICCVW_2023_paper.html": {
    "title": "Guiding Video Prediction with Explicit Procedural Knowledge",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Takenaka",
      "Johannes Maucher",
      "Marco F. Huber"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Lin_Frequency-Aware_Self-Supervised_Long-Tailed_Learning_ICCVW_2023_paper.html": {
    "title": "Frequency-Aware Self-Supervised Long-Tailed Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ci-Siang Lin",
      "Min-Hung Chen",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/html/Sharma_Tensor_Factorization_for_Leveraging_Cross-Modal_Knowledge_in_Data-Constrained_Infrared_Object_ICCVW_2023_paper.html": {
    "title": "Tensor Factorization for Leveraging Cross-Modal Knowledge in Data-Constrained Infrared Object Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manish Sharma",
      "Moitreya Chatterjee",
      "Kuan-Chuan Peng",
      "Suhas Lohit",
      "Michael Jones"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RHWC/html/Zheng_ILSH_The_Imperial_Light-Stage_Head_Dataset_for_Human_Head_View_ICCVW_2023_paper.html": {
    "title": "ILSH: The Imperial Light-Stage Head Dataset for Human Head View Synthesis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiali Zheng",
      "Youngkyoon Jang",
      "Athanasios Papaioannou",
      "Christos Kampouris",
      "Rolandos Alexandros Potamias",
      "Foivos Paraperas Papantoniou",
      "Efstathios Galanakis",
      "AleÅ¡ Leonardis",
      "Stefanos Zafeiriou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RHWC/html/Jang_VSCHH_2023_A_Benchmark_for_the_View_Synthesis_Challenge_of_ICCVW_2023_paper.html": {
    "title": "VSCHH 2023: A Benchmark for the View Synthesis Challenge of Human Heads",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngkyoon Jang",
      "Jiali Zheng",
      "Jifei Song",
      "Helisa Dhamo",
      "Eduardo PÃ©rez-Pellitero",
      "Thomas Tanay",
      "Matteo Maggioni",
      "Richard Shaw",
      "Sibi Catley-Chandar",
      "Yiren Zhou",
      "Jiankang Deng",
      "Ruijie Zhu",
      "Jiahao Chang",
      "Ziyang Song",
      "Jiahuan Yu",
      "Tianzhu Zhang",
      "Khanh-Binh Nguyen",
      "Joon-Sung Yang",
      "Andreea Dogaru",
      "Bernhard Egger",
      "Heng Yu",
      "Aarush Gupta",
      "Joel Julin",
      "LÃ¡szlÃ³ A. Jeni",
      "Hyeseong Kim",
      "Jungbin Cho",
      "Dosik Hwang",
      "Deukhee Lee",
      "Doyeon Kim",
      "Dongseong Seo",
      "SeungJin Jeon",
      "YoungDon Choi",
      "Jun Seok Kang",
      "Ahmet Cagatay Seker",
      "Sang Chul Ahn",
      "Ales Leonardis",
      "Stefanos Zafeiriou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Soria_Tiny_and_Efficient_Model_for_the_Edge_Detection_Generalization_ICCVW_2023_paper.html": {
    "title": "Tiny and Efficient Model for the Edge Detection Generalization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xavier Soria",
      "Yachuan Li",
      "Mohammad Rouhani",
      "Angel D. Sappa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Djenouri_Shapley_Deep_Learning_A_Consensus_for_General-Purpose_Vision_Systems_ICCVW_2023_paper.html": {
    "title": "Shapley Deep Learning: A Consensus for General-Purpose Vision Systems",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youcef Djenouri",
      "Ahmed Nabil Belbachir",
      "Tomasz Michalak",
      "Anis Yazidi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Liu_A_Simple_and_Generic_Framework_for_Feature_Distillation_via_Channel-Wise_ICCVW_2023_paper.html": {
    "title": "A Simple and Generic Framework for Feature Distillation via Channel-Wise Transformation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziwei Liu",
      "Yongtao Wang",
      "Xiaojie Chu",
      "Nan Dong",
      "Shengxiang Qi",
      "Haibin Ling"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Lazarevich_YOLOBench_Benchmarking_Efficient_Object_Detectors_on_Embedded_Systems_ICCVW_2023_paper.html": {
    "title": "YOLOBench: Benchmarking Efficient Object Detectors on Embedded Systems",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Lazarevich",
      "Matteo Grimaldi",
      "Ravish Kumar",
      "Saptarshi Mitra",
      "Shahrukh Khan",
      "Sudhakar Sah"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Priyadarshi_DONNAv2_-_Lightweight_Neural_Architecture_Search_for_Vision_Tasks_ICCVW_2023_paper.html": {
    "title": "DONNAv2 - Lightweight Neural Architecture Search for Vision Tasks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sweta Priyadarshi",
      "Tianyu Jiang",
      "Hsin-Pai Cheng",
      "Sendil Krishna",
      "Viswanath Ganapathy",
      "Chirag Patel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Chen_MOFA_A_Model_Simplification_Roadmap_for_Image_Restoration_on_Mobile_ICCVW_2023_paper.html": {
    "title": "MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Chen",
      "Ruiwen Zhen",
      "Shuai Li",
      "Xiaotian Li",
      "Guanghui Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Pandey_Softmax_Bias_Correction_for_Quantized_Generative_Models_ICCVW_2023_paper.html": {
    "title": "Softmax Bias Correction for Quantized Generative Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nilesh Prasad Pandey",
      "Marios Fournarakis",
      "Chirag Patel",
      "Markus Nagel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Pegeot_A_Comprehensive_Study_of_Transfer_Learning_Under_Constraints_ICCVW_2023_paper.html": {
    "title": "A Comprehensive Study of Transfer Learning Under Constraints",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom PÃ©geot",
      "Inna Kucher",
      "Adrian Popescu",
      "Bertrand Delezoide"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Kohama_Single-Shot_Pruning_for_Pre-Trained_Models_Rethinking_the_Importance_of_Magnitude_ICCVW_2023_paper.html": {
    "title": "Single-Shot Pruning for Pre-Trained Models: Rethinking the Importance of Magnitude Pruning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hirokazu Kohama",
      "Hiroaki Minoura",
      "Tsubasa Hirakawa",
      "Takayoshi Yamashita",
      "Hironobu Fujiyoshi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Sakuma_DetOFA_Efficient_Training_of_Once-for-All_Networks_for_Object_Detection_Using_ICCVW_2023_paper.html": {
    "title": "DetOFA: Efficient Training of Once-for-All Networks for Object Detection Using Path Filter",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuiko Sakuma",
      "Masato Ishii",
      "Takuya Narihira"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Bhardwaj_ZiCo-BC_A_Bias_Corrected_Zero-Shot_NAS_for_Vision_Tasks_ICCVW_2023_paper.html": {
    "title": "ZiCo-BC: A Bias Corrected Zero-Shot NAS for Vision Tasks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartikeya Bhardwaj",
      "Hsin-Pai Cheng",
      "Sweta Priyadarshi",
      "Zhuojin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Martins_Ray-Patch_An_Efficient_Querying_for_Light_Field_Transformers_ICCVW_2023_paper.html": {
    "title": "Ray-Patch: An Efficient Querying for Light Field Transformers",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "TomÃ¡s Berriel Martins",
      "Javier Civera"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Tiwari_RCV2023_Challenges_Benchmarking_Model_Training_and_Inference_for_Resource-Constrained_Deep_ICCVW_2023_paper.html": {
    "title": "RCV2023 Challenges: Benchmarking Model Training and Inference for Resource-Constrained Deep Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishabh Tiwari",
      "Arnav Chavan",
      "Deepak Gupta",
      "Gowreesh Mago",
      "Animesh Gupta",
      "Akash Gupta",
      "Suraj Sharan",
      "Yukun Yang",
      "Shanwei Zhao",
      "Shihao Wang",
      "Youngjun Kwak",
      "Seonghun Jeong",
      "Yunseung Lee",
      "Changick Kim",
      "Subin Kim",
      "Ganzorig Gankhuyag",
      "Ho Jung",
      "Junwhan Ryu",
      "HaeMoon Kim",
      "Byeong H. Kim",
      "Tu Vo",
      "Sheir Zaheer",
      "Alexander Holston",
      "Chan Park",
      "Dheemant Dixit",
      "Nahush Lele",
      "Kushagra Bhushan",
      "Debjani Bhowmick",
      "Devanshu Arya",
      "Sadaf Gulshad",
      "Amirhossein Habibian",
      "Amir Ghodrati",
      "Babak Bejnordi",
      "Jai Gupta",
      "Zhuang Liu",
      "Jiahui Yu",
      "Dilip Prasad",
      "Zhiqiang Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Chauhan_Post_Training_Mixed_Precision_Quantization_of_Neural_Networks_Using_First-Order_ICCVW_2023_paper.html": {
    "title": "Post Training Mixed Precision Quantization of Neural Networks Using First-Order Information",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arun Chauhan",
      "Utsav Tiwari",
      "Vikram N R"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Glandorf_HyperSparse_Neural_Networks_Shifting_Exploration_to_Exploitation_Through_Adaptive_Regularization_ICCVW_2023_paper.html": {
    "title": "HyperSparse Neural Networks: Shifting Exploration to Exploitation Through Adaptive Regularization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Glandorf",
      "Timo Kaiser",
      "Bodo Rosenhahn"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Biswas_Characterizing_Face_Recognition_for_Resource_Efficient_Deployment_on_Edge_ICCVW_2023_paper.html": {
    "title": "Characterizing Face Recognition for Resource Efficient Deployment on Edge",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayan Biswas",
      "Sai Amrit Patnaik",
      "A. H. Abdul Hafez",
      "Anoop M. Namboodiri"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Soro_Enhancing_Differentiable_Architecture_Search_A_Study_on_Small_Number_of_ICCVW_2023_paper.html": {
    "title": "Enhancing Differentiable Architecture Search: A Study on Small Number of Cell Blocks in the Search Stage, and Important Branches-Based Cells Selection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bedionita Soro",
      "Chong Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Tran_Fast_Object_Detection_in_High-Resolution_Videos_ICCVW_2023_paper.html": {
    "title": "Fast Object Detection in High-Resolution Videos",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Tran",
      "Atul Kanaujia",
      "Vasu Parameswaran"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Haque_AntiNODE_Evaluating_Efficiency_Robustness_of_Neural_ODEs_ICCVW_2023_paper.html": {
    "title": "AntiNODE: Evaluating Efficiency Robustness of Neural ODEs",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mirazul Haque",
      "Simin Chen",
      "Wasif Haque",
      "Cong Liu",
      "Wei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Abraham_NCQS_Nonlinear_Convex_Quadrature_Surrogate_Hyperparameter_Optimization_ICCVW_2023_paper.html": {
    "title": "NCQS: Nonlinear Convex Quadrature Surrogate Hyperparameter Optimization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sophia Abraham",
      "Kehelwala Dewage Gayan Maduranga",
      "Jeffery Kinnison",
      "Jonathan Hauenstein",
      "Walter Scheirer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Jordao_When_Layers_Play_the_Lottery_all_Tickets_Win_at_Initialization_ICCVW_2023_paper.html": {
    "title": "When Layers Play the Lottery, all Tickets Win at Initialization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artur Jordao",
      "George de AraÃºjo",
      "Helena de Almeida Maia",
      "Helio Pedrini"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Sridhar_InstaTune_Instantaneous_Neural_Architecture_Search_During_Fine-Tuning_ICCVW_2023_paper.html": {
    "title": "InstaTune: Instantaneous Neural Architecture Search During Fine-Tuning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharath Nittur Sridhar",
      "Souvik Kundu",
      "Sairam Sundaresan",
      "Maciej Szankin",
      "Anthony Sarah"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Kinli_Deterministic_Neural_Illumination_Mapping_for_Efficient_Auto-White_Balance_Correction_ICCVW_2023_paper.html": {
    "title": "Deterministic Neural Illumination Mapping for Efficient Auto-White Balance Correction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Furkan KÄ±nlÄ±",
      "DoÄa YÄ±lmaz",
      "BarÄ±Å Ãzcan",
      "Furkan KÄ±raÃ§"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Afham_Revisiting_Kernel_Temporal_Segmentation_as_an_Adaptive_Tokenizer_for_Long-form_ICCVW_2023_paper.html": {
    "title": "Revisiting Kernel Temporal Segmentation as an Adaptive Tokenizer for Long-form Video Understanding",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Afham",
      "Satya Narayan Shukla",
      "Omid Poursaeed",
      "Pengchuan Zhang",
      "Ashish Shah",
      "Sernam Lim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Cuttano_Cross-Domain_Transfer_Learning_with_CoRTe_Consistent_and_Reliable_Transfer_from_ICCVW_2023_paper.html": {
    "title": "Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Claudia Cuttano",
      "Antonio Tavera",
      "Fabio Cermelli",
      "Giuseppe Averta",
      "Barbara Caputo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/van_den_Dool_Efficient_Neural_PDE-Solvers_Using_Quantization_Aware_Training_ICCVW_2023_paper.html": {
    "title": "Efficient Neural PDE-Solvers Using Quantization Aware Training",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Winfried van den Dool",
      "Tijmen Blankevoort",
      "Max Welling",
      "Yuki Asano"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Trinci_Cross-Model_Temporal_Cooperation_via_Saliency_Maps_for_Efficient_Frame_Classification_ICCVW_2023_paper.html": {
    "title": "Cross-Model Temporal Cooperation via Saliency Maps for Efficient Frame Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomaso Trinci",
      "Tommaso Bianconcini",
      "Leonardo Sarti",
      "Leonardo Taccari",
      "Francesco Sambo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Shahabinejad_Video_Action_Recognition_with_Adaptive_Zooming_Using_Motion_Residuals_ICCVW_2023_paper.html": {
    "title": "Video Action Recognition with Adaptive Zooming Using Motion Residuals",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mostafa Shahabinejad",
      "Irina Kezele",
      "Seyed Shahabeddin Nabavi",
      "Wentao Liu",
      "Seel Patel",
      "Yuanhao Yu",
      "Yang Wang",
      "Jin Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Li_SCoTTi_Save_Computation_at_Training_Time_with_an_Adaptive_Framework_ICCVW_2023_paper.html": {
    "title": "SCoTTi: Save Computation at Training Time with an Adaptive Framework",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Li",
      "Enzo Tartaglione",
      "Van-Tam Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Bifis_Developing_Robust_and_Lightweight_Adversarial_Defenders_by_Enforcing_Orthogonality_on_ICCVW_2023_paper.html": {
    "title": "Developing Robust and Lightweight Adversarial Defenders by Enforcing Orthogonality on Attack-Agnostic Denoising Autoencoders",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aristeidis Bifis",
      "Emmanouil Z. Psarakis",
      "Dimitrios Kosmopoulos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Rossigneux_Surround_the_Nonlinearity_Inserting_Foldable_Convolutional_Autoencoders_to_Reduce_Activation_ICCVW_2023_paper.html": {
    "title": "Surround the Nonlinearity: Inserting Foldable Convolutional Autoencoders to Reduce Activation Footprint",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baptiste Rossigneux",
      "Inna Kucher",
      "Vincent Lorrain",
      "Emmanuel Casseau"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Liao_Can_Unstructured_Pruning_Reduce_the_Depth_in_Deep_Neural_Networks_ICCVW_2023_paper.html": {
    "title": "Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhu Liao",
      "Victor QuÃ©tu",
      "Van-Tam Nguyen",
      "Enzo Tartaglione"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Gao_Accumulation_Knowledge_Distillation_for_Conditional_GAN_Compression_ICCVW_2023_paper.html": {
    "title": "Accumulation Knowledge Distillation for Conditional GAN Compression",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingwei Gao",
      "Rujiao Long"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/He_RCD-SGD_Resource-Constrained_Distributed_SGD_in_Heterogeneous_Environment_Via_Submodular_Partitioning_ICCVW_2023_paper.html": {
    "title": "RCD-SGD: Resource-Constrained Distributed SGD in Heterogeneous Environment Via Submodular Partitioning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoze He",
      "Parijat Dube"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/van_Betteray_MGiaD_Multigrid_in_all_Dimensions._Efficiency_and_Robustness_by_Weight_ICCVW_2023_paper.html": {
    "title": "MGiaD: Multigrid in all Dimensions. Efficiency and Robustness by Weight Sharing and Coarsening in Resolution and Channel Dimensions",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonia van Betteray",
      "Matthias Rottmann",
      "Karsten Kahl"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Spadaro_Shannon_Strikes_Again_Entropy-Based_Pruning_in_Deep_Neural_Networks_for_ICCVW_2023_paper.html": {
    "title": "Shannon Strikes Again! Entropy-Based Pruning in Deep Neural Networks for Transfer Learning Under Extreme Memory and Computation Budgets",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriele Spadaro",
      "Riccardo Renzulli",
      "Andrea Bragagnolo",
      "Jhony H. Giraldo",
      "Attilio Fiandrotti",
      "Marco Grangetto",
      "Enzo Tartaglione"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Zhang_Extending_TrOCR_for_Text_Localization-Free_OCR_of_Full-Page_Scanned_Receipt_ICCVW_2023_paper.html": {
    "title": "Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned Receipt Images",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongkuan Zhang",
      "Edward Whittaker",
      "Ikuo Kitagishi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Zheng_Lightweight_Vision_Transformer_with_Spatial_and_Channel_Enhanced_Self-Attention_ICCVW_2023_paper.html": {
    "title": "Lightweight Vision Transformer with Spatial and Channel Enhanced Self-Attention",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Zheng",
      "Longqi Yang",
      "Yiying Li",
      "Ke Yang",
      "Zhiyuan Wang",
      "Jun Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Miles_Reconstructing_Pruned_Filters_Using_Cheap_Spatial_Transformations_ICCVW_2023_paper.html": {
    "title": "Reconstructing Pruned Filters Using Cheap Spatial Transformations",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roy Miles",
      "Krystian Mikolajczyk"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Reddy_Quantized_Generative_Models_for_Solving_Inverse_Problems_ICCVW_2023_paper.html": {
    "title": "Quantized Generative Models for Solving Inverse Problems",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nareddy Kartheek Kumar Reddy",
      "Vinayak Killedar",
      "Chandra Sekhar Seelamantula"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Grimaldi_Accelerating_Deep_Neural_Networks_via_Semi-Structured_Activation_Sparsity_ICCVW_2023_paper.html": {
    "title": "Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Grimaldi",
      "Darshan C. Ganji",
      "Ivan Lazarevich",
      "Sudhakar Sah"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Peters_QBitOpt_Fast_and_Accurate_Bitwidth_Reallocation_During_Training_ICCVW_2023_paper.html": {
    "title": "QBitOpt: Fast and Accurate Bitwidth Reallocation During Training",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jorn Peters",
      "Marios Fournarakis",
      "Markus Nagel",
      "Mart van Baalen",
      "Tijmen Blankevoort"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Babiloni_Factorized_Dynamic_Fully-Connected_Layers_for_Neural_Networks_ICCVW_2023_paper.html": {
    "title": "Factorized Dynamic Fully-Connected Layers for Neural Networks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesca Babiloni",
      "Thomas Tanay",
      "Jiankang Deng",
      "Matteo Maggioni",
      "Stefanos Zafeiriou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Honig_Bi-Encoder_Cascades_for_Efficient_Image_Search_ICCVW_2023_paper.html": {
    "title": "Bi-Encoder Cascades for Efficient Image Search",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert HÃ¶nig",
      "Jan Ackermann",
      "Mingyuan Chi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Addad_Multi-Exit_Resource-Efficient_Neural_Architecture_for_Image_Classification_with_Optimized_Fusion_ICCVW_2023_paper.html": {
    "title": "Multi-Exit Resource-Efficient Neural Architecture for Image Classification with Optimized Fusion Block",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youva Addad",
      "Alexis Lechervy",
      "FrÃ©dÃ©ric Jurie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Cavagnero_Entropic_Score_Metric_Decoupling_Topology_and_Size_in_Training-Free_NAS_ICCVW_2023_paper.html": {
    "title": "Entropic Score Metric: Decoupling Topology and Size in Training-Free NAS",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "NiccolÃ² Cavagnero",
      "Luca Robbiano",
      "Francesca Pistilli",
      "Barbara Caputo",
      "Giuseppe Averta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Kumar_CoroNetGAN_Controlled_Pruning_of_GANs_via_Hypernetworks_ICCVW_2023_paper.html": {
    "title": "CoroNetGAN: Controlled Pruning of GANs via Hypernetworks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aman Kumar",
      "Khushboo Anand",
      "Shubham Mandloi",
      "Ashutosh Mishra",
      "Avinash Thakur",
      "Neeraj Kasera",
      "Prathosh A P"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Haque_Dynamic_Neural_Network_is_All_You_Need_Understanding_the_Robustness_ICCVW_2023_paper.html": {
    "title": "Dynamic Neural Network is All You Need: Understanding the Robustness of Dynamic Mechanisms in Neural Networks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mirazul Haque",
      "Wei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAAD/html/Gueze_Floor_Plan_Reconstruction_from_Sparse_Views_Combining_Graph_Neural_Network_ICCVW_2023_paper.html": {
    "title": "Floor Plan Reconstruction from Sparse Views: Combining Graph Neural Network with Constrained Diffusion",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnaud Gueze",
      "Matthieu Ospici",
      "Damien Rohmer",
      "Marie-Paule Cani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAAD/html/van_Engelenburg_SSIG_A_Visually-Guided_Graph_Edit_Distance_for_Floor_Plan_Similarity_ICCVW_2023_paper.html": {
    "title": "SSIG: A Visually-Guided Graph Edit Distance for Floor Plan Similarity",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Casper C. J. van Engelenburg",
      "Seyran Khademi",
      "Jan C. van Gemert"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAAD/html/Li_Scalable_MAV_Indoor_Reconstruction_with_Neural_Implicit_Surfaces_ICCVW_2023_paper.html": {
    "title": "Scalable MAV Indoor Reconstruction with Neural Implicit Surfaces",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoda Li",
      "Puyuan Yi",
      "Yunhao Liu",
      "Avideh Zakhor"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAAD/html/Tukur_PanoStyle_Semantic_Geometry-Aware_and_Shading_Independent_Photorealistic_Style_Transfer_for_ICCVW_2023_paper.html": {
    "title": "PanoStyle: Semantic, Geometry-Aware and Shading Independent Photorealistic Style Transfer for Indoor Panoramic Scenes",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "M. Tukur",
      "A. Ur Rehman",
      "G. Pintore",
      "E. Gobbetti",
      "J. Schneider",
      "M. Agus"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAAD/html/Zhuang_MARL_Multi-scale_Archetype_Representation_Learning_for_Urban_Building_Energy_Modeling_ICCVW_2023_paper.html": {
    "title": "MARL: Multi-scale Archetype Representation Learning for Urban Building Energy Modeling",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinwei Zhuang",
      "Zixun Huang",
      "Wentao Zeng",
      "Luisa Caldas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Kessy_Hyperspectral_Imaging_of_In-Site_Stained_Glasses_Illumination_Variation_Compensation_Using_ICCVW_2023_paper.html": {
    "title": "Hyperspectral Imaging of In-Site Stained Glasses: Illumination Variation Compensation Using Two Perpendicular Scans",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suzan Joseph Kessy",
      "Takuya Funatomi",
      "Kazuya Kitano",
      "Yuki Fujimura",
      "Guillaume Caron",
      "El Mustapha Mouaddib",
      "Yasuhiro Mukaigawa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Llull_Evaluation_of_3D_Reconstruction_for_Cultural_Heritage_Applications_ICCVW_2023_paper.html": {
    "title": "Evaluation of 3D Reconstruction for Cultural Heritage Applications",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "CristiÃ¡n Llull",
      "Nelson Baloian",
      "Benjamin Bustos",
      "Kornelius Kupczik",
      "Ivan Sipiran",
      "AndrÃ©s Baloian"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Stotzner_CNN_Based_Cuneiform_Sign_Detection_Learned_from_Annotated_3D_Renderings_ICCVW_2023_paper.html": {
    "title": "CNN Based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ernst StÃ¶tzner",
      "Timo Homburg",
      "Hubert Mara"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Quattrini_Volumetric_Fast_Fourier_Convolution_for_Detecting_Ink_on_the_Carbonized_ICCVW_2023_paper.html": {
    "title": "Volumetric Fast Fourier Convolution for Detecting Ink on the Carbonized Herculaneum Papyri",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabio Quattrini",
      "Vittorio Pippi",
      "Silvia Cascianelli",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Kumbar_ASUR3D_Arbitrary_Scale_Upsampling_and_Refinement_of_3D_Point_Clouds_ICCVW_2023_paper.html": {
    "title": "ASUR3D: Arbitrary Scale Upsampling and Refinement of 3D Point Clouds Using Local Occupancy Fields",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akash Kumbar",
      "Tejas Anvekar",
      "Ramesh Ashok Tabib",
      "Uma Mudenagudi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Brahim_Facsimiles-Based_Deep_Learning_for_Matching_Relief-Printed_Decorations_on_Medieval_Ceramic_ICCVW_2023_paper.html": {
    "title": "Facsimiles-Based Deep Learning for Matching Relief-Printed Decorations on Medieval Ceramic Sherds",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khawla Brahim",
      "Sylvie Treuillet",
      "Matthieu Exbrayat",
      "Sebastien Jesset"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Tabib_DeFi_Detection_and_Filling_of_Holes_in_Point_Clouds_Towards_ICCVW_2023_paper.html": {
    "title": "DeFi: Detection and Filling of Holes in Point Clouds Towards Restoration of Digitized Cultural Heritage Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramesh Ashok Tabib",
      "Dikshit Hegde",
      "Tejas Anvekar",
      "Uma Mudenagudi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Redon_3D_Surface_Approximation_of_the_Entire_Bayeux_Tapestry_for_Improved_ICCVW_2023_paper.html": {
    "title": "3D Surface Approximation of the Entire Bayeux Tapestry for Improved Pedagogical Access",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marjorie Redon",
      "Matthieu Pizenberg",
      "Yvain QuÃ©au",
      "Abderrahim Elmoataz"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Khawaja_An_Interactive_Method_for_Adaptive_Acquisition_in_Reflectance_Transformation_Imaging_ICCVW_2023_paper.html": {
    "title": "An Interactive Method for Adaptive Acquisition in Reflectance Transformation Imaging for Cultural Heritage",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Arsalan Khawaja",
      "Sony George",
      "Franck Marzani",
      "Jon Yngve Hardeberg",
      "Alamin Mansouri"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Cioni_Diffusion_Based_Augmentation_for_Captioning_and_Retrieval_in_Cultural_Heritage_ICCVW_2023_paper.html": {
    "title": "Diffusion Based Augmentation for Captioning and Retrieval in Cultural Heritage",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dario Cioni",
      "Lorenzo Berlincioni",
      "Federico Becattini",
      "Alberto Del Bimbo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Villegas-Suarez_MatchMakerNet_Enabling_Fragment_Matching_for_Cultural_Heritage_Analysis_ICCVW_2023_paper.html": {
    "title": "MatchMakerNet: Enabling Fragment Matching for Cultural Heritage Analysis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ariana M. Villegas-Suarez",
      "Cristian Lopez",
      "Ivan Sipiran"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Enayati_Semantic_Motif_Segmentation_of_Archaeological_Fresco_Fragments_ICCVW_2023_paper.html": {
    "title": "Semantic Motif Segmentation of Archaeological Fresco Fragments",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aref Enayati",
      "Luca Palmieri",
      "Sebastiano Vascon",
      "Marcello Pelillo",
      "Sinem Aslan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Yemelianenko_Learning_to_Rank_Approach_for_Refining_Image_Retrieval_in_Visual_ICCVW_2023_paper.html": {
    "title": "Learning to Rank Approach for Refining Image Retrieval in Visual Arts",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tetiana Yemelianenko",
      "Iuliia Tkachenko",
      "Tess Masclef",
      "Mihaela Scuturici",
      "Serge Miguet"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Tsuji_Pigment_Mapping_for_Tomb_Murals_Using_Neural_Representation_and_Physics-Based_ICCVW_2023_paper.html": {
    "title": "Pigment Mapping for Tomb Murals Using Neural Representation and Physics-Based Model",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mayuka Tsuji",
      "Yuki Fujimura",
      "Takuya Funatomi",
      "Yasuhiro Mukaigawa",
      "Tetsuro Morimoto",
      "Takeshi Oishi",
      "Jun Takamatsu",
      "Katsushi Ikeuchi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/html/Reby_Semantic_Segmentation_Using_Foundation_Models_for_Cultural_Heritage_an_Experimental_ICCVW_2023_paper.html": {
    "title": "Semantic Segmentation Using Foundation Models for Cultural Heritage: an Experimental Study on Notre-Dame de Paris",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "KÃ©vin RÃ©by",
      "AnaÃ¯s Guilhelm",
      "Livio De Luca"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SHARP/html/Besbes_2D_Cross-View_Object_Segmentation_and_Perceptual_Grouping_in_Computer-Aided_Design_ICCVW_2023_paper.html": {
    "title": "2D Cross-View Object Segmentation and Perceptual Grouping in Computer-Aided Design Drawings",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Dhia Elhak Besbes",
      "Zahra Vahidi Ferdousi",
      "Hedi Tabia",
      "Mouna Fradi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SHARP/html/Onghena_Rotation-Invariant_Hierarchical_Segmentation_on_Poincare_Ball_for_3D_Point_Cloud_ICCVW_2023_paper.html": {
    "title": "Rotation-Invariant Hierarchical Segmentation on Poincare Ball for 3D Point Cloud",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Onghena",
      "Leonardo Gigli",
      "Santiago Velasco-Forero"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SHARP/html/Berardi_Fine-Tuned_but_Zero-Shot_3D_Shape_Sketch_View_Similarity_and_Retrieval_ICCVW_2023_paper.html": {
    "title": "Fine-Tuned but Zero-Shot 3D Shape Sketch View Similarity and Retrieval",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gianluca Berardi",
      "Yulia Gryaditskaya"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SHARP/html/Shinohara_Building_CAD_Model_Reconstruction_from_Point_Clouds_via_Instance_Segmentation_ICCVW_2023_paper.html": {
    "title": "Building CAD Model Reconstruction from Point Clouds via Instance Segmentation, Signed Distance Function, and Graph Cut",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takayuki Shinohara",
      "Li Yonghe",
      "Mitsuteru Sakamoto",
      "Toshiaki Satoh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SHARP/html/Wei_APNet_Urban-Level_Scene_Segmentation_of_Aerial_Images_and_Point_Clouds_ICCVW_2023_paper.html": {
    "title": "APNet: Urban-Level Scene Segmentation of Aerial Images and Point Clouds",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Wei",
      "Martin R. Oswald",
      "Fatemeh Karimi Nejadasl",
      "Theo Gevers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/SHARP/html/Mallis_SHARP_Challenge_2023_Solving_CAD_History_and_pArameters_Recovery_from_ICCVW_2023_paper.html": {
    "title": "SHARP Challenge 2023: Solving CAD History and pArameters Recovery from Point Clouds and 3D Scans. Overview, Datasets, Metrics, and Baselines",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitrios Mallis",
      "Ali Sk Aziz",
      "Elona Dupont",
      "Kseniya Cherenkova",
      "Ahmet Serdar Karadeniz",
      "Mohammad Sadil Khan",
      "Anis Kacem",
      "Gleb Gusev",
      "Djamila Aouada"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VOTS/html/Kristan_The_First_Visual_Object_Tracking_Segmentation_VOTS2023_Challenge_Results_ICCVW_2023_paper.html": {
    "title": "The First Visual Object Tracking Segmentation VOTS2023 Challenge Results",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matej Kristan",
      "JiÅÃ­ Matas",
      "Martin Danelljan",
      "Michael Felsberg",
      "Hyung Jin Chang",
      "Luka Äehovin Zajc",
      "Alan LukeÅ¾iÄ",
      "Ondrej Drbohlav",
      "Zhongqun Zhang",
      "Khanh-Tung Tran",
      "Xuan-Son Vu",
      "Johanna BjÃ¶rklund",
      "Christoph Mayer",
      "Yushan Zhang",
      "Lei Ke",
      "Jie Zhao",
      "Gustavo FernÃ¡ndez",
      "Noor Al-Shakarji",
      "Dong An",
      "Michael Arens",
      "Stefan Becker",
      "Goutam Bhat",
      "Sebastian Bullinger",
      "Antoni B. Chan",
      "Shijie Chang",
      "Hanyuan Chen",
      "Xin Chen",
      "Yan Chen",
      "Zhenyu Chen",
      "Yangming Cheng",
      "Yutao Cui",
      "Chunyuan Deng",
      "Jiahua Dong",
      "Matteo Dunnhofer",
      "Wei Feng",
      "Jianlong Fu",
      "Jie Gao",
      "Ruize Han",
      "Zeqi Hao",
      "Jun-Yan He",
      "Keji He",
      "Zhenyu He",
      "Xiantao Hu",
      "Kaer Huang",
      "Yuqing Huang",
      "Yi Jiang",
      "Ben Kang",
      "Jin-Peng Lan",
      "Hyungjun Lee",
      "Chenyang Li",
      "Jiahao Li",
      "Ning Li",
      "Wangkai Li",
      "Xiaodi Li",
      "Xin Li",
      "Pengyu Liu",
      "Yue Liu",
      "Huchuan Lu",
      "Bin Luo",
      "Ping Luo",
      "Yinchao Ma",
      "Deshui Miao",
      "Christian Micheloni",
      "Kannappan Palaniappan",
      "Hancheol Park",
      "Matthieu Paul",
      "HouWen Peng",
      "Zekun Qian",
      "Gani Rahmon",
      "Norbert Scherer-Negenborn",
      "Pengcheng Shao",
      "Wooksu Shin",
      "Elham Soltani Kazemi",
      "Tianhui Song",
      "Rainer Stiefelhagen",
      "Rui Sun",
      "Chuanming Tang",
      "Zhangyong Tang",
      "Imad Eddine Toubal",
      "Jack Valmadre",
      "Joost van de Weijer",
      "Luc Van Gool",
      "Jash Vira",
      "StÃ¨phane VujasinoviÄ",
      "Cheng Wan",
      "Jia Wan",
      "Dong Wang",
      "Fei Wang",
      "Feifan Wang",
      "He Wang",
      "Limin Wang",
      "Song Wang",
      "Yaowei Wang",
      "Zhepeng Wang",
      "Gangshan Wu",
      "Jiannan Wu",
      "Qiangqiang Wu",
      "Xiaojun Wu",
      "Anqi Xiao",
      "Jinxia Xie",
      "Chenlong Xu",
      "Min Xu",
      "Tianyang Xu",
      "Yuanyou Xu",
      "Bin Yan",
      "Dawei Yang",
      "Ming-Hsuan Yang",
      "Tianyu Yang",
      "Yi Yang",
      "Zongxin Yang",
      "Xuanwu Yin",
      "Fisher Yu",
      "Hongyuan Yu",
      "Qianjin Yu",
      "Weichen Yu",
      "YongSheng Yuan",
      "Zehuan Yuan",
      "Jianlin Zhang",
      "Lu Zhang",
      "Tianzhu Zhang",
      "Guodongfang Zhao",
      "Shaochuan Zhao",
      "Yaozong Zheng",
      "Bineng Zhong",
      "Jiawen Zhu",
      "Xuefeng Zhu",
      "Yueting Zhuang",
      "ChengAo Zong",
      "Kunlong Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Kryeem_Personalized_Monitoring_in_Home_Healthcare_An_Assistive_System_for_Post_ICCVW_2023_paper.html": {
    "title": "Personalized Monitoring in Home Healthcare: An Assistive System for Post Hip Replacement Rehabilitation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alaa Kryeem",
      "Shmuel Raz",
      "Dana Eluz",
      "Dorit Itah",
      "Hagit Hel-Or",
      "Ilan Shimshoni"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Liu_Open_Scene_Understanding_Grounded_Situation_Recognition_Meets_Segment_Anything_for_ICCVW_2023_paper.html": {
    "title": "Open Scene Understanding: Grounded Situation Recognition Meets Segment Anything for Helping People with Visual Impairments",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiping Liu",
      "Jiaming Zhang",
      "Kunyu Peng",
      "Junwei Zheng",
      "Ke Cao",
      "Yufan Chen",
      "Kailun Yang",
      "Rainer Stiefelhagen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Powers_Vision-Based_Treatment_Localization_with_Limited_Data_Automated_Documentation_of_Military_ICCVW_2023_paper.html": {
    "title": "Vision-Based Treatment Localization with Limited Data: Automated Documentation of Military Emergency Medical Procedures",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trevor Powers",
      "Elaheh Hatamimajoumerd",
      "William Chu",
      "Vishakk Rajendran",
      "Rishi Shah",
      "Frank Diabour",
      "Marc Vaillant",
      "Richard Fletcher",
      "Sarah Ostadabbas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Constantin_Multimodal_Error_Correction_with_Natural_Language_and_Pointing_Gestures_ICCVW_2023_paper.html": {
    "title": "Multimodal Error Correction with Natural Language and Pointing Gestures",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Constantin",
      "Fevziye Irem Eyiokur",
      "Dogucan Yaman",
      "Leonard BÃ¤rmann",
      "Alex Waibel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Wong_Learnt_Contrastive_Concept_Embeddings_for_Sign_Recognition_ICCVW_2023_paper.html": {
    "title": "Learnt Contrastive Concept Embeddings for Sign Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Wong",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Voskou_A_New_Dataset_for_End-to-End_Sign_Language_Translation_The_Greek_ICCVW_2023_paper.html": {
    "title": "A New Dataset for End-to-End Sign Language Translation: The Greek Elementary School Dataset",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Voskou",
      "Konstantinos P. Panousis",
      "Harris Partaourides",
      "Kyriakos Tolias",
      "Sotirios Chatzis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Apicella_Affordance_Segmentation_of_Hand-Occluded_Containers_from_Exocentric_Images_ICCVW_2023_paper.html": {
    "title": "Affordance Segmentation of Hand-Occluded Containers from Exocentric Images",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tommaso Apicella",
      "Alessio Xompero",
      "Edoardo Ragusa",
      "Riccardo Berta",
      "Andrea Cavallaro",
      "Paolo Gastaldo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Bacharidis_Repetition-Aware_Image_Sequence_Sampling_for_Recognizing_Repetitive_Human_Actions_ICCVW_2023_paper.html": {
    "title": "Repetition-Aware Image Sequence Sampling for Recognizing Repetitive Human Actions",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantinos Bacharidis",
      "Antonis Argyros"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Surougi_Real-Time_Optimisation-Based_Path_Planning_for_Visually_Impaired_People_in_Dynamic_ICCVW_2023_paper.html": {
    "title": "Real-Time Optimisation-Based Path Planning for Visually Impaired People in Dynamic Environments",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadeel R. Surougi",
      "Julie A. McCann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Ahmed_Towards_Estimation_of_Human_Intent_in_Assistive_Robotic_Teleoperation_Using_ICCVW_2023_paper.html": {
    "title": "Towards Estimation of Human Intent in Assistive Robotic Teleoperation Using Kinaesthetic and Visual Feedback",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muneeb Ahmed",
      "Brejesh Lall",
      "Rajesh Kumar",
      "Arzad A. Kherani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Schiatti_Modeling_Visual_Impairments_with_Artificial_Neural_Networks_a_Review_ICCVW_2023_paper.html": {
    "title": "Modeling Visual Impairments with Artificial Neural Networks: a Review",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucia Schiatti",
      "Monica Gori",
      "Martin Schrimpf",
      "Giulia Cappagli",
      "Federica Morelli",
      "Sabrina Signorini",
      "Boris Katz",
      "Andrei Barbu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Ishii_Enhancing_Human-Robot_Collaborative_Object_Search_Through_Human_Behavior_Observation_and_ICCVW_2023_paper.html": {
    "title": "Enhancing Human-Robot Collaborative Object Search Through Human Behavior Observation and Dialog",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takahiro Ishii",
      "Jun Miura",
      "Kotaro Hayashi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Lv_IFPNet_Integrated_Feature_Pyramid_Network_with_Fusion_Factor_for_Lane_ICCVW_2023_paper.html": {
    "title": "IFPNet: Integrated Feature Pyramid Network with Fusion Factor for Lane Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zinan Lv",
      "Dong Han",
      "Wenzhe Wang",
      "Cheng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Swamy_SHOWMe_Benchmarking_Object-Agnostic_Hand-Object_3D_Reconstruction_ICCVW_2023_paper.html": {
    "title": "SHOWMe: Benchmarking Object-Agnostic Hand-Object 3D Reconstruction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anilkumar Swamy",
      "Vincent Leroy",
      "Philippe Weinzaepfel",
      "Fabien Baradel",
      "Salma Galaaoui",
      "Romain BrÃ©gier",
      "Matthieu Armando",
      "Jean-Sebastien Franco",
      "GrÃ©gory Rogez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/De_Simone_Autonomous_Mobile_Robot_for_Automatic_out_of_Stock_Detection_in_ICCVW_2023_paper.html": {
    "title": "Autonomous Mobile Robot for Automatic out of Stock Detection in a Supermarket",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuseppe De Simone",
      "Pasquale Foggia",
      "Alessia Saggese",
      "Mario Vento"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Sincan_Is_Context_all_you_Need_Scaling_Neural_Sign_Language_Translation_ICCVW_2023_paper.html": {
    "title": "Is Context all you Need? Scaling Neural Sign Language Translation to Large Domains of Discourse",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ozge Mercanoglu Sincan",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Holmes_From_Scarcity_to_Understanding_Transfer_Learning_for_the_Extremely_Low_ICCVW_2023_paper.html": {
    "title": "From Scarcity to Understanding: Transfer Learning for the Extremely Low Resource Irish Sign Language",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruth Holmes",
      "Ellen Rushe",
      "Mathieu De Coster",
      "Maxim Bonnaerens",
      "Shin'ichi Satoh",
      "Akihiro Sugimoto",
      "Anthony Ventresque"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Matsuda_Multi-Camera_3D_Position_Estimation_Using_Conditional_Random_Field_ICCVW_2023_paper.html": {
    "title": "Multi-Camera 3D Position Estimation Using Conditional Random Field",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shusuke Matsuda",
      "Nattaon Techasarntikul",
      "Hideyuki Shimonishi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Kwolek_Continuous_Hand_Gesture_Recognition_for_Human-Robot_Collaborative_Assembly_ICCVW_2023_paper.html": {
    "title": "Continuous Hand Gesture Recognition for Human-Robot Collaborative Assembly",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bogdan Kwolek"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Manousaki_VLMAH_Visual-Linguistic_Modeling_of_Action_History_for_Effective_Action_Anticipation_ICCVW_2023_paper.html": {
    "title": "VLMAH: Visual-Linguistic Modeling of Action History for Effective Action Anticipation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victoria Manousaki",
      "Konstantinos Bacharidis",
      "Konstantinos Papoutsakis",
      "Antonis Argyros"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/html/Sufian_FewFaceNet_A_Lightweight_Few-Shot_Learning-Based_Incremental_Face_Authentication_for_Edge_ICCVW_2023_paper.html": {
    "title": "FewFaceNet: A Lightweight Few-Shot Learning-Based Incremental Face Authentication for Edge Cameras",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abu Sufian",
      "Anirudha Ghosh",
      "Debaditya Barman",
      "Marco Leo",
      "Cosimo Distante",
      "Baihua Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OpenSUN3D/html/Sakaino_Dynamic_Texts_From_UAV_Perspective_Natural_Images_ICCVW_2023_paper.html": {
    "title": "Dynamic Texts From UAV Perspective Natural Images",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hidetomo Sakaino"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OpenSUN3D/html/Zhang_CLIP-FO3D_Learning_Free_Open-World_3D_Scene_Representations_from_2D_Dense_ICCVW_2023_paper.html": {
    "title": "CLIP-FO3D: Learning Free Open-World 3D Scene Representations from 2D Dense CLIP",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junbo Zhang",
      "Runpei Dong",
      "Kaisheng Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OpenSUN3D/html/Sachdeva_The_Change_You_Want_to_See_Now_in_3D_ICCVW_2023_paper.html": {
    "title": "The Change You Want to See (Now in 3D)",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ragav Sachdeva",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OpenSUN3D/html/Auty_Learning_to_Prompt_CLIP_for_Monocular_Depth_Estimation_Exploring_the_ICCVW_2023_paper.html": {
    "title": "Learning to Prompt CLIP for Monocular Depth Estimation: Exploring the Limits of Human Language",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dylan Auty",
      "Krystian Mikolajczyk"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OpenSUN3D/html/Hegde_CLIP_Goes_3D_Leveraging_Prompt_Tuning_for_Language_Grounded_3D_ICCVW_2023_paper.html": {
    "title": "CLIP Goes 3D: Leveraging Prompt Tuning for Language Grounded 3D Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deepti Hegde",
      "Jeya Maria Jose Valanarasu",
      "Vishal Patel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Zhou_Diff3DHPE_A_Diffusion_Model_for_3D_Human_Pose_Estimation_ICCVW_2023_paper.html": {
    "title": "Diff3DHPE: A Diffusion Model for 3D Human Pose Estimation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieming Zhou",
      "Tong Zhang",
      "Zeeshan Hayder",
      "Lars Petersson",
      "Mehrtash Harandi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Li_NeRF-Pose_A_First-Reconstruct-Then-Regress_Approach_for_Weakly-Supervised_6D_Object_Pose_Estimation_ICCVW_2023_paper.html": {
    "title": "NeRF-Pose: A First-Reconstruct-Then-Regress Approach for Weakly-Supervised 6D Object Pose Estimation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu Li",
      "Shishir Reddy Vutukur",
      "Hao Yu",
      "Ivan Shugurov",
      "Benjamin Busam",
      "Shaowu Yang",
      "Slobodan Ilic"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Castro_PoseMatcher_One-Shot_6D_Object_Pose_Estimation_by_Deep_Feature_Matching_ICCVW_2023_paper.html": {
    "title": "PoseMatcher: One-Shot 6D Object Pose Estimation by Deep Feature Matching",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro Castro",
      "Tae-Kyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Mirmohammadi_Reconstruction_of_3D_Interaction_Models_from_Images_Using_Shape_Prior_ICCVW_2023_paper.html": {
    "title": "Reconstruction of 3D Interaction Models from Images Using Shape Prior",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehrshad Mirmohammadi",
      "Parham Saremi",
      "Yen-Ling Kuo",
      "Xi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Cheng_Accidental_Turntables_Learning_3D_Pose_by_Watching_Objects_Turn_ICCVW_2023_paper.html": {
    "title": "Accidental Turntables: Learning 3D Pose by Watching Objects Turn",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zezhou Cheng",
      "Matheus Gadelha",
      "Subhransu Maji"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Nguyen_CNOS_A_Strong_Baseline_for_CAD-Based_Novel_Object_Segmentation_ICCVW_2023_paper.html": {
    "title": "CNOS: A Strong Baseline for CAD-Based Novel Object Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Van Nguyen Nguyen",
      "Thibault Groueix",
      "Georgy Ponimatkin",
      "Vincent Lepetit",
      "Tomas Hodan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Corsetti_Revisiting_Fully_Convolutional_Geometric_Features_for_Object_6D_Pose_Estimation_ICCVW_2023_paper.html": {
    "title": "Revisiting Fully Convolutional Geometric Features for Object 6D Pose Estimation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaime Corsetti",
      "Davide Boscaini",
      "Fabio Poiesi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Haugaard_SpyroPose_SE3_Pyramids_for_Object_Pose_Distribution_Estimation_ICCVW_2023_paper.html": {
    "title": "SpyroPose: SE(3) Pyramids for Object Pose Distribution Estimation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rasmus Laurvig Haugaard",
      "Frederik HagelskjÃ¦r",
      "ThorbjÃ¸rn MosekjÃ¦r Iversen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/JRDB/html/Hong_Cross-Dimensional_Refined_Learning_for_Real-Time_3D_Visual_Perception_from_Monocular_ICCVW_2023_paper.html": {
    "title": "Cross-Dimensional Refined Learning for Real-Time 3D Visual Perception from Monocular Video",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Hong",
      "C. Patrick Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/JRDB/html/Noor_A_Lightweight_Skeleton-Based_3D-CNN_for_Real-Time_Fall_Detection_and_Action_ICCVW_2023_paper.html": {
    "title": "A Lightweight Skeleton-Based 3D-CNN for Real-Time Fall Detection and Action Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nadhira Noor",
      "In Kyu Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/JRDB/html/Khandelwal_SegDA_Maximum_Separable_Segment_Mask_with_Pseudo_Labels_for_Domain_ICCVW_2023_paper.html": {
    "title": "SegDA: Maximum Separable Segment Mask with Pseudo Labels for Domain Adaptive Semantic Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anant Khandelwal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/JRDB/html/Zaier_A_Dual_Perspective_of_Human_Motion_Analysis_-_3D_Pose_ICCVW_2023_paper.html": {
    "title": "A Dual Perspective of Human Motion Analysis - 3D Pose Estimation and 2D Trajectory Prediction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mayssa Zaier",
      "Hazem Wannous",
      "Hassen Drira",
      "Jacques Boonaert"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/JRDB/html/de_Almeida_THOR-Magni_Comparative_Analysis_of_Deep_Learning_Models_for_Role-Conditioned_Human_ICCVW_2023_paper.html": {
    "title": "THOR-Magni: Comparative Analysis of Deep Learning Models for Role-Conditioned Human Motion Prediction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiago Rodrigues de Almeida",
      "Andrey Rudenko",
      "Tim Schreiter",
      "Yufei Zhu",
      "Eduardo Gutierrez Maestro",
      "Lucas Morillo-Mendez",
      "Tomasz P. Kucner",
      "Oscar Martinez Mozos",
      "Martin Magnusson",
      "Luigi Palmieri",
      "Kai O. Arras",
      "Achim J. Lilienthal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/WiCV/html/Arndt_Do_Planar_Constraints_Improve_Camera_Pose_Estimation_in_Monocular_SLAM_ICCVW_2023_paper.html": {
    "title": "Do Planar Constraints Improve Camera Pose Estimation in Monocular SLAM?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charlotte Arndt",
      "Reza Sabzevari",
      "Javier Civera"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/WiCV/html/Hoang_Embedded_Deformation-Based_Compression_for_Human_3D_Dynamic_Meshes_with_Changing_ICCVW_2023_paper.html": {
    "title": "Embedded Deformation-Based Compression for Human 3D Dynamic Meshes with Changing Topology",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huong Hoang",
      "Kunyao Chen",
      "Truong Nguyen",
      "Pamela Cosman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/WiCV/html/Kumbar_TP-NoDe_Topology-Aware_Progressive_Noising_and_Denoising_of_Point_Clouds_Towards_ICCVW_2023_paper.html": {
    "title": "TP-NoDe: Topology-Aware Progressive Noising and Denoising of Point Clouds Towards Upsampling",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akash Kumbar",
      "Tejas Anvekar",
      "Tulasi Amitha Vikrama",
      "Ramesh Ashok Tabib",
      "Uma Mudenagudi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/WiCV/html/Kapoor_Domain_Adversarial_Learning_Towards_Underwater_Image_Enhancement_ICCVW_2023_paper.html": {
    "title": "Domain Adversarial Learning Towards Underwater Image Enhancement",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meghna Kapoor",
      "Rohan Baghel",
      "Badri Narayan Subudhi",
      "Vinit Jakhetiya",
      "Ankur Bansal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/WiCV/html/Desai_LightNet_Generative_Model_for_Enhancement_of_Low-Light_Images_ICCVW_2023_paper.html": {
    "title": "LightNet: Generative Model for Enhancement of Low-Light Images",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaitra Desai",
      "Nikhil Akalwadi",
      "Amogh Joshi",
      "Sampada Malagi",
      "Chinmayee Mandi",
      "Ramesh Ashok Tabib",
      "Ujwala Patil",
      "Uma Mudenagudi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/WiCV/html/Rizzoli_SynDrone_-_Multi-Modal_UAV_Dataset_for_Urban_Scenarios_ICCVW_2023_paper.html": {
    "title": "SynDrone - Multi-Modal UAV Dataset for Urban Scenarios",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giulia Rizzoli",
      "Francesco Barbato",
      "Matteo Caligiuri",
      "Pietro Zanuttigh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/WiCV/html/Caldarola_Window-Based_Model_Averaging_Improves_Generalization_in_Heterogeneous_Federated_Learning_ICCVW_2023_paper.html": {
    "title": "Window-Based Model Averaging Improves Generalization in Heterogeneous Federated Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debora Caldarola",
      "Barbara Caputo",
      "Marco Ciccone"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Park_Robust_Asymmetric_Loss_for_Multi-Label_Long-Tailed_Learning_ICCVW_2023_paper.html": {
    "title": "Robust Asymmetric Loss for Multi-Label Long-Tailed Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wongi Park",
      "Inhyuk Park",
      "Sungeun Kim",
      "Jongbin Ryu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Nurgazin_A_Comparative_Study_of_Vision_Transformer_Encoders_and_Few-Shot_Learning_ICCVW_2023_paper.html": {
    "title": "A Comparative Study of Vision Transformer Encoders and Few-Shot Learning for Medical Image Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxat Nurgazin",
      "Nguyen Anh Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Marinov_Mirror_U-Net_Marrying_Multimodal_Fission_with_Multi-Task_Learning_for_Semantic_ICCVW_2023_paper.html": {
    "title": "Mirror U-Net: Marrying Multimodal Fission with Multi-Task Learning for Semantic Segmentation in Medical Imaging",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zdravko Marinov",
      "Simon ReiÃ",
      "David Kersting",
      "Jens Kleesiek",
      "Rainer Stiefelhagen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Sikaroudi_ALFA_-_Leveraging_All_Levels_of_Feature_Abstraction_for_Enhancing_ICCVW_2023_paper.html": {
    "title": "ALFA - Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milad Sikaroudi",
      "Maryam Hosseini",
      "Shahryar Rahnamayan",
      "H. R. Tizhoosh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Jeong_An_Optimized_Ensemble_Framework_for_Multi-Label_Classification_on_Long-Tailed_Chest_ICCVW_2023_paper.html": {
    "title": "An Optimized Ensemble Framework for Multi-Label Classification on Long-Tailed Chest X-ray Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehyup Jeong",
      "Bosoung Jeoun",
      "Yeonju Park",
      "Bohyung Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Karimijafarbigloo_Self-Supervised_Semantic_Segmentation_Consistency_over_Transformation_ICCVW_2023_paper.html": {
    "title": "Self-Supervised Semantic Segmentation: Consistency over Transformation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanaz Karimijafarbigloo",
      "Reza Azad",
      "Amirhossein Kazerouni",
      "Yury Velichko",
      "Ulas Bagci",
      "Dorit Merhof"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Mejia_SEPAL_Spatial_Gene_Expression_Prediction_from_Local_Graphs_ICCVW_2023_paper.html": {
    "title": "SEPAL: Spatial Gene Expression Prediction from Local Graphs",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Mejia",
      "Paula CÃ¡rdenas",
      "Daniela Ruiz",
      "Angela Castillo",
      "Pablo ArbelÃ¡ez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Li_Weakly_Semi-Supervised_Detector-Based_Video_Classification_with_Temporal_Context_for_Lung_ICCVW_2023_paper.html": {
    "title": "Weakly Semi-Supervised Detector-Based Video Classification with Temporal Context for Lung Ultrasound",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gary Y. Li",
      "Li Chen",
      "Mohsen Zahiri",
      "Naveen Balaraju",
      "Shubham Patil",
      "Courosh Mehanian",
      "Cynthia Gregory",
      "Kenton Gregory",
      "Balasundar Raju",
      "Jochen Kruecker",
      "Alvin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Pleasure_Pathology-Based_Ischemic_Stroke_Etiology_Classification_via_Clot_Composition_Guided_Multiple_ICCVW_2023_paper.html": {
    "title": "Pathology-Based Ischemic Stroke Etiology Classification via Clot Composition Guided Multiple Instance Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mara Pleasure",
      "Ekaterina Redekop",
      "Jennifer S Polson",
      "Haoyue Zhang",
      "Naoki Kaneko",
      "William Speier",
      "Corey W Arnold"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Pal_AW-Net_A_Novel_Fully_Connected_Attention-Based_Medical_Image_Segmentation_Model_ICCVW_2023_paper.html": {
    "title": "AW-Net: A Novel Fully Connected Attention-Based Medical Image Segmentation Model",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debojyoti Pal",
      "Tanushree Meena",
      "Dwarikanath Mahapatra",
      "Sudipta Roy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Wang_DISGAN_Wavelet-Informed_Discriminator_Guides_GAN_to_MRI_Super-Resolution_with_Noise_ICCVW_2023_paper.html": {
    "title": "DISGAN: Wavelet-Informed Discriminator Guides GAN to MRI Super-Resolution with Noise Cleaning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Wang",
      "Lucas Mahler",
      "Julius Steiglechner",
      "Florian Birk",
      "Klaus Scheffler",
      "Gabriele Lohmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Ahmed_Topo-CXR_Chest_X-ray_TB_and_Pneumonia_Screening_with_Topological_Machine_ICCVW_2023_paper.html": {
    "title": "Topo-CXR: Chest X-ray TB and Pneumonia Screening with Topological Machine Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Faisal Ahmed",
      "Brighton Nuwagira",
      "Furkan Torlak",
      "Baris Coskunuzer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Wirth_ShaRPy_Shape_Reconstruction_and_Hand_Pose_Estimation_from_RGB-D_with_ICCVW_2023_paper.html": {
    "title": "ShaRPy: Shape Reconstruction and Hand Pose Estimation from RGB-D with Uncertainty",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vanessa Wirth",
      "Anna-Maria Liphardt",
      "Birte Coppers",
      "Johanna BrÃ¤unig",
      "Simon Heinrich",
      "Sigrid Leyendecker",
      "Arnd Kleyer",
      "Georg Schett",
      "Martin Vossiek",
      "Bernhard Egger",
      "Marc Stamminger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Seo_Enhancing_Multi-Label_Long-Tailed_Classification_on_Chest_X-Rays_Through_ML-GCN_Augmentation_ICCVW_2023_paper.html": {
    "title": "Enhancing Multi-Label Long-Tailed Classification on Chest X-Rays Through ML-GCN Augmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HyeRyeong Seo",
      "MinHyuk Lee",
      "WooJin Cheong",
      "HyeKyung Yoon",
      "SoHyung Kim",
      "MyungJoo Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Lee_Order-ViT_Order_Learning_Vision_Transformer_for_Cancer_Classification_in_Pathology_ICCVW_2023_paper.html": {
    "title": "Order-ViT: Order Learning Vision Transformer for Cancer Classification in Pathology Images",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ju Cheon Lee",
      "Jin Tae Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Stolpovsky_RheumaVIT_Transformer-Based_Model_for_Automated_Scoring_of_Hand_Joints_in_ICCVW_2023_paper.html": {
    "title": "RheumaVIT: Transformer-Based Model for Automated Scoring of Hand Joints in Rheumatoid Arthritis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Stolpovsky",
      "Elizaveta Dakhova",
      "Polina Druzhinina",
      "Polina Postnikova",
      "Daniil Kudinsky",
      "Alexander Smirnov",
      "Anastasia Sukhinina",
      "Alexander Lila",
      "Anvar Kurmukov"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Singh_Enhancing_Medical_Image_Segmentation_Optimizing_Cross-Entropy_Weights_and_Post-Processing_with_ICCVW_2023_paper.html": {
    "title": "Enhancing Medical Image Segmentation: Optimizing Cross-Entropy Weights and Post-Processing with Autoencoders",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranav Singh",
      "Luoyao Chen",
      "Mei Chen",
      "Jinqian Pan",
      "Raviteja Chukkapalli",
      "Shravan Chaudhari",
      "Jacopo Cirrone"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Dack_An_Empirical_Analysis_for_Zero-Shot_Multi-Label_Classification_on_COVID-19_CT_ICCVW_2023_paper.html": {
    "title": "An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ethan Dack",
      "Lorenzo Brigato",
      "Matthew McMurray",
      "Matthias Fontanellaz",
      "Thomas Frauenfelder",
      "Hanno Hoppe",
      "Aristomenis Exadaktylos",
      "Thomas Geiser",
      "Manuela Funke-Chambour",
      "Andreas Christe",
      "Lukas Ebner",
      "Stavroula Mougiakakou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Mikhailov_Sharing_is_Caring_Concurrent_Interactive_Segmentation_and_Model_Training_Using_ICCVW_2023_paper.html": {
    "title": "Sharing is Caring: Concurrent Interactive Segmentation and Model Training Using a Joint Model",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Mikhailov",
      "Benoit Chauveau",
      "Nicolas Bourdel",
      "Adrien Bartoli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Zhang_Robust_AMD_Stage_Grading_with_Exclusively_OCTA_Modality_Leveraging_3D_ICCVW_2023_paper.html": {
    "title": "Robust AMD Stage Grading with Exclusively OCTA Modality Leveraging 3D Volume",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Zhang",
      "Anna Heinke",
      "Carlo Miguel B. Galang",
      "Daniel N. Deussen",
      "Bo Wen",
      "Dirk-Uwe G. Bartsch",
      "William R. Freeman",
      "Truong Q. Nguyen",
      "Cheolhong An"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Chennuri_Fusion_Approaches_to_Predict_Post-Stroke_Aphasia_Severity_from_Multimodal_Neuroimaging_ICCVW_2023_paper.html": {
    "title": "Fusion Approaches to Predict Post-Stroke Aphasia Severity from Multimodal Neuroimaging Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saurav Chennuri",
      "Sha Lai",
      "Anne Billot",
      "Maria Varkanitsa",
      "Emily J. Braun",
      "Swathi Kiran",
      "Archana Venkataraman",
      "Janusz Konrad",
      "Prakash Ishwar",
      "Margrit Betke"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Dulau_Ensuring_a_Connected_Structure_for_Retinal_Vessels_Deep-Learning_Segmentation_ICCVW_2023_paper.html": {
    "title": "Ensuring a Connected Structure for Retinal Vessels Deep-Learning Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Idris Dulau",
      "Catherine Helmer",
      "Cecile Delcourt",
      "Marie Beurton-Aimar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Nguyen-Mau_Advanced_Augmentation_and_Ensemble_Approaches_for_Classifying_Long-Tailed_Multi-Label_Chest_ICCVW_2023_paper.html": {
    "title": "Advanced Augmentation and Ensemble Approaches for Classifying Long-Tailed Multi-Label Chest X-Rays",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trong-Hieu Nguyen-Mau",
      "Tuan-Luc Huynh",
      "Thanh-Danh Le",
      "Hai-Dang Nguyen",
      "Minh-Triet Tran"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Wang_End-to-End_Deep_Learning_for_Reconstructing_Segmented_3D_CT_Image_from_ICCVW_2023_paper.html": {
    "title": "End-to-End Deep Learning for Reconstructing Segmented 3D CT Image from Multi-Energy X-ray Projections",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Wang",
      "Tatsuya Yatagawa",
      "Yutaka Ohtake",
      "Toru Aoki",
      "Jun Hotta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Kelner_Semantic_Parsing_of_Colonoscopy_Videos_with_Multi-Label_Temporal_Networks_ICCVW_2023_paper.html": {
    "title": "Semantic Parsing of Colonoscopy Videos with Multi-Label Temporal Networks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ori Kelner",
      "Or Weinstein",
      "Ehud Rivlin",
      "Roman Goldenberg"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Kumar_Mind_the_Clot_Automated_LVO_Detection_on_CTA_Using_Deep_ICCVW_2023_paper.html": {
    "title": "Mind the Clot: Automated LVO Detection on CTA Using Deep Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubham Kumar",
      "Arjun Agarwal",
      "Satish Golla",
      "Swetha Tanamala",
      "Ujjwal Upadhyay",
      "Subhankar Chattoraj",
      "Preetham Putha",
      "Sasank Chilamkurthy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Ramanarayanan_HyperCoil-Recon_A_Hypernetwork-Based_Adaptive_Coil_Configuration_Task_Switching_Network_for_ICCVW_2023_paper.html": {
    "title": "HyperCoil-Recon: A Hypernetwork-Based Adaptive Coil Configuration Task Switching Network for MRI Reconstruction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sriprabha Ramanarayanan",
      "Mohammad Al Fahim",
      "Rahul G S",
      "Amrit Kumar Jethi",
      "Keerthi Ram",
      "Mohanasankar Sivaprakasam"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Kim_CheXFusion_Effective_Fusion_of_Multi-View_Features_Using_Transformers_for_Long-Tailed_ICCVW_2023_paper.html": {
    "title": "CheXFusion: Effective Fusion of Multi-View Features Using Transformers for Long-Tailed Chest X-Ray Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongkyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Hu_Contrastive_Image_Synthesis_and_Self-Supervised_Feature_Adaptation_for_Cross-Modality_Biomedical_ICCVW_2023_paper.html": {
    "title": "Contrastive Image Synthesis and Self-Supervised Feature Adaptation for Cross-Modality Biomedical Image Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinrong Hu",
      "Corey Wang",
      "Yiyu Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Viviers_Segmentation-Based_Assessment_of_Tumor-Vessel_Involvement_for_Surgical_Resectability_Prediction_of_ICCVW_2023_paper.html": {
    "title": "Segmentation-Based Assessment of Tumor-Vessel Involvement for Surgical Resectability Prediction of Pancreatic Ductal Adenocarcinoma",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christiaan Viviers",
      "Mark Ramaekers",
      "Amaan Valiuddin",
      "Terese HellstrÃ¶m",
      "Nick Tasios",
      "John van der Ven",
      "Igor Jacobs",
      "Lotte Ewals",
      "Joost Nederend",
      "Peter de With",
      "Misha Luyer",
      "Fons van der Sommen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Rao_Studying_the_Impact_of_Augmentations_on_Medical_Confidence_Calibration_ICCVW_2023_paper.html": {
    "title": "Studying the Impact of Augmentations on Medical Confidence Calibration",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrit Rao",
      "Joon-Young Lee",
      "Oliver Aalami"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Li_Combating_Coronary_Calcium_Scoring_Bias_for_Non-Gated_CT_by_Semantic_ICCVW_2023_paper.html": {
    "title": "Combating Coronary Calcium Scoring Bias for Non-Gated CT by Semantic Learning on Gated CT",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajian Li",
      "Anwei Li",
      "Jiansheng Fang",
      "Yonghe Hou",
      "Chao Song",
      "Huifang Yang",
      "Jingwen Wang",
      "Hongbo Liu",
      "Jiang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Yu_Cross-Grained_Contrastive_Representation_for_Unsupervised_Lesion_Segmentation_in_Medical_Images_ICCVW_2023_paper.html": {
    "title": "Cross-Grained Contrastive Representation for Unsupervised Lesion Segmentation in Medical Images",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Yu",
      "Botao Zhao",
      "Yipin Zhang",
      "Shengjie Zhang",
      "Xiang Chen",
      "Haibo Yang",
      "Tingying Peng",
      "Xiao-Yong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Huang_Multimodal_Contrastive_Learning_and_Tabular_Attention_for_Automated_Alzheimers_Disease_ICCVW_2023_paper.html": {
    "title": "Multimodal Contrastive Learning and Tabular Attention for Automated Alzheimer's Disease Prediction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weichen Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Yamagishi_Effect_of_Stage_Training_for_Long-Tailed_Multi-Label_Image_Classification_ICCVW_2023_paper.html": {
    "title": "Effect of Stage Training for Long-Tailed Multi-Label Image Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yosuke Yamagishi",
      "Shohei Hanaoka"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Carloni_Causality-Driven_One-Shot_Learning_for_Prostate_Cancer_Grading_from_MRI_ICCVW_2023_paper.html": {
    "title": "Causality-Driven One-Shot Learning for Prostate Cancer Grading from MRI",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gianluca Carloni",
      "Eva Pachetti",
      "Sara Colantonio"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Yeganeh_Transformers_Pay_Attention_to_Convolutions_Leveraging_Emerging_Properties_of_ViTs_ICCVW_2023_paper.html": {
    "title": "Transformers Pay Attention to Convolutions Leveraging Emerging Properties of ViTs by Dual Attention-Image Network",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yousef Yeganeh",
      "Azade Farshad",
      "Peter Weinberger",
      "Seyed-Ahmad Ahmadi",
      "Ehsan Adeli",
      "Nassir Navab"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Nguyen_Towards_Robust_Natural-Looking_Mammography_Lesion_Synthesis_on_Ipsilateral_Dual-Views_Breast_ICCVW_2023_paper.html": {
    "title": "Towards Robust Natural-Looking Mammography Lesion Synthesis on Ipsilateral Dual-Views Breast Cancer Analysis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thanh-Huy Nguyen",
      "Quang Hien Kha",
      "Thai Ngoc Toan Truong",
      "Ba Thinh Lam",
      "Ba Hung Ngo",
      "Quang Vinh Dinh",
      "Nguyen Quoc Khanh Le"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Bender_Towards_Fixing_Clever-Hans_Predictors_with_Counterfactual_Knowledge_Distillation_ICCVW_2023_paper.html": {
    "title": "Towards Fixing Clever-Hans Predictors with Counterfactual Knowledge Distillation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sidney Bender",
      "Christopher J. Anders",
      "Pattarawat Chormai",
      "Heike Antje Marxfeld",
      "Jan Herrmann",
      "GrÃ©goire Montavon"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Kumar_Robust_MSFM_Learning_Network_for_Classification_and_Weakly_Supervised_Localization_ICCVW_2023_paper.html": {
    "title": "Robust MSFM Learning Network for Classification and Weakly Supervised Localization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Komal Kumar",
      "Balakrishna Pailla",
      "Kalyan Tadepalli",
      "Sudipta Roy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Jimenez_Computational_Evaluation_of_the_Combination_of_Semi-Supervised_and_Active_Learning_ICCVW_2023_paper.html": {
    "title": "Computational Evaluation of the Combination of Semi-Supervised and Active Learning for Histopathology Image Segmentation with Missing Annotations",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laura GÃ¡lvÃ©z Jimenez",
      "Lucile Dierckx",
      "Maxime Amodei",
      "Hamed Razavi Khosroshahi",
      "Natarajan Chidambaran",
      "Anh-Thu Phan Ho",
      "Alberto Franzin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Pandey_Comprehensive_Multimodal_Segmentation_in_Medical_Imaging_Combining_YOLOv8_with_SAM_ICCVW_2023_paper.html": {
    "title": "Comprehensive Multimodal Segmentation in Medical Imaging: Combining YOLOv8 with SAM and HQ-SAM Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumit Pandey",
      "Kuan-Fu Chen",
      "Erik B. Dam"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Patel_Self-Supervised_Anomaly_Detection_from_Anomalous_Training_Data_via_Iterative_Latent_ICCVW_2023_paper.html": {
    "title": "Self-Supervised Anomaly Detection from Anomalous Training Data via Iterative Latent Token Masking",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashay Patel",
      "Petru-Daniel Tudosiu",
      "Walter H.L. Pinaya",
      "Mark S. Graham",
      "Olusola Adeleke",
      "Gary Cook",
      "Vicky Goh",
      "Sebastien Ourselin",
      "M. Jorge Cardoso"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Myers_Geodesic_Regression_Characterizes_3D_Shape_Changes_in_the_Female_Brain_ICCVW_2023_paper.html": {
    "title": "Geodesic Regression Characterizes 3D Shape Changes in the Female Brain During Menstruation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adele Myers",
      "Caitlin Taylor",
      "Emily Jacobs",
      "Nina Miolane"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Molaei_Implicit_Neural_Representation_in_Medical_Imaging_A_Comparative_Survey_ICCVW_2023_paper.html": {
    "title": "Implicit Neural Representation in Medical Imaging: A Comparative Survey",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirali Molaei",
      "Amirhossein Aminimehr",
      "Armin Tavakoli",
      "Amirhossein Kazerouni",
      "Bobby Azad",
      "Reza Azad",
      "Dorit Merhof"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Kim_Chest_X-Ray_Feature_Pyramid_Sum_Model_with_Diseased_Area_Data_ICCVW_2023_paper.html": {
    "title": "Chest X-Ray Feature Pyramid Sum Model with Diseased Area Data Augmentation Method",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changhyun Kim",
      "Giyeol Kim",
      "Sooyoung Yang",
      "Hyunsu Kim",
      "Sangyool Lee",
      "Hansu Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Rajapaksa_Using_Large_Text_To_Image_Models_with_Structured_Prompts_for_ICCVW_2023_paper.html": {
    "title": "Using Large Text To Image Models with Structured Prompts for Skin Disease Identification: A Case Study",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sajith Rajapaksa",
      "Jean Marie Uwabeza Vianney",
      "Renell Castro",
      "Farzad Khalvati",
      "Shubhra Aich"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Lai_CLIPath_Fine-Tune_CLIP_with_Visual_Feature_Fusion_for_Pathology_Image_ICCVW_2023_paper.html": {
    "title": "CLIPath: Fine-Tune CLIP with Visual Feature Fusion for Pathology Image Analysis Towards Minimizing Data Collection Efforts",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengfeng Lai",
      "Zhuoheng Li",
      "Luca Cerny Oliveira",
      "Joohi Chauhan",
      "Brittany N. Dugger",
      "Chen-Nee Chuah"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Kligvasser_Semi-Supervised_Quality_Evaluation_of_Colonoscopy_Procedures_ICCVW_2023_paper.html": {
    "title": "Semi-Supervised Quality Evaluation of Colonoscopy Procedures",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Idan Kligvasser",
      "George Leifman",
      "Roman Goldenberg",
      "Ehud Rivlin",
      "Michael Elad"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Le_RRc-UNet_3D_for_Lung_Tumor_Segmentation_from_CT_Scans_of_ICCVW_2023_paper.html": {
    "title": "RRc-UNet 3D for Lung Tumor Segmentation from CT Scans of Non-Small Cell Lung Cancer Patients",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Van-Linh Le",
      "Olivier Saut"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Hall_Vision-Language_Models_Performing_Zero-Shot_Tasks_Exhibit_Disparities_Between_Gender_Groups_ICCVW_2023_paper.html": {
    "title": "Vision-Language Models Performing Zero-Shot Tasks Exhibit Disparities Between Gender Groups",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Melissa Hall",
      "Laura Gustafson",
      "Aaron Adcock",
      "Ishan Misra",
      "Candace Ross"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Schwettmann_Multimodal_Neurons_in_Pretrained_Text-Only_Transformers_ICCVW_2023_paper.html": {
    "title": "Multimodal Neurons in Pretrained Text-Only Transformers",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Schwettmann",
      "Neil Chowdhury",
      "Samuel Klein",
      "David Bau",
      "Antonio Torralba"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Masala_Explaining_Vision_and_Language_Through_Graphs_of_Events_in_Space_ICCVW_2023_paper.html": {
    "title": "Explaining Vision and Language Through Graphs of Events in Space and Time",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mihai Masala",
      "Nicolae Cudlenco",
      "Traian Rebedea",
      "Marius Leordeanu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Panousis_Sparse_Linear_Concept_Discovery_Models_ICCVW_2023_paper.html": {
    "title": "Sparse Linear Concept Discovery Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantinos Panagiotis Panousis",
      "Dino Ienco",
      "Diego Marcos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Ma_LLaViLo_Boosting_Video_Moment_Retrieval_via_Adapter-Based_Multimodal_Modeling_ICCVW_2023_paper.html": {
    "title": "LLaViLo: Boosting Video Moment Retrieval via Adapter-Based Multimodal Modeling",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaijing Ma",
      "Xianghao Zang",
      "Zerun Feng",
      "Han Fang",
      "Chao Ban",
      "Yuhan Wei",
      "Zhongjiang He",
      "Yongxiang Li",
      "Hao Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/De_la_Jara_An_Empirical_Study_of_the_Effect_of_Video_Encoders_on_ICCVW_2023_paper.html": {
    "title": "An Empirical Study of the Effect of Video Encoders on Temporal Video Grounding",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ignacio M. De la Jara",
      "Cristian Rodriguez-Opazo",
      "Edison Marrese-Taylor",
      "Felipe Bravo-Marquez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Aubakirova_PatFig_Generating_Short_and_Long_Captions_for_Patent_Figures_ICCVW_2023_paper.html": {
    "title": "PatFig: Generating Short and Long Captions for Patent Figures",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dana Aubakirova",
      "Kim Gerdes",
      "Lufei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/de_Avellar_Sarmento_A_Cross-Dataset_Study_on_the_Brazilian_Sign_Language_Translation_ICCVW_2023_paper.html": {
    "title": "A Cross-Dataset Study on the Brazilian Sign Language Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amanda Hellen de Avellar Sarmento",
      "Moacir Antonelli Ponti"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Agnolucci_ECO_Ensembling_Context_Optimization_for_Vision-Language_Models_ICCVW_2023_paper.html": {
    "title": "ECO: Ensembling Context Optimization for Vision-Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Agnolucci",
      "Alberto Baldrati",
      "Francesco Todino",
      "Federico Becattini",
      "Marco Bertini",
      "Alberto Del Bimbo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Reichman_Cross-Modal_Dense_Passage_Retrieval_for_Outside_Knowledge_Visual_Question_Answering_ICCVW_2023_paper.html": {
    "title": "Cross-Modal Dense Passage Retrieval for Outside Knowledge Visual Question Answering",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Reichman",
      "Larry Heck"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Palit_Towards_Vision-Language_Mechanistic_Interpretability_A_Causal_Tracing_Tool_for_BLIP_ICCVW_2023_paper.html": {
    "title": "Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vedant Palit",
      "Rohan Pandey",
      "Aryaman Arora",
      "Paul Pu Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Burbi_Mapping_Memes_to_Words_for_Multimodal_Hateful_Meme_Classification_ICCVW_2023_paper.html": {
    "title": "Mapping Memes to Words for Multimodal Hateful Meme Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni Burbi",
      "Alberto Baldrati",
      "Lorenzo Agnolucci",
      "Marco Bertini",
      "Alberto Del Bimbo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Fujii_BiLMa_Bidirectional_Local-Matching_for_Text-based_Person_Re-identification_ICCVW_2023_paper.html": {
    "title": "BiLMa: Bidirectional Local-Matching for Text-based Person Re-identification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takuro Fujii",
      "Shuhei Tarashima"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Hu_ProVLA_Compositional_Image_Search_with_Progressive_Vision-Language_Alignment_and_Multimodal_ICCVW_2023_paper.html": {
    "title": "ProVLA: Compositional Image Search with Progressive Vision-Language Alignment and Multimodal Fusion",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhizhang Hu",
      "Xinliang Zhu",
      "Son Tran",
      "RenÃ© Vidal",
      "Arnab Dhua"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Naik_Context-VQA_Towards_Context-Aware_and_Purposeful_Visual_Question_Answering_ICCVW_2023_paper.html": {
    "title": "Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nandita Naik",
      "Christopher Potts",
      "Elisa Kreiss"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Engin_Zero-Shot_and_Few-Shot_Video_Question_Answering_with_Multi-Modal_Prompts_ICCVW_2023_paper.html": {
    "title": "Zero-Shot and Few-Shot Video Question Answering with Multi-Modal Prompts",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deniz Engin",
      "Yannis Avrithis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Fang_Alignment_and_Generation_Adapter_for_Efficient_Video-Text_Understanding_ICCVW_2023_paper.html": {
    "title": "Alignment and Generation Adapter for Efficient Video-Text Understanding",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Fang",
      "Zhifei Yang",
      "Yuhan Wei",
      "Xianghao Zang",
      "Chao Ban",
      "Zerun Feng",
      "Zhongjiang He",
      "Yongxiang Li",
      "Hao Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/html/Amaduzzi_Looking_at_Words_and_Points_with_Attention_a_Benchmark_for_ICCVW_2023_paper.html": {
    "title": "Looking at Words and Points with Attention: a Benchmark for text-to-Shape Coherence",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Amaduzzi",
      "Giuseppe Lisanti",
      "Samuele Salti",
      "Luigi Di Stefano"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/html/Hamdi_SPARF_Large-Scale_Learning_of_3D_Sparse_Radiance_Fields_from_Few_ICCVW_2023_paper.html": {
    "title": "SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input Images",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdullah Hamdi",
      "Bernard Ghanem",
      "Matthias NieÃsner"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/html/Gordon_Blended-NeRF_Zero-Shot_Object_Generation_and_Blending_in_Existing_Neural_Radiance_ICCVW_2023_paper.html": {
    "title": "Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ori Gordon",
      "Omri Avrahami",
      "Dani Lischinski"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/html/Simsar_LatentSwap3D_Semantic_Edits_on_3D_Image_GANs_ICCVW_2023_paper.html": {
    "title": "LatentSwap3D: Semantic Edits on 3D Image GANs",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enis Simsar",
      "Alessio Tonioni",
      "Evin Pinar Ornek",
      "Federico Tombari"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/html/Kumar_S2RF_Semantically_Stylized_Radiance_Fields_ICCVW_2023_paper.html": {
    "title": "S2RF: Semantically Stylized Radiance Fields",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moneish Kumar",
      "Neeraj Panse",
      "Dishani Lahiri"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/html/Shahbazi_NeRF-GAN_Distillation_for_Efficient_3D-Aware_Generation_with_Convolutions_ICCVW_2023_paper.html": {
    "title": "NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamad Shahbazi",
      "Evangelos Ntavelis",
      "Alessio Tonioni",
      "Edo Collins",
      "Danda Pani Paudel",
      "Martin Danelljan",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/html/Wei_BuilDiff_3D_Building_Shape_Generation_Using_Single-Image_Conditional_Point_Cloud_ICCVW_2023_paper.html": {
    "title": "BuilDiff: 3D Building Shape Generation Using Single-Image Conditional Point Cloud Diffusion Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Wei",
      "George Vosselman",
      "Michael Ying Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/html/Cohen-Bar_Set-the-Scene_Global-Local_Training_for_Generating_Controllable_NeRF_Scenes_ICCVW_2023_paper.html": {
    "title": "Set-the-Scene: Global-Local Training for Generating Controllable NeRF Scenes",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dana Cohen-Bar",
      "Elad Richardson",
      "Gal Metzer",
      "Raja Giryes",
      "Daniel Cohen-Or"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/html/Courant_BluNF_Blueprint_Neural_Field_ICCVW_2023_paper.html": {
    "title": "BluNF: Blueprint Neural Field",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robin Courant",
      "Xi Wang",
      "Marc Christie",
      "Vicky Kalogeiton"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVEU/html/Warchocki_Benchmarking_Data_Efficiency_and_Computational_Efficiency_of_Temporal_Action_Localization_ICCVW_2023_paper.html": {
    "title": "Benchmarking Data Efficiency and Computational Efficiency of Temporal Action Localization Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Warchocki",
      "Teodor Oprescu",
      "Yunhan Wang",
      "Alexandru DÄmÄcuÅ",
      "Paul Misterka",
      "Robert-Jan Bruintjes",
      "Attila Lengyel",
      "Ombretta Strafforello",
      "Jan van Gemert"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVEU/html/de_Boer_Is_There_Progress_in_Activity_Progress_Prediction_ICCVW_2023_paper.html": {
    "title": "Is There Progress in Activity Progress Prediction?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frans de Boer",
      "Jan C. van Gemert",
      "Jouke Dijkstra",
      "Silvia L. Pintea"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVEU/html/Khandelwal_InFusion_Inject_and_Attention_Fusion_for_Multi_Concept_Zero-Shot_Text-Based_ICCVW_2023_paper.html": {
    "title": "InFusion: Inject and Attention Fusion for Multi Concept Zero-Shot Text-Based Video Editing",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anant Khandelwal"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVEU/html/Sardari_PAT_Position-Aware_Transformer_for_Dense_Multi-Label_Action_Detection_ICCVW_2023_paper.html": {
    "title": "PAT: Position-Aware Transformer for Dense Multi-Label Action Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Faegheh Sardari",
      "Armin Mustafa",
      "Philip J. B. Jackson",
      "Adrian Hilton"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVEU/html/Oorloff_Expressive_Talking_Head_Video_Encoding_in_StyleGAN2_Latent_Space_ICCVW_2023_paper.html": {
    "title": "Expressive Talking Head Video Encoding in StyleGAN2 Latent Space",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trevine Oorloff",
      "Yaser Yacoob"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVEU/html/Strafforello_Are_Current_Long-Term_Video_Understanding_Datasets_Long-Term_ICCVW_2023_paper.html": {
    "title": "Are Current Long-Term Video Understanding Datasets Long-Term?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ombretta Strafforello",
      "Klamer Schutte",
      "Jan van Gemert"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVEU/html/Chen_VAST_Vivify_Your_Talking_Avatar_via_Zero-Shot_Expressive_Facial_Style_ICCVW_2023_paper.html": {
    "title": "VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style Transfer",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyang Chen",
      "Zhiyong Wu",
      "Runnan Li",
      "Weihong Bao",
      "Jun Ling",
      "Xu Tan",
      "Sheng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CVEU/html/Vacchetti_LEMMS_Label_Estimation_of_Multi-Feature_Movie_Segments_ICCVW_2023_paper.html": {
    "title": "LEMMS: Label Estimation of Multi-Feature Movie Segments",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bartolomeo Vacchetti",
      "Dawit Mureja",
      "Tania Cerquitelli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ASI/html/Kim_D-ViSA_A_Dataset_for_Detecting_Visual_Sentiment_from_Art_Images_ICCVW_2023_paper.html": {
    "title": "D-ViSA: A Dataset for Detecting Visual Sentiment from Art Images",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seoyun Kim",
      "ChaeHee An",
      "Junyeop Cha",
      "Dongjae Kim",
      "Eunil Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ASI/html/Pirhadi_Just_Ask_Plus_Using_Transcripts_for_VideoQA_ICCVW_2023_paper.html": {
    "title": "Just Ask Plus: Using Transcripts for VideoQA",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Javad Pirhadi",
      "Motahhare Mirzaei",
      "Sauleh Eetemadi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ASI/html/Yoo_Pointing_Gesture_Recognition_via_Self-Supervised_Regularization_for_ASD_Screening_ICCVW_2023_paper.html": {
    "title": "Pointing Gesture Recognition via Self-Supervised Regularization for ASD Screening",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheol-Hwan Yoo",
      "Jang-Hee Yoo",
      "Ho-Won Kim",
      "ByungOk Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ASI/html/Dhaussy_Interaction_Acceptance_Modelling_and_Estimation_for_a_Proactive_Engagement_in_ICCVW_2023_paper.html": {
    "title": "Interaction Acceptance Modelling and Estimation for a Proactive Engagement in the Context of Human-Robot Interactions",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "TimothÃ©e Dhaussy",
      "Bassam Jabaian",
      "Fabrice LefÃ¨vre"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ASI/html/Natu_External_Commonsense_Knowledge_as_a_Modality_for_Social_Intelligence_Question-Answering_ICCVW_2023_paper.html": {
    "title": "External Commonsense Knowledge as a Modality for Social Intelligence Question-Answering",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanika Natu",
      "Shounak Sural",
      "Sulagna Sarkar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ASI/html/Corbellini_Few_Labels_are_Enough_Semi-Supervised_Graph_Learning_for_Social_Interaction_ICCVW_2023_paper.html": {
    "title": "Few Labels are Enough! Semi-Supervised Graph Learning for Social Interaction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicola Corbellini",
      "Jhony H. Giraldo",
      "Giovanna Varni",
      "Gualtiero Volpe"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/ASI/html/Xie_Multi-Modal_Correlated_Network_with_Emotional_Reasoning_Knowledge_for_Social_Intelligence_ICCVW_2023_paper.html": {
    "title": "Multi-Modal Correlated Network with Emotional Reasoning Knowledge for Social Intelligence Question-Answering",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baijun Xie",
      "Chung Hyuk Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Choithwani_PoseBias_On_Dataset_Bias_and_Task_Difficulty_-_Is_There_ICCVW_2023_paper.html": {
    "title": "PoseBias: On Dataset Bias and Task Difficulty - Is There an Optimal Camera Position for Facial Image Analysis?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohit Choithwani",
      "Sneha Almeida",
      "Bernhard Egger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Zheng_POSTER_A_Pyramid_Cross-Fusion_Transformer_Network_for_Facial_Expression_Recognition_ICCVW_2023_paper.html": {
    "title": "POSTER: A Pyramid Cross-Fusion Transformer Network for Facial Expression Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ce Zheng",
      "Matias Mendieta",
      "Chen Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Sarhan_Unraveling_a_Decade_A_Comprehensive_Survey_on_Isolated_Sign_Language_ICCVW_2023_paper.html": {
    "title": "Unraveling a Decade: A Comprehensive Survey on Isolated Sign Language Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noha Sarhan",
      "Simone Frintrop"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Ivashechkin_Denoising_Diffusion_for_3D_Hand_Pose_Estimation_from_Images_ICCVW_2023_paper.html": {
    "title": "Denoising Diffusion for 3D Hand Pose Estimation from Images",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maksym Ivashechkin",
      "Oscar Mendez",
      "Richard Bowden"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Qammaz_A_Unified_Approach_for_Occlusion_Tolerant_3D_Facial_Pose_Capture_ICCVW_2023_paper.html": {
    "title": "A Unified Approach for Occlusion Tolerant 3D Facial Pose Capture and Gaze Estimation Using MocapNETs",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ammar Qammaz",
      "Antonis A. Argyros"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Kumar_Disjoint_Pose_and_Shape_for_3D_Face_Reconstruction_ICCVW_2023_paper.html": {
    "title": "Disjoint Pose and Shape for 3D Face Reconstruction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raja Kumar",
      "Jiahao Luo",
      "Alex Pang",
      "James Davis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Su_Kinship_Representation_Learning_with_Face_Componential_Relation_ICCVW_2023_paper.html": {
    "title": "Kinship Representation Learning with Face Componential Relation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wengtai Su",
      "Min-Hung Chen",
      "Chien-Yi Wang",
      "Shang-Hong Lai",
      "Trista Pei-chun Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Melzi_GANDiffFace_Controllable_Generation_of_Synthetic_Datasets_for_Face_Recognition_with_ICCVW_2023_paper.html": {
    "title": "GANDiffFace: Controllable Generation of Synthetic Datasets for Face Recognition with Realistic Variations",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pietro Melzi",
      "Christian Rathgeb",
      "Ruben Tolosana",
      "Ruben Vera-Rodriguez",
      "Dominik Lawatsch",
      "Florian Domin",
      "Maxim Schaubert"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Doering_A_Gated_Attention_Transformer_for_Multi-Person_Pose_Tracking_ICCVW_2023_paper.html": {
    "title": "A Gated Attention Transformer for Multi-Person Pose Tracking",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Doering",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Xu_Occluded_Gait_Recognition_via_Silhouette_Registration_Guided_by_Automated_Occlusion_ICCVW_2023_paper.html": {
    "title": "Occluded Gait Recognition via Silhouette Registration Guided by Automated Occlusion Degree Estimation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Xu",
      "Shogo Tsuji",
      "Yasushi Makihara",
      "Xiang Li",
      "Yasushi Yagi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Karvounas_Dynamic_Multiview_Refinement_of_3D_Hand_Datasets_Using_Differentiable_Ray_ICCVW_2023_paper.html": {
    "title": "Dynamic Multiview Refinement of 3D Hand Datasets Using Differentiable Ray Tracing",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giorgos Karvounas",
      "Nikolaos Kyriazis",
      "Iason Oikonomidis",
      "Antonis Argyros"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Kansy_Controllable_Inversion_of_Black-Box_Face_Recognition_Models_via_Diffusion_ICCVW_2023_paper.html": {
    "title": "Controllable Inversion of Black-Box Face Recognition Models via Diffusion",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Kansy",
      "Anton RaÃ«l",
      "Graziana Mignone",
      "Jacek Naruniec",
      "Christopher Schroers",
      "Markus Gross",
      "Romann M. Weber"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Rommel_DiffHPE_Robust_Coherent_3D_Human_Pose_Lifting_with_Diffusion_ICCVW_2023_paper.html": {
    "title": "DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cedric Rommel",
      "Eduardo Valle",
      "Mickael Chen",
      "Souhaiel Khalfaoui",
      "Renaud Marlet",
      "Matthieu Cord",
      "Patrick Perez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AMFG/html/Marchellus_M2C_Concise_Music_Representation_for_3D_Dance_Generation_ICCVW_2023_paper.html": {
    "title": "M2C: Concise Music Representation for 3D Dance Generation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Marchellus",
      "In Kyu Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Sakaino_Unseen_And_Adverse_Outdoor_Scenes_Recognition_Through_Event-Based_Captions_ICCVW_2023_paper.html": {
    "title": "Unseen And Adverse Outdoor Scenes Recognition Through Event-Based Captions",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hidetomo Sakaino"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Chakrabarty_A_Simple_Signal_for_Domain_Shift_ICCVW_2023_paper.html": {
    "title": "A Simple Signal for Domain Shift",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Goirik Chakrabarty",
      "Manogna Sreenivas",
      "Soma Biswas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Xu_Progressive_Feature_Adjustment_for_Semi-Supervised_Learning_from_Pretrained_Models_ICCVW_2023_paper.html": {
    "title": "Progressive Feature Adjustment for Semi-Supervised Learning from Pretrained Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai-Ming Xu",
      "Lingqiao Liu",
      "Hao Chen",
      "Ehsan Abbasnejad",
      "Rafael Felix"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Hurtado_Memory_Population_in_Continual_Learning_via_Outlier_Elimination_ICCVW_2023_paper.html": {
    "title": "Memory Population in Continual Learning via Outlier Elimination",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julio Hurtado",
      "Alain Raymond-SÃ¡ez",
      "Vladimir Araujo",
      "Vincenzo Lomonaco",
      "Alvaro Soto",
      "Davide Bacciu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Hekimoglu_Multi-Task_Consistency_for_Active_Learning_ICCVW_2023_paper.html": {
    "title": "Multi-Task Consistency for Active Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aral Hekimoglu",
      "Philipp Friedrich",
      "Walter Zimmer",
      "Michael Schmidt",
      "Alvaro Marcos-Ramiro",
      "Alois Knoll"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/DAlessandro_Multimodal_Parameter-Efficient_Few-Shot_Class_Incremental_Learning_ICCVW_2023_paper.html": {
    "title": "Multimodal Parameter-Efficient Few-Shot Class Incremental Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco D'Alessandro",
      "Alberto Alonso",
      "Enrique CalabrÃ©s",
      "Mikel Galar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Brignac_Improving_Replay_Sample_Selection_and_Storage_for_Less_Forgetting_in_ICCVW_2023_paper.html": {
    "title": "Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Brignac",
      "Niels Lobo",
      "Abhijit Mahalanobis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Jodelet_Class-Incremental_Learning_Using_Diffusion_Model_for_Distillation_and_Replay_ICCVW_2023_paper.html": {
    "title": "Class-Incremental Learning Using Diffusion Model for Distillation and Replay",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quentin Jodelet",
      "Xin Liu",
      "Yin Jun Phua",
      "Tsuyoshi Murata"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Singh_Trajectory-Prediction_with_Vision_A_Survey_ICCVW_2023_paper.html": {
    "title": "Trajectory-Prediction with Vision: A Survey",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Apoorv Singh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Yang_ScrollNet_DynamicWeight_Importance_for_Continual_Learning_ICCVW_2023_paper.html": {
    "title": "ScrollNet: DynamicWeight Importance for Continual Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Yang",
      "Kai Wang",
      "Joost van de Weijer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Liu_Memory-Augmented_Variational_Adaptation_for_Online_Few-Shot_Segmentation_ICCVW_2023_paper.html": {
    "title": "Memory-Augmented Variational Adaptation for Online Few-Shot Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Liu",
      "Yingjun Du",
      "Zehao Xiao",
      "Cees G.M Snoek",
      "Jan-Jakob Sonke",
      "Efstratios Gavves"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Sojka_AR-TTA_A_Simple_Method_for_Real-World_Continual_Test-Time_Adaptation_ICCVW_2023_paper.html": {
    "title": "AR-TTA: A Simple Method for Real-World Continual Test-Time Adaptation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damian SÃ³jka",
      "Sebastian Cygert",
      "BartÅomiej Twardowski",
      "Tomasz TrzciÅski"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Wagner_Comparative_Study_of_Natural_Replay_and_Experience_Replay_in_Online_ICCVW_2023_paper.html": {
    "title": "Comparative Study of Natural Replay and Experience Replay in Online Object Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baptiste Wagner",
      "Denis Pellerin",
      "Sylvain Huet"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Sorrenti_Selective_Freezing_for_Efficient_Continual_Learning_ICCVW_2023_paper.html": {
    "title": "Selective Freezing for Efficient Continual Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amelia Sorrenti",
      "Giovanni Bellitto",
      "Federica Proietto Salanitri",
      "Matteo Pennisi",
      "Concetto Spampinato",
      "Simone Palazzo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Pirvu_Multi-Task_Hypergraphs_for_Semi-Supervised_Learning_Using_Earth_Observations_ICCVW_2023_paper.html": {
    "title": "Multi-Task Hypergraphs for Semi-Supervised Learning Using Earth Observations",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mihai Pirvu",
      "Alina Marcu",
      "Maria Alexandra Dobrescu",
      "Ahmed Nabil Belbachir",
      "Marius Leordeanu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Shangguan_Identification_of_Novel_Classes_for_Improving_Few-Shot_Object_Detection_ICCVW_2023_paper.html": {
    "title": "Identification of Novel Classes for Improving Few-Shot Object Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Shangguan",
      "Mohammad Rostami"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Khawand_Continual_Learning_with_Deep_Streaming_Regularized_Discriminant_Analysis_ICCVW_2023_paper.html": {
    "title": "Continual Learning with Deep Streaming Regularized Discriminant Analysis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joe Khawand",
      "Peter Hanappe",
      "David Colliaux"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Guo_Decision_Boundary_Optimization_for_Few-Shot_Class-Incremental_Learning_ICCVW_2023_paper.html": {
    "title": "Decision Boundary Optimization for Few-Shot Class-Incremental Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxu Guo",
      "Qi Zhao",
      "Shuchang Lyu",
      "Binghao Liu",
      "Chunlei Wang",
      "Lijiang Chen",
      "Guangliang Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Szatkowski_Adapt_Your_Teacher_Improving_Knowledge_Distillation_for_Exemplar-Free_Continual_Learning_ICCVW_2023_paper.html": {
    "title": "Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-Free Continual Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filip Szatkowski",
      "Mateusz Pyla",
      "Marcin PrzewiÄÅºlikowski",
      "Sebastian Cygert",
      "BartÅomiej Twardowski",
      "Tomasz TrzciÅski"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Chen_SAM-Adapter_Adapting_Segment_Anything_in_Underperformed_Scenes_ICCVW_2023_paper.html": {
    "title": "SAM-Adapter: Adapting Segment Anything in Underperformed Scenes",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianrun Chen",
      "Lanyun Zhu",
      "Chaotao Deng",
      "Runlong Cao",
      "Yan Wang",
      "Shangzhan Zhang",
      "Zejian Li",
      "Lingyun Sun",
      "Ying Zang",
      "Papa Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Mahmoodi_Flashback_for_Continual_Learning_ICCVW_2023_paper.html": {
    "title": "Flashback for Continual Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leila Mahmoodi",
      "Mehrtash Harandi",
      "Peyman Moghadam"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Wang_Confusion_Mixup_Regularized_Multimodal_Fusion_Network_for_Continual_Egocentric_Activity_ICCVW_2023_paper.html": {
    "title": "Confusion Mixup Regularized Multimodal Fusion Network for Continual Egocentric Activity Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxin Wang",
      "Shuchang Zhou",
      "Qingbo Wu",
      "Hongliang Li",
      "Fanman Meng",
      "Linfeng Xu",
      "Heqian Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Xu_OpenIncrement_A_Unified_Framework_for_Open_Set_Recognition_and_Deep_ICCVW_2023_paper.html": {
    "title": "OpenIncrement: A Unified Framework for Open Set Recognition and Deep Class-Incremental Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawen Xu",
      "Claas Grohnfeldt",
      "Odej Kao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Nagata_Margin_Contrastive_Learning_with_Learnable-Vector_for_Continual_Learning_ICCVW_2023_paper.html": {
    "title": "Margin Contrastive Learning with Learnable-Vector for Continual Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kotaro Nagata",
      "Kazuhiro Hotta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Kanagarajah_SATHUR_Self_Augmenting_Task_Hallucinal_Unified_Representation_for_Generalized_Class_ICCVW_2023_paper.html": {
    "title": "SATHUR: Self Augmenting Task Hallucinal Unified Representation for Generalized Class Incremental Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sathursan Kanagarajah",
      "Thanuja Ambegoda",
      "Ranga Rodrigo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Singh_Transformer-Based_Sensor_Fusion_for_Autonomous_Driving_A_Survey_ICCVW_2023_paper.html": {
    "title": "Transformer-Based Sensor Fusion for Autonomous Driving: A Survey",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Apoorv Singh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Aguilar_Continual_Evidential_Deep_Learning_for_Out-of-Distribution_Detection_ICCVW_2023_paper.html": {
    "title": "Continual Evidential Deep Learning for Out-of-Distribution Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduardo Aguilar",
      "Bogdan Raducanu",
      "Petia Radeva",
      "Joost Van de Weijer"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Psaltis_FedRCIL_Federated_Knowledge_Distillation_for_Representation_based_Contrastive_Incremental_Learning_ICCVW_2023_paper.html": {
    "title": "FedRCIL: Federated Knowledge Distillation for Representation based Contrastive Incremental Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Athanasios Psaltis",
      "Christos Chatzikonstantinou",
      "Charalampos Z. Patrikakis",
      "Petros Daras"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Po_Instant_Continual_Learning_of_Neural_Radiance_Fields_ICCVW_2023_paper.html": {
    "title": "Instant Continual Learning of Neural Radiance Fields",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Po",
      "Zhengyang Dong",
      "Alexander W. Bergman",
      "Gordon Wetzstein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Soutif-Cormerais_A_Comprehensive_Empirical_Evaluation_on_Online_Continual_Learning_ICCVW_2023_paper.html": {
    "title": "A Comprehensive Empirical Evaluation on Online Continual Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Albin Soutif-Cormerais",
      "Antonio Carta",
      "Andrea Cossu",
      "Julio Hurtado",
      "Vincenzo Lomonaco",
      "Joost Van de Weijer",
      "Hamed Hemati"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Lamers_Clustering-Based_Domain-Incremental_Learning_ICCVW_2023_paper.html": {
    "title": "Clustering-Based Domain-Incremental Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christiaan Lamers",
      "RenÃ© Vidal",
      "Nabil Belbachir",
      "Niki van Stein",
      "Thomas BÃ¤eck",
      "Paris Giampouras"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Xiang_TKIL_Tangent_Kernel_Optimization_for_Class_Balanced_Incremental_Learning_ICCVW_2023_paper.html": {
    "title": "TKIL: Tangent Kernel Optimization for Class Balanced Incremental Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinlin Xiang",
      "Eli Shlizerman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Khan_Looking_Through_the_Past_Better_Knowledge_Retention_for_Generative_Replay_ICCVW_2023_paper.html": {
    "title": "Looking Through the Past: Better Knowledge Retention for Generative Replay in Continual Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valeriya Khan",
      "Sebastian Cygert",
      "Bartlomiej Twardowski",
      "Tomasz TrzciÅski"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Pennisi_Experience_Replay_as_an_Effective_Strategy_for_Optimizing_Decentralized_Federated_ICCVW_2023_paper.html": {
    "title": "Experience Replay as an Effective Strategy for Optimizing Decentralized Federated Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Pennisi",
      "Federica Proietto Salanitri",
      "Giovanni Bellitto",
      "Concetto Spampinato",
      "Simone Palazzo",
      "Bruno Casella",
      "Marco Aldinucci"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AROW/html/Baia_Black-Box_Attacks_on_Image_Activity_Prediction_and_its_Natural_Language_ICCVW_2023_paper.html": {
    "title": "Black-Box Attacks on Image Activity Prediction and its Natural Language Explanations",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alina Elena Baia",
      "Valentina Poggioni",
      "Andrea Cavallaro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AROW/html/Agnihotri_On_the_Unreasonable_Vulnerability_of_Transformers_for_Image_Restoration_-_ICCVW_2023_paper.html": {
    "title": "On the Unreasonable Vulnerability of Transformers for Image Restoration - and an easy fix",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashank Agnihotri",
      "Kanchana Vaishnavi Gandikota",
      "Julia Grabinski",
      "Paramanand Chandramouli",
      "Margret Keuper"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AROW/html/Azuma_Defense-Prefix_for_Preventing_Typographic_Attacks_on_CLIP_ICCVW_2023_paper.html": {
    "title": "Defense-Prefix for Preventing Typographic Attacks on CLIP",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroki Azuma",
      "Yusuke Matsui"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AROW/html/Vats_Adversarial_Examples_with_Specular_Highlights_ICCVW_2023_paper.html": {
    "title": "Adversarial Examples with Specular Highlights",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vanshika Vats",
      "Koteswar Rao Jerripothula"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AROW/html/Ambati_PRAT_PRofiling_Adversarial_a_Ttacks_ICCVW_2023_paper.html": {
    "title": "PRAT: PRofiling Adversarial a Ttacks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Ambati",
      "Naveed Akhtar",
      "Ajmal Mian",
      "Yogesh S Rawat"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AROW/html/Wu_Fair_Robust_Active_Learning_by_Joint_Inconsistency_ICCVW_2023_paper.html": {
    "title": "Fair Robust Active Learning by Joint Inconsistency",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsung-Han Wu",
      "Hung-Ting Su",
      "Shang-Tse Chen",
      "Winston H. Hsu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AROW/html/Tal_OMG-ATTACK_Self-Supervised_On-Manifold_Generation_of_Transferable_Evasion_Attacks_ICCVW_2023_paper.html": {
    "title": "OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ofir Bar Tal",
      "Adi Haviv",
      "Amit H. Bermano"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AROW/html/Sakaino_Semantically_Enhanced_Scene_Captions_with_Physical_and_Weather_Condition_Changes_ICCVW_2023_paper.html": {
    "title": "Semantically Enhanced Scene Captions with Physical and Weather Condition Changes",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hidetomo Sakaino"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AROW/html/Muller_Classification_Robustness_to_Common_Optical_Aberrations_ICCVW_2023_paper.html": {
    "title": "Classification Robustness to Common Optical Aberrations",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick MÃ¼ller",
      "Alexander Braun",
      "Margret Keuper"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AROW/html/Horvath_Targeted_Adversarial_Attacks_on_Generalizable_Neural_Radiance_Fields_ICCVW_2023_paper.html": {
    "title": "Targeted Adversarial Attacks on Generalizable Neural Radiance Fields",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "AndrÃ¡s HorvÃ¡th",
      "Csaba M. JÃ³zsa"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AROW/html/Schlarmann_On_the_Adversarial_Robustness_of_Multi-Modal_Foundation_Models_ICCVW_2023_paper.html": {
    "title": "On the Adversarial Robustness of Multi-Modal Foundation Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Schlarmann",
      "Matthias Hein"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AROW/html/Jiang_IPCert_Provably_Robust_Intellectual_Property_Protection_for_Machine_Learning_ICCVW_2023_paper.html": {
    "title": "IPCert: Provably Robust Intellectual Property Protection for Machine Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Jiang",
      "Minghong Fang",
      "Neil Zhenqiang Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AIHADR/html/Constantinou_Deep_Learning_Driven_Detection_of_Tsunami_Related_Internal_GravityWaves_A_ICCVW_2023_paper.html": {
    "title": "Deep Learning Driven Detection of Tsunami Related Internal GravityWaves: A Path Towards open-Ocean Natural Hazards Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valentino Constantinou",
      "Michela Ravanelli",
      "Hamlin Liu",
      "Jacob Bortnik"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AIHADR/html/Robinson_Rapid_Building_Damage_Assessment_Workflow_An_Implementation_for_the_2023_ICCVW_2023_paper.html": {
    "title": "Rapid Building Damage Assessment Workflow: An Implementation for the 2023 Rolling Fork, Mississippi Tornado Event",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caleb Robinson",
      "Simone Fobi Nsutezo",
      "Anthony Ortiz",
      "Tina Sederholm",
      "Rahul Dodhia",
      "Cameron Birge",
      "Kasie Richards",
      "Kris Pitcher",
      "Paulo Duarte",
      "Juan M. Lavista Ferres"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AIHADR/html/Sun_Rapid_Flood_Inundation_Forecast_Using_Fourier_Neural_Operator_ICCVW_2023_paper.html": {
    "title": "Rapid Flood Inundation Forecast Using Fourier Neural Operator",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Y. Sun",
      "Zhi Li",
      "Wonhyun Lee",
      "Qixing Huang",
      "Bridget R. Scanlon",
      "Clint Dawson"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AIHADR/html/Hu_FireFly_A_Synthetic_Dataset_for_Ember_Detection_in_Wildfire_ICCVW_2023_paper.html": {
    "title": "FireFly: A Synthetic Dataset for Ember Detection in Wildfire",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Hu",
      "Xinan Ye",
      "Yifei Liu",
      "Souvik Kundu",
      "Gourav Datta",
      "Srikar Mutnuri",
      "Namo Asavisanu",
      "Nora Ayanian",
      "Konstantinos Psounis",
      "Peter Beerel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AIHADR/html/Manzini_Open_Problems_in_Computer_Vision_for_Wilderness_SAR_and_The_ICCVW_2023_paper.html": {
    "title": "Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Manzini",
      "Robin Murphy"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AIHADR/html/Tingzon_Fusing_VHR_Post-Disaster_Aerial_Imagery_and_LiDAR_Data_for_Roof_ICCVW_2023_paper.html": {
    "title": "Fusing VHR Post-Disaster Aerial Imagery and LiDAR Data for Roof Classification in the Caribbean",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isabelle Tingzon",
      "Nuala Margaret Cowan",
      "Pierre Chrzanowski"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AIHADR/html/Arai_Estimation_of_Human_Condition_at_Disaster_Site_Using_Aerial_Drone_ICCVW_2023_paper.html": {
    "title": "Estimation of Human Condition at Disaster Site Using Aerial Drone Images",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomoki Arai",
      "Kenji Iwata",
      "Kensho Hara",
      "Yutaka Satoh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AIHADR/html/Puentes_Guarding_the_Guardians_Automated_Analysis_of_Online_Child_Sexual_Abuse_ICCVW_2023_paper.html": {
    "title": "Guarding the Guardians: Automated Analysis of Online Child Sexual Abuse",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juanita Puentes",
      "Angela Castillo",
      "Wilmar Osejo",
      "Yuly CalderÃ³n",
      "Viviana Quintero",
      "Lina Saldarriaga",
      "Diana Agudelo",
      "Pablo ArbelÃ¡ez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AIHADR/html/Prapas_TeleViT_Teleconnection-Driven_Transformers_Improve_Subseasonal_to_Seasonal_Wildfire_Forecasting_ICCVW_2023_paper.html": {
    "title": "TeleViT: Teleconnection-Driven Transformers Improve Subseasonal to Seasonal Wildfire Forecasting",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioannis Prapas",
      "Nikolaos-Ioannis Bountos",
      "Spyros Kondylatos",
      "Dimitrios Michail",
      "Gustau Camps-Valls",
      "Ioannis Papoutsis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/AIHADR/html/Merkle_Drones4Good_Supporting_Disaster_Relief_Through_Remote_Sensing_and_AI_ICCVW_2023_paper.html": {
    "title": "Drones4Good: Supporting Disaster Relief Through Remote Sensing and AI",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina Merkle",
      "Reza Bahmanyar",
      "Corentin Henry",
      "Seyed Majid Azimi",
      "Xiangtian Yuan",
      "Simon Schopferer",
      "Veronika Gstaiger",
      "Stefan Auer",
      "Anne Schneibel",
      "Marc Wieland",
      "Thomas Kraft"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Chen_PCTrans_Position-Guided_Transformer_with_Query_Contrast_for_Biological_Instance_Segmentation_ICCVW_2023_paper.html": {
    "title": "PCTrans: Position-Guided Transformer with Query Contrast for Biological Instance Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Chen",
      "Wei Huang",
      "Xiaoyu Liu",
      "Jiacheng Li",
      "Zhiwei Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Lim_NU-Net_A_Self-Supervised_Smart_Filter_for_Enhancing_Blobs_in_Bioimages_ICCVW_2023_paper.html": {
    "title": "NU-Net: A Self-Supervised Smart Filter for Enhancing Blobs in Bioimages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongbin Lim",
      "Emmanuel Beaurepaire",
      "Anatole Chessel"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Sonneck_On_the_Risk_of_Manual_Annotations_in_3D_Confocal_Microscopy_ICCVW_2023_paper.html": {
    "title": "On the Risk of Manual Annotations in 3D Confocal Microscopy Image Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Sonneck",
      "Shuo Zhao",
      "Jianxu Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Utz_Focus_on_Content_not_Noise_Improving_Image_Generation_for_Nuclei_ICCVW_2023_paper.html": {
    "title": "Focus on Content not Noise: Improving Image Generation for Nuclei Segmentation by Suppressing Steganography in CycleGAN",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Utz",
      "Tobias Weise",
      "Maja Schlereth",
      "Fabian Wagner",
      "Mareike Thies",
      "Mingxuan Gu",
      "Stefan Uderhardt",
      "Katharina Breininger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Salmon_Direct_Unsupervised_Denoising_ICCVW_2023_paper.html": {
    "title": "Direct Unsupervised Denoising",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Salmon",
      "Alexander Krull"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Rumberger_ACTIS_Improving_Data_Efficiency_by_Leveraging_Semi-Supervised_Augmentation_Consistency_Training_ICCVW_2023_paper.html": {
    "title": "ACTIS: Improving Data Efficiency by Leveraging Semi-Supervised Augmentation Consistency Training for Instance Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josef Lorenz Rumberger",
      "Jannik Franzen",
      "Peter Hirsch",
      "Jan-Philipp Albrecht",
      "Dagmar Kainmueller"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Bhattacharya_Generating_Synthetic_Computed_Tomography_CT_Images_to_Improve_the_Performance_ICCVW_2023_paper.html": {
    "title": "Generating Synthetic Computed Tomography (CT) Images to Improve the Performance of Machine Learning Model for Pediatric Abdominal Anomaly Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samayan Bhattacharya",
      "Avigyan Bhattacharya",
      "Sk Shahnawaz"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Hilt_Reinforcement_Learning_for_Instance_Segmentation_with_high-Level_Priors_ICCVW_2023_paper.html": {
    "title": "Reinforcement Learning for Instance Segmentation with high-Level Priors",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Hilt",
      "Maedeh Zarvandi",
      "Edgar Kaziakhmedov",
      "Sourabh Bhide",
      "Maria Leptin",
      "Constantin Pape",
      "Anna Kreshuk"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Soelistyo_Virtual_Perturbations_to_Assess_Explainability_of_Deep-Learning_Based_Cell_Fate_ICCVW_2023_paper.html": {
    "title": "Virtual Perturbations to Assess Explainability of Deep-Learning Based Cell Fate Predictors",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher J. Soelistyo",
      "Guillaume Charras",
      "Alan R. Lowe"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Chobola_Leveraging_Classic_Deconvolution_and_Feature_Extraction_in_Zero-Shot_Image_Restoration_ICCVW_2023_paper.html": {
    "title": "Leveraging Classic Deconvolution and Feature Extraction in Zero-Shot Image Restoration",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "TomÃ¡Å¡ Chobola",
      "Gesine MÃ¼ller",
      "Veit Dausmann",
      "Anton Theileis",
      "Jan Taucher",
      "Jan Huisken",
      "Tingying Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Yarlagadda_Discrete_Representation_Learning_for_Modeling_Imaging-Based_Spatial_Transcriptomics_Data_ICCVW_2023_paper.html": {
    "title": "Discrete Representation Learning for Modeling Imaging-Based Spatial Transcriptomics Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dig Vijay Kumar Yarlagadda",
      "Joan MassaguÃ©",
      "Christina Leslie"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Ebert_Transformer-Based_Detection_of_Microorganisms_on_High-Resolution_Petri_Dish_Images_ICCVW_2023_paper.html": {
    "title": "Transformer-Based Detection of Microorganisms on High-Resolution Petri Dish Images",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolas Ebert",
      "Didier Stricker",
      "Oliver WasenmÃ¼ller"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Reich_The_TYC_Dataset_for_Understanding_Instance-Level_Semantics_and_Motions_of_ICCVW_2023_paper.html": {
    "title": "The TYC Dataset for Understanding Instance-Level Semantics and Motions of Cells in Microstructures",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoph Reich",
      "Tim Prangemeier",
      "Heinz Koeppl"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Chen_SortedAP_Rethinking_Evaluation_Metrics_for_Instance_Segmentation_ICCVW_2023_paper.html": {
    "title": "SortedAP: Rethinking Evaluation Metrics for Instance Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Chen",
      "Yuli Wu",
      "Johannes Stegmaier",
      "Dorit Merhof"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Torem_Complex-Valued_Retrievals_from_Noisy_Images_Using_Diffusion_Models_ICCVW_2023_paper.html": {
    "title": "Complex-Valued Retrievals from Noisy Images Using Diffusion Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nadav Torem",
      "Roi Ronen",
      "Yoav Y. Schechner",
      "Michael Elad"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Cross-Zamirski_Class-Guided_Image-to-Image_Diffusion_Cell_Painting_from_Brightfield_Images_with_Class_ICCVW_2023_paper.html": {
    "title": "Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Oscar Cross-Zamirski",
      "Praveen Anand",
      "Guy Williams",
      "Elizabeth Mouchet",
      "Yinhai Wang",
      "Carola-Bibiane SchÃ¶nlieb"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Martins_DeepContrast_Deep_Tissue_Contrast_Enhancement_Using_Synthetic_Data_Degradations_and_ICCVW_2023_paper.html": {
    "title": "DeepContrast: Deep Tissue Contrast Enhancement Using Synthetic Data Degradations and OOD Model Predictions",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuno PimpÃ£o Martins",
      "Yannis Kalaidzidis",
      "Marino Zerial",
      "Florian Jug"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Tiwari_Deep_Learning_Framework_Using_Sparse_Diffusion_MRI_for_Diagnosis_of_ICCVW_2023_paper.html": {
    "title": "Deep Learning Framework Using Sparse Diffusion MRI for Diagnosis of Frontotemporal Dementia",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhishek Tiwari",
      "Ananya Singhal",
      "Saurabh J. Shigwan",
      "Rajeev Kumar Singh"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Cersovsky_Towards_Hierarchical_Regional_Transformer-Based_Multiple_Instance_Learning_ICCVW_2023_paper.html": {
    "title": "Towards Hierarchical Regional Transformer-Based Multiple Instance Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josef Cersovsky",
      "Sadegh Mohammadi",
      "Dagmar Kainmueller",
      "Johannes Hoehne"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Fillioux_Spatio-Temporal_Analysis_of_Patient-Derived_Organoid_Videos_Using_Deep_Learning_for_ICCVW_2023_paper.html": {
    "title": "Spatio-Temporal Analysis of Patient-Derived Organoid Videos Using Deep Learning for the Prediction of Drug Efficacy",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leo Fillioux",
      "Emilie Gontran",
      "JÃ©rÃ´me Cartry",
      "Jacques RR Mathieu",
      "Sabrina Bedja",
      "Alice BoilÃ¨ve",
      "Paul-Henry CournÃ¨de",
      "Fanny Jaulin",
      "Stergios Christodoulidis",
      "Maria Vakalopoulou"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Schreier_On_Offline_Evaluation_of_3D_Object_Detection_for_Autonomous_Driving_ICCVW_2023_paper.html": {
    "title": "On Offline Evaluation of 3D Object Detection for Autonomous Driving",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Schreier",
      "Katrin Renz",
      "Andreas Geiger",
      "Kashyap Chitta"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Lee_GPS-GLASS_Learning_Nighttime_Semantic_Segmentation_Using_Daytime_Video_and_GPS_ICCVW_2023_paper.html": {
    "title": "GPS-GLASS: Learning Nighttime Semantic Segmentation Using Daytime Video and GPS Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjae Lee",
      "Changwoo Han",
      "Jun-Sang Yoo",
      "Seung-Won Jung"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Zhang_Anomaly-Aware_Semantic_Segmentation_via_Style-Aligned_OoD_Augmentation_ICCVW_2023_paper.html": {
    "title": "Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Zhang",
      "Kaspar Sakmann",
      "William Beluch",
      "Robin Hutmacher",
      "Yumeng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Cordes_Camera-Based_Road_Snow_Coverage_Estimation_ICCVW_2023_paper.html": {
    "title": "Camera-Based Road Snow Coverage Estimation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Cordes",
      "Hellward Broszio"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Giroux_T-FFTRadNet_Object_Detection_with_Swin_Vision_Transformers_from_Raw_ADC_ICCVW_2023_paper.html": {
    "title": "T-FFTRadNet: Object Detection with Swin Vision Transformers from Raw ADC Radar Signals",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Giroux",
      "Martin Bouchard",
      "Robert Laganiere"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Boreiko_Identifying_Systematic_Errors_in_Object_Detectors_with_the_SCROD_Pipeline_ICCVW_2023_paper.html": {
    "title": "Identifying Systematic Errors in Object Detectors with the SCROD Pipeline",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valentyn Boreiko",
      "Matthias Hein",
      "Jan Hendrik Metzen"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Yatbaz_Introspection_of_2D_Object_Detection_Using_Processed_Neural_Activation_Patterns_ICCVW_2023_paper.html": {
    "title": "Introspection of 2D Object Detection Using Processed Neural Activation Patterns in Automated Driving Systems",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hakan Yekta Yatbaz",
      "Mehrdad Dianati",
      "Konstantinos Koufos",
      "Roger Woodman"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Meding_You_can_have_your_ensemble_and_run_it_too_-_ICCVW_2023_paper.html": {
    "title": "You can have your ensemble and run it too - Deep Ensembles Spread Over Time",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isak Meding",
      "Alexander Bodin",
      "Adam Tonderski",
      "Joakim Johnander",
      "Christoffer Petersson",
      "Lennart Svensson"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Niemeijer_Synthetic_Dataset_Acquisition_for_a_Specific_Target_Domain_ICCVW_2023_paper.html": {
    "title": "Synthetic Dataset Acquisition for a Specific Target Domain",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Niemeijer",
      "Sudhanshu Mittal",
      "Thomas Brox"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Gavrikov_On_the_Interplay_of_Convolutional_Padding_and_Adversarial_Robustness_ICCVW_2023_paper.html": {
    "title": "On the Interplay of Convolutional Padding and Adversarial Robustness",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Gavrikov",
      "Janis Keuper"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Zhang_Unsupervised_Domain_Adaptation_for_Self-Driving_from_Past_Traversal_Features_ICCVW_2023_paper.html": {
    "title": "Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Travis Zhang",
      "Katie Luo",
      "Cheng Perng Phoo",
      "Yurong You",
      "Wei-Lun Chao",
      "Bharath Hariharan",
      "Mark Campbell",
      "Kilian Q. Weinberger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Wolf_Sensitivity_Analysis_of_AI-Based_Algorithms_for_Autonomous_Driving_on_Optical_ICCVW_2023_paper.html": {
    "title": "Sensitivity Analysis of AI-Based Algorithms for Autonomous Driving on Optical Wavefront Aberrations Induced by the Windshield",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Werner Wolf",
      "Markus Ulrich",
      "Nikhil Kapoor"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Breitenstein_What_Does_Really_Count_Estimating_Relevance_of_Corner_Cases_for_ICCVW_2023_paper.html": {
    "title": "What Does Really Count? Estimating Relevance of Corner Cases for Semantic Segmentation in Automated Driving",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jasmin Breitenstein",
      "Florian Heidecker",
      "Maria Lyssenko",
      "Daniel Bogdoll",
      "Maarten Bieshaar",
      "J. Marius ZÃ¶llner",
      "Bernhard Sick",
      "Tim Fingscheidt"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/BRAVO/html/Peri_An_Empirical_Analysis_of_Range_for_3D_Object_Detection_ICCVW_2023_paper.html": {
    "title": "An Empirical Analysis of Range for 3D Object Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neehar Peri",
      "Mengtian Li",
      "Benjamin Wilson",
      "Yu-Xiong Wang",
      "James Hays",
      "Deva Ramanan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LXCV/html/Gula_Gaussian_Image_Anomaly_Detection_with_Greedy_Eigencomponent_Selection_ICCVW_2023_paper.html": {
    "title": "Gaussian Image Anomaly Detection with Greedy Eigencomponent Selection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tetiana Gula",
      "JoÃ£o P.C. Bertoldo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LXCV/html/Garcia_An_Experimental_Protocol_for_Neural_Architecture_Search_in_Super-Resolution_ICCVW_2023_paper.html": {
    "title": "An Experimental Protocol for Neural Architecture Search in Super-Resolution",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JesÃºs Leopoldo Llano GarcÃ­a",
      "RaÃºl Monroy",
      "VÃ­ctor AdriÃ¡n Sosa HernÃ¡ndez"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LXCV/html/Lopez-Tiro_Improving_Automatic_Endoscopic_Stone_Recognition_Using_a_Multi-view_Fusion_Approach_ICCVW_2023_paper.html": {
    "title": "Improving Automatic Endoscopic Stone Recognition Using a Multi-view Fusion Approach Enhanced with Two-Step Transfer Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francisco Lopez-Tiro",
      "Elias Villalvazo-Avila",
      "Juan Pablo Betancur-Rengifo",
      "Ivan Reyes-Amezcua",
      "Jacques Hubert",
      "Gilberto Ochoa-Ruiz",
      "Christian Daul"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LXCV/html/Valdenegro-Toro_Sub-Ensembles_for_Fast_Uncertainty_Estimation_in_Neural_Networks_ICCVW_2023_paper.html": {
    "title": "Sub-Ensembles for Fast Uncertainty Estimation in Neural Networks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matias Valdenegro-Toro"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LXCV/html/Urrea_Optical_Solutions_for_Spectral_Imaging_Inverse_Problems_with_a_Shift-Variant_ICCVW_2023_paper.html": {
    "title": "Optical Solutions for Spectral Imaging Inverse Problems with a Shift-Variant System",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergio Urrea",
      "Roman Jacome",
      "M. Salman Asif",
      "Henry Arguello",
      "Hans Garcia"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LXCV/html/Bastico_A_Simple_and_Robust_Framework_for_Cross-Modality_Medical_Image_Segmentation_ICCVW_2023_paper.html": {
    "title": "A Simple and Robust Framework for Cross-Modality Medical Image Segmentation Applied to Vision Transformers",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Bastico",
      "David Ryckelynck",
      "Laurent CortÃ©",
      "Yannick Tillier",
      "Etienne DecenciÃ¨re"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/LXCV/html/Santos_Exploring_Image_Classification_Robustness_and_Interpretability_with_Right_for_the_ICCVW_2023_paper.html": {
    "title": "Exploring Image Classification Robustness and Interpretability with Right for the Right Reasons Data Augmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "FlÃ¡vio Arthur Oliveira Santos",
      "Cleber Zanchettin"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Abdari_FArMARe_a_Furniture-Aware_Multi-Task_Methodology_for_Recommending_Apartments_Based_on_ICCVW_2023_paper.html": {
    "title": "FArMARe: a Furniture-Aware Multi-Task Methodology for Recommending Apartments Based on the user Interests",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Abdari",
      "Alex Falcon",
      "Giuseppe Serra"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Wehrbein_Personalized_3D_Human_Pose_and_Shape_Refinement_ICCVW_2023_paper.html": {
    "title": "Personalized 3D Human Pose and Shape Refinement",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Wehrbein",
      "Bodo Rosenhahn",
      "Iain Matthews",
      "Carsten Stoll"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Van_Holland_Efficient_3D_Reconstruction_Streaming_and_Visualization_of_Static_and_Dynamic_ICCVW_2023_paper.html": {
    "title": "Efficient 3D Reconstruction, Streaming and Visualization of Static and Dynamic Scene Parts for Multi-Client Live-Telepresence in Large-Scale Environments",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leif Van Holland",
      "Patrick Stotko",
      "Stefan Krumpen",
      "Reinhard Klein",
      "Michael Weinmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Lim_MAMMOS_MApping_Multiple_Human_MOtion_with_Scene_Understanding_and_Natural_ICCVW_2023_paper.html": {
    "title": "MAMMOS: MApping Multiple Human MOtion with Scene Understanding and Natural Interactions",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donggeun Lim",
      "Cheongi Jeong",
      "Young Min Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Uboweja_On-Device_Real-Time_Custom_Hand_Gesture_Recognition_ICCVW_2023_paper.html": {
    "title": "On-Device Real-Time Custom Hand Gesture Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Esha Uboweja",
      "David Tian",
      "Qifei Wang",
      "Yi-Chun Kuo",
      "Joe Zou",
      "Lu Wang",
      "George Sung",
      "Matthias Grundmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Albanis_Noise-in_Bias-out_Balanced_and_Real-Time_MoCap_Solving_ICCVW_2023_paper.html": {
    "title": "Noise-in, Bias-out: Balanced and Real-Time MoCap Solving",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Albanis",
      "Nikolaos Zioulis",
      "Spyridon Thermos",
      "Anargyros Chatzitofis",
      "Kostas Kolomvatsos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Castillo_BoDiffusion_Diffusing_Sparse_Observations_for_Full-Body_Human_Motion_Synthesis_ICCVW_2023_paper.html": {
    "title": "BoDiffusion: Diffusing Sparse Observations for Full-Body Human Motion Synthesis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angela Castillo",
      "Maria Escobar",
      "Guillaume Jeanneret",
      "Albert Pumarola",
      "Pablo ArbelÃ¡ez",
      "Ali Thabet",
      "Artsiom Sanakoyeu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Cho_Generative_Approach_for_Probabilistic_Human_Mesh_Recovery_Using_Diffusion_Models_ICCVW_2023_paper.html": {
    "title": "Generative Approach for Probabilistic Human Mesh Recovery Using Diffusion Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanbyel Cho",
      "Junmo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Agrawal_NOVA_NOvel_View_Augmentation_for_Neural_Composition_of_Dynamic_Objects_ICCVW_2023_paper.html": {
    "title": "NOVA: NOvel View Augmentation for Neural Composition of Dynamic Objects",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dakshit Agrawal",
      "Jiajie Xu",
      "Siva Karthik Mustikovela",
      "Ioannis Gkioulekas",
      "Ashish Shrivastava",
      "Yuning Chai"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Park_Extract-and-Adaptation_Network_for_3D_Interacting_Hand_Mesh_Recovery_ICCVW_2023_paper.html": {
    "title": "Extract-and-Adaptation Network for 3D Interacting Hand Mesh Recovery",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joonkyu Park",
      "Daniel Sungho Jung",
      "Gyeongsik Moon",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Yang_Effective_Whole-Body_Pose_Estimation_with_Two-Stages_Distillation_ICCVW_2023_paper.html": {
    "title": "Effective Whole-Body Pose Estimation with Two-Stages Distillation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhendong Yang",
      "Ailing Zeng",
      "Chun Yuan",
      "Yu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Xing_Intrinsic_Appearance_Decomposition_Using_Point_Cloud_Representation_ICCVW_2023_paper.html": {
    "title": "Intrinsic Appearance Decomposition Using Point Cloud Representation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyan Xing",
      "Konrad Groh",
      "Sezer Karaoglu",
      "Theo Gevers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Sun_Temporally_Consistent_Semantic_Segmentation_Using_Spatially_Aware_Multi-view_Semantic_Fusion_ICCVW_2023_paper.html": {
    "title": "Temporally Consistent Semantic Segmentation Using Spatially Aware Multi-view Semantic Fusion for Indoor RGB-D Videos",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyuan Sun",
      "Sezer Karaoglu",
      "Theo Gevers"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Cheong_UPGPT_Universal_Diffusion_Model_for_Person_Image_Generation_Editing_and_ICCVW_2023_paper.html": {
    "title": "UPGPT: Universal Diffusion Model for Person Image Generation, Editing and Pose Transfer",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soon Yau Cheong",
      "Armin Mustafa",
      "Andrew Gilbert"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Singha_AD-CLIP_Adapting_Domains_in_Prompt_Space_Using_CLIP_ICCVW_2023_paper.html": {
    "title": "AD-CLIP: Adapting Domains in Prompt Space Using CLIP",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mainak Singha",
      "Harsh Pal",
      "Ankit Jha",
      "Biplab Banerjee"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Zhang_Unsupervised_Camouflaged_Object_Segmentation_as_Domain_Adaptation_ICCVW_2023_paper.html": {
    "title": "Unsupervised Camouflaged Object Segmentation as Domain Adaptation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhang",
      "Chengyi Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Shrivastava_DatasetEquity_Are_All_Samples_Created_Equal_In_The_Quest_For_ICCVW_2023_paper.html": {
    "title": "DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubham Shrivastava",
      "Xianling Zhang",
      "Sushruth Nagesh",
      "Armin Parchami"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Albiero_Confusing_Large_Models_by_Confusing_Small_Models_ICCVW_2023_paper.html": {
    "title": "Confusing Large Models by Confusing Small Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "VÃ­tor Albiero",
      "Raghav Mehta",
      "Ivan Evtimov",
      "Samuel Bell",
      "Levent Sagun",
      "Aram Markosyan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Koch_LORD_Leveraging_Open-Set_Recognition_with_Unknown_Data_ICCVW_2023_paper.html": {
    "title": "LORD: Leveraging Open-Set Recognition with Unknown Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Koch",
      "Christian Riess",
      "Thomas KÃ¶hler"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Chen_SC2GAN_Rethinking_Entanglement_by_Self-Correcting_Correlated_GAN_Space_ICCVW_2023_paper.html": {
    "title": "SC2GAN: Rethinking Entanglement by Self-Correcting Correlated GAN Space",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zikun Chen",
      "Han Zhao",
      "Parham Aarabi",
      "Ruowei Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Cultrera_Leveraging_Visual_Attention_for_out-of-Distribution_Detection_ICCVW_2023_paper.html": {
    "title": "Leveraging Visual Attention for out-of-Distribution Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Cultrera",
      "Lorenzo Seidenari",
      "Alberto Del Bimbo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Tang_Consistency_Regularization_for_Generalizable_Source-Free_Domain_Adaptation_ICCVW_2023_paper.html": {
    "title": "Consistency Regularization for Generalizable Source-Free Domain Adaptation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longxiang Tang",
      "Kai Li",
      "Chunming He",
      "Yulun Zhang",
      "Xiu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Yeh_Misalignment-Free_Relation_Aggregation_for_Multi-Source-Free_Domain_Adaptation_ICCVW_2023_paper.html": {
    "title": "Misalignment-Free Relation Aggregation for Multi-Source-Free Domain Adaptation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao-Wei Yeh",
      "Qier Meng",
      "Tatsuya Harada"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Mukhoti_Raising_the_Bar_on_the_Evaluation_of_Out-of-Distribution_Detection_ICCVW_2023_paper.html": {
    "title": "Raising the Bar on the Evaluation of Out-of-Distribution Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jishnu Mukhoti",
      "Tsung-Yu Lin",
      "Bor-Chun Chen",
      "Ashish Shah",
      "Philip H.S. Torr",
      "Puneet K. Dokania",
      "Ser-Nam Lim"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Lang_Class-Aware_Memory_Guided_Unbiased_Weighting_for_Universal_Domain_Adaptive_Object_ICCVW_2023_paper.html": {
    "title": "Class-Aware Memory Guided Unbiased Weighting for Universal Domain Adaptive Object Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinghai Lang",
      "Zhenwei He",
      "Xiaowei Fu",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Chhipa_Can_Self-Supervised_Representation_Learning_MethodsWithstand_Distribution_Shifts_and_Corruptions_ICCVW_2023_paper.html": {
    "title": "Can Self-Supervised Representation Learning MethodsWithstand Distribution Shifts and Corruptions?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prakash Chandra Chhipa",
      "Johan Rodahl Holmgren",
      "Kanjar De",
      "Rajkumar Saini",
      "Marcus Liwicki"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Rosales_Assessing_the_Impact_of_Diversity_on_the_Resilience_of_Deep_ICCVW_2023_paper.html": {
    "title": "Assessing the Impact of Diversity on the Resilience of Deep Learning Ensembles: A Comparative Study on Model Architecture, Output, Activation, and Attribution",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rafael Rosales",
      "Pablo Munoz",
      "Michael Paulitsch"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Lew_Gradient_Estimation_for_Unseen_Domain_Risk_Minimization_with_Pre-Trained_Models_ICCVW_2023_paper.html": {
    "title": "Gradient Estimation for Unseen Domain Risk Minimization with Pre-Trained Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byounggyu Lew",
      "Donghyun Son",
      "Buru Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Termohlen_A_Re-Parameterized_Vision_Transformer_ReVT_for_Domain-Generalized_Semantic_Segmentation_ICCVW_2023_paper.html": {
    "title": "A Re-Parameterized Vision Transformer (ReVT) for Domain-Generalized Semantic Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan-Aike TermÃ¶hlen",
      "Timo Bartels",
      "Tim Fingscheidt"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Ojaswee_Benchmarking_Image_Classifiers_for_Physical_Out-of-Distribution_Examples_Detection_ICCVW_2023_paper.html": {
    "title": "Benchmarking Image Classifiers for Physical Out-of-Distribution Examples Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ojaswee Ojaswee",
      "Akshay Agarwal",
      "Nalini Ratha"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Aniraj_Masking_Strategies_for_Background_Bias_Removal_in_Computer_Vision_Models_ICCVW_2023_paper.html": {
    "title": "Masking Strategies for Background Bias Removal in Computer Vision Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ananthu Aniraj",
      "Cassio F. Dantas",
      "Dino Ienco",
      "Diego Marcos"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Alcover-Couso_Biased_Class_disagreement_detection_of_out_of_distribution_instances_by_ICCVW_2023_paper.html": {
    "title": "Biased Class disagreement: detection of out of distribution instances by using differently biased semantic segmentation models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roberto Alcover-Couso",
      "Juan C. SanMiguel",
      "Marcos Escudero-ViÃ±olo"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Roschewitz_Distance_Matters_For_Improving_Performance_Estimation_Under_Covariate_Shift_ICCVW_2023_paper.html": {
    "title": "Distance Matters For Improving Performance Estimation Under Covariate Shift",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MÃ©lanie Roschewitz",
      "Ben Glocker"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Yu_The_Robust_Semantic_Segmentation_UNCV2023_Challenge_Results_ICCVW_2023_paper.html": {
    "title": "The Robust Semantic Segmentation UNCV2023 Challenge Results",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanlong Yu",
      "Yi Zuo",
      "Zitao Wang",
      "Xiaowen Zhang",
      "Jiaxuan Zhao",
      "Yuting Yang",
      "Licheng Jiao",
      "Rui Peng",
      "Xinyi Wang",
      "Junpei Zhang",
      "Kexin Zhang",
      "Fang Liu",
      "Roberto Alcover-Couso",
      "Juan C. SanMiguel",
      "Marcos Escudero-ViÃ±olo",
      "Hanlin Tian",
      "Kenta Matsui",
      "Tianhao Wang",
      "Fahmy Adan",
      "Zhitong Gao",
      "Xuming He",
      "Quentin Bouniot",
      "Hossein Moghaddam",
      "Shyam Nandan Rai",
      "Fabio Cermelli",
      "Carlo Masone",
      "Andrea Pilzer",
      "Elisa Ricci",
      "Andrei Bursuc",
      "Arno Solin",
      "Martin Trapp",
      "Rui Li",
      "Angela Yao",
      "Wenlong Chen",
      "Ivor Simpson",
      "Neill D. F. Campbell",
      "Gianni Franchi"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Ali_DELO_Deep_Evidential_LiDAR_Odometry_Using_Partial_Optimal_Transport_ICCVW_2023_paper.html": {
    "title": "DELO: Deep Evidential LiDAR Odometry Using Partial Optimal Transport",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sk Aziz Ali",
      "Djamila Aouada",
      "Gerd Reis",
      "Didier Stricker"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Baumann_Probabilistic_MIMO_U-Net_Efficient_and_Accurate_Uncertainty_Estimation_for_Pixel-Wise_ICCVW_2023_paper.html": {
    "title": "Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-Wise Regression",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anton Baumann",
      "Thomas RoÃberg",
      "Michael Schmitt"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Sandstrom_UncLe-SLAM_Uncertainty_Learning_for_Dense_Neural_SLAM_ICCVW_2023_paper.html": {
    "title": "UncLe-SLAM: Uncertainty Learning for Dense Neural SLAM",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik SandstrÃ¶m",
      "Kevin Ta",
      "Luc Van Gool",
      "Martin R. Oswald"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Narayanaswamy_Exploring_Inlier_and_Outlier_Specification_for_Improved_Medical_OOD_Detection_ICCVW_2023_paper.html": {
    "title": "Exploring Inlier and Outlier Specification for Improved Medical OOD Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivek Narayanaswamy",
      "Yamen Mubarka",
      "Rushil Anirudh",
      "Deepta Rajan",
      "Jayaraman J. Thiagarajan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Galesso_Far_Away_in_the_Deep_Space_Dense_Nearest-Neighbor-Based_Out-of-Distribution_Detection_ICCVW_2023_paper.html": {
    "title": "Far Away in the Deep Space: Dense Nearest-Neighbor-Based Out-of-Distribution Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Silvio Galesso",
      "Max Argus",
      "Thomas Brox"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Ledda_Adversarial_Attacks_Against_Uncertainty_Quantification_ICCVW_2023_paper.html": {
    "title": "Adversarial Attacks Against Uncertainty Quantification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuele Ledda",
      "Daniele Angioni",
      "Giorgio Piras",
      "Giorgio Fumera",
      "Battista Biggio",
      "Fabio Roli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Yao_Dual-Level_Interaction_for_Domain_Adaptive_Semantic_Segmentation_ICCVW_2023_paper.html": {
    "title": "Dual-Level Interaction for Domain Adaptive Semantic Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyu Yao",
      "Boheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Zelenka_A_Simple_and_Explainable_Method_for_Uncertainty_Estimation_Using_Attribute_ICCVW_2023_paper.html": {
    "title": "A Simple and Explainable Method for Uncertainty Estimation Using Attribute Prototype Networks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Claudius Zelenka",
      "Andrea GÃ¶hring",
      "Daniyal Kazempour",
      "Maximilian HÃ¼nemÃ¶rder",
      "Lars Schmarje",
      "Peer KrÃ¶ger"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Venkataramanan_Gaussian_Latent_Representations_for_Uncertainty_Estimation_Using_Mahalanobis_Distance_in_ICCVW_2023_paper.html": {
    "title": "Gaussian Latent Representations for Uncertainty Estimation Using Mahalanobis Distance in Deep Classifiers",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aishwarya Venkataramanan",
      "Assia Benbihi",
      "Martin Laviale",
      "CÃ©dric Pradalier"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Rabbani_Unsupervised_Confidence_Approximation_Trustworthy_Learning_from_Noisy_Labelled_Data_ICCVW_2023_paper.html": {
    "title": "Unsupervised Confidence Approximation: Trustworthy Learning from Noisy Labelled Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Navid Rabbani",
      "Adrien Bartoli"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Hammam_Identifying_Out-of-Domain_Objects_with_Dirichlet_Deep_Neural_Networks_ICCVW_2023_paper.html": {
    "title": "Identifying Out-of-Domain Objects with Dirichlet Deep Neural Networks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Hammam",
      "Frank Bonarens",
      "Seyed Eghbal Ghobadi",
      "Christoph Stiller"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Vojir_Calibrated_Out-of-Distribution_Detection_with_a_Generic_Representation_ICCVW_2023_paper.html": {
    "title": "Calibrated Out-of-Distribution Detection with a Generic Representation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "TomÃ¡Å¡ VojÃ­Å",
      "Jan Å ochman",
      "Rahaf Aljundi",
      "JiÅÃ­ Matas"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VLAR/html/Ahmad_MMTF_Multi-Modal_Temporal_Fusion_for_Commonsense_Video_Question_Answering_ICCVW_2023_paper.html": {
    "title": "MMTF: Multi-Modal Temporal Fusion for Commonsense Video Question Answering",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mobeen Ahmad",
      "Geonwoo Park",
      "Dongchan Park",
      "Sanguk Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VLAR/html/Jahagirdar_Understanding_Video_Scenes_Through_Text_Insights_from_Text-Based_Video_Question_ICCVW_2023_paper.html": {
    "title": "Understanding Video Scenes Through Text: Insights from Text-Based Video Question Answering",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumya Jahagirdar",
      "Minesh Mathew",
      "Dimosthenis Karatzas",
      "C. V. Jawahar"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VLAR/html/Ali_CLIP-Decoder__ZeroShot_Multilabel_Classification_Using_Multimodal_CLIP_Aligned_Representations_ICCVW_2023_paper.html": {
    "title": "CLIP-Decoder : ZeroShot Multilabel Classification Using Multimodal CLIP Aligned Representations",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Ali",
      "Salman Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VLAR/html/Sammani_Uni-NLX_Unifying_Textual_Explanations_for_Vision_and_Vision-Language_Tasks_ICCVW_2023_paper.html": {
    "title": "Uni-NLX: Unifying Textual Explanations for Vision and Vision-Language Tasks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fawaz Sammani",
      "Nikos Deligiannis"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VLAR/html/Taioli_Language-Enhanced_RNR-Map_Querying_Renderable_Neural_Radiance_Field_Maps_with_Natural_ICCVW_2023_paper.html": {
    "title": "Language-Enhanced RNR-Map: Querying Renderable Neural Radiance Field Maps with Natural Language",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Taioli",
      "Federico Cunico",
      "Federico Girella",
      "Riccardo Bologna",
      "Alessandro Farinelli",
      "Marco Cristani"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VLAR/html/de_Oliveira_Souza_SelfGraphVQA_A_Self-Supervised_Graph_Neural_Network_for_Scene-Based_Question_Answering_ICCVW_2023_paper.html": {
    "title": "SelfGraphVQA: A Self-Supervised Graph Neural Network for Scene-Based Question Answering",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bruno Cesar de Oliveira Souza",
      "Marius Aasan",
      "Helio Pedrini",
      "Adin Ramirez Rivera"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VLAR/html/Li_Iterative_Robust_Visual_Grounding_with_Masked_Reference_Based_Centerpoint_Supervision_ICCVW_2023_paper.html": {
    "title": "Iterative Robust Visual Grounding with Masked Reference Based Centerpoint Supervision",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Menghao Li",
      "Chunlei Wang",
      "Wenquan Feng",
      "Shuchang Lyu",
      "Guangliang Cheng",
      "Xiangtai Li",
      "Binghao Liu",
      "Qi Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VLAR/html/Oshima_Pointing_out_Human_Answer_Mistakes_in_a_Goal-Oriented_Visual_Dialogue_ICCVW_2023_paper.html": {
    "title": "Pointing out Human Answer Mistakes in a Goal-Oriented Visual Dialogue",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryosuke Oshima",
      "Seitaro Shinagawa",
      "Hideki Tsunashima",
      "Qi Feng",
      "Shigeo Morishima"
    ]
  },
  "https://openaccess.thecvf.com/content/ICCV2023W/VLAR/html/Zhang_What_If_the_TV_Was_Off_Examining_Counterfactual_Reasoning_Abilities_ICCVW_2023_paper.html": {
    "title": "What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-Modal Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Letian Zhang",
      "Xiaotong Zhai",
      "Zhongkai Zhao",
      "Xin Wen",
      "Bingchen Zhao"
    ]
  }
}