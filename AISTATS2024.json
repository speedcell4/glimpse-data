{
  "https://proceedings.mlr.press/v238/ruegamer24a.html": {
    "title": "Scalable Higher-Order Tensor Product Spline Models",
    "volume": "main",
    "abstract": "In the current era of vast data and transparent machine learning, it is essential for techniques to operate at a large scale while providing a clear mathematical comprehension of the internal workings of the method. Although there already exist interpretable semi-parametric regression methods for large-scale applications that take into account non-linearity in the data, the complexity of the models is still often limited. One of the main challenges is the absence of interactions in these models, which are left out for the sake of better interpretability but also due to impractical computational costs. To overcome this limitation, we propose a new approach using a factorization method to derive a highly scalable higher-order tensor product spline model. Our method allows for the incorporation of all (higher-order) interactions of non-linear feature effects while having computational costs proportional to a model without interactions. We further develop a meaningful penalization scheme and examine the induced optimization problem. We conclude by evaluating the predictive and estimation performance of our method",
    "checked": true,
    "id": "dfacaf4f4de33745be4a1937e9b6d6708a496cf0",
    "semantic_title": "scalable higher-order tensor product spline models",
    "citation_count": 0,
    "authors": [
      "David Ruegamer"
    ]
  },
  "https://proceedings.mlr.press/v238/amagata24a.html": {
    "title": "Fair k-center Clustering with Outliers",
    "volume": "main",
    "abstract": "The importance of dealing with big data is further increasing, as machine learning (ML) systems obtain useful knowledge from big datasets. However, using all data is practically prohibitive because of the massive sizes of the datasets, so summarizing them by centers obtained from k-center clustering is a promising approach. We have two concerns here. One is fairness, because if the summary does not have some specific groups, subsequent applications may provide unfair results for the groups. The other is the presence of outliers, and if outliers dominate the summary, it cannot be useful. To overcome these concerns, we address the problem of fair k-center clustering with outliers. Although prior works studied the fair k-center clustering problem, they do not consider outliers. This paper yields a linear time algorithm that satisfies the fairness constraint of our problem and probabilistically guarantees the almost 3-approximation bound. Its empirical efficiency and effectiveness are also reported",
    "checked": true,
    "id": "3cf3d1e04481b34bb910e3e74afc0b9bc4cdc8d2",
    "semantic_title": "fair k-center clustering with outliers",
    "citation_count": 0,
    "authors": [
      "Daichi Amagata"
    ]
  },
  "https://proceedings.mlr.press/v238/shankar24a.html": {
    "title": "A/B testing under Interference with Partial Network Information",
    "volume": "main",
    "abstract": "A/B tests are often required to be conducted on subjects that might have social connections. For e.g., experiments on social media, or medical and social interventions to control the spread of an epidemic. In such settings, the SUTVA assumption for randomized-controlled trials is violated due to network interference, or spill-over effects, as treatments to group A can potentially also affect the control group B. When the underlying social network is known exactly, prior works have demonstrated how to conduct A/B tests adequately to estimate the global average treatment effect (GATE). However, in practice, it is often impossible to obtain knowledge about the exact underlying network. In this paper, we present UNITE: a novel estimator that relax this assumption and can identify GATE while only relying on knowledge of the superset of neighbors for any subject in the graph. Through theoretical analysis and extensive experiments, we show that the proposed approach performs better in comparison to standard estimators",
    "checked": true,
    "id": "23e9ddc6a752b966e10c8e60b59b491fb92fedad",
    "semantic_title": "a/b testing under interference with partial network information",
    "citation_count": 1,
    "authors": [
      "Shiv Shankar",
      "Ritwik Sinha",
      "Yash Chandak",
      "Saayan Mitra",
      "Madalina Fiterau"
    ]
  },
  "https://proceedings.mlr.press/v238/jang24a.html": {
    "title": "Achieving Fairness through Separability: A Unified Framework for Fair Representation Learning",
    "volume": "main",
    "abstract": "Fairness is a growing concern in machine learning as state-of-the-art models may amplify social prejudice by making biased predictions against specific demographics such as race and gender. Such discrimination raises issues in various fields such as employment, criminal justice, and trust score evaluation. To address the concerns, we propose learning fair representation through a straightforward yet effective approach to project intrinsic information while filtering sensitive information for downstream tasks. Our model consists of two goals: one is to ensure that the latent data from different demographic groups is non-separable (i.e., make the latent data distribution independent of the sensitive feature to improve fairness); the other is to maximize the separability of latent data from different classes (i.e., maintain the discriminative power of data for the sake of the downstream tasks like classification). Our method adopts a non-zero-sum adversarial game to minimize the distance between data from different demographic groups while maximizing the margin between data from different classes. Moreover, the proposed objective function can be easily generalized to multiple sensitive attributes and multi-class scenarios as it upper bounds popular fairness metrics in these cases. We provide theoretical analysis of the fairness of our model and validate w.r.t. both fairness and predictive performance on benchmark datasets",
    "checked": true,
    "id": "7696d4cc5f1ae70528e5fbd70bf14c3ea90b0891",
    "semantic_title": "achieving fairness through separability: a unified framework for fair representation learning",
    "citation_count": 0,
    "authors": [
      "Taeuk Jang",
      "Hongchang Gao",
      "Pengyi Shi",
      "Xiaoqian Wang"
    ]
  },
  "https://proceedings.mlr.press/v238/li24a.html": {
    "title": "Personalized Federated X-armed Bandit",
    "volume": "main",
    "abstract": "In this work, we study the personalized federated $\\mathcal{X}$-armed bandit problem, where the heterogeneous local objectives of the clients are optimized simultaneously in the federated learning paradigm. We propose the \\texttt{PF-PNE} algorithm with a unique double elimination strategy, which safely eliminates the non-optimal regions while encouraging federated collaboration through biased but effective evaluations of the local objectives. The proposed \\texttt{PF-PNE} algorithm is able to optimize local objectives with arbitrary levels of heterogeneity, and its limited communications protects the confidentiality of the client-wise reward data. Our theoretical analysis shows the benefit of the proposed algorithm over single-client algorithms. Experimentally, \\texttt{PF-PNE} outperforms multiple baselines on both synthetic and real life datasets",
    "checked": false,
    "id": "186f745dc4518e1df276a57b5f57eda775c93034",
    "semantic_title": "personalized federated x -armed bandit",
    "citation_count": 0,
    "authors": [
      "Wenjie Li",
      "Qifan Song",
      "Jean Honorio"
    ]
  },
  "https://proceedings.mlr.press/v238/li24b.html": {
    "title": "Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical Learning",
    "volume": "main",
    "abstract": "Kriging aims to estimate the attributes of unseen geo-locations from observations in the spatial vicinity or physical connections. Existing works assume that neighbors' information offers the basis for estimating the unobserved target while ignoring non-neighbors. However, neighbors could also be quite different or even misleading, and the non-neighbors could still offer constructive information. To this end, we propose \"Contrastive-Prototypical\" self-supervised learning for Kriging (KCP): (1) The neighboring contrastive module coarsely pushes neighbors together and non-neighbors apart. (2) In parallel, the prototypical module identifies similar representations via exchanged prediction, such that it refines the misleading neighbors and recycles the useful non-neighbors from the neighboring contrast component. As a result, not all the neighbors and some of the non-neighbors will be used to infer the target. (3) To learn general and robust representations, we design an adaptive augmentation module that encourages data diversity. Theoretical bound is derived for the proposed augmentation. Extensive experiments on real-world datasets demonstrate the superior performance of KCP compared to its peers with 6% improvements and exceptional transferability and robustness",
    "checked": true,
    "id": "074664024ec6005741748e4e3af7f1e3bfe22b77",
    "semantic_title": "non-neighbors also matter to kriging: a new contrastive-prototypical learning",
    "citation_count": 2,
    "authors": [
      "Zhishuai Li",
      "Yunhao Nie",
      "Ziyue Li",
      "Lei Bai",
      "Yisheng Lv",
      "Rui Zhao"
    ]
  },
  "https://proceedings.mlr.press/v238/hill24a.html": {
    "title": "Boundary-Aware Uncertainty for Feature Attribution Explainers",
    "volume": "main",
    "abstract": "Post-hoc explanation methods have become a critical tool for understanding black-box classifiers in high-stakes applications. However, high-performing classifiers are often highly nonlinear and can exhibit complex behavior around the decision boundary, leading to brittle or misleading local explanations. Therefore there is an impending need to quantify the uncertainty of such explanation methods in order to understand when explanations are trustworthy. In this work we propose the Gaussian Process Explanation unCertainty (GPEC) framework, which generates a unified uncertainty estimate combining decision boundary-aware uncertainty with explanation function approximation uncertainty. We introduce a novel geodesic-based kernel, which captures the complexity of the target black-box decision boundary. We show theoretically that the proposed kernel similarity increases with decision boundary complexity. The proposed framework is highly flexible; it can be used with any black-box classifier and feature attribution method. Empirical results on multiple tabular and image datasets show that the GPEC uncertainty estimate improves understanding of explanations as compared to existing methods",
    "checked": true,
    "id": "51cdedfb9850844e9f24235d5c79f624a93280a9",
    "semantic_title": "boundary-aware uncertainty for feature attribution explainers",
    "citation_count": 1,
    "authors": [
      "Davin Hill",
      "Aria Masoomi",
      "Max Torop",
      "Sandesh Ghimire",
      "Jennifer Dy"
    ]
  },
  "https://proceedings.mlr.press/v238/even24a.html": {
    "title": "Asynchronous SGD on Graphs: a Unified Framework for Asynchronous Decentralized and Federated Optimization",
    "volume": "main",
    "abstract": "Decentralized and asynchronous communications are two popular techniques to speedup communication complexity of distributed machine learning, by respectively removing the dependency over a central orchestrator and the need for synchronization. Yet, combining these two techniques together still remains a challenge. In this paper, we take a step in this direction and introduce Asynchronous SGD on Graphs (AGRAF SGD) — a general algorithmic framework that covers asynchronous versions of many popular algorithms including SGD, Decentralized SGD, Local SGD, FedBuff, thanks to its relaxed communication and computation assumptions. We provide rates of convergence under much milder assumptions than previous decentralized asynchronous works, while still recovering or even improving over the best know results for all the algorithms covered",
    "checked": true,
    "id": "0f9ded926e889e35695e2028a059c96476fc8daa",
    "semantic_title": "asynchronous sgd on graphs: a unified framework for asynchronous decentralized and federated optimization",
    "citation_count": 7,
    "authors": [
      "Mathieu Even",
      "Anastasia Koloskova",
      "Laurent Massoulie"
    ]
  },
  "https://proceedings.mlr.press/v238/hellstrom24a.html": {
    "title": "Comparing Comparators in Generalization Bounds",
    "volume": "main",
    "abstract": "We derive generic information-theoretic and PAC-Bayesian generalization bounds involving an arbitrary convex comparator function, which measures the discrepancy between the training loss and the population loss. The bounds hold under the assumption that the cumulant-generating function (CGF) of the comparator is upper-bounded by the corresponding CGF within a family of bounding distributions. We show that the tightest possible bound is obtained with the comparator being the convex conjugate of the CGF of the bounding distribution, also known as the Cramér function. This conclusion applies more broadly to generalization bounds with a similar structure. This confirms the near-optimality of known bounds for bounded and sub-Gaussian losses and leads to novel bounds under other bounding distributions",
    "checked": true,
    "id": "8ec2d05bdeb8820d656ffdc898aafb3bc8f92dab",
    "semantic_title": "comparing comparators in generalization bounds",
    "citation_count": 1,
    "authors": [
      "Fredrik Hellström",
      "Benjamin Guedj"
    ]
  },
  "https://proceedings.mlr.press/v238/dagreou24a.html": {
    "title": "A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization",
    "volume": "main",
    "abstract": "Bilevel optimization problems, which are problems where two optimization problems are nested, have more and more applications in machine learning. In many practical cases, the upper and the lower objectives correspond to empirical risk minimization problems and therefore have a sum structure. In this context, we propose a bilevel extension of the celebrated SARAH algorithm. We demonstrate that the algorithm requires $O((n+m)^{1/2}\\epsilon^{-1})$ oracle calls to achieve $\\epsilon$-stationarity with $n+m$ the total number of samples, which improves over all previous bilevel algorithms. Moreover, we provide a lower bound on the number of oracle calls required to get an approximate stationary point of the objective function of the bilevel problem. This lower bound is attained by our algorithm, making it optimal in terms of sample complexity",
    "checked": true,
    "id": "d86c87c44071f149508b5b0f718c607e908966fd",
    "semantic_title": "a lower bound and a near-optimal algorithm for bilevel empirical risk minimization",
    "citation_count": 6,
    "authors": [
      "Mathieu Dagréou",
      "Thomas Moreau",
      "Samuel Vaiter",
      "Pierre Ablin"
    ]
  },
  "https://proceedings.mlr.press/v238/zheng24a.html": {
    "title": "Better Batch for Deep Probabilistic Time Series Forecasting",
    "volume": "main",
    "abstract": "Deep probabilistic time series forecasting has gained attention for its ability to provide nonlinear approximation and valuable uncertainty quantification for decision-making. However, existing models often oversimplify the problem by assuming a time-independent error process and overlooking serial correlation. To overcome this limitation, we propose an innovative training method that incorporates error autocorrelation to enhance probabilistic forecasting accuracy. Our method constructs a mini-batch as a collection of D consecutive time series segments for model training. It explicitly learns a time-varying covariance matrix over each mini-batch, encoding error correlation among adjacent time steps. The learned covariance matrix can be used to improve prediction accuracy and enhance uncertainty quantification. We evaluate our method on two different neural forecasting models and multiple public datasets. Experimental results confirm the effectiveness of the proposed approach in improving the performance of both models across a range of datasets, resulting in notable improvements in predictive accuracy",
    "checked": true,
    "id": "5a37896de7f956b6cff13e914db09f858d004b21",
    "semantic_title": "better batch for deep probabilistic time series forecasting",
    "citation_count": 1,
    "authors": [
      "Zhihao Zheng",
      "Seongjin Choi",
      "Lijun Sun"
    ]
  },
  "https://proceedings.mlr.press/v238/sundhar-ramesh24a.html": {
    "title": "Distributionally Robust Model-based Reinforcement Learning with Large State Spaces",
    "volume": "main",
    "abstract": "Three major challenges in reinforcement learning are the complex dynamical systems with large state spaces, the costly data acquisition processes, and the deviation of real-world dynamics from the training environment deployment. To overcome these issues, we study distributionally robust Markov decision processes with continuous state spaces under the widely used Kullback-Leibler, chi-square, and total variation uncertainty sets. We propose a model-based approach that utilizes Gaussian Processes and the maximum variance reduction algorithm to efficiently learn multi-output nominal transition dynamics, leveraging access to a generative model (i.e., simulator). We further demonstrate the statistical sample complexity of the proposed method for different uncertainty sets. These complexity bounds are independent of the number of states and extend beyond linear dynamics, ensuring the effectiveness of our approach in identifying near-optimal distributionally-robust policies. The proposed method can be further combined with other model-free distributionally robust reinforcement learning methods to obtain a near-optimal robust policy. Experimental results demonstrate the robustness of our algorithm to distributional shifts and its superior performance in terms of the number of samples needed",
    "checked": true,
    "id": "e3556000f28908e124c5facb6cb0a2dc55d0c373",
    "semantic_title": "distributionally robust model-based reinforcement learning with large state spaces",
    "citation_count": 6,
    "authors": [
      "Shyam Sundhar Ramesh",
      "Pier Giuseppe Sessa",
      "Yifan Hu",
      "Andreas Krause",
      "Ilija Bogunovic"
    ]
  },
  "https://proceedings.mlr.press/v238/el-ahmad24a.html": {
    "title": "Sketch In, Sketch Out: Accelerating both Learning and Inference for Structured Prediction with Kernels",
    "volume": "main",
    "abstract": "Leveraging the kernel trick in both the input and output spaces, surrogate kernel methods are a flexible and theoretically grounded solution to structured output prediction. If they provide state-of-the-art performance on complex data sets of moderate size (e.g., in chemoinformatics), these approaches however fail to scale. We propose to equip surrogate kernel methods with sketching-based approximations, applied to both the input and output feature maps. We prove excess risk bounds on the original structured prediction problem, showing how to attain close-to-optimal rates with a reduced sketch size that depends on the eigendecay of the input/output covariance operators. From a computational perspective, we show that the two approximations have distinct but complementary impacts: sketching the input kernel mostly reduces training time, while sketching the output kernel decreases the inference time. Empirically, our approach is shown to scale, achieving state-of-the-art performance on benchmark data sets where non-sketched methods are intractable",
    "checked": true,
    "id": "0e42aa9569404ed314ed9d6cb1142b62cd355b6e",
    "semantic_title": "sketch in, sketch out: accelerating both learning and inference for structured prediction with kernels",
    "citation_count": 4,
    "authors": [
      "Tamim El Ahmad",
      "Luc Brogat-Motte",
      "Pierre Laforgue",
      "Florence d’Alché-Buc"
    ]
  },
  "https://proceedings.mlr.press/v238/vadori24a.html": {
    "title": "Ordinal Potential-based Player Rating",
    "volume": "main",
    "abstract": "It was recently observed that Elo ratings fail at preserving transitive relations among strategies and therefore cannot correctly extract the transitive component of a game. We provide a characterization of transitive games as a weak variant of ordinal potential games and show that Elo ratings actually do preserve transitivity when computed in the right space, using suitable invertible mappings. Leveraging this insight, we introduce a new game decomposition of an arbitrary game into transitive and cyclic components that is learnt using a neural network-based architecture and that prioritises capturing the sign pattern of the game, namely transitive and cyclic relations among strategies. We link our approach to the known concept of sign-rank, and evaluate our methodology using both toy examples and empirical data from real-world games",
    "checked": true,
    "id": "c9a24d35bf506e2aac1aed5d5d6eedf209204b44",
    "semantic_title": "ordinal potential-based player rating",
    "citation_count": 1,
    "authors": [
      "Nelson Vadori",
      "Rahul Savani"
    ]
  },
  "https://proceedings.mlr.press/v238/rudner24a.html": {
    "title": "Mind the GAP: Improving Robustness to Subpopulation Shifts with Group-Aware Priors",
    "volume": "main",
    "abstract": "Machine learning models often perform poorly under subpopulation shifts in the data distribution. Developing methods that allow machine learning models to better generalize to such shifts is crucial for safe deployment in real-world settings. In this paper, we develop a family of group-aware prior (GAP) distributions over neural network parameters that explicitly favor models that generalize well under subpopulation shifts. We design a simple group-aware prior that only requires access to a small set of data with group information and demonstrate that training with this prior yields state-of-the-art performance—even when only retraining the final layer of a previously trained non-robust model. Group aware-priors are conceptually simple, complementary to existing approaches, such as attribute pseudo labeling and data reweighting, and open up promising new avenues for harnessing Bayesian inference to enable robustness to subpopulation shifts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim G. J. Rudner",
      "Ya Shi Zhang",
      "Andrew Gordon Wilson",
      "Julia Kempe"
    ]
  },
  "https://proceedings.mlr.press/v238/buch24a.html": {
    "title": "Simple and scalable algorithms for cluster-aware precision medicine",
    "volume": "main",
    "abstract": "AI-enabled precision medicine promises a transformational improvement in healthcare outcomes. However, training on biomedical data presents significant challenges as they are often high dimensional, clustered, and of limited sample size. To overcome these challenges, we propose a simple and scalable approach for cluster-aware embedding that combines latent factor methods with a convex clustering penalty in a modular way. Our novel approach overcomes the complexity and limitations of current joint embedding and clustering methods and enables hierarchically clustered principal component analysis (PCA), locally linear embedding (LLE), and canonical correlation analysis (CCA). Through numerical experiments and real-world examples, we demonstrate that our approach outperforms fourteen clustering methods on highly underdetermined problems (e.g., with limited sample size) as well as on large sample datasets. Importantly, our approach does not require the user to choose the desired number of clusters, yields improved model selection if they do, and yields interpretable hierarchically clustered embedding dendrograms. Thus, our approach improves significantly on existing methods for identifying patient subgroups in multiomics and neuroimaging data and enables scalable and interpretable biomarkers for precision medicine",
    "checked": true,
    "id": "18401e3c8b867c6fb822605cb41bd6e2e5ce7edc",
    "semantic_title": "simple and scalable algorithms for cluster-aware precision medicine",
    "citation_count": 0,
    "authors": [
      "Amanda M. Buch",
      "Conor Liston",
      "Logan Grosenick"
    ]
  },
  "https://proceedings.mlr.press/v238/lin24a.html": {
    "title": "A Specialized Semismooth Newton Method for Kernel-Based Optimal Transport",
    "volume": "main",
    "abstract": "Kernel-based optimal transport (OT) estimators offer an alternative, functional estimation procedure to address OT problems from samples. Recent works suggest that these estimators are more statistically efficient than plug-in (linear programming-based) OT estimators when comparing probability measures in high-dimensions (Vacher et al., 2021). Unfortunately,that statistical benefit comes at a very steep computational price: because their computation relies on the short-step interior-point method (SSIPM), which comes with a large iteration count in practice, these estimators quickly become intractable w.r.t. sample size $n$. To scale these estimators to larger $n$, we propose a nonsmooth fixedpoint model for the kernel-based OT problem, and show that it can be efficiently solved via a specialized semismooth Newton (SSN) method: We show, exploring the problem's structure, that the per-iteration cost of performing one SSN step can be significantly reduced in practice. We prove that our SSN method achieves a global convergence rate of $O(1/\\sqrt{k})$, and a local quadratic convergence rate under standard regularity conditions. We show substantial speedups over SSIPM on both synthetic and real datasets",
    "checked": true,
    "id": "157c69ef084364273ef763cb7058fca84828f9ee",
    "semantic_title": "a specialized semismooth newton method for kernel-based optimal transport",
    "citation_count": 0,
    "authors": [
      "Tianyi Lin",
      "Marco Cuturi",
      "Michael Jordan"
    ]
  },
  "https://proceedings.mlr.press/v238/dai24a.html": {
    "title": "Local Causal Discovery with Linear non-Gaussian Cyclic Models",
    "volume": "main",
    "abstract": "Local causal discovery is of great practical significance, as there are often situations where the discovery of the global causal structure is unnecessary, and the interest lies solely on a single target variable. Most existing local methods utilize conditional independence relations, providing only a partially directed graph, and assume acyclicity for the ground-truth structure, even though real-world scenarios often involve cycles like feedback mechanisms. In this work, we present a general, unified local causal discovery method with linear non-Gaussian models, whether they are cyclic or acyclic. We extend the application of independent component analysis from the global context to independent subspace analysis, enabling the exact identification of the equivalent local directed structures and causal strengths from the Markov blanket of the target variable. We also propose an alternative regression-based method in the particular acyclic scenarios. Our identifiability results are empirically validated using both synthetic and real-world datasets",
    "checked": true,
    "id": "1304f88cf92fecccbcf2c26aaac3d4f0a7964e05",
    "semantic_title": "local causal discovery with linear non-gaussian cyclic models",
    "citation_count": 2,
    "authors": [
      "Haoyue Dai",
      "Ignavier Ng",
      "Yujia Zheng",
      "Zhengqing Gao",
      "Kun Zhang"
    ]
  },
  "https://proceedings.mlr.press/v238/park24a.html": {
    "title": "Density Uncertainty Layers for Reliable Uncertainty Estimation",
    "volume": "main",
    "abstract": "Assessing the predictive uncertainty of deep neural networks is crucial for safety-related applications of deep learning. Although Bayesian deep learning offers a principled framework for estimating model uncertainty, the common approaches that approximate the parameter posterior often fail to deliver reliable estimates of predictive uncertainty. In this paper, we propose a novel criterion for reliable predictive uncertainty: a model's predictive variance should be grounded in the empirical density of the input. That is, the model should produce higher uncertainty for inputs that are improbable in the training data and lower uncertainty for inputs that are more probable. To operationalize this criterion, we develop the density uncertainty layer, a stochastic neural network architecture that satisfies the density uncertain criterion by design. We study density uncertainty layers on the UCI and CIFAR-10/100 uncertainty benchmarks. Compared to existing approaches, density uncertainty layers provide more reliable uncertainty estimates and robust out-of-distribution detection performance",
    "checked": true,
    "id": "3cb746ee9ab49920ead4bd832d94ca0bc1ee5d3b",
    "semantic_title": "density uncertainty layers for reliable uncertainty estimation",
    "citation_count": 1,
    "authors": [
      "Yookoon Park",
      "David Blei"
    ]
  },
  "https://proceedings.mlr.press/v238/carton24a.html": {
    "title": "Double InfoGAN for Contrastive Analysis",
    "volume": "main",
    "abstract": "Contrastive Analysis (CA) deals with the discovery of what is common and what is distinctive of a target domain compared to a background one. This is of great interest in many applications, such as medical imaging. Current state-of-the-art (SOTA) methods are latent variable models based on VAE (CA-VAEs). However, they all either ignore important constraints or they don't enforce fundamental assumptions. This may lead to sub-optimal solutions where distinctive factors are mistaken for common ones (or viceversa). Furthermore, the generated images have a rather poor quality, typical of VAEs, decreasing their interpretability and usefulness. Here, we propose Double InfoGAN, the first GAN based method for CA that leverages the high-quality synthesis of GAN and the separation power of InfoGAN. Experimental results on four visual datasets, from simple synthetic examples to complex medical images, show that the proposed method outperforms SOTA CA-VAEs in terms of latent separation and image quality. Datasets and code are available online",
    "checked": true,
    "id": "bcc1580fbd66a0c9f1a008537a02f190b1d07f57",
    "semantic_title": "double infogan for contrastive analysis",
    "citation_count": 2,
    "authors": [
      "Florence Carton",
      "Robin Louiset",
      "Pietro Gori"
    ]
  },
  "https://proceedings.mlr.press/v238/feng24a.html": {
    "title": "Is this model reliable for everyone? Testing for strong calibration",
    "volume": "main",
    "abstract": "In a well-calibrated risk prediction model, the average predicted probability is close to the true event rate for any given subgroup. Such models are reliable across heterogeneous populations and satisfy strong notions of algorithmic fairness. However, the task of auditing a model for strong calibration is well-known to be difficult—particularly for machine learning (ML) algorithms—due to the sheer number of potential subgroups. As such, common practice is to only assess calibration with respect to a few predefined subgroups. Recent developments in goodness-of-fit testing offer potential solutions but are not designed for settings with weak signal or where the poorly calibrated subgroup is small, as they either overly subdivide the data or fail to divide the data at all. We introduce a new testing procedure based on the following insight: if we can reorder observations by their expected residuals, there should be a change in the association between the predicted and observed residuals along this sequence if a poorly calibrated subgroup exists. This lets us reframe the problem of calibration testing into one of changepoint detection, for which powerful methods already exist. We begin with introducing a sample-splitting procedure where a portion of the data is used to train a suite of candidate models for predicting the residual, and the remaining data are used to perform a score-based cumulative sum (CUSUM) test. To further improve power, we then extend this adaptive CUSUM test to incorporate cross-validation, while maintaining Type I error control under minimal assumptions. Compared to existing methods, the proposed procedure consistently achieved higher power in empirical analyses",
    "checked": true,
    "id": "1c965a39aae320fb048ef7d8aa2cf55c8dfb49c1",
    "semantic_title": "is this model reliable for everyone? testing for strong calibration",
    "citation_count": 2,
    "authors": [
      "Jean Feng",
      "Alexej Gossmann",
      "Romain Pirracchio",
      "Nicholas Petrick",
      "Gene A Pennello",
      "Berkman Sahiner"
    ]
  },
  "https://proceedings.mlr.press/v238/palm24a.html": {
    "title": "An Online Bootstrap for Time Series",
    "volume": "main",
    "abstract": "Resampling methods such as the bootstrap have proven invaluable in the field of machine learning. However, the applicability of traditional bootstrap methods is limited when dealing with large streams of dependent data, such as time series or spatially correlated observations. In this paper, we propose a novel bootstrap method that is designed to account for data dependencies and can be executed online, making it particularly suitable for real-time applications. This method is based on an autoregressive sequence of increasingly dependent resampling weights. We prove the theoretical validity of the proposed bootstrap scheme under general conditions. We demonstrate the effectiveness of our approach through extensive simulations and show that it provides reliable uncertainty quantification even in the presence of complex data dependencies. Our work bridges the gap between classical resampling techniques and the demands of modern data analysis, providing a valuable tool for researchers and practitioners in dynamic, data-rich environments",
    "checked": true,
    "id": "80d7b73777ff734ab496f19a153872332d0eb55a",
    "semantic_title": "an online bootstrap for time series",
    "citation_count": 0,
    "authors": [
      "Nicolai Palm",
      "Thomas Nagler"
    ]
  },
  "https://proceedings.mlr.press/v238/xing24a.html": {
    "title": "Better Representations via Adversarial Training in Pre-Training: A Theoretical Perspective",
    "volume": "main",
    "abstract": "Pre-training is known to generate universal representations for downstream tasks in large-scale deep learning such as large language models. Existing literature, e.g., Kim et al. (2020), empirically observe that the downstream tasks can inherit the adversarial robustness of the pre-trained model. We provide theoretical justifications for this robustness inheritance phenomenon. Our theoretical results reveal that feature purification plays an important role in connecting the adversarial robustness of the pre-trained model and the downstream tasks in two-layer neural networks. Specifically, we show that (i) with adversarial training, each hidden node tends to pick only one (or a few) feature; (ii) without adversarial training, the hidden nodes can be vulnerable to attacks. This observation is valid for both supervised pre-training and contrastive learning. With purified nodes, it turns out that clean training is enough to achieve adversarial robustness in downstream tasks",
    "checked": true,
    "id": "56ce2e63d91c8ec1a999f552cbf9918c39006b12",
    "semantic_title": "better representations via adversarial training in pre-training: a theoretical perspective",
    "citation_count": 0,
    "authors": [
      "Yue Xing",
      "Xiaofeng Lin",
      "Qifan Song",
      "Yi Xu",
      "Belinda Zeng",
      "Guang Cheng"
    ]
  },
  "https://proceedings.mlr.press/v238/song24a.html": {
    "title": "Solving Attention Kernel Regression Problem via Pre-conditioner",
    "volume": "main",
    "abstract": "Attention mechanism is the key to large language models, and attention matrix serves as an algorithmic and computational bottleneck for such a scheme. In this paper, we define two problems, motivated by designing fast algorithms for \\emph{proxy} of attention matrix and solving regressions against them. Given an input matrix $A\\in \\mathbb{R}^{n\\times d}$ with $n\\gg d$ and a response vector $b$, we first consider the matrix exponential of the matrix $A^\\top A$ as a proxy, and we in turn design algorithms for two types of regression problems: $\\min_{x\\in \\mathbb{R}^d}\\|(A^\\top A)^jx-b\\|_2$ and $\\min_{x\\in \\mathbb{R}^d}\\|A(A^\\top A)^jx-b\\|_2$ for any positive integer $j$. Studying algorithms for these regressions is essential, as matrix exponential can be approximated term-by-term via these smaller problems. The second proxy is applying exponential entrywise to the Gram matrix, denoted by $\\exp(AA^\\top)$ and solving the regression $\\min_{x\\in \\mathbb{R}^n}\\|\\exp(AA^\\top)x-b \\|_2$. We call this problem the \\emph{attention kernel regression} problem, as the matrix $\\exp(AA^\\top)$ could be viewed as a kernel function with respect to $A$. We design fast algorithms for these regression problems, based on sketching and preconditioning. We hope these efforts will provide an alternative perspective of studying efficient approximation of attention matrices",
    "checked": true,
    "id": "420533d1fc0ef9e652a228a669a858ad7d7162ea",
    "semantic_title": "solving attention kernel regression problem via pre-conditioner",
    "citation_count": 5,
    "authors": [
      "Zhao Song",
      "Junze Yin",
      "Lichen Zhang"
    ]
  },
  "https://proceedings.mlr.press/v238/hu24a.html": {
    "title": "Pixel-wise Smoothing for Certified Robustness against Camera Motion Perturbations",
    "volume": "main",
    "abstract": "Deep learning-based visual perception models lack robustness when faced with camera motion perturbations in practice. The current certification process for assessing robustness is costly and time-consuming due to the extensive number of image projections required for Monte Carlo sampling in the 3D camera motion space. To address these challenges, we present a novel, efficient, and practical framework for certifying the robustness of 3D-2D projective transformations against camera motion perturbations. Our approach leverages a smoothing distribution over the 2D-pixel space instead of in the 3D physical space, eliminating the need for costly camera motion sampling and significantly enhancing the efficiency of robustness certifications. With the pixel-wise smoothed classifier, we are able to fully upper bound the projection errors using a technique of uniform partitioning in camera motion space. Additionally, we extend our certification framework to a more general scenario where only a single-frame point cloud is required in the projection oracle. Through extensive experimentation, we validate the trade-off between effectiveness and efficiency enabled by our proposed method. Remarkably, our approach achieves approximately 80% certified accuracy while utilizing only 30% of the projected image frames",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanjiang Hu",
      "Zuxin Liu",
      "Linyi Li",
      "Jiacheng Zhu",
      "Ding Zhao"
    ]
  },
  "https://proceedings.mlr.press/v238/bengs24a.html": {
    "title": "Identifying Copeland Winners in Dueling Bandits with Indifferences",
    "volume": "main",
    "abstract": "We consider the task of identifying the Copeland winner(s) in a dueling bandits problem with ternary feedback. This is an underexplored but practically relevant variant of the conventional dueling bandits problem, in which, in addition to strict preference between two arms, one may observe feedback in the form of an indifference. We provide a lower bound on the sample complexity for any learning algorithm finding the Copeland winner(s) with a fixed error probability. Moreover, we propose POCOWISTA, an algorithm with a sample complexity that almost matches this lower bound, and which shows excellent empirical performance, even for the conventional dueling bandits problem. For the case where the preference probabilities satisfy a specific type of stochastic transitivity, we provide a refined version with an improved worst case sample complexity",
    "checked": true,
    "id": "52d4a52de42491c2bc2c2414e959f9fde5be105f",
    "semantic_title": "identifying copeland winners in dueling bandits with indifferences",
    "citation_count": 0,
    "authors": [
      "Viktor Bengs",
      "Björn Haddenhorst",
      "Eyke Hüllermeier"
    ]
  },
  "https://proceedings.mlr.press/v238/kim24a.html": {
    "title": "Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?",
    "volume": "main",
    "abstract": "We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called \"linear\") rate under perfect variational family specification. In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families. Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent. For the projection operator, we consider a domain with triangular scale matrices, which the projection onto is computable in $\\theta(d)$ time, where $d$ is the dimensionality of the target posterior. We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator, providing explicit non-asymptotic complexity guarantees for both",
    "checked": true,
    "id": "0e1e358a47a640667311787e54b013e6af505cd6",
    "semantic_title": "linear convergence of black-box variational inference: should we stick the landing?",
    "citation_count": 4,
    "authors": [
      "Kyurae Kim",
      "Yian Ma",
      "Jacob Gardner"
    ]
  },
  "https://proceedings.mlr.press/v238/song24b.html": {
    "title": "Fast Dynamic Sampling for Determinantal Point Processes",
    "volume": "main",
    "abstract": "n this work, we provide fast dynamic algorithms for repeatedly sampling from distributions characterized by Determinantal Point Processes (DPPs) and Nonsymmetric Determinantal Point Processes (NDPPs). DPPs are a very well-studied class of distributions on subsets of items drawn from a ground set of cardinality $n$ characterized by a symmetric $n \\times n$ kernel matrix $L$ such that the probability of any subset is proportional to the determinant of its corresponding principal submatrix. Recent work has shown that the kernel symmetry constraint can be relaxed, leading to NDPPs, which can better model data in several machine learning applications. Given a low-rank kernel matrix ${\\cal L}=L+L^\\top\\in \\mathbb{R}^{n\\times n}$ and its corresponding eigendecomposition specified by $\\{\\lambda_i, u_i \\}_{i=1}^d$ where $d\\leq n$ is the rank, we design a data structure that uses $O(nd)$ space and preprocesses data in $O(nd^{\\omega-1})$ time where $\\omega\\approx 2.37$ is the exponent of matrix multiplication. The data structure can generate a sample according to DPP distribution in time $O(|E|^3\\log n+|E|^{\\omega-1}d^2)$ or according to NDPP distribution in time $O((|E|^3 \\log n+ |E|^{\\omega-1}d^2)(1+w)^d)$ for $E$ being the sampled indices and $w$ is a data-dependent parameter. This improves upon the space and preprocessing time over prior works, and achieves a state-of-the-art sampling time when the sampling set is relatively dense. At the heart of our data structure is an efficient sampling tree that can leverage batch initialization and fast inner product query simultaneously",
    "checked": true,
    "id": "987ec3bbf4132cfb9477c2960702cb44dcf8af2f",
    "semantic_title": "fast dynamic sampling for determinantal point processes",
    "citation_count": 0,
    "authors": [
      "Zhao Song",
      "Junze Yin",
      "Lichen Zhang",
      "Ruizhe Zhang"
    ]
  },
  "https://proceedings.mlr.press/v238/li24c.html": {
    "title": "Best Arm Identification with Resource Constraints",
    "volume": "main",
    "abstract": "Motivated by the cost heterogeneity in experimentation across different alternatives, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem. The agent aims to identify the best arm under resource constraints, where resources are consumed for each arm pull. We make two novel contributions. We design and analyze the Successive Halving with Resource Rationing algorithm (SH-RR). The SH-RR achieves a near-optimal non-asymptotic rate of convergence in terms of the probability of successively identifying an optimal arm. Interestingly, we identify a difference in convergence rates between the cases of deterministic and stochastic resource consumption",
    "checked": true,
    "id": "3cf60b3397bb216818fe3e907c5059be448b616b",
    "semantic_title": "best arm identification with resource constraints",
    "citation_count": 0,
    "authors": [
      "Zitian Li",
      "Wang Chi Cheung"
    ]
  },
  "https://proceedings.mlr.press/v238/yang24a.html": {
    "title": "Neural McKean-Vlasov Processes: Distributional Dependence in Diffusion Processes",
    "volume": "main",
    "abstract": "McKean-Vlasov stochastic differential equations (MV-SDEs) provide a mathematical description of the behavior of an infinite number of interacting particles by imposing a dependence on the particle density. We study the influence of explicitly including distributional information in the parameterization of the SDE. We propose a series of semi-parametric methods for representing MV-SDEs, and corresponding estimators for inferring parameters from data based on the properties of the MV-SDE. We analyze the characteristics of the different architectures and estimators, and consider their applicability in relevant machine learning problems. We empirically compare the performance of the different architectures and estimators on real and synthetic datasets for time series and probabilistic modeling. The results suggest that explicitly including distributional dependence in the parameterization of the SDE is effective in modeling temporal data with interaction under an exchangeability assumption while maintaining strong performance for standard Itô-SDEs due to the richer class of probability flows associated with MV-SDEs",
    "checked": true,
    "id": "e9c726973fc3ad0267c4646a40e6ca3910b2ed4f",
    "semantic_title": "neural mckean-vlasov processes: distributional dependence in diffusion processes",
    "citation_count": 1,
    "authors": [
      "Haoming Yang",
      "Ali Hasan",
      "Yuting Ng",
      "Vahid Tarokh"
    ]
  },
  "https://proceedings.mlr.press/v238/zhang24a.html": {
    "title": "HintMiner: Automatic Question Hints Mining From Q&A Web Posts with Language Model via Self-Supervised Learning",
    "volume": "main",
    "abstract": "Users often need ask questions and seek answers online. The Question - Answering (QA) forums such as Stack Overflow cannot always respond to the questions timely and properly. In this paper, we propose HintMiner, a novel automatic question hints mining tool for users to help them find answers. HintMiner leverages the machine comprehension and sequence generation techniques to automatically generate hints for users' questions. It firstly retrieve many web Q&A posts and then extract some hints from the posts using MiningNet that is built via a language model. Using the huge amount of online Q&A posts, we design a self-supervised objective to train the MiningNet that is a neural encoder-decoder model based on the transformer and copying mechanisms. We have evaluated HintMiner on 60,000 Stack Overflow questions. The experiment results show that the proposed approach is effective. For example, HintMiner achieves an average BLEU score of 36.17% and an average ROUGE-2 score of 36.29%. Our tool and experimental data are publicly available at \\url{https://github.com/zhangzhenyu13/HintMiner}",
    "checked": true,
    "id": "fd37e67e4c4ac05e0a314c240fb8dafb8b197579",
    "semantic_title": "hintminer: automatic question hints mining from q&a web posts with language model via self-supervised learning",
    "citation_count": 0,
    "authors": [
      "Zhenyu Zhang",
      "JiuDong Yang"
    ]
  },
  "https://proceedings.mlr.press/v238/hong24a.html": {
    "title": "A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline constrained reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward subject to constraints on expected cumulative cost using an existing dataset. In this paper, we propose Primal-Dual-Critic Algorithm (PDCA), a novel algorithm for offline constrained RL with general function approximation. PDCA runs a primal-dual algorithm on the Lagrangian function estimated by critics. The primal player employs a no-regret policy optimization oracle to maximize the Lagrangian estimate and the dual player acts greedily to minimize the Lagrangian estimate. We show that PDCA finds a near saddle point of the Lagrangian, which is nearly optimal for the constrained RL problem. Unlike previous work that requires concentrability and a strong Bellman completeness assumption, PDCA only requires concentrability and realizability assumptions for sample-efficient learning",
    "checked": true,
    "id": "75605dfe587630c365d092db9f60094067c4110f",
    "semantic_title": "a primal-dual-critic algorithm for offline constrained reinforcement learning",
    "citation_count": 4,
    "authors": [
      "Kihyuk Hong",
      "Yuhang Li",
      "Ambuj Tewari"
    ]
  },
  "https://proceedings.mlr.press/v238/huang24a.html": {
    "title": "On the Statistical Efficiency of Mean-Field Reinforcement Learning with General Function Approximation",
    "volume": "main",
    "abstract": "In this paper, we study the fundamental statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general model-based function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MF-MBED), which characterizes the inherent complexity of mean-field model classes. We show that a rich family of Mean-Field RL problems exhibits low MF-MBED. Additionally, we propose algorithms based on maximal likelihood estimation, which can return an $\\epsilon$-optimal policy for MFC or an $\\epsilon$-Nash Equilibrium policy for MFG. The overall sample complexity depends only polynomially on MF-MBED, which is potentially much lower than the size of state-action space. Compared with previous works, our results only require the minimal assumptions including realizability and Lipschitz continuity",
    "checked": false,
    "id": "93c5963791eda842b1897a4660d737e76c852b25",
    "semantic_title": "on the statistical efficiency of mean field reinforcement learning with general function approximation",
    "citation_count": 6,
    "authors": [
      "Jiawei Huang",
      "Batuhan Yardim",
      "Niao He"
    ]
  },
  "https://proceedings.mlr.press/v238/demetci24a.html": {
    "title": "Breaking isometric ties and introducing priors in Gromov-Wasserstein distances",
    "volume": "main",
    "abstract": "Gromov-Wasserstein distance has many applications in machine learning due to its ability to compare measures across metric spaces and its invariance to isometric transformations. However, in certain applications, this invariant property can be too flexible, thus undesirable. Moreover, the Gromov-Wasserstein distance solely considers pairwise sample similarities in input datasets, disregarding the raw feature representations. We propose a new optimal transport formulation, called Augmented Gromov-Wasserstein (AGW), that allows for some control over the level of rigidity to transformations. It also incorporates feature alignments, enabling us to better leverage prior knowledge on the input data for improved performance. We first present theoretical insights into the proposed method. We then demonstrate its usefulness for single-cell multi-omic alignment tasks and heterogeneous domain adaptation in machine learning",
    "checked": true,
    "id": "385085f40b48070b4bcf557c7a2eb2c7b90192c2",
    "semantic_title": "breaking isometric ties and introducing priors in gromov-wasserstein distances",
    "citation_count": 0,
    "authors": [
      "Pinar Demetci",
      "Quang Huy Tran",
      "Ievgen Redko",
      "Ritambhara Singh"
    ]
  },
  "https://proceedings.mlr.press/v238/abbas24a.html": {
    "title": "Enhancing In-context Learning via Linear Probe Calibration",
    "volume": "main",
    "abstract": "In-context learning (ICL) is a new paradigm for natural language processing that utilizes Generative Pre-trained Transformer (GPT)-like models. This approach uses prompts that include in-context demonstrations to generate the corresponding output for a new query input. However, applying ICL in real cases does not scale with the number of samples, and lacks robustness to different prompt templates and demonstration permutations. In this paper, we first show that GPT-like models using ICL result in unreliable predictions based on a new metric based on Shannon entropy. Then, to solve this problem, we propose a new technique called the Linear Probe Calibration (LinC), a method that calibrates the model's output probabilities, resulting in reliable predictions and improved performance, while requiring only minimal additional samples (as few as five labeled data samples). LinC significantly enhances the ICL test performance of GPT models on various benchmark datasets, with an average improvement of up to 21%, and up to a 50% improvement in some cases, and significantly boosts the performance of PEFT methods, especially in the low resource regime. Moreover, LinC achieves lower expected calibration error, and is highly robust to varying label proportions, prompt templates, and demonstration permutations",
    "checked": true,
    "id": "2fd6d186eb5648c71eefbcd9baf83471aeefc92b",
    "semantic_title": "enhancing in-context learning via linear probe calibration",
    "citation_count": 3,
    "authors": [
      "Momin Abbas",
      "Yi Zhou",
      "Parikshit Ram",
      "Nathalie Baracaldo",
      "Horst Samulowitz",
      "Theodoros Salonidis",
      "Tianyi Chen"
    ]
  },
  "https://proceedings.mlr.press/v238/lin24b.html": {
    "title": "DNNLasso: Scalable Graph Learning for Matrix-Variate Data",
    "volume": "main",
    "abstract": "We consider the problem of jointly learning row-wise and column-wise dependencies of matrix-variate observations, which are modelled separately by two precision matrices. Due to the complicated structure of Kronecker-product precision matrices in the commonly used matrix-variate Gaussian graphical models, a sparser Kronecker-sum structure was proposed recently based on the Cartesian product of graphs. However, existing methods for estimating Kronecker-sum structured precision matrices do not scale well to large scale datasets. In this paper, we introduce DNNLasso, a diagonally non-negative graphical lasso model for estimating the Kronecker-sum structured precision matrix, which outperforms the state-of-the-art methods by a large margin in both accuracy and computational time",
    "checked": true,
    "id": "e799d36fe2750c994fedd80782bf8c6b5cc9064d",
    "semantic_title": "dnnlasso: scalable graph learning for matrix-variate data",
    "citation_count": 0,
    "authors": [
      "Meixia Lin",
      "Yangjing Zhang"
    ]
  },
  "https://proceedings.mlr.press/v238/houry24a.html": {
    "title": "Fast 1-Wasserstein distance approximations using greedy strategies",
    "volume": "main",
    "abstract": "Among numerous linear approximation methods proposed for optimal transport (OT), tree-based methods appear to be fairly reliable, notably for language processing applications. Inspired by these tree methods, we introduce several greedy heuristics aiming to compute even faster approximations of OT. We first explicitly establish the equivalence between greedy matching and optimal transport for tree metrics, and then we show that tree greedy matching can be reduced to greedy matching on a one-dimensional line. Next, we propose two new greedy-based algorithms in one dimension: the $k$-Greedy and 1D-ICT algorithms. This novel approach provides Wasserstein approximations with accuracy similar to the original tree methods on text datasets while being faster in practice. Finally, these algorithms are applicable beyond tree approximations: using sliced projections of the original data still provides fairly good accuracy while eliminating the need for embedding the data in a fixed and rigid tree structure. This property makes these approaches even more versatile than the original tree OT methods",
    "checked": true,
    "id": "cac1ea8c0376c199feaff683d24ed482ba770dcf",
    "semantic_title": "fast 1-wasserstein distance approximations using greedy strategies",
    "citation_count": 0,
    "authors": [
      "Guillaume Houry",
      "Han Bao",
      "Han Zhao",
      "Makoto Yamada"
    ]
  },
  "https://proceedings.mlr.press/v238/carlsson24a.html": {
    "title": "Pure Exploration in Bandits with Linear Constraints",
    "volume": "main",
    "abstract": "We address the problem of identifying the optimal policy with a fixed confidence level in a multi-armed bandit setup, when \\emph{the arms are subject to linear constraints}. Unlike the standard best-arm identification problem which is well studied, the optimal policy in this case may not be deterministic and could mix between several arms. This changes the geometry of the problem which we characterize via an information-theoretic lower bound. We introduce two asymptotically optimal algorithms for this setting, one based on the Track-and-Stop method and the other based on a game-theoretic approach. Both these algorithms try to track an optimal allocation based on the lower bound and computed by a weighted projection onto the boundary of a normal cone. Finally, we provide empirical results that validate our bounds and visualize how constraints change the hardness of the problem",
    "checked": true,
    "id": "2b0fc501d8a776263df434df499322c45933359f",
    "semantic_title": "pure exploration in bandits with linear constraints",
    "citation_count": 2,
    "authors": [
      "Emil Carlsson",
      "Debabrota Basu",
      "Fredrik Johansson",
      "Devdatt Dubhashi"
    ]
  },
  "https://proceedings.mlr.press/v238/dean24a.html": {
    "title": "Emergent specialization from participation dynamics and multi-learner retraining",
    "volume": "main",
    "abstract": "Numerous online services are data-driven: the behavior of users affects the system's parameters, and the system's parameters affect the users' experience of the service, which in turn affects the way users may interact with the system. For example, people may choose to use a service only for tasks that already works well, or they may choose to switch to a different service. These adaptations influence the ability of a system to learn about a population of users and tasks in order to improve its performance broadly. In this work, we analyze a class of such dynamics—where users allocate their participation amongst services to reduce the individual risk they experience, and services update their model parameters to reduce the service's risk on their current user population. We refer to these dynamics as \\emph{risk-reducing}, which cover a broad class of common model updates including gradient descent and multiplicative weights. For this general class of dynamics, we show that asymptotically stable equilibria are always segmented, with sub-populations allocated to a single learner. Under mild assumptions, the utilitarian social optimum is a stable equilibrium. In contrast to previous work, which shows that repeated risk minimization can result in representation disparity and high overall loss with a single learner (Hashimoto et al., 2018; Miller et al., 2021), we find that repeated myopic updates with multiple learners lead to better outcomes. We illustrate the phenomena via a simulated example initialized from real data",
    "checked": true,
    "id": "e42a3e626322f3fb46f31d40dacfc5142cdca044",
    "semantic_title": "emergent specialization from participation dynamics and multi-learner retraining",
    "citation_count": 2,
    "authors": [
      "Sarah Dean",
      "Mihaela Curmei",
      "Lillian Ratliff",
      "Jamie Morgenstern",
      "Maryam Fazel"
    ]
  },
  "https://proceedings.mlr.press/v238/zhang24b.html": {
    "title": "Optimal Sparse Survival Trees",
    "volume": "main",
    "abstract": "Interpretability is crucial for doctors, hospitals, pharmaceutical companies and biotechnology corporations to analyze and make decisions for high stakes problems that involve human health. Tree-based methods have been widely adopted for survival analysis due to their appealing interpretablility and their ability to capture complex relationships. However, most existing methods to produce survival trees rely on heuristic (or greedy) algorithms, which risk producing sub-optimal models. We present a dynamic-programming-with-bounds approach that finds provably-optimal sparse survival tree models, frequently in only a few seconds",
    "checked": true,
    "id": "360409a49002e741d350c0051eddec6bc1fa896f",
    "semantic_title": "optimal sparse survival trees",
    "citation_count": 0,
    "authors": [
      "Rui Zhang",
      "Rui Xin",
      "Margo Seltzer",
      "Cynthia Rudin"
    ]
  },
  "https://proceedings.mlr.press/v238/li24d.html": {
    "title": "TenGAN: Pure Transformer Encoders Make an Efficient Discrete GAN for De Novo Molecular Generation",
    "volume": "main",
    "abstract": "Deep generative models for de novo molecular generation using discrete data, such as the simplified molecular-input line-entry system (SMILES) strings, have attracted widespread attention in drug design. However, training instability often plagues generative adversarial networks (GANs), leading to problems such as mode collapse and low diversity. This study proposes a pure transformer encoder-based GAN (TenGAN) to solve these issues. The generator and discriminator of TenGAN are variants of the transformer encoders and are combined with reinforcement learning (RL) to generate molecules with the desired chemical properties. Besides, data augmentation of the variant SMILES is leveraged for the TenGAN training to learn the semantics and syntax of SMILES strings. Additionally, we introduce an enhanced variant of TenGAN, named Ten(W)GAN, which incorporates mini-batch discrimination and Wasserstein GAN to improve the ability to generate molecules. The experimental results and ablation studies on the QM9 and ZINC datasets showed that the proposed models generated highly valid and novel molecules with the desired chemical properties in a computationally efficient manner",
    "checked": true,
    "id": "4a7e8e7dfab88a4613eff758d74e023c12a73ef1",
    "semantic_title": "tengan: pure transformer encoders make an efficient discrete gan for de novo molecular generation",
    "citation_count": 1,
    "authors": [
      "Chen Li",
      "Yoshihiro Yamanishi"
    ]
  },
  "https://proceedings.mlr.press/v238/yoshikawa24a.html": {
    "title": "Explanation-based Training with Differentiable Insertion/Deletion Metric-aware Regularizers",
    "volume": "main",
    "abstract": "The quality of explanations for the predictions made by complex machine learning predictors is often measured using insertion and deletion metrics, which assess the faithfulness of the explanations, i.e., how accurately the explanations reflect the predictor's behavior. To improve the faithfulness, we propose insertion/deletion metric-aware explanation-based optimization (ID-ExpO), which optimizes differentiable predictors to improve both the insertion and deletion scores of the explanations while maintaining their predictive accuracy. Because the original insertion and deletion metrics are non-differentiable with respect to the explanations and directly unavailable for gradient-based optimization, we extend the metrics so that they are differentiable and use them to formalize insertion and deletion metric-based regularizers. Our experimental results on image and tabular datasets show that the deep neural network-based predictors that are fine-tuned using ID-ExpO enable popular post-hoc explainers to produce more faithful and easier-to-interpret explanations while maintaining high predictive accuracy. The code is available at https://github.com/yuyay/idexpo",
    "checked": true,
    "id": "4b0b320baf8f7d1b2d4c9b252d227116be935970",
    "semantic_title": "explanation-based training with differentiable insertion/deletion metric-aware regularizers",
    "citation_count": 0,
    "authors": [
      "Yuya Yoshikawa",
      "Tomoharu Iwata"
    ]
  },
  "https://proceedings.mlr.press/v238/baudry24a.html": {
    "title": "Multi-armed bandits with guaranteed revenue per arm",
    "volume": "main",
    "abstract": "We consider a Multi-Armed Bandit problem with covering constraints, where the primary goal is to ensure that each arm receives a minimum expected reward while maximizing the total cumulative reward. In this scenario, the optimal policy then belongs to some unknown feasible set. Unlike much of the existing literature, we do not assume the presence of a safe policy or a feasibility margin, which hinders the exclusive use of conservative approaches. Consequently, we propose and analyze an algorithm that switches between pessimism and optimism in the face of uncertainty. We prove both precise problem-dependent and problem-independent bounds, demonstrating that our algorithm achieves the best of the two approaches—depending on the presence or absence of a feasibility margin—in terms of constraint violation guarantees. Furthermore, our results indicate that playing greedily on the constraints actually outperforms pessimism when considering long-term violations rather than violations on a per-round basis",
    "checked": true,
    "id": "62eafea540141083378a1f83d3d8d42f0289539a",
    "semantic_title": "multi-armed bandits with guaranteed revenue per arm",
    "citation_count": 0,
    "authors": [
      "Dorian Baudry",
      "Nadav Merlis",
      "Mathieu Benjamin Molina",
      "Hugo Richard",
      "Vianney Perchet"
    ]
  },
  "https://proceedings.mlr.press/v238/richard24a.html": {
    "title": "Constant or Logarithmic Regret in Asynchronous Multiplayer Bandits with Limited Communication",
    "volume": "main",
    "abstract": "Multiplayer bandits have recently garnered significant attention due to their relevance in cognitive radio networks. While the existing body of literature predominantly focuses on synchronous players, real-world radio networks, such as those in IoT applications, often feature asynchronous (i.e., randomly activated) devices. This highlights the need for addressing the more challenging asynchronous multiplayer bandits problem. Our first result shows that a natural extension of UCB achieves a minimax regret of $\\mathcal{O}(\\sqrt{T\\log(T)})$ in the centralized setting. More significantly, we introduce Cautious Greedy, which uses $\\mathcal{O}(\\log(T))$ communications and whose instance-dependent regret is constant if the optimal policy assigns at least one player to each arm (a situation proven to occur when arm means are sufficiently close). Otherwise, the regret is, as usual, $\\log(T)$ times the sum of some inverse sub-optimality gaps. We substantiate the optimality of Cautious Greedy through lower-bound analysis based on data-dependent terms. Therefore, we establish a strong baseline for asynchronous multiplayer bandits, at least with $\\mathcal{O}(\\log(T))$ communications",
    "checked": true,
    "id": "b1ef4b93d38ddc77995383e9f99b9504144f5620",
    "semantic_title": "constant or logarithmic regret in asynchronous multiplayer bandits with limited communication",
    "citation_count": 0,
    "authors": [
      "Hugo Richard",
      "Etienne Boursier",
      "Vianney Perchet"
    ]
  },
  "https://proceedings.mlr.press/v238/savvides24a.html": {
    "title": "Error bounds for any regression model using Gaussian processes with gradient information",
    "volume": "main",
    "abstract": "We provide an upper bound for the expected quadratic loss on new data for any regression model. We derive the bound by modelling the underlying function by a Gaussian process (GP). Instead of a single kernel or family of kernels of the same form, we consider all GPs with translation-invariant and continuously twice differentiable kernels having a bounded signal variance and prior covariance of the gradient. To obtain a bound for the expected posterior loss, we present bounds for the posterior variance and squared bias. The squared bias bound depends on the regression model used, which can be arbitrary and not based on GPs. The bounds scale well with data size, in contrast to computing the GP posterior by a Cholesky factorisation of a large matrix. More importantly, our bounds do not require strong prior knowledge as we do not specify the exact kernel form. We validate our theoretical findings by numerical experiments and show that the bounds have applications in uncertainty estimation and concept drift detection",
    "checked": true,
    "id": "c975d41eb6b7e16d0e3c3740981ac44dc6679a2e",
    "semantic_title": "error bounds for any regression model using gaussian processes with gradient information",
    "citation_count": 0,
    "authors": [
      "Rafael Savvides",
      "Hoang Phuc Hau Luu",
      "Kai Puolamäki"
    ]
  },
  "https://proceedings.mlr.press/v238/guimera-cuevas24a.html": {
    "title": "Robust Non-linear Normalization of Heterogeneous Feature Distributions with Adaptive Tanh-Estimators",
    "volume": "main",
    "abstract": "Feature normalization is a crucial step in machine learning that scales numerical values to improve model effectiveness. Noisy or impure datasets can pose a challenge for traditional normalization methods as they may contain outliers that violate statistical assumptions, leading to reduced model performance and increased unpredictability. Non-linear Tanh-Estimators (TE) have been found to provide robust feature normalization, but their fixed scaling factor may not be appropriate for all distributions of feature values. This work presents a refinement to the TE that employs the Wasserstein distance to adaptively estimate the optimal scaling factor for each feature individually against a specified target distribution. The results demonstrate that this adaptive approach can outperform the current TE method in the literature in terms of convergence speed by enabling better initial training starts, thus reducing or eliminating the need to re-adjust model weights during early training phases due to inadequately scaled features. Empirical evaluation was done on synthetic data, standard toy computer vision datasets, and a real-world numeric tabular dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felip Guimerà Cuevas",
      "Helmut Schmid"
    ]
  },
  "https://proceedings.mlr.press/v238/wu24a.html": {
    "title": "Learning Granger Causality from Instance-wise Self-attentive Hawkes Processes",
    "volume": "main",
    "abstract": "We address the problem of learning Granger causality from asynchronous, interdependent, multi-type event sequences. In particular, we are interested in discovering instance-level causal structures in an unsupervised manner. Instance-level causality identifies causal relationships among individual events, providing more fine-grained information for decision-making. Existing work in the literature either requires strong assumptions, such as linearity in the intensity function, or heuristically defined model parameters that do not necessarily meet the requirements of Granger causality. We propose Instance-wise Self-Attentive Hawkes Processes (ISAHP), a novel deep learning framework that can directly infer the Granger causality at the event instance level. ISAHP is the first neural point process model that meets the requirements of Granger causality. It leverages the self-attention mechanism of the transformer to align with the principles of Granger causality. We empirically demonstrate that ISAHP is capable of discovering complex instance-level causal structures that cannot be handled by classical models. We also show that ISAHP achieves state-of-the-art performance in proxy tasks involving type-level causal discovery and instance-level event type prediction",
    "checked": true,
    "id": "5fcd8a50ae8ad279ebfe2566956815155b32a891",
    "semantic_title": "learning granger causality from instance-wise self-attentive hawkes processes",
    "citation_count": 0,
    "authors": [
      "Dongxia Wu",
      "Tsuyoshi Ide",
      "Georgios Kollias",
      "Jiri Navratil",
      "Aurelie Lozano",
      "Naoki Abe",
      "Yian Ma",
      "Rose Yu"
    ]
  },
  "https://proceedings.mlr.press/v238/hands24a.html": {
    "title": "P-tensors: a General Framework for Higher Order Message Passing in Subgraph Neural Networks",
    "volume": "main",
    "abstract": "Several recent papers have proposed increasing the expressiveness of graph neural networks by exploiting subgraphs or other topological structures. In parallel, researchers have investigated higher order permutation equivariant networks. In this paper we tie these two threads together by providing a general framework for higher order permutation equivariant message passing in subgraph neural networks. Our exposition hinges on so-called $P$-tensors, which provide a simple way to define the most general form of permutation equivariant message passing in this category of networks. We show that this paradigm can achieve state-of-the-art performance on benchmark molecular datasets",
    "checked": true,
    "id": "d795f9eac83f783151c7a610620c271433d2953d",
    "semantic_title": "p-tensors: a general framework for higher order message passing in subgraph neural networks",
    "citation_count": 0,
    "authors": [
      "Andrew R. Hands",
      "Tianyi Sun",
      "Risi Kondor"
    ]
  },
  "https://proceedings.mlr.press/v238/saha24a.html": {
    "title": "Faster Convergence with MultiWay Preferences",
    "volume": "main",
    "abstract": "We address the problem of convex optimization with preference feedback, where the goal is to minimize a convex function given a weaker form of comparison queries. Each query consists of two points and the dueling feedback returns a (noisy) single-bit binary comparison of the function values of the two queried points. Here we consider the sign-function-based comparison feedback model and analyze the convergence rates with batched and multiway (argmin of a set queried points) comparisons. Our main goal is to understand the improved convergence rates owing to parallelization in sign-feedback-based optimization problems. Our work is the first to study the problem of convex optimization with multiway preferences and analyze the optimal convergence rates. Our first contribution lies in designing efficient algorithms with a convergence rate of $\\smash{\\widetilde O}(\\frac{d}{\\min\\{m,d\\} \\epsilon})$ for $m$-batched preference feedback where the learner can query $m$-pairs in parallel. We next study a $m$-multiway comparison (‘battling') feedback, where the learner can get to see the argmin feedback of $m$-subset of queried points and show a convergence rate of $\\smash{\\widetilde O}(\\frac{d}{ \\min\\{\\log m,d\\}\\epsilon })$. We show further improved convergence rates with an additional assumption of strong convexity. Finally, we also study the convergence lower bounds for batched preferences and multiway feedback optimization showing the optimality of our convergence rates w.r.t. $m$",
    "checked": true,
    "id": "ebd5cee7c0b327360f8ffc2d1f002c88c20a02f0",
    "semantic_title": "faster convergence with multiway preferences",
    "citation_count": 0,
    "authors": [
      "Aadirupa Saha",
      "Vitaly Feldman",
      "Yishay Mansour",
      "Tomer Koren"
    ]
  },
  "https://proceedings.mlr.press/v238/gong24a.html": {
    "title": "Testing Generated Distributions in GANs to Penalize Mode Collapse",
    "volume": "main",
    "abstract": "Mode collapse remains the primary unresolved challenge within generative adversarial networks (GANs). In this work, we introduce an innovative approach that supplements the discriminator by additionally enforcing the similarity between the generated and real distributions. We implement a one-sample test on the generated samples and employ the resulting test statistic to penalize deviations from the real distribution. Our method encompasses a practical strategy to estimate distributions, compute the test statistic via a differentiable function, and seamlessly incorporate test outcomes into the training objective. Crucially, our approach preserves the convergence and theoretical integrity of GANs, as the introduced constraint represents a requisite condition for optimizing the generator training objective. Notably, our method circumvents reliance on regularization or network modules, enhancing compatibility and facilitating its practical application. Empirical evaluations on diverse public datasets validate the efficacy of our proposed approach",
    "checked": true,
    "id": "b601bbbd465c99af0217d52058f824e543d32523",
    "semantic_title": "testing generated distributions in gans to penalize mode collapse",
    "citation_count": 0,
    "authors": [
      "Yanxiang Gong",
      "Zhiwei Xie",
      "Mei Xie",
      "Xin Ma"
    ]
  },
  "https://proceedings.mlr.press/v238/cabannes24a.html": {
    "title": "The Galerkin method beats Graph-Based Approaches for Spectral Algorithms",
    "volume": "main",
    "abstract": "Historically, the machine learning community has derived spectral decompositions from graph-based approaches. We break with this approach and prove the statistical and computational superiority of the Galerkin method, which consists in restricting the study to a small set of test functions. In particular, we introduce implementation tricks to deal with differential operators in large dimensions with structured kernels. Finally, we extend on the core principles beyond our approach to apply them to non-linear spaces of functions, such as the ones parameterized by deep neural networks, through loss-based optimization procedures",
    "checked": true,
    "id": "c6fa9d1ec3ac195c5020c552c135b926b7713ba4",
    "semantic_title": "the galerkin method beats graph-based approaches for spectral algorithms",
    "citation_count": 2,
    "authors": [
      "Vivien A. Cabannes",
      "Francis Bach"
    ]
  },
  "https://proceedings.mlr.press/v238/sima24a.html": {
    "title": "Online Distribution Learning with Local Privacy Constraints",
    "volume": "main",
    "abstract": "We study the problem of online conditional distribution estimation with \\emph{unbounded} label sets under local differential privacy. The problem may be succinctly stated as follows. Let $\\mathcal{F}$ be a distribution-valued function class with an unbounded label set. Our aim is to estimate an \\emph{unknown} function $f\\in \\mathcal{F}$ in an online fashion. More precisely, at time $t$, given a sample ${\\mathbf{x}}_t$, we generate an estimate of $f({\\mathbf{x}}_t)$ using only a \\emph{privatized} version of the true \\emph{labels} sampled from $f({\\mathbf{x}}_t)$. The objective is to minimize the cumulative KL-risk of a finite horizon $T$. We show that under $(\\epsilon,0)$-local differential privacy for the labels, the KL-risk equals $\\tilde{\\Theta}(\\frac{1}{\\epsilon}\\sqrt{KT}),$ up to poly-logarithmic factors, where $K=|\\mathcal{F}|$. This result significantly differs from the $\\tilde{\\Theta}(\\sqrt{T\\log K})$ bound derived in Wu et al., (2023a) for \\emph{bounded} label sets. As a side-result, our approach recovers a nearly tight upper bound for the hypothesis selection problem of Gopi et al., (2020), which has only been established for the \\emph{batch} setting",
    "checked": true,
    "id": "076be151b0912594c601504da397759a9d60eedd",
    "semantic_title": "online distribution learning with local privacy constraints",
    "citation_count": 0,
    "authors": [
      "Jin Sima",
      "Changlong Wu",
      "Olgica Milenkovic",
      "Wojciech Szpankowski"
    ]
  },
  "https://proceedings.mlr.press/v238/kyu-kwon24a.html": {
    "title": "Minimax optimal density estimation using a shallow generative model with a one-dimensional latent variable",
    "volume": "main",
    "abstract": "A deep generative model yields an implicit estimator for the unknown distribution or density function of the observation. This paper investigates some statistical properties of the implicit density estimator pursued by VAE-type methods from a nonparametric density estimation framework. More specifically, we obtain convergence rates of the VAE-type density estimator under the assumption that the underlying true density function belongs to a locally Holder class. Remarkably, a near minimax optimal rate with respect to the Hellinger metric can be achieved by the simplest network architecture, a shallow generative model with a one-dimensional latent variable",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeok Kyu Kwon",
      "Minwoo Chae"
    ]
  },
  "https://proceedings.mlr.press/v238/ananthakrishnan24a.html": {
    "title": "Delegating Data Collection in Decentralized Machine Learning",
    "volume": "main",
    "abstract": "Motivated by the emergence of decentralized machine learning (ML) ecosystems, we study the delegation of data collection. Taking the field of contract theory as our starting point, we design optimal and near-optimal contracts that deal with two fundamental information asymmetries that arise in decentralized ML: uncertainty in the assessment of model quality and uncertainty regarding the optimal performance of any model. We show that a principal can cope with such asymmetry via simple linear contracts that achieve $1-1/\\epsilon$ fraction of the optimal utility. To address the lack of a priori knowledge regarding the optimal performance, we give a convex program that can adaptively and efficiently compute the optimal contract. We also analyze the optimal utility and linear contracts for the more complex setting of multiple interactions",
    "checked": true,
    "id": "a34bad7b02ba6572c020b4a0347872153392ec17",
    "semantic_title": "delegating data collection in decentralized machine learning",
    "citation_count": 3,
    "authors": [
      "Nivasini Ananthakrishnan",
      "Stephen Bates",
      "Michael Jordan",
      "Nika Haghtalab"
    ]
  },
  "https://proceedings.mlr.press/v238/isik24a.html": {
    "title": "Adaptive Compression in Federated Learning via Side Information",
    "volume": "main",
    "abstract": "The high communication cost of sending model updates from the clients to the server is a significant bottleneck for scalable federated learning (FL). Among existing approaches, state-of-the-art bitrate-accuracy tradeoffs have been achieved using stochastic compression methods – in which the client n sends a sample from a client-only probability distribution $q_{\\phi^{(n)}}$, and the server estimates the mean of the clients' distributions using these samples. However, such methods do not take full advantage of the FL setup where the server, throughout the training process, has side information in the form of a global distribution $p_{\\theta}$ that is close to the client-only distribution $q_{\\phi^{(n)}}$ in Kullback-Leibler (KL) divergence. In this work, we exploit this \\emph{closeness} between the clients' distributions $q_{\\phi^{(n)}}$'s and the side information $p_{\\theta}$ at the server, and propose a framework that requires approximately $D_{KL}(q_{\\phi^{(n)}}|| p_{\\theta})$ bits of communication. We show that our method can be integrated into many existing stochastic compression frameworks to attain the same (and often higher) test accuracy with up to 82 times smaller bitrate than the prior work – corresponding to 2,650 times overall compression",
    "checked": true,
    "id": "82b0100ee958fa6d831391cb4aa7b1bc8042a13b",
    "semantic_title": "adaptive compression in federated learning via side information",
    "citation_count": 4,
    "authors": [
      "Berivan Isik",
      "Francesco Pase",
      "Deniz Gunduz",
      "Sanmi Koyejo",
      "Tsachy Weissman",
      "Michele Zorzi"
    ]
  },
  "https://proceedings.mlr.press/v238/adachi24b.html": {
    "title": "Adaptive Batch Sizes for Active Learning: A Probabilistic Numerics Approach",
    "volume": "main",
    "abstract": "Active learning parallelization is widely used, but typically relies on fixing the batch size throughout experimentation. This fixed approach is inefficient because of a dynamic trade-off between cost and speed—larger batches are more costly, smaller batches lead to slower wall-clock run-times—and the trade-off may change over the run (larger batches are often preferable earlier). To address this trade-off, we propose a novel Probabilistic Numerics framework that adaptively changes batch sizes. By framing batch selection as a quadrature task, our integration-error-aware algorithm facilitates the automatic tuning of batch sizes to meet predefined quadrature precision objectives, akin to how typical optimizers terminate based on convergence thresholds. This approach obviates the necessity for exhaustive searches across all potential batch sizes. We also extend this to scenarios with constrained active learning and constrained optimization, interpreting constraint violations as reductions in the precision requirement, to subsequently adapt batch construction. Through extensive experiments, we demonstrate that our approach significantly enhances learning efficiency and flexibility in diverse Bayesian batch active learning and Bayesian optimization applications",
    "checked": true,
    "id": "a420222563da4ed3412ce3f453ec0b99bb3ca4db",
    "semantic_title": "adaptive batch sizes for active learning: a probabilistic numerics approach",
    "citation_count": 4,
    "authors": [
      "Masaki Adachi",
      "Satoshi Hayakawa",
      "Martin Jørgensen",
      "Xingchen Wan",
      "Vu Nguyen",
      "Harald Oberhauser",
      "Michael A. Osborne"
    ]
  },
  "https://proceedings.mlr.press/v238/adachi24a.html": {
    "title": "Looping in the Human: Collaborative and Explainable Bayesian Optimization",
    "volume": "main",
    "abstract": "Like many optimizers, Bayesian optimization often falls short of gaining user trust due to opacity. While attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. We relax these assumptions and propose a more balanced human-AI partnership with our Collaborative and Explainable Bayesian Optimization (CoExBO) framework. Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. Furthermore, CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to a vanilla Bayesian optimization. We validate CoExBO's efficacy through human-AI teaming experiments in lithium-ion battery design, highlighting substantial improvements over conventional methods. Code is available https://github.com/ma921/CoExBO",
    "checked": true,
    "id": "e74aae28c1295639d0c4f89b6778d978f0cff9d3",
    "semantic_title": "looping in the human: collaborative and explainable bayesian optimization",
    "citation_count": 6,
    "authors": [
      "Masaki Adachi",
      "Brady Planden",
      "David Howey",
      "Michael A. Osborne",
      "Sebastian Orbell",
      "Natalia Ares",
      "Krikamol Muandet",
      "Siu Lun Chau"
    ]
  },
  "https://proceedings.mlr.press/v238/chatterjee24a.html": {
    "title": "Efficient Quantum Agnostic Improper Learning of Decision Trees",
    "volume": "main",
    "abstract": "The agnostic setting is the hardest generalization of the PAC model since it is akin to learning with adversarial noise. In this paper, we give a poly $(n, t, 1/\\epsilon)$ quantum algorithm for learning size $t$ decision trees over $n$-bit inputs with uniform marginal over instances, in the agnostic setting, without membership queries (MQ). This is the first algorithm (classical or quantum) for efficiently learning decision trees without MQ. First, we construct a quantum agnostic weak learner by designing a quantum variant of the classical Goldreich-Levin algorithm that works with strongly biased function oracles. Next, we show how to quantize the agnostic boosting algorithm by Kalai and Kanade (2009) to obtain the first efficient quantum agnostic boosting algorithm (that has a polynomial speedup over existing adaptive quantum boosting algorithms). We then use the quantum agnostic boosting algorithm to boost the weak quantum agnostic learner constructed previously to obtain a quantum agnostic learner for decision trees. Using the above framework, we also give quantum decision tree learning algorithms without MQ in weaker noise models",
    "checked": true,
    "id": "d787681327d1f1d44ab608e4505592d386e14535",
    "semantic_title": "efficient quantum agnostic improper learning of decision trees",
    "citation_count": 1,
    "authors": [
      "Sagnik Chatterjee",
      "Tharrmashastha SAPV",
      "Debajyoti Bera"
    ]
  },
  "https://proceedings.mlr.press/v238/bilaj24a.html": {
    "title": "Meta Learning in Bandits within shared affine Subspaces",
    "volume": "main",
    "abstract": "We study the problem of meta-learning several contextual stochastic bandits tasks by leveraging their concentration around a low dimensional affine subspace, which we learn via online principal component analysis to reduce the expected regret over the encountered bandits. We propose and theoretically analyze two strategies that solve the problem: One based on the principle of optimism in the face of uncertainty and the other via Thompson sampling. Our framework is generic and includes previously proposed approaches as special cases. Besides, the empirical results show that our methods significantly reduce the regret on several bandit tasks",
    "checked": true,
    "id": "43b4c6e0e60d5b6ba7dc481755e114e4eb40ddbc",
    "semantic_title": "meta learning in bandits within shared affine subspaces",
    "citation_count": 1,
    "authors": [
      "Steven Bilaj",
      "Sofien Dhouib",
      "Setareh Maghsudi"
    ]
  },
  "https://proceedings.mlr.press/v238/braun24a.html": {
    "title": "VEC-SBM: Optimal Community Detection with Vectorial Edges Covariates",
    "volume": "main",
    "abstract": "Social networks are often associated with rich side information, such as texts and images. While numerous methods have been developed to identify communities from pairwise interactions, they usually ignore such side information. In this work, we study an extension of the Stochastic Block Model (SBM), a widely used statistical framework for community detection, that integrates vectorial edges covariates: the Vectorial Edges Covariates Stochastic Block Model (VEC-SBM). We propose a novel algorithm based on iterative refinement techniques and show that it optimally recovers the latent communities under the VEC-SBM. Furthermore, we rigorously assess the added value of leveraging edge's side information in the community detection process. We complement our theoretical results with numerical experiments on synthetic and semi-synthetic data",
    "checked": true,
    "id": "27c134900d10436cbd0025ea224b95c5d3594b36",
    "semantic_title": "vec-sbm: optimal community detection with vectorial edges covariates",
    "citation_count": 0,
    "authors": [
      "Guillaume Braun",
      "Masashi Sugiyama"
    ]
  },
  "https://proceedings.mlr.press/v238/zhu24a.html": {
    "title": "Robust Offline Reinforcement Learning with Heavy-Tailed Rewards",
    "volume": "main",
    "abstract": "This paper endeavors to augment the robustness of offline reinforcement learning (RL) in scenarios laden with heavy-tailed rewards, a prevalent circumstance in real-world applications. We propose two algorithmic frameworks, ROAM and ROOM, for robust off-policy evaluation and offline policy optimization (OPO), respectively. Central to our frameworks is the strategic incorporation of the median-of-means method with offline RL, enabling straightforward uncertainty estimation for the value function estimator. This not only adheres to the principle of pessimism in OPO but also adeptly manages heavy-tailed rewards. Theoretical results and extensive experiments demonstrate that our two frameworks outperform existing methods on the logged dataset exhibits heavy-tailed reward distributions. The implementation of the proposal is available at \\url{https://github.com/Mamba413/ROOM}",
    "checked": true,
    "id": "179addc882c4c977215a0125651119e8cefa4ccc",
    "semantic_title": "robust offline reinforcement learning with heavy-tailed rewards",
    "citation_count": 0,
    "authors": [
      "Jin Zhu",
      "Runzhe Wan",
      "Zhengling Qi",
      "Shikai Luo",
      "Chengchun Shi"
    ]
  },
  "https://proceedings.mlr.press/v238/fokkema24a.html": {
    "title": "The Risks of Recourse in Binary Classification",
    "volume": "main",
    "abstract": "Algorithmic recourse provides explanations that help users overturn an unfavorable decision by a machine learning system. But so far very little attention has been paid to whether providing recourse is beneficial or not. We introduce an abstract learning-theoretic framework that compares the risks (i.e., expected losses) for classification with and without algorithmic recourse. This allows us to answer the question of when providing recourse is beneficial or harmful at the population level. Surprisingly, we find that there are many plausible scenarios in which providing recourse turns out to be harmful, because it pushes users to regions of higher class uncertainty and therefore leads to more mistakes. We further study whether the party deploying the classifier has an incentive to strategize in anticipation of having to provide recourse, and we find that sometimes they do, to the detriment of their users. Providing algorithmic recourse may therefore also be harmful at the systemic level. We confirm our theoretical findings in experiments on simulated and real-world data. All in all, we conclude that the current concept of algorithmic recourse is not reliably beneficial, and therefore requires rethinking",
    "checked": true,
    "id": "a69f15c4659b8c69b9b3446c3cc45e27d3e10251",
    "semantic_title": "the risks of recourse in binary classification",
    "citation_count": 1,
    "authors": [
      "Hidde Fokkema",
      "Damien Garreau",
      "Tim van Erven"
    ]
  },
  "https://proceedings.mlr.press/v238/li24e.html": {
    "title": "Prior-dependent analysis of posterior sampling reinforcement learning with function approximation",
    "volume": "main",
    "abstract": "This work advances randomized exploration in reinforcement learning (RL) with function approximation modeled by linear mixture MDPs. We establish the first prior-dependent Bayesian regret bound for RL with function approximation; and refine the Bayesian regret analysis for posterior sampling reinforcement learning (PSRL), presenting an upper bound of $\\tilde{\\mathcal{O}}(d\\sqrt{H^3 T \\log T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the planning horizon, and $T$ the total number of interactions. This signifies a methodological enhancement by optimizing the $\\mathcal{O}(\\sqrt{\\log T})$ factor over the previous benchmark (Osband and Van Roy, 2014) specified to linear mixture MDPs. Our approach, leveraging a value-targeted model learning perspective, introduces a decoupling argument and a variance reduction technique, moving beyond traditional analyses reliant on confidence sets and concentration inequalities to formalize Bayesian regret bounds more effectively",
    "checked": true,
    "id": "dd089f7e75f8e7fc12b0e8051b328fa6779d9632",
    "semantic_title": "prior-dependent analysis of posterior sampling reinforcement learning with function approximation",
    "citation_count": 0,
    "authors": [
      "Yingru Li",
      "Zhiquan Luo"
    ]
  },
  "https://proceedings.mlr.press/v238/dalirrooyfard24a.html": {
    "title": "Graph Partitioning with a Move Budget",
    "volume": "main",
    "abstract": "In many real world networks, there already exists a (not necessarily optimal) $k$-partitioning of the network. Oftentimes, for such networks, one aims to find a $k$-partitioning with a smaller cut value by moving only a few nodes across partitions. The number of nodes that can be moved across partitions is often a constraint forced by budgetary limitations. Motivated by such real-world applications, we introduce and study the $r$-move $k$-partitioning problem, a natural variant of the Multiway cut problem. Given a graph, a set of $k$ terminals and an initial partitioning of the graph, the $r$-move $k$-partitioning problem aims to find a $k$-partitioning with the minimum-weighted cut among all the $k$-partitionings that can be obtained by moving at most $r$ non-terminal nodes to partitions different from their initial ones. Our main result is a polynomial time $3(r+1)$ approximation algorithm for this problem. We further show that this problem is $W[1]$-hard, and give an FPTAS for when $r$ is a small constant",
    "checked": true,
    "id": "413f566badb404c45109ab07c1b49c645fbfa12b",
    "semantic_title": "graph partitioning with a move budget",
    "citation_count": 0,
    "authors": [
      "Mina Dalirrooyfard",
      "Elaheh Fata",
      "Majid Behbahani",
      "Yuriy Nevmyvaka"
    ]
  },
  "https://proceedings.mlr.press/v238/limnios24a.html": {
    "title": "On Ranking-based Tests of Independence",
    "volume": "main",
    "abstract": "In this paper we develop a novel nonparametric framework to test the independence of two random variables $X$ and $Y$ with unknown respective marginals $H(dx)$ and $G(dy)$ and joint distribution $F(dxdy)$, based on Receiver Operating Characteristic (ROC) analysis and bipartite ranking. The rationale behind our approach relies on the fact that, the independence hypothesis $\\mathcal{H}_0$ is necessarily false as soon as the optimal scoring function related to the pair of distributions $(H\\otimes G,;{F})$, obtained from a bipartite ranking algorithm, has a ROC curve that deviates from the main diagonal of the unit square. We consider a wide class of rank statistics encompassing many ways of deviating from the diagonal in the ROC space to build tests of independence. Beyond its great flexibility, this new method has theoretical properties that far surpass those of its competitors. Nonasymptotic bounds for the two types of testing errors are established. From an empirical perspective, the novel procedure we promote in this paper exhibits a remarkable ability to detect small departures, of various types, from the null assumption $\\mathcal{H}_0$, even in high dimension, as supported by the numerical experiments presented here",
    "checked": true,
    "id": "c40b4625730bfcc6ee9e30dd6aa136152256f8f6",
    "semantic_title": "on ranking-based tests of independence",
    "citation_count": 0,
    "authors": [
      "Myrto Limnios",
      "Stéphan Clémençon"
    ]
  },
  "https://proceedings.mlr.press/v238/sebbouh24a.html": {
    "title": "Structured Transforms Across Spaces with Cost-Regularized Optimal Transport",
    "volume": "main",
    "abstract": "Matching a source to a target probability measure is often solved by instantiating a linear optimal transport (OT) problem, parameterized by a ground cost function that quantifies discrepancy between points. When these measures live in the same metric space, the ground cost often defaults to its distance. When instantiated across two different spaces, however, choosing that cost in the absence of aligned data is a conundrum. As a result, practitioners often resort to solving instead a quadratic Gromow-Wasserstein (GW) problem. We exploit in this work a parallel between GW and cost-regularized OT, the regularized minimization of a linear OT objective parameterized by a ground cost. We use this cost-regularized formulation to match measures across two different Euclidean spaces, where the cost is evaluated between transformed source points and target points. We show that several quadratic OT problems fall in this category, and consider enforcing structure in linear transform (e.g., sparsity), by introducing structure-inducing regularizers. We provide a proximal algorithm to extract such transforms from unaligned data, and demonstrate its applicability to single-cell spatial transcriptomics/multiomics matching tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Othmane Sebbouh",
      "Marco Cuturi",
      "Gabriel Peyré"
    ]
  },
  "https://proceedings.mlr.press/v238/odonnat24a.html": {
    "title": "Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias",
    "volume": "main",
    "abstract": "Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, \\texttt{softmax} prediction probabilities are often used as a confidence measure, although they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraints. To address this issue, we propose a novel confidence measure, called $\\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on classification datasets of various data modalities. The code is available at https://github.com/ambroiseodt/tsim",
    "checked": true,
    "id": "df51bc92af21e51b7a13b340722a9a0fff39a342",
    "semantic_title": "leveraging ensemble diversity for robust self-training in the presence of sample selection bias",
    "citation_count": 5,
    "authors": [
      "Ambroise Odonnat",
      "Vasilii Feofanov",
      "Ievgen Redko"
    ]
  },
  "https://proceedings.mlr.press/v238/gupta24a.html": {
    "title": "Clustering Items From Adaptively Collected Inconsistent Feedback",
    "volume": "main",
    "abstract": "We study clustering in a query-based model where the learner can repeatedly query an oracle to determine if two items belong to the same cluster. However, these queries are costly and the oracle's responses are marred by inconsistency and noise. The learner's goal is to adaptively make a small number of queries and return the correct clusters for \\emph{all} $n$ items with high confidence. We develop efficient algorithms for this problem using the sequential hypothesis testing framework. We derive high probability upper bounds on their sample complexity (the number of queries they make) and complement this analysis with an information-theoretic lower bound. In particular, we show that our algorithm for two clusters is nearly optimal when the oracle's error probability is a constant. Our experiments verify these findings and highlight a few shortcomings of our algorithms. Namely, we show that their sample complexity deviates from the lower bound when the error probability of the oracle depends on $n$. We suggest an improvement based on a more efficient sequential hypothesis test and demonstrate it empirically",
    "checked": true,
    "id": "ab6a4591f3a4f17cc61ec3151781913d19016833",
    "semantic_title": "clustering items from adaptively collected inconsistent feedback",
    "citation_count": 1,
    "authors": [
      "Shubham Gupta",
      "Peter W J Staar",
      "Christian de Sainte Marie"
    ]
  },
  "https://proceedings.mlr.press/v238/hegazy24a.html": {
    "title": "Compression with Exact Error Distribution for Federated Learning",
    "volume": "main",
    "abstract": "Compression schemes have been extensively used in Federated Learning (FL) to reduce the communication cost of distributed learning. While most approaches rely on a bounded variance assumption of the noise produced by the compressor, this paper investigates the use of compression and aggregation schemes that produce a specific error distribution, e.g., Gaussian or Laplace, on the aggregated data. We present and analyze different aggregation schemes based on layered quantizers achieving exact error distribution. We provide different methods to leverage the proposed compression schemes to obtain compression-for-free in differential privacy applications. Our general compression methods can recover and improve standard FL schemes with Gaussian perturbations such as Langevin dynamics and randomized smoothing",
    "checked": true,
    "id": "ebd8202d9b2ab5a4ea458a1062fe7a703cc17ab6",
    "semantic_title": "compression with exact error distribution for federated learning",
    "citation_count": 4,
    "authors": [
      "Mahmoud Hegazy",
      "Rémi Leluc",
      "Cheuk Ting Li",
      "Aymeric Dieuleveut"
    ]
  },
  "https://proceedings.mlr.press/v238/pandeva24a.html": {
    "title": "Deep anytime-valid hypothesis testing",
    "volume": "main",
    "abstract": "We propose a general framework for constructing powerful, sequential hypothesis tests for a large class of nonparametric testing problems. The null hypothesis for these problems is defined in an abstract form using the action of two known operators on the data distribution. This abstraction allows for a unified treatment of several classical tasks, such as two-sample testing, independence testing, and conditional-independence testing, as well as modern problems, such as testing for adversarial robustness of machine learning (ML) models. Our proposed framework has the following advantages over classical batch tests: 1) it continuously monitors online data streams and efficiently aggregates evidence against the null, 2) it provides tight control over the type I error without the need for multiple testing correction, 3) it adapts the sample size requirement to the unknown hardness of the problem. We develop a principled approach of leveraging the representation capability of ML models within the testing-by-betting framework, a game-theoretic approach for designing sequential tests. Empirical results on synthetic and real-world datasets demonstrate that tests instantiated using our general framework are competitive against specialized baselines on several tasks",
    "checked": true,
    "id": "6a59a8008852236c06ebf1739809d45ec3261f48",
    "semantic_title": "deep anytime-valid hypothesis testing",
    "citation_count": 0,
    "authors": [
      "Teodora Pandeva",
      "Patrick Forré",
      "Aaditya Ramdas",
      "Shubhanshu Shekhar"
    ]
  },
  "https://proceedings.mlr.press/v238/blaser24a.html": {
    "title": "Federated Linear Contextual Bandits with Heterogeneous Clients",
    "volume": "main",
    "abstract": "The demand for collaborative and private bandit learning across multiple agents is surging due to the growing quantity of data generated from distributed systems. Federated bandit learning has emerged as a promising framework for private, efficient, and decentralized online learning. However, almost all previous works rely on strong assumptions of client homogeneity, i.e., all participating clients shall share the same bandit model; otherwise, they all would suffer linear regret. This greatly restricts the application of federated bandit learning in practice. In this work, we introduce a new approach for federated bandits for heterogeneous clients, which clusters clients for collaborative bandit learning under the federated learning setting. Our proposed algorithm achieves non-trivial sub-linear regret and communication cost for all clients, subject to the communication protocol under federated learning that at anytime only one model can be shared by the server",
    "checked": true,
    "id": "c0f7c58216f357c4215a481b39d63957fb9c4e4f",
    "semantic_title": "federated linear contextual bandits with heterogeneous clients",
    "citation_count": 0,
    "authors": [
      "Ethan Blaser",
      "Chuanhao Li",
      "Hongning Wang"
    ]
  },
  "https://proceedings.mlr.press/v238/vu-tran24a.html": {
    "title": "LEDetection: A Simple Framework for Semi-Supervised Few-Shot Object Detection",
    "volume": "main",
    "abstract": "Few-shot object detection (FSOD) is a challenging problem aimed at detecting novel concepts from few exemplars. Existing approaches to FSOD all assume abundant base labels to adapt to novel objects. This paper studies the new task of semi-supervised FSOD by considering a realistic scenario in which both base and novel labels are simultaneously scarce. We explore the utility of unlabeled data within our proposed label-efficient detection framework and discover its remarkable ability to boost semi-supervised FSOD by way of region proposals. Motivated by this finding, we introduce SoftER Teacher, a robust detector combining pseudo-labeling with consistency learning on region proposals, to harness unlabeled data for improved FSOD without relying on abundant labels. Rigorous experiments show that SoftER Teacher surpasses the novel performance of a strong supervised detector using only 10% of required base labels, without catastrophic forgetting observed in prior approaches. Our work also sheds light on a potential relationship between semi-supervised and few-shot detection suggesting that a stronger semi-supervised detector leads to a more effective few-shot detector",
    "checked": true,
    "id": "e9dc60acbf043116888603c35913f70b2386ebbf",
    "semantic_title": "ledetection: a simple framework for semi-supervised few-shot object detection",
    "citation_count": 0,
    "authors": [
      "Phi Vu Tran"
    ]
  },
  "https://proceedings.mlr.press/v238/islamov24a.html": {
    "title": "AsGrad: A Sharp Unified Analysis of Asynchronous-SGD Algorithms",
    "volume": "main",
    "abstract": "We analyze asynchronous-type algorithms for distributed SGD in the heterogeneous setting, where each worker has its own computation and communication speeds, as well as data distribution. In these algorithms, workers compute possibly stale and stochastic gradients associated with their local data at some iteration back in history and then return those gradients to the server without synchronizing with other workers. We present a unified convergence theory for non-convex smooth functions in the heterogeneous regime. The proposed analysis provides convergence for pure asynchronous SGD and its various modifications. Moreover, our theory explains what affects the convergence rate and what can be done to improve the performance of asynchronous algorithms. In particular, we introduce a novel asynchronous method based on worker shuffling. As a by-product of our analysis, we also demonstrate convergence guarantees for gradient-type algorithms such as SGD with random reshuffling and shuffle-once mini-batch SGD. The derived rates match the best-known results for those algorithms, highlighting the tightness of our approach. Finally, our numerical evaluations support theoretical findings and show the good practical performance of our method",
    "checked": true,
    "id": "ed91e54f56a2c9519e164f23e4eb7e531dec1132",
    "semantic_title": "asgrad: a sharp unified analysis of asynchronous-sgd algorithms",
    "citation_count": 6,
    "authors": [
      "Rustem Islamov",
      "Mher Safaryan",
      "Dan Alistarh"
    ]
  },
  "https://proceedings.mlr.press/v238/hutchinson24a.html": {
    "title": "Directional Optimism for Safe Linear Bandits",
    "volume": "main",
    "abstract": "The safe linear bandit problem is a version of the classical stochastic linear bandit problem where the learner's actions must satisfy an uncertain constraint at all rounds. Due its applicability to many real-world settings, this problem has received considerable attention in recent years. By leveraging a novel approach that we call directional optimism, we find that it is possible to achieve improved regret guarantees for both well-separated problem instances and action sets that are finite star convex sets. Furthermore, we propose a novel algorithm for this setting that improves on existing algorithms in terms of empirical performance, while enjoying matching regret guarantees. Lastly, we introduce a generalization of the safe linear bandit setting where the constraints are convex and adapt our algorithms and analyses to this setting by leveraging a novel convex-analysis based approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Spencer Hutchinson",
      "Berkay Turan",
      "Mahnoosh Alizadeh"
    ]
  },
  "https://proceedings.mlr.press/v238/cui24a.html": {
    "title": "Theory-guided Message Passing Neural Network for Probabilistic Inference",
    "volume": "main",
    "abstract": "Probabilistic inference can be tackled by minimizing a variational free energy through message passing. To improve performance, neural networks are adopted for message computation. Neural message learning is heuristic and requires strong guidance to perform well. In this work, we propose a {\\em theory-guided message passing neural network} (TMPNN) for probabilistic inference. Inspired by existing work, we consider a generalized Bethe free energy which allows for a learnable variational assumption. Instead of using a black-box neural network for message computation, we utilize a general message equation and introduce a symbolic message function with semantically meaningful parameters. The analytically derived symbolic message function is seamlessly integrated into the MPNN framework, giving rise to the proposed TMPNN. TMPNN is trained using algorithmic supervision without requiring exact inference results. Leveraging the theory-guided symbolic function, TMPNN offers strengthened theoretical guarantees compared to conventional heuristic neural models. It presents a novel contribution by demonstrating its applicability to both MAP and marginal inference tasks, outperforming SOTAs in both cases. Furthermore, TMPNN provides improved generalizability across various graph structures and exhibits enhanced data efficiency",
    "checked": true,
    "id": "bec1205132b8bf0a3abe1dd300ad5171d18ed730",
    "semantic_title": "theory-guided message passing neural network for probabilistic inference",
    "citation_count": 0,
    "authors": [
      "Zijun Cui",
      "Hanjing Wang",
      "Tian Gao",
      "Kartik Talamadupula",
      "Qiang Ji"
    ]
  },
  "https://proceedings.mlr.press/v238/sun24a.html": {
    "title": "Understanding Generalization of Federated Learning via Stability: Heterogeneity Matters",
    "volume": "main",
    "abstract": "Generalization performance is a key metric in evaluating machine learning models when applied to real-world applications. Good generalization indicates the model can predict unseen data correctly when trained under a limited number of data. Federated learning (FL), which has emerged as a popular distributed learning framework, allows multiple devices or clients to train a shared model without violating privacy requirements. While the existing literature has studied extensively the generalization performances of centralized machine learning algorithms, similar analysis in the federated settings is either absent or with very restrictive assumptions on the loss functions. In this paper, we aim to analyze the generalization performances of federated learning by means of algorithmic stability, which measures the change of the output model of an algorithm when perturbing one data point. Three widely-used algorithms are studied, including FedAvg, SCAFFOLD, and FedProx, under convex and non-convex loss functions. Our analysis shows that the generalization performances of models trained by these three algorithms are closely related to the heterogeneity of clients' datasets as well as the convergence behaviors of the algorithms. Particularly, in the i.i.d. setting, our results recover the classical results of stochastic gradient descent (SGD)",
    "checked": true,
    "id": "d66227cfab0c27fa759b4000d37643ac55af8eab",
    "semantic_title": "understanding generalization of federated learning via stability: heterogeneity matters",
    "citation_count": 5,
    "authors": [
      "Zhenyu Sun",
      "Xiaochun Niu",
      "Ermin Wei"
    ]
  },
  "https://proceedings.mlr.press/v238/li24f.html": {
    "title": "Mechanics of Next Token Prediction with Self-Attention",
    "volume": "main",
    "abstract": "Transformer-based language models are trained on large datasets to predict the next token given an input sequence. Despite this simple training objective, they have led to revolutionary advances in natural language processing. Underlying this success is the self-attention mechanism. In this work, we ask: What does a single self-attention layer learn from next-token prediction? We show that training self-attention with gradient descent learns an automaton which generates the next token in two distinct steps: (1) Hard retrieval: Given input sequence, self-attention precisely selects the high-priority input tokens associated with the last input token. (2) Soft composition: It then creates a convex combination of the high-priority tokens from which the next token can be sampled. Under suitable conditions, we rigorously characterize these mechanics through a directed graph over tokens extracted from the training data. We prove that gradient descent implicitly discovers the strongly-connected components (SCC) of this graph and self-attention learns to retrieve the tokens that belong to the highest-priority SCC available in the context window. Our theory relies on decomposing the model weights into a directional component and a finite component that correspond to hard retrieval and soft composition steps respectively. This also formalizes a related implicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that these findings shed light on how self-attention processes sequential data and pave the path toward demystifying more complex architectures",
    "checked": true,
    "id": "24a2468a1a16d6c332efc44be90854e4f748eeca",
    "semantic_title": "mechanics of next token prediction with self-attention",
    "citation_count": 7,
    "authors": [
      "Yingcong Li",
      "Yixiao Huang",
      "Muhammed E. Ildiz",
      "Ankit Singh Rawat",
      "Samet Oymak"
    ]
  },
  "https://proceedings.mlr.press/v238/zhang24c.html": {
    "title": "Generalization Bounds of Nonconvex-(Strongly)-Concave Stochastic Minimax Optimization",
    "volume": "main",
    "abstract": "This paper studies the generalization performance of algorithms for solving nonconvex-(strongly)-concave (NC-SC/NC-C) stochastic minimax optimization measured by the stationarity of primal functions. We first establish algorithm-agnostic generalization bounds via uniform convergence between the empirical minimax problem and the population minimax problem. The sample complexities for achieving $\\epsilon$-generalization are $\\tilde{\\mathcal{O}}(d\\kappa^2\\epsilon^{-2})$ and $\\tilde{\\mathcal{O}}(d\\epsilon^{-4})$ for NC-SC and NC-C settings, respectively, where $d$ is the dimension of the primal variable and $\\kappa$ is the condition number. We further study the algorithm-dependent generalization bounds via stability arguments of algorithms. In particular, we introduce a novel stability notion for minimax problems and build a connection between stability and generalization. As a result, we establish algorithm-dependent generalization bounds for stochastic gradient descent ascent (SGDA) and the more general sampling-determined algorithms (SDA)",
    "checked": true,
    "id": "c525a1ec675224bab3574fdb0fe3616078e186a0",
    "semantic_title": "generalization bounds of nonconvex-(strongly)-concave stochastic minimax optimization",
    "citation_count": 0,
    "authors": [
      "Siqi Zhang",
      "Yifan Hu",
      "Liang Zhang",
      "Niao He"
    ]
  },
  "https://proceedings.mlr.press/v238/he24a.html": {
    "title": "TransFusion: Covariate-Shift Robust Transfer Learning for High-Dimensional Regression",
    "volume": "main",
    "abstract": "The main challenge that sets transfer learning apart from traditional supervised learning is the distribution shift, reflected as the shift between the source and target models and that between the marginal covariate distributions. In this work, we tackle model shifts in the presence of covariate shifts in the high-dimensional regression setting. Specifically, we propose a two-step method with a novel fused regularizer that effectively leverages samples from source tasks to improve the learning performance on a target task with limited samples. Nonasymptotic bound is provided for the estimation error of the target model, showing the robustness of the proposed method to covariate shifts. We further establish conditions under which the estimator is minimax-optimal. Additionally, we extend the method to a distributed setting, allowing for a pretraining-finetuning strategy, requiring just one round of communication while retaining the estimation rate of the centralized version. Numerical tests validate our theory, highlighting the method's robustness to covariate shifts",
    "checked": true,
    "id": "f112786edff69388e46f549d4ce7964ad7695861",
    "semantic_title": "transfusion: covariate-shift robust transfer learning for high-dimensional regression",
    "citation_count": 0,
    "authors": [
      "Zelin He",
      "Ying Sun",
      "Runze Li"
    ]
  },
  "https://proceedings.mlr.press/v238/gao24a.html": {
    "title": "Fusing Individualized Treatment Rules Using Secondary Outcomes",
    "volume": "main",
    "abstract": "An individualized treatment rule (ITR) is a decision rule that recommends treatments for patients based on their individual feature variables. In many practices, the ideal ITR for the primary outcome is also expected to cause minimal harm to other secondary outcomes. Therefore, our objective is to learn an ITR that not only maximizes the value function for the primary outcome, but also approximates the optimal rule for the secondary outcomes as closely as possible. To achieve this goal, we introduce a fusion penalty to encourage the ITRs based on different outcomes to yield similar recommendations. Two algorithms are proposed to estimate the ITR using surrogate loss functions. We prove that the agreement rate between the estimated ITR of the primary outcome and the optimal ITRs of the secondary outcomes converges to the true agreement rate faster than if the secondary outcomes are not taken into consideration. Furthermore, we derive the non-asymptotic properties of the value function and misclassification rate for the proposed method. Finally, simulation studies and a real data example are used to demonstrate the finite-sample performance of the proposed method",
    "checked": true,
    "id": "96b7cfbdc109166b702a56a08bbc2a7afdf4f45e",
    "semantic_title": "fusing individualized treatment rules using secondary outcomes",
    "citation_count": 0,
    "authors": [
      "Daiqi Gao",
      "Yuanjia Wang",
      "Donglin Zeng"
    ]
  },
  "https://proceedings.mlr.press/v238/janz24a.html": {
    "title": "Exploration via linearly perturbed loss minimisation",
    "volume": "main",
    "abstract": "We introduce \\emph{exploration via linear loss perturbations} (EVILL), a randomised exploration method for structured stochastic bandit problems that works by solving for the minimiser of a linearly perturbed regularised negative log-likelihood function. We show that, for the case of generalised linear bandits, EVILL reduces to perturbed history exploration (PHE), a method where exploration is done by training on randomly perturbed rewards. In doing so, we provide a simple and clean explanation of when and why random reward perturbations give rise to good bandit algorithms. We propose data-dependent perturbations not present in previous PHE-type methods that allow EVILL to match the performance of Thompson-sampling-style parameter-perturbation methods, both in theory and in practice. Moreover, we show an example outside generalised linear bandits where PHE leads to inconsistent estimates, and thus linear regret, while EVILL remains performant. Like PHE, EVILL can be implemented in just a few lines of code",
    "checked": true,
    "id": "09b1555fc11a15ed3bbf727e65df6410d788aec8",
    "semantic_title": "exploration via linearly perturbed loss minimisation",
    "citation_count": 4,
    "authors": [
      "David Janz",
      "Shuai Liu",
      "Alex Ayoub",
      "Csaba Szepesvári"
    ]
  },
  "https://proceedings.mlr.press/v238/liu24a.html": {
    "title": "Proximal Causal Inference for Synthetic Control with Surrogates",
    "volume": "main",
    "abstract": "The synthetic control method (SCM) has become a popular tool for estimating causal effects in policy evaluation, where a single treated unit is observed. However, SCM faces challenges in accurately predicting post-intervention potential outcomes had, contrary to fact, the treatment been withheld, when the pre-intervention period is short or the post-intervention period is long. To address these issues, we propose a novel method that leverages post-intervention information, specifically time-varying correlates of the causal effect called \"surrogates\", within the synthetic control framework. We establish conditions for identifying model parameters using the proximal inference framework and apply the generalized method of moments (GMM) approach for estimation and inference about the average treatment effect on the treated (ATT). Interestingly, we uncover specific conditions under which exclusively using post-intervention data suffices for estimation within our framework. Through a synthetic experiment and a real-world application, we demonstrate that our method can outperform other synthetic control methods in estimating both short-term and long-term effects, yielding more accurate inferences",
    "checked": true,
    "id": "5403c745d76bef75dc634f9b7e9ca6d772245f0d",
    "semantic_title": "proximal causal inference for synthetic control with surrogates",
    "citation_count": 2,
    "authors": [
      "Jizhou Liu",
      "Eric Tchetgen Tchetgen",
      "Carlos Varjão"
    ]
  },
  "https://proceedings.mlr.press/v238/jankowiak24a.html": {
    "title": "Reparameterized Variational Rejection Sampling",
    "volume": "main",
    "abstract": "Traditional approaches to variational inference rely on parametric families of variational distributions, with the choice of family playing a critical role in determining the accuracy of the resulting posterior approximation. Simple mean-field families often lead to poor approximations, while rich families of distributions like normalizing flows can be difficult to optimize and usually do not incorporate the known structure of the target distribution due to their black-box nature. To expand the space of flexible variational families, we revisit Variational Rejection Sampling (VRS) [Grover et al., 2018], which combines a parametric proposal distribution with rejection sampling to define a rich non-parametric family of distributions that explicitly utilizes the known target distribution. By introducing a low-variance reparameterized gradient estimator for the parameters of the proposal distribution, we make VRS an attractive inference strategy for models with continuous latent variables. We argue theoretically and demonstrate empirically that the resulting method–Reparameterized Variational Rejection Sampling (RVRS)–offers an attractive trade-off between computational cost and inference fidelity. In experiments we show that our method performs well in practice and that it is well suited for black-box inference, especially for models with local latent variables",
    "checked": true,
    "id": "6952eac32f0a921ecfe089cc3721ec2181b189f8",
    "semantic_title": "reparameterized variational rejection sampling",
    "citation_count": 0,
    "authors": [
      "Martin Jankowiak",
      "Du Phan"
    ]
  },
  "https://proceedings.mlr.press/v238/anh-trang24a.html": {
    "title": "E(3)-Equivariant Mesh Neural Networks",
    "volume": "main",
    "abstract": "Triangular meshes are widely used to represent three-dimensional objects. As a result, many recent works have addressed the need for geometric deep learning on 3D meshes. However, we observe that the complexities in many of these architectures do not translate to practical performance, and simple deep models for geometric graphs are competitive in practice. Motivated by this observation, we minimally extend the update equations of E(n)-Equivariant Graph Neural Networks (EGNNs) (Satorras et al., 2021) to incorporate mesh face information and further improve it to account for long-range interactions through a hierarchy. The resulting architecture, Equivariant Mesh Neural Network (EMNN), outperforms other, more complicated equivariant methods on mesh tasks, with a fast run-time and no expensive preprocessing. Our implementation is available at \\url{https://github.com/HySonLab/EquiMesh}",
    "checked": true,
    "id": "c5b655805d8a48d1974db7aa09dde43ff59916fa",
    "semantic_title": "e(3)-equivariant mesh neural networks",
    "citation_count": 1,
    "authors": [
      "Thuan Anh Trang",
      "Nhat Khang Ngo",
      "Daniel T. Levy",
      "Thieu Ngoc Vo",
      "Siamak Ravanbakhsh",
      "Truong Son Hy"
    ]
  },
  "https://proceedings.mlr.press/v238/qin24a.html": {
    "title": "A General Algorithm for Solving Rank-one Matrix Sensing",
    "volume": "main",
    "abstract": "Matrix sensing has many real-world applications in science and engineering, such as system control, distance embedding, and computer vision. The goal of matrix sensing is to recover a matrix $A_\\star \\in \\mathbb{R}^{n \\times n}$, based on a sequence of measurements $(u_i,b_i) \\in \\mathbb{R}^{n} \\times \\mathbb{R}$ such that $u_i^\\top A_\\star u_i = b_i$. Previous work (Zhong et al., 2015) focused on the scenario where matrix $A_{\\star}$ has a small rank, e.g. rank-$k$. Their analysis heavily relies on the RIP assumption, making it unclear how to generalize to high-rank matrices. In this paper, we relax that rank-$k$ assumption and solve a much more general matrix sensing problem. Given an accuracy parameter $\\delta \\in (0,1)$, we can compute $A \\in \\mathbb{R}^{n \\times n}$ in $\\widetilde{O}(m^{3/2} n^2 \\delta^{-1} )$, such that $ |u_i^\\top A u_i - b_i| \\leq \\delta$ for all $i \\in [m]$. We design an efficient algorithm with provable convergence guarantees using stochastic gradient descent for this problem",
    "checked": true,
    "id": "ea683506bf202e612ea0c644affd052a09d90e36",
    "semantic_title": "a general algorithm for solving rank-one matrix sensing",
    "citation_count": 15,
    "authors": [
      "Lianke Qin",
      "Zhao Song",
      "Ruizhe Zhang"
    ]
  },
  "https://proceedings.mlr.press/v238/wang24a.html": {
    "title": "Oracle-Efficient Pessimism: Offline Policy Optimization In Contextual Bandits",
    "volume": "main",
    "abstract": "We consider offline policy optimization (OPO) in contextual bandits, where one is given a fixed dataset of logged interactions. While pessimistic regularizers are typically used to mitigate distribution shift, prior implementations thereof are either specialized or computationally inefficient. We present the first \\emph{general} oracle-efficient algorithm for pessimistic OPO: it reduces to supervised learning, leading to broad applicability. We obtain statistical guarantees analogous to those for prior pessimistic approaches. We instantiate our approach for both discrete and continuous actions and perform experiments in both settings, showing advantage over unregularized OPO across a wide range of configurations",
    "checked": true,
    "id": "282ca5950e831b08dd22f90662090e77febc3ff0",
    "semantic_title": "oracle-efficient pessimism: offline policy optimization in contextual bandits",
    "citation_count": 6,
    "authors": [
      "Lequn Wang",
      "Akshay Krishnamurthy",
      "Alex Slivkins"
    ]
  },
  "https://proceedings.mlr.press/v238/dupuis24a.html": {
    "title": "The Solution Path of SLOPE",
    "volume": "main",
    "abstract": "The SLOPE estimator has the particularity of having null components (sparsity) and components that are equal in absolute value (clustering). The number of clusters depends on the regularization parameter of the estimator. This parameter can be chosen as a trade-off between interpretability (with a small number of clusters) and accuracy (with a small mean squared error or a small prediction error). Finding such a compromise requires to compute the solution path, that is the function mapping the regularization parameter to the estimator. We provide in this article an algorithm to compute the solution path of SLOPE and show how it can be used to adjust the regularization parameter",
    "checked": true,
    "id": "7e4def491e0f9d154efc9be24352cd3716fb2a06",
    "semantic_title": "the solution path of slope",
    "citation_count": 1,
    "authors": [
      "Xavier Dupuis",
      "Patrick Tardivel"
    ]
  },
  "https://proceedings.mlr.press/v238/chen24a.html": {
    "title": "Lower-level Duality Based Reformulation and Majorization Minimization Algorithm for Hyperparameter Optimization",
    "volume": "main",
    "abstract": "Hyperparameter tuning is an important task of machine learning, which can be formulated as a bilevel program (BLP). However, most existing algorithms are not applicable for BLP with non-smooth lower-level problems. To address this, we propose a single-level reformulation of the BLP based on lower-level duality without involving any implicit value function. To solve the reformulation, we propose a majorization minimization algorithm that marjorizes the constraint in each iteration. Furthermore, we show that the subproblems of the proposed algorithm for several widely-used hyperparameter turning models can be reformulated into conic programs that can be efficiently solved by the off-the-shelf solvers. We theoretically prove the convergence of the proposed algorithm and demonstrate its superiority through numerical experiments",
    "checked": true,
    "id": "45f36475475f10892c88db41f62ce138abca682a",
    "semantic_title": "lower-level duality based reformulation and majorization minimization algorithm for hyperparameter optimization",
    "citation_count": 0,
    "authors": [
      "He Chen",
      "Haochen Xu",
      "Rujun Jiang",
      "Anthony Man-Cho So"
    ]
  },
  "https://proceedings.mlr.press/v238/karjol24a.html": {
    "title": "A Unified Framework for Discovering Discrete Symmetries",
    "volume": "main",
    "abstract": "We consider the problem of learning a function respecting a symmetry from among a class of symmetries. We develop a unified framework that enables symmetry discovery across a broad range of subgroups including locally symmetric, dihedral and cyclic subgroups. At the core of the framework is a novel architecture composed of linear, matrix-valued and non-linear functions that expresses functions invariant to these subgroups in a principled manner. The structure of the architecture enables us to leverage multi-armed bandit algorithms and gradient descent to efficiently optimize over the linear and the non-linear functions, respectively, and to infer the symmetry that is ultimately learnt. We also discuss the necessity of the matrix-valued functions in the architecture. Experiments on image-digit sum and polynomial regression tasks demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "310fd7597ea3d6702dbc0041e575ad643dfbe0b6",
    "semantic_title": "a unified framework for discovering discrete symmetries",
    "citation_count": 1,
    "authors": [
      "Pavan Karjol",
      "Rohan Kashyap",
      "Aditya Gopalan",
      "A. P. Prathosh"
    ]
  },
  "https://proceedings.mlr.press/v238/amiraz24a.html": {
    "title": "Recovery Guarantees for Distributed-OMP",
    "volume": "main",
    "abstract": "We study distributed schemes for high-dimensional sparse linear regression, based on orthogonal matching pursuit (OMP). Such schemes are particularly suited for settings where a central fusion center is connected to end machines, that have both computation and communication limitations. We prove that under suitable assumptions, distributed-OMP schemes recover the support of the regression vector with communication per machine linear in its sparsity and logarithmic in the dimension. Remarkably, this holds even at low signal-to-noise-ratios, where individual machines are unable to detect the support. Our simulations show that distributed-OMP schemes are competitive with more computationally intensive methods, and in some cases even outperform them",
    "checked": true,
    "id": "1486b624ff064a41b5ba824a86a4073e0e2343a9",
    "semantic_title": "recovery guarantees for distributed-omp",
    "citation_count": 0,
    "authors": [
      "Chen Amiraz",
      "Robert Krauthgamer",
      "Boaz Nadler"
    ]
  },
  "https://proceedings.mlr.press/v238/vilucchio24a.html": {
    "title": "Asymptotic Characterisation of the Performance of Robust Linear Regression in the Presence of Outliers",
    "volume": "main",
    "abstract": "We study robust linear regression in high-dimension, when both the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\\alpha=n/d$, and study a data model that includes outliers. We provide exact asymptotics for the performances of the empirical risk minimisation (ERM) using $\\ell_2$-regularised $\\ell_2$, $\\ell_1$, and Huber losses, which are the standard approach to such problems. We focus on two metrics for the performance: the generalisation error to similar datasets with outliers, and the estimation error of the original, unpolluted function. Our results are compared with the information theoretic Bayes-optimal estimation bound. For the generalization error, we find that optimally-regularised ERM is asymptotically consistent in the large sample complexity limit if one perform a simple calibration, and compute the rates of convergence. For the estimation error however, we show that due to a norm calibration mismatch, the consistency of the estimator requires an oracle estimate of the optimal norm, or the presence of a cross-validation set not corrupted by the outliers. We examine in detail how performance depends on the loss function and on the degree of outlier corruption in the training set and identify a region of parameters where the optimal performance of the Huber loss is identical to that of the $\\ell_2$ loss, offering insights into the use cases of different loss functions",
    "checked": true,
    "id": "2ddef0ffaa8e09df9e3169692b96b62c4fc5a17e",
    "semantic_title": "asymptotic characterisation of the performance of robust linear regression in the presence of outliers",
    "citation_count": 2,
    "authors": [
      "Matteo Vilucchio",
      "Emanuele Troiani",
      "Vittorio Erba",
      "Florent Krzakala"
    ]
  },
  "https://proceedings.mlr.press/v238/yu24a.html": {
    "title": "Riemannian Laplace Approximation with the Fisher Metric",
    "volume": "main",
    "abstract": "Laplace's method approximates a target density with a Gaussian distribution at its mode. It is computationally efficient and asymptotically exact for Bayesian inference due to the Bernstein-von Mises theorem, but for complex targets and finite-data posteriors it is often too crude an approximation. A recent generalization of the Laplace Approximation transforms the Gaussian approximation according to a chosen Riemannian geometry providing a richer approximation family, while still retaining computational efficiency. However, as shown here, its properties depend heavily on the chosen metric, indeed the metric adopted in previous work results in approximations that are overly narrow as well as being biased even at the limit of infinite data. We correct this shortcoming by developing the approximation family further, deriving two alternative variants that are exact at the limit of infinite data, extending the theoretical analysis of the method, and demonstrating practical improvements in a range of experiments",
    "checked": true,
    "id": "89a222ae4b4ff78a11671d698854f5eede34ee53",
    "semantic_title": "riemannian laplace approximation with the fisher metric",
    "citation_count": 1,
    "authors": [
      "Hanlin Yu",
      "Marcelo Hartmann",
      "Bernardo Williams Moreno Sanchez",
      "Mark Girolami",
      "Arto Klami"
    ]
  },
  "https://proceedings.mlr.press/v238/reichelt24a.html": {
    "title": "Beyond Bayesian Model Averaging over Paths in Probabilistic Programs with Stochastic Support",
    "volume": "main",
    "abstract": "The posterior in probabilistic programs with stochastic support decomposes as a weighted sum of the local posterior distributions associated with each possible program path. We show that making predictions with this full posterior implicitly performs a Bayesian model averaging (BMA) over paths. This is potentially problematic, as BMA weights can be unstable due to model misspecification or inference approximations, leading to sub-optimal predictions in turn. To remedy this issue, we propose alternative mechanisms for path weighting: one based on stacking and one based on ideas from PAC-Bayes. We show how both can be implemented as a cheap post-processing step on top of existing inference engines. In our experiments, we find them to be more robust and lead to better predictions compared to the default BMA weights",
    "checked": true,
    "id": "ba49a846177829dde8c148854ef67d138e6a85b6",
    "semantic_title": "beyond bayesian model averaging over paths in probabilistic programs with stochastic support",
    "citation_count": 0,
    "authors": [
      "Tim Reichelt",
      "Luke Ong",
      "Tom Rainforth"
    ]
  },
  "https://proceedings.mlr.press/v238/aghbalou24a.html": {
    "title": "Sharp error bounds for imbalanced classification: how many examples in the minority class?",
    "volume": "main",
    "abstract": "When dealing with imbalanced classification data, reweighting the loss function is a standard procedure allowing to equilibrate between the true positive and true negative rates within the risk measure. Despite significant theoretical work in this area, existing results do not adequately address a main challenge within the imbalanced classification framework, which is the negligible size of one class in relation to the full sample size and the need to rescale the risk function by a probability tending to zero. To address this gap, we present two novel contributions in the setting where the rare class probability approaches zero: (1) a non asymptotic fast rate probability bound for constrained balanced empirical risk minimization, and (2) a consistent upper bound for balanced nearest neighbors estimates. Our findings provide a clearer understanding of the benefits of class-weighting in realistic settings, opening new avenues for further research in this field",
    "checked": true,
    "id": "da0936a195a63852ab2376f91ca947ef61a30337",
    "semantic_title": "sharp error bounds for imbalanced classification: how many examples in the minority class?",
    "citation_count": 1,
    "authors": [
      "Anass Aghbalou",
      "Anne Sabourin",
      "François Portier"
    ]
  },
  "https://proceedings.mlr.press/v238/bickford-smith24a.html": {
    "title": "Making Better Use of Unlabelled Data in Bayesian Active Learning",
    "volume": "main",
    "abstract": "Fully supervised models are predominant in Bayesian active learning. We argue that their neglect of the information present in unlabelled data harms not just predictive performance but also decisions about what data to acquire. Our proposed solution is a simple framework for semi-supervised Bayesian active learning. We find it produces better-performing models than either conventional Bayesian active learning or semi-supervised learning with randomly acquired data. It is also easier to scale up than the conventional approach. As well as supporting a shift towards semi-supervised models, our findings highlight the importance of studying models and acquisition methods in conjunction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Freddie Bickford Smith",
      "Adam Foster",
      "Tom Rainforth"
    ]
  },
  "https://proceedings.mlr.press/v238/puchkin24a.html": {
    "title": "Breaking the Heavy-Tailed Noise Barrier in Stochastic Optimization Problems",
    "volume": "main",
    "abstract": "We consider stochastic optimization problems with heavy-tailed noise with structured density. For such problems, we show that it is possible to get faster rates of convergence than $O(K^{-2(\\alpha - 1) / \\alpha})$, when the stochastic gradients have finite $\\alpha$-th moment, $\\alpha \\in (1, 2]$. In particular, our analysis allows the noise norm to have an unbounded expectation. To achieve these results, we stabilize stochastic gradients, using smoothed medians of means. We prove that the resulting estimates have negligible bias and controllable variance. This allows us to carefully incorporate them into clipped-SGD and clipped-SSTM and derive new high-probability complexity bounds in the considered setup",
    "checked": true,
    "id": "77d8d32251aec2aad6b4f317e12b0028ea0c4926",
    "semantic_title": "breaking the heavy-tailed noise barrier in stochastic optimization problems",
    "citation_count": 0,
    "authors": [
      "Nikita Puchkin",
      "Eduard Gorbunov",
      "Nickolay Kutuzov",
      "Alexander Gasnikov"
    ]
  },
  "https://proceedings.mlr.press/v238/ahuja24a.html": {
    "title": "Multi-Domain Causal Representation Learning via Weak Distributional Invariances",
    "volume": "main",
    "abstract": "Causal representation learning has emerged as the center of action in causal machine learning research. In particular, multi-domain datasets present a natural opportunity for showcasing the advantages of causal representation learning over standard unsupervised representation learning. While recent works have taken crucial steps towards learning causal representations, they often lack applicability to multi-domain datasets due to over-simplifying assumptions about the data; e.g. each domain comes from a different single-node perfect intervention. In this work, we relax these assumptions and capitalize on the following observation: there often exists a subset of latents whose certain distributional properties (e.g., support, variance) remain stable across domains; this property holds when, for example, each domain comes from a multi-node imperfect intervention. Leveraging this observation, we show that autoencoders that incorporate such invariances can provably identify the stable set of latents from the rest across different settings",
    "checked": true,
    "id": "c5cef109bbf67e4d837bb2f841ae80e5dd655a05",
    "semantic_title": "multi-domain causal representation learning via weak distributional invariances",
    "citation_count": 5,
    "authors": [
      "Kartik Ahuja",
      "Amin Mansouri",
      "Yixin Wang"
    ]
  },
  "https://proceedings.mlr.press/v238/ahmadian24a.html": {
    "title": "Unsupervised Novelty Detection in Pretrained Representation Space with Locally Adapted Likelihood Ratio",
    "volume": "main",
    "abstract": "Detecting novelties given unlabeled examples of normal data is a challenging task in machine learning, particularly when the novel and normal categories are semantically close. Large deep models pretrained on massive datasets can provide a rich representation space in which the simple k-nearest neighbor distance works as a novelty measure. However, as we show in this paper, the basic k-NN method might be insufficient in this context due to ignoring the 'local geometry' of the distribution over representations as well as the impact of irrelevant 'background features'. To address this, we propose a fully unsupervised novelty detection approach that integrates the flexibility of k-NN with a locally adapted scaling of dimensions based on the 'neighbors of nearest neighbor' and computing a 'likelihood ratio' in pretrained (self-supervised) representation spaces. Our experiments with image data show the advantage of this method when off-the-shelf vision transformers (e.g., pretrained by DINO) are used as the feature extractor without any fine-tuning",
    "checked": true,
    "id": "23643ac9a94c066330094a0884a5372fc744b205",
    "semantic_title": "unsupervised novelty detection in pretrained representation space with locally adapted likelihood ratio",
    "citation_count": 0,
    "authors": [
      "Amirhossein Ahmadian",
      "Yifan Ding",
      "Gabriel Eilertsen",
      "Fredrik Lindsten"
    ]
  },
  "https://proceedings.mlr.press/v238/scieur24a.html": {
    "title": "Adaptive Quasi-Newton and Anderson Acceleration Framework with Explicit Global (Accelerated) Convergence Rates",
    "volume": "main",
    "abstract": "Despite the impressive numerical performance of the quasi-Newton and Anderson/nonlinear acceleration methods, their global convergence rates have remained elusive for over 50 years. This study addresses this long-standing issue by introducing a framework that derives novel, adaptive quasi-Newton and nonlinear/Anderson acceleration schemes. Under mild assumptions, the proposed iterative methods exhibit explicit, non-asymptotic convergence rates that blend those of the gradient descent and Cubic Regularized Newton's methods. The proposed approach also includes an accelerated version for convex functions. Notably, these rates are achieved adaptively without prior knowledge of the function's parameters. The framework presented in this study is generic, and its special cases include algorithms such as Newton's method with random subspaces, finite differences, or lazy Hessian. Numerical experiments demonstrated the efficiency of the proposed framework, even compared to the l-BFGS algorithm with Wolfe line-search. The code used in the experiments is available on \\url{https://github.com/windows7lover/QN_With_Guarantees}",
    "checked": true,
    "id": "eb0c5c9298faac81d59214068d7d49c4fd28457a",
    "semantic_title": "adaptive quasi-newton and anderson acceleration framework with explicit global (accelerated) convergence rates",
    "citation_count": 0,
    "authors": [
      "Damien Scieur"
    ]
  },
  "https://proceedings.mlr.press/v238/bao24a.html": {
    "title": "BOBA: Byzantine-Robust Federated Learning with Label Skewness",
    "volume": "main",
    "abstract": "In federated learning, most existing robust aggregation rules (AGRs) combat Byzantine attacks in the IID setting, where client data is assumed to be independent and identically distributed. In this paper, we address label skewness, a more realistic and challenging non-IID setting, where each client only has access to a few classes of data. In this setting, state-of-the-art AGRs suffer from selection bias, leading to significant performance drop for particular classes; they are also more vulnerable to Byzantine attacks due to the increased variation among gradients of honest clients. To address these limitations, we propose an efficient two-stage method named BOBA. Theoretically, we prove the convergence of BOBA with an error of the optimal order. Our empirical evaluations demonstrate BOBA's superior unbiasedness and robustness across diverse models and datasets when compared to various baselines",
    "checked": true,
    "id": "7aeeea5d31314e0c531e6112f0c5202ceab04127",
    "semantic_title": "boba: byzantine-robust federated learning with label skewness",
    "citation_count": 0,
    "authors": [
      "Wenxuan Bao",
      "Jun Wu",
      "Jingrui He"
    ]
  },
  "https://proceedings.mlr.press/v238/guo24a.html": {
    "title": "A White-Box False Positive Adversarial Attack Method on Contrastive Loss Based Offline Handwritten Signature Verification Models",
    "volume": "main",
    "abstract": "In this paper, we tackle the challenge of white-box false positive adversarial attacks on contrastive loss based offline handwritten signature verification models. We propose a novel attack method that treats the attack as a style transfer between closely related but distinct writing styles. To guide the generation of deceptive images, we introduce two new loss functions that enhance the attack success rate by perturbing the Euclidean distance between the embedding vectors of the original and synthesized samples, while ensuring minimal perturbations by reducing the difference between the generated image and the original image. Our method demonstrates state-of-the-art performance in white-box attacks on contrastive loss based offline handwritten signature verification models, as evidenced by our experiments. The key contributions of this paper include a novel false positive attack method, two new loss functions, effective style transfer in handwriting styles, and superior performance in white-box false positive attacks compared to other white-box attack methods",
    "checked": false,
    "id": "72f5c20c887e1def6fa3f70281b7e8acd33a2af5",
    "semantic_title": "a white-box false positive adversarial attack method on contrastive loss-based offline handwritten signature verification models",
    "citation_count": 6,
    "authors": [
      "Zhongliang Guo",
      "Weiye Li",
      "Yifei Qian",
      "Ognjen Arandjelovic",
      "Lei Fang"
    ]
  },
  "https://proceedings.mlr.press/v238/regol24a.html": {
    "title": "Categorical Generative Model Evaluation via Synthetic Distribution Coarsening",
    "volume": "main",
    "abstract": "As we expect to see a rapid integration of generative models in our day to day lives, the development of rigorous methods of evaluation and analysis for generative models has never been more pressing. Multiple works have highlighted the shortcomings of widely used metrics and exposed how they fail to behave as expected in some settings. So far, the response has been to use a variety of metrics that target different desirable and interpretable properties such as fidelity, diversity, and authenticity, to obtain a clearer picture of a generative model's capabilities. These methods mainly focus on ordinal data and they all suffer from the same unavoidable issues stemming from estimating quantities of high-dimensional data from a limited number of samples. We propose to take an alternative approach and to return to the synthetic data setting where the ground truth is explicit and known. We focus on nominal categorical data and introduce an evaluation method that can scale to the high-dimensional settings often encountered in practice. Our method involves successively binning the large space to obtain smaller probability spaces and coarser distributions where meaningful statistical estimates can be obtained. This allows us to provide probabilistic guarantees and sample complexities and we illustrate how our method can be applied to distinguish between the capabilities of several state-of-the-art categorical models",
    "checked": true,
    "id": "1dea33faa4b268e527546825af2eea988ffcd1d2",
    "semantic_title": "categorical generative model evaluation via synthetic distribution coarsening",
    "citation_count": 0,
    "authors": [
      "Florence Regol",
      "Mark Coates"
    ]
  },
  "https://proceedings.mlr.press/v238/feng24b.html": {
    "title": "Monitoring machine learning-based risk prediction algorithms in the presence of performativity",
    "volume": "main",
    "abstract": "Performance monitoring of machine learning (ML)-based risk prediction models in healthcare is complicated by the issue of performativity: when an algorithm predicts a patient to be at high risk for an adverse event, clinicians are more likely to administer prophylactic treatment and alter the very target that the algorithm aims to predict. A simple approach is to ignore performativity and monitor only the untreated patients, whose outcomes remain unaltered. In general, ignoring performativity may inflate Type I error because (i) untreated patients disproportionally represent those with low predicted risk, and (ii) changes in the clinician's trust in the ML algorithm and the algorithm itself can induce complex dependencies that violate standard assumptions. Nevertheless, we show that valid inference is still possible when monitoring \\textit{conditional} rather than marginal performance measures under either the assumption of conditional exchangeability or time-constant selection bias. Finally, performativity can vary over time and induce nonstationarity in the data, which presents challenges for monitoring. To this end, we introduce a new score-based cumulative sum (CUSUM) monitoring procedure with dynamic control limits. Through extensive simulation studies, we study applications of the score-based CUSUM and how it is affected by various factors, including the efficiency of model updating procedures and the level of clinician trust. Finally, we apply the procedure to detect calibration decay of a risk model during the COVID-19 pandemic",
    "checked": true,
    "id": "15367f7a34062a94796073cd98f4c86e56c18a65",
    "semantic_title": "monitoring machine learning-based risk prediction algorithms in the presence of performativity",
    "citation_count": 1,
    "authors": [
      "Jean Feng",
      "Alexej Gossmann",
      "Gene A Pennello",
      "Nicholas Petrick",
      "Berkman Sahiner",
      "Romain Pirracchio"
    ]
  },
  "https://proceedings.mlr.press/v238/depavia24a.html": {
    "title": "Learning-Based Algorithms for Graph Searching Problems",
    "volume": "main",
    "abstract": "We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2023). In this problem, an agent starting at some vertex r has to traverse a (potentially unknown) graph G to find a hidden goal node g while minimizing the total distance traveled. We study a setting in which at any node v, the agent receives a noisy estimate of the distance from v to g. We design algorithms for this search task on unknown graphs. We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error. Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic. Finally, we provide simpler performance bounds on the algorithms of Banerjee et al. (2023) for the case of searching on a known graph and establish new lower bounds for this setting",
    "checked": true,
    "id": "b8b2d2efa6e9d979757f5f2336395c1da2d92a34",
    "semantic_title": "learning-based algorithms for graph searching problems",
    "citation_count": 0,
    "authors": [
      "Adela F. DePavia",
      "Erasmo Tani",
      "Ali Vakilian"
    ]
  },
  "https://proceedings.mlr.press/v238/bacchiocchi24a.html": {
    "title": "Autoregressive Bandits",
    "volume": "main",
    "abstract": "Autoregressive processes naturally arise in a large variety of real-world scenarios, including stock markets, sales forecasting, weather prediction, advertising, and pricing. When facing a sequential decision-making problem in such a context, the temporal dependence between consecutive observations should be properly accounted for guaranteeing convergence to the optimal policy. In this work, we propose a novel online learning setting, namely, Autoregressive Bandits (ARBs), in which the observed reward is governed by an autoregressive process of order $k$, whose parameters depend on the chosen action. We show that, under mild assumptions on the reward process, the optimal policy can be conveniently computed. Then, we devise a new optimistic regret minimization algorithm, namely, AutoRegressive Upper Confidence Bound (AR-UCB), that suffers sublinear regret of order $\\tilde{O} ( \\frac{(k+1)^{3/2}\\sqrt{nT}}{(1-\\Gamma)^2} )$, where $T$ is the optimization horizon, $n$ is the number of actions, and $\\Gamma < 1$ is a stability index of the process. Finally, we empirically validate our algorithm, illustrating its advantages w.r.t. bandit baselines and its robustness to misspecification of key parameters",
    "checked": true,
    "id": "043e4c58f6cbdb881a6b402a28239da2fe45184c",
    "semantic_title": "autoregressive bandits",
    "citation_count": 2,
    "authors": [
      "Francesco Bacchiocchi",
      "Gianmarco Genalti",
      "Davide Maran",
      "Marco Mussi",
      "Marcello Restelli",
      "Nicola Gatti",
      "Alberto Maria Metelli"
    ]
  },
  "https://proceedings.mlr.press/v238/kim24b.html": {
    "title": "DeepFDR: A Deep Learning-based False Discovery Rate Control Method for Neuroimaging Data",
    "volume": "main",
    "abstract": "Voxel-based multiple testing is widely used in neuroimaging data analysis. Traditional false discovery rate (FDR) control methods often ignore the spatial dependence among the voxel-based tests and thus suffer from substantial loss of testing power. While recent spatial FDR control methods have emerged, their validity and optimality remain questionable when handling the complex spatial dependencies of the brain. Concurrently, deep learning methods have revolutionized image segmentation, a task closely related to voxel-based multiple testing. In this paper, we propose DeepFDR, a novel spatial FDR control method that leverages unsupervised deep learning-based image segmentation to address the voxel-based multiple testing problem. Numerical studies, including comprehensive simulations and Alzheimer's disease FDG-PET image analysis, demonstrate DeepFDR's superiority over existing methods. DeepFDR not only excels in FDR control and effectively diminishes the false nondiscovery rate, but also boasts exceptional computational efficiency highly suited for tackling large-scale neuroimaging data",
    "checked": true,
    "id": "993bdb90f6bed53f7607a56fbd83defd46de3478",
    "semantic_title": "deepfdr: a deep learning-based false discovery rate control method for neuroimaging data",
    "citation_count": 0,
    "authors": [
      "Taehyo Kim",
      "Hai Shu",
      "Qiran Jia",
      "Mony de Leon"
    ]
  },
  "https://proceedings.mlr.press/v238/ye24a.html": {
    "title": "Enhancing Hypergradients Estimation: A Study of Preconditioning and Reparameterization",
    "volume": "main",
    "abstract": "Bilevel optimization aims to optimize an outer objective function that depends on the solution to an inner optimization problem. It is routinely used in Machine Learning, notably for hyperparameter tuning. The conventional method to compute the so-called hypergradient of the outer problem is to use the Implicit Function Theorem (IFT). As a function of the error of the inner problem resolution, we study the error of the IFT method. We analyze two strategies to reduce this error: preconditioning the IFT formula and reparameterizing the inner problem. We give a detailed account of the impact of these two modifications on the error, highlighting the role played by higher-order derivatives of the functionals at stake. Our theoretical findings explain when super efficiency, namely reaching an error on the hypergradient that depends quadratically on the error on the inner problem, is achievable and compare the two approaches when this is impossible. Numerical evaluations on hyperparameter tuning for regression problems substantiate our theoretical findings",
    "checked": true,
    "id": "21abf1c043ef91ac7f75b182eac2a1d9963f7104",
    "semantic_title": "enhancing hypergradients estimation: a study of preconditioning and reparameterization",
    "citation_count": 0,
    "authors": [
      "Zhenzhang Ye",
      "Gabriel Peyré",
      "Daniel Cremers",
      "Pierre Ablin"
    ]
  },
  "https://proceedings.mlr.press/v238/stempfle24a.html": {
    "title": "MINTY: Rule-based models that minimize the need for imputing features with missing values",
    "volume": "main",
    "abstract": "Rule models are often preferred in prediction tasks with tabular inputs as they can be easily interpreted using natural language and provide predictive performance on par with more complex models. However, most rule models' predictions are undefined or ambiguous when some inputs are missing, forcing users to rely on statistical imputation models or heuristics like zero imputation, undermining the interpretability of the models. In this work, we propose fitting concise yet precise rule models that learn to avoid relying on features with missing values and, therefore, limit their reliance on imputation at test time. We develop MINTY, a method that learns rules in the form of disjunctions between variables that act as replacements for each other when one or more is missing. This results in a sparse linear rule model, regularized to have small dependence on features with missing values, that allows a trade-off between goodness of fit, interpretability, and robustness to missing values at test time. We demonstrate the value of MINTY in experiments using synthetic and real-world data sets and find its predictive performance comparable or favorable to baselines, with smaller reliance on features with missing values",
    "checked": true,
    "id": "aa6f8f0988b81e578470ae9cef6c4e93e12864fe",
    "semantic_title": "minty: rule-based models that minimize the need for imputing features with missing values",
    "citation_count": 0,
    "authors": [
      "Lena Stempfle",
      "Fredrik Johansson"
    ]
  },
  "https://proceedings.mlr.press/v238/zimerman24a.html": {
    "title": "Multi-Dimensional Hyena for Spatial Inductive Bias",
    "volume": "main",
    "abstract": "The advantage of Vision Transformers over CNNs is only fully manifested when trained over a large dataset, mainly due to the reduced inductive bias towards spatial locality within the transformer's self-attention mechanism. In this work, we present a data-efficient vision transformer that does not rely on self-attention. Instead, it employs a novel generalization to multiple axes of the very recent Hyena layer. We propose several alternative approaches for obtaining this generalization and delve into their unique distinctions and considerations from both empirical and theoretical perspectives. The proposed Hyena N-D layer boosts the performance of various Vision Transformer architectures, such as ViT, Swin, and DeiT across multiple datasets. Furthermore, in the small dataset regime, our Hyena-based ViT is favorable to ViT variants from the recent literature that are specifically designed for solving the same challenge. Finally, we show that a hybrid approach that is based on Hyena N-D for the first layers in ViT, followed by layers that incorporate conventional attention, consistently boosts the performance of various vision transformer architectures. Our code is attached as supplementary",
    "checked": true,
    "id": "e6917b14918f90e8fb89ad4debebd3937e57a123",
    "semantic_title": "multi-dimensional hyena for spatial inductive bias",
    "citation_count": 3,
    "authors": [
      "Itamar Zimerman",
      "Lior Wolf"
    ]
  },
  "https://proceedings.mlr.press/v238/yijia-zheng24a.html": {
    "title": "Graph Machine Learning through the Lens of Bilevel Optimization",
    "volume": "main",
    "abstract": "Bilevel optimization refers to scenarios whereby the optimal solution of a lower-level energy function serves as input features to an upper-level objective of interest. These optimal features typically depend on tunable parameters of the lower-level energy in such a way that the entire bilevel pipeline can be trained end-to-end. Although not generally presented as such, this paper demonstrates how a variety of graph learning techniques can be recast as special cases of bilevel optimization or simplifications thereof. In brief, building on prior work we first derive a more flexible class of energy functions that, when paired with various descent steps (e.g., gradient descent, proximal methods, momentum, etc.), form graph neural network (GNN) message-passing layers; critically, we also carefully unpack where any residual approximation error lies with respect to the underlying constituent message-passing functions. We then probe several simplifications of this framework to derive close connections with non-GNN-based graph learning approaches, including knowledge graph embeddings, various forms of label propagation, and efficient graph-regularized MLP models. And finally, we present supporting empirical results that demonstrate the versatility of the proposed bilevel lens, which we refer to as BloomGML, referencing that BiLevel Optimization Offers More Graph Machine Learning. Our code is available at \\url{https://github.com/amberyzheng/BloomGML}. Let graph ML bloom",
    "checked": false,
    "id": "741157c69f38613c10d910c3a500c7d040d76993",
    "semantic_title": "bloomgml: graph machine learning through the lens of bilevel optimization",
    "citation_count": 0,
    "authors": [
      "Amber Yijia Zheng",
      "Tong He",
      "Yixuan Qiu",
      "Minjie Wang",
      "David Wipf"
    ]
  },
  "https://proceedings.mlr.press/v238/allouah24a.html": {
    "title": "Robust Sparse Voting",
    "volume": "main",
    "abstract": "Many applications, such as content moderation and recommendation, require reviewing and scoring a large number of alternatives. Doing so robustly is however very challenging. Indeed, voters' inputs are inevitably sparse: most alternatives are only scored by a small fraction of voters. This sparsity amplifies the effects of biased voters introducing unfairness, and of malicious voters seeking to hack the voting process by reporting dishonest scores. We give a precise definition of the problem of robust sparse voting, highlight its underlying technical challenges, and present a novel voting mechanism addressing the problem. We prove that, using this mechanism, no voter can have more than a small parameterizable effect on each alternative's score; a property we call Lipschitz resilience. We also identify conditions of voters comparability under which any unanimous preferences can be recovered, even when each voter provides sparse scores, on a scale that is potentially very different from any other voter's score scale. Proving these properties required us to introduce, analyze and carefully compose novel aggregation primitives which could be of independent interest",
    "checked": true,
    "id": "5ab8d1451845185f31d90798a54ffb5fb18e39e1",
    "semantic_title": "robust sparse voting",
    "citation_count": 4,
    "authors": [
      "Youssef Allouah",
      "Rachid Guerraoui",
      "Lê-Nguyên Hoang",
      "Oscar Villemaud"
    ]
  },
  "https://proceedings.mlr.press/v238/joshi24a.html": {
    "title": "Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity",
    "volume": "main",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) on large-scale image-caption datasets learns representations that can achieve remarkable zero-shot generalization. However, such models require a massive amount of pre-training data. Improving the quality of the pre-training data has been shown to be much more effective in improving CLIP's performance than increasing its volume. Nevertheless, finding small subsets of training data that provably generalize best has remained an open question. In this work, we propose the first theoretically rigorous data selection method for CLIP. We show that subsets that closely preserve the cross-covariance of the images and captions of the full data provably achieve a superior generalization performance.Our extensive experiments on ConceptualCaptions3M and ConceptualCaptions12M demonstrate that subsets found by \\textsc{ClipCov} achieve over 2.7x and 1.4x the accuracy of the next best baseline on ImageNet and its shifted versions. Moreover, we show that our subsets obtain 1.5x the average accuracy across 11 downstream datasets, of the next best baseline. The code is available at: \\url{https://github.com/BigML-CS-UCLA/clipcov-data-efficient-clip}",
    "checked": true,
    "id": "7cb06b1a932f96e418e767770305084b0c26e97d",
    "semantic_title": "data-efficient contrastive language-image pretraining: prioritizing data quality over quantity",
    "citation_count": 4,
    "authors": [
      "Siddharth Joshi",
      "Arnav Jain",
      "Ali Payani",
      "Baharan Mirzasoleiman"
    ]
  },
  "https://proceedings.mlr.press/v238/min-kwon24a.html": {
    "title": "Efficient Low-Dimensional Compression of Overparameterized Models",
    "volume": "main",
    "abstract": "In this work, we present a novel approach for compressing overparameterized models, developed through studying their learning dynamics. We observe that for many deep models, updates to the weight matrices occur within a low-dimensional invariant subspace. For deep linear models, we demonstrate that their principal components are fitted incrementally within a small subspace, and use these insights to propose a compression algorithm for deep linear networks that involve decreasing the width of their intermediate layers. We empirically evaluate the effectiveness of our compression technique on matrix recovery problems. Remarkably, by using an initialization that exploits the structure of the problem, we observe that our compressed network converges faster than the original network, consistently yielding smaller recovery errors. We substantiate this observation by developing a theory focused on deep matrix factorization. Finally, we empirically demonstrate how our compressed model has the potential to improve the utility of deep nonlinear models. Overall, our algorithm improves the training efficiency by more than 2x, without compromising generalization",
    "checked": true,
    "id": "2f7ffe3f031864d621e41e2b51e8de7daa0e0393",
    "semantic_title": "efficient low-dimensional compression of overparameterized models",
    "citation_count": 1,
    "authors": [
      "Soo Min Kwon",
      "Zekai Zhang",
      "Dogyoon Song",
      "Laura Balzano",
      "Qing Qu"
    ]
  },
  "https://proceedings.mlr.press/v238/wu24b.html": {
    "title": "Data-Adaptive Probabilistic Likelihood Approximation for Ordinary Differential Equations",
    "volume": "main",
    "abstract": "Estimating the parameters of ordinary differential equations (ODEs) is of fundamental importance in many scientific applications. While ODEs are typically approximated with deterministic algorithms, new research on probabilistic solvers indicates that they produce more reliable parameter estimates by better accounting for numerical errors. However, many ODE systems are highly sensitive to their parameter values. This produces deep local maxima in the likelihood function – a problem which existing probabilistic solvers have yet to resolve. Here we present a novel probabilistic ODE likelihood approximation, DALTON, which can dramatically reduce parameter sensitivity by learning from noisy ODE measurements in a data-adaptive manner. Our approximation scales linearly in both ODE variables and time discretization points, and is applicable to ODEs with both partially-unobserved components and non-Gaussian measurement models. Several examples demonstrate that DALTON produces more accurate parameter estimates via numerical optimization than existing probabilistic ODE solvers, and even in some cases than the exact ODE likelihood itself",
    "checked": true,
    "id": "c5e0ac4479fe346421f272933651887dae0a82df",
    "semantic_title": "data-adaptive probabilistic likelihood approximation for ordinary differential equations",
    "citation_count": 2,
    "authors": [
      "Mohan Wu",
      "Martin Lysy"
    ]
  },
  "https://proceedings.mlr.press/v238/el-halabi24a.html": {
    "title": "Fairness in Submodular Maximization over a Matroid Constraint",
    "volume": "main",
    "abstract": "Submodular maximization over a matroid constraint is a fundamental problem with various applications in machine learning. Some of these applications involve decision-making over datapoints with sensitive attributes such as gender or race. In such settings, it is crucial to guarantee that the selected solution is fairly distributed with respect to this attribute. Recently, fairness has been investigated in submodular maximization under a cardinality constraint in both the streaming and offline settings, however the more general problem with matroid constraint has only been considered in the streaming setting and only for monotone objectives. This work fills this gap. We propose various algorithms and impossibility results offering different trade-offs between quality, fairness, and generality",
    "checked": true,
    "id": "043d2624593266f7c6026e240fc6141938b3a1b9",
    "semantic_title": "fairness in submodular maximization over a matroid constraint",
    "citation_count": 1,
    "authors": [
      "Marwa El Halabi",
      "Jakub Tarnawski",
      "Ashkan Norouzi-Fard",
      "Thuy-Duong Vuong"
    ]
  },
  "https://proceedings.mlr.press/v238/shuo-liu24a.html": {
    "title": "Unified Transfer Learning in High-Dimensional Linear Regression",
    "volume": "main",
    "abstract": "Transfer learning plays a key role in modern data analysis when: (1) the target data are scarce but the source data are sufficient; (2) the distributions of the source and target data are heterogeneous. This paper develops an interpretable unified transfer learning model, termed as UTrans, which can detect both transferable variables and source data. More specifically, we establish the estimation error bounds and prove that our bounds are lower than those with target data only. Besides, we propose a source detection algorithm based on hypothesis testing to exclude the nontransferable data. We evaluate and compare UTrans to the existing algorithms in multiple experiments. It is shown that UTrans attains much lower estimation and prediction errors than the existing methods, while preserving interpretability. We finally apply it to the US intergenerational mobility data and compare our proposed algorithms to the classical machine learning algorithms",
    "checked": true,
    "id": "77adff15b396409836db53107f07efb167e4c43b",
    "semantic_title": "unified transfer learning in high-dimensional linear regression",
    "citation_count": 0,
    "authors": [
      "Shuo Shuo Liu"
    ]
  },
  "https://proceedings.mlr.press/v238/de-bartolomeis24a.html": {
    "title": "Hidden yet quantifiable: A lower bound for confounding strength using randomized trials",
    "volume": "main",
    "abstract": "In the era of fast-paced precision medicine, observational studies play a major role in properly evaluating new treatments in clinical practice. Yet, unobserved confounding can significantly compromise causal conclusions drawn from non-randomized data. We propose a novel strategy that leverages randomized trials to quantify unobserved confounding. First, we design a statistical test to detect unobserved confounding above a certain strength. Then, we use the test to estimate an asymptotically valid lower bound on the unobserved confounding strength. We evaluate the power and validity of our statistical test on several synthetic and semi-synthetic datasets. Further, we show how our lower bound can correctly identify the absence and presence of unobserved confounding in a real-world example",
    "checked": true,
    "id": "913822f40013f33d783f1122c9decd4e60c63beb",
    "semantic_title": "hidden yet quantifiable: a lower bound for confounding strength using randomized trials",
    "citation_count": 2,
    "authors": [
      "Piersilvio De Bartolomeis",
      "Javier Abad Martinez",
      "Konstantin Donhauser",
      "Fanny Yang"
    ]
  },
  "https://proceedings.mlr.press/v238/ghosh24a.html": {
    "title": "Towards Achieving Sub-linear Regret and Hard Constraint Violation in Model-free RL",
    "volume": "main",
    "abstract": "We study the constrained Markov decision processes (CMDPs), in which an agent aims to maximize the expected cumulative reward subject to a constraint on the expected total value of a utility function. Existing approaches have primarily focused on \\emph{soft} constraint violation, which allows compensation across episodes, making it easier to satisfy the constraints. In contrast, we consider a stronger \\emph{hard} constraint violation metric, where only positive constraint violations are accumulated. Our main result is the development of the \\emph{first model-free}, \\emph{simulator-free} algorithm that achieves a sub-linear regret and a sub-linear hard constraint violation simultaneously, even in \\emph{large-scale} systems. In particular, we show that $\\tilde{\\mathcal{O}}(\\sqrt{d^3H^4K})$ regret and $\\tilde{\\mathcal{O}}(\\sqrt{d^3H^4K})$ hard constraint violation bounds can be achieved, where $K$ is the number of episodes, $d$ is the dimension of the feature mapping, $H$ is the length of the episode. Our results are achieved via novel adaptations of the primal-dual LSVI-UCB algorithm, i.e., it searches for the dual variable that balances between regret and constraint violation within every episode, rather than updating it at the end of each episode. This turns out to be crucial for our theoretical guarantees when dealing with hard constraint violations",
    "checked": true,
    "id": "270d870719c717ae82f0e97e8ee29887c93e2983",
    "semantic_title": "towards achieving sub-linear regret and hard constraint violation in model-free rl",
    "citation_count": 3,
    "authors": [
      "Arnob Ghosh",
      "Xingyu Zhou",
      "Ness Shroff"
    ]
  },
  "https://proceedings.mlr.press/v238/xie24a.html": {
    "title": "Distributionally Robust Quickest Change Detection using Wasserstein Uncertainty Sets",
    "volume": "main",
    "abstract": "The problem of quickest detection of a change in the distribution of streaming data is considered. It is assumed that the pre-change distribution is known, while the only information about the post-change is through a (small) set of labeled data. This post-change data is used in a data-driven minimax robust framework, where an uncertainty set for the post-change distribution is constructed. The robust change detection problem is studied in an asymptotic setting where the mean time to false alarm goes to infinity. It is shown that the least favorable distribution (LFD) is an exponentially tilted version of the pre-change density and can be obtained efficiently. A Cumulative Sum (CuSum) test based on the LFD, which is referred to as the distributionally robust (DR) CuSum test, is then shown to be asymptotically robust. The results are extended to the case with multiple post-change uncertainty sets and validated using synthetic and real data examples",
    "checked": true,
    "id": "881b0d069e6764e3919471bb9d8706b77b80107d",
    "semantic_title": "distributionally robust quickest change detection using wasserstein uncertainty sets",
    "citation_count": 1,
    "authors": [
      "Liyan Xie",
      "Yuchen Liang",
      "Venugopal V. Veeravalli"
    ]
  },
  "https://proceedings.mlr.press/v238/harsha-tanneru24a.html": {
    "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLM's behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics — Verbalized Uncertainty and Probing Uncertainty — to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models",
    "checked": true,
    "id": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
    "semantic_title": "quantifying uncertainty in natural language explanations of large language models",
    "citation_count": 5,
    "authors": [
      "Sree Harsha Tanneru",
      "Chirag Agarwal",
      "Himabindu Lakkaraju"
    ]
  },
  "https://proceedings.mlr.press/v238/raed-mualem24a.html": {
    "title": "Submodular Minimax Optimization: Finding Effective Sets",
    "volume": "main",
    "abstract": "Despite the rich existing literature about minimax optimization in continuous settings, only very partial results of this kind have been obtained for combinatorial settings. In this paper, we fill this gap by providing a characterization of submodular minimax optimization, the problem of finding a set (for either the min or the max player) that is effective against every possible response. We show when and under what conditions we can find such sets. We also demonstrate how minimax submodular optimization provides robust solutions for downstream machine learning applications such as (i) prompt engineering in large language models, (ii) identifying robust waiting locations for ride-sharing, (iii) kernelization of the difficulty of instances of the last setting, and (iv) finding adversarial images. Our experiments show that our proposed algorithms consistently outperform other baselines",
    "checked": true,
    "id": "ea982b0cdd588340c1b6e2abc2dae51bf5463b21",
    "semantic_title": "submodular minimax optimization: finding effective sets",
    "citation_count": 4,
    "authors": [
      "Loay Raed Mualem",
      "Ethan R Elenberg",
      "Moran Feldman",
      "Amin Karbasi"
    ]
  },
  "https://proceedings.mlr.press/v238/haldar24a.html": {
    "title": "Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability",
    "volume": "main",
    "abstract": "The existence of adversarial attacks on machine learning models imperceptible to a human is still quite a mystery from a theoretical perspective. In this work, we introduce two notions of adversarial attacks: natural or on-manifold attacks, which are perceptible by a human/oracle, and unnatural or off-manifold attacks, which are not. We argue that the existence of the off-manifold attacks is a natural consequence of the dimension gap between the intrinsic and ambient dimensions of the data. For 2-layer ReLU networks, we prove that even though the dimension gap does not affect generalization performance on samples drawn from the observed data space, it makes the clean-trained model more vulnerable to adversarial perturbations in the off-manifold direction of the data space. Our main results provide an explicit relationship between the $\\ell_2,\\ell_{\\infty}$ attack strength of the on/off-manifold attack and the dimension gap",
    "checked": true,
    "id": "d29ed434340b91c4addb84cc6d03de5dc73ca638",
    "semantic_title": "effect of ambient-intrinsic dimension gap on adversarial vulnerability",
    "citation_count": 1,
    "authors": [
      "Rajdeep Haldar",
      "Yue Xing",
      "Qifan Song"
    ]
  },
  "https://proceedings.mlr.press/v238/futami24a.html": {
    "title": "Information-theoretic Analysis of Bayesian Test Data Sensitivity",
    "volume": "main",
    "abstract": "Bayesian inference is often used to quantify uncertainty. Several recent analyses have rigorously decomposed uncertainty in prediction by Bayesian inference into two types: the inherent randomness in the data generation process and the variability due to lack of data respectively. Existing studies have analyzed these uncertainties from an information-theoretic perspective, assuming the model is well-specified and treating the model parameters as latent variables. However, such information-theoretic uncertainty analysis fails to account for a widely believed property of uncertainty known as sensitivity between test and training data. This means that if the test data is similar to the training data in some sense, the uncertainty will be smaller. In this study, we study such sensitivity using a new decomposition of uncertainty. Our analysis successfully defines such sensitivity using information-theoretic quantities. Furthermore, we extend the existing analysis of Bayesian meta-learning and show the novel sensitivities among tasks for the first time",
    "checked": true,
    "id": "e746c2ff2a5e18d0787905e4dbd0d6ec9b208511",
    "semantic_title": "information-theoretic analysis of bayesian test data sensitivity",
    "citation_count": 0,
    "authors": [
      "Futoshi Futami",
      "Tomoharu Iwata"
    ]
  },
  "https://proceedings.mlr.press/v238/mosenzon24a.html": {
    "title": "Scalable Algorithms for Individual Preference Stable Clustering",
    "volume": "main",
    "abstract": "In this paper, we study the individual preference (IP) stability, which is an notion capturing individual fairness and stability in clustering. Within this setting, a clustering is $\\alpha$-IP stable when each data point's average distance to its cluster is no more than $\\alpha$ times its average distance to any other cluster. In this paper, we study the natural local search algorithm for IP stable clustering. Our analysis confirms a $O(\\log n)$-IP stability guarantee for this algorithm, where $n$ denotes the number of points in the input. Furthermore, by refining the local search approach, we show it runs in an almost linear time, $\\tilde{O}(nk)$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ron Mosenzon",
      "Ali Vakilian"
    ]
  },
  "https://proceedings.mlr.press/v238/yang24b.html": {
    "title": "Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles",
    "volume": "main",
    "abstract": "Gradient boosting of prediction rules is an efficient approach to learn potentially interpretable yet accurate probabilistic models. However, actual interpretability requires to limit the number and size of the generated rules, and existing boosting variants are not designed for this purpose. Though corrective boosting refits all rule weights in each iteration to minimise prediction risk, the included rule conditions tend to be sub-optimal, because commonly used objective functions fail to anticipate this refitting. Here, we address this issue by a new objective function that measures the angle between the risk gradient vector and the projection of the condition output vector onto the orthogonal complement of the already selected conditions. This approach correctly approximates the ideal update of adding the risk gradient itself to the model and favours the inclusion of more general and thus shorter rules. As we demonstrate using a wide range of prediction tasks, this significantly improves the comprehensibility/accuracy trade-off of the fitted ensemble. Additionally, we show how objective values for related rule conditions can be computed incrementally to avoid any substantial computational overhead of the new method",
    "checked": true,
    "id": "de8ad7e6d58c831c9889077dfd9bf1052ed3fdbf",
    "semantic_title": "orthogonal gradient boosting for simpler additive rule ensembles",
    "citation_count": 0,
    "authors": [
      "Fan Yang",
      "Pierre Le Bodic",
      "Michael Kamp",
      "Mario Boley"
    ]
  },
  "https://proceedings.mlr.press/v238/li24g.html": {
    "title": "When No-Rejection Learning is Consistent for Regression with Rejection",
    "volume": "main",
    "abstract": "Learning with rejection has been a prototypical model for studying the human-AI interaction on prediction tasks. Upon the arrival of a sample instance, the model first uses a rejector to decide whether to accept and use the AI predictor to make a prediction or reject and defer the sample to humans. Learning such a model changes the structure of the original loss function and often results in undesirable non-convexity and inconsistency issues. For the classification with rejection problem, several works develop consistent surrogate losses for the joint learning of the predictor and the rejector, while there have been fewer works for the regression counterpart. This paper studies the regression with rejection (RwR) problem and investigates a no-rejection learning strategy that uses all the data to learn the predictor. We first establish the consistency for such a strategy under the weak realizability condition. Then for the case without the weak realizability, we show that the excessive risk can also be upper bounded with the sum of two parts: prediction error and calibration error. Lastly, we demonstrate the advantage of such a proposed learning strategy with empirical evidence",
    "checked": true,
    "id": "282068bc365f5ad531c4efc3784c0786f55ae498",
    "semantic_title": "when no-rejection learning is consistent for regression with rejection",
    "citation_count": 0,
    "authors": [
      "Xiaocheng Li",
      "Shang Liu",
      "Chunlin Sun",
      "Hanzhao Wang"
    ]
  },
  "https://proceedings.mlr.press/v238/yi24a.html": {
    "title": "Filter, Rank, and Prune: Learning Linear Cyclic Gaussian Graphical Models",
    "volume": "main",
    "abstract": "Causal structures in the real world often exhibit cycles naturally due to equilibrium, homeostasis, or feedback. However, causal discovery from observational studies regarding cyclic models has not been investigated extensively because the underlying structure of a linear cyclic structural equation model (SEM) cannot be determined solely from observational data. Inspired by the Bayesian information Criterion (BIC), we construct a score function that assesses both accuracy and sparsity of the structure to determine which linear Gaussian SEM is the best when only observational data is given. Then, we formulate a causal discovery problem as an optimization problem of the measure and propose the Filter, Rank, and Prune (FRP) method for solving it. We empirically demonstrate that our method outperforms competitive cyclic causal discovery baselines",
    "checked": true,
    "id": "e32b65410fa003b9880b80a666906db0452cfef3",
    "semantic_title": "filter, rank, and prune: learning linear cyclic gaussian graphical models",
    "citation_count": 0,
    "authors": [
      "Soheun Yi",
      "Sanghack Lee"
    ]
  },
  "https://proceedings.mlr.press/v238/holland24a.html": {
    "title": "Robust variance-regularized risk minimization with concomitant scaling",
    "volume": "main",
    "abstract": "Under losses which are potentially heavy-tailed, we consider the task of minimizing sums of the loss mean and standard deviation, without trying to accurately estimate the variance. By modifying a technique for variance-free robust mean estimation to fit our problem setting, we derive a simple learning procedure which can be easily combined with standard gradient-based solvers to be used in traditional machine learning workflows. Empirically, we verify that our proposed approach, despite its simplicity, performs as well or better than even the best-performing candidates derived from alternative criteria such as CVaR or DRO risks on a variety of datasets",
    "checked": true,
    "id": "cf422c3ab66720a94b199e049b6986e2ecad1411",
    "semantic_title": "robust variance-regularized risk minimization with concomitant scaling",
    "citation_count": 0,
    "authors": [
      "Matthew J. Holland"
    ]
  },
  "https://proceedings.mlr.press/v238/fan24a.html": {
    "title": "Fast and Adversarial Robust Kernelized SDU Learning",
    "volume": "main",
    "abstract": "SDU learning, a weakly supervised learning problem with only pairwise similarities, dissimilarities data points and unlabeled data available, has many practical applications. However, it is still lacking in defense against adversarial samples, and its learning process can be expensive. To address this gap, we propose a novel adversarial training framework for SDU learning. Our approach reformulates the conventional minimax problem as an equivalent minimization problem based on the kernel perspective, departing from traditional confrontational training methods. Additionally, we employ the random gradient method and random features to accelerate the training process. Theoretical analysis shows that our method can converge to a stationary point at a rate of $\\mathcal{O}(1/T^{1/4})$. Our experimental results show that our algorithm is superior to other adversarial training methods in terms of generalization, efficiency and scalability against various adversarial attacks",
    "checked": true,
    "id": "cdf499af47df6e734071a5f4c327fe58735da0cd",
    "semantic_title": "fast and adversarial robust kernelized sdu learning",
    "citation_count": 0,
    "authors": [
      "Yajing Fan",
      "wanli shi",
      "Yi Chang",
      "Bin Gu"
    ]
  },
  "https://proceedings.mlr.press/v238/zhai24a.html": {
    "title": "Learning Sampling Policy to Achieve Fewer Queries for Zeroth-Order Optimization",
    "volume": "main",
    "abstract": "Zeroth-order (ZO) methods, which use the finite difference of two function evaluations (also called ZO gradient) to approximate first-order gradient, have attracted much attention recently in machine learning because of their broad applications. The accuracy of the ZO gradient highly depends on how many finite differences are averaged, which are intrinsically determined by the number of perturbations randomly drawn from a distribution. Existing ZO methods try to learn a data-driven distribution for sampling the perturbations to improve the efficiency of ZO optimization (ZOO) algorithms. In this paper, we explore a new and parallel direction, \\textit{i.e.}, learn an optimal sampling policy instead of using a totally random strategy to generate perturbations based on the techniques of reinforcement learning (RL), which makes it possible to approximate the gradient with only two function evaluations. Specifically, we first formulate the problem of learning a sampling policy as a Markov decision process. Then, we propose our ZO-RL algorithm, \\textit{i.e.}, using deep deterministic policy gradient, an actor-critic RL algorithm to learn a sampling policy that can guide the generation of perturbed vectors in getting ZO gradients as accurately as possible. Importantly, the existing ZOO algorithms for learning a distribution can be plugged in to improve the exploration of ZO-RL. Experimental results with different ZO estimators show that our ZO-RL algorithm can effectively reduce the query complexity of ZOO algorithms and converge faster than existing ZOO algorithms, especially in the later stage of the optimization process",
    "checked": true,
    "id": "8c2bad0494c378b4ea7fc1446967289d020a6bda",
    "semantic_title": "learning sampling policy to achieve fewer queries for zeroth-order optimization",
    "citation_count": 0,
    "authors": [
      "Zhou Zhai",
      "Wanli Shi",
      "Heng Huang",
      "Yi Chang",
      "Bin Gu"
    ]
  },
  "https://proceedings.mlr.press/v238/medvedovsky24a.html": {
    "title": "Efficient Graph Laplacian Estimation by Proximal Newton",
    "volume": "main",
    "abstract": "The Laplacian-constrained Gaussian Markov Random Field (LGMRF) is a common multivariate statistical model for learning a weighted sparse dependency graph from given data. This graph learning problem can be formulated as a maximum likelihood estimation (MLE) of the precision matrix, subject to Laplacian structural constraints, with a sparsity-inducing penalty term. This paper aims to solve this learning problem accurately and efficiently. First, since the commonly used $\\ell_1$-norm penalty is inappropriate in this setting and may lead to a complete graph, we employ the nonconvex minimax concave penalty (MCP), which promotes sparse solutions with lower estimation bias. Second, as opposed to existing first-order methods for this problem, we develop a second-order proximal Newton approach to obtain an efficient solver, utilizing several algorithmic features, such as using conjugate gradients, preconditioning, and splitting to active/free sets. Numerical experiments demonstrate the advantages of the proposed method in terms of both computational complexity and graph learning accuracy compared to existing methods",
    "checked": true,
    "id": "38baee574980808dc04adcaaf3631c99dc260c18",
    "semantic_title": "efficient graph laplacian estimation by proximal newton",
    "citation_count": 1,
    "authors": [
      "Yakov Medvedovsky",
      "Eran Treister",
      "Tirza S Routtenberg"
    ]
  },
  "https://proceedings.mlr.press/v238/huyuk24a.html": {
    "title": "Adaptive Experiment Design with Synthetic Controls",
    "volume": "main",
    "abstract": "Clinical trials are typically run in order to understand the effects of a new treatment on a given population of patients. However, patients in large populations rarely respond the same way to the same treatment. This heterogeneity in patient responses necessitates trials that investigate effects on multiple subpopulations—especially when a treatment has marginal or no benefit for the overall population but might have significant benefit for a particular subpopulation. Motivated by this need, we propose Syntax, an exploratory trial design that identifies subpopulations with positive treatment effect among many subpopulations. Syntax is sample efficient as it (i) recruits and allocates patients adaptively and (ii) estimates treatment effects by forming synthetic controls for each subpopulation that combines control samples from other subpopulations. We validate the performance of Syntax and provide insights into when it might have an advantage over conventional trial designs through experiments",
    "checked": true,
    "id": "d50e832266ca4476bbf26a8e6808c9b8950cca01",
    "semantic_title": "adaptive experiment design with synthetic controls",
    "citation_count": 1,
    "authors": [
      "Alihan Hüyük",
      "Zhaozhi Qian",
      "Mihaela van der Schaar"
    ]
  },
  "https://proceedings.mlr.press/v238/concha-duarte24a.html": {
    "title": "Online non-parametric likelihood-ratio estimation by Pearson-divergence functional minimization",
    "volume": "main",
    "abstract": "Quantifying the difference between two probability density functions, $p$ and $q$, using available data, is a fundamental problem in Statistics and Machine Learning. A usual approach for addressing this problem is the likelihood-ratio estimation (LRE) between $p$ and $q$, which -to our best knowledge- has been investigated mainly for the offline case. This paper contributes by introducing a new framework for online non-parametric LRE (OLRE) for the setting where pairs of iid observations $(x_t \\sim p, x'_t \\sim q)$ are observed over time. The non-parametric nature of our approach has the advantage of being agnostic to the forms of $p$ and $q$. Moreover, we capitalize on the recent advances in Kernel Methods and functional minimization to develop an estimator that can be efficiently updated at every iteration. We provide theoretical guarantees for the performance of the OLRE method along with empirical validation in synthetic experiments",
    "checked": true,
    "id": "ee6565cf693f417712e3028da7f0a3a576bebc15",
    "semantic_title": "online non-parametric likelihood-ratio estimation by pearson-divergence functional minimization",
    "citation_count": 0,
    "authors": [
      "Alejandro D. de la Concha Duarte",
      "Nicolas Vayatis",
      "Argyris Kalogeratos"
    ]
  },
  "https://proceedings.mlr.press/v238/barrainkua24a.html": {
    "title": "Uncertainty Matters: Stable Conclusions under Unstable Assessment of Fairness Results",
    "volume": "main",
    "abstract": "Recent studies highlight the effectiveness of Bayesian methods in assessing algorithm performance, particularly in fairness and bias evaluation. We present Uncertainty Matters, a multi-objective uncertainty-aware algorithmic comparison framework. In fairness focused scenarios, it models sensitive group confusion matrices using Bayesian updates and facilitates joint comparison of performance (e.g., accuracy) and fairness metrics (e.g., true positive rate parity). Our approach works seamlessly with common evaluation methods like K-fold cross-validation, effectively addressing dependencies among the K posterior metric distributions. The integration of correlated information is carried out through a procedure tailored to the classifier's complexity. Experiments demonstrate that the insights derived from algorithmic comparisons employing the Uncertainty Matters approach are more informative, reliable, and less influenced by particular data partitions. Code for the paper is publicly available at \\url{https://github.com/abarrainkua/UncertaintyMatters}",
    "checked": true,
    "id": "f4d859cd13e72cf9047d973a2505869c2c230621",
    "semantic_title": "uncertainty matters: stable conclusions under unstable assessment of fairness results",
    "citation_count": 0,
    "authors": [
      "Ainhize Barrainkua",
      "Paula Gordaliza",
      "Jose A. Lozano",
      "Novi Quadrianto"
    ]
  },
  "https://proceedings.mlr.press/v238/rammal24a.html": {
    "title": "Communication Compression for Byzantine Robust Learning: New Efficient Algorithms and Improved Rates",
    "volume": "main",
    "abstract": "Byzantine robustness is an essential feature of algorithms for certain distributed optimization problems, typically encountered in collaborative/federated learning. These problems are usually huge-scale, implying that communication compression is also imperative for their resolution. These factors have spurred recent algorithmic and theoretical developments in the literature of Byzantine-robust learning with compression. In this paper, we contribute to this research area in two main directions. First, we propose a new Byzantine-robust method with compression – Byz-DASHA-PAGE – and prove that the new method has better convergence rate (for non-convex and Polyak-Lojasiewicz smooth optimization problems), smaller neighborhood size in the heterogeneous case, and tolerates more Byzantine workers under over-parametrization than the previous method with SOTA theoretical convergence guarantees (Byz-VR-MARINA). Secondly, we develop the first Byzantine-robust method with communication compression and error feedback – Byz-EF21 – along with its bi-directional compression version – Byz-EF21-BC – and derive the convergence rates for these methods for non-convex and Polyak-Lojasiewicz smooth case. We test the proposed methods and illustrate our theoretical findings in the numerical experiments",
    "checked": true,
    "id": "42b380c5c36092400e6ad6259709c90bf37855c1",
    "semantic_title": "communication compression for byzantine robust learning: new efficient algorithms and improved rates",
    "citation_count": 1,
    "authors": [
      "Ahmad Rammal",
      "Kaja Gruntkowska",
      "Nikita Fedin",
      "Eduard Gorbunov",
      "Peter Richtarik"
    ]
  },
  "https://proceedings.mlr.press/v238/kuroki24a.html": {
    "title": "Best-of-Both-Worlds Algorithms for Linear Contextual Bandits",
    "volume": "main",
    "abstract": "We study best-of-both-worlds algorithms for $K$-armed linear contextual bandits. Our algorithms deliver near-optimal regret bounds in both the adversarial and stochastic regimes, without prior knowledge about the environment. In the stochastic regime, we achieve the polylogarithmic rate $\\frac{(dK)^2\\mathrm{poly}\\!\\log(dKT)}{\\Delta_{\\min}}$, where $\\Delta_{\\min}$ is the minimum suboptimality gap over the $d$-dimensional context space. In the adversarial regime, we obtain either the first-order $\\widetilde{\\mathcal{O}}(dK\\sqrt{L^*})$ bound, or the second-order $\\widetilde{\\mathcal{O}}(dK\\sqrt{\\Lambda^*})$ bound, where $L^*$ is the cumulative loss of the best action and $\\Lambda^*$ is a notion of the cumulative second moment for the losses incurred by the algorithm. Moreover, we develop an algorithm based on FTRL with Shannon entropy regularizer that does not require the knowledge of the inverse of the covariance matrix, and achieves a polylogarithmic regret in the stochastic regime while obtaining $\\widetilde{\\mathcal{O}}\\big(dK\\sqrt{T}\\big)$ regret bounds in the adversarial regime",
    "checked": true,
    "id": "df7d75f4066de1a86cbb669d20948535d25cf235",
    "semantic_title": "best-of-both-worlds algorithms for linear contextual bandits",
    "citation_count": 1,
    "authors": [
      "Yuko Kuroki",
      "Alberto Rumi",
      "Taira Tsuchiya",
      "Fabio Vitale",
      "Nicolò Cesa-Bianchi"
    ]
  },
  "https://proceedings.mlr.press/v238/nakamura24a.html": {
    "title": "Fixed-Budget Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit",
    "volume": "main",
    "abstract": "We study the real-valued combinatorial pure exploration of the multi-armed bandit in the fixed-budget setting. We first introduce an algorithm named the Combinatorial Successive Asign (CSA) algorithm, which is the first algorithm that can identify the best action even when the size of the action class is exponentially large with respect to the number of arms. We show that the upper bound of the probability of error of the CSA algorithm matches a lower bound up to a logarithmic factor in the exponent. Then, we introduce another algorithm named the Minimax Combinatorial Successive Accepts and Rejects (Minimax-CombSAR) algorithm for the case where the size of the action class is polynomial, and show that it is optimal, which matches a lower bound. Finally, we experimentally compare the algorithms with previous methods and show that our algorithm performs better",
    "checked": true,
    "id": "b67d97145201272e9aeb498d46f99d78d30149d6",
    "semantic_title": "fixed-budget real-valued combinatorial pure exploration of multi-armed bandit",
    "citation_count": 1,
    "authors": [
      "Shintaro Nakamura",
      "Masashi Sugiyama"
    ]
  },
  "https://proceedings.mlr.press/v238/frick24a.html": {
    "title": "Scalable Learning of Item Response Theory Models",
    "volume": "main",
    "abstract": "Item Response Theory (IRT) models aim to assess latent abilities of $n$ examinees along with latent difficulty characteristics of $m$ test items from categorical data that indicates the quality of their corresponding answers. Classical psychometric assessments are based on a relatively small number of examinees and items, say a class of $200$ students solving an exam comprising $10$ problems. More recent global large scale assessments such as PISA, or internet studies, may lead to significantly increased numbers of participants. Additionally, in the context of Machine Learning where algorithms take the role of examinees and data analysis problems take the role of items, both $n$ and $m$ may become very large, challenging the efficiency and scalability of computations. To learn the latent variables in IRT models from large data, we leverage the similarity of these models to logistic regression, which can be approximated accurately using small weighted subsets called coresets. We develop coresets for their use in alternating IRT training algorithms, facilitating scalable learning from large data",
    "checked": true,
    "id": "f4c0ee84ea25c9e1e9623e5da7482711e8e5a834",
    "semantic_title": "scalable learning of item response theory models",
    "citation_count": 2,
    "authors": [
      "Susanne Frick",
      "Amer Krivosija",
      "Alexander Munteanu"
    ]
  },
  "https://proceedings.mlr.press/v238/nika24a.html": {
    "title": "Corruption-Robust Offline Two-Player Zero-Sum Markov Games",
    "volume": "main",
    "abstract": "We study data corruption robustness in offline two-player zero-sum Markov games. Given a dataset of realized trajectories of two players, an adversary is allowed to modify an $\\epsilon$-fraction of it. The learner's goal is to identify an approximate Nash Equilibrium policy pair from the corrupted data. We consider this problem in linear Markov games under different degrees of data coverage and corruption. We start by providing an information-theoretic lower bound on the suboptimality gap of any learner. Next, we propose robust versions of the Pessimistic Minimax Value Iteration algorithm (Zhong et al., 2022), both under coverage on the corrupted data and under coverage only on the clean data, and show that they achieve (near)-optimal suboptimality gap bounds with respect to $\\epsilon$. We note that we are the first to provide such a characterization of the problem of learning approximate Nash Equilibrium policies in offline two-player zero-sum Markov games under data corruption",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andi Nika",
      "Debmalya Mandal",
      "Adish Singla",
      "Goran Radanovic"
    ]
  },
  "https://proceedings.mlr.press/v238/iwazaki24a.html": {
    "title": "Risk Seeking Bayesian Optimization under Uncertainty for Obtaining Extremum",
    "volume": "main",
    "abstract": "Real-world black-box optimization tasks often focus on obtaining the best reward, which includes an intrinsic random quantity from uncontrollable environmental factors. For this problem, we formulate a novel risk-seeking optimization problem whose aim is to obtain the best possible reward within a fixed budget under uncontrollable factors. We consider two settings: (1) environmental model setting for the case that we can observe uncontrollable environmental variables that affect the observation as the input of a target function, and (2) heteroscedastic model setting for the case that any uncontrollable variables cannot be observed. We propose a novel Bayesian optimization method called kernel explore-then-commit (kernel-ETC) and provide the regret upper bound for both settings. We demonstrate the effectiveness of kernel-ETC through several numerical experiments, including the hyperparameter tuning task and the simulation function derived from polymer synthesis real data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shogo Iwazaki",
      "Tomohiko Tanabe",
      "Mitsuru Irie",
      "Shion Takeno",
      "Yu Inatsu"
    ]
  },
  "https://proceedings.mlr.press/v238/wesel24a.html": {
    "title": "Quantized Fourier and Polynomial Features for more Expressive Tensor Network Models",
    "volume": "main",
    "abstract": "In the context of kernel machines, polynomial and Fourier features are commonly used to provide a nonlinear extension to linear models by mapping the data to a higher-dimensional space. Unless one considers the dual formulation of the learning problem, which renders exact large-scale learning unfeasible, the exponential increase of model parameters in the dimensionality of the data caused by their tensor-product structure prohibits to tackle high-dimensional problems. One of the possible approaches to circumvent this exponential scaling is to exploit the tensor structure present in the features by constraining the model weights to be an underparametrized tensor network. In this paper we quantize, i.e. further tensorize, polynomial and Fourier features. Based on this feature quantization we propose to quantize the associated model weights, yielding quantized models. We show that, for the same number of model parameters, the resulting quantized models have a higher bound on the VC-dimension as opposed to their non-quantized counterparts, at no additional computational cost while learning from identical features. We verify experimentally how this additional tensorization regularizes the learning problem by prioritizing the most salient features in the data and how it provides models with increased generalization capabilities. We finally benchmark our approach on large regression task, achieving state-of-the-art results on a laptop computer",
    "checked": true,
    "id": "9965b5514fcb3d810edff44b9bf1449bef5f6135",
    "semantic_title": "quantized fourier and polynomial features for more expressive tensor network models",
    "citation_count": 0,
    "authors": [
      "Frederiek Wesel",
      "Kim Batselier"
    ]
  },
  "https://proceedings.mlr.press/v238/kjaersgaard24a.html": {
    "title": "Fair Soft Clustering",
    "volume": "main",
    "abstract": "Scholars in the machine learning community have recently focused on analyzing the fairness of learning models, including clustering algorithms. In this work we study fair clustering in a probabilistic (soft) setting, where observations may belong to several clusters determined by probabilities. We introduce new probabilistic fairness metrics, which generalize and extend existing non-probabilistic fairness frameworks and propose an algorithm for obtaining a fair probabilistic cluster solution from a data representation known as a fairlet decomposition. Finally, we demonstrate our proposed fairness metrics and algorithm by constructing a fair Gaussian mixture model on three real-world datasets. We achieve this by identifying balanced micro-clusters which minimize the distances induced by the model, and on which traditional clustering can be performed while ensuring the fairness of the solution",
    "checked": true,
    "id": "2b7f26a1b4b6ab59099d24f6a5d06391a988b8c0",
    "semantic_title": "fair soft clustering",
    "citation_count": 0,
    "authors": [
      "Rune D. Kjærsgaard",
      "Pekka Parviainen",
      "Saket Saurabh",
      "Madhumita Kundu",
      "Line Clemmensen"
    ]
  },
  "https://proceedings.mlr.press/v238/tong24a.html": {
    "title": "Simulation-Free Schrödinger Bridges via Score and Flow Matching",
    "volume": "main",
    "abstract": "We present simulation-free score and flow matching ([SF]$^2$M), a simulation-free objective for inferring stochastic dynamics given unpaired samples drawn from arbitrary source and target distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic generative modeling as a Schrödinger bridge problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]$^2$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]$^2$M is the first method to accurately model cell dynamics in high dimensions and can recover known gene regulatory networks from simulated data. Our code is available in the TorchCFM package at \\url{https://github.com/atong01/conditional-flow-matching}",
    "checked": true,
    "id": "38780dbcee61d67aeeb800d33eded440d7fcd32c",
    "semantic_title": "simulation-free schrödinger bridges via score and flow matching",
    "citation_count": 22,
    "authors": [
      "Alexander Y. Tong",
      "Nikolay Malkin",
      "Kilian Fatras",
      "Lazar Atanackovic",
      "Yanlei Zhang",
      "Guillaume Huguet",
      "Guy Wolf",
      "Yoshua Bengio"
    ]
  },
  "https://proceedings.mlr.press/v238/jolicoeur-martineau24a.html": {
    "title": "Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees",
    "volume": "main",
    "abstract": "Tabular data is hard to acquire and is subject to missing values. This paper introduces a novel approach for generating and imputing mixed-type (continuous and categorical) tabular data utilizing score-based diffusion and conditional flow matching. In contrast to prior methods that rely on neural networks to learn the score function or the vector field, we adopt XGBoost, a widely used Gradient-Boosted Tree (GBT) technique. To test our method, we build one of the most extensive benchmarks for tabular data generation and imputation, containing 27 diverse datasets and 9 metrics. Through empirical evaluation across the benchmark, we demonstrate that our approach outperforms deep-learning generation methods in data generation tasks and remains competitive in data imputation. Notably, it can be trained in parallel using CPUs without requiring a GPU. Our Python and R code is available at \\url{https://github.com/SamsungSAILMontreal/ForestDiffusion}",
    "checked": true,
    "id": "2ba40e67539b2ea5c2ca8882ebf4497743e55e96",
    "semantic_title": "generating and imputing tabular data via diffusion and flow-based gradient-boosted trees",
    "citation_count": 11,
    "authors": [
      "Alexia Jolicoeur-Martineau",
      "Kilian Fatras",
      "Tal Kachman"
    ]
  },
  "https://proceedings.mlr.press/v238/carpintero-perez24a.html": {
    "title": "Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman graph kernels",
    "volume": "main",
    "abstract": "Supervised learning has recently garnered significant attention in the field of computational physics due to its ability to effectively extract complex patterns for tasks like solving partial differential equations, or predicting material properties. Traditionally, such datasets consist of inputs given as meshes with a large number of nodes representing the problem geometry (seen as graphs), and corresponding outputs obtained with a numerical solver. This means the supervised learning model must be able to handle large and sparse graphs with continuous node attributes. In this work, we focus on Gaussian process regression, for which we introduce the Sliced Wasserstein Weisfeiler-Lehman (SWWL) graph kernel. In contrast to existing graph kernels, the proposed SWWL kernel enjoys positive definiteness and a drastic complexity reduction, which makes it possible to process datasets that were previously impossible to handle. The new kernel is first validated on graph classification for molecular datasets, where the input graphs have a few tens of nodes. The efficiency of the SWWL kernel is then illustrated on graph regression in computational fluid dynamics and solid mechanics, where the input graphs are made up of tens of thousands of nodes",
    "checked": true,
    "id": "b989adba7eb8e8e0013df170b8a781ee5dd53a3b",
    "semantic_title": "gaussian process regression with sliced wasserstein weisfeiler-lehman graph kernels",
    "citation_count": 1,
    "authors": [
      "Raphaël Carpintero Perez",
      "Sébastien Da Veiga",
      "Josselin Garnier",
      "Brian Staber"
    ]
  },
  "https://proceedings.mlr.press/v238/robert-nicoud24a.html": {
    "title": "Intrinsic Gaussian Vector Fields on Manifolds",
    "volume": "main",
    "abstract": "Various applications ranging from robotics to climate science require modeling signals on non-Euclidean domains, such as the sphere. Gaussian process models on manifolds have recently been proposed for such tasks, in particular when uncertainty quantification is needed. In the manifold setting, vector-valued signals can behave very differently from scalar-valued ones, with much of the progress so far focused on modeling the latter. The former, however, are crucial for many applications, such as modeling wind speeds or force fields of unknown dynamical systems. In this paper, we propose novel Gaussian process models for vector-valued signals on manifolds that are intrinsically defined and account for the geometry of the space in consideration. We provide computational primitives needed to deploy the resulting Hodge-Matérn Gaussian vector fields on the two-dimensional sphere and the hypertori. Further, we highlight two generalization directions: discrete two-dimensional meshes and \"ideal\" manifolds like hyperspheres, Lie groups, and homogeneous spaces. Finally, we show that our Gaussian vector fields constitute considerably more refined inductive biases than the extrinsic fields proposed before",
    "checked": true,
    "id": "0c100d324968f0a3c2ed14027b5b6c21bc5827f2",
    "semantic_title": "intrinsic gaussian vector fields on manifolds",
    "citation_count": 2,
    "authors": [
      "Daniel Robert-Nicoud",
      "Andreas Krause",
      "Viacheslav Borovitskiy"
    ]
  },
  "https://proceedings.mlr.press/v238/cosier24a.html": {
    "title": "A Unifying Variational Framework for Gaussian Process Motion Planning",
    "volume": "main",
    "abstract": "To control how a robot moves, motion planning algorithms must compute paths in high-dimensional state spaces while accounting for physical constraints related to motors and joints, generating smooth and stable motions, avoiding obstacles, and preventing collisions. A motion planning algorithm must therefore balance competing demands, and should ideally incorporate uncertainty to handle noise, model errors, and facilitate deployment in complex environments. To address these issues, we introduce a framework for robot motion planning based on variational Gaussian processes, which unifies and generalizes various probabilistic-inference-based motion planning algorithms, and connects them with optimization-based planners. Our framework provides a principled and flexible way to incorporate equality-based, inequality-based, and soft motion-planning constraints during end-to-end training, is straightforward to implement, and provides both interval-based and Monte-Carlo-based uncertainty estimates. We conduct experiments using different environments and robots, comparing against baseline approaches based on the feasibility of the planned paths, and obstacle avoidance quality. Results show that our proposed approach yields a good balance between success rates and path quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas C. Cosier",
      "Rares Iordan",
      "Sicelukwanda N. T. Zwane",
      "Giovanni Franzese",
      "James T. Wilson",
      "Marc Deisenroth",
      "Alexander Terenin",
      "Yasemin Bekiroglu"
    ]
  },
  "https://proceedings.mlr.press/v238/benard24a.html": {
    "title": "MMD-based Variable Importance for Distributional Random Forest",
    "volume": "main",
    "abstract": "Distributional Random Forest (DRF) is a flexible forest-based method to estimate the full conditional distribution of a multivariate output of interest given input variables. In this article, we introduce a variable importance algorithm for DRFs, based on the well-established drop and relearn principle and MMD distance. While traditional importance measures only detect variables with an influence on the output mean, our algorithm detects variables impacting the output distribution more generally. We show that the introduced importance measure is consistent, exhibits high empirical performance on both real and simulated data, and outperforms competitors. In particular, our algorithm is highly efficient to select variables through recursive feature elimination, and can therefore provide small sets of variables to build accurate estimates of conditional output distributions",
    "checked": true,
    "id": "944442583f0bb86699b07101863c103516cc5297",
    "semantic_title": "mmd-based variable importance for distributional random forest",
    "citation_count": 0,
    "authors": [
      "Clément Bénard",
      "Jeffrey Näf",
      "Julie Josse"
    ]
  },
  "https://proceedings.mlr.press/v238/tebbe24a.html": {
    "title": "Efficiently Computable Safety Bounds for Gaussian Processes in Active Learning",
    "volume": "main",
    "abstract": "Active learning of physical systems must commonly respect practical safety constraints, which restricts the exploration of the design space. Gaussian Processes (GPs) and their calibrated uncertainty estimations are widely used for this purpose. In many technical applications the design space is explored via continuous trajectories, along which the safety needs to be assessed. This is particularly challenging for strict safety requirements in GP methods, as it employs computationally expensive Monte Carlo sampling of high quantiles. We address these challenges by providing provable safety bounds based on the adaptively sampled median of the supremum of the posterior GP. Our method significantly reduces the number of samples required for estimating high safety probabilities, resulting in faster evaluation without sacrificing accuracy and exploration speed. The effectiveness of our safe active learning approach is demonstrated through extensive simulations and validated using a real-world engine example",
    "checked": true,
    "id": "54fe70c363ca7fccc24ecc528959f4f3c0577e07",
    "semantic_title": "efficiently computable safety bounds for gaussian processes in active learning",
    "citation_count": 2,
    "authors": [
      "Jörn Tebbe",
      "Christoph Zimmer",
      "Ansgar Steland",
      "Markus Lange-Hegermann",
      "Fabian Mies"
    ]
  },
  "https://proceedings.mlr.press/v238/molaei24a.html": {
    "title": "Federated Learning For Heterogeneous Electronic Health Records Utilising Augmented Temporal Graph Attention Networks",
    "volume": "main",
    "abstract": "The proliferation of decentralised electronic healthcare records (EHRs) across medical institutions requires innovative federated learning strategies for collaborative data analysis and global model training, prioritising data privacy. A prevalent issue during decentralised model training is the data-view discrepancies across medical institutions that arises from differences or availability of healthcare services, such as blood test panels. The prevailing way to handle this issue is to select a common subset of features across institutions to make data-views consistent. This approach, however, constrains some institutions to shed some critical features that may play a significant role in improving the model performance. This paper introduces a federated learning framework that relies on augmented graph attention networks to address data-view heterogeneity. The proposed framework utilises an alignment augmentation layer over self-attention mechanisms to weigh the importance of neighbouring nodes when updating a node's embedding irrespective of the data-views. Furthermore, our framework adeptly addresses both the temporal nuances and structural intricacies of EHR datasets. This dual capability not only offers deeper insights but also effectively encapsulates EHR graphs' time-evolving nature. Using diverse real-world datasets, we show that the proposed framework significantly outperforms conventional FL methodology for dealing with heterogeneous data-views",
    "checked": true,
    "id": "0b89aa4353a23ba1a86a37be4623f7ad04330cea",
    "semantic_title": "federated learning for heterogeneous electronic health records utilising augmented temporal graph attention networks",
    "citation_count": 0,
    "authors": [
      "Soheila Molaei",
      "Anshul Thakur",
      "Ghazaleh Niknam",
      "Andrew Soltan",
      "Hadi Zare",
      "David A Clifton"
    ]
  },
  "https://proceedings.mlr.press/v238/hickey24a.html": {
    "title": "Adaptive Discretization for Event PredicTion (ADEPT)",
    "volume": "main",
    "abstract": "Recently developed survival analysis methods improve upon existing approaches by predicting the probability of event occurrence in each of a number pre-specified (discrete) time intervals. By avoiding placing strong parametric assumptions on the event density, this approach tends to improve prediction performance, particularly when data are plentiful. However, in clinical settings with limited available data, it is often preferable to judiciously partition the event time space into a limited number of intervals well suited to the prediction task at hand. In this work, we develop Adaptive Discretization for Event PredicTion (ADEPT) to learn from data a set of cut points defining such a partition. We show that in two simulated datasets, we are able to recover intervals that match the underlying generative model. We then demonstrate improved prediction performance on three real-world observational datasets, including a large, newly harmonized stroke risk prediction dataset. Finally, we argue that our approach facilitates clinical decision-making by suggesting time intervals that are most appropriate for each task, in the sense that they facilitate more accurate risk prediction",
    "checked": true,
    "id": "a1091dd548dc50cc5c92aa262267505151ebcf94",
    "semantic_title": "adaptive discretization for event prediction (adept)",
    "citation_count": 0,
    "authors": [
      "Jimmy Hickey",
      "Ricardo Henao",
      "Daniel Wojdyla",
      "Michael Pencina",
      "Matthew Engelhard"
    ]
  },
  "https://proceedings.mlr.press/v238/eun-huh24a.html": {
    "title": "Generalization Bounds for Label Noise Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "We develop generalization error bounds for stochastic gradient descent (SGD) with label noise in non-convex settings under uniform dissipativity and smoothness conditions. Under a suitable choice of semimetric, we establish a contraction in Wasserstein distance of the label noise stochastic gradient flow that depends polynomially on the parameter dimension $d$. Using the framework of algorithmic stability, we derive time-independent generalisation error bounds for the discretized algorithm with a constant learning rate. The error bound we achieve scales polynomially with $d$ and with the rate of $n^{-2/3}$, where $n$ is the sample size. This rate is better than the best-known rate of $n^{-1/2}$ established for stochastic gradient Langevin dynamics (SGLD)—which employs parameter-independent Gaussian noise—under similar conditions. Our analysis offers quantitative insights into the effect of label noise",
    "checked": true,
    "id": "ff3549609627a6d26ad461aa01c449f0cbc6820a",
    "semantic_title": "generalization bounds for label noise stochastic gradient descent",
    "citation_count": 0,
    "authors": [
      "Jung Eun Huh",
      "Patrick Rebeschini"
    ]
  },
  "https://proceedings.mlr.press/v238/heidari24a.html": {
    "title": "Adaptive Parametric Prototype Learning for Cross-Domain Few-Shot Classification",
    "volume": "main",
    "abstract": "Cross-domain few-shot classification induces a much more challenging problem than its in-domain counterpart due to the existence of domain shifts between the training and test tasks. In this paper, we develop a novel Adaptive Parametric Prototype Learning (APPL) method under the meta-learning convention for cross-domain few-shot classification. Different from existing prototypical few-shot methods that use the averages of support instances to calculate the class prototypes, we propose to learn class prototypes from the concatenated features of the support set in a parametric fashion and meta-learn the model by enforcing prototype-based regularization on the query set. In addition, we fine-tune the model in the target domain in a transductive manner using a weighted-moving-average self-training approach on the query instances. We conduct experiments on multiple cross-domain few-shot benchmark datasets. The empirical results demonstrate that APPL yields superior performance to many state-of-the-art cross-domain few-shot learning methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marzi Heidari",
      "Abdullah Alchihabi",
      "Qing En",
      "Yuhong Guo"
    ]
  },
  "https://proceedings.mlr.press/v238/khan24a.html": {
    "title": "Analyzing Explainer Robustness via Probabilistic Lipschitzness of Prediction Functions",
    "volume": "main",
    "abstract": "Machine learning methods have significantly improved in their predictive capabilities, but at the same time they are becoming more complex and less transparent. As a result, explainers are often relied on to provide interpretability to these black-box prediction models. As crucial diagnostics tools, it is important that these explainers themselves are robust. In this paper we focus on one particular aspect of robustness, namely that an explainer should give similar explanations for similar data inputs. We formalize this notion by introducing and defining explainer astuteness, analogous to astuteness of prediction functions. Our formalism allows us to connect explainer robustness to the predictor's probabilistic Lipschitzness, which captures the probability of local smoothness of a function. We provide lower bound guarantees on the astuteness of a variety of explainers (e.g., SHAP, RISE, CXPlain) given the Lipschitzness of the prediction function. These theoretical results imply that locally smooth prediction functions lend themselves to locally robust explanations. We evaluate these results empirically on simulated as well as real datasets",
    "checked": true,
    "id": "3e39f21182f9fb3fc2423639a30a05998ce06b46",
    "semantic_title": "analyzing explainer robustness via probabilistic lipschitzness of prediction functions",
    "citation_count": 2,
    "authors": [
      "Zulqarnain Q. Khan",
      "Davin Hill",
      "Aria Masoomi",
      "Joshua T. Bone",
      "Jennifer Dy"
    ]
  },
  "https://proceedings.mlr.press/v238/phan24a.html": {
    "title": "Importance Matching Lemma for Lossy Compression with Side Information",
    "volume": "main",
    "abstract": "We propose two extensions to existing importance sampling based methods for lossy compression. First, we introduce an importance sampling based compression scheme that is a variant of ordered random coding (Theis and Ahmed, 2022) and is amenable to direct evaluation of the achievable compression rate for a finite number of samples. Our second and major contribution is the \\emph{importance matching lemma}, which is a finite proposal counterpart of the recently introduced {Poisson matching lemma} (Li and Anantharam, 2021). By integrating with deep learning, we provide a new coding scheme for distributed lossy compression with side information at the decoder. We demonstrate the effectiveness of the proposed scheme through experiments involving synthetic Gaussian sources, distributed image compression with MNIST and vertical federated learning with CIFAR-10",
    "checked": true,
    "id": "6ee037cee4fe00e78926f83a66cd7f75be1aeed5",
    "semantic_title": "importance matching lemma for lossy compression with side information",
    "citation_count": 4,
    "authors": [
      "Buu Phan",
      "Ashish Khisti",
      "Christos Louizos"
    ]
  },
  "https://proceedings.mlr.press/v238/donhauser24a.html": {
    "title": "Certified private data release for sparse Lipschitz functions",
    "volume": "main",
    "abstract": "As machine learning has become more relevant for everyday applications, a natural requirement is the protection of the privacy of the training data. When the relevant learning questions are unknown in advance, or hyper-parameter tuning plays a central role, one solution is to release a differentially private synthetic data set that leads to similar conclusions as the original training data. In this work, we introduce an algorithm that enjoys fast rates for the utility loss for sparse Lipschitz queries. Furthermore, we show how to obtain a certificate for the utility loss for a large class of algorithms",
    "checked": true,
    "id": "d945ceab4084449387e30d59d0f86fcadef47d90",
    "semantic_title": "certified private data release for sparse lipschitz functions",
    "citation_count": 2,
    "authors": [
      "Konstantin Donhauser",
      "Johan Lokna",
      "Amartya Sanyal",
      "March Boedihardjo",
      "Robert Hönig",
      "Fanny Yang"
    ]
  },
  "https://proceedings.mlr.press/v238/trauger24a.html": {
    "title": "Sequence Length Independent Norm-Based Generalization Bounds for Transformers",
    "volume": "main",
    "abstract": "This paper provides norm-based generalization bounds for the Transformer architecture that do not depend on the input sequence length. We employ a covering number based approach to prove our bounds. We use three novel covering number bounds for the function class of bounded linear mappings to upper bound the Rademacher complexity of the Transformer. Furthermore, we show this generalization bound applies to the common Transformer training technique of masking and then predicting the masked word. We also run a simulated study on a sparse majority data set that empirically validates our theoretical findings",
    "checked": true,
    "id": "a51bd29d341ea8944f030a337beeb97f1ac230cb",
    "semantic_title": "sequence length independent norm-based generalization bounds for transformers",
    "citation_count": 4,
    "authors": [
      "Jacob Trauger",
      "Ambuj Tewari"
    ]
  },
  "https://proceedings.mlr.press/v238/jin24a.html": {
    "title": "Subsampling Error in Stochastic Gradient Langevin Diffusions",
    "volume": "main",
    "abstract": "The Stochastic Gradient Langevin Dynamics (SGLD) are popularly used to approximate Bayesian posterior distributions in statistical learning procedures with large-scale data. As opposed to many usual Markov chain Monte Carlo (MCMC) algorithms, SGLD is not stationary with respect to the posterior distribution; two sources of error appear: The first error is introduced by an Euler–Maruyama discretisation of a Langevin diffusion process, the second error comes from the data subsampling that enables its use in large-scale data settings. In this work, we consider an idealised version of SGLD to analyse the method's pure subsampling error that we then see as a best-case error for diffusion-based subsampling MCMC methods. Indeed, we introduce and study the Stochastic Gradient Langevin Diffusion (SGLDiff), a continuous-time Markov process that follows the Langevin diffusion corresponding to a data subset and switches this data subset after exponential waiting times. There, we show the exponential ergodicity of SLGDiff and that the Wasserstein distance between the posterior and the limiting distribution of SGLDiff is bounded above by a fractional power of the mean waiting time. We bring our results into context with other analyses of SGLD",
    "checked": true,
    "id": "554b0f1b0822cf13a572202a36862c506eba22ba",
    "semantic_title": "subsampling error in stochastic gradient langevin diffusions",
    "citation_count": 0,
    "authors": [
      "Kexin Jin",
      "Chenguang Liu",
      "Jonas Latz"
    ]
  },
  "https://proceedings.mlr.press/v238/vu24a.html": {
    "title": "Analysis of Privacy Leakage in Federated Large Language Models",
    "volume": "main",
    "abstract": "With the rapid adoption of Federated Learning (FL) as the training and tuning protocol for applications utilizing Large Language Models (LLMs), recent research highlights the need for significant modifications to FL to accommodate the large-scale of LLMs. While substantial adjustments to the protocol have been introduced as a response, comprehensive privacy analysis for the adapted FL protocol is currently lacking. To address this gap, our work delves into an extensive examination of the privacy analysis of FL when used for training LLMs, both from theoretical and practical perspectives. In particular, we design two active membership inference attacks with guaranteed theoretical success rates to assess the privacy leakages of various adapted FL configurations. Our theoretical findings are translated into practical attacks, revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's GPTs, across multiple real-world language datasets. Additionally, we conduct thorough experiments to evaluate the privacy leakage of these models when data is protected by state-of-the-art differential privacy (DP) mechanisms",
    "checked": true,
    "id": "4f5e020ca9ad8339f1f2026e9a93f1a70da324e2",
    "semantic_title": "analysis of privacy leakage in federated large language models",
    "citation_count": 2,
    "authors": [
      "Minh Vu",
      "Truc Nguyen",
      "Tre’ Jeter",
      "My T. Thai"
    ]
  },
  "https://proceedings.mlr.press/v238/qian24a.html": {
    "title": "Exploring the Power of Graph Neural Networks in Solving Linear Optimization Problems",
    "volume": "main",
    "abstract": "Recently, machine learning, particularly message-passing graph neural networks (MPNNs), has gained traction in enhancing exact optimization algorithms. For example, MPNNs speed up solving mixed-integer optimization problems by imitating computational intensive heuristics like strong branching, which entails solving multiple linear optimization problems (LPs). Despite the empirical success, the reasons behind MPNNs' effectiveness in emulating linear optimization remain largely unclear. Here, we show that MPNNs can simulate standard interior-point methods for LPs, explaining their practical success. Furthermore, we highlight how MPNNs can serve as a lightweight proxy for solving LPs, adapting to a given problem instance distribution. Empirically, we show that MPNNs solve LP relaxations of standard combinatorial optimization problems close to optimality, often surpassing conventional solvers and competing approaches in solving time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chendi Qian",
      "Didier Chételat",
      "Christopher Morris"
    ]
  },
  "https://proceedings.mlr.press/v238/en24a.html": {
    "title": "Cross-model Mutual Learning for Exemplar-based Medical Image Segmentation",
    "volume": "main",
    "abstract": "Medical image segmentation typically demands extensive dense annotations for model training, which is both time-consuming and skill-intensive. To mitigate this burden, exemplar-based medical image segmentation methods have been introduced to achieve effective training with only one annotated image. In this paper, we introduce a novel Cross-model Mutual learning framework for Exemplar-based Medical image Segmentation (CMEMS), which leverages two models to mutually excavate implicit information from unlabeled data at multiple granularities. CMEMS can eliminate confirmation bias and enable collaborative training to learn complementary information by enforcing consistency at different granularities across models. Concretely, cross-model image perturbation based mutual learning is devised by using weakly perturbed images to generate high-confidence pseudo-labels, supervising predictions of strongly perturbed images across models. This approach enables joint pursuit of prediction consistency at the image granularity. Moreover, cross-model multi-level feature perturbation based mutual learning is designed by letting pseudo-labels supervise predictions from perturbed multi-level features with different resolutions, which can broaden the perturbation space and enhance the robustness of our framework. CMEMS is jointly trained using exemplar data, synthetic data, and unlabeled data in an end-to-end manner. Experimental results on two medical image datasets indicate that the proposed CMEMS outperforms the state-of-the-art segmentation methods with extremely limited supervision",
    "checked": true,
    "id": "5f4257d65c727ab3e5d5741a0756318585a10abf",
    "semantic_title": "cross-model mutual learning for exemplar-based medical image segmentation",
    "citation_count": 0,
    "authors": [
      "Qing En",
      "Yuhong Guo"
    ]
  },
  "https://proceedings.mlr.press/v238/deshpande24a.html": {
    "title": "Online Calibrated and Conformal Prediction Improves Bayesian Optimization",
    "volume": "main",
    "abstract": "Accurate uncertainty estimates are important in sequential model-based decision-making tasks such as Bayesian optimization. However, these estimates can be imperfect if the data violates assumptions made by the model (e.g., Gaussianity). This paper studies which uncertainties are needed in model-based decision-making and in Bayesian optimization, and argues that uncertainties can benefit from calibration—i.e., an 80% predictive interval should contain the true outcome 80% of the time. Maintaining calibration, however, can be challenging when the data is non-stationary and depends on our actions. We propose using simple algorithms based on online learning to provably maintain calibration on non-i.i.d. data, and we show how to integrate these algorithms in Bayesian optimization with minimal overhead. Empirically, we find that calibrated Bayesian optimization converges to better optima in fewer steps, and we demonstrate improved performance on standard benchmark functions and hyperparameter optimization tasks",
    "checked": false,
    "id": "9872cc5150ecccafb4406b549d1652552b50527a",
    "semantic_title": "calibrated regression against an adversary without regret",
    "citation_count": 0,
    "authors": [
      "Shachi Deshpande",
      "Charles Marx",
      "Volodymyr Kuleshov"
    ]
  },
  "https://proceedings.mlr.press/v238/kausik24a.html": {
    "title": "Offline Policy Evaluation and Optimization Under Confounding",
    "volume": "main",
    "abstract": "Evaluating and optimizing policies in the presence of unobserved confounders is a problem of growing interest in offline reinforcement learning. Using conventional methods for offline RL in the presence of confounding can not only lead to poor decisions and poor policies, but also have disastrous effects in critical applications such as healthcare and education. We map out the landscape of offline policy evaluation for confounded MDPs, distinguishing assumptions on confounding based on whether they are memoryless and on their effect on the data-collection policies. We characterize settings where consistent value estimates are provably not achievable, and provide algorithms with guarantees to instead estimate lower bounds on the value. When consistent estimates are achievable, we provide algorithms for value estimation with sample complexity guarantees. We also present new algorithms for offline policy improvement and prove local convergence guarantees. Finally, we experimentally evaluate our algorithms on both a gridworld environment and a simulated healthcare setting of managing sepsis patients. In gridworld, our model-based method provides tighter lower bounds than existing methods, while in the sepsis simulator, we demonstrate the effectiveness of our method and investigate the importance of a clustering sub-routine",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chinmaya Kausik",
      "Yangyi Lu",
      "Kevin Tan",
      "Maggie Makar",
      "Yixin Wang",
      "Ambuj Tewari"
    ]
  },
  "https://proceedings.mlr.press/v238/neuhof24a.html": {
    "title": "Confident Feature Ranking",
    "volume": "main",
    "abstract": "Machine learning models are widely applied in various fields. Stakeholders often use post-hoc feature importance methods to better understand the input features' contribution to the models' predictions. The interpretation of the importance values provided by these methods is frequently based on the relative order of the features (their ranking) rather than the importance values themselves. Since the order may be unstable, we present a framework for quantifying the uncertainty in global importance values. We propose a novel method for the post-hoc interpretation of feature importance values that is based on the framework and pairwise comparisons of the feature importance values. This method produces simultaneous confidence intervals for the features' ranks, which include the \"true\" (infinite sample) ranks with high probability, and enables the selection of the set of top-k important features",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bitya Neuhof",
      "Yuval Benjamini"
    ]
  },
  "https://proceedings.mlr.press/v238/hu24b.html": {
    "title": "Central Limit Theorem for Two-Timescale Stochastic Approximation with Markovian Noise: Theory and Applications",
    "volume": "main",
    "abstract": "Two-timescale stochastic approximation (TTSA) is among the most general frameworks for iterative stochastic algorithms. This includes well-known stochastic optimization methods such as SGD variants and those designed for bilevel or minimax problems, as well as reinforcement learning like the family of gradient-based temporal difference (GTD) algorithms. In this paper, we conduct an in-depth asymptotic analysis of TTSA under controlled Markovian noise via central limit theorem (CLT), uncovering the coupled dynamics of TTSA influenced by the underlying Markov chain, which has not been addressed by previous CLT results of TTSA only with Martingale difference noise. Building upon our CLT, we expand its application horizon of efficient sampling strategies from vanilla SGD to a wider TTSA context in distributed learning, thus broadening the scope of Hu et al. 2020. In addition, we leverage our CLT result to deduce the statistical properties of GTD algorithms with nonlinear function approximation using Markovian samples and show their identical asymptotic performance, a perspective not evident from current finite-time bounds",
    "checked": true,
    "id": "6852bb068066549b37e477ea0b4fcccdbd8a21ed",
    "semantic_title": "central limit theorem for two-timescale stochastic approximation with markovian noise: theory and applications",
    "citation_count": 2,
    "authors": [
      "Jie Hu",
      "Vishwaraj Doshi",
      "Do Young Eun"
    ]
  },
  "https://proceedings.mlr.press/v238/vishwakarma24a.html": {
    "title": "Taming False Positives in Out-of-Distribution Detection with Human Feedback",
    "volume": "main",
    "abstract": "Robustness to out-of-distribution (OOD) samples is crucial for the safe deployment of machine learning models in the open world. Recent works have focused on designing scoring functions to quantify OOD uncertainty. Setting appropriate thresholds for these scoring functions for OOD detection is challenging as OOD samples are often unavailable up front. Typically, thresholds are set to achieve a desired true positive rate (TPR), e.g., $95%$ TPR. However, this can lead to very high false positive rates (FPR), ranging from 60 to 96%, as observed in the Open-OOD benchmark. In safety critical real-life applications, e.g., medical diagnosis, controlling the FPR is essential when dealing with various OOD samples dynamically. To address these challenges, we propose a mathematically grounded OOD detection framework that leverages expert feedback to \\emph{safely} update the threshold on the fly. We provide theoretical results showing that it is guaranteed to meet the FPR constraint at all times while minimizing the use of human feedback. Another key feature of our framework is that it can work with any scoring function for OOD uncertainty quantification. Empirical evaluation of our system on synthetic and benchmark OOD datasets shows that our method can maintain FPR at most $5%$ while maximizing TPR",
    "checked": true,
    "id": "b3f21af3032246b6fa87e05a6d9455433b25ce55",
    "semantic_title": "taming false positives in out-of-distribution detection with human feedback",
    "citation_count": 0,
    "authors": [
      "Harit Vishwakarma",
      "Heguang Lin",
      "Ramya Korlakai Vinayak"
    ]
  },
  "https://proceedings.mlr.press/v238/lebensold24a.html": {
    "title": "On the Privacy of Selection Mechanisms with Gaussian Noise",
    "volume": "main",
    "abstract": "Report Noisy Max and Above Threshold are two classical differentially private (DP) selection mechanisms. Their output is obtained by adding noise to a sequence of low-sensitivity queries and reporting the identity of the query whose (noisy) answer satisfies a certain condition. Pure DP guarantees for these mechanisms are easy to obtain when Laplace noise is added to the queries. On the other hand, when instantiated using Gaussian noise, standard analyses only yield approximate DP guarantees despite the fact that the outputs of these mechanisms lie in a discrete space. In this work, we revisit the analysis of Report Noisy Max and Above Threshold with Gaussian noise and show that, under the additional assumption that the underlying queries are bounded, it is possible to provide pure ex-ante DP bounds for Report Noisy Max and pure ex-post DP bounds for Above Threshold. The resulting bounds are tight and depend on closed-form expressions that can be numerically evaluated using standard methods. Empirically we find these lead to tighter privacy accounting in the high privacy, low data regime. Further, we propose a simple privacy filter for composing pure ex-post DP guarantees, and use it to derive a fully adaptive Gaussian Sparse Vector Technique mechanism. Finally, we provide experiments on mobility and energy consumption datasets demonstrating that our Sparse Vector Technique is practically competitive with previous approaches and requires less hyper-parameter tuning",
    "checked": true,
    "id": "843fbf43f0fdf1378d423542239f3f2ef0d471e0",
    "semantic_title": "on the privacy of selection mechanisms with gaussian noise",
    "citation_count": 0,
    "authors": [
      "Jonathan Lebensold",
      "Doina Precup",
      "Borja Balle"
    ]
  },
  "https://proceedings.mlr.press/v238/gazin24a.html": {
    "title": "Transductive conformal inference with adaptive scores",
    "volume": "main",
    "abstract": "Conformal inference is a fundamental and versatile tool that provides distribution-free guarantees for many machine learning tasks. We consider the transductive setting, where decisions are made on a test sample of $m$ new points, giving rise to $m$ conformal $p$-values. While classical results only concern their marginal distribution, we show that their joint distribution follows a Pólya urn model, and establish a concentration inequality for their empirical distribution function. The results hold for arbitrary exchangeable scores, including adaptive ones that can use the covariates of the test${+}$calibration samples at training stage for increased accuracy. We demonstrate the usefulness of these theoretical results through uniform, in-probability guarantees for two machine learning tasks of current interest: interval prediction for transductive transfer learning and novelty detection based on two-class classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ulysse Gazin",
      "Gilles Blanchard",
      "Etienne Roquain"
    ]
  },
  "https://proceedings.mlr.press/v238/cohen-indelman24a.html": {
    "title": "Learning Latent Partial Matchings with Gumbel-IPF Networks",
    "volume": "main",
    "abstract": "Learning to match discrete objects has been a central task in machine learning, often facilitated by a continuous relaxation of the matching structure. However, practical problems entail partial matchings due to missing correspondences, which pose difficulties to the one-to-one matching learning techniques that dominate the state-of-the-art. This paper introduces Gumbel-IPF networks for learning latent partial matchings. At the core of our method is the differentiable Iterative Proportional Fitting (IPF) procedure that biproportionally projects onto the transportation polytope of target marginals. Our theoretical framework also allows drawing samples from the temperature-dependent partial matching distribution. We investigate the properties of common-practice relaxations through the lens of biproportional fitting and introduce a new metric, the empirical prediction shift. Our method's advantages are demonstrated in experimental results on the semantic keypoints partial matching task on the Pascal VOC, IMC-PT-SparseGM, and CUB2001 datasets",
    "checked": true,
    "id": "832888baa5a95a3329b0166bccaf0ac81433a06c",
    "semantic_title": "learning latent partial matchings with gumbel-ipf networks",
    "citation_count": 0,
    "authors": [
      "Hedda Cohen Indelman",
      "Tamir Hazan"
    ]
  },
  "https://proceedings.mlr.press/v238/wang24b.html": {
    "title": "On Counterfactual Metrics for Social Welfare: Incentives, Ranking, and Information Asymmetry",
    "volume": "main",
    "abstract": "From the social sciences to machine learning, it is well documented that metrics do not always align with social welfare. In healthcare, Dranove et al. (2003) showed that publishing surgery mortality metrics actually harmed sicker patients by increasing provider selection behavior. Using a principal-agent model, we analyze the incentive misalignments that arise from such average treated outcome metrics, and show that the incentives driving treatment decisions would align with maximizing total patient welfare if the metrics (i) accounted for counterfactual untreated outcomes and (ii) considered total welfare instead of averaging over treated patients. Operationalizing this, we show how counterfactual metrics can be modified to behave reasonably in patient-facing ranking systems. Extending to realistic settings when providers observe more about patients than the regulatory agencies do, we bound the decay in performance by the degree of information asymmetry between principal and agent. In doing so, our model connects principal-agent information asymmetry with unobserved heterogeneity in causal inference",
    "checked": true,
    "id": "a34b3742aab9a03aae4fdb8cff23247bbc4cff4e",
    "semantic_title": "on counterfactual metrics for social welfare: incentives, ranking, and information asymmetry",
    "citation_count": 1,
    "authors": [
      "Serena Wang",
      "Stephen Bates",
      "P Aronow",
      "Michael Jordan"
    ]
  },
  "https://proceedings.mlr.press/v238/dann24a.html": {
    "title": "Data-Driven Online Model Selection With Regret Guarantees",
    "volume": "main",
    "abstract": "We consider model selection for sequential decision making in stochastic environments with bandit feedback, where a meta-learner has at its disposal a pool of base learners, and decides on the fly which action to take based on the policies recommended by each base learner. Model selection is performed by regret balancing but, unlike the recent literature on this subject, we do not assume any prior knowledge about the base learners like candidate regret guarantees; instead, we uncover these quantities in a data-driven manner. The meta-learner is therefore able to leverage the *realized* regret incurred by each base learner for the learning environment at hand (as opposed to the *expected* regret), and single out the best such regret. We design two model selection algorithms operating with this more ambitious notion of regret and, besides proving model selection guarantees via regret balancing, we experimentally demonstrate the compelling practical benefits of dealing with actual regrets instead of candidate regret bounds",
    "checked": true,
    "id": "1c322b9c3a5ccc4964591a6f40cf593ad4e3a96c",
    "semantic_title": "data-driven online model selection with regret guarantees",
    "citation_count": 2,
    "authors": [
      "Chris Dann",
      "Claudio Gentile",
      "Aldo Pacchiano"
    ]
  },
  "https://proceedings.mlr.press/v238/rossellini24a.html": {
    "title": "Integrating Uncertainty Awareness into Conformalized Quantile Regression",
    "volume": "main",
    "abstract": "Conformalized Quantile Regression (CQR) is a recently proposed method for constructing prediction intervals for a response $Y$ given covariates $X$, without making distributional assumptions. However, existing constructions of CQR can be ineffective for problems where the quantile regressors perform better in certain parts of the feature space than others. The reason is that the prediction intervals of CQR do not distinguish between two forms of uncertainty: first, the variability of the conditional distribution of $Y$ given $X$ (i.e., aleatoric uncertainty), and second, our uncertainty in estimating this conditional distribution (i.e., epistemic uncertainty). This can lead to intervals that are overly narrow in regions where epistemic uncertainty is high. To address this, we propose a new variant of the CQR methodology, Uncertainty-Aware CQR (UACQR), that explicitly separates these two sources of uncertainty to adjust quantile regressors differentially across the feature space. Compared to CQR, our methods enjoy the same distribution-free theoretical coverage guarantees, while demonstrating in our experiments stronger conditional coverage properties in simulated settings and real-world data sets alike",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raphael Rossellini",
      "Rina Foygel Barber",
      "Rebecca Willett"
    ]
  },
  "https://proceedings.mlr.press/v238/dhillon24a.html": {
    "title": "On the Expected Size of Conformal Prediction Sets",
    "volume": "main",
    "abstract": "While conformal predictors reap the benefits of rigorous statistical guarantees on their error frequency, the size of their corresponding prediction sets is critical to their practical utility. Unfortunately, there is currently a lack of finite-sample analysis and guarantees for their prediction set sizes. To address this shortfall, we theoretically quantify the expected size of the prediction sets under the split conformal prediction framework. As this precise formulation cannot usually be calculated directly, we further derive point estimates and high-probability interval bounds that can be empirically computed, providing a practical method for characterizing the expected set size. We corroborate the efficacy of our results with experiments on real-world datasets for both regression and classification problems",
    "checked": true,
    "id": "b0886865ca3275b21a1aee37f9898f416a18a4bc",
    "semantic_title": "on the expected size of conformal prediction sets",
    "citation_count": 5,
    "authors": [
      "Guneet S. Dhillon",
      "George Deligiannidis",
      "Tom Rainforth"
    ]
  },
  "https://proceedings.mlr.press/v238/sevilla24a.html": {
    "title": "Estimation of partially known Gaussian graphical models with score-based structural priors",
    "volume": "main",
    "abstract": "We propose a novel algorithm for the support estimation of partially known Gaussian graphical models that incorporates prior information about the underlying graph. In contrast to classical approaches that provide a point estimate based on a maximum likelihood or maximum a posteriori approach using (simple) priors on the precision matrix, we consider a prior on the graph and rely on annealed Langevin diffusion to generate samples from the posterior distribution. Since the Langevin sampler requires access to the score function of the underlying graph prior, we use graph neural networks to effectively estimate the score from a graph dataset (either available beforehand or generated from a known distribution). Numerical experiments in different setups demonstrate the benefits of our approach",
    "checked": true,
    "id": "5b65ce36a5988b687c9a123c0d0b009d9f770b53",
    "semantic_title": "estimation of partially known gaussian graphical models with score-based structural priors",
    "citation_count": 2,
    "authors": [
      "Martín Sevilla",
      "Antonio G. Marques",
      "Santiago Segarra"
    ]
  },
  "https://proceedings.mlr.press/v238/takemori24a.html": {
    "title": "Model-Based Best Arm Identification for Decreasing Bandits",
    "volume": "main",
    "abstract": "We study the problem of reliably identifying the best (lowest loss) arm in a stochastic multi-armed bandit when the expected loss of each arm is monotone decreasing as a function of its pull count. This models, for instance, scenarios where each arm itself represents an optimization algorithm for finding the minimizer of a common function, and there is a limited time available to test the algorithms before committing to one of them. We assume that the decreasing expected loss of each arm depends on the number of its pulls as a (inverse) polynomial with unknown coefficients. We propose two fixed-budget best arm identification algorithms – one for the case of sparse polynomial decay models and the other for general polynomial models – along with bounds on the identification error probability. We also derive algorithm-independent lower bounds on the error probability. These bounds are seen to be factored into the product of the usual problem complexity and the model complexity that only depends on the parameters of the model. This indicates that our methods can identify the best arm even when the budget is smaller. We conduct empirical studies of our algorithms to complement our theoretical findings",
    "checked": true,
    "id": "f5e40367b3892693f991e0d6914367089f85e09f",
    "semantic_title": "model-based best arm identification for decreasing bandits",
    "citation_count": 0,
    "authors": [
      "Sho Takemori",
      "Yuhei Umeda",
      "Aditya Gopalan"
    ]
  },
  "https://proceedings.mlr.press/v238/ou24a.html": {
    "title": "Thompson Sampling Itself is Differentially Private",
    "volume": "main",
    "abstract": "In this work we first show that the classical Thompson sampling algorithm for multi-arm bandits is differentially private as-is, without any modification. We provide per-round privacy guarantees as a function of problem parameters and show composition over $T$ rounds; since the algorithm is unchanged, existing $O(\\sqrt{NT\\log N})$ regret bounds still hold and there is no loss in performance due to privacy. We then show that simple modifications – such as pre-pulling all arms a fixed number of times, increasing the sampling variance – can provide tighter privacy guarantees. We again provide privacy guarantees that now depend on the new parameters introduced in the modification, which allows the analyst to tune the privacy guarantee as desired. We also provide a novel regret analysis for this new algorithm, and show how the new parameters also impact expected regret. Finally, we empirically validate and illustrate our theoretical findings in two parameter regimes and demonstrate that tuning the new parameters substantially improve the privacy-regret tradeoff",
    "checked": true,
    "id": "2681883f50833e16b896716d17adced220f73a44",
    "semantic_title": "thompson sampling itself is differentially private",
    "citation_count": 0,
    "authors": [
      "Tingting Ou",
      "Rachel Cummings",
      "Marco Avella Medina"
    ]
  },
  "https://proceedings.mlr.press/v238/xiong24a.html": {
    "title": "A/B Testing and Best-arm Identification for Linear Bandits with Robustness to Non-stationarity",
    "volume": "main",
    "abstract": "We investigate the fixed-budget best-arm identification (BAI) problem for linear bandits in a potentially non-stationary environment. Given a finite arm set $\\mathcal{X}\\subset\\mathbb{R}^d$, a fixed budget $T$, and an unpredictable sequence of parameters $\\left\\lbrace\\theta_t\\right\\rbrace_{t=1}^{T}$, an algorithm will aim to correctly identify the best arm $x^* := \\arg\\max_{x\\in\\mathcal{X}}x^\\top\\sum_{t=1}^{T}\\theta_t$ with probability as high as possible. Prior work has addressed the stationary setting where $\\theta_t = \\theta_1$ for all $t$ and demonstrated that the error probability decreases as $\\exp(-T /\\rho^*)$ for a problem-dependent constant $\\rho^*$. But in many real-world $A/B/n$ multivariate testing scenarios that motivate our work, the environment is non-stationary and an algorithm expecting a stationary setting can easily fail. For robust identification, it is well-known that if arms are chosen randomly and non-adaptively from a G-optimal design over $\\mathcal{X}$ at each time then the error probability decreases as $\\exp(-T\\Delta^2_{(1)}/d)$, where $\\Delta_{(1)} = \\min_{x \\neq x^*} (x^* - x)^\\top \\frac{1}{T}\\sum_{t=1}^T \\theta_t$. As there exist environments where $\\Delta_{(1)}^2/ d \\ll 1/ \\rho^*$, we are motivated to propose a novel algorithm P1-RAGE that aims to obtain the best of both worlds: robustness to non-stationarity and fast rates of identification in benign settings. We characterize the error probability of P1-RAGE and demonstrate empirically that the algorithm indeed never performs worse than G-optimal design but compares favorably to the best algorithms in the stationary setting",
    "checked": true,
    "id": "6bf5c5a62a984e112b331dc1b538682656899c5d",
    "semantic_title": "a/b testing and best-arm identification for linear bandits with robustness to non-stationarity",
    "citation_count": 0,
    "authors": [
      "Zhihan Xiong",
      "Romain Camilleri",
      "Maryam Fazel",
      "Lalit Jain",
      "Kevin Jamieson"
    ]
  },
  "https://proceedings.mlr.press/v238/sohn24a.html": {
    "title": "Fair Supervised Learning with A Simple Random Sampler of Sensitive Attributes",
    "volume": "main",
    "abstract": "As the data-driven decision process becomes dominating for industrial applications, fairness-aware machine learning arouses great attention in various areas. This work proposes fairness penalties learned by neural networks with a simple random sampler of sensitive attributes for non-discriminatory supervised learning. In contrast to many existing works that critically rely on the discreteness of sensitive attributes and response variables, the proposed penalty is able to handle versatile formats of the sensitive attributes, so it is more extensively applicable in practice than many existing algorithms. This penalty enables us to build a computationally efficient group-level in-processing fairness-aware training framework. Empirical evidence shows that our framework enjoys better utility and fairness measures on popular benchmark data sets than competing methods. We also theoretically characterize estimation errors and loss of utility of the proposed neural-penalized risk minimization problem",
    "checked": true,
    "id": "34c8fdbe9ecff8382135ea6118bad8054623c972",
    "semantic_title": "fair supervised learning with a simple random sampler of sensitive attributes",
    "citation_count": 0,
    "authors": [
      "Jinwon Sohn",
      "Qifan Song",
      "Guang Lin"
    ]
  },
  "https://proceedings.mlr.press/v238/ma24a.html": {
    "title": "Absence of spurious solutions far from ground truth: A low-rank analysis with high-order losses",
    "volume": "main",
    "abstract": "Matrix sensing problems exhibit pervasive non-convexity, plaguing optimization with a proliferation of suboptimal spurious solutions. Avoiding convergence to these critical points poses a major challenge. This work provides new theoretical insights that help demystify the intricacies of the non-convex landscape. In this work, we prove that under certain conditions, critical points sufficiently distant from the ground truth matrix exhibit favorable geometry by being strict saddle points rather than troublesome local minima. Moreover, we introduce the notion of higher-order losses for the matrix sensing problem and show that the incorporation of such losses into the objective function amplifies the negative curvature around those distant critical points. This implies that increasing the complexity of the objective function via high-order losses accelerates the escape from such critical points and acts as a desirable alternative to increasing the complexity of the optimization problem via over-parametrization. By elucidating key characteristics of the non-convex optimization landscape, this work makes progress towards a comprehensive framework for tackling broader machine learning objectives plagued by non-convexity",
    "checked": true,
    "id": "73d3081dd07a388fb7e736cd5816edbdcd160a26",
    "semantic_title": "absence of spurious solutions far from ground truth: a low-rank analysis with high-order losses",
    "citation_count": 0,
    "authors": [
      "Ziye Ma",
      "Ying Chen",
      "Javad Lavaei",
      "Somayeh Sojoudi"
    ]
  },
  "https://proceedings.mlr.press/v238/jhunjhunwala24a.html": {
    "title": "FedFisher: Leveraging Fisher Information for One-Shot Federated Learning",
    "volume": "main",
    "abstract": "Standard federated learning (FL) algorithms typically require multiple rounds of communication between the server and the clients, which has several drawbacks, including requiring constant network connectivity, repeated investment of computational resources, and susceptibility to privacy attacks. One-Shot FL is a new paradigm that aims to address this challenge by enabling the server to train a global model in a single round of communication. In this work, we present FedFisher, a novel algorithm for one-shot FL that makes use of Fisher information matrices computed on local client models, motivated by a Bayesian perspective of FL. First, we theoretically analyze FedFisher for two-layer over-parameterized ReLU neural networks and show that the error of our one-shot FedFisher global model becomes vanishingly small as the width of the neural networks and amount of local training at clients increases. Next, we propose practical variants of FedFisher using the diagonal Fisher and K-FAC approximation for the full Fisher and highlight their communication and compute efficiency for FL. Finally, we conduct extensive experiments on various datasets, which show that these variants of FedFisher consistently improve over competing baselines",
    "checked": true,
    "id": "94665ddaa6ca8b7fe6f19c392a72604c7396f1e8",
    "semantic_title": "fedfisher: leveraging fisher information for one-shot federated learning",
    "citation_count": 1,
    "authors": [
      "Divyansh Jhunjhunwala",
      "Shiqiang Wang",
      "Gauri Joshi"
    ]
  },
  "https://proceedings.mlr.press/v238/choo24a.html": {
    "title": "Causal Discovery under Off-Target Interventions",
    "volume": "main",
    "abstract": "Causal graph discovery is a significant problem with applications across various disciplines. However, with observational data alone, the underlying causal graph can only be recovered up to its Markov equivalence class, and further assumptions or interventions are necessary to narrow down the true graph. This work addresses the causal discovery problem under the setting of stochastic interventions with the natural goal of minimizing the number of interventions performed. We propose the following stochastic intervention model which subsumes existing adaptive noiseless interventions in the literature while capturing scenarios such as fat-hand interventions and CRISPR gene knockouts: any intervention attempt results in an actual intervention on a random subset of vertices, drawn from a \\emph{distribution dependent on attempted action}. Under this model, we study the two fundamental problems in causal discovery of verification and search and provide approximation algorithms with polylogarithmic competitive ratios and provide some preliminary experimental results",
    "checked": true,
    "id": "213230dc1e3bbdd09326ada1d7beea3104e5fb52",
    "semantic_title": "causal discovery under off-target interventions",
    "citation_count": 0,
    "authors": [
      "Davin Choo",
      "Kirankumar Shiragur",
      "Caroline Uhler"
    ]
  },
  "https://proceedings.mlr.press/v238/jin24b.html": {
    "title": "Feasible $Q$-Learning for Average Reward Reinforcement Learning",
    "volume": "main",
    "abstract": "Average reward reinforcement learning (RL) provides a suitable framework for capturing the objective (i.e. long-run average reward) for continuing tasks, where there is often no natural way to identify a discount factor. However, existing average reward RL algorithms with sample complexity guarantees are not feasible, as they take as input the (unknown) mixing time of the Markov decision process (MDP). In this paper, we make initial progress towards addressing this open problem. We design a feasible average-reward $Q$-learning framework that requires no knowledge of any problem parameter as input. Our framework is based on discounted $Q$-learning, while we dynamically adapt the discount factor (and hence the effective horizon) to progressively approximate the average reward. In the synchronous setting, we solve three tasks: (i) learn a policy that is $\\epsilon$-close to optimal, (ii) estimate optimal average reward with $\\epsilon$-accuracy, and (iii) estimate the bias function (similar to $Q$-function in discounted case) with $\\epsilon$-accuracy. We show that with carefully designed adaptation schemes, (i) can be achieved with $\\tilde{O}(\\frac{SA t_{\\mathrm{mix}}^{8}}{\\epsilon^{8}})$ samples, (ii) with $\\tilde{O}(\\frac{SA t_{\\mathrm{mix}}^5}{\\epsilon^5})$ samples, and (iii) with $\\tilde{O}(\\frac{SA B}{\\epsilon^9})$ samples, where $t_\\mathrm{mix}$ is the mixing time, and $B > 0$ is an MDP-dependent constant. To our knowledge, we provide the first finite-sample guarantees that are polynomial in $S, A, t_{\\mathrm{mix}}, \\epsilon$ for a feasible variant of $Q$-learning. That said, the sample complexity bounds have tremendous room for improvement, which we leave for the community's best minds. Preliminary simulations verify that our framework is effective without prior knowledge of parameters as input",
    "checked": false,
    "id": "d529590de080b688c646591a501862684ae061c1",
    "semantic_title": "feasible q-learning for average reward reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Ying Jin",
      "Ramki Gummadi",
      "Zhengyuan Zhou",
      "Jose Blanchet"
    ]
  },
  "https://proceedings.mlr.press/v238/wang24c.html": {
    "title": "Joint control variate for faster black-box variational inference",
    "volume": "main",
    "abstract": "Black-box variational inference performance is sometimes hindered by the use of gradient estimators with high variance. This variance comes from two sources of randomness: Data subsampling and Monte Carlo sampling. While existing control variates only address Monte Carlo noise, and incremental gradient methods typically only address data subsampling, we propose a new \"joint\" control variate that jointly reduces variance from both sources of noise. This significantly reduces gradient variance, leading to faster optimization in several applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Wang",
      "Tomas Geffner",
      "Justin Domke"
    ]
  },
  "https://proceedings.mlr.press/v238/tang24a.html": {
    "title": "Adaptivity of Diffusion Models to Manifold Structures",
    "volume": "main",
    "abstract": "Empirical studies have demonstrated the effectiveness of (score-based) diffusion models in generating high-dimensional data, such as texts and images, which typically exhibit a low-dimensional manifold nature. These empirical successes raise the theoretical question of whether score-based diffusion models can optimally adapt to low-dimensional manifold structures. While recent work has validated the minimax optimality of diffusion models when the target distribution admits a smooth density with respect to the Lebesgue measure of the ambient data space, these findings do not fully account for the ability of diffusion models in avoiding the the curse of dimensionality when estimating high-dimensional distributions. This work considers two common classes of diffusion models: Langevin diffusion and forward-backward diffusion. We show that both models can adapt to the intrinsic manifold structure by showing that the convergence rate of the inducing distribution estimator depends only on the intrinsic dimension of the data. Moreover, our considered estimator does not require knowing or explicitly estimating the manifold. We also demonstrate that the forward-backward diffusion can achieve the minimax optimal rate under the Wasserstein metric when the target distribution possesses a smooth density with respect to the volume measure of the low-dimensional manifold",
    "checked": true,
    "id": "a634b8ca66e1d1998f0d7b791fef8369d8ae0fc8",
    "semantic_title": "adaptivity of diffusion models to manifold structures",
    "citation_count": 1,
    "authors": [
      "Rong Tang",
      "Yun Yang"
    ]
  },
  "https://proceedings.mlr.press/v238/diamant24a.html": {
    "title": "Conformalized Deep Splines for Optimal and Efficient Prediction Sets",
    "volume": "main",
    "abstract": "Uncertainty estimation is critical in high-stakes machine learning applications. One effective way to estimate uncertainty is conformal prediction, which can provide predictive inference with statistical coverage guarantees. We present a new conformal regression method, Spline Prediction Intervals via Conformal Estimation (SPICE), that estimates the conditional density using neural- network-parameterized splines. We prove universal approximation and optimality results for SPICE, which are empirically reflected by our experiments. SPICE is compatible with two different efficient-to- compute conformal scores, one designed for size-efficient marginal coverage (SPICE-ND) and the other for size-efficient conditional coverage (SPICE-HPD). Results on benchmark datasets demonstrate SPICE-ND models achieve the smallest average prediction set sizes, including average size reductions of nearly 50% for some datasets compared to the next best baseline. SPICE-HPD models achieve the best conditional coverage compared to baselines. The SPICE implementation is made available",
    "checked": true,
    "id": "a2b04f844a805715a18b0ede5c7694a346a167c0",
    "semantic_title": "conformalized deep splines for optimal and efficient prediction sets",
    "citation_count": 1,
    "authors": [
      "Nathaniel Diamant",
      "Ehsan Hajiramezanali",
      "Tommaso Biancalani",
      "Gabriele Scalia"
    ]
  },
  "https://proceedings.mlr.press/v238/esaki24a.html": {
    "title": "Accuracy-Preserving Calibration via Statistical Modeling on Probability Simplex",
    "volume": "main",
    "abstract": "Classification models based on deep neural networks (DNNs) must be calibrated to measure the reliability of predictions. Some recent calibration methods have employed a probabilistic model on the probability simplex. However, these calibration methods cannot preserve the accuracy of pre-trained models, even those with a high classification accuracy. We propose an accuracy-preserving calibration method using the Concrete distribution as the probabilistic model on the probability simplex. We theoretically prove that a DNN model trained on cross-entropy loss has optimality as the parameter of the Concrete distribution. We also propose an efficient method that synthetically generates samples for training probabilistic models on the probability simplex. We demonstrate that the proposed method can outperform previous methods in accuracy-preserving calibration tasks using benchmarks",
    "checked": true,
    "id": "6d6e545ce1fd637fbce7549e668dbded40ac8581",
    "semantic_title": "accuracy-preserving calibration via statistical modeling on probability simplex",
    "citation_count": 0,
    "authors": [
      "Yasushi Esaki",
      "Akihiro Nakamura",
      "Keisuke Kawano",
      "Ryoko Tokuhisa",
      "Takuro Kutsuna"
    ]
  },
  "https://proceedings.mlr.press/v238/ye24b.html": {
    "title": "Smoothness-Adaptive Dynamic Pricing with Nonparametric Demand Learning",
    "volume": "main",
    "abstract": "We study the dynamic pricing problem where the demand function is nonparametric and Hölder smooth, and we focus on adaptivity to the unknown Hölder smoothness parameter $\\beta$ of the demand function. Traditionally the optimal dynamic pricing algorithm heavily relies on the knowledge of $\\beta$ to achieve a minimax optimal regret of $\\widetilde{O}(T^{\\frac{\\beta+1}{2\\beta+1}})$. However, we highlight the challenge of adaptivity in this dynamic pricing problem by proving that no pricing policy can adaptively achieve this minimax optimal regret without knowledge of $\\beta$. Motivated by the impossibility result, we propose a self-similarity condition to enable adaptivity. Importantly, we show that the self-similarity condition does not compromise the problem's inherent complexity since it preserves the regret lower bound $\\Omega(T^{\\frac{\\beta+1}{2\\beta+1}})$. Furthermore, we develop a smoothness-adaptive dynamic pricing algorithm and theoretically prove that the algorithm achieves this minimax optimal regret bound without the prior knowledge $\\beta$",
    "checked": true,
    "id": "a943983d43a282bd252340bb175bb1781b74256b",
    "semantic_title": "smoothness-adaptive dynamic pricing with nonparametric demand learning",
    "citation_count": 0,
    "authors": [
      "Zeqi Ye",
      "Hansheng Jiang"
    ]
  },
  "https://proceedings.mlr.press/v238/li24h.html": {
    "title": "Optimal Exploration is no harder than Thompson Sampling",
    "volume": "main",
    "abstract": "Given a set of arms $\\mathcal{Z}\\subset \\mathbb{R}^d$ and an unknown parameter vector $\\theta_\\ast\\in\\mathbb{R}^d$, the pure exploration linear bandits problem aims to return $\\arg\\max_{z\\in \\mathcal{Z}} z^{\\top}\\theta_{\\ast}$, with high probability through noisy measurements of $x^{\\top}\\theta_{\\ast}$ with $x\\in \\mathcal{X}\\subset \\mathbb{R}^d$. Existing (asymptotically) optimal methods require either a) potentially costly projections for each arm $z\\in \\mathcal{Z}$ or b) explicitly maintaining a subset of $\\mathcal{Z}$ under consideration at each time. This complexity is at odds with the popular and simple Thompson Sampling algorithm for regret minimization, which just requires access to a posterior sampling and argmax oracle, and does not need to enumerate $\\mathcal{Z}$ at any point. Unfortunately, Thompson sampling is known to be sub-optimal for pure exploration. In this work, we pose a natural question: is there an algorithm that can explore optimally and only needs the same computational primitives as Thompson Sampling? We answer the question in the affirmative. We provide an algorithm that leverages only sampling and argmax oracles and achieves an exponential convergence rate, with the exponent equal to the exponent of the optimal fixed allocation asymptotically. In addition, we show that our algorithm can be easily implemented and performs as well empirically as existing asymptotically optimal methods",
    "checked": true,
    "id": "ce251ccc31da6f1b8130b678de3c3936d3b0bfdf",
    "semantic_title": "optimal exploration is no harder than thompson sampling",
    "citation_count": 0,
    "authors": [
      "Zhaoqi Li",
      "Kevin Jamieson",
      "Lalit Jain"
    ]
  },
  "https://proceedings.mlr.press/v238/deng24a.html": {
    "title": "Sample Complexity Characterization for Linear Contextual MDPs",
    "volume": "main",
    "abstract": "Contextual Markov decision processes (CMDPs) describe a class of reinforcement learning problems in which the transition kernels and reward functions can change over time with different MDPs indexed by a context variable. While CMDPs serve as an important framework to model many real-world applications with time-varying environments, they are largely unexplored from theoretical perspective. In this paper, we study CMDPs under two linear function approximation models: Model I with context-varying representations and common linear weights for all contexts; and Model II with common representations for all contexts and context-varying linear weights. For both models, we propose novel model-based algorithms and show that they enjoy guaranteed $\\epsilon$-suboptimality gap with desired polynomial sample complexity. In particular, instantiating our result for the first model to the tabular CMDP improves the existing result by removing the reachability assumption. Our result for the second model is the first-known result for such a type of function approximation models. Comparison between our results for the two models further indicates that having context-varying features leads to much better sample efficiency than having common representations for all contexts under linear CMDPs",
    "checked": true,
    "id": "839b6c3bd3b1eac35fca726b31dbe9ab216b494b",
    "semantic_title": "sample complexity characterization for linear contextual mdps",
    "citation_count": 0,
    "authors": [
      "Junze Deng",
      "Yuan Cheng",
      "Shaofeng Zou",
      "Yingbin Liang"
    ]
  },
  "https://proceedings.mlr.press/v238/pal24a.html": {
    "title": "Sample-Efficient Personalization: Modeling User Parameters as Low Rank Plus Sparse Components",
    "volume": "main",
    "abstract": "Personalization of machine learning (ML) predictions for individual users/domains/enterprises is critical for practical recommendation systems. Standard personalization approaches involve learning a user/domain specific \\emph{embedding} that is fed into a fixed global model which can be limiting. On the other hand, personalizing/fine-tuning model itself for each user/domain — a.k.a meta-learning — has high storage/infrastructure cost. Moreover, rigorous theoretical studies of scalable personalization approaches have been very limited. To address the above issues, we propose a novel meta-learning style approach that models network weights as a sum of low-rank and sparse components. This captures common information from multiple individuals/users together in the low-rank part while sparse part captures user-specific idiosyncrasies. We then study the framework in the linear setting, where the problem reduces to that of estimating the sum of a rank-$r$ and a $k$-column sparse matrix using a small number of linear measurements. We propose a computationally efficient alternating minimization method with iterative hard thresholding — AMHT-LRS — to learn the low-rank and sparse part. Theoretically, for the realizable Gaussian data setting, we show that AMHT-LRS solves the problem efficiently with nearly optimal sample complexity. Finally, a significant challenge in personalization is ensuring privacy of each user's sensitive data. We alleviate this problem by proposing a differentially private variant of our method that also is equipped with strong generalization guarantees",
    "checked": true,
    "id": "847f2f224409e85783b19425d2785a8a868be224",
    "semantic_title": "sample-efficient personalization: modeling user parameters as low rank plus sparse components",
    "citation_count": 0,
    "authors": [
      "Soumyabrata Pal",
      "Prateek Varshney",
      "Gagan Madan",
      "Prateek Jain",
      "Abhradeep Thakurta",
      "Gaurav Aggarwal",
      "Pradeep Shenoy",
      "Gaurav Srivastava"
    ]
  },
  "https://proceedings.mlr.press/v238/leconte24a.html": {
    "title": "Queuing dynamics of asynchronous Federated Learning",
    "volume": "main",
    "abstract": "We study asynchronous federated learning mechanisms with nodes having potentially different computational speeds. In such an environment, each node is allowed to work on models with potential delays and contribute to updates to the central server at its own pace. Existing analyses of such algorithms typically depend on intractable quantities such as the maximum node delay and do not consider the underlying queuing dynamics of the system. In this paper, we propose a non-uniform sampling scheme for the central server that allows for lower delays with better complexity, taking into account the closed Jackson network structure of the associated computational graph. Our experiments clearly show a significant improvement of our method over current state-of-the-art asynchronous algorithms on image classification problems",
    "checked": true,
    "id": "1ec9c44667bf4c4461fd4962989f038cbe6d0f3b",
    "semantic_title": "queuing dynamics of asynchronous federated learning",
    "citation_count": 2,
    "authors": [
      "Louis Leconte",
      "Matthieu Jonckheere",
      "Sergey Samsonov",
      "Eric Moulines"
    ]
  },
  "https://proceedings.mlr.press/v238/tatli24a.html": {
    "title": "Learning Populations of Preferences via Pairwise Comparison Queries",
    "volume": "main",
    "abstract": "Ideal point based preference learning using pairwise comparisons of type \"Do you prefer a or b?\" has emerged as a powerful tool for understanding how we make preferences. Existing preference learning approaches assume homogeneity and focus on learning preference on average over the population or require a large number of queries per individual to localize individual preferences. However, in practical scenarios with heterogeneous preferences and limited availability of responses, these approaches are impractical. Therefore, we introduce the problem of learning the distribution of preferences over a population via pairwise comparisons using only one response per individual. Due to binary answers from comparison queries, we focus on learning the mass of the underlying distribution in the regions created by the intersection of bisecting hyperplanes between queried item pairs. We investigate this fundamental question in both 1-D and higher dimensional settings with noiseless response to comparison queries. We show that the problem is identifiable in 1-D setting and provide recovery guarantees. We show that the problem is not identifiable for higher dimensional settings in general and establish sufficient condition for identifiability. We propose using a regularized recovery, and provide guarantees on the total variation distance between the true mass and the learned distribution. We validate our findings through simulations and experiments on real datasets. We also introduce a new dataset for this task collected on a real crowdsourcing platform",
    "checked": true,
    "id": "849e41554381451f986e58d5f0ec519205911865",
    "semantic_title": "learning populations of preferences via pairwise comparison queries",
    "citation_count": 1,
    "authors": [
      "Gokcan Tatli",
      "Yi Chen",
      "Ramya Korlakai Vinayak"
    ]
  },
  "https://proceedings.mlr.press/v238/xiang24a.html": {
    "title": "A Neural Architecture Predictor based on GNN-Enhanced Transformer",
    "volume": "main",
    "abstract": "Neural architecture performance predictor is an efficient approach for architecture estimation in Neural Architecture Search (NAS). However, existing predictors based on Graph Neural Networks (GNNs) are deficient in modeling long-range interactions between operation nodes and prone to the problem of over-smoothing, which limits their ability to learn neural architecture representation. Furthermore, some Transformer-based predictors use simple position encodings to improve performance via self-attention mechanism, but they fail to fully exploit the subgraph structure information of the graph. To solve this problem, we propose a novel method to enhance the graph representation of neural architectures by combining GNNs and Transformer blocks. We evaluate the effectiveness of our predictor on NAS-Bench-101 and NAS-bench-201 benchmarks, the discovered architecture on DARTS search space achieves an accuracy of 97.61% on CIFAR-10 dataset, which outperforms traditional position encoding methods such as adjacency and Laplacian matrices. The code of our work is available at \\url{https://github.com/GNET}",
    "checked": true,
    "id": "b7c5b7077145c4f7641a111498d0d2be0555c930",
    "semantic_title": "a neural architecture predictor based on gnn-enhanced transformer",
    "citation_count": 0,
    "authors": [
      "Xunzhi Xiang",
      "Kun Jing",
      "Jungang Xu"
    ]
  },
  "https://proceedings.mlr.press/v238/liu24b.html": {
    "title": "Efficient Neural Architecture Design via Capturing Architecture-Performance Joint Distribution",
    "volume": "main",
    "abstract": "The relationship between architecture and performance is critical for improving the efficiency of neural architecture design, yet few efforts have been devoted to understanding this relationship between architecture and performance, especially architecture-performance joint distribution. In this paper, we propose Semi-Supervised Generative Adversarial Networks Neural Architecture Design Method or SemiGAN-NAD to capture the architecture-performance joint distribution with few performance labels. It is composed of Bidirectional Transformer of Architecture and Performance (Bi-Arch2Perf) and Neural Architecture Conditional Generation (NACG). Bi-Arch2Perf is developed to learn the joint distribution of architecture and performance from bidirectional conditional distribution through the adversarial training of the discriminator, the architecture generator, and the performance predictor. Then, the incorporation of semi-supervised learning optimizes the construction of Bi-Arch2Perf by utilizing a large amount of architecture information without performance annotation in search space. Based on the learned bidirectional relationship, the performance of architecture is predicted by NACG in high-performance architecture space to efficiently discover well-promising neural architectures. The experimental results on NAS benchmarks demonstrate that SemiGAN-NAD achieves competitive performance with reduced evaluation time compared with the latest NAS methods. Moreover, the high-performance architecture signatures learned by Bi-Arch2Perf are also illustrated in our experiments",
    "checked": true,
    "id": "ee63836d45ba1edc179d47386bed291f104f6958",
    "semantic_title": "efficient neural architecture design via capturing architecture-performance joint distribution",
    "citation_count": 0,
    "authors": [
      "Yue Liu",
      "Ziyi Yu",
      "Zitu Liu",
      "Wenjie Tian"
    ]
  },
  "https://proceedings.mlr.press/v238/lee24a.html": {
    "title": "Analysis of Using Sigmoid Loss for Contrastive Learning",
    "volume": "main",
    "abstract": "Contrastive learning has emerged as a prominent branch of self-supervised learning for several years. Especially, CLIP, which applies contrastive learning to large sets of captioned images, has garnered significant attention. Recently, SigLIP, a variant of CLIP, has been proposed, which uses the sigmoid loss instead of the standard InfoNCE loss. SigLIP achieves the performance comparable to CLIP in a more efficient manner by eliminating the need for a global view. However, theoretical understanding of using the sigmoid loss in contrastive learning is underexplored. In this paper, we provide a theoretical analysis of using the sigmoid loss in contrastive learning, in the perspective of the geometric structure of learned embeddings. First, we propose the double-Constant Embedding Model (CCEM), a framework for parameterizing various well-known embedding structures by a single variable. Interestingly, the proposed CCEM is proven to contain the optimal embedding with respect to the sigmoid loss. Second, we mathematically analyze the optimal embedding minimizing the sigmoid loss for contrastive learning. The optimal embedding ranges from simplex equiangular-tight-frame to antipodal structure, depending on the temperature parameter used in the sigmoid loss. Third, our experimental results on synthetic datasets coincide with the theoretical results on the optimal embedding structures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chungpa Lee",
      "Joonhwan Chang",
      "Jy-yong Sohn"
    ]
  },
  "https://proceedings.mlr.press/v238/wu24c.html": {
    "title": "Robust Data Clustering with Outliers via Transformed Tensor Low-Rank Representation",
    "volume": "main",
    "abstract": "Recently, tensor low-rank representation (TLRR) has become a popular tool for tensor data recovery and clustering, due to its empirical success and theoretical guarantees. However, existing TLRR methods consider Gaussian or gross sparse noise, inevitably leading to performance degradation when the tensor data are contaminated by outliers or sample-specific corruptions. This paper develops an outlier-robust tensor low-rank representation (OR-TLRR) method that provides outlier detection and tensor data clustering simultaneously based on the t-SVD framework. For tensor observations with arbitrary outlier corruptions, OR-TLRR has provable performance guarantee for exactly recovering the row space of clean data and detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is proposed to handle the case when parts of the data are missing. Finally, extensive experimental results on synthetic and real data demonstrate the effectiveness of the proposed algorithms. We release our code at \\url{https://github.com/twugithub/2024-AISTATS-ORTLRR}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Wu"
    ]
  },
  "https://proceedings.mlr.press/v238/han24a.html": {
    "title": "Robust SVD Made Easy: A fast and reliable algorithm for large-scale data analysis",
    "volume": "main",
    "abstract": "The singular value decomposition (SVD) is a crucial tool in machine learning and statistical data analysis. However, it is highly susceptible to outliers in the data matrix. Existing robust SVD algorithms often sacrifice speed for robustness or fail in the presence of only a few outliers. This study introduces an efficient algorithm, called Spherically Normalized SVD, for robust SVD approximation that is highly insensitive to outliers, computationally scalable, and provides accurate approximations of singular vectors. The proposed algorithm achieves remarkable speed by utilizing only two applications of a standard reduced-rank SVD algorithm to appropriately scaled data, significantly outperforming competing algorithms in computation times. To assess the robustness of the approximated singular vectors and their subspaces against data contamination, we introduce new notions of breakdown points for matrix-valued input, including row-wise, column-wise, and block-wise breakdown points. Theoretical and empirical analyses demonstrate that our algorithm exhibits higher breakdown points compared to standard SVD and its modifications. We empirically validate the effectiveness of our approach in applications such as robust low-rank approximation and robust principal component analysis of high-dimensional microarray datasets. Overall, our study presents a highly efficient and robust solution for SVD approximation that overcomes the limitations of existing algorithms in the presence of outliers",
    "checked": true,
    "id": "05ba0cdd689b9ebb26612321f90ddb4c4fcbbed7",
    "semantic_title": "robust svd made easy: a fast and reliable algorithm for large-scale data analysis",
    "citation_count": 0,
    "authors": [
      "Sangil Han",
      "Sungkyu Jung",
      "Kyoowon Kim"
    ]
  },
  "https://proceedings.mlr.press/v238/liang24a.html": {
    "title": "Regret Bounds for Risk-sensitive Reinforcement Learning with Lipschitz Dynamic Risk Measures",
    "volume": "main",
    "abstract": "We study finite episodic Markov decision processes incorporating dynamic risk measures to capture risk sensitivity. To this end, we present two model-based algorithms applied to \\emph{Lipschitz} dynamic risk measures, a wide range of risk measures that subsumes spectral risk measure, optimized certainty equivalent, and distortion risk measures, among others. We establish both regret upper bounds and lower bounds. Notably, our upper bounds demonstrate optimal dependencies on the number of actions and episodes while reflecting the inherent trade-off between risk sensitivity and sample complexity. Our approach offers a unified framework that not only encompasses multiple existing formulations in the literature but also broadens the application spectrum",
    "checked": true,
    "id": "a8af763f4bb0f52b32217633fd87c22eb54d4352",
    "semantic_title": "regret bounds for risk-sensitive reinforcement learning with lipschitz dynamic risk measures",
    "citation_count": 3,
    "authors": [
      "Hao Liang",
      "Zhiquan Luo"
    ]
  },
  "https://proceedings.mlr.press/v238/frederik-thielmann24a.html": {
    "title": "Neural Additive Models for Location Scale and Shape: A Framework for Interpretable Neural Regression Beyond the Mean",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have proven to be highly effective in a variety of tasks, making them the go-to method for problems requiring high-level predictive power. Despite this success, the inner workings of DNNs are often not transparent, making them difficult to interpret or understand. This lack of interpretability has led to increased research on inherently interpretable neural networks in recent years. Models such as Neural Additive Models (NAMs) achieve visual interpretability through the combination of classical statistical methods with DNNs. However, these approaches only concentrate on mean response predictions, leaving out other properties of the response distribution of the underlying data. We propose Neural Additive Models for Location Scale and Shape (NAMLSS), a modelling framework that combines the predictive power of classical deep learning models with the inherent advantages of distributional regression while maintaining the interpretability of additive models. The code is available at the following link: \\url{https://github.com/AnFreTh/NAMpy}",
    "checked": true,
    "id": "c64d89501acd21c55bf18786e76b8df9e091bcdd",
    "semantic_title": "neural additive models for location scale and shape: a framework for interpretable neural regression beyond the mean",
    "citation_count": 4,
    "authors": [
      "Anton Frederik Thielmann",
      "René-Marcel Kruse",
      "Thomas Kneib",
      "Benjamin Säfken"
    ]
  },
  "https://proceedings.mlr.press/v238/eliasof24a.html": {
    "title": "On The Temporal Domain of Differential Equation Inspired Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable success in modeling complex relationships in graph-structured data. A recent innovation in this field is the family of Differential Equation-Inspired Graph Neural Networks (DE-GNNs), which leverage principles from continuous dynamical systems to model information flow on graphs with built-in properties such as feature smoothing or preservation. However, existing DE-GNNs rely on first or second-order temporal dependencies. In this paper, we propose a neural extension to those pre-defined temporal dependencies. We show that our model, called TDE-GNN, can capture a wide range of temporal dynamics that go beyond typical first or second-order methods, and provide use cases where existing temporal models are challenged. We demonstrate the benefit of learning the temporal dependencies using our method rather than using pre-defined temporal dynamics on several graph benchmarks",
    "checked": true,
    "id": "9a227e7014c89095b713bdd0263b13ec6d6256f4",
    "semantic_title": "on the temporal domain of differential equation inspired graph neural networks",
    "citation_count": 3,
    "authors": [
      "Moshe Eliasof",
      "Eldad Haber",
      "Eran Treister",
      "Carola-Bibiane B Schönlieb"
    ]
  },
  "https://proceedings.mlr.press/v238/wagner24a.html": {
    "title": "Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing",
    "volume": "main",
    "abstract": "It is well-known that the reparameterisation gradient estimator, which exhibits low variance in practice, is biased for non-differentiable models. This may compromise correctness of gradient-based optimisation methods such as stochastic gradient descent (SGD). We introduce a simple syntactic framework to define non-differentiable functions piecewisely and present a systematic approach to obtain smoothings for which the reparameterisation gradient estimator is unbiased. Our main contribution is a novel variant of SGD, Diagonalisation Stochastic Gradient Descent, which progressively enhances the accuracy of the smoothed approximation during optimisation, and we prove convergence to stationary points of the unsmoothed (original) objective. Our empirical evaluation reveals benefits over the state of the art: our approach is simple, fast, stable and attains orders of magnitude reduction in work-normalised variance",
    "checked": true,
    "id": "d0e3a2dfab1c3f573c504288d7a4c2b125ee3000",
    "semantic_title": "diagonalisation sgd: fast & convergent sgd for non-differentiable models via reparameterisation and smoothing",
    "citation_count": 0,
    "authors": [
      "Dominik Wagner",
      "Basim Khajwal",
      "Luke Ong"
    ]
  },
  "https://proceedings.mlr.press/v238/sharrock24a.html": {
    "title": "Tuning-Free Maximum Likelihood Training of Latent Variable Models via Coin Betting",
    "volume": "main",
    "abstract": "We introduce two new particle-based algorithms for learning latent variable models via marginal maximum likelihood estimation, including one which is entirely tuning-free. Our methods are based on the perspective of marginal maximum likelihood estimation as an optimization problem: namely, as the minimization of a free energy functional. One way to solve this problem is via the discretization of a gradient flow associated with the free energy. We study one such approach, which resembles an extension of Stein variational gradient descent, establishing a descent lemma which guarantees that the free energy decreases at each iteration. This method, and any other obtained as the discretization of the gradient flow, necessarily depends on a learning rate which must be carefully tuned by the practitioner in order to ensure convergence at a suitable rate. With this in mind, we also propose another algorithm for optimizing the free energy which is entirely learning rate free, based on coin betting techniques from convex optimization. We validate the performance of our algorithms across several numerical experiments, including several high-dimensional settings. Our results are competitive with existing particle-based methods, without the need for any hyperparameter tuning",
    "checked": true,
    "id": "fc0fb830bbe7fab47d10ff102d999a1ead9c59f9",
    "semantic_title": "tuning-free maximum likelihood training of latent variable models via coin betting",
    "citation_count": 7,
    "authors": [
      "Louis Sharrock",
      "Daniel Dodd",
      "Christopher Nemeth"
    ]
  },
  "https://proceedings.mlr.press/v238/dold24a.html": {
    "title": "Bayesian Semi-structured Subspace Inference",
    "volume": "main",
    "abstract": "Semi-structured regression models enable the joint modeling of interpretable structured and complex unstructured feature effects. The structured model part is inspired by statistical models and can be used to infer the input-output relationship for features of particular importance. The complex unstructured part defines an arbitrary deep neural network and thereby provides enough flexibility to achieve competitive prediction performance. While these models can also account for aleatoric uncertainty, there is still a lack of work on accounting for epistemic uncertainty. In this paper, we address this problem by presenting a Bayesian approximation for semi-structured regression models using subspace inference. To this end, we extend subspace inference for joint posterior sampling from a full parameter space for structured effects and a subspace for unstructured effects. Apart from this hybrid sampling scheme, our method allows for tunable complexity of the subspace and can capture multiple minima in the loss landscape. Numerical experiments validate our approach's efficacy in recovering structured effect parameter posteriors in semi-structured models and approaching the full-space posterior distribution of MCMC for increasing subspace dimension. Further, our approach exhibits competitive predictive performance across simulated and real-world datasets",
    "checked": true,
    "id": "e87b7dfc84420d3a866af94810277c0e25e9f7d3",
    "semantic_title": "bayesian semi-structured subspace inference",
    "citation_count": 0,
    "authors": [
      "Daniel Dold",
      "David Ruegamer",
      "Beate Sick",
      "Oliver Dürr"
    ]
  },
  "https://proceedings.mlr.press/v238/nguyen-le-duy24a.html": {
    "title": "CAD-DA: Controllable Anomaly Detection after Domain Adaptation by Statistical Inference",
    "volume": "main",
    "abstract": "We propose a novel statistical method for testing the results of anomaly detection (AD) under domain adaptation (DA), which we call CAD-DA—controllable AD under DA. The distinct advantage of the CAD-DA lies in its ability to control the probability of misidentifying anomalies under a pre-specified level $\\alpha$ (e.g., 0.05). The challenge within this DA setting is the necessity to account for the influence of DA to ensure the validity of the inference results. We overcome the challenge by leveraging the concept of Selective Inference to handle the impact of DA. To our knowledge, this is the first work capable of conducting a valid statistical inference within the context of DA. We evaluate the performance of the CAD-DA method on both synthetic and real-world datasets",
    "checked": true,
    "id": "839b423f20d0bbf31e81fe8a3466c28915289861",
    "semantic_title": "cad-da: controllable anomaly detection after domain adaptation by statistical inference",
    "citation_count": 2,
    "authors": [
      "Vo Nguyen Le Duy",
      "Hsuan-Tien Lin",
      "Ichiro Takeuchi"
    ]
  },
  "https://proceedings.mlr.press/v238/jaffard24a.html": {
    "title": "Provable local learning rule by expert aggregation for a Hawkes network",
    "volume": "main",
    "abstract": "We propose a simple network of Hawkes processes as a cognitive model capable of learning to classify objects. Our learning algorithm, named HAN for Hawkes Aggregation of Neurons, is based on a local synaptic learning rule based on spiking probabilities at each output node. We were able to use local regret bounds to prove mathematically that the network is able to learn on average and even asymptotically under more restrictive assumptions",
    "checked": true,
    "id": "c8f186cb4fd33c85f7c0bc67706dcaeb42150098",
    "semantic_title": "provable local learning rule by expert aggregation for a hawkes network",
    "citation_count": 1,
    "authors": [
      "Sophie Jaffard",
      "Samuel Vaiter",
      "Alexandre Muzy",
      "Patricia Reynaud-Bouret"
    ]
  },
  "https://proceedings.mlr.press/v238/achddou24a.html": {
    "title": "Multitask Online Learning: Listen to the Neighborhood Buzz",
    "volume": "main",
    "abstract": "We study multitask online learning in a setting where agents can only exchange information with their neighbors on an arbitrary communication network. We introduce MT-CO\\textsubscript{2}OL, a decentralized algorithm for this setting whose regret depends on the interplay between the task similarities and the network structure. Our analysis shows that the regret of MT-CO\\textsubscript{2}OL is never worse (up to constants) than the bound obtained when agents do not share information. On the other hand, our bounds significantly improve when neighboring agents operate on similar tasks. In addition, we prove that our algorithm can be made differentially private with a negligible impact on the regret. Finally, we provide experimental support for our theory",
    "checked": true,
    "id": "1c50175a5a11cd084237e05d254332b0ac397656",
    "semantic_title": "multitask online learning: listen to the neighborhood buzz",
    "citation_count": 0,
    "authors": [
      "Juliette Achddou",
      "Nicolò Cesa-Bianchi",
      "Pierre Laforgue"
    ]
  },
  "https://proceedings.mlr.press/v238/korhonen24a.html": {
    "title": "Structural perspective on constraint-based learning of Markov networks",
    "volume": "main",
    "abstract": "Markov networks are probabilistic graphical models that employ undirected graphs to depict conditional independence relationships among variables. Our focus lies in constraint-based structure learning, which entails learning the undirected graph from data through the execution of conditional independence tests. We establish theoretical limits concerning two critical aspects of constraint-based learning of Markov networks: the number of tests and the sizes of the conditioning sets. These bounds uncover an exciting interplay between the structural properties of the graph and the amount of tests required to learn a Markov network. The starting point of our work is that the graph parameter maximum pairwise connectivity, $\\kappa$, that is, the maximum number of vertex-disjoint paths connecting a pair of vertices in the graph, is responsible for the sizes of independence tests required to learn the graph. On one hand, we show that at least one test with the size of the conditioning set at least $\\kappa$ is always necessary. On the other hand, we prove that any graph can be learned by performing tests of size at most $\\kappa$. This completely resolves the question of the minimum size of conditioning sets required to learn the graph. When it comes to the number of tests, our upper bound on the sizes of conditioning sets implies that every $n$-vertex graph can be learned by at most $n^{\\kappa}$ tests with conditioning sets of sizes at most $\\kappa$. We show that for any upper bound q on the sizes of the conditioning sets, there exist graphs with $O(nq)$ vertices that require at least $n^{\\Omega(\\kappa)}$ tests to learn. This lower bound holds even when the treewidth and the maximum degree of the graph are at most $\\kappa+2$. On the positive side, we prove that every graph of bounded treewidth can be learned by a polynomial number of tests with conditioning sets of sizes at most $2*\\kappa$",
    "checked": true,
    "id": "b130463eee68880a0e045ae76a5b45fc9c5350ee",
    "semantic_title": "structural perspective on constraint-based learning of markov networks",
    "citation_count": 0,
    "authors": [
      "Tuukka Korhonen",
      "Fedor Fomin",
      "Pekka Parviainen"
    ]
  },
  "https://proceedings.mlr.press/v238/huynh24a.html": {
    "title": "DAGnosis: Localized Identification of Data Inconsistencies using Structures",
    "volume": "main",
    "abstract": "Identification and appropriate handling of inconsistencies in data at deployment time is crucial to reliably use machine learning models. While recent data-centric methods are able to identify such inconsistencies with respect to the training set, they suffer from two key limitations: (1) suboptimality in settings where features exhibit statistical independencies, due to their usage of compressive representations and (2) lack of localization to pin-point why a sample might be flagged as inconsistent, which is important to guide future data collection. We solve these two fundamental limitations using directed acyclic graphs (DAGs) to encode the training set's features probability distribution and independencies as a structure. Our method, called DAGnosis, leverages these structural interactions to bring valuable and insightful data-centric conclusions. DAGnosis unlocks the localization of the causes of inconsistencies on a DAG, an aspect overlooked by previous approaches. Moreover, we show empirically that leveraging these interactions (1) leads to more accurate conclusions in detecting inconsistencies, as well as (2) provides more detailed insights into why some samples are flagged",
    "checked": true,
    "id": "e0b0b1a1f8fe7a46f4792c071f55cd91c35570e7",
    "semantic_title": "dagnosis: localized identification of data inconsistencies using structures",
    "citation_count": 0,
    "authors": [
      "Nicolas Huynh",
      "Jeroen Berrevoets",
      "Nabeel Seedat",
      "Jonathan Crabbé",
      "Zhaozhi Qian",
      "Mihaela van der Schaar"
    ]
  },
  "https://proceedings.mlr.press/v238/haasler24a.html": {
    "title": "Bures-Wasserstein Means of Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isabel Haasler",
      "Pascal Frossard"
    ]
  },
  "https://proceedings.mlr.press/v238/nakis24a.html": {
    "title": "Time to Cite: Modeling Citation Networks using the Dynamic Impact Single-Event Embedding Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolaos Nakis",
      "Abdulkadir Celikkanat",
      "Louis Boucherie",
      "Sune Lehmann",
      "Morten Mørup"
    ]
  },
  "https://proceedings.mlr.press/v238/september24a.html": {
    "title": "Extended Deep Adaptive Input Normalization for Preprocessing Time Series Data for Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcus A. K. September",
      "Francesco Sanna Passino",
      "Leonie Goldmann",
      "Anton Hinel"
    ]
  },
  "https://proceedings.mlr.press/v238/zhang24d.html": {
    "title": "Restricted Isometry Property of Rank-One Measurements with Random Unit-Modulus Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Zhang",
      "Zhenni Wang"
    ]
  },
  "https://proceedings.mlr.press/v238/verma24a.html": {
    "title": "Variational Gaussian Process Diffusion Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prakhar Verma",
      "Vincent Adam",
      "Arno Solin"
    ]
  },
  "https://proceedings.mlr.press/v238/zhao24a.html": {
    "title": "Positivity-free Policy Learning with Observational Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pan Zhao",
      "Antoine Chambaz",
      "Julie Josse",
      "Shu Yang"
    ]
  },
  "https://proceedings.mlr.press/v238/lorch24a.html": {
    "title": "Causal Modeling with Stationary Diffusions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lars Lorch",
      "Andreas Krause",
      "Bernhard Schölkopf"
    ]
  },
  "https://proceedings.mlr.press/v238/ichikawa24a.html": {
    "title": "Learning Dynamics in Linear VAE: Posterior Collapse Threshold, Superfluous Latent Space Pitfalls, and Speedup with KL Annealing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuma Ichikawa",
      "Koji Hukushima"
    ]
  },
  "https://proceedings.mlr.press/v238/heidrich24a.html": {
    "title": "A 4-Approximation Algorithm for Min Max Correlation Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Holger S. G. Heidrich",
      "Jannik Irmai",
      "Bjoern Andres"
    ]
  },
  "https://proceedings.mlr.press/v238/li24i.html": {
    "title": "Ethics in Action: Training Reinforcement Learning Agents for Moral Decision-making In Text-based Adventure Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weichen Li",
      "Rati Devidze",
      "Waleed Mustafa",
      "Sophie Fellenz"
    ]
  },
  "https://proceedings.mlr.press/v238/waldchen24a.html": {
    "title": "Interpretability Guarantees with Merlin-Arthur Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephan Wäldchen",
      "Kartikey Sharma",
      "Berkant Turan",
      "Max Zimmer",
      "Sebastian Pokutta"
    ]
  },
  "https://proceedings.mlr.press/v238/berta24a.html": {
    "title": "Classifier Calibration with ROC-Regularized Isotonic Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eugène Berta",
      "Francis Bach",
      "Michael Jordan"
    ]
  },
  "https://proceedings.mlr.press/v238/tighineanu24a.html": {
    "title": "Scalable Meta-Learning with Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Petru Tighineanu",
      "Lukas Grossberger",
      "Paul Baireuther",
      "Kathrin Skubch",
      "Stefan Falkner",
      "Julia Vinogradska",
      "Felix Berkenkamp"
    ]
  },
  "https://proceedings.mlr.press/v238/chen24b.html": {
    "title": "An Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lesi Chen",
      "Haishan Ye",
      "Luo Luo"
    ]
  },
  "https://proceedings.mlr.press/v238/pegoraro24a.html": {
    "title": "Vector Quantile Regression on Manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Pegoraro",
      "Sanketh Vedula",
      "Aviv A Rosenberg",
      "Irene Tallini",
      "Emanuele Rodola",
      "Alex Bronstein"
    ]
  },
  "https://proceedings.mlr.press/v238/wang24d.html": {
    "title": "Near-Optimal Convex Simple Bilevel Optimization with a Bisection Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiulin Wang",
      "Xu Shi",
      "Rujun Jiang"
    ]
  },
  "https://proceedings.mlr.press/v238/laberge24a.html": {
    "title": "Tackling the XAI Disagreement Problem with Regional Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Laberge",
      "Yann Batiste Pequignot",
      "Mario Marchand",
      "Foutse Khomh"
    ]
  },
  "https://proceedings.mlr.press/v238/frutos24a.html": {
    "title": "Training Implicit Generative Models via an Invariant Statistical Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "José Manuel de Frutos",
      "Pablo Olmos",
      "Manuel Alberto Vazquez Lopez",
      "Joaquín Míguez"
    ]
  },
  "https://proceedings.mlr.press/v238/fan24b.html": {
    "title": "RL in Markov Games with Independent Function Approximation: Improved Sample Complexity Bound under the Local Access Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Fan",
      "Yuxuan Han",
      "Jialin Zeng",
      "Jian-Feng Cai",
      "Yang Wang",
      "Yang Xiang",
      "Jiheng Zhang"
    ]
  },
  "https://proceedings.mlr.press/v238/dong24a.html": {
    "title": "Convergence to Nash Equilibrium and No-regret Guarantee in (Markov) Potential Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Dong",
      "Baoxiang Wang",
      "Yaoliang Yu"
    ]
  },
  "https://proceedings.mlr.press/v238/andrew24a.html": {
    "title": "GmGM: a fast multi-axis Gaussian graphical model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ethan B. Andrew",
      "David Westhead",
      "Luisa Cutillo"
    ]
  },
  "https://proceedings.mlr.press/v238/ting-li24a.html": {
    "title": "On Convergence in Wasserstein Distance and f-divergence Minimization Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheuk Ting Li",
      "Jingwei Zhang",
      "Farzan Farnia"
    ]
  },
  "https://proceedings.mlr.press/v238/sun24b.html": {
    "title": "Sparse and Faithful Explanations Without Sparse Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Sun",
      "Zhi Chen",
      "Vittorio Orlandi",
      "Tong Wang",
      "Cynthia Rudin"
    ]
  },
  "https://proceedings.mlr.press/v238/hu24c.html": {
    "title": "Extragradient Type Methods for Riemannian Variational Inequality Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Hu",
      "Guanghui Wang",
      "Xi Wang",
      "Andre Wibisono",
      "Jacob D Abernethy",
      "Molei Tao"
    ]
  },
  "https://proceedings.mlr.press/v238/velychko24a.html": {
    "title": "Learning Sparse Codes with Entropy-Based ELBOs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmytro Velychko",
      "Simon Damm",
      "Asja Fischer",
      "Jörg Lücke"
    ]
  },
  "https://proceedings.mlr.press/v238/zuo24a.html": {
    "title": "Near Optimal Adversarial Attacks on Stochastic Bandits and Defenses with Smoothed Responses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiliang Zuo"
    ]
  },
  "https://proceedings.mlr.press/v238/mauri24a.html": {
    "title": "Robust Approximate Sampling via Stochastic Gradient Barker Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Mauri",
      "Giacomo Zanella"
    ]
  },
  "https://proceedings.mlr.press/v238/tang24b.html": {
    "title": "Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyue Tang",
      "Tian Xie",
      "Aosong Feng",
      "Hanyu Wang",
      "Chenyang Zhang",
      "Yang Bai"
    ]
  },
  "https://proceedings.mlr.press/v238/liu24c.html": {
    "title": "Enhancing Distributional Stability among Sub-populations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashuo Liu",
      "Jiayun Wu",
      "Jie Peng",
      "Xiaoyu Wu",
      "Yang Zheng",
      "Bo Li",
      "Peng Cui"
    ]
  },
  "https://proceedings.mlr.press/v238/parikh24a.html": {
    "title": "Safe and Interpretable Estimation of Optimal Treatment Regimes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Parikh",
      "Quinn M Lanners",
      "Zade Akras",
      "Sahar Zafar",
      "M Brandon Westover",
      "Cynthia Rudin",
      "Alexander Volfovsky"
    ]
  },
  "https://proceedings.mlr.press/v238/gala24a.html": {
    "title": "Probabilistic Integral Circuits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gennaro Gala",
      "Cassio de Campos",
      "Robert Peharz",
      "Antonio Vergari",
      "Erik Quaeghebeur"
    ]
  },
  "https://proceedings.mlr.press/v238/bernasconi24a.html": {
    "title": "Learning Extensive-Form Perfect Equilibria in Two-Player Zero-Sum Sequential Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martino Bernasconi",
      "Alberto Marchesi",
      "Francesco Trovò"
    ]
  },
  "https://proceedings.mlr.press/v238/szlendak24a.html": {
    "title": "Understanding Progressive Training Through the Framework of Randomized Coordinate Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rafał Szlendak",
      "Elnur Gasanov",
      "Peter Richtarik"
    ]
  },
  "https://proceedings.mlr.press/v238/zhang24e.html": {
    "title": "Multiclass Learning from Noisy Labels for Non-decomposable Performance Measures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyuan Zhang",
      "Shivani Agarwal"
    ]
  },
  "https://proceedings.mlr.press/v238/zhou24a.html": {
    "title": "On the Theoretical Expressive Power and the Design Space of Higher-Order Graph Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cai Zhou",
      "Rose Yu",
      "Yusu Wang"
    ]
  },
  "https://proceedings.mlr.press/v238/janzing24a.html": {
    "title": "Quantifying intrinsic causal contributions via structure preserving interventions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Janzing",
      "Patrick Blöbaum",
      "Atalanti A Mastakouri",
      "Philipp M Faller",
      "Lenon Minorics",
      "Kailash Budhathoki"
    ]
  },
  "https://proceedings.mlr.press/v238/draxler24a.html": {
    "title": "Free-form Flows: Make Any Architecture a Normalizing Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Draxler",
      "Peter Sorrenson",
      "Lea Zimmermann",
      "Armand Rousselot",
      "Ullrich Köthe"
    ]
  },
  "https://proceedings.mlr.press/v238/moreno24a.html": {
    "title": "Efficient Model-Based Concave Utility Reinforcement Learning through Greedy Mirror Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bianca M. Moreno",
      "Margaux Bregere",
      "Pierre Gaillard",
      "Nadia Oudjane"
    ]
  },
  "https://proceedings.mlr.press/v238/guo24b.html": {
    "title": "Online learning in bandits with predicted context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongyi Guo",
      "Ziping Xu",
      "Susan Murphy"
    ]
  },
  "https://proceedings.mlr.press/v238/so24a.html": {
    "title": "Optimising Distributions with Natural Gradient Surrogates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan So",
      "Richard E. Turner"
    ]
  },
  "https://proceedings.mlr.press/v238/baker24a.html": {
    "title": "Monotone Operator Theory-Inspired Message Passing for Learning Long-Range Interaction on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin M. Baker",
      "Qingsong Wang",
      "Martin Berzins",
      "Thomas Strohmer",
      "Bao Wang"
    ]
  },
  "https://proceedings.mlr.press/v238/ahmadi24a.html": {
    "title": "Agnostic Multi-Robust Learning using ERM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saba Ahmadi",
      "Avrim Blum",
      "Omar Montasser",
      "Kevin M Stangl"
    ]
  },
  "https://proceedings.mlr.press/v238/dimlioglu24a.html": {
    "title": "GRAWA: Gradient-based Weighted Averaging for Distributed Training of Deep Learning Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tolga Dimlioglu",
      "Anna Choromanska"
    ]
  },
  "https://proceedings.mlr.press/v238/patil24a.html": {
    "title": "Failures and Successes of Cross-Validation for Early-Stopped Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratik Patil",
      "Yuchen Wu",
      "Ryan Tibshirani"
    ]
  },
  "https://proceedings.mlr.press/v238/abroshan24a.html": {
    "title": "Imposing Fairness Constraints in Synthetic Data Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahed Abroshan",
      "Andrew Elliott",
      "Mohammad Mahdi Khalili"
    ]
  },
  "https://proceedings.mlr.press/v238/choromanski24a.html": {
    "title": "Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krzysztof Choromanski",
      "Shanda Li",
      "Valerii Likhosherstov",
      "Kumar Avinava Dubey",
      "Shengjie Luo",
      "Di He",
      "Yiming Yang",
      "Tamas Sarlos",
      "Thomas Weingarten",
      "Adrian Weller"
    ]
  },
  "https://proceedings.mlr.press/v238/li24j.html": {
    "title": "Backward Filtering Forward Deciding in Linear Non-Gaussian State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun-Peng Li",
      "Hans-Andrea Loeliger"
    ]
  },
  "https://proceedings.mlr.press/v238/hoang-khoi-do24a.html": {
    "title": "MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex Influence Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nguyen Hoang Khoi Do",
      "Tanmoy Chowdhury",
      "Chen Ling",
      "Liang Zhao",
      "My T. Thai"
    ]
  },
  "https://proceedings.mlr.press/v238/kim24c.html": {
    "title": "A Doubly Robust Approach to Sparse Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonyoung Kim",
      "Garud Iyengar",
      "Assaf Zeevi"
    ]
  },
  "https://proceedings.mlr.press/v238/varici24a.html": {
    "title": "General Identifiability and Achievability for Causal Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Burak Varici",
      "Emre Acartürk",
      "Karthikeyan Shanmugam",
      "Ali Tajer"
    ]
  },
  "https://proceedings.mlr.press/v238/pasteris24a.html": {
    "title": "Sum-max Submodular Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen U. Pasteris",
      "Alberto Rumi",
      "Fabio Vitale",
      "Nicolò Cesa-Bianchi"
    ]
  },
  "https://proceedings.mlr.press/v238/gruffaz24a.html": {
    "title": "Stochastic Approximation with Biased MCMC for Expectation Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Gruffaz",
      "Kyurae Kim",
      "Alain Durmus",
      "Jacob Gardner"
    ]
  },
  "https://proceedings.mlr.press/v238/reisizadeh24a.html": {
    "title": "EM for Mixture of Linear Regression with Clustered Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirhossein Reisizadeh",
      "Khashayar Gatmiry",
      "Asuman Ozdaglar"
    ]
  },
  "https://proceedings.mlr.press/v238/dvurechensky24a.html": {
    "title": "Analysis of Kernel Mirror Prox for Measure Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavel Dvurechensky",
      "Jia-Jie Zhu"
    ]
  },
  "https://proceedings.mlr.press/v238/hariz24a.html": {
    "title": "Implicit Regularization in Deep Tucker Factorization: Low-Rankness via Structured Sparsity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kais Hariz",
      "Hachem Kadri",
      "Stéphane Ayache",
      "Maher Moakher",
      "Thierry Artières"
    ]
  },
  "https://proceedings.mlr.press/v238/rizvi-martel24a.html": {
    "title": "Simulating weighted automata over sequences and trees with transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Rizvi-Martel",
      "Maude Lizaire",
      "Clara Lacroce",
      "Guillaume Rabusseau"
    ]
  },
  "https://proceedings.mlr.press/v238/auddy24a.html": {
    "title": "Approximate Leave-one-out Cross Validation for Regression with $\\ell_1$ Regularizers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnab Auddy",
      "Haolin Zou",
      "Kamiar Rahnamarad",
      "Arian Maleki"
    ]
  },
  "https://proceedings.mlr.press/v238/lindner24a.html": {
    "title": "Learning Safety Constraints from Demonstrations with Unknown Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Lindner",
      "Xin Chen",
      "Sebastian Tschiatschek",
      "Katja Hofmann",
      "Andreas Krause"
    ]
  },
  "https://proceedings.mlr.press/v238/zhang24f.html": {
    "title": "Online Learning in Contextual Second-Price Pay-Per-Click Auctions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxiao Zhang",
      "Haipeng Luo"
    ]
  },
  "https://proceedings.mlr.press/v238/fuentes24a.html": {
    "title": "Joint Selection: Adaptively Incorporating Public Information for Private Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miguel Fuentes",
      "Brett C. Mullins",
      "Ryan McKenna",
      "Gerome Miklau",
      "Daniel Sheldon"
    ]
  },
  "https://proceedings.mlr.press/v238/long24a.html": {
    "title": "Equation Discovery with Bayesian Spike-and-Slab Priors and Efficient Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Da Long",
      "Wei Xing",
      "Aditi Krishnapriyan",
      "Robert Kirby",
      "Shandian Zhe",
      "Michael W. Mahoney"
    ]
  },
  "https://proceedings.mlr.press/v238/mitchell-roddenberry24a.html": {
    "title": "An Impossibility Theorem for Node Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "T. Mitchell Roddenberry",
      "Yu Zhu",
      "Santiago Segarra"
    ]
  },
  "https://proceedings.mlr.press/v238/diluvi24a.html": {
    "title": "Mixed variational flows for discrete variables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gian C. Diluvi",
      "Benjamin Bloem-Reddy",
      "Trevor Campbell"
    ]
  },
  "https://proceedings.mlr.press/v238/li24k.html": {
    "title": "Multi-Resolution Active Learning of Fourier Neural Operators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shibo Li",
      "Xin Yu",
      "Wei Xing",
      "Robert Kirby",
      "Akil Narayan",
      "Shandian Zhe"
    ]
  },
  "https://proceedings.mlr.press/v238/grudzien24a.html": {
    "title": "Functional Graphical Models: Structure Enables Offline Data-Driven Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuba Grudzien",
      "Masatoshi Uehara",
      "Sergey Levine",
      "Pieter Abbeel"
    ]
  },
  "https://proceedings.mlr.press/v238/chen24c.html": {
    "title": "Federated Experiment Design under Distributed Differential Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Ning Chen",
      "Graham Cormode",
      "Akash Bharadwaj",
      "Peter Romov",
      "Ayfer Ozgur"
    ]
  },
  "https://proceedings.mlr.press/v238/granese24a.html": {
    "title": "Optimal Zero-Shot Detector for Multi-Armed Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federica Granese",
      "Marco Romanelli",
      "Pablo Piantanida"
    ]
  },
  "https://proceedings.mlr.press/v238/kumar-krishnamurthy24a.html": {
    "title": "Towards Costless Model Selection in Contextual Bandits: A Bias-Variance Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanath Kumar Krishnamurthy",
      "Adrienne M Propp",
      "Susan Athey"
    ]
  },
  "https://proceedings.mlr.press/v238/patel24a.html": {
    "title": "Conformal Contextual Robust Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash P. Patel",
      "Sahana Rayan",
      "Ambuj Tewari"
    ]
  },
  "https://proceedings.mlr.press/v238/ren24a.html": {
    "title": "Learning Adaptive Kernels for Statistical Independence Tests",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Ren",
      "Yewei Xia",
      "Hao Zhang",
      "Jihong Guan",
      "Shuigeng Zhou"
    ]
  },
  "https://proceedings.mlr.press/v238/abernethy24a.html": {
    "title": "Lexicographic Optimization: Algorithms and Stability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob A. Abernethy",
      "Robert Schapire",
      "Umar Syed"
    ]
  },
  "https://proceedings.mlr.press/v238/dai24b.html": {
    "title": "Can Probabilistic Feedback Drive User Impacts in Online Platforms?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jessica Dai",
      "Bailey Flanigan",
      "Nika Haghtalab",
      "Meena Jagadeesan",
      "Chara Podimata"
    ]
  },
  "https://proceedings.mlr.press/v238/shi24a.html": {
    "title": "Learning Cartesian Product Graphs with Laplacian Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changhao Shi",
      "Gal Mishne"
    ]
  },
  "https://proceedings.mlr.press/v238/yao24a.html": {
    "title": "Minimizing Convex Functionals over Space of Probability Measures via KL Divergence Gradient Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rentian Yao",
      "Linjun Huang",
      "Yun Yang"
    ]
  },
  "https://proceedings.mlr.press/v238/showalter24a.html": {
    "title": "Bayesian Online Learning for Consensus Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Showalter",
      "Alex J Boyd",
      "Padhraic Smyth",
      "Mark Steyvers"
    ]
  },
  "https://proceedings.mlr.press/v238/kone24a.html": {
    "title": "Bandit Pareto Set Identification: the Fixed Budget Setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cyrille Kone",
      "Emilie Kaufmann",
      "Laura Richert"
    ]
  },
  "https://proceedings.mlr.press/v238/wang24e.html": {
    "title": "Efficient Data Shapley for Weighted Nearest Neighbor Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen T. Wang",
      "Prateek Mittal",
      "Ruoxi Jia"
    ]
  },
  "https://proceedings.mlr.press/v238/hsiao24a.html": {
    "title": "Surrogate Bayesian Networks for Approximating Evolutionary Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Hsiao",
      "Dana S Nau",
      "Bobak Pezeshki",
      "Rina Dechter"
    ]
  },
  "https://proceedings.mlr.press/v238/ramos24a.html": {
    "title": "BlockBoost: Scalable and Efficient Blocking through Boosting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thiago Ramos",
      "Rodrigo Loro Schuller",
      "Alex Akira Okuno",
      "Lucas Nissenbaum",
      "Roberto I Oliveira",
      "Paulo Orenstein"
    ]
  },
  "https://proceedings.mlr.press/v238/shen24a.html": {
    "title": "Continual Domain Adversarial Adaptation via Double-Head Discriminators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Shen",
      "Zhanghexuan Ji",
      "Chunwei Ma",
      "Mingchen Gao"
    ]
  },
  "https://proceedings.mlr.press/v238/mohammadpour24a.html": {
    "title": "Maximum entropy GFlowNets with soft Q-learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sobhan Mohammadpour",
      "Emmanuel Bengio",
      "Emma Frejinger",
      "Pierre-Luc Bacon"
    ]
  },
  "https://proceedings.mlr.press/v238/maiti24a.html": {
    "title": "Near-Optimal Pure Exploration in Matrix Games: A Generalization of Stochastic Bandits & Dueling Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnab Maiti",
      "Ross Boczar",
      "Kevin Jamieson",
      "Lillian Ratliff"
    ]
  },
  "https://proceedings.mlr.press/v238/zheng24b.html": {
    "title": "Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang Zheng",
      "Wei Deng",
      "Christian Moya",
      "Guang Lin"
    ]
  },
  "https://proceedings.mlr.press/v238/wu24d.html": {
    "title": "Large-Scale Gaussian Processes via Alternating Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Wu",
      "Jonathan Wenger",
      "Haydn T Jones",
      "Geoff Pleiss",
      "Jacob Gardner"
    ]
  },
  "https://proceedings.mlr.press/v238/martinez24a.html": {
    "title": "Achieving Group Distributional Robustness and Minimax Group Fairness with Interpolating Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Natalia L. Martinez",
      "Martin A. Bertran",
      "Guillermo Sapiro"
    ]
  },
  "https://proceedings.mlr.press/v238/leiner24a.html": {
    "title": "Graph fission and cross-validation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Leiner",
      "Aaditya Ramdas"
    ]
  },
  "https://proceedings.mlr.press/v238/lymperopoulos24a.html": {
    "title": "Graph Pruning for Enumeration of Minimal Unsatisfiable Subsets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Panagiotis Lymperopoulos",
      "Liping Liu"
    ]
  },
  "https://proceedings.mlr.press/v238/shao24a.html": {
    "title": "Nonparametric Automatic Differentiation Variational Inference with Spline Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuda Shao",
      "Shan N Yu",
      "Tianshu Feng"
    ]
  },
  "https://proceedings.mlr.press/v238/shekhtman24a.html": {
    "title": "Strategic Usage in a Multi-Learner Setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eliot Shekhtman",
      "Sarah Dean"
    ]
  },
  "https://proceedings.mlr.press/v238/nguyen24a.html": {
    "title": "On Parameter Estimation in Deviated Gaussian Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Nguyen",
      "Khai Nguyen",
      "Nhat Ho"
    ]
  },
  "https://proceedings.mlr.press/v238/nguyen24b.html": {
    "title": "Towards Convergence Rates for Parameter Estimation in Gaussian-gated Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Nguyen",
      "TrungTin Nguyen",
      "Khai Nguyen",
      "Nhat Ho"
    ]
  },
  "https://proceedings.mlr.press/v238/chakraborty24a.html": {
    "title": "PrIsing: Privacy-Preserving Peer Effect Estimation via Ising Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhinav Chakraborty",
      "Anirban Chatterjee",
      "Abhinandan Dalal"
    ]
  },
  "https://proceedings.mlr.press/v238/chen24d.html": {
    "title": "Escaping Saddle Points in Heterogeneous Federated Learning via Distributed SGD with Communication Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijin Chen",
      "Zhize Li",
      "Yuejie Chi"
    ]
  },
  "https://proceedings.mlr.press/v238/nguyen24c.html": {
    "title": "From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuan Nguyen",
      "Hirotada Honda",
      "Takashi Sano",
      "Vinh Nguyen",
      "Shugo Nakamura",
      "Tan Minh Nguyen"
    ]
  },
  "https://proceedings.mlr.press/v238/liu24d.html": {
    "title": "Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhishuai Liu",
      "Pan Xu"
    ]
  },
  "https://proceedings.mlr.press/v238/wang24f.html": {
    "title": "Invariant Aggregator for Defending against Federated Backdoor Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyang Wang",
      "Dimitrios Dimitriadis",
      "Sanmi Koyejo",
      "Shruti Tople"
    ]
  },
  "https://proceedings.mlr.press/v238/li24l.html": {
    "title": "Policy Evaluation for Reinforcement Learning from Human Feedback: A Sample Complexity Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Li",
      "Xiang Ji",
      "Minshuo Chen",
      "Mengdi Wang"
    ]
  },
  "https://proceedings.mlr.press/v238/adibi24a.html": {
    "title": "Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arman Adibi",
      "Nicolò Dal Fabbro",
      "Luca Schenato",
      "Sanjeev Kulkarni",
      "H. Vincent Poor",
      "George J. Pappas",
      "Hamed Hassani",
      "Aritra Mitra"
    ]
  },
  "https://proceedings.mlr.press/v238/ahmed24a.html": {
    "title": "Privacy-Preserving Decentralized Actor-Critic for Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maheed A. Ahmed",
      "Mahsa Ghasemi"
    ]
  },
  "https://proceedings.mlr.press/v238/li24m.html": {
    "title": "On the Model-Misspecification in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfan Li",
      "Lin Yang"
    ]
  },
  "https://proceedings.mlr.press/v238/levin24a.html": {
    "title": "Any-dimensional equivariant neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eitan Levin",
      "Mateo Diaz"
    ]
  },
  "https://proceedings.mlr.press/v238/laplante24a.html": {
    "title": "Conditional Adjustment in a Markov Equivalence Class",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara LaPlante",
      "Emilija Perkovic"
    ]
  },
  "https://proceedings.mlr.press/v238/arya24b.html": {
    "title": "Learning to Solve the Constrained Most Probable Explanation Task in Probabilistic Graphical Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivvrat Arya",
      "Tahrima Rahman",
      "Vibhav Gogate"
    ]
  },
  "https://proceedings.mlr.press/v238/shi24b.html": {
    "title": "Adaptive and non-adaptive minimax rates for weighted Laplacian-Eigenmap based nonparametric regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Shi",
      "Krishna Balasubramanian",
      "Wolfgang Polonik"
    ]
  },
  "https://proceedings.mlr.press/v238/cundy24a.html": {
    "title": "Privacy-Constrained Policies via Mutual Information Regularized Policy Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris J. Cundy",
      "Rishi Desai",
      "Stefano Ermon"
    ]
  },
  "https://proceedings.mlr.press/v238/arya24a.html": {
    "title": "Deep Dependency Networks and Advanced Inference Schemes for Multi-Label Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivvrat Arya",
      "Yu Xiang",
      "Vibhav Gogate"
    ]
  },
  "https://proceedings.mlr.press/v238/nguyen24d.html": {
    "title": "Near-optimal Per-Action Regret Bounds for Sleeping Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan M. Nguyen",
      "Nishant Mehta"
    ]
  },
  "https://proceedings.mlr.press/v238/ruan24a.html": {
    "title": "Electronic Medical Records Assisted Digital Clinical Trial Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinrui Ruan",
      "Jingshen Wang",
      "Yingfei Wang",
      "Waverly Wei"
    ]
  },
  "https://proceedings.mlr.press/v238/zhang24g.html": {
    "title": "Multivariate Time Series Forecasting By Graph Attention Networks With Theoretical Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Zhang",
      "Weijian Li",
      "Han Liu"
    ]
  },
  "https://proceedings.mlr.press/v238/ataee-tarzanagh24a.html": {
    "title": "Online Bilevel Optimization: Regret Analysis of Online Alternating Gradient Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davoud Ataee Tarzanagh",
      "Parvin Nazari",
      "Bojian Hou",
      "Li Shen",
      "Laura Balzano"
    ]
  },
  "https://proceedings.mlr.press/v238/ibrahim24a.html": {
    "title": "End-to-end Feature Selection Approach for Learning Skinny Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shibal Ibrahim",
      "Kayhan Behdin",
      "Rahul Mazumder"
    ]
  },
  "https://proceedings.mlr.press/v238/thompson24a.html": {
    "title": "Contextual Directed Acyclic Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Thompson",
      "Edwin V. Bonilla",
      "Robert Kohn"
    ]
  },
  "https://proceedings.mlr.press/v238/han24b.html": {
    "title": "Conformalized Semi-supervised Random Forest for Classification and Abnormality Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujin Han",
      "Mingwenchan Xu",
      "Leying Guan"
    ]
  },
  "https://proceedings.mlr.press/v238/sen-fong24a.html": {
    "title": "Multi-Level Symbolic Regression: Function Structure Learning for Multi-Level Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kei Sen Fong",
      "Mehul Motani"
    ]
  },
  "https://proceedings.mlr.press/v238/chen24e.html": {
    "title": "Non-Convex Joint Community Detection and Group Synchronization via Generalized Power Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijin Chen",
      "Xiwei Cheng",
      "Anthony Man-Cho So"
    ]
  },
  "https://proceedings.mlr.press/v238/tsai24a.html": {
    "title": "Fast Minimization of Expected Logarithmic Loss via Stochastic Dual Averaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chung-En Tsai",
      "Hao-Chung Cheng",
      "Yen-Huan Li"
    ]
  },
  "https://proceedings.mlr.press/v238/zhang24h.html": {
    "title": "Discriminant Distance-Aware Representation on Deterministic Uncertainty Quantification Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Zhang",
      "Kamalika Das",
      "Sricharan Kumar"
    ]
  },
  "https://proceedings.mlr.press/v238/haussmann24a.html": {
    "title": "Estimating treatment effects from single-arm trials via latent-variable modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Haussmann",
      "Tran Minh Son Le",
      "Viivi Halla-aho",
      "Samu Kurki",
      "Jussi Leinonen",
      "Miika Koskinen",
      "Samuel Kaski",
      "Harri Lähdesmäki"
    ]
  },
  "https://proceedings.mlr.press/v238/kuang24a.html": {
    "title": "Unveiling Latent Causal Rules: A Temporal Point Process Approach for Abnormal Event Explanation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiling Kuang",
      "Chao Yang",
      "Yang Yang",
      "Shuang Li"
    ]
  },
  "https://proceedings.mlr.press/v238/chaouki24a.html": {
    "title": "Online Learning of Decision Trees with Thompson Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayman Chaouki",
      "Jesse Read",
      "Albert Bifet"
    ]
  },
  "https://proceedings.mlr.press/v238/yang24c.html": {
    "title": "Identifying Spurious Biases Early in Training through the Lens of Simplicity Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Yang",
      "Eric Gan",
      "Gintare Karolina Dziugaite",
      "Baharan Mirzasoleiman"
    ]
  },
  "https://proceedings.mlr.press/v238/mukherjee24a.html": {
    "title": "SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhojyoti Mukherjee",
      "Qiaomin Xie",
      "Josiah P Hanna",
      "Robert Nowak"
    ]
  },
  "https://proceedings.mlr.press/v238/ebrahimpour-boroojeny24a.html": {
    "title": "Spectrum Extraction and Clipping for Implicitly Linear Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Ebrahimpour Boroojeny",
      "Matus Telgarsky",
      "Hari Sundaram"
    ]
  },
  "https://proceedings.mlr.press/v238/alizadeh24a.html": {
    "title": "Pessimistic Off-Policy Multi-Objective Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shima Alizadeh",
      "Aniruddha Bhargava",
      "Karthick Gopalswamy",
      "Lalit Jain",
      "Branislav Kveton",
      "Ge Liu"
    ]
  },
  "https://proceedings.mlr.press/v238/mogensen24a.html": {
    "title": "Faithful graphical representations of local independence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Søren W. Mogensen"
    ]
  },
  "https://proceedings.mlr.press/v238/manh-bui24a.html": {
    "title": "Density-Regression: Efficient and Distance-aware Deep Regressor for Uncertainty Estimation under Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ha Manh Bui",
      "Anqi Liu"
    ]
  },
  "https://proceedings.mlr.press/v238/viallard24a.html": {
    "title": "Leveraging PAC-Bayes Theory and Gibbs Distributions for Generalization Bounds with Complexity Measures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Viallard",
      "Rémi Emonet",
      "Amaury Habrard",
      "Emilie Morvant",
      "Valentina Zantedeschi"
    ]
  },
  "https://proceedings.mlr.press/v238/olmin24a.html": {
    "title": "On the connection between Noise-Contrastive Estimation and Contrastive Divergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amanda Olmin",
      "Jakob Lindqvist",
      "Lennart Svensson",
      "Fredrik Lindsten"
    ]
  },
  "https://proceedings.mlr.press/v238/zhou24b.html": {
    "title": "Reward-Relevance-Filtered Linear Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angela Zhou"
    ]
  },
  "https://proceedings.mlr.press/v238/rashid24a.html": {
    "title": "Preventing Arbitrarily High Confidence on Far-Away Data in Point-Estimated Discriminative Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Rashid",
      "Serena Hacker",
      "Guojun Zhang",
      "Agustinus Kristiadi",
      "Pascal Poupart"
    ]
  },
  "https://proceedings.mlr.press/v238/tang24c.html": {
    "title": "Stochastic Multi-Armed Bandits with Strongly Reward-Dependent Delays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifu Tang",
      "Yingfei Wang",
      "Zeyu Zheng"
    ]
  },
  "https://proceedings.mlr.press/v238/grosse24a.html": {
    "title": "A Greedy Approximation for k-Determinantal Point Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julia Grosse",
      "Rahel Fischer",
      "Roman Garnett",
      "Philipp Hennig"
    ]
  },
  "https://proceedings.mlr.press/v238/li24n.html": {
    "title": "Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit Feedback and Unknown Transition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long-Fei Li",
      "Peng Zhao",
      "Zhi-Hua Zhou"
    ]
  },
  "https://proceedings.mlr.press/v238/karagozlu24a.html": {
    "title": "Learning the Pareto Set Under Incomplete Preferences: Pure Exploration in Vector Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Efe Mert Karagözlü",
      "Yaşar Cahit Yıldırım",
      "Cağın Ararat",
      "Cem Tekin"
    ]
  },
  "https://proceedings.mlr.press/v238/hendrikx24a.html": {
    "title": "The Relative Gaussian Mechanism and its Application to Private Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadrien Hendrikx",
      "Paul Mangold",
      "Aurélien Bellet"
    ]
  },
  "https://proceedings.mlr.press/v238/haan24a.html": {
    "title": "Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pim de Haan",
      "Taco Cohen",
      "Johann Brehmer"
    ]
  },
  "https://proceedings.mlr.press/v238/mondal24a.html": {
    "title": "Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Washim U. Mondal",
      "Vaneet Aggarwal"
    ]
  },
  "https://proceedings.mlr.press/v238/yamada24a.html": {
    "title": "Learning Fair Division from Bandit Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hakuei Yamada",
      "Junpei Komiyama",
      "Kenshi Abe",
      "Atsushi Iwasaki"
    ]
  },
  "https://proceedings.mlr.press/v238/le24a.html": {
    "title": "Optimal Transport for Measures with Noisy Tree Metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tam Le",
      "Truyen Nguyen",
      "Kenji Fukumizu"
    ]
  },
  "https://proceedings.mlr.press/v238/salaudeen24a.html": {
    "title": "Causally Inspired Regularization Enables Domain General Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olawale Salaudeen",
      "Sanmi Koyejo"
    ]
  },
  "https://proceedings.mlr.press/v238/dheur24a.html": {
    "title": "Probabilistic Calibration by Design for Neural Network Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Dheur",
      "Souhaib Ben Taieb"
    ]
  },
  "https://proceedings.mlr.press/v238/maddux24a.html": {
    "title": "Multi-Agent Learning in Contextual Games under Unknown Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna M. Maddux",
      "Maryam Kamgarpour"
    ]
  },
  "https://proceedings.mlr.press/v238/bateni24a.html": {
    "title": "A Scalable Algorithm for Individually Fair k-Means Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MohammadHossein Bateni",
      "Vincent Cohen-Addad",
      "Alessandro Epasto",
      "Silvio Lattanzi"
    ]
  },
  "https://proceedings.mlr.press/v238/eich24a.html": {
    "title": "Approximate Control for Continuous-Time POMDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yannick Eich",
      "Bastian Alt",
      "Heinz Koeppl"
    ]
  },
  "https://proceedings.mlr.press/v238/gabbianelli24a.html": {
    "title": "Offline Primal-Dual Reinforcement Learning for Linear MDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Germano Gabbianelli",
      "Gergely Neu",
      "Matteo Papini",
      "Nneka M Okolo"
    ]
  },
  "https://proceedings.mlr.press/v238/souveton24a.html": {
    "title": "Fixed-kinetic Neural Hamiltonian Flows for enhanced interpretability and reduced complexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Souveton",
      "Arnaud Guillin",
      "Jens Jasche",
      "Guilhem Lavaux",
      "Manon Michel"
    ]
  },
  "https://proceedings.mlr.press/v238/yang24d.html": {
    "title": "Learning Unknown Intervention Targets in Structural Causal Models from Heterogeneous Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqin Yang",
      "Saber Salehkaleybar",
      "Negar Kiyavash"
    ]
  },
  "https://proceedings.mlr.press/v238/lee24b.html": {
    "title": "XB-MAML: Learning Expandable Basis Parameters for Effective Meta-Learning with Wide Task Coverage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae-Jun Lee",
      "Sung Whan Yoon"
    ]
  },
  "https://proceedings.mlr.press/v238/eldowa24a.html": {
    "title": "General Tail Bounds for Non-Smooth Stochastic Mirror Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khaled Eldowa",
      "Andrea Paudice"
    ]
  },
  "https://proceedings.mlr.press/v238/flach24a.html": {
    "title": "Symmetric Equilibrium Learning of VAEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boris Flach",
      "Dmitrij Schlesinger",
      "Alexander Shekhovtsov"
    ]
  },
  "https://proceedings.mlr.press/v238/zhao24b.html": {
    "title": "On Feynman-Kac training of partial Bayesian neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhao",
      "Sebastian Mair",
      "Thomas B. Schön",
      "Jens Sjölund"
    ]
  },
  "https://proceedings.mlr.press/v238/losalka24a.html": {
    "title": "No-Regret Algorithms for Safe Bayesian Optimization with Monotonicity Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arpan Losalka",
      "Jonathan Scarlett"
    ]
  },
  "https://proceedings.mlr.press/v238/augusto-zagatti24a.html": {
    "title": "Learning multivariate temporal point processes via the time-change theorem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guilherme Augusto Zagatti",
      "See Kiong Ng",
      "Stéphane Bressan"
    ]
  },
  "https://proceedings.mlr.press/v238/wang24g.html": {
    "title": "Model-based Policy Optimization under Approximate Bayesian Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoqi Wang",
      "Yuxin Chen",
      "Kevin Murphy"
    ]
  },
  "https://proceedings.mlr.press/v238/zeng24a.html": {
    "title": "SDMTR: A Brain-inspired Transformer for Relation Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Zeng",
      "Jie Lin",
      "Piao Hu",
      "Zhihao Li",
      "Tianxi Huang"
    ]
  },
  "https://proceedings.mlr.press/v238/ma24b.html": {
    "title": "Directed Hypergraph Representation Learning for Link Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitong Ma",
      "Wenbo Zhao",
      "Zhe Yang"
    ]
  },
  "https://proceedings.mlr.press/v238/zhang24i.html": {
    "title": "Formal Verification of Unknown Stochastic Systems via Non-parametric Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Zhang",
      "Chenyu Ma",
      "Saleh Soudijani",
      "Sadegh Soudjani"
    ]
  },
  "https://proceedings.mlr.press/v238/kviman24a.html": {
    "title": "Variational Resampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oskar Kviman",
      "Nicola Branchini",
      "Víctor Elvira",
      "Jens Lagergren"
    ]
  },
  "https://proceedings.mlr.press/v238/sander24a.html": {
    "title": "Implicit Bias in Noisy-SGD: With Applications to Differentially Private Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Sander",
      "Maxime Sylvestre",
      "Alain Durmus"
    ]
  },
  "https://proceedings.mlr.press/v238/peshekhonov24a.html": {
    "title": "Training a Tucker Model With Shared Factors: a Riemannian Optimization Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Peshekhonov",
      "Aleksey Arzhantsev",
      "Maxim Rakhuba"
    ]
  },
  "https://proceedings.mlr.press/v238/wang24h.html": {
    "title": "Don't Be Pessimistic Too Early: Look K Steps Ahead!",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoqi Wang",
      "Ziyu Ye",
      "Kevin Murphy",
      "Yuxin Chen"
    ]
  },
  "https://proceedings.mlr.press/v238/garcia-carrasco24a.html": {
    "title": "How does GPT-2 Predict Acronyms? Extracting and Understanding a Circuit via Mechanistic Interpretability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jorge García-Carrasco",
      "Alejandro Maté",
      "Juan Carlos Trujillo"
    ]
  },
  "https://proceedings.mlr.press/v238/halva24a.html": {
    "title": "Identifiable Feature Learning for Spatial Data with Nonlinear ICA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hermanni Hälvä",
      "Jonathan So",
      "Richard E. Turner",
      "Aapo Hyvärinen"
    ]
  },
  "https://proceedings.mlr.press/v238/katta24a.html": {
    "title": "Interpretable Causal Inference for Analyzing Wearable, Sensor, and Distributional Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srikar Katta",
      "Harsh Parikh",
      "Cynthia Rudin",
      "Alexander Volfovsky"
    ]
  },
  "https://proceedings.mlr.press/v238/chandramoorthy24a.html": {
    "title": "Score Operator Newton transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nisha Chandramoorthy",
      "Florian T Schaefer",
      "Youssef M Marzouk"
    ]
  },
  "https://proceedings.mlr.press/v238/novitasari24a.html": {
    "title": "ALAS: Active Learning for Autoconversion Rates Prediction from Satellite Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria C. Novitasari",
      "Johannes Quaas",
      "Miguel Rodrigues"
    ]
  },
  "https://proceedings.mlr.press/v238/verine24a.html": {
    "title": "Optimal Budgeted Rejection Sampling for Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Verine",
      "Muni Sreenivas Pydi",
      "Benjamin Negrevergne",
      "Yann Chevaleyre"
    ]
  },
  "https://proceedings.mlr.press/v238/wu24e.html": {
    "title": "Posterior Uncertainty Quantification in Neural Networks using Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luhuan Wu",
      "Sinead A Williamson"
    ]
  },
  "https://proceedings.mlr.press/v238/zhao24c.html": {
    "title": "DHMConv: Directed Hypergraph Momentum Convolution Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Zhao",
      "Zitong Ma",
      "Zhe Yang"
    ]
  },
  "https://proceedings.mlr.press/v238/jager24a.html": {
    "title": "From Data Imputation to Data Cleaning — Automated Cleaning of Tabular Data Improves Downstream Predictive Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Jäger",
      "Felix Biessmann"
    ]
  },
  "https://proceedings.mlr.press/v238/ekstrom-kelvinius24a.html": {
    "title": "Discriminator Guidance for Autoregressive Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filip Ekström Kelvinius",
      "Fredrik Lindsten"
    ]
  },
  "https://proceedings.mlr.press/v238/ding24a.html": {
    "title": "Resilient Constrained Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongsheng Ding",
      "Zhengyan Huan",
      "Alejandro Ribeiro"
    ]
  },
  "https://proceedings.mlr.press/v238/jeong24a.html": {
    "title": "On-Demand Federated Learning for Arbitrary Target Class Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isu Jeong",
      "Seulki Lee"
    ]
  },
  "https://proceedings.mlr.press/v238/shukla24a.html": {
    "title": "DiffRed: Dimensionality reduction guided by stable rank",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prarabdh Shukla",
      "Gagan Raj Gupta",
      "Kunal Dutta"
    ]
  },
  "https://proceedings.mlr.press/v238/tamas24a.html": {
    "title": "Data-Driven Confidence Intervals with Optimal Rates for the Mean of Heavy-Tailed Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ambrus Tamás",
      "Szabolcs Szentpéteri",
      "Balázs Csáji"
    ]
  },
  "https://proceedings.mlr.press/v238/zakerinia24a.html": {
    "title": "Communication-Efficient Federated Learning With Data and Client Heterogeneity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hossein Zakerinia",
      "Shayan Talaei",
      "Giorgi Nadiradze",
      "Dan Alistarh"
    ]
  },
  "https://proceedings.mlr.press/v238/fraboni24a.html": {
    "title": "SIFU: Sequential Informed Federated Unlearning for Efficient and Provable Client Unlearning in Federated Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yann Fraboni",
      "Martin Van Waerebeke",
      "Kevin Scaman",
      "Richard Vidal",
      "Laetitia Kameni",
      "Marco Lorenzi"
    ]
  },
  "https://proceedings.mlr.press/v238/popordanoska24a.html": {
    "title": "Consistent and Asymptotically Unbiased Estimation of Proper Calibration Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teodora Popordanoska",
      "Sebastian Gregor Gruber",
      "Aleksei Tiulpin",
      "Florian Buettner",
      "Matthew B. Blaschko"
    ]
  },
  "https://proceedings.mlr.press/v238/tailor24a.html": {
    "title": "Learning to Defer to a Population: A Meta-Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dharmesh Tailor",
      "Aditya Patra",
      "Rajeev Verma",
      "Putra Manggala",
      "Eric Nalisnick"
    ]
  },
  "https://proceedings.mlr.press/v238/li24o.html": {
    "title": "Trigonometric Quadrature Fourier Features for Scalable Gaussian Process Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Li",
      "Max Balakirsky",
      "Simon Mak"
    ]
  },
  "https://proceedings.mlr.press/v238/fatkhullin24a.html": {
    "title": "Taming Nonconvex Stochastic Mirror Descent with General Bregman Divergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilyas Fatkhullin",
      "Niao He"
    ]
  },
  "https://proceedings.mlr.press/v238/rashidi24a.html": {
    "title": "Cylindrical Thompson Sampling for High-Dimensional Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bahador Rashidi",
      "Kerrick Johnstonbaugh",
      "Chao Gao"
    ]
  },
  "https://proceedings.mlr.press/v238/patil24b.html": {
    "title": "On learning history-based policies for controlling Markov decision processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gandharv Patil",
      "Aditya Mahajan",
      "Doina Precup"
    ]
  },
  "https://proceedings.mlr.press/v238/kolpaczki24a.html": {
    "title": "SVARM-IQ: Efficient Approximation of Any-order Shapley Interactions through Stratification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Kolpaczki",
      "Maximilian Muschalik",
      "Fabian Fumagalli",
      "Barbara Hammer",
      "Eyke Hüllermeier"
    ]
  },
  "https://proceedings.mlr.press/v238/chauhan24a.html": {
    "title": "Dynamic Inter-treatment Information Sharing for Individualized Treatment Effects Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinod Kumar Chauhan",
      "Jiandong Zhou",
      "Ghadeer Ghosheh",
      "Soheila Molaei",
      "David A Clifton"
    ]
  },
  "https://proceedings.mlr.press/v238/hotti24a.html": {
    "title": "Benefits of Non-Linear Scale Parameterizations in Black Box Variational Inference through Smoothness Results and Gradient Variance Bounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandra Maria Hotti",
      "Lennart Alexander Van der Goten",
      "Jens Lagergren"
    ]
  },
  "https://proceedings.mlr.press/v238/mitarchuk24a.html": {
    "title": "Length independent PAC-Bayes bounds for Simple RNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Volodimir Mitarchuk",
      "Clara Lacroce",
      "Rémi Eyraud",
      "Rémi Emonet",
      "Amaury Habrard",
      "Guillaume Rabusseau"
    ]
  },
  "https://proceedings.mlr.press/v238/papazov24a.html": {
    "title": "Leveraging Continuous Time to Understand Momentum When Training Diagonal Linear Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hristo Papazov",
      "Scott Pesme",
      "Nicolas Flammarion"
    ]
  },
  "https://proceedings.mlr.press/v238/nitanda24a.html": {
    "title": "Why is parameter averaging beneficial in SGD? An objective smoothing perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atsushi Nitanda",
      "Ryuhei Kikuchi",
      "Shugo Maeda",
      "Denny Wu"
    ]
  },
  "https://proceedings.mlr.press/v238/shingaki24a.html": {
    "title": "Identification and Estimation of \"Causes of Effects\" using Covariate-Mediator Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryusei Shingaki",
      "Manabu Kuroki"
    ]
  },
  "https://proceedings.mlr.press/v238/crepon24a.html": {
    "title": "Sequential learning of the Pareto front for multi-objective bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "élise crepon",
      "Aurélien Garivier",
      "Wouter M Koolen"
    ]
  },
  "https://proceedings.mlr.press/v238/chakraborty24b.html": {
    "title": "Equivalence Testing: The Power of Bounded Adaptivity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diptarka Chakraborty",
      "Sourav Chakraborty",
      "Gunjan Kumar",
      "Kuldeep Meel"
    ]
  },
  "https://proceedings.mlr.press/v238/kacprzyk24a.html": {
    "title": "Shape Arithmetic Expressions: Advancing Scientific Discovery Beyond Closed-Form Equations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krzysztof Kacprzyk",
      "Mihaela van der Schaar"
    ]
  },
  "https://proceedings.mlr.press/v238/wu24f.html": {
    "title": "On the estimation of persistence intensity functions and linear representations of persistence diagrams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weichen Wu",
      "Jisu Kim",
      "Alessandro Rinaldo"
    ]
  },
  "https://proceedings.mlr.press/v238/wang24i.html": {
    "title": "Optimal estimation of Gaussian (poly)trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Wang",
      "Ming Gao",
      "Wai Ming Tai",
      "Bryon Aragam",
      "Arnab Bhattacharyya"
    ]
  },
  "https://proceedings.mlr.press/v238/lee24c.html": {
    "title": "Approximate Bayesian Class-Conditional Models under Continuous Representation Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas L. Lee",
      "Amos Storkey"
    ]
  },
  "https://proceedings.mlr.press/v238/battellani24a.html": {
    "title": "Dissimilarity Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paolo Battellani",
      "Alberto Maria Metelli",
      "Francesco Trovò"
    ]
  },
  "https://proceedings.mlr.press/v238/manupriya24a.html": {
    "title": "Consistent Optimal Transport with Empirical Conditional Measures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piyushi Manupriya",
      "Rachit K. Das",
      "Sayantan Biswas",
      "SakethaNath N Jagarlapudi"
    ]
  },
  "https://proceedings.mlr.press/v238/martin24a.html": {
    "title": "On the Impact of Overparameterization on the Training of a Shallow Neural Network in High Dimensions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Martin",
      "Francis Bach",
      "Giulio Biroli"
    ]
  },
  "https://proceedings.mlr.press/v238/engelmann24a.html": {
    "title": "Mixed Models with Multiple Instance Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan P. Engelmann",
      "Alessandro Palma",
      "Jakub M. Tomczak",
      "Fabian Theis",
      "Francesco Paolo Casale"
    ]
  },
  "https://proceedings.mlr.press/v238/huang24b.html": {
    "title": "Horizon-Free and Instance-Dependent Regret Bounds for Reinforcement Learning with General Function Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Huang",
      "Han Zhong",
      "Liwei Wang",
      "Lin Yang"
    ]
  },
  "https://proceedings.mlr.press/v238/emmanouilidis24a.html": {
    "title": "Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantinos Emmanouilidis",
      "Rene Vidal",
      "Nicolas Loizou"
    ]
  },
  "https://proceedings.mlr.press/v238/lyu24a.html": {
    "title": "Inconsistency of Cross-Validation for Structure Learning in Gaussian Graphical Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhao Lyu",
      "Wai Ming Tai",
      "Mladen Kolar",
      "Bryon Aragam"
    ]
  },
  "https://proceedings.mlr.press/v238/kalemaj24a.html": {
    "title": "Differentially Private Conditional Independence Testing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iden Kalemaj",
      "Shiva Kasiviswanathan",
      "Aaditya Ramdas"
    ]
  },
  "https://proceedings.mlr.press/v238/scaman24a.html": {
    "title": "Minimax Excess Risk of First-Order Methods for Statistical Learning with Data-Dependent Oracles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Scaman",
      "Mathieu Even",
      "Batiste Le Bars",
      "Laurent Massoulie"
    ]
  },
  "https://proceedings.mlr.press/v238/abedsoltan24a.html": {
    "title": "On the Nyström Approximation for Preconditioning in Kernel Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirhesam Abedsoltan",
      "Parthe Pandit",
      "Luis Rademacher",
      "Mikhail Belkin"
    ]
  },
  "https://proceedings.mlr.press/v238/kamran24a.html": {
    "title": "Learning to Rank for Optimal Treatment Allocation Under Resource Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fahad Kamran",
      "Maggie Makar",
      "Jenna Wiens"
    ]
  },
  "https://proceedings.mlr.press/v238/oesterling24a.html": {
    "title": "Fair Machine Unlearning: Data Removal while Mitigating Disparities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Oesterling",
      "Jiaqi Ma",
      "Flavio Calmon",
      "Himabindu Lakkaraju"
    ]
  },
  "https://proceedings.mlr.press/v238/wang24j.html": {
    "title": "On the Effect of Key Factors in Spurious Correlation: A theoretical Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yipei Wang",
      "Xiaoqian Wang"
    ]
  },
  "https://proceedings.mlr.press/v238/yang24e.html": {
    "title": "Hodge-Compositional Edge Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maosheng Yang",
      "Viacheslav Borovitskiy",
      "Elvin Isufi"
    ]
  },
  "https://proceedings.mlr.press/v238/tsiourvas24a.html": {
    "title": "Manifold-Aligned Counterfactual Explanations for Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asterios Tsiourvas",
      "Wei Sun",
      "Georgia Perakis"
    ]
  },
  "https://proceedings.mlr.press/v238/zhang24j.html": {
    "title": "Fast and Accurate Estimation of Low-Rank Matrices from Noisy Measurements via Preconditioned Non-Convex Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialun Zhang",
      "Richard Y Zhang",
      "Hong-Ming Chiu"
    ]
  },
  "https://proceedings.mlr.press/v238/murti24a.html": {
    "title": "LP-based Construction of DC Decompositions for Efficient Inference of Markov Random Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaitanya Murti",
      "Dhruva Kashyap",
      "Chiranjib Bhattacharyya"
    ]
  },
  "https://proceedings.mlr.press/v238/nazaret24a.html": {
    "title": "On the Misspecification of Linear Assumptions in Synthetic Controls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Achille O. R. Nazaret",
      "Claudia Shi",
      "David Blei"
    ]
  },
  "https://proceedings.mlr.press/v238/carmon24a.html": {
    "title": "The sample complexity of ERMs in stochastic convex optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Carmon",
      "Amir Yehudayoff",
      "Roi Livni"
    ]
  },
  "https://proceedings.mlr.press/v238/pasarkar24a.html": {
    "title": "Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amey P. Pasarkar",
      "Adji Bousso Dieng"
    ]
  },
  "https://proceedings.mlr.press/v238/wang24k.html": {
    "title": "On cyclical MCMC sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liwei Wang",
      "Xinru Liu",
      "Aaron Smith",
      "Aguemon Y Atchade"
    ]
  },
  "https://proceedings.mlr.press/v238/john-ward24a.html": {
    "title": "FairRR: Pre-Processing for Group Fairness through Randomized Response",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua John Ward",
      "Xianli Zeng",
      "Guang Cheng"
    ]
  },
  "https://proceedings.mlr.press/v238/liu24e.html": {
    "title": "Fitting ARMA Time Series Models without Identification: A Proximal Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yin Liu",
      "Sam Davanloo Tajbakhsh"
    ]
  },
  "https://proceedings.mlr.press/v238/wu24g.html": {
    "title": "Unsupervised Change Point Detection in Multivariate Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daoping Wu",
      "Suhas Gundimeda",
      "Shaoshuai Mou",
      "Christopher Quinn"
    ]
  },
  "https://proceedings.mlr.press/v238/ferbach24a.html": {
    "title": "Proving Linear Mode Connectivity of Neural Networks via Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damien Ferbach",
      "Baptiste Goujaud",
      "Gauthier Gidel",
      "Aymeric Dieuleveut"
    ]
  },
  "https://proceedings.mlr.press/v238/ren24b.html": {
    "title": "Multi-objective Optimization via Wasserstein-Fisher-Rao Gradient Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinuo Ren",
      "Tesi Xiao",
      "Tanmay Gangwani",
      "Anshuka Rangi",
      "Holakou Rahmanian",
      "Lexing Ying",
      "Subhajit Sanyal"
    ]
  },
  "https://proceedings.mlr.press/v238/guilmeau24a.html": {
    "title": "Adaptive importance sampling for heavy-tailed distributions via $α$-divergence minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Guilmeau",
      "Nicola Branchini",
      "Emilie Chouzenoux",
      "Victor Elvira"
    ]
  },
  "https://proceedings.mlr.press/v238/jafarnia-jahromi24a.html": {
    "title": "A Bayesian Learning Algorithm for Unknown Zero-sum Stochastic Games with an Arbitrary Opponent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehdi Jafarnia Jahromi",
      "Rahul A Jain",
      "Ashutosh Nayyar"
    ]
  },
  "https://proceedings.mlr.press/v238/cai24a.html": {
    "title": "Near-Optimal Policy Optimization for Correlated Equilibrium in General-Sum Markov Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Cai",
      "Haipeng Luo",
      "Chen-Yu Wei",
      "Weiqiang Zheng"
    ]
  },
  "https://proceedings.mlr.press/v238/hanna24a.html": {
    "title": "Multi-Agent Bandit Learning through Heterogeneous Action Erasure Channels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Osama A Hanna",
      "Merve Karakas",
      "Lin Yang",
      "Christina Fragouli"
    ]
  },
  "https://proceedings.mlr.press/v238/shen24b.html": {
    "title": "Efficient Variational Sequential Information Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Shen",
      "Jason Pacheco"
    ]
  },
  "https://proceedings.mlr.press/v238/colaco-carr24a.html": {
    "title": "Conditions on Preference Relations that Guarantee the Existence of Optimal Policies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Colaço Carr",
      "Prakash Panangaden",
      "Doina Precup"
    ]
  },
  "https://proceedings.mlr.press/v238/zhang24k.html": {
    "title": "Membership Testing in Markov Equivalence Classes via Independence Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Zhang",
      "Kirankumar Shiragur",
      "Caroline Uhler"
    ]
  },
  "https://proceedings.mlr.press/v238/kerrigan24a.html": {
    "title": "Functional Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gavin Kerrigan",
      "Giosue Migliorini",
      "Padhraic Smyth"
    ]
  },
  "https://proceedings.mlr.press/v238/bansak24a.html": {
    "title": "Learning Under Random Distributional Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kirk C. Bansak",
      "Elisabeth Paulson",
      "Dominik Rothenhaeusler"
    ]
  },
  "https://proceedings.mlr.press/v238/liu24f.html": {
    "title": "Supervised Feature Selection via Ensemble Gradient Information from Sparse Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiting Liu",
      "Zahra Atashgahi",
      "Ghada Sokar",
      "Mykola Pechenizkiy",
      "Decebal Constantin Mocanu"
    ]
  },
  "https://proceedings.mlr.press/v238/tsai24b.html": {
    "title": "Proxy Methods for Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katherine Tsai",
      "Stephen R Pfohl",
      "Olawale Salaudeen",
      "Nicole Chiou",
      "Matt Kusner",
      "Alexander D’Amour",
      "Sanmi Koyejo",
      "Arthur Gretton"
    ]
  },
  "https://proceedings.mlr.press/v238/gan24a.html": {
    "title": "Contextual Bandits with Budgeted Information Reveal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyra Gan",
      "Esmaeil Keyvanshokooh",
      "Xueqing Liu",
      "Susan Murphy"
    ]
  },
  "https://proceedings.mlr.press/v238/zhou24c.html": {
    "title": "Timing as an Action: Learning When to Observe and Act",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Helen Zhou",
      "Audrey Huang",
      "Kamyar Azizzadenesheli",
      "David Childers",
      "Zachary Lipton"
    ]
  },
  "https://proceedings.mlr.press/v238/shen24c.html": {
    "title": "Stochastic Smoothed Gradient Descent Ascent for Federated Minimax Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Shen",
      "Minhui Huang",
      "Jiawei Zhang",
      "Cong Shen"
    ]
  },
  "https://proceedings.mlr.press/v238/xu24a.html": {
    "title": "Online multiple testing with e-values",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Xu",
      "Aaditya Ramdas"
    ]
  },
  "https://proceedings.mlr.press/v238/tan24a.html": {
    "title": "Informative Path Planning with Limited Adaptivity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rayen Tan",
      "Rohan Ghuge",
      "Viswanath Nagarajan"
    ]
  },
  "https://proceedings.mlr.press/v238/lion24a.html": {
    "title": "How Good is a Single Basin?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Lion",
      "Lorenzo Noci",
      "Thomas Hofmann",
      "Gregor Bachmann"
    ]
  },
  "https://proceedings.mlr.press/v238/jordan24a.html": {
    "title": "Independent Learning in Constrained Markov Potential Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philip Jordan",
      "Anas Barakat",
      "Niao He"
    ]
  },
  "https://proceedings.mlr.press/v238/erichson24a.html": {
    "title": "NoisyMix: Boosting Model Robustness to Common Corruptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Erichson",
      "Soon Hoe Lim",
      "Winnie Xu",
      "Francisco Utrera",
      "Ziang Cao",
      "Michael Mahoney"
    ]
  },
  "https://proceedings.mlr.press/v238/mahmudul-alam24a.html": {
    "title": "Holographic Global Convolutional Networks for Long-Range Prediction Tasks in Malware Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Mahmudul Alam",
      "Edward Raff",
      "Stella R Biderman",
      "Tim Oates",
      "James Holt"
    ]
  },
  "https://proceedings.mlr.press/v238/pichler24a.html": {
    "title": "On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georg Pichler",
      "Marco Romanelli",
      "Divya Prakash Manivannan",
      "Prashanth Krishnamurthy",
      "Farshad khorrami",
      "Siddharth Garg"
    ]
  },
  "https://proceedings.mlr.press/v238/maunu24a.html": {
    "title": "Acceleration and Implicit Regularization in Gaussian Phase Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyler Maunu",
      "Martin Molina-Fructuoso"
    ]
  },
  "https://proceedings.mlr.press/v238/oprescu24a.html": {
    "title": "Low-rank MDPs with Continuous Action Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miruna Oprescu",
      "Andrew Bennett",
      "Nathan Kallus"
    ]
  },
  "https://proceedings.mlr.press/v238/zhai24b.html": {
    "title": "Deep Learning-Based Alternative Route Computation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Zhai",
      "Dee Guo",
      "Sreenivas Gollapudi",
      "Kostas Kollias",
      "Daniel Delling"
    ]
  },
  "https://proceedings.mlr.press/v238/wright24a.html": {
    "title": "An Analytic Solution to Covariance Propagation in Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oren Wright",
      "Yorie Nakahira",
      "José M. F. Moura"
    ]
  },
  "https://proceedings.mlr.press/v238/blum24a.html": {
    "title": "On the Vulnerability of Fairness Constrained Learning to Malicious Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avrim Blum",
      "Princewill Okoroafor",
      "Aadirupa Saha",
      "Kevin M. Stangl"
    ]
  },
  "https://proceedings.mlr.press/v238/xu24b.html": {
    "title": "Uncertainty-aware Continuous Implicit Neural Representations for Remote Sensing Object Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Xu",
      "Yucheng Wang",
      "Mingzhou Fan",
      "Byung-Jun Yoon",
      "Xiaoning Qian"
    ]
  },
  "https://proceedings.mlr.press/v238/olsen24a.html": {
    "title": "Think Global, Adapt Local: Learning Locally Adaptive K-Nearest Neighbor Kernel Density Estimators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenny Olsen",
      "Rasmus M. Hoeegh Lindrup",
      "Morten Mørup"
    ]
  },
  "https://proceedings.mlr.press/v238/vasileios-vlatakis-gkaragkounis24a.html": {
    "title": "Stochastic Methods in Variational Inequalities: Ergodicity, Bias and Refinements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emmanouil Vasileios Vlatakis-Gkaragkounis",
      "Angeliki Giannou",
      "Yudong Chen",
      "Qiaomin Xie"
    ]
  },
  "https://proceedings.mlr.press/v238/faller24a.html": {
    "title": "Self-Compatibility: Evaluating Causal Discovery without Ground Truth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp M. Faller",
      "Leena C. Vankadara",
      "Atalanti A. Mastakouri",
      "Francesco Locatello",
      "Dominik Janzing"
    ]
  },
  "https://proceedings.mlr.press/v238/pereyra24a.html": {
    "title": "Equivariant bootstrapping for uncertainty quantification in imaging inverse problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcelo Pereyra",
      "Julián Tachella"
    ]
  },
  "https://proceedings.mlr.press/v238/krichene24a.html": {
    "title": "Private Learning with Public Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Walid Krichene",
      "Nicolas E Mayoraz",
      "Steffen Rendle",
      "Shuang Song",
      "Abhradeep Thakurta",
      "Li Zhang"
    ]
  },
  "https://proceedings.mlr.press/v238/mazzetto24a.html": {
    "title": "An Improved Algorithm for Learning Drifting Discrete Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessio Mazzetto"
    ]
  },
  "https://proceedings.mlr.press/v238/chae24a.html": {
    "title": "Towards a Complete Benchmark on Video Moment Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyeong Chae",
      "Donghwa Kim",
      "Kwanseok Kim",
      "Doyeon Lee",
      "Sangho Lee",
      "Seongsu Ha",
      "Jonghwan Mun",
      "Wooyoung Kang",
      "Byungseok Roh",
      "Joonseok Lee"
    ]
  },
  "https://proceedings.mlr.press/v238/jali24a.html": {
    "title": "Efficient Reinforcement Learning for Routing Jobs in Heterogeneous Queueing Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neharika Jali",
      "Guannan Qu",
      "Weina Wang",
      "Gauri Joshi"
    ]
  },
  "https://proceedings.mlr.press/v238/reza-karimi24a.html": {
    "title": "Sinkhorn Flow as Mirror Flow: A Continuous-Time Framework for Generalizing the Sinkhorn Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Reza Karimi",
      "Ya-Ping Hsieh",
      "Andreas Krause"
    ]
  },
  "https://proceedings.mlr.press/v238/dai24c.html": {
    "title": "SADI: Similarity-Aware Diffusion Model-Based Imputation for Incomplete Temporal EHR Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongyu Dai",
      "Emily Getzen",
      "Qi Long"
    ]
  },
  "https://proceedings.mlr.press/v238/shakerinava24a.html": {
    "title": "Weight-Sharing Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehran Shakerinava",
      "Motahareh MS Sohrabi",
      "Siamak Ravanbakhsh",
      "Simon Lacoste-Julien"
    ]
  },
  "https://proceedings.mlr.press/v238/tiapkin24a.html": {
    "title": "Generative Flow Networks as Entropy-Regularized RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniil Tiapkin",
      "Nikita Morozov",
      "Alexey Naumov",
      "Dmitry P Vetrov"
    ]
  },
  "https://proceedings.mlr.press/v238/zhang24l.html": {
    "title": "Multi-resolution Time-Series Transformer for Long-term Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yitian Zhang",
      "Liheng Ma",
      "Soumyasundar Pal",
      "Yingxue Zhang",
      "Mark Coates"
    ]
  },
  "https://proceedings.mlr.press/v238/karntikoon24a.html": {
    "title": "First Passage Percolation with Queried Hints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kritkorn Karntikoon",
      "Yiheng Shen",
      "Sreenivas Gollapudi",
      "Kostas Kollias",
      "Aaron Schild",
      "Ali K Sinop"
    ]
  },
  "https://proceedings.mlr.press/v238/liu24g.html": {
    "title": "User-level Differentially Private Stochastic Convex Optimization: Efficient Algorithms with Optimal Rates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daogao Liu",
      "Hilal Asi"
    ]
  },
  "https://proceedings.mlr.press/v238/giaffar24a.html": {
    "title": "The Effective Number of Shared Dimensions Between Paired Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamza Giaffar",
      "Camille Rullán Buxó",
      "Mikio Aoi"
    ]
  },
  "https://proceedings.mlr.press/v238/luo24a.html": {
    "title": "DE-HNN: An effective neural model for Circuit Netlist representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhishang Luo",
      "Truong Son Hy",
      "Puoya Tabaghi",
      "Michaël Defferrard",
      "Elahe Rezaei",
      "Ryan M. Carey",
      "Rhett Davis",
      "Rajeev Jain",
      "Yusu Wang"
    ]
  },
  "https://proceedings.mlr.press/v238/yao24b.html": {
    "title": "Simulation-Based Stacking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuling Yao",
      "Bruno Régaldo-Saint Blancard",
      "Justin Domke"
    ]
  },
  "https://proceedings.mlr.press/v238/gong24b.html": {
    "title": "Towards Practical Non-Adversarial Distribution Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Gong",
      "Ben Usman",
      "Han Zhao",
      "David I Inouye"
    ]
  },
  "https://proceedings.mlr.press/v238/demirel24a.html": {
    "title": "Benchmarking Observational Studies with Experimental Data under Right-Censoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilker Demirel",
      "Edward De Brouwer",
      "Zeshan M Hussain",
      "Michael Oberst",
      "Anthony A Philippakis",
      "David Sontag"
    ]
  },
  "https://proceedings.mlr.press/v238/kalantzis24a.html": {
    "title": "Asynchronous Randomized Trace Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vasileios Kalantzis",
      "Shashanka Ubaru",
      "Chai Wah Wu",
      "Georgios Kollias",
      "Lior Horesh"
    ]
  },
  "https://proceedings.mlr.press/v238/li24q.html": {
    "title": "Computing epidemic metrics with edge differential privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "George Z. Li",
      "Dung Nguyen",
      "Anil Vullikanti"
    ]
  },
  "https://proceedings.mlr.press/v238/mcnamara24a.html": {
    "title": "Sequential Monte Carlo for Inclusive KL Minimization in Amortized Variational Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Declan McNamara",
      "Jackson Loper",
      "Jeffrey Regier"
    ]
  },
  "https://proceedings.mlr.press/v238/mcmahan24a.html": {
    "title": "Anytime-Constrained Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy McMahan",
      "Xiaojin Zhu"
    ]
  },
  "https://proceedings.mlr.press/v238/wen24a.html": {
    "title": "Tensor-view Topological Graph Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Wen",
      "Elynn Chen",
      "Yuzhou Chen"
    ]
  },
  "https://proceedings.mlr.press/v238/byun24a.html": {
    "title": "Auditing Fairness under Unobserved Confounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yewon Byun",
      "Dylan Sam",
      "Michael Oberst",
      "Zachary Lipton",
      "Bryan Wilder"
    ]
  },
  "https://proceedings.mlr.press/v238/koelle24a.html": {
    "title": "Consistency of Dictionary-Based Manifold Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samson J. Koelle",
      "Hanyu Zhang",
      "Octavian-Vlad Murad",
      "Marina Meila"
    ]
  },
  "https://proceedings.mlr.press/v238/chang24a.html": {
    "title": "Probabilistic Modeling for Sequences of Sets in Continuous-Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Chang",
      "Alex J Boyd",
      "Padhraic Smyth"
    ]
  },
  "https://proceedings.mlr.press/v238/lan24a.html": {
    "title": "Causal Q-Aggregation for CATE Model Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Lan",
      "Vasilis Syrgkanis"
    ]
  },
  "https://proceedings.mlr.press/v238/zhao24d.html": {
    "title": "Self-Supervised Quantization-Aware Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiqi Zhao",
      "Ming Zhao"
    ]
  },
  "https://proceedings.mlr.press/v238/meng24a.html": {
    "title": "FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Meng",
      "Wenyu Chen",
      "Riade Benbaki",
      "Rahul Mazumder"
    ]
  },
  "https://proceedings.mlr.press/v238/guo24c.html": {
    "title": "The effect of Leaky ReLUs on the training and generalization of overparameterized networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinglong Guo",
      "Shaohan Li",
      "Gilad Lerman"
    ]
  },
  "https://proceedings.mlr.press/v238/gao24b.html": {
    "title": "Decentralized Multi-Level Compositional Optimization Algorithms with Level-Independent Convergence Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongchang Gao"
    ]
  },
  "https://proceedings.mlr.press/v238/jiang24a.html": {
    "title": "Krylov Cubic Regularized Newton: A Subspace Second-Order Method with Dimension-Free Convergence Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruichen Jiang",
      "Parameswaran Raman",
      "Shoham Sabach",
      "Aryan Mokhtari",
      "Mingyi Hong",
      "Volkan Cevher"
    ]
  },
  "https://proceedings.mlr.press/v238/suttle24a.html": {
    "title": "Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wesley Suttle",
      "Vipul Kumar Sharma",
      "Krishna Chaitanya Kosaraju",
      "Sivaranjani Seetharaman",
      "Ji Liu",
      "Vijay Gupta",
      "Brian M Sadler"
    ]
  },
  "https://proceedings.mlr.press/v238/garg24a.html": {
    "title": "Soft-constrained Schrödinger Bridge: a Stochastic Control Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jhanvi Garg",
      "Xianyang Zhang",
      "Quan Zhou"
    ]
  },
  "https://proceedings.mlr.press/v238/chen24f.html": {
    "title": "Coreset Markov chain Monte Carlo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naitong Chen",
      "Trevor Campbell"
    ]
  },
  "https://proceedings.mlr.press/v238/gheshlaghi-azar24a.html": {
    "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Gheshlaghi Azar",
      "Zhaohan Daniel Guo",
      "Bilal Piot",
      "Remi Munos",
      "Mark Rowland",
      "Michal Valko",
      "Daniele Calandriello"
    ]
  },
  "https://proceedings.mlr.press/v238/marmarelis24a.html": {
    "title": "Policy Learning for Localized Interventions from Observational Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myrl G. Marmarelis",
      "Fred Morstatter",
      "Aram Galstyan",
      "Greg Ver Steeg"
    ]
  },
  "https://proceedings.mlr.press/v238/ren24c.html": {
    "title": "Understanding the Generalization Benefits of Late Learning Rate Decay",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinuo Ren",
      "Chao Ma",
      "Lexing Ying"
    ]
  },
  "https://proceedings.mlr.press/v238/lee24d.html": {
    "title": "Improved Regret Bounds of (Multinomial) Logistic Bandits via Regret-to-Confidence-Set Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junghyun Lee",
      "Se-Young Yun",
      "Kwang-Sung Jun"
    ]
  },
  "https://proceedings.mlr.press/v238/wang24l.html": {
    "title": "Near-Interpolators: Rapid Norm Growth and the Trade-Off between Interpolation and Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Wang",
      "Rishi Sonthalia",
      "Wei Hu"
    ]
  },
  "https://proceedings.mlr.press/v238/kant24a.html": {
    "title": "Identifiability of Product of Experts Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manav Kant",
      "Eric Y Ma",
      "Andrei Staicu",
      "Leonard J Schulman",
      "Spencer Gordon"
    ]
  },
  "https://proceedings.mlr.press/v238/chen24g.html": {
    "title": "Gibbs-Based Information Criteria and the Over-Parameterized Regime",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobo Chen",
      "Gregory W Wornell",
      "Yuheng Bu"
    ]
  },
  "https://proceedings.mlr.press/v238/puranik24a.html": {
    "title": "Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhagyashree Puranik",
      "Ahmad Beirami",
      "Yao Qin",
      "Upamanyu Madhow"
    ]
  },
  "https://proceedings.mlr.press/v238/deng24b.html": {
    "title": "On the Generalization Ability of Unsupervised Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Deng",
      "Junyuan Hong",
      "Jiayu Zhou",
      "Mehrdad Mahdavi"
    ]
  },
  "https://proceedings.mlr.press/v238/mustafa24a.html": {
    "title": "Non-vacuous Generalization Bounds for Adversarial Risk in Stochastic Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Waleed Mustafa",
      "Philipp Liznerski",
      "Antoine Ledent",
      "Dennis Wagner",
      "Puyu Wang",
      "Marius Kloft"
    ]
  },
  "https://proceedings.mlr.press/v238/xu24c.html": {
    "title": "BLIS-Net: Classifying and Analyzing Signals on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles Xu",
      "Laney Goldman",
      "Valentina Guo",
      "Benjamin Hollander-Bodie",
      "Maedee Trank-Greene",
      "Ian Adelstein",
      "Edward De Brouwer",
      "Rex Ying",
      "Smita Krishnaswamy",
      "Michael Perlmutter"
    ]
  },
  "https://proceedings.mlr.press/v238/deb24a.html": {
    "title": "Think Before You Duel: Understanding Complexities of Preference Learning under Constrained Resources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohan Deb",
      "Aadirupa Saha",
      "Arindam Banerjee"
    ]
  },
  "https://proceedings.mlr.press/v238/warren24a.html": {
    "title": "Fast Fourier Bayesian Quadrature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Houston Warren",
      "Fabio Ramos"
    ]
  },
  "https://proceedings.mlr.press/v238/inatsu24a.html": {
    "title": "Bounding Box-based Multi-objective Bayesian Optimization of Risk Measures under Input Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Inatsu",
      "Shion Takeno",
      "Hiroyuki Hanada",
      "Kazuki Iwata",
      "Ichiro Takeuchi"
    ]
  },
  "https://proceedings.mlr.press/v238/cousins24a.html": {
    "title": "To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair Training on Shared Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cyrus Cousins",
      "I. Elizabeth Kumar",
      "Suresh Venkatasubramanian"
    ]
  },
  "https://proceedings.mlr.press/v238/kong24a.html": {
    "title": "Two Birds with One Stone: Enhancing Uncertainty Quantification and Interpretability with Graph Functional Neural Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingkai Kong",
      "Haotian Sun",
      "Yuchen Zhuang",
      "Haorui Wang",
      "Wenhao Mu",
      "Chao Zhang"
    ]
  },
  "https://proceedings.mlr.press/v238/heo24a.html": {
    "title": "Sample Efficient Learning of Factored Embeddings of Tensor Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taemin Heo",
      "Chandrajit Bajaj"
    ]
  },
  "https://proceedings.mlr.press/v238/biron-lattes24a.html": {
    "title": "autoMALA: Locally adaptive Metropolis-adjusted Langevin algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miguel Biron-Lattes",
      "Nikola Surjanovic",
      "Saifuddin Syed",
      "Trevor Campbell",
      "Alexandre Bouchard-Cote"
    ]
  },
  "https://proceedings.mlr.press/v238/yan24a.html": {
    "title": "Causal Bandits with General Causal Models and Interventions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Yan",
      "Dennis Wei",
      "Dmitriy A Katz",
      "Prasanna Sattigeri",
      "Ali Tajer"
    ]
  },
  "https://proceedings.mlr.press/v238/wycoff24a.html": {
    "title": "Surrogate Active Subspaces for Jump-Discontinuous Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Wycoff"
    ]
  },
  "https://proceedings.mlr.press/v238/alacaoglu24a.html": {
    "title": "Complexity of Single Loop Algorithms for Nonlinear Programming with Stochastic Objective and Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmet Alacaoglu",
      "Stephen J Wright"
    ]
  },
  "https://proceedings.mlr.press/v238/shaikh-veedu24a.html": {
    "title": "Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mishfad Shaikh Veedu",
      "Deepjyoti Deka",
      "Murti Salapaka"
    ]
  },
  "https://proceedings.mlr.press/v238/lim24a.html": {
    "title": "Pathwise Explanation of ReLU Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongwoo Lim",
      "Won Jo",
      "Joohyung Lee",
      "Jaesik Choi"
    ]
  },
  "https://proceedings.mlr.press/v238/hood24a.html": {
    "title": "The AL$\\ell_0$CORE Tensor Decomposition for Sparse Count Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Hood",
      "Aaron J. Schein"
    ]
  },
  "https://proceedings.mlr.press/v238/huang24c.html": {
    "title": "Adaptive Federated Minimax Optimization with Lower Complexities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feihu Huang",
      "Xinrui Wang",
      "Junyi Li",
      "Songcan Chen"
    ]
  },
  "https://proceedings.mlr.press/v238/ni24a.html": {
    "title": "Mixture-of-Linear-Experts for Long-term Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronghao Ni",
      "Zinan Lin",
      "Shuaiqi Wang",
      "Giulia Fanti"
    ]
  },
  "https://proceedings.mlr.press/v238/mortazavi24a.html": {
    "title": "On the price of exact truthfulness in incentive-compatible online learning with bandit feedback: a regret lower bound for WSU-UX",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Mortazavi",
      "Junhao Lin",
      "Nishant Mehta"
    ]
  },
  "https://proceedings.mlr.press/v238/okoroafor24a.html": {
    "title": "Faster Recalibration of an Online Predictor via Approachability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Princewill Okoroafor",
      "Bobby Kleinberg",
      "Wen Sun"
    ]
  },
  "https://proceedings.mlr.press/v238/cheng24a.html": {
    "title": "Provable Policy Gradient Methods for Average-Reward Markov Potential Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Cheng",
      "Ruida Zhou",
      "P. R. Kumar",
      "Chao Tian"
    ]
  },
  "https://proceedings.mlr.press/v238/maniyar24a.html": {
    "title": "A Cubic-regularized Policy Newton Algorithm for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mizhaan P. Maniyar",
      "Prashanth L.A.",
      "Akash Mondal",
      "Shalabh Bhatnagar"
    ]
  },
  "https://proceedings.mlr.press/v238/lu24a.html": {
    "title": "Towards Generalizable and Interpretable Motion Prediction: A Deep Variational Bayes Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juanwu Lu",
      "Wei Zhan",
      "Masayoshi Tomizuka",
      "Yeping Hu"
    ]
  },
  "https://proceedings.mlr.press/v238/ildiz24a.html": {
    "title": "Understanding Inverse Scaling and Emergence in Multitask Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammed E. Ildiz",
      "Zhe Zhao",
      "Samet Oymak"
    ]
  },
  "https://proceedings.mlr.press/v238/sunil-lahoti24a.html": {
    "title": "Sharpened Lazy Incremental Quasi-Newton Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aakash Sunil Lahoti",
      "Spandan Senapati",
      "Ketan Rajawat",
      "Alec Koppel"
    ]
  },
  "https://proceedings.mlr.press/v238/li24p.html": {
    "title": "Efficient Active Learning Halfspaces with Tsybakov Noise: A Non-convex Optimization Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinan Li",
      "Chicheng Zhang"
    ]
  },
  "https://proceedings.mlr.press/v238/mao24a.html": {
    "title": "Theoretically Grounded Loss Functions and Algorithms for Score-Based Multi-Class Abstention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Mao",
      "Mehryar Mohri",
      "Yutao Zhong"
    ]
  },
  "https://proceedings.mlr.press/v238/braun24b.html": {
    "title": "Deep Classifier Mimicry without Data Access",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steven Braun",
      "Martin Mundt",
      "Kristian Kersting"
    ]
  },
  "https://proceedings.mlr.press/v238/bojkovic24a.html": {
    "title": "Data Driven Threshold and Potential Initialization for Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Velibor Bojkovic",
      "Srinivas Anumasa",
      "Giulia De Masi",
      "Bin Gu",
      "Huan Xiong"
    ]
  },
  "https://proceedings.mlr.press/v238/battash24a.html": {
    "title": "Revisiting the Noise Model of Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Barak Battash",
      "Lior Wolf",
      "Ofir Lindenbaum"
    ]
  },
  "https://proceedings.mlr.press/v238/nakano24a.html": {
    "title": "Warped Diffusion for Latent Differentiation Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masahiro Nakano",
      "Hiroki Sakuma",
      "Ryo Nishikimi",
      "Ryohei Shibue",
      "Takashi Sato",
      "Tomoharu Iwata",
      "Kunio Kashino"
    ]
  },
  "https://proceedings.mlr.press/v238/tsoy24a.html": {
    "title": "Provable Mutual Benefits from Federated Learning in Privacy-Sensitive Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Tsoy",
      "Anna Mihalkova",
      "Teodora N Todorova",
      "Nikola Konstantinov"
    ]
  },
  "https://proceedings.mlr.press/v238/ceni24a.html": {
    "title": "Random Oscillators Network for Time Series Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Ceni",
      "Andrea Cossu",
      "Maximilian W Stölzle",
      "Jingyue Liu",
      "Cosimo Della Santina",
      "Davide Bacciu",
      "Claudio Gallicchio"
    ]
  },
  "https://proceedings.mlr.press/v238/liu24h.html": {
    "title": "Mitigating Underfitting in Learning to Defer with Consistent Losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuqi Liu",
      "Yuzhou Cao",
      "Qiaozhen Zhang",
      "Lei Feng",
      "Bo An"
    ]
  },
  "https://proceedings.mlr.press/v238/cao24a.html": {
    "title": "Consistent Hierarchical Classification with A Generalized Metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhou Cao",
      "Lei Feng",
      "Bo An"
    ]
  },
  "https://proceedings.mlr.press/v238/monzio-compagnoni24a.html": {
    "title": "SDEs for Minimax Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enea Monzio Compagnoni",
      "Antonio Orvieto",
      "Hans Kersting",
      "Frank Proske",
      "Aurelien Lucchi"
    ]
  },
  "https://proceedings.mlr.press/v238/ray-chowdhury24a.html": {
    "title": "Differentially Private Reward Estimation with Preference Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayak Ray Chowdhury",
      "Xingyu Zhou",
      "Nagarajan Natarajan"
    ]
  },
  "https://proceedings.mlr.press/v238/morozov24a.html": {
    "title": "Differentiable Rendering with Reparameterized Volume Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Morozov",
      "Denis Rakitin",
      "Oleg Desheulin",
      "Dmitry P Vetrov",
      "Kirill Struminsky"
    ]
  },
  "https://proceedings.mlr.press/v238/hubler24a.html": {
    "title": "Parameter-Agnostic Optimization under Relaxed Smoothness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Hübler",
      "Junchi Yang",
      "Xiang Li",
      "Niao He"
    ]
  },
  "https://proceedings.mlr.press/v238/nazykov24a.html": {
    "title": "Stochastic Frank-Wolfe: Unified Analysis and Zoo of Special Cases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruslan Nazykov",
      "Aleksandr Shestakov",
      "Vladimir Solodkin",
      "Aleksandr Beznosikov",
      "Gauthier Gidel",
      "Alexander Gasnikov"
    ]
  },
  "https://proceedings.mlr.press/v238/plassier24a.html": {
    "title": "Efficient Conformal Prediction under Data Heterogeneity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Plassier",
      "Nikita Kotelevskii",
      "Aleksandr Rubashevskii",
      "Fedor Noskov",
      "Maksim Velikanov",
      "Alexander Fishkov",
      "Samuel Horvath",
      "Martin Takac",
      "Eric Moulines",
      "Maxim Panov"
    ]
  },
  "https://proceedings.mlr.press/v238/ghosh24b.html": {
    "title": "Sample-efficient neural likelihood-free Bayesian inference of implicit HMMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanmitra Ghosh",
      "Paul Birrell",
      "Daniela De Angelis"
    ]
  },
  "https://proceedings.mlr.press/v238/mameche24a.html": {
    "title": "Identifying Confounding from Causal Mechanism Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Mameche",
      "Jilles Vreeken",
      "David Kaltenpoth"
    ]
  },
  "https://proceedings.mlr.press/v238/batten24a.html": {
    "title": "Tight Verification of Probabilistic Robustness in Bayesian Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Batten",
      "Mehran Hosseini",
      "Alessio Lomuscio"
    ]
  },
  "https://proceedings.mlr.press/v238/saha24b.html": {
    "title": "Testing exchangeability by pairwise betting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aytijhya Saha",
      "Aaditya Ramdas"
    ]
  },
  "https://proceedings.mlr.press/v238/m-buch24a.html": {
    "title": "Simple and scalable algorithms for cluster-aware precision medicine",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amanda M Buch",
      "Conor Liston",
      "Logan Grosenick"
    ]
  },
  "https://proceedings.mlr.press/v238/r-hands24a.html": {
    "title": "P-tensors: a General Framework for Higher Order Message Passing in Subgraph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew R Hands",
      "Tianyi Sun",
      "Risi Kondor"
    ]
  },
  "https://proceedings.mlr.press/v238/a-cabannnes24a.html": {
    "title": "The Galerkin method beats Graph-Based Approaches for Spectral Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivien A Cabannnes",
      "Francis Bach"
    ]
  },
  "https://proceedings.mlr.press/v238/j-holland24a.html": {
    "title": "Robust variance-regularized risk minimization with concomitant scaling",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew J Holland"
    ]
  },
  "https://proceedings.mlr.press/v238/d-kjaersgaard24a.html": {
    "title": "Fair Soft Clustering",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rune D. Kjærsgaard",
      "Pekka Parviainen",
      "Saket Saurabh",
      "Madhumita Kundu",
      "Line Clemmensen"
    ]
  },
  "https://proceedings.mlr.press/v238/y-tong24a.html": {
    "title": "Simulation-Free Schrödinger Bridges via Score and Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Y Tong",
      "Nikolay Malkin",
      "Kilian Fatras",
      "Lazar Atanackovic",
      "Yanlei Zhang",
      "Guillaume Huguet",
      "Guy Wolf",
      "Yoshua Bengio"
    ]
  },
  "https://proceedings.mlr.press/v238/c-cosier24a.html": {
    "title": "A Unifying Variational Framework for Gaussian Process Motion Planning",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas C. Cosier",
      "Rares Iordan",
      "Sicelukwanda N.T. Zwane",
      "Giovanni Franzese",
      "James T. Wilson",
      "Marc Deisenroth",
      "Alexander Terenin",
      "Yasemin Bekiroglu"
    ]
  },
  "https://proceedings.mlr.press/v238/q-khan24a.html": {
    "title": "Analyzing Explainer Robustness via Probabilistic Lipschitzness of Prediction Functions",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zulqarnain Q Khan",
      "Davin Hill",
      "Aria Masoomi",
      "Joshua T Bone",
      "Jennifer Dy"
    ]
  },
  "https://proceedings.mlr.press/v238/a-k-september24a.html": {
    "title": "Extended Deep Adaptive Input Normalization for Preprocessing Time Series Data for Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcus A K September",
      "Francesco Sanna Passino",
      "Leonie Goldmann",
      "Anton Hinel"
    ]
  },
  "https://proceedings.mlr.press/v238/s-g-heidrich24a.html": {
    "title": "A 4-Approximation Algorithm for Min Max Correlation Clustering",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Holger S.G. Heidrich",
      "Jannik Irmai",
      "Bjoern Andres"
    ]
  },
  "https://proceedings.mlr.press/v238/b-andrew24a.html": {
    "title": "GmGM: a fast multi-axis Gaussian graphical model",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ethan B Andrew",
      "David Westhead",
      "Luisa Cutillo"
    ]
  },
  "https://proceedings.mlr.press/v238/m-moreno24a.html": {
    "title": "Efficient Model-Based Concave Utility Reinforcement Learning through Greedy Mirror Descent",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bianca M Moreno",
      "Margaux Bregere",
      "Pierre Gaillard",
      "Nadia Oudjane"
    ]
  },
  "https://proceedings.mlr.press/v238/m-baker24a.html": {
    "title": "Monotone Operator Theory-Inspired Message Passing for Learning Long-Range Interaction on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin M Baker",
      "Qingsong Wang",
      "Martin Berzins",
      "Thomas Strohmer",
      "Bao Wang"
    ]
  },
  "https://proceedings.mlr.press/v238/u-pasteris24a.html": {
    "title": "Sum-max Submodular Bandits",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen U Pasteris",
      "Alberto Rumi",
      "Fabio Vitale",
      "Nicolò Cesa-Bianchi"
    ]
  },
  "https://proceedings.mlr.press/v238/c-diluvi24a.html": {
    "title": "Mixed variational flows for discrete variables",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gian C Diluvi",
      "Benjamin Bloem-Reddy",
      "Trevor Campbell"
    ]
  },
  "https://proceedings.mlr.press/v238/p-patel24a.html": {
    "title": "Conformal Contextual Robust Optimization",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash P Patel",
      "Sahana Rayan",
      "Ambuj Tewari"
    ]
  },
  "https://proceedings.mlr.press/v238/d-abernethy24a.html": {
    "title": "Lexicographic Optimization: Algorithms and Stability",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob D Abernethy",
      "Robert Schapire",
      "Umar Syed"
    ]
  },
  "https://proceedings.mlr.press/v238/t-wang24a.html": {
    "title": "Efficient Data Shapley for Weighted Nearest Neighbor Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen T. Wang",
      "Prateek Mittal",
      "Ruoxi Jia"
    ]
  },
  "https://proceedings.mlr.press/v238/l-martinez24a.html": {
    "title": "Achieving Group Distributional Robustness and Minimax Group Fairness with Interpolating Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Natalia L Martinez",
      "Martin A Bertran",
      "Guillermo Sapiro"
    ]
  },
  "https://proceedings.mlr.press/v238/h-ahmed24a.html": {
    "title": "Privacy-Preserving Decentralized Actor-Critic for Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maheed H Ahmed",
      "Mahsa Ghasemi"
    ]
  },
  "https://proceedings.mlr.press/v238/j-cundy24a.html": {
    "title": "Privacy-Constrained Policies via Mutual Information Regularized Policy Gradients",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris J Cundy",
      "Rishi Desai",
      "Stefano Ermon"
    ]
  },
  "https://proceedings.mlr.press/v238/m-nguyen24a.html": {
    "title": "Near-optimal Per-Action Regret Bounds for Sleeping Bandits",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan M Nguyen",
      "Nishant Mehta"
    ]
  },
  "https://proceedings.mlr.press/v238/w-mogensen24a.html": {
    "title": "Faithful graphical representations of local independence",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Søren W Mogensen"
    ]
  },
  "https://proceedings.mlr.press/v238/u-mondal24a.html": {
    "title": "Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Washim U Mondal",
      "Vaneet Aggarwal"
    ]
  },
  "https://proceedings.mlr.press/v238/m-maddux24a.html": {
    "title": "Multi-Agent Learning in Contextual Games under Unknown Constraints",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna M Maddux",
      "Maryam Kamgarpour"
    ]
  },
  "https://proceedings.mlr.press/v238/c-novitasari24a.html": {
    "title": "ALAS: Active Learning for Autoconversion Rates Prediction from Satellite Data",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria C Novitasari",
      "Johannes Quaas",
      "Miguel Rodrigues"
    ]
  },
  "https://proceedings.mlr.press/v238/l-lee24a.html": {
    "title": "Approximate Bayesian Class-Conditional Models under Continuous Representation Shift",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas L Lee",
      "Amos Storkey"
    ]
  },
  "https://proceedings.mlr.press/v238/p-engelmann24a.html": {
    "title": "Mixed Models with Multiple Instance Learning",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan P. Engelmann",
      "Alessandro Palma",
      "Jakub M Tomczak",
      "Fabian Theis",
      "Francesco Paolo Casale"
    ]
  },
  "https://proceedings.mlr.press/v238/p-pasarkar24a.html": {
    "title": "Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amey P Pasarkar",
      "Adji Bousso Dieng"
    ]
  },
  "https://proceedings.mlr.press/v238/a-hanna24a.html": {
    "title": "Multi-Agent Bandit Learning through Heterogeneous Action Erasure Channels",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Osama A Hanna",
      "Merve Karakas",
      "Lin Yang",
      "Christina Fragouli"
    ]
  },
  "https://proceedings.mlr.press/v238/c-bansak24a.html": {
    "title": "Learning Under Random Distributional Shifts",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kirk C Bansak",
      "Elisabeth Paulson",
      "Dominik Rothenhaeusler"
    ]
  },
  "https://proceedings.mlr.press/v238/m-faller24a.html": {
    "title": "Self-Compatibility: Evaluating Causal Discovery without Ground Truth",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp M Faller",
      "Leena C Vankadara",
      "Atalanti A Mastakouri",
      "Francesco Locatello",
      "Dominik Janzing"
    ]
  },
  "https://proceedings.mlr.press/v238/z-li24a.html": {
    "title": "Computing epidemic metrics with edge differential privacy",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "George Z Li",
      "Dung Nguyen",
      "Anil Vullikanti"
    ]
  },
  "https://proceedings.mlr.press/v238/j-koelle24a.html": {
    "title": "Consistency of Dictionary-Based Manifold Learning",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samson J Koelle",
      "Hanyu Zhang",
      "Octavian-Vlad Murad",
      "Marina Meila"
    ]
  },
  "https://proceedings.mlr.press/v238/e-ildiz24a.html": {
    "title": "Understanding Inverse Scaling and Emergence in Multitask Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammed E Ildiz",
      "Zhe Zhao",
      "Samet Oymak"
    ]
  }
}