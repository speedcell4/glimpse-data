{
  "https://proceedings.mlr.press/v75/li18a.html": {
    "title": "Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations",
    "abstract": "We show that the gradient descent algorithm provides an implicit regularization effect in the learning of over-parameterized matrix factorization models and one-hidden-layer neural networks with quadratic activations. Concretely, we show that given $\\tilde{O}(dr^{2})$ random linear measurements of a rank $r$ positive semidefinite matrix $X^{\\star}$, we can recover $X^{\\star}$ by parameterizing it by $UU^\\top$ with $U\\in \\mathbb R^{d\\times d}$ and minimizing the squared loss, even if $r \\ll d$. We prove that starting from a small initialization, gradient descent recovers $X^{\\star}$ in $\\tilde{O}(\\sqrt{r})$ iterations approximately. The results solve the conjecture of Gunasekar et al.’17 under the restricted isometry property.  The technique can be applied to analyzing neural networks with one-hidden-layer quadratic activations with some technical modifications",
    "volume": "main",
    "checked": true,
    "id": "8b4b861583f698e89c8cd9e198aad86809a71de7",
    "citation_count": 202
  },
  "https://proceedings.mlr.press/v75/brennan18a.html": {
    "title": "Reducibility and Computational Lower Bounds for Problems with Planted Sparse Structure",
    "abstract": "Recently, research in unsupervised learning has gravitated towards exploring statistical-computational gaps induced by sparsity. A line of work initiated in Berthet and Rigollet (2013) has aimed to explain these gaps through reductions to conjecturally hard problems from complexity theory. However, the delicate nature of average-case reductions has limited the development of techniques and often led to weaker hardness results that only apply to algorithms robust to different noise distributions or that do not need to know the parameters of the problem. We introduce several new techniques to give a web of average-case reductions showing strong computational lower bounds based on the planted clique conjecture. Our new lower bounds include:   Planted Independent Set: We show tight lower bounds for detecting a planted independent set of size $k$ in a sparse Erdős-Rényi graph of size $n$ with edge density $\\tilde{\\Theta}(n^{-\\alpha})$.  Planted Dense Subgraph: If $p > q$ are the edge densities inside and outside of the community, we show the first lower bounds for the general regime $q = \\tilde{\\Theta}(n^{-\\alpha})$ and $p - q = \\tilde{\\Theta}(n^{-\\gamma})$ where $\\gamma \\ge \\alpha$, matching the lower bounds predicted in Chen and Xu (2016). Our lower bounds apply to a deterministic community size $k$, resolving a question raised in Hajek et al. (2015).  Biclustering: We show strong lower bounds for Gaussian biclustering as a simple hypothesis testing problem to detect a uniformly at random planted flat $k \\times k$ submatrix.  Sparse Rank-1 Submatrix: We show that detection in the sparse spiked Wigner model is often harder than biclustering, and are able to obtain two different tight lower bounds for these problems with different reductions from planted clique.  Sparse PCA: We give a reduction between rank-1 submatrix and sparse PCA to obtain tight lower bounds in the less sparse regime $k \\gg \\sqrt{n}$, when the spectral algorithm is optimal over the SDP. We give an alternate reduction recovering the lower bounds of Berthet and Rigollet (2013) and Gao et al. (2017) in the simple hypothesis testing variant of sparse PCA. We also observe a subtlety in the complexity of sparse PCA that arises when the planted vector is biased.  Subgraph Stochastic Block Model: We introduce a model where two small communities are planted in an Erdős-Rényi graph of the same average edge density and give tight lower bounds yielding different hard regimes than planted dense subgraph.  Our results demonstrate that, despite the delicate nature of average-case reductions, using natural problems as intermediates can often be beneficial, as is the case in worst-case complexity. Our main technical contribution is to introduce a set of techniques for average-case reductions that: (1) maintain the level of signal in an instance of a problem; (2) alter its planted structure; and (3) map two initial high-dimensional distributions simultaneously to two target distributions approximately under total variation. We also give algorithms matching our lower bounds and identify the information-theoretic limits of the models we consider",
    "volume": "main",
    "checked": true,
    "id": "62681d176bbd78c3effd342f29a93cb762e124f0",
    "citation_count": 65
  },
  "https://proceedings.mlr.press/v75/foster18a.html": {
    "title": "Logistic Regression: The Importance of Being Improper",
    "abstract": "Learning linear predictors with the logistic loss—both in stochastic and online settings—is a fundamental task in machine learning and statistics, with direct connections to classification and boosting. Existing “fast rates” for this setting exhibit exponential dependence on the predictor norm, and Hazan et al. (2014) showed that this is unfortunately unimprovable. Starting with the simple observation that the logistic loss is $1$-mixable, we design a new efficient improper learning algorithm for online logistic regression that circumvents the aforementioned lower bound with a regret bound exhibiting a doubly-exponential improvement in dependence on the predictor norm. This provides a positive resolution to a variant of the COLT 2012 open problem of McMahan and Streeter (2012) when improper learning is allowed. This improvement is obtained both in the online setting and, with some extra work, in the batch statistical setting with high probability. We also show that the improved dependence on predictor norm is near-optimal.  Leveraging this improved dependency on the predictor norm yields the following applications: (a) we give algorithms for online bandit multiclass learning with the logistic loss with an $\\tilde{O}(\\sqrt{n})$ relative mistake bound across essentially all parameter ranges, thus providing a solution to the COLT 2009 open problem of Abernethy and Rakhlin (2009), and (b) we give an adaptive algorithm for online multiclass boosting with optimal sample complexity, thus partially resolving an open problem of Beygelzimer et al. (2015) and Jung et al. (2017). Finally, we give information-theoretic bounds on the optimal rates for improper logistic regression with general function classes, thereby characterizing the extent to which our improvement for linear classes extends to other parametric and even nonparametric settings",
    "volume": "main",
    "checked": true,
    "id": "0d373e1f5313c946f04ec251168c3b0a82b0f5b0",
    "citation_count": 48
  },
  "https://proceedings.mlr.press/v75/hanneke18a.html": {
    "title": "Actively Avoiding Nonsense in Generative Models",
    "abstract": "A generative model may generate utter nonsense when it is fit to maximize the likelihood of observed data. This happens due to “model error,” i.e., when the true data generating distribution does not fit within the class of generative models being learned. To address this, we propose a model of active distribution learning using a binary invalidity oracle that identifies some examples as clearly invalid, together with random positive examples sampled from the true distribution. The goal is to maximize the likelihood of the positive examples subject to the constraint of (almost) never generating examples labeled invalid by the oracle. Guarantees are agnostic compared to a class of probability distributions. We first show that proper learning may require exponentially many queries to the invalidity oracle. We then give an improper distribution learning algorithm that uses only polynomially many queries",
    "volume": "main",
    "checked": true,
    "id": "b3a5d069f32915ee6684cd690b0731f961dff56a",
    "citation_count": 5
  },
  "https://proceedings.mlr.press/v75/kolmogorov18a.html": {
    "title": "A Faster Approximation Algorithm for the Gibbs Partition Function",
    "abstract": "We consider the problem of estimating the partition function $Z(\\beta)=\\sum_x \\exp(-\\beta H(x))$ of a Gibbs distribution with a Hamilton $H(\\cdot)$, or more precisely the logarithm of the ratio $q=\\ln Z(0)/Z(\\beta)$. It has been recently shown how to approximate $q$ with high probability assuming the existence of an oracle that produces samples from the Gibbs distribution for a given parameter value in $[0,\\beta]$. The current best known approach due to Huber (2015) uses $O(q\\ln n\\cdot[\\ln q + \\ln \\ln n+\\varepsilon^{-2}])$  oracle calls on average where $\\varepsilon$ is the desired accuracy of approximation and $H(\\cdot)$ is assumed to lie in $\\{0\\}\\cup[1,n]$. We improve the complexity to $O(q\\ln n\\cdot\\varepsilon^{-2})$ oracle calls. We also show that the same complexity can be achieved if exact oracles are replaced with approximate sampling oracles that are within $O(\\frac{\\varepsilon^2}{q\\ln n})$ variation distance from exact oracles. Finally, we prove a lower bound of $\\Omega(q\\cdot \\varepsilon^{-2})$ oracle calls under a natural model of computation",
    "volume": "main",
    "checked": true,
    "id": "b54eb1c24feac2d32b2f1c8efee76e0e7573571b",
    "citation_count": 21
  },
  "https://proceedings.mlr.press/v75/pillaud-vivien18a.html": {
    "title": "Exponential Convergence of Testing Error for Stochastic Gradient Methods",
    "abstract": "We consider binary classification problems with positive definite kernels and square loss, and study the convergence rates of stochastic gradient methods. We show that while the excess testing \\emph{loss} (squared loss) converges slowly to zero as the number of observations (and thus iterations) goes to infinity, the testing \\emph{error} (classification error) converges exponentially fast if low-noise conditions are assumed. To achieve these rates of convergence we show sharper high-probability bounds with respect to the number of observations for stochastic gradient descent",
    "volume": "main",
    "checked": true,
    "id": "ff9fd90db9bafcd0aa24ab49547ade4425faf65b",
    "citation_count": 25
  },
  "https://proceedings.mlr.press/v75/golowich18a.html": {
    "title": "Size-Independent Sample Complexity of Neural Networks",
    "abstract": "We study the sample complexity of learning neural networks, by  providing new bounds on their Rademacher complexity assuming norm constraints  on the parameter matrix of each layer. Compared to previous work, these  complexity bounds have improved dependence on the network depth, and under some  additional assumptions, are fully independent of the network size (both depth  and width). These results are derived using some novel techniques, which may be  of independent interest",
    "volume": "main",
    "checked": true,
    "id": "2dc5286bdc86d1dc55644739611300c5121fbc21",
    "citation_count": 347
  },
  "https://proceedings.mlr.press/v75/cheng18a.html": {
    "title": "Underdamped Langevin MCMC: A non-asymptotic analysis",
    "abstract": "We study the underdamped Langevin diffusion when the log of the target distribution is smooth and strongly concave. We present a MCMC algorithm based on its discretization and show that it achieves $\\varepsilon$ error (in 2-Wasserstein distance) in $\\mathcal{O}(\\sqrt{d}/\\varepsilon)$ steps. This is a significant improvement over the best known rate for overdamped Langevin MCMC, which is $\\mathcal{O}(d/\\varepsilon^2)$ steps under the same smoothness/concavity assumptions. The underdamped Langevin MCMC scheme can be viewed as a version of Hamiltonian Monte Carlo (HMC) which has been observed to outperform overdamped Langevin MCMC methods in a number of application areas. We provide quantitative rates that support this empirical wisdom",
    "volume": "main",
    "checked": true,
    "id": "24e31e8cf65a099950ac3a5a3c18328f500887df",
    "citation_count": 205
  },
  "https://proceedings.mlr.press/v75/borsos18a.html": {
    "title": "Online Variance Reduction for Stochastic Optimization",
    "abstract": "Modern stochastic optimization methods often rely on uniform sampling which is agnostic to the underlying characteristics of the data. This might degrade the convergence by  yielding estimates that suffer from a high variance. A possible remedy is to employ non-uniform \\emph{importance sampling} techniques, which take the structure of the dataset into account. In this work, we investigate a recently proposed setting which poses variance reduction as an online optimization problem with bandit feedback. We devise a novel and efficient algorithm for this setting that finds a sequence of importance sampling distributions competitive with the best fixed distribution in hindsight, the first result of this kind. While we present our method for sampling data points, it naturally extends to selecting coordinates or even blocks of thereof. Empirical validations underline the benefits of our method in several settings",
    "volume": "main",
    "checked": true,
    "id": "4512501b0e1ea7454d6ab03a2cd491e462b81da9",
    "citation_count": 19
  },
  "https://proceedings.mlr.press/v75/kirschner18a.html": {
    "title": "Information Directed Sampling and Bandits with Heteroscedastic Noise",
    "abstract": "In the stochastic bandit problem, the goal is to maximize an unknown function via a sequence of noisy evaluations. Typically, the observation noise is assumed to be independent of the evaluation point and to satisfy a tail bound uniformly on the domain; a restrictive assumption for many applications. In this work, we consider bandits with heteroscedastic noise, where we explicitly allow the noise distribution to depend on the evaluation point. We show that this leads to new trade-offs for information and regret, which are not taken into account by existing approaches like upper confidence bound algorithms (UCB) or Thompson Sampling. To address these shortcomings, we introduce a frequentist regret analysis framework, that is similar to the Bayesian framework of Russo and Van Roy (2014), and we prove a new high-probability regret bound for general, possibly randomized policies, which depends on a quantity we refer to as regret-information ratio. From this bound, we define a frequentist version of Information Directed Sampling (IDS) to minimize the regret-information ratio over all possible action sampling distributions. This further relies on concentration inequalities for online least squares regression in separable Hilbert spaces, which we generalize to the case of heteroscedastic noise. We then formulate several variants of IDS for linear and reproducing kernel Hilbert space response functions, yielding novel algorithms for Bayesian optimization. We also prove frequentist regret bounds, which in the homoscedastic case recover known bounds for UCB, but can be much better when the noise is heteroscedastic. Empirically, we demonstrate in a linear setting with heteroscedastic noise, that some of our methods can outperform UCB and Thompson Sampling, while staying competitive when the noise is homoscedastic",
    "volume": "main",
    "checked": true,
    "id": "4e1cff5dee4671db7138a938469a2d062cb5f025",
    "citation_count": 70
  },
  "https://proceedings.mlr.press/v75/daskalakis18a.html": {
    "title": "Testing Symmetric Markov Chains From a Single Trajectory",
    "abstract": "The paper’s abstract in valid LaTeX, without non-standard macros or \\cite commands. Classical distribution testing assumes access to i.i.d. samples from the distribution that is being tested. We initiate the study of Markov chain  testing, assuming access to a {\\em single trajectory of a Markov Chain.} In particular, we observe a single trajectory $X_0,\\ldots,X_t,\\ldots$ of an unknown, symmetric, and finite state Markov Chain $\\cal M$. We do not control the starting state $X_0$, and we cannot restart the chain. Given our single trajectory, the goal is to test whether  $\\cal M$ is identical to a model Markov Chain ${\\cal M}’$, or far from it under an appropriate notion of difference. We propose a measure of difference between two Markov chains, motivated by the early work of Kazakos [78],  which captures the scaling behavior of the total variation distance between trajectories sampled from the Markov chains as the length of these trajectories grows. We provide efficient testers and information-theoretic lower bounds for testing identity of symmetric Markov chains under our proposed measure of difference, which are tight up to logarithmic factors if the hitting times of the model chain ${\\cal M}’$ is $\\tilde{O}(n)$ in  the size of the state space $n$",
    "volume": "main",
    "checked": true,
    "id": "4de5f3c06d8d52e5bc1a8a7b389fd649fd8a2483",
    "citation_count": 17
  },
  "https://proceedings.mlr.press/v75/el-alaoui18a.html": {
    "title": "Detection limits in the high-dimensional spiked rectangular model",
    "abstract": "We study the problem of detecting the presence of a single unknown spike in a rectangular data matrix, in a high-dimensional regime where the spike has fixed strength and the aspect ratio of the matrix converges to a finite limit. This setup includes Johnstone’s spiked covariance model. We analyze the likelihood ratio of the spiked model against an “all noise\" null model of reference, and show it has asymptotically Gaussian fluctuations in a region below—but in general not up to—the so-called BBP threshold from random matrix theory. Our result parallels earlier findings of Onatski et al. (2013) and Johnstone-Onatski (2015) for spherical spikes. We present a probabilistic approach capable of treating generic product priors. In particular, sparsity in the spike is allowed. Our approach operates through the principle of the cavity method from spin-glass theory.  The question of the maximal parameter region where asymptotic normality is expected to hold is left open. This region, not necessarily given by BBP, is shaped by the prior in a non-trivial way. We conjecture that this is the entire paramagnetic phase of an associated spin-glass model, and is defined by the vanishing of the replica-symmetric solution of Lesieur et al. (2015)",
    "volume": "main",
    "checked": true,
    "id": "4307d6ec2a28d12cb92f5c69378a666140397a58",
    "citation_count": 13
  },
  "https://proceedings.mlr.press/v75/simchowitz18a.html": {
    "title": "Learning Without Mixing: Towards A Sharp Analysis of Linear System Identification",
    "abstract": "We prove that the ordinary least-squares (OLS) estimator attains nearly minimax optimal performance for the identification of linear dynamical systems from a single observed trajectory. Our upper bound relies on a generalization of Mendelson’s small-ball method to dependent data, eschewing the use of standard mixing-time arguments. Our lower bounds reveal that these upper bounds match up to logarithmic factors. In particular, we capture the correct signal-to-noise behavior of the problem, showing that \\emph{more unstable} linear systems are \\emph{easier} to estimate. This behavior is qualitatively different from arguments which rely on mixing-time calculations that suggest that unstable systems are more difficult to estimate. We generalize our technique to provide bounds for a more general class of linear response time-series",
    "volume": "main",
    "checked": true,
    "id": "88141e44d400bd1b38f6420358d17e621d6472ee",
    "citation_count": 212
  },
  "https://proceedings.mlr.press/v75/blum18a.html": {
    "title": "Active Tolerant Testing",
    "abstract": "In this work, we show that for a nontrivial hypothesis class $\\mathcal C$, we can estimate the distance of a target function $f$ to $\\mathcal C$ (estimate the error rate of the best $h\\in \\mathcal C$) using substantially fewer labeled examples than would be needed to actually {\\em learn} a good $h \\in \\mathcal C$.   Specifically, we show that for the class $\\mathcal C$ of unions of $d$ intervals on the line, in the active learning setting in which we have access to a pool of unlabeled examples drawn from an arbitrary underlying distribution $\\mathcal D$, we can estimate the error rate of the best $h \\in \\mathcal C$ to an additive error $\\epsilon$ with a number of label requests that is {\\em independent of $d$} and depends only on $\\epsilon$.  In particular, we make $O(\\frac{1}{\\epsilon^6}\\log \\frac{1}{\\epsilon})$ label queries to an unlabeled pool of size $O(\\frac{d}{\\epsilon^2}\\log \\frac{1}{\\epsilon})$.  This task of estimating the distance of an unknown $f$ to a given class $\\mathcal C$  is called {\\em tolerant testing} or {\\em distance estimation} in the testing literature, usually studied in a membership query model and with respect to the uniform distribution.  Our work extends that of Balcan et al. (2012) who solved the {\\em non}-tolerant testing problem for this class (distinguishing the zero-error case from the case that the best hypothesis in the class has error greater than $\\epsilon$).   We also consider the related problem of estimating the performance of a given learning algorithm $\\mathcal A$ in this setting.  That is, given a large pool of unlabeled examples drawn from distribution $\\mathcal D$, can we, from only a few label queries, estimate how well $\\mathcal A$ would perform if the entire dataset were labeled and given as training data to $\\mathcal A$?   We focus on $k$-Nearest Neighbor style algorithms, and also show how our results can be applied to the problem of hyperparameter tuning (selecting the best value of $k$ for the given learning problem)",
    "volume": "main",
    "checked": true,
    "id": "1c538f86e0ec3d3cfec7ac7bf346ee26b6605912",
    "citation_count": 12
  },
  "https://proceedings.mlr.press/v75/tan18a.html": {
    "title": "Polynomial Time and Sample Complexity for Non-Gaussian Component Analysis: Spectral Methods",
    "abstract": "The problem of Non-Gaussian Component Analysis (NGCA) is about finding a maximal low-dimensional subspace $E$ in $\\mathbb{R}^n$ so that data points projected onto $E$ follow a non-Gaussian distribution. Vempala and Xiao (2011) proposed a local search algorithm, and showed that it was able to estimate $E$ accurately with polynomial time and sample complexity, if the dimension of $E$ is treated as a constant and with the assumption that all one-dimensional marginals of the non-Gaussian distribution over $E$ have non-Gaussian moments. In this paper, we propose a simple spectral algorithm called \\textsc{Reweighted PCA}, and prove that it possesses the same guarantee. The principle that underlies this approach is a new characterization of multivariate Gaussian distributions",
    "volume": "main",
    "checked": true,
    "id": "7cb4128be833551f43fc7ef98716812a0e461b23",
    "citation_count": 7
  },
  "https://proceedings.mlr.press/v75/feldman18a.html": {
    "title": "Calibrating Noise to Variance in Adaptive Data Analysis",
    "abstract": "Datasets are often used multiple times and each successive analysis may depend on the outcome of previous analyses. Standard techniques for ensuring generalization and statistical validity do not account for this adaptive dependence. A recent line of work studies the challenges that arise from such adaptive data reuse by considering the problem of answering a sequence of “queries” about the data distribution where each query may depend arbitrarily on answers to previous queries. The strongest results obtained for this problem rely on differential privacy – a strong notion of algorithmic stability with the important property that it “composes” well when data is reused. However the notion is rather strict, as it requires stability under replacement of an arbitrary data element. The simplest algorithm is to add Gaussian (or Laplace) noise to distort the empirical answers. However, analysing this technique using differential privacy yields suboptimal accuracy guarantees when the queries have low variance. Here we propose a relaxed notion of stability based on KL divergence that also composes adaptively. We show that our notion of stability implies a bound on the mutual information between the dataset and the output of the algorithm and then derive new generalization guarantees implied by bounded mutual information. We demonstrate that a simple and natural algorithm based on adding noise scaled to the standard deviation of the query provides our notion of stability. This implies an algorithm that can answer statistical queries about the dataset with substantially improved accuracy guarantees for low-variance queries. The only previous approach that provides such accuracy guarantees is based on a more involved differentially private median-of-means algorithm and its analysis exploits stronger “group” stability of the algorithm",
    "volume": "main",
    "checked": true,
    "id": "a51108203a894e7f26a602a484331aa792c35e5c",
    "citation_count": 34
  },
  "https://proceedings.mlr.press/v75/jain18a.html": {
    "title": "Accelerating Stochastic Gradient Descent for Least Squares Regression",
    "abstract": "There is widespread sentiment that fast gradient methods (e.g. Nesterov’s acceleration, conjugate gradient, heavy ball) are not effective for the purposes of stochastic optimization due to their instability and error accumulation. Numerous works have attempted to quantify these instabilities in the face of either statistical or non-statistical. This work considers these issues for the special case of stochastic approximation for the least squares regression problem, and our main result refutes this conventional wisdom by showing that acceleration can be made robust to statistical errors.  In particular, this work introduces an accelerated stochastic gradient method that provably achieves the minimax optimal statistical risk faster than stochastic gradient descent.  Critical to the analysis is a sharp characterization of accelerated stochastic gradient descent as a stochastic process. We hope this characterization gives insights towards the broader question of designing simple and effective accelerated stochastic methods for more general convex and non-convex optimization problems",
    "volume": "main",
    "checked": true,
    "id": "2273422c70bdb5520c03ec34e7cce3435ece3a00",
    "citation_count": 82
  },
  "https://proceedings.mlr.press/v75/mou18a.html": {
    "title": "Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints",
    "abstract": "We study the generalization errors of \\emph{non-convex} regularized ERM procedures using Stochastic Gradient Langevin Dynamics (SGLD). Two theories are proposed with non-asymptotic discrete-time analysis, using stability and PAC-Bayesian theory respectively. The stability-based theory obtains a bound of $O\\left(\\frac{1}{n}L\\sqrt{\\beta T_N}\\right)$, where $L$ is Lipschitz parameter, $\\beta$ is inverse temperature, and $T_N$ is the sum of step sizes. For PAC-Bayesian theory, though the bound has a slower $O(1/\\sqrt{n})$ rate, the contribution of each step decays exponentially through time, and the uniform Lipschitz constant is also replaced by actual norms of gradients along the optimization trajectory. Our bounds have reasonable dependence on aggregated step sizes, and do not explicitly depend on dimensions, norms or other capacity measures of the parameter. The bounds characterize how the noises in the algorithm itself controls the statistical learning behavior in non-convex problems, without uniform convergence in the hypothesis space, which sheds light on the effect of training algorithms on the generalization error for deep neural networks",
    "volume": "main",
    "checked": true,
    "id": "b74f852a082b5c9396aa14d2c965b05a505340d1",
    "citation_count": 106
  },
  "https://proceedings.mlr.press/v75/yarotsky18a.html": {
    "title": "Optimal approximation of continuous functions by very deep ReLU networks",
    "abstract": "We consider approximations of general continuous functions on finite-dimensional cubes by general deep ReLU neural networks and study the approximation rates with respect to the modulus of continuity of the function and the total number of weights $W$ in the network. We establish the complete phase diagram of feasible approximation rates and show that it includes two distinct phases. One phase corresponds to slower approximations that can be achieved with constant-depth networks and continuous weight assignments. The other phase provides faster approximations at the cost of depths necessarily growing as a power law $L\\sim W^{\\alpha}, 0<\\alpha\\le 1,$ and with necessarily discontinuous weight assignments. In particular, we prove that constant-width fully-connected networks of depth $L\\sim W$ provide the fastest possible approximation rate $\\|f-\\widetilde f\\|_\\infty = O(\\omega_f(O(W^{-2/\\nu})))$ that cannot be achieved with less deep networks",
    "volume": "main",
    "checked": true,
    "id": "71100f4c11a43137d316c93ba313e584cd111acf",
    "citation_count": 179
  },
  "https://proceedings.mlr.press/v75/tripuraneni18a.html": {
    "title": "Averaging Stochastic Gradient Descent on Riemannian Manifolds",
    "abstract": "We consider the minimization of a function defined on a Riemannian manifold $\\mathcal{M}$ accessible only through unbiased estimates of its gradients. We develop a geometric framework to transform a sequence of slowly converging iterates generated from stochastic gradient descent (SGD) on $\\mathcal{M}$ to an averaged iterate sequence with a robust and fast $O(1/n)$ convergence rate. We then present an application of our framework to geodesically-strongly-convex (and possibly Euclidean non-convex) problems.  Finally, we demonstrate how these ideas apply to the case of streaming $k$-PCA, where we show how to accelerate the slow rate of the randomized power method (without requiring knowledge of the eigengap) into a robust algorithm achieving the optimal rate of convergence",
    "volume": "main",
    "checked": true,
    "id": "046af04e7c45b90287862a0104f699dc32c18779",
    "citation_count": 67
  },
  "https://proceedings.mlr.press/v75/fefferman18a.html": {
    "title": "Fitting a Putative Manifold to Noisy Data",
    "abstract": "In the present work,  we give a solution to the following  question from manifold learning.  Suppose data belonging to a high dimensional Euclidean space is drawn independently, identically distributed  from a measure supported on a low dimensional twice  differentiable embedded manifold $M$, and corrupted by a small amount of  gaussian noise. How can we produce a manifold $M’$ whose Hausdorff distance to $M$ is small and whose reach is not much smaller than the reach of $M$?",
    "volume": "main",
    "checked": true,
    "id": "8d7978b7b34ffa0497a6e6f90c685698c7bc63e1",
    "citation_count": 23
  },
  "https://proceedings.mlr.press/v75/tsitsiklis18a.html": {
    "title": "Private Sequential Learning",
    "abstract": "We formulate a private learning model to study an intrinsic tradeoff between privacy and query complexity in sequential learning. Our model involves a learner who aims to determine a scalar value, $v^*$, by sequentially querying an external database and receiving binary responses. In the meantime, an adversary observes the learner’s queries, though not the responses, and tries to infer from them the value of $v^*$. The objective of the learner is to obtain an accurate estimate of $v^*$ using only a small number of queries, while simultaneously protecting her privacy by making $v^*$ provably difficult to learn for the adversary. Our main results provide tight upper and lower bounds on the learner’s query complexity as a function of desired levels of privacy and estimation accuracy. We also construct explicit query strategies whose complexity is optimal up to an additive constant",
    "volume": "main",
    "checked": false,
    "id": "804b1090a79eba01ad871f8d142184605ea97745",
    "citation_count": 9
  },
  "https://proceedings.mlr.press/v75/barbier18a.html": {
    "title": "Optimal Errors and Phase Transitions in High-Dimensional Generalized Linear Models",
    "abstract": "Generalized linear models (GLMs) arise in high-dimensional machine learning, statistics, communications and signal processing. % In this paper we analyze GLMs when the data matrix is random, as relevant in problems such as compressed sensing, error-correcting codes or benchmarks models in neural networks. % We evaluate the mutual information (or “free entropy”) from which we deduce the Bayes-optimal inference and generalization errors.  Our analysis applies to the high-dimensional limit where both the number of samples and dimensions are large and their ratio is fixed. % Non-rigorous predictions for the optimal inference and generalization errors existed for special cases of GLMs, e.g. for the perceptron in the field of statistical physics based on the so-called replica method. Our present paper rigorously establishes those decades old conjectures and brings forward their algorithmic interpretation in terms of performance of the generalized approximate message-passing algorithm. % Furthermore, we tightly characterize, for many learning problems, regions of parameters for which this algorithm achieves the optimal performance, and locate the associated sharp phase transitions separating learnable and non-learnable regions",
    "volume": "main",
    "checked": true,
    "id": "998086d6d735fe7df6569bb3d570e112f84c0057",
    "citation_count": 151
  },
  "https://proceedings.mlr.press/v75/chernozhukov18a.html": {
    "title": "Exact and Robust Conformal Inference Methods for Predictive Machine Learning with Dependent Data",
    "abstract": "We extend conformal inference to general settings that allow for time series data. Our proposal is developed as a randomization method and  accounts for potential serial dependence by including  block structures in the permutation scheme. As a result, the proposed method retains the exact, model-free validity when the data are i.i.d. or more generally exchangeable, similar to usual conformal inference methods.  When exchangeability fails, as is the case for common time series data, the proposed approach is approximately valid under weak assumptions on the conformity score",
    "volume": "main",
    "checked": true,
    "id": "b1d8a0b2488db67635c67a812d5395207382e7b4",
    "citation_count": 48
  },
  "https://proceedings.mlr.press/v75/cesa-bianchi18a.html": {
    "title": "Nonstochastic Bandits with Composite Anonymous Feedback",
    "abstract": "We investigate a nonstochastic bandit setting in which the loss of an action is not immediately charged to the player, but rather spread over at most d consecutive steps in an adversarial way. This implies that the instantaneous loss observed by the player at the end of each round is a sum of as many as d loss components of previously played actions. Hence, unlike the standard bandit setting with delayed feedback, here the player cannot observe the individual delayed losses, but only their sum. Our main contribution is a general reduction transforming a standard bandit algorithm into one that can operate in this harder setting. We also show how the regret of the transformed algorithm can be bounded in terms of the regret of the original algorithm. Our reduction cannot be improved in general: we prove a lower bound on the regret of any bandit algorithm in this setting that matches (up to log factors) the upper bound obtained via our reduction. Finally, we show how our reduction can be extended to more complex bandit settings, such as combinatorial linear bandits and online bandit convex optimization",
    "volume": "main",
    "checked": true,
    "id": "b6c2b0d22920509b0d8dee1f1b55dfad372b9aa5",
    "citation_count": 30
  },
  "https://proceedings.mlr.press/v75/agarwal18a.html": {
    "title": "Lower Bounds for Higher-Order Convex Optimization",
    "abstract": "State-of-the-art methods in mathematical optimization employ higher-order derivative information. We explore the limitations of higher-order optimization and prove that even for convex optimization, a polynomial dependence on the approximation guarantee and higher-order smoothness parameters is necessary. This refutes the hope that higher-order smoothness and higher-order derivatives can lead to dimension free polynomial time algorithms for convex optimization. As a special case, we show Nesterov’s accelerated cubic regularization method and higher-order methods to be nearly tight",
    "volume": "main",
    "checked": true,
    "id": "d32d682349b09b2d3767448cfd6fafd199ccf92a",
    "citation_count": 33
  },
  "https://proceedings.mlr.press/v75/dwivedi18a.html": {
    "title": "Log-concave sampling: Metropolis-Hastings algorithms are fast!",
    "abstract": "We consider the problem of sampling from a strongly log-concave density in $\\mathbb{R}^d$, and prove a non-asymptotic upper bound on the mixing time of the Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by running a Markov chain obtained from the discretization of an appropriate Langevin diffusion, combined with an accept-reject step to ensure the correct stationary distribution. Relative to known guarantees for the unadjusted Langevin algorithm (ULA), our bounds reveal that the use of an accept-reject step in MALA leads to an exponentially improved dependence on the error-tolerance. Concretely, in order to obtain samples with TV error at most $\\delta$ for a density with condition number $\\kappa$, we show that MALA requires $\\mathcal{O} \\big(\\kappa d \\log(1/\\delta) \\big)$ steps, as compared to the $\\mathcal{O} \\big(\\kappa^2 d/\\delta^2 \\big)$ steps established in past work on ULA.  We also demonstrate the gains of MALA over ULA for weakly log-concave densities.  Furthermore, we derive mixing time bounds for a zeroth-order method Metropolized random walk (MRW) and show that it mixes $\\mathcal{O}(\\kappa d)$ slower than MALA",
    "volume": "main",
    "checked": true,
    "id": "faeac34337de9dd1cd554870725dd51630ef218e",
    "citation_count": 166
  },
  "https://proceedings.mlr.press/v75/chen18a.html": {
    "title": "Incentivizing Exploration by Heterogeneous Users",
    "abstract": "We consider the problem of incentivizing exploration with heterogeneous agents. In this problem, $N$ bandit arms provide vector-valued outcomes equal to an unknown arm-specific attribute vector, perturbed by independent noise.Agents arrive sequentially and choose arms to pull based on their own private and heterogeneous linear utility functions over attributes and the estimates of the arms’ attribute vectors derived from observations of other agents’ past pulls. Agents are myopic and selfish and thus would choose the arm with maximum estimated utility. A principal, knowing only the distribution from which agents’ preferences are drawn, but not the specific draws, can offer arm-specific incentive payments to encourage agents to explore underplayed arms. The principal seeks to minimize the total expected cumulative regret incurred by agents relative to their best arms, while also making a small expected cumulative payment. We propose an algorithm that incentivizes arms played infrequently in the past whose probability of being played in the next round would be small without incentives. Under the assumption that each arm is preferred by at least a fraction $p > 0$ of agents, we show that this algorithm achieves expected cumulative regret of $O (N \\e^{2/p} + N \\log^3(T))$, using expected cumulative payments of $O(N^2 \\e^{2/p})$. If $p$ is known or the distribution over agent preferences is discrete, the exponential term $\\e^{2/p}$ can be replaced with suitable polynomials in $N$ and $1/p$. For discrete preferences, the regret’s dependence on $T$ can be eliminated entirely, giving constant (depending only polynomially on $N$ and $1/p$) expected regret and payments. This constant regret stands in contrast to the $\\Theta(\\log(T))$ dependence of regret in standard multi-armed bandit problems. It arises because even unobserved heterogeneity in agent preferences causes exploitation of arms to also explore arms fully; succinctly, heterogeneity provides free exploration",
    "volume": "main",
    "checked": true,
    "id": "62502f632a1a96aaf41f898c53f4c228ca727a68",
    "citation_count": 20
  },
  "https://proceedings.mlr.press/v75/diakonikolas18a.html": {
    "title": "Fast and Sample Near-Optimal Algorithms for Learning Multidimensional Histograms",
    "abstract": "We study the problem of robustly learning multi-dimensional histograms.  A $d$-dimensional function $h: D \\to \\R$ is called a $k$-histogram if there exists a partition of the  domain $D \\subseteq \\R^d$ into $k$ axis-aligned rectangles such that $h$ is constant within each such rectangle. Let $f: D \\to \\R$ be a $d$-dimensional probability density function  and suppose that $f$ is $\\mathrm{OPT}$-close, in $L_1$-distance,  to an unknown $k$-histogram (with unknown partition). Our goal is to output a hypothesis that is $O(\\mathrm{OPT}) + \\epsilon$ close to $f$, in $L_1$-distance. We give an algorithm for this learning  problem that uses  $n = \\tilde{O}_d(k/\\eps^2)$ samples and runs in time $\\tilde{O}_d(n)$. For any fixed dimension, our algorithm has optimal sample complexity, up to logarithmic factors, and runs in near-linear time. Prior to our work, the time complexity of the $d=1$ case was well-understood,  but significant gaps in our understanding remained even for $d=2$",
    "volume": "main",
    "checked": true,
    "id": "4a0df26d60b3e0b905b41d8a83b28a91907bf1e0",
    "citation_count": 15
  },
  "https://proceedings.mlr.press/v75/beame18a.html": {
    "title": "Time-Space Tradeoffs for Learning Finite Functions from Random Evaluations, with Applications to Polynomials",
    "abstract": "We develop an extension of recent analytic methods for  obtaining time-space tradeoff lower bounds for problems of learning  from uniformly random labelled examples.  With our methods we can obtain bounds for learning concept classes of finite functions from random evaluations even when the sample space of random inputs can be significantly smaller than the concept class of functions and the function values can be from an arbitrary finite set. At the core of our results, we reduce the time-space complexity of learning from random evaluations to the question of how much the corresponding evaluation matrix amplifies the 2-norms of “almost uniform” probability distributions. To analyze the latter, we formulate it as a semidefinite program, and we analyze its dual.   In order to handle function values from arbitrary finite sets, we apply this norm amplification analysis to complex matrices. As applications that follow from our new techniques, we show that any algorithm that learns $n$-variate  polynomial functions of degree at most $d$ over $\\mathbb{F}_2$ with success at least $2^{-O(n)}$ from evaluations on randomly chosen inputs either requires space $\\Omega(nm/d)$ or $2^{\\Omega(n/d)}$ time where $m=(n/d)^{\\Theta(d)}$ is the dimension of the space of such polynomials.   These bounds are asymptotically optimal for polynomials of arbitrary constant degree since they match the tradeoffs achieved by natural learning algorithms for the problems. We extend these results to learning polynomials of degree at most $d$ over any odd prime field $\\mathbb{F}_p$ where we show that $\\Omega((mn/d)\\log p)$ space or time $p^{\\Omega(n/d)}$ is required. To derive our bounds for learning polynomials over finite fields, we show that an analysis of the dual of the corresponding semidefinite program follows from an understanding of the distribution of the bias of all degree $d$ polynomials with respect to uniformly random inputs",
    "volume": "main",
    "checked": true,
    "id": "0cc359939ef4c5add083f3f1416e5676c1ab64f2",
    "citation_count": 19
  },
  "https://proceedings.mlr.press/v75/tzen18a.html": {
    "title": "Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability",
    "abstract": "We study the detailed path-wise behavior of the discrete-time Langevin algorithm for non-convex Empirical Risk Minimization (ERM) through the lens of metastability, adopting some techniques from Berglund and Gentz (2003). For a particular local optimum of the empirical risk, with an \\textit{arbitrary initialization}, we show that, with high probability, at least one of the following two events will occur: (1) the Langevin trajectory ends up somewhere outside the $\\varepsilon$-neighborhood of this particular optimum within a short \\textit{recurrence time}; (2) it enters this $\\varepsilon$-neighborhood by the recurrence time and stays there until a potentially exponentially long \\textit{escape time}. We call this phenomenon \\textit{empirical metastability}. This two-timescale characterization aligns nicely with the existing literature in the following two senses. First, the effective recurrence time (i.e., number of iterations multiplied by stepsize) is dimension-independent, and resembles the convergence time of continuous-time deterministic Gradient Descent (GD). However unlike GD, the Langevin algorithm does not require strong conditions on local initialization, and has the possibility of eventually visiting all optima. Second, the scaling of the escape time is consistent with the Eyring-Kramers law, which states that the Langevin scheme will eventually visit all local minima, but it will take an exponentially long time to transit among them. We apply this path-wise concentration result in the context of statistical learning to examine local notions of generalization and optimality",
    "volume": "main",
    "checked": true,
    "id": "b8da32456a1eb9065836f36c232c75b84430e9c3",
    "citation_count": 28
  },
  "https://proceedings.mlr.press/v75/bhattacharyya18a.html": {
    "title": "Hardness of Learning Noisy Halfspaces using Polynomial Thresholds",
    "abstract": "We prove the hardness of weakly learning halfspaces in the presence of adversarial noise using polynomial threshold functions (PTFs). In particular, we prove that for any constants $d \\in \\mathbb{Z}^+$ and $\\eps > 0$, it is NP-hard to decide: given a set of $\\{-1,1\\}$-labeled points in $\\mathbb{R}^n$  whether (YES Case) there exists a halfspace that classifies $(1-\\eps)$-fraction of the points correctly, or (NO Case) any degree-$d$ PTF classifies at most $(1/2 + \\eps)$-fraction of the points correctly. This strengthens to all constant degrees the previous NP-hardness of learning using degree-$2$ PTFs shown by Diakonikolas et al. (2011). The latter result had remained the only progress over the works of Feldman et al. (2006) and Guruswami et al. (2006) ruling out weakly proper learning adversarially noisy halfspaces",
    "volume": "main",
    "checked": true,
    "id": "fb666ceea96b98bb25f75da5f5472dde6f5bcfee",
    "citation_count": 7
  },
  "https://proceedings.mlr.press/v75/abbasi-yadkori18a.html": {
    "title": "Best of both worlds: Stochastic & adversarial best-arm identification",
    "abstract": "We study bandit best-arm identification with arbitrary and potentially adversarial rewards. A simple random uniform learner obtains the optimal rate of error in the adversarial scenario. However, this type of strategy is suboptimal when the rewards are sampled stochastically. Therefore, we ask: $\\backslash$emph{\\{}Can we design a learner that performs optimally in both the stochastic and adversarial problems while not being aware of the nature of the rewards?{\\}} First, we show that designing such a learner is impossible in general. In particular, to be robust to adversarial rewards, we can only guarantee optimal rates of error on a subset of the stochastic problems. We give a lower bound that characterizes the optimal rate in stochastic problems if the strategy is constrained to be robust to adversarial rewards. Finally, we design a simple parameter-free algorithm and show that its probability of error matches (up to log factors) the lower bound in stochastic problems, and it is also robust to adversarial ones",
    "volume": "main",
    "checked": true,
    "id": "a5875901db2699b27b3670672d6c9a3fffdbb3d0",
    "citation_count": 34
  },
  "https://proceedings.mlr.press/v75/sharpnack18a.html": {
    "title": "Learning Patterns for Detection with Multiscale Scan Statistics",
    "abstract": "This paper addresses detecting anomalous patterns in images, time-series, and tensor data when the location and scale of the pattern and the pattern itself is unknown a priori. The multiscale scan statistic convolves the proposed pattern with the image at various scales and returns the maximum of the resulting tensor. Scale corrected multiscale scan statistics apply different standardizations at each scale, and the limiting distribution under the null hypothesis—that the data is only noise—is known for smooth patterns.  We consider the problem of simultaneously learning and detecting the anomalous pattern from a dictionary of smooth patterns and a database of many tensors. To this end, we show that the multiscale scan statistic is a subexponential random variable, and prove a chaining lemma for standardized suprema, which may be of independent interest. Then by averaging the statistics over the database of tensors we can learn the pattern and obtain Bernstein-type error bounds. We will also provide a construction of an $\\epsilon$-net of the location and scale parameters, providing a computationally tractable approximation with similar error bounds",
    "volume": "main",
    "checked": true,
    "id": "156e8a5adf9dcdbefc4332110c4c7539c8ed8474",
    "citation_count": 3
  },
  "https://proceedings.mlr.press/v75/hand18a.html": {
    "title": "Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk",
    "abstract": "We examine the theoretical properties of enforcing priors provided by generative deep neural networks via empirical risk minimization. In particular we consider two models, one in which the task is to invert a generative neural network given access to its last layer and another in which the task is to invert a generative neural network given only compressive linear observations of its last layer.  We establish that in both cases, in suitable regimes of network layer sizes and a randomness assumption on the network weights, that the non-convex objective function given by empirical risk minimization does not have any spurious stationary points. That is, we establish that with high probability, at any point away from small neighborhoods around two scalar multiples of the desired solution, there is a descent direction. Hence, there are no local minima, saddle points, or other stationary points outside these neighborhoods.  These results constitute the first theoretical guarantees which establish the favorable global geometry of these non-convex optimization problems, and they bridge the gap between the empirical success of  enforcing deep generative priors and a rigorous understanding of non-linear inverse problems",
    "volume": "main",
    "checked": true,
    "id": "7f040fe8c563e7e3fb358f234e05282b57a9ca6e",
    "citation_count": 122
  },
  "https://proceedings.mlr.press/v75/lykouris18a.html": {
    "title": "Small-loss bounds for online learning with partial information",
    "abstract": "We consider the problem of adversarial (non-stochastic) online learning with partial information feedback, where at each round, a decision maker selects an action from a finite set of alternatives. We develop a black-box approach for such problems where the learner observes as feedback only losses of a subset of the actions that includes the selected action. When losses of actions are non-negative, under the graph-based feedback model introduced by Mannor and Shamir, we offer algorithms that attain the so called “small-loss” $o(\\alpha L^{\\star})$ regret bounds with high probability, where $\\alpha$ is the independence number of the graph, and $L^{\\star}$ is the loss of the best action. Prior to our work, there was no data-dependent guarantee for general feedback graphs even for pseudo-regret (without dependence on the number of actions, i.e. utilizing the increased information feedback). Taking advantage of the black-box nature of our technique, we extend our results to many other applications such as semi-bandits (including routing in networks), contextual bandits (even with an infinite comparator class), as well as learning with slowly changing (shifting) comparators. In the special case of classical bandit and semi-bandit problems, we provide optimal small-loss,  high-probability guarantees of $\\tilde{O}(\\sqrt{dL^{\\star}})$ for actual regret, where $d$ is the number of actions, answering open questions of Neu.  Previous bounds for bandits and semi-bandits were known only for pseudo-regret and only in expectation. We also offer an optimal $\\tilde{O}(\\sqrt{\\kappa L^{\\star}})$ regret guarantee for fixed feedback graphs with clique-partition number at most $\\kappa$",
    "volume": "main",
    "checked": true,
    "id": "95f623cd01aaa579319ea22bffa373eff756ba6a",
    "citation_count": 32
  },
  "https://proceedings.mlr.press/v75/maurer18a.html": {
    "title": "Empirical bounds for functions with weak interactions",
    "abstract": "We provide sharp empirical estimates of expectation, variance and normal approximation for a class of statistics whose variation in any argument does not change too much when another argument is modified. Examples of such weak interactions are furnished by U- and V-statistics, Lipschitz L-statistics and various error functionals of $\\ell_2$-regularized algorithms and Gibbs algorithms",
    "volume": "main",
    "checked": true,
    "id": "b18ebe02f30b183f4aa54efc4c6d8a2badbd41a0",
    "citation_count": 7
  },
  "https://proceedings.mlr.press/v75/kasiviswanathan18a.html": {
    "title": "Restricted Eigenvalue from Stable Rank with Applications to Sparse Linear Regression",
    "abstract": "High-dimensional settings, where the data dimension ($d$) far exceeds the number of observations ($n$), are common in many statistical and machine learning applications. Methods based on $\\ell_1$-relaxation, such as Lasso, are very popular for sparse recovery in these settings. Restricted Eigenvalue (RE) condition is among the weakest, and hence the most general, condition in literature imposed on the Gram matrix that guarantees nice statistical properties for the Lasso estimator. It is hence natural to ask: what families of matrices satisfy the RE condition?  Following a line of work in this area, we construct a new broad ensemble of dependent random design matrices that have an explicit RE bound. Our construction starts with a fixed (deterministic) matrix $X \\in \\mathbb{R}^{n \\times d}$ satisfying a simple stable rank condition, and we show that a matrix drawn from the distribution $X \\Phi^\\top \\Phi$, where $\\Phi \\in \\mathbb{R}^{m \\times d}$ is a subgaussian random matrix, with high probability, satisfies the RE condition. This construction allows incorporating a fixed matrix that has an easily {\\em verifiable} condition into the design process, and allows for generation of {\\em compressed} design matrices that have a lower storage requirement than a standard design matrix. We give two applications of this construction to sparse linear regression problems, including one to a compressed sparse regression setting where the regression algorithm only has access to a compressed representation of a fixed design matrix $X$",
    "volume": "main",
    "checked": true,
    "id": "fbe08db7bdcf6b469696a78084f1ef44e7e79ef1",
    "citation_count": 2
  },
  "https://proceedings.mlr.press/v75/jin18a.html": {
    "title": "Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent",
    "abstract": "Nesterov’s accelerated gradient descent (AGD), an instance of the general family of “momentum methods,” provably achieves faster convergence rate than gradient descent (GD) in the convex setting. While these methods are widely used in modern \\emph{nonconvex} applications, including training of deep neural networks, whether they are provably superior to GD in the nonconvex setting remains open. This paper studies a simple variant of Nesterov’s AGD, and shows that it escapes saddle points and finds a second-order stationary point in $\\tilde{O}(1/\\epsilon^{7/4})$ iterations, matching the best known convergence rate, which is faster than the $\\tilde{O}(1/\\epsilon^{2})$ iterations required by GD. To the best of our knowledge, this is the first direct acceleration (single-loop) algorithm that is provably faster than GD in general nonconvex setting—all previous nonconvex accelerated algorithms rely on more complex mechanisms such as nested loops and proximal terms. Our analysis is based on two key ideas: (1) the use of a simple Hamiltonian function, inspired by a continuous-time perspective, which AGD monotonically decreases on each step even for nonconvex functions, and (2) a novel framework called \\emph{improve or localize}, which is useful for tracking the long-term behavior of gradient-based optimization algorithms. We believe that these techniques may deepen our understanding of both acceleration algorithms and nonconvex optimization",
    "volume": "main",
    "checked": true,
    "id": "0054eb42d8ad63b1988fe32df6284762619dcada",
    "citation_count": 216
  },
  "https://proceedings.mlr.press/v75/mangoubi18a.html": {
    "title": "Convex Optimization with Unbounded Nonconvex Oracles using Simulated Annealing",
    "abstract": "We consider the problem of minimizing a convex objective function $F$ when one can only evaluate its noisy approximation $\\hat{F}$. Unless one assumes some structure on the noise, $\\hat{F}$ may be an arbitrary nonconvex function, making the task of minimizing $F$ intractable. To overcome this, prior work has often focused on the case when $F(x)-\\hat{F}(x)$ is uniformly-bounded. In this paper we study the more general case when the noise has magnitude $\\alpha F(x) + \\beta$ for some $\\alpha, \\beta > 0$, and present a polynomial time algorithm that finds an approximate minimizer of $F$ for this noise model. Previously, Markov chains, such as the stochastic gradient Langevin dynamics, have been used to arrive at approximate solutions to these optimization problems. However, for the noise model considered in this paper, no single temperature allows such a Markov chain to both mix quickly and concentrate near the global minimizer. We bypass this by combining “simulated annealing\" with the stochastic gradient Langevin dynamics, and gradually decreasing the temperature of the chain in order to approach the global minimizer. As a corollary one can approximately minimize a nonconvex function that is close to a convex function; however, the closeness can deteriorate as one moves away from the optimum",
    "volume": "main",
    "checked": true,
    "id": "29bcac6a49641907334c8b4ac3e1a427bc2838ed",
    "citation_count": 11
  },
  "https://proceedings.mlr.press/v75/li18b.html": {
    "title": "Learning Mixtures of Linear Regressions with Nearly Optimal Complexity",
    "abstract": "Mixtures of Linear Regressions (MLR) is an important mixture model with many applications. In this model, each observation is generated from one of the several unknown linear regression components, where the identity of the generated component is also unknown. Previous works either assume strong assumptions on the data distribution or have high complexity. This paper proposes a fixed parameter tractable algorithm for the problem under general conditions, which achieves global convergence and the sample complexity scales nearly linearly in the dimension. In particular, different from previous works that require the data to be from the standard Gaussian, the algorithm allows the data from Gaussians with different covariances. When the conditional number of the covariances and the number of components are fixed, the algorithm has nearly optimal sample complexity $N = \\tilde{O}(d)$ as well as nearly optimal computational complexity $\\tilde{O}(Nd)$, where $d$ is the dimension of the data space. To the best of our knowledge, this approach provides the first such recovery guarantee for this general setting",
    "volume": "main",
    "checked": true,
    "id": "36ce8a72aa1538a5895d8fc872bf557f2f0da17c",
    "citation_count": 47
  }
}