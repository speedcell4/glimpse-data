{
  "https://jmlr.org/papers/v24/18-080.html": {
    "title": "Approximation Bounds for Hierarchical Clustering: Average Linkage, Bisecting K-means, and Local Search",
    "volume": "main",
    "abstract": "Hierarchical clustering is a data analysis method that has been used for decades. Despite its widespread use, the method has an underdeveloped analytical foundation. Having a well-understood foundation would both support the currently used methods and help guide future improvements. The goal of this paper is to give an analytic framework to better understand observations seen in practice. This paper considers the dual of a problem framework for hierarchical clustering introduced by Dasgupta. The main result is that one of the most popular algorithms used in practice, average linkage agglomerative clustering, has a small constant approximation ratio for this objective. To contrast, this paper establishes that using several other popular algorithms, including bisecting $k$-means divisive clustering, have a very poor lower bound on its approximation ratio for the same objective. However, we show that there are divisive algorithms that perform well with respect to this objective by giving two constant approximation algorithms. This paper is some of the first work to establish guarantees on widely used hierarchical algorithms for a natural objective function. This objective and analysis give insight into what these popular algorithms are optimizing and when they will perform well",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/19-1030.html": {
    "title": "The Brier Score under Administrative Censoring: Problems and a Solution",
    "volume": "main",
    "abstract": "The Brier score is commonly used for evaluating probability predictions. In survival analysis, with right-censored observations of the event times, this score can be weighted by the inverse probability of censoring (IPCW) to retain its original interpretation. It is common practice to estimate the censoring distribution with the Kaplan-Meier estimator, even though it assumes that the censoring distribution is independent of the covariates. This paper investigates problems that may arise for the IPCW weighting scheme when the covariates used in the prediction model contain information about the censoring times. In particular, this may occur for administratively censored data if the distribution of the covariates varies with calendar time. For administratively censored data, we propose an alternative version of the Brier score. This administrative Brier score does not require estimation of the censoring distribution and is valid also when the censoring times can be predicted from the covariates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1206.html": {
    "title": "Bayesian Spiked Laplacian Graphs",
    "volume": "main",
    "abstract": "In network analysis, it is common to work with a collection of graphs that exhibit heterogeneity. For example, neuroimaging data from patient cohorts are increasingly available. A critical analytical task is to identify communities, and graph Laplacian-based methods are routinely used. However, these methods are currently limited to a single network and also do not provide measures of uncertainty on the community assignment. In this work, we first propose a probabilistic network model called the \"Spiked Laplacian Graph\" that considers an observed network as a transform of the Laplacian and degree matrices of the network generating process, with the Laplacian eigenvalues modeled by a modified spiked structure. This effectively reduces the number of parameters in the eigenvectors, and their sign patterns allow efficient estimation of the underlying community structure. Further, the posterior distribution of the eigenvectors provides uncertainty quantification for the community estimates. Second, we introduce a Bayesian non-parametric approach to address the issue of heterogeneity in a collection of graphs. Theoretical results are established on the posterior consistency of the procedure and provide insights on the trade-off between model resolution and accuracy. We illustrate the performance of the methodology on synthetic data sets, as well as a neuroscience study related to brain activity in working memory",
    "checked": true,
    "id": "a204c6e116c9d8889f0eb241b75c1ef4a8488720",
    "semantic_title": "bayesian spiked laplacian graphs",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1310.html": {
    "title": "Efficient Structure-preserving Support Tensor Train Machine",
    "volume": "main",
    "abstract": "An increasing amount of the collected data are high-dimensional multi-way arrays (tensors), and it is crucial for efficient learning algorithms to exploit this tensorial structure as much as possible. The ever present curse of dimensionality for high dimensional data and the loss of structure when vectorizing the data motivates the use of tailored low-rank tensor classification methods. In the presence of small amounts of training data, kernel methods offer an attractive choice as they provide the possibility for a nonlinear decision boundary. We develop the Tensor Train Multi-way Multi-level Kernel (TT-MMK), which combines the simplicity of the Canonical Polyadic decomposition, the classification power of the Dual Structure-preserving Support Vector Machine, and the reliability of the Tensor Train (TT) approximation. We show by experiments that the TT-MMK method is usually more reliable computationally, less sensitive to tuning parameters, and gives higher prediction accuracy in the SVM classification when benchmarked against other state-of-the-art techniques",
    "checked": false,
    "id": "2ada3b23b7f3b2533931ff87f553266a1074c23d",
    "semantic_title": "structure-preserving graph kernel for brain network classification",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1321.html": {
    "title": "Cluster-Specific Predictions with Multi-Task Gaussian Processes",
    "volume": "main",
    "abstract": "A model involving Gaussian processes (GPs) is introduced to simultaneously handle multitask learning, clustering, and prediction for multiple functional data. This procedure acts as a model-based clustering method for functional data as well as a learning step for subsequent predictions for new tasks. The model is instantiated as a mixture of multi-task GPs with common mean processes. A variational EM algorithm is derived for dealing with the optimisation of the hyper-parameters along with the hyper-posteriors' estimation of latent variables and processes. We establish explicit formulas for integrating the mean processes and the latent clustering variables within a predictive distribution, accounting for uncertainty in both aspects. This distribution is defined as a mixture of cluster-specific GP predictions, which enhances the performance when dealing with group-structured data. The model handles irregular grids of observations and offers different hypotheses on the covariance structure for sharing additional information across tasks. The performances on both clustering and prediction tasks are assessed through various simulated scenarios and real data sets. The overall algorithm, called MagmaClust, is publicly available as an R package",
    "checked": false,
    "id": "d26de028b60654881b59f7cd073bc773c2bc2bd5",
    "semantic_title": "cluster-speciÔ¨Åc predictions with multi-task gaussian processes",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1355.html": {
    "title": "AutoKeras: An AutoML Library for Deep Learning",
    "volume": "MLOSS",
    "abstract": "To use deep learning, one needs to be familiar with various software tools like TensorFlow or Keras, as well as various model architecture and optimization best practices. Despite recent progress in software usability, deep learning remains a highly specialized occupation. To enable people with limited machine learning and programming experience to adopt deep learning, we developed AutoKeras, an Automated Machine Learning (AutoML) library that automates the process of model selection and hyperparameter tuning. AutoKeras encapsulates the complex process of building and training deep neural networks into a very simple and accessible interface, which enables novice users to solve standard machine learning problems with a few lines of code. Designed with practical applications in mind, AutoKeras is built on top of Keras and TensorFlow, and all AutoKeras-created models can be easily exported and deployed with the help of the TensorFlow ecosystem tooling",
    "checked": true,
    "id": "67f6178c0c115f55bcede4a82e956a7402d0533d",
    "semantic_title": "autokeras: an automl library for deep learning",
    "citation_count": 13,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-238.html": {
    "title": "On Distance and Kernel Measures of Conditional Dependence",
    "volume": "main",
    "abstract": "Measuring conditional dependence is one of the important tasks in statistical inference and is fundamental in causal discovery, feature selection, dimensionality reduction, Bayesian network learning, and others. In this work, we explore the connection between conditional dependence measures induced by distances on a metric space and reproducing kernels associated with a reproducing kernel Hilbert space (RKHS). For certain distance and kernel pairs, we show the distance-based conditional dependence measures to be equivalent to that of kernel-based measures. On the other hand, we also show that some popular kernel conditional dependence measures based on the Hilbert-Schmidt norm of a certain cross-conditional covariance operator, do not have a simple distance representation, except in some limiting cases",
    "checked": true,
    "id": "35d3a2e1399ee11c0238950c0990cd7c57d99f33",
    "semantic_title": "on distance and kernel measures of conditional dependence",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-267.html": {
    "title": "A Relaxed Inertial Forward-Backward-Forward Algorithm for Solving Monotone Inclusions with Application to GANs",
    "volume": "main",
    "abstract": "We introduce a relaxed inertial forward-backward-forward (RIFBF) splitting algorithm for approaching the set of zeros of the sum of a maximally monotone operator and a single-valued monotone and Lipschitz continuous operator. This work aims to extend Tseng's forward-backward-forward method by both using inertial effects as well as relaxation parameters. We formulate first a second order dynamical system that approaches the solution set of the monotone inclusion problem to be solved and provide an asymptotic analysis for its trajectories. We provide for RIFBF, which follows by explicit time discretization, a convergence analysis in the general monotone case as well as when applied to the solving of pseudo-monotone variational inequalities. We illustrate the proposed method by applications to a bilinear saddle point problem, in the context of which we also emphasize the interplay between the inertial and the relaxation parameters, and to the training of Generative Adversarial Networks (GANs)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-449.html": {
    "title": "Sampling random graph homomorphisms and applications to network data analysis",
    "volume": "main",
    "abstract": "A graph homomorphism is a map between two graphs that preserves adjacency relations. We consider the problem of sampling a random graph homomorphism from a graph into a large network. We propose two complementary MCMC algorithms for sampling random graph homomorphisms and establish bounds on their mixing times and the concentration of their time averages. Based on our sampling algorithms, we propose a novel framework for network data analysis that circumvents some of the drawbacks in methods based on independent and neighborhood sampling. Various time averages of the MCMC trajectory give us various computable observables, including well-known ones such as homomorphism density and average clustering coefficient and their generalizations. Furthermore, we show that these network observables are stable with respect to a suitably renormalized cut distance between networks. We provide various examples and simulations demonstrating our framework through synthetic networks. We also \\commHL{demonstrate the performance of} our framework on the tasks of network clustering and subgraph classification on the Facebook100 dataset and on Word Adjacency Networks of a set of classic novels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-608.html": {
    "title": "A Line-Search Descent Algorithm for Strict Saddle Functions with Complexity Guarantees",
    "volume": "main",
    "abstract": "We describe a line-search algorithm which achieves the best-known worst-case complexity results for problems with a certain \"strict saddle\" property that has been observed to hold in low-rank matrix optimization problems. Our algorithm is adaptive, in the sense that it makes use of backtracking line searches and does not require prior knowledge of the parameters that define the strict saddle property",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0048.html": {
    "title": "Optimal Strategies for Reject Option Classifiers",
    "volume": "main",
    "abstract": "In classification with a reject option, the classifier is allowed in uncertain cases to abstain from prediction. The classical cost-based model of a reject option classifier requires the rejection cost to be defined explicitly. The alternative bounded-improvement model and the bounded-abstention model avoid the notion of the reject cost. The bounded-improvement model seeks a classifier with a guaranteed selective risk and maximal cover. The bounded-abstention model seeks a classifier with guaranteed cover and minimal selective risk. We prove that despite their different formulations the three rejection models lead to the same prediction strategy: the Bayes classifier endowed with a randomized Bayes selection function. We define the notion of a proper uncertainty score as a scalar summary of the prediction uncertainty sufficient to construct the randomized Bayes selection function. We propose two algorithms to learn the proper uncertainty score from examples for an arbitrary black-box classifier. We prove that both algorithms provide Fisher consistent estimates of the proper uncertainty score and demonstrate their efficiency in different prediction problems, including classification, ordinal regression, and structured output classification",
    "checked": true,
    "id": "c80e05ed6b487960cfda900097f2f796343ee871",
    "semantic_title": "optimal strategies for reject option classifiers",
    "citation_count": 11,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0096.html": {
    "title": "Learning-augmented count-min sketches via Bayesian nonparametrics",
    "volume": "main",
    "abstract": "The count-min sketch (CMS) is a time and memory efficient randomized data structure that provides estimates of tokens' frequencies in a data stream of tokens, i.e. point queries, based on random hashed data. A learning-augmented version of the CMS, referred to as CMS-DP, has been proposed by Cai, Mitzenmacher and Adams (NeurIPS 2018), and it relies on Bayesian nonparametric (BNP) modeling of the data stream of tokens via a Dirichlet process (DP) prior, with estimates of a point query being that are obtained as mean functionals of the posterior distribution of the point query, given the hashed data. While the CMS-DP has proved to improve on some aspects of CMS, it has the major drawback of arising from a \"constructive\" proof that builds upon arguments that are tailored to the DP prior, namely arguments that are not usable for other nonparametric priors. In this paper, we present a \"Bayesian\" proof of the CMS-DP that has the main advantage of building upon arguments that are usable under the popular Pitman-Yor process (PYP) prior, which generalizes the DP prior by allowing for a more flexible tail behaviour, ranging from geometric tails to heavy power-law tails. This result leads to develop a novel learning-augmented CMS under power-law data streams, referred to as CMS-PYP, which relies on BNP modeling of the data stream of tokens via a PYP prior. Under this more general framework, we apply the arguments of the \"Bayesian\" proof of the CMS-DP, suitably adapted to the PYP prior, in order to compute the posterior distribution of a point query, given the hashed data. Applications to synthetic data and real textual data show that the CMS-PYP outperforms the CMS and the CMS-DP in estimating low-frequency tokens, which are known to be of critical interest in textual data, and it is competitive with respect to a variation of the CMS designed to deal with the estimation of low-frequency tokens. An extension of our BNP approach to more general queries, such as range queries, is also discussed",
    "checked": true,
    "id": "f5468a9950aca621f36d7d48d19b64f55ed99891",
    "semantic_title": "learning-augmented count-min sketches via bayesian nonparametrics",
    "citation_count": 4,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0148.html": {
    "title": "Adaptation to the Range in K-Armed Bandits",
    "volume": "main",
    "abstract": "We consider stochastic bandit problems with $K$ arms, each associated with a distribution supported on a given finite range $[m,M]$. We do not assume that the range $[m,M]$ is known and show that there is a cost for learning this range. Indeed, a new trade-off between distribution-dependent and distribution-free regret bounds arises, which prevents from simultaneously achieving the typical $\\ln T$ and $\\sqrt{T}$ bounds. For instance, a $\\sqrt{T}$ distribution-free regret bound may only be achieved if the distribution-dependent regret bounds are at least of order $\\sqrt{T}$. We exhibit a strategy achieving the rates for regret imposed by the new trade-off",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0321.html": {
    "title": "Python package for causal discovery based on LiNGAM",
    "volume": "MLOSS",
    "abstract": "Causal discovery is a methodology for learning causal graphs from data, and LiNGAM is a well-known model for causal discovery. This paper describes an open-source Python package for causal discovery based on LiNGAM. The package implements various LiNGAM methods under different settings like time series cases, multiple-group cases, mixed data cases, and hidden common cause cases, in addition to evaluation of statistical reliability and model assumptions. The source code is freely available under the MIT license at https://github.com/cdt15/lingam",
    "checked": true,
    "id": "830d563efad3a7af28742d03f734d6da3b8ae04c",
    "semantic_title": "python package for causal discovery based on lingam",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0326.html": {
    "title": "Extending Adversarial Attacks to Produce Adversarial Class Probability Distributions",
    "volume": "main",
    "abstract": "Despite the remarkable performance and generalization levels of deep learning models in a wide range of artificial intelligence tasks, it has been demonstrated that these models can be easily fooled by the addition of imperceptible yet malicious perturbations to natural inputs. These altered inputs are known in the literature as adversarial examples. In this paper, we propose a novel probabilistic framework to generalize and extend adversarial attacks in order to produce a desired probability distribution for the classes when we apply the attack method to a large number of inputs. This novel attack paradigm provides the adversary with greater control over the target model, thereby exposing, in a wide range of scenarios, threats against deep learning models that cannot be conducted by the conventional paradigms. We introduce four different strategies to efficiently generate such attacks, and illustrate our approach by extending multiple adversarial attack algorithms. We also experimentally validate our approach for the spoken command classification task and the Tweet emotion classification task, two exemplary machine learning problems in the audio and text domain, respectively. Our results demonstrate that we can closely approximate any probability distribution for the classes while maintaining a high fooling rate and even prevent the attacks from being detected by label-shift detection methods",
    "checked": false,
    "id": "a7364e7d0ccb01a76a19ad94558e2b07dff6f8aa",
    "semantic_title": "weak superimposed codes of improved asymptotic rate and their randomized construction",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0488.html": {
    "title": "Globally-Consistent Rule-Based Summary-Explanations for Machine Learning Models: Application to Credit-Risk Evaluation",
    "volume": "main",
    "abstract": "We develop a method for understanding specific predictions made by (global) predictive models by constructing (local) models tailored to each specific observation (these are also called \"explanations\" in the literature). Unlike existing work that \"explains\" specific observations by approximating global models in the vicinity of these observations, we fit models that are globally-consistent with predictions made by the global model on past data. We focus on rule-based models (also known as association rules or conjunctions of predicates), which are interpretable and widely used in practice. We design multiple algorithms to extract such rules from discrete and continuous datasets, and study their theoretical properties. Finally, we apply these algorithms to multiple credit-risk models trained on the Explainable Machine Learning Challenge data from FICO and demonstrate that our approach effectively produces sparse summary-explanations of these models in seconds. Our approach is model-agnostic (that is, can be used to explain any predictive model), and solves a minimum set cover problem to construct its summaries",
    "checked": false,
    "id": "9c2af3bfe449496bda535b48581e5d4304f5f33b",
    "semantic_title": "heavy-head sampling strategy of graph convolutional neural networks for q-consistent summary-explanations with application to credit evaluation systems",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0505.html": {
    "title": "Learning Mean-Field Games with Discounted and Average Costs",
    "volume": "main",
    "abstract": "We consider learning approximate Nash equilibria for discrete-time mean-field games with stochastic nonlinear state dynamics subject to both average and discounted costs. To this end, we introduce a mean-field equilibrium (MFE) operator, whose fixed point is a mean-field equilibrium, i.e., equilibrium in the infinite population limit. We first prove that this operator is a contraction, and propose a learning algorithm to compute an approximate mean-field equilibrium by approximating the MFE operator with a random one. Moreover, using the contraction property of the MFE operator, we establish the error analysis of the proposed learning algorithm. We then show that the learned mean-field equilibrium constitutes an approximate Nash equilibrium for finite-agent games",
    "checked": true,
    "id": "e48aad7db9380dd5902fd4c2b58a13c1f72a9194",
    "semantic_title": "learning mean-field games with discounted and average costs",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0571.html": {
    "title": "An Inertial Block Majorization Minimization Framework for Nonsmooth Nonconvex Optimization",
    "volume": "main",
    "abstract": "In this paper, we introduce TITAN, a novel inerTIal block majorizaTion minimizAtioN ramework for nonsmooth nonconvex optimization problems. To the best of our knowledge, TITAN is the first framework of block-coordinate update method that relies on the majorization-minimization framework while embedding inertial force to each step of the block updates. The inertial force is obtained via an extrapolation operator that subsumes heavy-ball and Nesterov-type accelerations for block proximal gradient methods as special cases. By choosing various surrogate functions, such as proximal, Lipschitz gradient, Bregman, quadratic, and composite surrogate functions, and by varying the extrapolation operator, TITAN produces a rich set of inertial block-coordinate update methods. We study sub-sequential convergence as well as global convergence for the generated sequence of TITAN. We illustrate the effectiveness of TITAN on two important machine learning problems, namely sparse non-negative matrix factorization and matrix completion",
    "checked": false,
    "id": "253226282e873d8a851eb268d0e0389f30090ef8",
    "semantic_title": "inertial alternating direction method of multipliers for non-convex non-smooth optimization",
    "citation_count": 5,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0796.html": {
    "title": "Regularized Joint Mixture Models",
    "volume": "main",
    "abstract": "Regularized regression models are well studied and, under appropriate conditions, offer fast and statistically interpretable results. However, large data in many applications are heterogeneous in the sense of harboring distributional differences between latent groups. Then, the assumption that the conditional distribution of response $Y$ given features $X$ is the same for all samples may not hold. Furthermore, in scientific applications, the covariance structure of the features may contain important signals and its learning is also affected by latent group structure. We propose a class of mixture models for paired data $(X,Y)$ that couples together the distribution of $X$ (using sparse graphical models) and the conditional $Y \\! \\mid \\! X$ (using sparse regression models). The regression and graphical models are specific to the latent groups and model parameters are estimated jointly. This allows signals in either or both of the feature distribution and regression model to inform learning of latent structure and provides automatic control of confounding by such structure. Estimation is handled via an expectation-maximization algorithm, whose convergence is established theoretically. We illustrate the key ideas via empirical examples. An R package is available at https://github.com/k-perrakis/regjmix",
    "checked": false,
    "id": "83acd73e39a4c11dbbb06d49e9f9d7f631644a54",
    "semantic_title": "heterogeneous tensor mixture models in high dimensions",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0844.html": {
    "title": "Interpolating Classifiers Make Few Mistakes",
    "volume": "main",
    "abstract": "This paper provides elementary analyses of the regret and generalization of minimum-norm interpolating classifiers (MNIC). The MNIC is the function of smallest Reproducing Kernel Hilbert Space norm that perfectly interpolates a label pattern on a finite data set. We derive a mistake bound for MNIC and a regularized variant that holds for all data sets. This bound follows from elementary properties of matrix inverses. Under the assumption that the data is independently and identically distributed, the mistake bound implies that MNIC generalizes at a rate proportional to the norm of the interpolating solution and inversely proportional to the number of data points. This rate matches similar rates derived for margin classifiers and perceptrons. We derive several plausible generative models where the norm of the interpolating classifier is bounded or grows at a rate sublinear in $n$. We also show that as long as the population class conditional distributions are sufficiently separable in total variation, then MNIC generalizes with a fast rate",
    "checked": true,
    "id": "506f12d67779d4f5ffe510f2aecdb588fbf3aaef",
    "semantic_title": "interpolating classifiers make few mistakes",
    "citation_count": 23,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0877.html": {
    "title": "Graph-Aided Online Multi-Kernel Learning",
    "volume": "main",
    "abstract": "Multi-kernel learning (MKL) has been widely used in learning problems involving function learning tasks. Compared with single kernel learning approach which relies on a pre-selected kernel, the advantage of MKL is its flexibility results from combining a dictionary of kernels. However, inclusion of irrelevant kernels in the dictionary may deteriorate the accuracy of MKL, and increase the computational complexity. Faced with this challenge, a novel graph-aided framework is developed to select a subset of kernels from the dictionary with the assistance of a graph. Different graph construction and refinement schemes are developed based on incurred losses or kernel similarities to assist the adaptive selection process. Moreover, to cope with the scenario where data may be collected in a sequential fashion, or cannot be stored in batch due to the massive scale, random feature approximation are adopted to enable online function learning. It is proved that our proposed algorithms enjoy sub-linear regret bounds. Experiments on a number of real datasets showcase the advantages of our novel graph-aided algorithms compared to state-of-the-art alternatives",
    "checked": true,
    "id": "5822d4e0d9593737c3d8d188dfbecc644e7a46fb",
    "semantic_title": "graph-aided online multi-kernel learning",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0949.html": {
    "title": "Lower Bounds and Accelerated Algorithms for Bilevel Optimization",
    "volume": "main",
    "abstract": "Bilevel optimization has recently attracted growing interests due to its wide applications in modern machine learning problems. Although recent studies have characterized the convergence rate for several such popular algorithms, it is still unclear how much further these convergence rates can be improved. In this paper, we address this fundamental question from two perspectives. First, we provide the first-known lower complexity bounds of $\\widetilde \\Omega\\bigg(\\sqrt{\\frac{L_y\\widetilde L_{xy}^2}{\\mu_x\\mu_y^2}}\\bigg)$ and $\\widetilde \\Omega\\big(\\frac{1}{\\sqrt{\\epsilon}}\\min\\{\\kappa_y,\\frac{1}{\\sqrt{\\epsilon^{3}}}\\}\\big)$ respectively for strongly-convex-strongly-convex and convex-strongly-convex bilevel optimizations. Second, we propose an accelerated bilevel optimizer named AccBiO, for which we provide the first-known complexity bounds without the gradient boundedness assumption (which was made in existing analyses) under the two aforementioned geometries. We also provide significantly tighter upper bounds than the existing complexity when the bounded gradient assumption does hold. We show that AccBiO achieves the optimal results (i.e., the upper and lower bounds match up to logarithmic factors) when the inner-level problem takes a quadratic form with a constant-level condition number. Interestingly, our lower bounds under both geometries are larger than the corresponding optimal complexities of minimax optimization, establishing that bilevel optimization is provably more challenging than minimax optimization. Our theoretical results are validated by numerical experiments",
    "checked": true,
    "id": "51ac8296b14d2563b718561059afc1532c3c4657",
    "semantic_title": "lower bounds and accelerated algorithms for bilevel optimization",
    "citation_count": 49,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1067.html": {
    "title": "Bayesian Data Selection",
    "volume": "main",
    "abstract": "Insights into complex, high-dimensional data can be obtained by discovering features of the data that match or do not match a model of interest. To formalize this task, we introduce the \"data selection\" problem: finding a lower-dimensional statistic - such as a subset of variables - that is well fit by a given parametric model of interest. A fully Bayesian approach to data selection would be to parametrically model the value of the statistic, nonparametrically model the remaining \"background\" components of the data, and perform standard Bayesian model selection for the choice of statistic. However, fitting a nonparametric model to high-dimensional data tends to be highly inefficient, statistically and computationally. We propose a novel score for performing data selection, the \"Stein volume criterion (SVC)\", that does not require fitting a nonparametric model. The SVC takes the form of a generalized marginal likelihood with a kernelized Stein discrepancy in place of the Kullback-Leibler divergence. We prove that the SVC is consistent for data selection, and establish consistency and asymptotic normality of the corresponding generalized posterior on parameters. We apply the SVC to the analysis of single-cell RNA sequencing data sets using probabilistic principal components analysis and a spin glass model of gene regulation",
    "checked": true,
    "id": "c5344932bce69747dae79a9031bbf6c9e0756c3f",
    "semantic_title": "bayesian data selection",
    "citation_count": 5,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1280.html": {
    "title": "Calibrated Multiple-Output Quantile Regression with Representation Learning",
    "volume": "main",
    "abstract": "We develop a method to generate predictive regions that cover a multivariate response variable with a user-specified probability. Our work is composed of two components. First, we use a deep generative model to learn a representation of the response that has a unimodal distribution. Existing multiple-output quantile regression approaches are effective in such cases, so we apply them on the learned representation, and then transform the solution to the original space of the response. This process results in a flexible and informative region that can have an arbitrary shape, a property that existing methods lack. Second, we propose an extension of conformal prediction to the multivariate response setting that modifies any method to return sets with a pre-specified coverage level. The desired coverage is theoretically guaranteed in the finite-sample case for any distribution. Experiments conducted on both real and synthetic data show that our method constructs regions that are significantly smaller compared to existing techniques",
    "checked": true,
    "id": "5d3736582f543313dc2ca6b9e8c16d25d7f16ba0",
    "semantic_title": "calibrated multiple-output quantile regression with representation learning",
    "citation_count": 9,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1323.html": {
    "title": "Discrete Variational Calculus for Accelerated Optimization",
    "volume": "main",
    "abstract": "Many of the new developments in machine learning are connected with gradient-based optimization methods. Recently, these methods have been studied using a variational perspective (Betancourt et al., 2018). This has opened up the possibility of introducing variational and symplectic methods using geometric integration. In particular, in this paper, we introduce variational integrators (Marsden and West, 2001) which allow us to derive different methods for optimization. Using both Hamilton's and Lagrange-d'Alembert's principle, we derive two families of optimization methods in one-to-one correspondence that generalize Polyak's heavy ball (Polyak, 1964) and Nesterov's accelerated gradient (Nesterov, 1983), the second of which mimics the behavior of the latter reducing the oscillations of classical momentum methods. However, since the systems considered are explicitly time-dependent, the preservation of symplecticity of autonomous systems occurs here solely on the fibers. Several experiments exemplify the result",
    "checked": true,
    "id": "e24574ec84b7348eebc034772b00610068ee0c46",
    "semantic_title": "discrete variational calculus for accelerated optimization",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1396.html": {
    "title": "Generalization Bounds for Noisy Iterative Algorithms Using Properties of Additive Noise Channels",
    "volume": "main",
    "abstract": "Machine learning models trained by different optimization algorithms under different data distributions can exhibit distinct generalization behaviors. In this paper, we analyze the generalization of models trained by noisy iterative algorithms. We derive distribution-dependent generalization bounds by connecting noisy iterative algorithms to additive noise channels found in communication and information theory. Our generalization bounds shed light on several applications, including differentially private stochastic gradient descent (DP-SGD), federated learning, and stochastic gradient Langevin dynamics (SGLD). We demonstrate our bounds through numerical experiments, showing that they can help understand recent empirical observations of the generalization phenomena of neural networks",
    "checked": true,
    "id": "41c7ee4680c65194a997c2ebf9f5454b53dac3c6",
    "semantic_title": "generalization bounds for noisy iterative algorithms using properties of additive noise channels",
    "citation_count": 6,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1403.html": {
    "title": "The SKIM-FA Kernel: High-Dimensional Variable Selection and Nonlinear Interaction Discovery in Linear Time",
    "volume": "main",
    "abstract": "Many scientific problems require identifying a small set of covariates that are associated with a target response and estimating their effects. Often, these effects are nonlinear and include interactions, so linear and additive methods can lead to poor estimation and variable selection. Unfortunately, methods that simultaneously express sparsity, nonlinearity, and interactions are computationally intractable --- with runtime at least quadratic in the number of covariates, and often worse. In the present work, we solve this computational bottleneck. We show that suitable interaction models have a kernel representation, namely there exists a \"kernel trick\" to perform variable selection and estimation in $O$(# covariates) time. Our resulting fit corresponds to a sparse orthogonal decomposition of the regression function in a Hilbert space (i.e., a functional ANOVA decomposition), where interaction effects represent all variation that cannot be explained by lower-order effects. On a variety of synthetic and real data sets, our approach outperforms existing methods used for large, high-dimensional data sets while remaining competitive (or being orders of magnitude faster) in runtime",
    "checked": true,
    "id": "856ad6a15218d47e2d6452b80972cad04168194a",
    "semantic_title": "the skim-fa kernel: high-dimensional variable selection and nonlinear interaction discovery in linear time",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1441.html": {
    "title": "Impact of classification difficulty on the weight matrices spectra in Deep Learning and application to early-stopping",
    "volume": "MLOSS",
    "abstract": "Much recent research effort has been devoted to explain the success of deep learning. Random Matrix Theory (RMT) provides an emerging way to this end by analyzing the spectra of large random matrices involved in a trained deep neural network (DNN) such as weight matrices or Hessian matrices in the stochastic gradient descent algorithm. To better understand spectra of weight matrices, we conduct extensive experiments on weight matrices under different settings for layers, networks and data sets. Based on the previous work of {martin2018implicit}, spectra of weight matrices at the terminal stage of training are classified into three main types: Light Tail (LT), Bulk Transition period (BT) and Heavy Tail (HT). These different types, especially HT, implicitly indicate some regularization in the DNNs. In this paper, inspired from {martin2018implicit}, we identify the difficulty of the classification problem as an important factor for the appearance of HT in weight matrices spectra. Higher the classification difficulty, higher the chance for HT to appear. Moreover, the classification difficulty can be affected either by the signal-to-noise ratio of the dataset, or by the complexity of the classification problem (complex features, large number of classes) as well. Leveraging on this finding, we further propose a spectral criterion to detect the appearance of HT and use it to early stop the training process without testing data. Such early stopped DNNs have the merit of avoiding overfitting and unnecessary extra training while preserving a much comparable generalization ability. These findings from the paper are validated in several NNs (LeNet, MiniAlexNet and VGG), using Gaussian synthetic data and real data sets (MNIST and CIFAR10)",
    "checked": true,
    "id": "81e7e44d62c16b7465a26376123d7e1a8b44f43a",
    "semantic_title": "impact of classification difficulty on the weight matrices spectra in deep learning and application to early-stopping",
    "citation_count": 3,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1518.html": {
    "title": "HiClass: a Python Library for Local Hierarchical Classification Compatible with Scikit-learn",
    "volume": "MLOSS",
    "abstract": "HiClass is an open-source Python library for local hierarchical classification entirely compatible with scikit-learn. It contains implementations of the most common design patterns for hierarchical machine learning models found in the literature, that is, the local classifiers per node, per parent node and per level. Additionally, the package contains implementations of hierarchical metrics, which are more appropriate for evaluating classification performance on hierarchical data. The documentation includes installation and usage instructions, examples within tutorials and interactive notebooks, and a complete description of the API. HiClass is released under the simplified BSD license, encouraging its use in both academic and commercial environments. Source code and documentation are available at https://github.com/scikit-learn-contrib/hiclass",
    "checked": true,
    "id": "7d4fd750d87ab72a0efdaa83e74d06f13597cd60",
    "semantic_title": "hiclass: a python library for local hierarchical classification compatible with scikit-learn",
    "citation_count": 6,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0014.html": {
    "title": "Attacks against Federated Learning Defense Systems and their Mitigation",
    "volume": "main",
    "abstract": "The susceptibility of federated learning (FL) to attacks from untrustworthy endpoints has led to the design of several defense systems. FL defense systems enhance the federated optimization algorithm using anomaly detection, scaling the updates from endpoints depending on their anomalous behavior. However, the defense systems themselves may be exploited by the endpoints with more sophisticated attacks. First, this paper proposes three categories of attacks and shows that they can effectively deceive some well-known FL defense systems. In the first two categories, referred to as on-off attacks, the adversary toggles between being honest and engaging in attacks. We analyse two such on-off attacks, label flipping and free riding, and show their impact against existing FL defense systems. As a third category, we propose attacks based on \"good mouthing\" and \"bad mouthing\", to boost or diminish influence of the victim endpoints on the global model. Secondly, we propose a new federated optimization algorithm, Viceroy, that can successfully mitigate all the proposed attacks. The proposed attacks and the mitigation strategy have been tested on a number of different experiments establishing their effectiveness in comparison with other contemporary methods. The proposed algorithm has also been made available as open source. Finally, in the appendices, we provide an induction proof for the on-off model poisoning attack, and the proof of convergence and adversarial tolerance for the new federated optimization algorithm",
    "checked": true,
    "id": "3864500a24eaa39711ed1e90d148e929b0026430",
    "semantic_title": "attacks against federated learning defense systems and their mitigation",
    "citation_count": 1,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0019.html": {
    "title": "Labels, Information, and Computation: Efficient Learning Using Sufficient Labels",
    "volume": "main",
    "abstract": "In supervised learning, obtaining a large set of fully-labeled training data is expensive. We show that we do not always need full label information on every single training example to train a competent classifier. Specifically, inspired by the principle of sufficiency in statistics, we present a statistic (a summary) of the fully-labeled training set that captures almost all the relevant information for classification but at the same time is easier to obtain directly. We call this statistic \"sufficiently-labeled data\" and prove its sufficiency and efficiency for finding the optimal hidden representations, on which competent classifier heads can be trained using as few as a single randomly-chosen fully-labeled example per class. Sufficiently-labeled data can be obtained from annotators directly without collecting the fully-labeled data first. And we prove that it is easier to directly obtain sufficiently-labeled data than obtaining fully-labeled data. Furthermore, sufficiently-labeled data is naturally more secure since it stores relative, instead of absolute, information. Extensive experimental results are provided to support our theory",
    "checked": true,
    "id": "9acd5b10179e0f1ca90cca4c787e8829edd2f35f",
    "semantic_title": "labels, information, and computation: efficient learning using sufficient labels",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0088.html": {
    "title": "Sparse PCA: a Geometric Approach",
    "volume": "main",
    "abstract": "We consider the problem of maximizing the variance explained from a data matrix using orthogonal sparse principal components that have a support of fixed cardinality. While most existing methods focus on building principal components (PCs) iteratively through deflation, we propose GeoSPCA, a novel algorithm to build all PCs at once while satisfying the orthogonality constraints which brings substantial benefits over deflation. This novel approach is based on the left eigenvalues of the covariance matrix which helps circumvent the non-convexity of the problem by approximating the optimal solution using a binary linear optimization problem that can find the optimal solution. The resulting approximation can be used to tackle different versions of the sparse PCA problem including the case in which the principal components share the same support or have disjoint supports and the Structured Sparse PCA problem. We also propose optimality bounds and illustrate the benefits of GeoSPCA in selected real world problems both in terms of explained variance, sparsity and tractability. Improvements vs. the greedy algorithm, which is often at par with state-of-the-art techniques, reaches up to 24% in terms of variance while solving real world problems with 10,000s of variables and support cardinality of 100s in minutes. We also apply GeoSPCA in a face recognition problem yielding more than 10% improvement vs. other PCA based technique such as structured sparse PCA",
    "checked": true,
    "id": "43208c8e33b36e571a30ea2f6e258f7200909965",
    "semantic_title": "sparse pca: a geometric approach",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0099.html": {
    "title": "Gap Minimization for Knowledge Sharing and Transfer",
    "volume": "main",
    "abstract": "Learning from multiple related tasks by knowledge sharing and transfer has become increasingly relevant over the last two decades. In order to successfully transfer information from one task to another, it is critical to understand the similarities and differences between the domains. In this paper, we introduce the notion of performance gap, an intuitive and novel measure of the distance between learning tasks. Unlike existing measures which are used as tools to bound the difference of expected risks between tasks (e.g., $\\mathcal{H}$-divergence or discrepancy distance), we theoretically show that the performance gap can be viewed as a data- and algorithm-dependent regularizer, which controls the model complexity and leads to finer guarantees. More importantly, it also provides new insights and motivates a novel principle for designing strategies for knowledge sharing and transfer: gap minimization. We instantiate this principle with two algorithms: 1. gapBoost, a novel and principled boosting algorithm that explicitly minimizes the performance gap between source and target domains for transfer learning; and 2. gapMTNN, a representation learning algorithm that reformulates gap minimization as semantic conditional matching for multitask learning. Our extensive evaluation on both transfer learning and multitask learning benchmark data sets shows that our methods outperform existing baselines",
    "checked": true,
    "id": "1149360879ddc6fd0966973b9146476c26709e52",
    "semantic_title": "gap minimization for knowledge sharing and transfer",
    "citation_count": 7,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0142.html": {
    "title": "Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond",
    "volume": "MLOSS",
    "abstract": "The evaluation of explanation methods is a research topic that has not yet been explored deeply, however, since explainability is supposed to strengthen trust in artificial intelligence, it is necessary to systematically review and compare explanation methods in order to confirm their correctness. Until now, no tool with focus on XAI evaluation exists that exhaustively and speedily allows researchers to evaluate the performance of explanations of neural network predictions. To increase transparency and reproducibility in the field, we therefore built Quantus‚Äîa comprehensive, evaluation toolkit in Python that includes a growing, well-organised collection of evaluation metrics and tutorials for evaluating explainable methods. The toolkit has been thoroughly tested and is available under an open-source license on PyPi (or on https://github.com/understandable-machine-intelligence-lab/Quantus/)",
    "checked": false,
    "id": "30e776268268e84becd2863b0632247da61238b9",
    "semantic_title": "quantus: an explainable ai toolkit for responsible evaluation of neural network explanations",
    "citation_count": 46,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0203.html": {
    "title": "Can Reinforcement Learning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopically Rational Followers?",
    "volume": "main",
    "abstract": "We study multi-player general-sum Markov games with one of the players designated as the leader and the other players regarded as followers. In particular, we focus on the class of games where the followers are myopically rational; i.e., they aim to maximize their instantaneous rewards. For such a game, our goal is to find a Stackelberg-Nash equilibrium (SNE), which is a policy pair $(\\pi^*, \\nu^*)$ such that: (i) $\\pi^*$ is the optimal policy for the leader when the followers always play their best response, and (ii) $\\nu^*$ is the best response policy of the followers, which is a Nash equilibrium of the followers' game induced by $\\pi^*$. We develop sample-efficient reinforcement learning (RL) algorithms for solving for an SNE in both online and offline settings. Our algorithms are optimistic and pessimistic variants of least-squares value iteration, and they are readily able to incorporate function approximation tools in the setting of large state spaces. Furthermore, for the case with linear function approximation, we prove that our algorithms achieve sublinear regret and suboptimality under online and offline setups respectively. To the best of our knowledge, we establish the first provably efficient RL algorithms for solving for SNEs in general-sum Markov games with myopically rational followers",
    "checked": true,
    "id": "243e9306be165c87339621bc3355c60595e685fa",
    "semantic_title": "can reinforcement learning find stackelberg-nash equilibria in general-sum markov games with myopically rational followers?",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0210.html": {
    "title": "Label Distribution Changing Learning with Sample Space Expanding",
    "volume": "main",
    "abstract": "With the evolution of data collection ways, label ambiguity has arisen from various applications. How to reduce its uncertainty and leverage its effectiveness is still a challenging task. As two types of representative label ambiguities, Label Distribution Learning (LDL), which annotates each instance with a label distribution, and Emerging New Class (ENC), which focuses on model reusing with new classes, have attached extensive attentions. Nevertheless, in many applications, such as emotion distribution recognition and facial age estimation, we may face a more complicated label ambiguity scenario, i.e., label distribution changing with sample space expanding owing to the new class. To solve this crucial but rarely studied problem, we propose a new framework named as Label Distribution Changing Learning (LDCL) in this paper, together with its theoretical guarantee with generalization error bound. Our approach expands the sample space by re-scaling previous distribution and then estimates the emerging label value via scaling constraint factor. For demonstration, we present two special cases within the framework, together with their optimizations and convergence analyses. Besides evaluating LDCL on most of the existing 13 data sets, we also apply it in the application of emotion distribution recognition. Experimental results demonstrate the effectiveness of our approach in both tackling label ambiguity problem and estimating facial emotion",
    "checked": true,
    "id": "1b25a9c991deac934c08ad724e76f9bcf0a48a36",
    "semantic_title": "label distribution changing learning with sample space expanding",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0227.html": {
    "title": "Ridges, Neural Networks, and the Radon Transform",
    "volume": "main",
    "abstract": "A ridge is a function that is characterized by a one-dimensional profile (activation) and a multidimensional direction vector. Ridges appear in the theory of neural networks as functional descriptors of the effect of a neuron, with the direction vector being encoded in the linear weights. In this paper, we investigate properties of the Radon transform in relation to ridges and to the characterization of neural networks. We introduce a broad category of hyper-spherical Banach subspaces (including the relevant subspace of measures) over which the back-projection operator is invertible. We also give conditions under which the back-projection operator is extendable to the full parent space with its null space being identifiable as a Banach complement. Starting from first principles, we then characterize the sampling functionals that are in the range of the filtered Radon transform. Next, we extend the definition of ridges for any distributional profile and determine their (filtered) Radon transform in full generality. Finally, we apply our formalism to clarify and simplify some of the results and proofs on the optimality of ReLU networks that have appeared in the literature",
    "checked": true,
    "id": "38cd3801900d7228a4403d96b3022a0ad54f31bf",
    "semantic_title": "ridges, neural networks, and the radon transform",
    "citation_count": 9,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0310.html": {
    "title": "First-Order Algorithms for Nonlinear Generalized Nash Equilibrium Problems",
    "volume": "main",
    "abstract": "We consider the problem of computing an equilibrium in a class of nonlinear generalized Nash equilibrium problems (NGNEPs) in which the strategy sets for each player are defined by the equality and inequality constraints that may depend on the choices of rival players. While the asymptotic global convergence and local convergence rate of certain algorithms have been extensively investigated, the iteration complexity analysis is still in its infancy. This paper provides two first-order algorithms based on quadratic penalty method (QPM) and augmented Lagrangian method (ALM), respectively, with an accelerated mirror-prox algorithm as the solver in each inner loop. We show the nonasymptotic convergence rate for these algorithms. In particular, we establish the global convergence guarantee for solving monotone and strongly monotone NGNEPs and provide the complexity bounds expressed in terms of the number of gradient evaluations. Experimental results demonstrate the efficiency of our algorithms in practice",
    "checked": true,
    "id": "0361096061b46cd68fbf3ecef3199eac700d7e09",
    "semantic_title": "first-order algorithms for nonlinear generalized nash equilibrium problems",
    "citation_count": 7,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0315.html": {
    "title": "Sensing Theorems for Unsupervised Learning in Linear Inverse Problems",
    "volume": "main",
    "abstract": "Solving an ill-posed linear inverse problem requires knowledge about the underlying signal model. In many applications, this model is a priori unknown and has to be learned from data. However, it is impossible to learn the model using observations obtained via a single incomplete measurement operator, as there is no information about the signal model in the nullspace of the operator, resulting in a chicken-and-egg problem: to learn the model we need reconstructed signals, but to reconstruct the signals we need to know the model. Two ways to overcome this limitation are using multiple measurement operators or assuming that the signal model is invariant to a certain group action. In this paper, we present necessary and sufficient sensing conditions for learning the signal model from measurement data alone which only depend on the dimension of the model and the number of operators or properties of the group action that the model is invariant to. As our results are agnostic of the learning algorithm, they shed light into the fundamental limitations of learning from incomplete data and have implications in a wide range set of practical algorithms, such as dictionary learning, matrix completion and deep neural networks",
    "checked": true,
    "id": "5edbe2962427c9090486dd4b623115afaac44adb",
    "semantic_title": "sensing theorems for unsupervised learning in linear inverse problems",
    "citation_count": 5,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0330.html": {
    "title": "On Batch Teaching Without Collusion",
    "volume": "main",
    "abstract": "Formal models of learning from teachers need to respect certain criteria to avoid collusion. The most commonly accepted notion of collusion-avoidance was proposed by Goldman and Mathias (1996), and various teaching models obeying their criterion have been studied. For each model $M$ and each concept class $\\mathcal{C}$, a parameter $M$-TD$(\\mathcal{C})$ refers to the teaching dimension of concept class $\\mathcal{C}$ in model $M$---defined to be the number of examples required for teaching a concept, in the worst case over all concepts in $\\mathcal{C}$. This paper introduces a new model of teaching, called no-clash teaching, together with the corresponding parameter NCTD$(\\mathcal{C})$. No-clash teaching is provably optimal in the strong sense that, given any concept class $\\mathcal{C}$ and any model $M$ obeying Goldman and Mathias's collusion-avoidance criterion, one obtains NCTD$(\\mathcal{C})\\le M$-TD$(\\mathcal{C})$. We also study a corresponding notion NCTD$^+$ for the case of learning from positive data only, establish useful bounds on NCTD and NCTD$^+$, and discuss relations of these parameters to other complexity parameters of interest in computational learning theory. We further argue that Goldman and Mathias's collusion-avoidance criterion may in some settings be too weak in that it admits certain forms of interaction between teacher and learner that could be considered collusion in practice. Therefore, we introduce a strictly stronger notion of collusion-avoidance and demonstrate that the well-studied notion of Preference-based Teaching is optimal among all teaching schemes that are strongly collusion-avoiding on all finite subsets of a given concept class",
    "checked": true,
    "id": "0729821bd80cd5354529acf901a3d7fd7cd77af4",
    "semantic_title": "on batch teaching without collusion",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0365.html": {
    "title": "Neural Implicit Flow: a mesh-agnostic dimensionality reduction paradigm of spatio-temporal data",
    "volume": "main",
    "abstract": "High-dimensional spatio-temporal dynamics can often be encoded in a low-dimensional subspace. Engineering applications for modeling, characterization, design, and control of such large-scale systems often rely on dimensionality reduction to make solutions computationally tractable in real time. Common existing paradigms for dimensionality reduction include linear methods, such as the singular value decomposition (SVD), and nonlinear methods, such as variants of convolutional autoencoders (CAE). However, these encoding techniques lack the ability to efficiently represent the complexity associated with spatio-temporal data, which often requires variable geometry, non-uniform grid resolution, adaptive meshing, and/or parametric dependencies. To resolve these practical engineering challenges, we propose a general framework called Neural Implicit Flow (NIF) that enables a mesh-agnostic, low-rank representation of large-scale, parametric, spatial-temporal data. NIF consists of two modified multilayer perceptrons (MLPs): (i) ShapeNet, which isolates and represents the spatial complexity, and (ii) ParameterNet, which accounts for any other input complexity, including parametric dependencies, time, and sensor measurements. We demonstrate the utility of NIF for parametric surrogate modeling, enabling the interpretable representation and compression of complex spatio-temporal dynamics, efficient many-spatial-query tasks, and improved generalization performance for sparse reconstruction",
    "checked": true,
    "id": "0e0742566d72740f371f6f5479d508c868301663",
    "semantic_title": "neural implicit flow: a mesh-agnostic dimensionality reduction paradigm of spatio-temporal data",
    "citation_count": 11,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0479.html": {
    "title": "A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness",
    "volume": "main",
    "abstract": "Accurate uncertainty quantification is a major challenge in deep learning, as neural networks can make overconfident errors and assign high confidence predictions to out-of-distribution (OOD) inputs. The most popular approaches to estimate predictive uncertainty in deep learning are methods that combine predictions from multiple neural networks, such as Bayesian neural networks (BNNs) and deep ensembles. However their practicality in real-time, industrial-scale applications are limited due to the high memory and computational cost. Furthermore, ensembles and BNNs do not necessarily fix all the issues with the underlying member networks. In this work, we study principled approaches to improve the uncertainty property of a single network, based on a single, deterministic representation. By formalizing the uncertainty quantification as a minimax learning problem, we first identify distance awareness, i.e., the model's ability to quantify the distance of a testing example from the training data, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs with two simple changes: (1) applying spectral normalization to hidden weights to enforce bi-Lipschitz smoothness in representations and (2) replacing the last output layer with a Gaussian process layer. On a suite of vision and language understanding benchmarks and on modern architectures (Wide-ResNet and BERT), SNGP consistently outperforms other single-model approaches in prediction, calibration and out-of-domain detection. Furthermore, SNGP provides complementary benefits to popular techniques such as deep ensembles and data augmentation, making it a simple and scalable building block for probabilistic deep learning",
    "checked": true,
    "id": "60f2ae3b448b035e957603e84d1a073ad708879b",
    "semantic_title": "a simple approach to improve single-model deep uncertainty via distance-awareness",
    "citation_count": 17,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0567.html": {
    "title": "Benchmarking Graph Neural Networks",
    "volume": "main",
    "abstract": "In the last few years, graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. This emerging field has witnessed an extensive growth of promising techniques that have been applied with success to computer science, mathematics, biology, physics and chemistry. But for any successful field to become mainstream and reliable, benchmarks must be developed to quantify progress. This led us in March 2020 to release a benchmark framework that i) comprises of a diverse collection of mathematical and real-world graphs, ii) enables fair model comparison with the same parameter budget to identify key architectures, iii) has an open-source, easy-to use and reproducible code infrastructure, and iv) is flexible for researchers to experiment with new theoretical ideas. As of December 2022, the GitHub repository has reached 2,000 stars and 380 forks, which demonstrates the utility of the proposed open-source framework through the wide usage by the GNN community. In this paper, we present an updated version of our benchmark with a concise presentation of the aforementioned framework characteristics, an additional medium-sized molecular dataset AQSOL, similar to the popular ZINC, but with a real-world measured chemical target, and discuss how this framework can be leveraged to explore new GNN designs and insights. As a proof of value of our benchmark, we study the case of graph positional encoding (PE) in GNNs, which was introduced with this benchmark and has since spurred interest of exploring more powerful PE for Transformers and GNNs in a robust experimental setting",
    "checked": true,
    "id": "3576cbc79bb552c6847ef5ff5f6cef217a4bee86",
    "semantic_title": "benchmarking graph neural networks",
    "citation_count": 555,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0629.html": {
    "title": "Robust Load Balancing with Machine Learned Advice",
    "volume": "main",
    "abstract": "Motivated by the exploding growth of web-based services and the importance of efficiently managing the computational resources of such systems, we introduce and study a theoretical model for load balancing of very large databases such as commercial search engines. Our model is a more realistic version of the well-received \\bab model with an additional constraint that limits the number of servers that carry each piece of the data. This additional constraint is necessary when, on one hand, the data is so large that we can not copy the whole data on each server. On the other hand, the query response time is so limited that we can not ignore the fact that the number of queries for each piece of the data changes over time, and hence we can not simply split the data over different machines. In this paper, we develop an almost optimal load balancing algorithm that works given an estimate of the load of each piece of the data. Our algorithm is almost perfectly robust to wrong estimates, to the extent that even when all of the loads are adversarially chosen the performance of our algorithm is $1-1/e$, which is provably optimal. Along the way, we develop various techniques for analyzing the balls-into-bins process under certain correlations and build a novel connection with the multiplicative weights update scheme",
    "checked": true,
    "id": "d3ba0fa50fc9034b320dbd82a1625a3638326d11",
    "semantic_title": "robust load balancing with machine learned advice",
    "citation_count": 6,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0698.html": {
    "title": "The multimarginal optimal transport formulation of adversarial multiclass classification",
    "volume": "main",
    "abstract": "We study a family of adversarial multiclass classification problems and provide equivalent reformulations in terms of: 1) a family of generalized barycenter problems introduced in the paper and 2) a family of multimarginal optimal transport problems where the number of marginals is equal to the number of classes in the original classification problem. These new theoretical results reveal a rich geometric structure of adversarial learning problems in multiclass classification and extend recent results restricted to the binary classification setting. A direct computational implication of our results is that by solving either the barycenter problem and its dual, or the MOT problem and its dual, we can recover the optimal robust classification rule and the optimal adversarial strategy for the original adversarial problem. Examples with synthetic and real data illustrate our results",
    "checked": true,
    "id": "ee4784b7329e068e4db2d82ccfe860dcc5f08138",
    "semantic_title": "the multimarginal optimal transport formulation of adversarial multiclass classification",
    "citation_count": 10,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0916.html": {
    "title": "The d-Separation Criterion in Categorical Probability",
    "volume": "main",
    "abstract": "The d-separation criterion detects the compatibility of a joint probability distribution with a directed acyclic graph through certain conditional independences. In this work, we study this problem in the context of categorical probability theory by introducing a categorical definition of causal models, a categorical notion of d-separation, and proving an abstract version of the d-separation criterion. This approach has two main benefits. First, categorical d-separation is a very intuitive criterion based on topological connectedness. Second, our results apply both to measure-theoretic probability (with standard Borel spaces) and beyond probability theory, including to deterministic and possibilistic networks. It therefore provides a clean proof of the equivalence of local and global Markov properties with causal compatibility for continuous and mixed random variables as well as deterministic and possibilistic variables",
    "checked": true,
    "id": "86fdb778944d8ea868455312a65b225b8a114821",
    "semantic_title": "the d-separation criterion in categorical probability",
    "citation_count": 10,
    "authors": []
  },
  "https://jmlr.org/papers/v24/18-508.html": {
    "title": "A Group-Theoretic Approach to Computational Abstraction: Symmetry-Driven Hierarchical Clustering",
    "volume": "main",
    "abstract": "Humans' abstraction ability plays a key role in concept learning and knowledge discovery. This theory paper presents the mathematical formulation for computationally emulating human-like abstractions---computational abstraction---and abstraction processes developed hierarchically from innate priors like symmetries. We study the nature of abstraction via a group-theoretic approach, formalizing and practically computing abstractions as symmetry-driven hierarchical clustering. Compared to data-driven clustering like k-means or agglomerative clustering (a chain), our abstraction model is data-free, feature-free, similarity-free, and globally hierarchical (a lattice). This paper also serves as a theoretical generalization of several existing works. These include generalizing Shannon's information lattice, specialized algorithms for certain symmetry-induced clusterings, as well as formalizing knowledge discovery applications such as learning music theory from scores and chemistry laws from molecules. We consider computational abstraction as a first step towards a principled and cognitive way of achieving human-level concept learning and knowledge discovery",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/19-1009.html": {
    "title": "On the Convergence of Stochastic Gradient Descent with Bandwidth-based Step Size",
    "volume": "main",
    "abstract": "We first propose a general step-size framework for the stochastic gradient descent(SGD) method: bandwidth-based step sizes that are allowed to vary within a banded region. The framework provides efficient and flexible step size selection in optimization, including cyclical and non-monotonic step sizes (e.g., triangular policy and cosine with restart), for which theoretical guarantees are rare. We provide state-of-the-art convergence guarantees for SGD under mild conditions and allow a large constant step size at the beginning of training. Moreover, we investigate the error bounds of SGD under the bandwidth step size where the boundary functions are in the same order and different orders, respectively. Finally, we propose a $1/t$ up-down policy and design novel non-monotonic step sizes. Numerical experiments demonstrate these bandwidth-based step sizes' efficiency and significant potential in training regularized logistic regression and several large-scale neural network tasks",
    "checked": true,
    "id": "7d3ee1352ae709b221f9e10b083e4ddddbf3d43b",
    "semantic_title": "on the convergence of stochastic gradient descent with bandwidth-based step size",
    "citation_count": 3,
    "authors": []
  },
  "https://jmlr.org/papers/v24/19-980.html": {
    "title": "Reinforcement Learning for Joint Optimization of Multiple Rewards",
    "volume": "main",
    "abstract": "Finding optimal policies which maximize long term rewards of Markov Decision Processes requires the use of dynamic programming and backward induction to solve the Bellman optimality equation. However, many real-world problems require optimization of an objective that is non-linear in cumulative rewards for which dynamic programming cannot be applied directly. For example, in a resource allocation problem, one of the objectives is to maximize long-term fairness among the users. We notice that when an agent aim to optimize some function of the sum of rewards is considered, the problem loses its Markov nature. This paper addresses and formalizes the problem of optimizing a non-linear function of the long term average of rewards. We propose model-based and model-free algorithms to learn the policy, where the model-based policy is shown to achieve a regret of $\\Tilde{O}\\left(LKDS\\sqrt{\\frac{A}{T}}\\right)$ for $K$ objectives combined with a concave $L$-Lipschitz function. Further, using the fairness in cellular base-station scheduling, and queueing system scheduling as examples, the proposed algorithm is shown to significantly outperform the conventional RL approaches",
    "checked": false,
    "id": "303839b66f1aa38c5bdcfc1cc30548663e610eb7",
    "semantic_title": "deep contextual bandit and reinforcement learning for irs-assisted mu-mimo systems",
    "citation_count": 4,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-060.html": {
    "title": "Convergence Rates of a Class of Multivariate Density Estimation Methods Based on Adaptive Partitioning",
    "volume": "main",
    "abstract": "Density estimation is a building block for many other statistical methods, such as classification, nonparametric testing, and data compression. In this paper, we focus on a non-parametric approach to multivariate density estimation, and study its asymptotic properties under both frequentist and Bayesian settings. The estimated density function is obtained by considering a sequence of approximating spaces to the space of densities. These spaces consist of piecewise constant density functions supported by binary partitions with increasing complexity. To obtain an estimate, the partition is learned by maximizing either the likelihood of the corresponding histogram on that partition, or the marginal posterior probability of the partition under a suitable prior. We analyze the convergence rate of the maximum likelihood estimator and the posterior concentration rate of the Bayesian estimator, and conclude that for a relatively rich class of density functions the rate does not directly depend on the dimension. We also show that the Bayesian method can adapt to the unknown smoothness of the density function. The method is applied to several specific function classes and explicit rates are obtained. These include spatially sparse functions, functions of bounded variation, and H{\\\"o}lder continuous functions. We also introduce an ensemble approach, obtained by aggregating multiple density estimates fit under carefully designed perturbations, and show that for density functions lying in a H{\\\"o}lder space ($\\mathcal{H}^{1, \\beta}, 0 < \\beta \\leq 1$), the ensemble method can achieve minimax convergence rate up to a logarithmic term, while the corresponding rate of the density estimator based on a single partition is suboptimal for this function class",
    "checked": true,
    "id": "dcb030ceb9f58a24d014156cc502d83acec8f58d",
    "semantic_title": "convergence rates of a class of multivariate density estimation methods based on adaptive partitioning",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1101.html": {
    "title": "Online Change-Point Detection in High-Dimensional Covariance Structure with Application to Dynamic Networks",
    "volume": "main",
    "abstract": "In this paper, we develop an online change-point detection procedure in the covariance structure of high-dimensional data. A new stopping rule is proposed to terminate the process as early as possible when a change in covariance structure occurs. The stopping rule allows spatial and temporal dependence and can be applied to non-Gaussian data. An explicit expression for the average run length is derived, so that the level of threshold in the stopping rule can be easily obtained with no need to run time-consuming Monte Carlo simulations. We also establish an upper bound for the expected detection delay, the expression of which demonstrates the impact of data dependence and magnitude of change in the covariance structure. Simulation studies are provided to confirm accuracy of the theoretical results. The practical usefulness of the proposed procedure is illustrated by detecting the change of brain's covariance network in a resting-state fMRI data set. The implementation of the methodology is provided in the R package OnlineCOV",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1202.html": {
    "title": "Necessary and Sufficient Conditions for Inverse Reinforcement Learning of Bayesian Stopping Time Problems",
    "volume": "main",
    "abstract": "This paper presents an inverse reinforcement learning (IRL) framework for Bayesian stopping time problems. By observing the actions of a Bayesian decision maker, we provide a necessary and sufficient condition to identify if these actions are consistent with optimizing a cost function. In a Bayesian (partially observed) setting, the inverse learner can at best identify optimality wrt the observed strategies. Our IRL algorithm identifies optimality and then constructs set-valued estimates of the cost function. To achieve this IRL objective, we use novel ideas from Bayesian revealed preferences stemming from microeconomics. We illustrate the proposed IRL scheme using two important examples of stopping time problems, namely, sequential hypothesis testing and Bayesian search. As a real-world example, we illustrate using a YouTube dataset comprising metadata from 190000 videos how the proposed IRL method predicts user engagement in online multimedia platforms with high accuracy. Finally, for finite datasets, we propose an IRL detection algorithm and give finite sample bounds on its error probabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1226.html": {
    "title": "VCG Mechanism Design with Unknown Agent Values under Stochastic Bandit Feedback",
    "volume": "main",
    "abstract": "We study a multi-round welfare-maximising mechanism design problem in instances where agents do not know their values. On each round, a mechanism first assigns an allocation to a set of agents and charges them a price; at the end of the round, the agents provide (stochastic) feedback to the mechanism for the allocation they received. This setting is motivated by applications in cloud markets and online advertising where an agent may know her value for an allocation only after experiencing it. Therefore, the mechanism needs to explore different allocations for each agent so that it can learn their values, while simultaneously attempting to find the socially optimal set of allocations. Our focus is on truthful and individually rational mechanisms which imitate the classical VCG mechanism in the long run. To that end, we first define three notions of regret for the welfare, the individual utilities of each agent and that of the mechanism. We show that these three terms are interdependent via an $\\Omega(T^{\\frac{2}{3}})$ lower bound for the maximum of these three terms after $T$ rounds of allocations, and describe an algorithm which essentially achieves this rate. Our framework also provides flexibility to control the pricing scheme so as to trade-off between the agent and seller regrets. Next, we define asymptotic variants for the truthfulness and individual rationality requirements and provide asymptotic rates to quantify the degree to which both properties are satisfied by the proposed algorithm",
    "checked": false,
    "id": "e58033abe114035ef7bffb50934e0abad9a99f77",
    "semantic_title": "dec . 8 , 2017 learning in the presence of strategic behavior",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1419.html": {
    "title": "Contextual Stochastic Block Model: Sharp Thresholds and Contiguity",
    "volume": "main",
    "abstract": "We study community detection in the \"contextual stochastic block model\" (Yan and Sarkar (2020), Deshpande et al. (2018)). Deshpande et al. (2018) studied this problem in the setting of sparse graphs with high-dimensional node-covariates. Using the non-rigorous \"cavity method\" from statistical physics (Mezard and Montanari (2009)), they calculated the sharp limit for community detection in this setting, and verified that the limit matches the information theoretic threshold when the average degree of the observed graph is large. They conjectured that the limit should hold as soon as the average degree exceeds one. We establish this conjecture, and characterize the sharp threshold for detection and weak recovery",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1461.html": {
    "title": "Kernel-based estimation for partially functional linear model: Minimax rates and randomized sketches",
    "volume": "main",
    "abstract": "This paper considers the partially functional linear model (PFLM) where all predictive features consist of a functional covariate and a high dimensional scalar vector. Over an infinite dimensional reproducing kernel Hilbert space, the proposed estimation for PFLM is a least square approach with two mixed regularizations of a function-norm and an $\\ell_1$-norm. Our main task in this paper is to establish the minimax rates for PFLM under high dimensional setting, and the optimal minimax rates of estimation are established by using various techniques in empirical process theory for analyzing kernel classes. In addition, we propose an efficient numerical algorithm based on randomized sketches of the kernel matrix. Several numerical experiments are implemented to support our method and optimization strategy",
    "checked": true,
    "id": "7f34ec860201a116e173d6b51cb7567a44735c0b",
    "semantic_title": "kernel-based estimation for partially functional linear model: minimax rates and randomized sketches",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-602.html": {
    "title": "On the geometry of Stein variational gradient descent",
    "volume": "main",
    "abstract": "Bayesian inference problems require sampling or approximating high-dimensional probability distributions. The focus of this paper is on the recently introduced Stein variational gradient descent methodology, a class of algorithms that rely on iterated steepest descent steps with respect to a reproducing kernel Hilbert space norm. This construction leads to interacting particle systems, the mean field limit of which is a gradient flow on the space of probability distributions equipped with a certain geometrical structure. We leverage this viewpoint to shed some light on the convergence properties of the algorithm, in particular addressing the problem of choosing a suitable positive definite kernel function. Our analysis leads us to considering certain nondifferentiable kernels with adjusted tails. We demonstrate significant performance gains of these in various numerical experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-695.html": {
    "title": "Tree-AMP: Compositional Inference with Tree Approximate Message Passing",
    "volume": "main",
    "abstract": "We introduce Tree-AMP, standing for Tree Approximate Message Passing, a python package for compositional inference in high-dimensional tree-structured models. The package provides a unifying framework to study several approximate message passing algorithms previously derived for a variety of machine learning tasks such as generalized linear models, inference in multi-layer networks, matrix factorization, and reconstruction using non-separable penalties. For some models, the asymptotic performance of the algorithm can be theoretically predicted by the state evolution, and the measurements entropy estimated by the free entropy formalism. The implementation is modular by design: each module, which implements a factor, can be composed at will with other modules to solve complex inference tasks. The user only needs to declare the factor graph of the model: the inference algorithm, state evolution and entropy estimation are fully automated",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-902.html": {
    "title": "Online Stochastic Gradient Descent with Arbitrary Initialization Solves Non-smooth, Non-convex Phase Retrieval",
    "volume": "main",
    "abstract": "In recent literature, a general two step procedure has been formulated for solving the problem of phase retrieval. First, a spectral technique is used to obtain a constant-error initial estimate, following which, the estimate is refined to arbitrary precision by first-order optimization of a non-convex loss function. Numerical experiments, however, seem to suggest that simply running the iterative schemes from a random initialization may also lead to convergence, albeit at the cost of slightly higher sample complexity. In this paper, we prove that, in fact, constant step size online stochastic gradient descent (SGD) converges from arbitrary initializations for the non-smooth, non-convex amplitude squared loss objective. In this setting, online SGD is also equivalent to the randomized Kaczmarz algorithm from numerical analysis. Our analysis can easily be generalized to other single index models. It also makes use of new ideas from stochastic process theory, including the notion of a summary state space, which we believe will be of use for the broader field of non-convex optimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0073.html": {
    "title": "Topological Convolutional Layers for Deep Learning",
    "volume": "main",
    "abstract": "This work introduces the Topological CNN (TCNN), which encompasses several topologically defined convolutional methods. Manifolds with important relationships to the natural image space are used to parameterize image filters which are used as convolutional weights in a TCNN. These manifolds also parameterize slices in layers of a TCNN across which the weights are localized. We show evidence that TCNNs learn faster, on less data, with fewer learned parameters, and with greater generalizability and interpretability than conventional CNNs. We introduce and explore TCNN layers for both image and video data. We propose extensions to 3D images and 3D video",
    "checked": true,
    "id": "7395dfa4501c4952a096e11fd2910dc7cdc864f1",
    "semantic_title": "topological convolutional layers for deep learning",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0117.html": {
    "title": "Provably Sample-Efficient Model-Free Algorithm for MDPs with Peak Constraints",
    "volume": "main",
    "abstract": "In the optimization of dynamic systems, the variables typically have constraints. Such problems can be modeled as a Constrained Markov Decision Process (CMDP). This paper considers the peak Constrained Markov Decision Process (PCMDP), where the agent chooses the policy to maximize total reward in the finite horizon as well as satisfy constraints at each epoch with probability 1. We propose a model-free algorithm that converts PCMDP problem to an unconstrained problem and a Q-learning based approach is applied. We define the concept of probably approximately correct (PAC) to the proposed PCMDP problem. The proposed algorithm is proved to achieve an $(\\epsilon,p)$-PAC policy when the episode $K\\geq\\Omega(\\frac{I^2H^6SA\\ell}{\\epsilon^2})$, where $S$ and $A$ are the number of states and actions, respectively. $H$ is the number of epochs per episode. $I$ is the number of constraint functions, and $\\ell=\\log(\\frac{SAT}{p})$. We note that this is the first result on PAC kind of analysis for PCMDP with peak constraints, where the transition dynamics are not known apriori. We demonstrate the proposed algorithm on an energy harvesting problem and a single machine scheduling problem, where it performs close to the theoretical upper bound of the studied optimization problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0235.html": {
    "title": "Density estimation on low-dimensional manifolds: an inflation-deflation approach",
    "volume": "main",
    "abstract": "Normalizing flows (NFs) are universal density estimators based on neural networks. However, this universality is limited: the density's support needs to be diffeomorphic to a Euclidean space. In this paper, we propose a novel method to overcome this limitation without sacrificing universality. The proposed method inflates the data manifold by adding noise in the normal space, trains an NF on this inflated manifold, and, finally, deflates the learned density. Our main result provides sufficient conditions on the manifold and the specific choice of noise under which the corresponding estimator is exact. Our method has the same computational complexity as NFs and does not require computing an inverse flow. We also demonstrate theoretically (under certain conditions) and empirically (on a wide range of toy examples) that noise in the normal space can be well approximated by Gaussian noise. This allows using our method for approximating arbitrary densities on unknown manifolds provided that the manifold dimension is known",
    "checked": true,
    "id": "c8ecd0a6b062124dcc350d863960c6cef08f0412",
    "semantic_title": "density estimation on low-dimensional manifolds: an inflation-deflation approach",
    "citation_count": 11,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0249.html": {
    "title": "Monotonic Alpha-divergence Minimisation for Variational Inference",
    "volume": "main",
    "abstract": "In this paper, we introduce a novel family of iterative algorithms which carry out $\\alpha$-divergence minimisation in a Variational Inference context. They do so by ensuring a systematic decrease at each step in the $\\alpha$-divergence between the variational and the posterior distributions. In its most general form, the variational distribution is a mixture model and our framework allows us to simultaneously optimise the weights and components parameters of this mixture model. Our approach permits us to build on various methods previously proposed for $\\alpha$-divergence minimisation such as Gradient or Power Descent schemes and we also shed a new light on an integrated Expectation Maximization algorithm. Lastly, we provide empirical evidence that our methodology yields improved results on several multimodal target distributions and on a real data example",
    "checked": true,
    "id": "2cdf5466849e920217f560389feb119340a1927b",
    "semantic_title": "monotonic alpha-divergence minimisation for variational inference",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0389.html": {
    "title": "On the Complexity of SHAP-Score-Based Explanations: Tractability via Knowledge Compilation and Non-Approximability Results",
    "volume": "main",
    "abstract": "Scores based on Shapley values are widely used for providing explanations to classification results over machine learning models. A prime example of this is the influential~ Shap-score, a version of the Shapley value that can help explain the result of a learned model on a specific entity by assigning a score to every feature. While in general computing Shapley values is a computationally intractable problem, we prove a strong positive result stating that the Shap-score can be computed in polynomial time over deterministic and decomposable Boolean circuits under the so-called product distributions on entities. Such circuits are studied in the field of Knowledge Compilation and generalize a wide range of Boolean circuits and binary decision diagrams classes, including binary decision trees, Ordered Binary Decision Diagrams (OBDDs) and Free Binary Decision Diagrams (FBDDs). Our positive result extends even beyond binary classifiers, as it continues to hold if each feature is associated with a finite domain of possible values. We also establish the computational limits of the notion of Shap-score by observing that, under a mild condition, computing it over a class of Boolean models is always polynomially as hard as the model counting problem for that class. This implies that both determinism and decomposability are essential properties for the circuits that we consider, as removing one or the other renders the problem of computing the Shap-score intractable (namely, $\\#P$-hard). It also implies that computing Shap-scores is $\\#P$-hard even over the class of propositional formulas in DNF. Based on this negative result, we look for the existence of fully-polynomial randomized approximation schemes (FPRAS) for computing Shap-scores over such class. In stark contrast to the model counting problem for DNF formulas, which admits an FPRAS, we prove that no such FPRAS exists (under widely believed complexity assumptions) for the computation of Shap-scores. Surprisingly, this negative result holds even for the class of monotone formulas in DNF. These techniques can be further extended to prove another strong negative result: Under widely believed complexity assumptions, there is no polynomial-time algorithm that checks, given a monotone DNF formula $\\varphi$ and features $x,y$, whether the Shap-score of $x$ in $\\varphi$ is smaller than the Shap-score of $y$ in $\\varphi$",
    "checked": true,
    "id": "85b18ca8e61bf3e71985bd20b1da876c86eb7f1b",
    "semantic_title": "on the complexity of shap-score-based explanations: tractability via knowledge compilation and non-approximability results",
    "citation_count": 15,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0543.html": {
    "title": "Fundamental limits and algorithms for sparse linear regression with sublinear sparsity",
    "volume": "main",
    "abstract": "We establish exact asymptotic expressions for the normalized mutual information and minimum mean-square-error (MMSE) of sparse linear regression in the sub-linear sparsity regime. Our result is achieved by a generalization of the adaptive interpolation method in Bayesian inference for linear regimes to sub-linear ones. A modification of the well-known approximate message passing algorithm to approach the MMSE fundamental limit is also proposed, and its state evolution is rigorously analysed. Our results show that the traditional linear assumption between the signal dimension and number of observations in the replica and adaptive interpolation methods is not necessary for sparse signals. They also show how to modify the existing well-known AMP algorithms for linear regimes to sub-linear ones",
    "checked": true,
    "id": "774049aa143a3dcfad9b9def698d512bed3344dd",
    "semantic_title": "fundamental limits and algorithms for sparse linear regression with sublinear sparsity",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0549.html": {
    "title": "Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule",
    "volume": "main",
    "abstract": "Several papers argue that wide minima generalize better than narrow minima. In this paper, through detailed experiments that not only corroborate the generalization properties of wide minima, we also provide empirical evidence for a new hypothesis that the density of wide minima is likely lower than the density of narrow minima. Further, motivated by this hypothesis, we design a novel explore-exploit learning rate schedule. On a variety of image and natural language datasets, compared to their original hand-tuned learning rate baselines, we show that our explore-exploit schedule can result in either up to 0.84% higher absolute accuracy using the original training budget or up to 57% reduced training time while achieving the original reported accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0556.html": {
    "title": "Posterior Contraction for Deep Gaussian Process Priors",
    "volume": "main",
    "abstract": "We study posterior contraction rates for a class of deep Gaussian process priors in the nonparametric regression setting under a general composition assumption on the regression function. It is shown that the contraction rates can achieve the minimax convergence rate (up to log n factors), while being adaptive to the underlying structure and smoothness of the target function. The proposed framework extends the Bayesian nonparametric theory for Gaussian process priors",
    "checked": true,
    "id": "2d44a3de24cea01492489f6f7d3f3d2b7c37b220",
    "semantic_title": "posterior contraction for deep gaussian process priors",
    "citation_count": 4,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0623.html": {
    "title": "Prior Specification for Bayesian Matrix Factorization via Prior Predictive Matching",
    "volume": "main",
    "abstract": "The behavior of many Bayesian models used in machine learning critically depends on the choice of prior distributions, controlled by some hyperparameters typically selected through Bayesian optimization or cross-validation. This requires repeated, costly, posterior inference. We provide an alternative for selecting good priors without carrying out posterior inference, building on the prior predictive distribution that marginalizes the model parameters. We estimate virtual statistics for data generated by the prior predictive distribution and then optimize over the hyperparameters to learn those for which the virtual statistics match the target values provided by the user or estimated from (a subset of) the observed data. We apply the principle for probabilistic matrix factorization, for which good solutions for prior selection have been missing. We show that for Poisson factorization models we can analytically determine the hyperparameters, including the number of factors, that best replicate the target statistics, and we empirically study the sensitivity of the approach for the model mismatch. We also present a model-independent procedure that determines the hyperparameters for general models by stochastic optimization and demonstrate this extension in the context of hierarchical matrix factorization models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0673.html": {
    "title": "Distributed Nonparametric Regression Imputation for Missing Response Problems with Large-scale Data",
    "volume": "main",
    "abstract": "Nonparametric regression imputation is commonly used in missing data analysis. However, it suffers from the curse of dimension. The problem can be alleviated by the explosive sample size in the era of big data, while the large-scale data size presents some challenges in the storage of data and the calculation of estimators. These challenges make the classical nonparametric regression imputation methods no longer applicable. This motivates us to develop two distributed nonparametric regression imputation methods. One is based on kernel smoothing and the other on the sieve method. The kernel-based distributed imputation method has extremely low communication cost, and the sieve-based distributed imputation method can accommodate more local machines. The response mean estimation is considered to illustrate the proposed imputation methods. Two distributed nonparametric regression imputation estimators are proposed for the response mean, which are proved to be asymptotically normal with asymptotic variances achieving the semiparametric efficiency bound. The proposed methods are evaluated through simulation studies and illustrated in real data analysis",
    "checked": true,
    "id": "a2138ba39640a97407180d3661786fd4e9f4dbcf",
    "semantic_title": "distributed nonparametric regression imputation for missing response problems with large-scale data",
    "citation_count": 1,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0697.html": {
    "title": "When Locally Linear Embedding Hits Boundary",
    "volume": "main",
    "abstract": "Based on the Riemannian manifold model, we study the asymptotic behavior of a widely applied unsupervised learning algorithm, locally linear embedding (LLE), when the point cloud is sampled from a compact, smooth manifold with boundary. We show several peculiar behaviors of LLE near the boundary that are different from those diffusion-based algorithms. In particular, we show that LLE pointwisely converges to a mixed-type differential operator with degeneracy and we calculate the convergence rate. The impact of the hyperbolic part of the operator is discussed and we propose a clipped LLE algorithm which is a potential approach to recover the Dirichlet Laplace-Beltrami operator",
    "checked": false,
    "id": "d2a67de6c37d87a49e08ab2d0c839cc6956638f3",
    "semantic_title": "a new locally linear embedding scheme in light of hessian eigenmap",
    "citation_count": 1,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0751.html": {
    "title": "Optimizing ROC Curves with a Sort-Based Surrogate Loss for Binary Classification and Changepoint Detection",
    "volume": "main",
    "abstract": "Receiver Operating Characteristic (ROC) curves are useful for evaluating binary classification models, but difficult to use for learning since the Area Under the Curve (AUC) is a piecewise constant function of predicted values. ROC curves can also be used in other problems with false positive and true positive rates such as changepoint detection. We show that in this more general context, the ROC curve can have loops, points with highly sub-optimal error rates, and AUC greater than one. This observation motivates a new optimization objective: rather than maximizing the AUC, we would like a monotonic ROC curve with AUC=1 that avoids points with large values for Min(FP,FN). We propose an L1 relaxation of this objective that results in a new surrogate loss function called the AUM, short for Area Under Min(FP, FN). Whereas previous loss functions are based on summing over all labeled examples or pairs, the AUM requires a sort and a sum over the sequence of points on the ROC curve. We show that AUM directional derivatives can be efficiently computed and used in a gradient descent learning algorithm. In our empirical study of supervised binary classification and changepoint detection problems, we show that our new AUM minimization learning algorithm results in improved AUC and speed relative to previous baselines",
    "checked": true,
    "id": "95da20feda669fc2580963d89a0823271294624e",
    "semantic_title": "optimizing roc curves with a sort-based surrogate loss for binary classification and changepoint detection",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0781.html": {
    "title": "Kernel-Matrix Determinant Estimates from stopped Cholesky Decomposition",
    "volume": "main",
    "abstract": "Algorithms involving Gaussian processes or determinantal point processes typically require computing the determinant of a kernel matrix. Frequently, the latter is computed from the Cholesky decomposition, an algorithm of cubic complexity in the size of the matrix. We show that, under mild assumptions, it is possible to estimate the determinant from only a sub-matrix, with probabilistic guarantee on the relative error. We present an augmentation of the Cholesky decomposition that stops under certain conditions before processing the whole matrix. Experiments demonstrate that this can save a considerable amount of time while rarely exceeding an overhead of more than 5% when not stopping early. More generally, we present a probabilistic stopping strategy for the approximation of a sum of known length where addends are revealed sequentially. We do not assume independence between addends, only that they are bounded from below and decrease in conditional expectation",
    "checked": true,
    "id": "fe7cffe2e44318876920dd1174cb2a0b616b2e79",
    "semantic_title": "kernel-matrix determinant estimates from stopped cholesky decomposition",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0782.html": {
    "title": "How Do You Want Your Greedy: Simultaneous or Repeated?",
    "volume": "main",
    "abstract": "We present SimulatneousGreedys, a deterministic algorithm for constrained submodular maximization. At a high level, the algorithm maintains $\\ell$ solutions and greedily updates them in a simultaneous fashion. SimultaneousGreedys achieves the tightest known approximation guarantees for both $k$-extendible systems and the more general $k$-systems, which are $(k+1)^2/k = k + \\mathcal{O}(1)$ and $(1 + \\sqrt{k+2})^2 = k + \\mathcal{O}(\\sqrt{k})$, respectively. We also improve the analysis of RepeatedGreedy, showing that it achieves an approximation ratio of $k + \\mathcal{O}(\\sqrt{k})$ for $k$-systems when allowed to run for $\\mathcal{O}(\\sqrt{k})$ iterations, an improvement in both the runtime and approximation over previous analyses. We demonstrate that both algorithms may be modified to run in nearly linear time with an arbitrarily small loss in the approximation. Both SimultaneousGreedys and RepeatedGreedy are flexible enough to incorporate the intersection of $m$ additional knapsack constraints, while retaining similar approximation guarantees: both algorithms yield an approximation guarantee of roughly $k + 2m + \\mathcal{O}(\\sqrt{k+m})$ for $k$-systems and SimultaneousGreedys enjoys an improved approximation guarantee of $k+2m + \\mathcal{O}(\\sqrt{m})$ for $k$-extendible systems. To complement our algorithmic contributions, we prove that no algorithm making polynomially many oracle queries can achieve an approximation better than $k + 1/2 - \\epsilon$. We also present SubmodularGreedy.jl, a Julia package which implements these algorithms. Finally, we test these algorithms on real datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0855.html": {
    "title": "Inference for a Large Directed Acyclic Graph with Unspecified Interventions",
    "volume": "main",
    "abstract": "Statistical inference of directed relations given some unspecified interventions (i.e., the intervention targets are unknown) is challenging. In this article, we test hypothesized directed relations with unspecified interventions. First, we derive conditions to yield an identifiable model. Unlike classical inference, testing directed relations requires identifying the ancestors and relevant interventions of hypothesis-specific primary variables. To this end, we propose a peeling algorithm based on nodewise regressions to establish a topological order of primary variables. Moreover, we prove that the peeling algorithm yields a consistent estimator in low-order polynomial time. Second, we propose a likelihood ratio test integrated with a data perturbation scheme to account for the uncertainty of identifying ancestors and interventions. Also, we show that the distribution of a data perturbation test statistic converges to the target distribution. Numerical examples demonstrate the utility and effectiveness of the proposed methods, including an application to infer gene regulatory networks",
    "checked": true,
    "id": "018d699d687cfdd46741249cad371b3ea10f2347",
    "semantic_title": "inference for a large directed acyclic graph with unspecified interventions",
    "citation_count": 3,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0870.html": {
    "title": "Privacy-Aware Rejection Sampling",
    "volume": "main",
    "abstract": "While differential privacy (DP) offers strong theoretical privacy guarantees, implementations of DP mechanisms may be vulnerable to side-channel attacks, such as timing attacks. When sampling methods such as MCMC or rejection sampling are used to implement a privacy mechanism, the runtime can leak private information. We characterize the additional privacy cost due to the runtime of a rejection sampler in terms of both $(\\epsilon,\\delta)$-DP as well as $f$-DP. We also show that unless the acceptance probability is constant across databases, the runtime of a rejection sampler does not satisfy $\\epsilon$-DP for any $\\epsilon$. We show that there is a similar breakdown in privacy with adaptive rejection samplers. We propose three modifications to the rejection sampling algorithm, with varying assumptions, to protect against timing attacks by making the runtime independent of the data. The modification with the weakest assumptions is an approximate sampler, introducing a small increase in the privacy cost, whereas the other modifications give perfect samplers. We also use our techniques to develop an adaptive rejection sampler for log-H√∂lder densities, which also has data-independent runtime. We give several examples of DP mechanisms that fit the assumptions of our methods and can thus be implemented using our samplers",
    "checked": true,
    "id": "2e938bbde056b79e3a96348df5cb6a8599f6f76b",
    "semantic_title": "privacy-aware rejection sampling",
    "citation_count": 4,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1044.html": {
    "title": "Intrinsic Persistent Homology via Density-based Metric Learning",
    "volume": "main",
    "abstract": "We address the problem of estimating topological features from data in high dimensional Euclidean spaces under the manifold assumption. Our approach is based on the computation of persistent homology of the space of data points endowed with a sample metric known as Fermat distance. We prove that such metric space converges almost surely to the manifold itself endowed with an intrinsic metric that accounts for both the geometry of the manifold and the density that produces the sample. This fact implies the convergence of the associated persistence diagrams. The use of this intrinsic distance when computing persistent homology presents advantageous properties such as robustness to the presence of outliers in the input data and less sensitiveness to the particular embedding of the underlying manifold in the ambient space. We use these ideas to propose and implement a method for pattern recognition and anomaly detection in time series, which is evaluated in applications to real data",
    "checked": false,
    "id": "311855820df19c8c6d9c31870fa0ec315031fe75",
    "semantic_title": "tlife-gdn: detecting and forecasting spatio-temporal anomalies via persistent homology and geometric deep learning",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1046.html": {
    "title": "A Randomized Subspace-based Approach for Dimensionality Reduction and Important Variable Selection",
    "volume": "main",
    "abstract": "An analysis of high-dimensional data can offer a detailed description of a system but is often challenged by the curse of dimensionality. General dimensionality reduction techniques can alleviate such difficulty by extracting a few important features, but they are limited due to the lack of interpretability and connectivity to actual decision making associated with each physical variable. Variable selection techniques, as an alternative, can maintain the interpretability, but they often involve a greedy search that is susceptible to failure in capturing important interactions or a metaheuristic search that requires extensive computations. This research proposes a novel method that identifies critical subspaces, reduced-dimensional physical spaces, to achieve dimensionality reduction and variable selection. We apply a randomized search for subspace exploration and leverage ensemble techniques to enhance model performance. When applied to high-dimensional data collected from the failure prediction of a composite/metal hybrid structure exhibiting complex progressive damage failure under loading, the proposed method outperforms the existing and potential alternatives in prediction and important variable selection",
    "checked": true,
    "id": "9740219fe1a5d2df5514bb210be80dde7f5443cb",
    "semantic_title": "a randomized subspace-based approach for dimensionality reduction and important variable selection",
    "citation_count": 1,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1099.html": {
    "title": "A Likelihood Approach to Nonparametric Estimation of a Singular Distribution Using Deep Generative Models",
    "volume": "main",
    "abstract": "We investigate statistical properties of a likelihood approach to nonparametric estimation of a singular distribution using deep generative models. More specifically, a deep generative model is used to model high-dimensional data that are assumed to concentrate around some low-dimensional structure. Estimating the distribution supported on this low-dimensional structure, such as a low-dimensional manifold, is challenging due to its singularity with respect to the Lebesgue measure in the ambient space. In the considered model, a usual likelihood approach can fail to estimate the target distribution consistently due to the singularity. We prove that a novel and effective solution exists by perturbing the data with an instance noise, which leads to consistent estimation of the underlying distribution with desirable convergence rates. We also characterize the class of distributions that can be efficiently estimated via deep generative models. This class is sufficiently general to contain various structured distributions such as product distributions, classically smooth distributions and distributions supported on a low-dimensional manifold. Our analysis provides some insights on how deep generative models can avoid the curse of dimensionality for nonparametric distribution estimation. We conduct a thorough simulation study and real data analysis to empirically demonstrate that the proposed data perturbation technique improves the estimation performance significantly",
    "checked": true,
    "id": "9bfa1bf454384168d01d80aff788ea6ddd1fd6b1",
    "semantic_title": "a likelihood approach to nonparametric estimation of a singular distribution using deep generative models",
    "citation_count": 10,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1174.html": {
    "title": "Towards Learning to Imitate from a Single Video Demonstration",
    "volume": "main",
    "abstract": "Agents that can learn to imitate behaviours observed in video -- without having direct access to internal state or action information of the observed agent -- are more suitable for learning in the natural world. However, formulating a reinforcement learning (RL) agent that facilitates this goal remains a significant challenge. We approach this challenge using contrastive training to learn a reward function by comparing an agent's behaviour with a single demonstration. We use a Siamese recurrent neural network architecture to learn rewards in space and time between motion clips while training an RL policy to minimize this distance. Through experimentation, we also find that the inclusion of multi-task data and additional image encoding losses improve the temporal consistency of the learned rewards and, as a result, significantly improve policy learning. We demonstrate our approach on simulated humanoid, dog, and raptor agents in 2D and quadruped and humanoid agents in 3D. We show that our method outperforms current state-of-the-art techniques and can learn to imitate behaviours from a single video demonstration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1186.html": {
    "title": "Approximate Post-Selective Inference for Regression with the Group LASSO",
    "volume": "main",
    "abstract": "After selection with the Group LASSO (or generalized variants such as the overlapping, sparse, or standardized Group LASSO), inference for the selected parameters is unreliable in the absence of adjustments for selection bias. In the penalized Gaussian regression setup, existing approaches provide adjustments for selection events that can be expressed as linear inequalities in the data variables. Such a representation, however, fails to hold for selection with the Group LASSO and substantially obstructs the scope of subsequent post-selective inference. Key questions of inferential interest, e.g., inference for the effects of selected variables on the outcome, remain unanswered. In the present paper, we develop a consistent, post-selective, Bayesian method to address the existing gaps by deriving a likelihood adjustment factor and an approximation thereof that eliminates bias from the selection of groups. Experiments on simulated data and data from the Human Connectome Project demonstrate that our method recovers the effects of parameters within the selected groups while paying only a small price for bias adjustment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1213.html": {
    "title": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
    "volume": "main",
    "abstract": "Reasoning at multiple levels of temporal abstraction is one of the key attributes of intelligence. In reinforcement learning, this is often modeled through temporally extended courses of actions called options. Options allow agents to make predictions and to operate at different levels of abstraction within an environment. Nevertheless, approaches based on the options framework often start with the assumption that a reasonable set of options is known beforehand. When this is not the case, there are no definitive answers for which options one should consider. In this paper, we argue that the successor representation, which encodes states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstractions. To support our claim, we take a big picture view of recent results, showing how the successor representation can be used to discover options that facilitate either temporally-extended exploration or planning. We cast these results as instantiations of a general framework for option discovery in which the agent's representation is used to identify useful options, which are then used to further improve its representation. This results in a virtuous, never-ending, cycle in which both the representation and the options are constantly refined based on each other. Beyond option discovery itself, we also discuss how the successor representation allows us to augment a set of options into a combinatorially large counterpart without additional learning. This is achieved through the combination of previously learned options. Our empirical evaluation focuses on options discovered for temporally-extended exploration and on the use of the successor representation to combine them. Our results shed light on important design decisions involved in the definition of options and demonstrate the synergy of different methods based on the successor representation, such as eigenoptions and the option keyboard",
    "checked": true,
    "id": "ccb01a90b16b119db0201ed012f989ed48c68d9f",
    "semantic_title": "temporal abstraction in reinforcement learning with the successor representation",
    "citation_count": 14,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1230.html": {
    "title": "Fast Online Changepoint Detection via Functional Pruning CUSUM Statistics",
    "volume": "main",
    "abstract": "Many modern applications of online changepoint detection require the ability to process high-frequency observations, sometimes with limited available computational resources. Online algorithms for detecting a change in mean often involve using a moving window, or specifying the expected size of change. Such choices affect which changes the algorithms have most power to detect. We introduce an algorithm, Functional Online CuSUM (FOCuS), which is equivalent to running these earlier methods simultaneously for all sizes of windows, or all possible values for the size of change. Our theoretical results give tight bounds on the expected computational cost per iteration of FOCuS, with this being logarithmic in the number of observations. We show how FOCuS can be applied to a number of different changes in mean scenarios, and demonstrate its practical utility through its state-of-the-art performance at detecting anomalous behaviour in computer server data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1253.html": {
    "title": "Iterated Block Particle Filter for High-dimensional Parameter Learning: Beating the Curse of Dimensionality",
    "volume": "main",
    "abstract": "Parameter learning for high-dimensional, partially observed, and nonlinear stochastic processes is a methodological challenge. Spatiotemporal disease transmission systems provide examples of such processes giving rise to open inference problems. We propose the iterated block particle filter (IBPF) algorithm for learning high-dimensional parameters over graphical state space models with general state spaces, measures, transition densities and graph structure. Theoretical performance guarantees are obtained on beating the curse of dimensionality (COD), algorithm convergence, and likelihood maximization. Experiments on a highly nonlinear and non-Gaussian spatiotemporal model for measles transmission reveal that the iterated ensemble Kalman filter algorithm (Li et al., 2020) is ineffective and the iterated filtering algorithm (Ionides et al., 2015) suffers from the COD, while our IBPF algorithm beats COD consistently across various experiments with different metrics",
    "checked": true,
    "id": "40ce49aa47a19e9ccfbee5a3f2aebcbb05110bef",
    "semantic_title": "iterated block particle filter for high-dimensional parameter learning: beating the curse of dimensionality",
    "citation_count": 6,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1298.html": {
    "title": "Bayes-Newton Methods for Approximate Bayesian Inference with PSD Guarantees",
    "volume": "main",
    "abstract": "We formulate natural gradient variational inference (VI), expectation propagation (EP), and posterior linearisation (PL) as extensions of Newton's method for optimising the parameters of a Bayesian posterior distribution. This viewpoint explicitly casts inference algorithms under the framework of numerical optimisation. We show that common approximations to Newton's method from the optimisation literature, namely Gauss-Newton and quasi-Newton methods (e.g., the BFGS algorithm), are still valid under this 'Bayes-Newton' framework. This leads to a suite of novel algorithms which are guaranteed to result in positive semi-definite (PSD) covariance matrices, unlike standard VI and EP. Our unifying viewpoint provides new insights into the connections between various inference schemes. All the presented methods apply to any model with a Gaussian prior and non-conjugate likelihood, which we demonstrate with (sparse) Gaussian processes and state space models",
    "checked": true,
    "id": "3e0be3b1c950bb5fb507b0ed2b0ebd3871f58d56",
    "semantic_title": "bayes-newton methods for approximate bayesian inference with psd guarantees",
    "citation_count": 8,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1308.html": {
    "title": "Online Optimization over Riemannian Manifolds",
    "volume": "main",
    "abstract": "Online optimization has witnessed a massive surge of research attention in recent years. In this paper, we propose online gradient descent and online bandit algorithms over Riemannian manifolds in full information and bandit feedback settings respectively, for both geodesically convex and strongly geodesically convex functions. We establish a series of upper bounds on the regrets for the proposed algorithms over Hadamard manifolds. We also find a universal lower bound for achievable regret on Hadamard manifolds. Our analysis shows how time horizon, dimension, and sectional curvature bounds have impact on the regret bounds. When the manifold permits positive sectional curvature, we prove similar regret bound can be established by handling non-constrictive project maps. In addition, numerical studies on problems defined on symmetric positive definite matrix manifold, hyperbolic spaces, and Grassmann manifolds are provided to validate our theoretical findings, using synthetic and real-world data",
    "checked": true,
    "id": "b923bfc7af90f431ca066d6cb409b5260dc28366",
    "semantic_title": "online optimization over riemannian manifolds",
    "citation_count": 3,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1313.html": {
    "title": "Doubly Robust Stein-Kernelized Monte Carlo Estimator: Simultaneous Bias-Variance Reduction and Supercanonical Convergence",
    "volume": "main",
    "abstract": "Standard Monte Carlo computation is widely known to exhibit a canonical square-root convergence speed in terms of sample size. Two recent techniques, one based on control variate and one on importance sampling, both derived from an integration of reproducing kernels and Stein's identity, have been proposed to reduce the error in Monte Carlo computation to supercanonical convergence. This paper presents a more general framework to encompass both techniques that is especially beneficial when the sample generator is biased and noise-corrupted. We show our general estimator, which we call the doubly robust Stein-kernelized estimator, outperforms both existing methods in terms of mean squared error rates across different scenarios. We also demonstrate the superior performance of our method via numerical examples",
    "checked": true,
    "id": "ccec6a81dffed8462bf7c39e1a50381bf50ddf03",
    "semantic_title": "doubly robust stein-kernelized monte carlo estimator: simultaneous bias-variance reduction and supercanonical convergence",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1363.html": {
    "title": "Learning Partial Differential Equations in Reproducing Kernel Hilbert Spaces",
    "volume": "main",
    "abstract": "We propose a new data-driven approach for learning the fundamental solutions (Green's functions) of various linear partial differential equations (PDEs) given sample pairs of input-output functions. Building off the theory of functional linear regression (FLR), we estimate the best-fit Green's function and bias term of the fundamental solution in a reproducing kernel Hilbert space (RKHS) which allows us to regularize their smoothness and impose various structural constraints. We derive a general representer theorem for operator RKHSs to approximate the original infinite-dimensional regression problem by a finite-dimensional one, reducing the search space to a parametric class of Green's functions. In order to study the prediction error of our Green's function estimator, we extend prior results on FLR with scalar outputs to the case with functional outputs. Finally, we demonstrate our method on several linear PDEs including the Poisson, Helmholtz, Schr√∂dinger, Fokker-Planck, and heat equation. We highlight its robustness to noise as well as its ability to generalize to new data with varying degrees of smoothness and mesh discretization without any additional training",
    "checked": true,
    "id": "d7db503a1a8cf6d6cd5ba8070caefe14926c1ffe",
    "semantic_title": "learning partial differential equations in reproducing kernel hilbert spaces",
    "citation_count": 7,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1480.html": {
    "title": "Gaussian Processes with Errors in Variables: Theory and Computation",
    "volume": "main",
    "abstract": "Covariate measurement error in nonparametric regression is a common problem in nutritional epidemiology and geostatistics, and other fields. Over the last two decades, this problem has received substantial attention in the frequentist literature. Bayesian approaches for handling measurement error have only been explored recently and are surprisingly successful, although there still is a lack of a proper theoretical justification regarding the asymptotic performance of the estimators. By specifying a Gaussian process prior on the regression function and a Dirichlet process Gaussian mixture prior on the unknown distribution of the unobserved covariates, we show that the posterior distribution of the regression function and the unknown covariate density attain optimal rates of contraction adaptively over a range of Holder classes, up to logarithmic terms. We also develop a novel surrogate prior for approximating the Gaussian process prior that leads to efficient computation and preserves the covariance structure, thereby facilitating easy prior elicitation. We demonstrate the empirical performance of our approach and compare it with competitors in a wide range of simulation experiments and a real data example",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1513.html": {
    "title": "Dimension-Grouped Mixed Membership Models for Multivariate Categorical Data",
    "volume": "main",
    "abstract": "Mixed Membership Models (MMMs) are a popular family of latent structure models for complex multivariate data. Instead of forcing each subject to belong to a single cluster, MMMs incorporate a vector of subject-specific weights characterizing partial membership across clusters. With this flexibility come challenges in uniquely identifying, estimating, and interpreting the parameters. In this article, we propose a new class of Dimension-Grouped MMMs (Gro-M$^3$s) for multivariate categorical data, which improve parsimony and interpretability. In Gro-M$^3$s, observed variables are partitioned into groups such that the latent membership is constant for variables within a group but can differ across groups. Traditional latent class models are obtained when all variables are in one group, while traditional MMMs are obtained when each variable is in its own group. The new model corresponds to a novel decomposition of probability tensors. Theoretically, we derive transparent identifiability conditions for both the unknown grouping structure and model parameters in general settings. Methodologically, we propose a Bayesian approach for Dirichlet Gro-M$^3$s to inferring the variable grouping structure and estimating model parameters. Simulation results demonstrate good computational performance and empirically confirm the identifiability results. We illustrate the new methodology through applications to a functional disability survey dataset and a personality test dataset",
    "checked": true,
    "id": "6349602c4f4bef5f190bade8fe17a50a62c8187f",
    "semantic_title": "dimension-grouped mixed membership models for multivariate categorical data",
    "citation_count": 4,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1524.html": {
    "title": "Neural Operator: Learning Maps Between Function Spaces With Applications to PDEs",
    "volume": "main",
    "abstract": "The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps for the solution operators of partial differential equations (PDEs). We consider standard PDEs such as the Burgers, Darcy subsurface flow, and the Navier-Stokes equations, and show that the proposed neural operators have superior performance compared to existing machine learning based methodologies, while being several orders of magnitude faster than conventional PDE solvers",
    "checked": true,
    "id": "f3c546e922a85839c1f19cd0a96a25c4986dbce8",
    "semantic_title": "neural operator: learning maps between function spaces with applications to pdes",
    "citation_count": 36,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1526.html": {
    "title": "Outlier-Robust Subsampling Techniques for Persistent Homology",
    "volume": "main",
    "abstract": "In recent years, persistent homology has been successfully applied to real-world data in many different settings. Despite significant computational advances, persistent homology algorithms do not yet scale to large datasets preventing interesting applications. One approach to address computational issues posed by persistent homology is to select a set of landmarks by subsampling from the data. Currently, these landmark points are chosen either at random or using the maxmin algorithm. Neither is ideal as random selection tends to favour dense areas of the data while the maxmin algorithm is very sensitive to noise. Here, we propose a novel approach to select landmarks specifically for persistent homology that preserves coarse topological information of the original dataset. Our method is motivated by the Mayer-Vietoris sequence and requires only local persistent homology calculations thus enabling efficient computation. We test our landmarks on artificial data sets which contain different levels of noise and compare them to standard landmark selection techniques. We demonstrate that our landmark selection outperforms standard methods as well as a subsampling technique based on an outlier-robust version of the k-means algorithm for low sampling densities in noisy data with respect to robustness to outliers",
    "checked": true,
    "id": "487b535016c49a83d2a8e7a395ef0cb83441a128",
    "semantic_title": "outlier-robust subsampling techniques for persistent homology",
    "citation_count": 3,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0021.html": {
    "title": "Recursive Quantile Estimation: Non-Asymptotic Confidence Bounds",
    "volume": "main",
    "abstract": "This paper considers the recursive estimation of quantiles using the stochastic gradient descent (SGD) algorithm with Polyak-Ruppert averaging. The algorithm offers a computationally and memory efficient alternative to the usual empirical estimator. Our focus is on studying the non-asymptotic behavior by providing exponentially decreasing tail probability bounds under mild assumptions on the smoothness of the density functions. This novel non-asymptotic result is based on a bound of the moment generating function of the SGD estimate. We apply our result to the problem of best arm identification in a multi-armed stochastic bandit setting under quantile preferences",
    "checked": true,
    "id": "decbfc356600332a30af4d7c84bd8f8724e875d1",
    "semantic_title": "recursive quantile estimation: non-asymptotic confidence bounds",
    "citation_count": 1,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0034.html": {
    "title": "Non-Asymptotic Guarantees for Robust Statistical Learning under Infinite Variance Assumption",
    "volume": "main",
    "abstract": "There has been a surge of interest in developing robust estimators for models with heavy-tailed and bounded variance data in statistics and machine learning, while few works impose unbounded variance. This paper proposes two types of robust estimators, the ridge log-truncated M-estimator and the elastic net log-truncated M-estimator. The first estimator is applied to convex regressions such as quantile regression and generalized linear models, while the other one is applied to high dimensional non-convex learning problems such as regressions via deep neural networks. Simulations and real data analysis demonstrate the robustness of log-truncated estimations over standard estimations",
    "checked": true,
    "id": "d95446b66c62435243ab8aa67174ecce00d66f88",
    "semantic_title": "non-asymptotic guarantees for robust statistical learning under infinite variance assumption",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0044.html": {
    "title": "Decentralized Learning: Theoretical Optimality and Practical Improvements",
    "volume": "main",
    "abstract": "Decentralization is a promising method of scaling up parallel machine learning systems. In this paper, we provide a tight lower bound on the iteration complexity for such methods in a stochastic non-convex setting. Our lower bound reveals a theoretical gap in known convergence rates of many existing decentralized training algorithms, such as D-PSGD. We prove by construction this lower bound is tight and achievable. Motivated by our insights, we further propose DeTAG, a practical gossip-style decentralized algorithm that achieves the lower bound with only a logarithm gap. While a simple version of DeTAG with plain SGD and constant step size suffice for achieving theoretical limits, we additionally provide convergence bound for DeTAG under general non-increasing step size and momentum. Empirically, we compare DeTAG with other decentralized algorithms on multiple vision benchmarks, including CIFAR10/100 and ImageNet. We substantiate our theory and show DeTAG converges faster on unshuffled data and in sparse networks. Furthermore, we study a DeTAG variant, DeTAG*, that practically speeds up data-center-scale model training. This manuscript provides extended contents to its ICML version",
    "checked": true,
    "id": "f5b78a60bd96cd30f7a0a6d448fe8bec16bc16ba",
    "semantic_title": "decentralized learning: theoretical optimality and practical improvements",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0202.html": {
    "title": "Faith-Shap: The Faithful Shapley Interaction Index",
    "volume": "main",
    "abstract": "Shapley values, which were originally designed to assign attributions to individual players in coalition games, have become a commonly used approach in explainable machine learning to provide attributions to input features for black-box machine learning models. A key attraction of Shapley values is that they uniquely satisfy a very natural set of axiomatic properties. However, extending the Shapley value to assigning attributions to interactions rather than individual players, an interaction index, is non-trivial: as the natural set of axioms for the original Shapley values, extended to the context of interactions, no longer specify a unique interaction index. Many proposals thus introduce additional possibly stringent axioms, while sacrificing the key axiom of efficiency, in order to obtain unique interaction indices. In this work, rather than introduce additional conflicting axioms, we adopt the viewpoint of Shapley values as coefficients of the most faithful linear approximation to the pseudo-Boolean coalition game value function. By extending linear to higher order polynomial approximations, we can then define the general family of faithful interaction indices. We show that by additionally requiring the faithful interaction indices to satisfy interaction-extensions of the standard individual Shapley axioms (dummy, symmetry, linearity, and efficiency), we obtain a unique Faithful Shapley Interaction index, which we denote Faith-Shap, as a natural generalization of the Shapley value to interactions. We then provide some illustrative contrasts of Faith-Shap with previously proposed interaction indices, and further investigate some of its interesting algebraic properties. We further show the computational efficiency of computing Faith-Shap, together with some additional qualitative insights, via some illustrative experiments",
    "checked": true,
    "id": "47c380459a9311c0686c2e17ef05313821133cd7",
    "semantic_title": "faith-shap: the faithful shapley interaction index",
    "citation_count": 16,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0214.html": {
    "title": "Statistical Inference for Noisy Incomplete Binary Matrix",
    "volume": "main",
    "abstract": "We consider the statistical inference for noisy incomplete binary (or 1-bit) matrix. Despite the importance of uncertainty quantification to matrix completion, most of the categorical matrix completion literature focuses on point estimation and prediction. This paper moves one step further toward statistical inference for binary matrix completion. Under a popular nonlinear factor analysis model, we obtain a point estimator and derive its asymptotic normality. Moreover, our analysis adopts a flexible missing-entry design that does not require a random sampling scheme as required by most of the existing asymptotic results for matrix completion. Under reasonable conditions, the proposed estimator is statistically efficient and optimal in the sense that the Cramer-Rao lower bound is achieved asymptotically for the model parameters. Two applications are considered, including (1) linking two forms of an educational test and (2) linking the roll call voting records from multiple years in the United States Senate. The first application enables the comparison between examinees who took different test forms, and the second application allows us to compare the liberal-conservativeness of senators who did not serve in the Senate at the same time",
    "checked": true,
    "id": "7890ac76fa24a8ec6ea1f20bec2a6fff047809ab",
    "semantic_title": "statistical inference for noisy incomplete binary matrix",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0233.html": {
    "title": "Global Convergence of Sub-gradient Method for Robust Matrix Recovery: Small Initialization, Noisy Measurements, and Over-parameterization",
    "volume": "main",
    "abstract": "In this work, we study the performance of sub-gradient method (SubGM) on a natural nonconvex and nonsmooth formulation of low-rank matrix recovery with $\\ell_1$-loss, where the goal is to recover a low-rank matrix from a limited number of measurements, a subset of which may be grossly corrupted with noise. We study a scenario where the rank of the true solution is unknown and over-estimated instead. The over-estimation of the rank gives rise to an over-parameterized model in which there are more degrees of freedom than needed. Such over-parameterization may lead to overfitting, or adversely affect the performance of the algorithm. We prove that a simple SubGM with small initialization is agnostic to both over-parameterization and noise in the measurements. In particular, we show that small initialization nullifies the effect of over-parameterization on the performance of SubGM, leading to an exponential improvement in its convergence rate. Moreover, we provide the first unifying framework for analyzing the behavior of SubGM under both outlier and Gaussian noise models, showing that SubGM converges to the true solution, even under arbitrarily large and arbitrarily dense noise values, and, perhaps surprisingly, even if the globally optimal solutions do not correspond to the ground truth. At the core of our results is a robust variant of restricted isometry property, called Sign-RIP, which controls the deviation of the sub-differential of the $\\ell_1$-loss from that of an ideal, expected loss. As a byproduct of our results, we consider a subclass of robust low-rank matrix recovery with Gaussian measurements, and show that the number of required samples to guarantee the global convergence of SubGM is independent of the over-parameterized rank",
    "checked": true,
    "id": "aefc65f8b98c118305c84a9f4ec962f7fc833eee",
    "semantic_title": "global convergence of sub-gradient method for robust matrix recovery: small initialization, noisy measurements, and over-parameterization",
    "citation_count": 16,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0337.html": {
    "title": "Fitting Autoregressive Graph Generative Models through Maximum Likelihood Estimation",
    "volume": "main",
    "abstract": "We consider the problem of fitting autoregressive graph generative models via maximum likelihood estimation (MLE). MLE is intractable for graph autoregressive models because the nodes in a graph can be arbitrarily reordered; thus the exact likelihood involves a sum over all possible node orders leading to the same graph. In this work, we fit the graph models by maximizing a variational bound, which is built by first deriving the joint probability over the graph and the node order of the autoregressive process. This approach avoids the need to specify ad-hoc node orders, since an inference network learns the most likely node sequences that have generated a given graph. We improve the approach by developing a graph generative model based on attention mechanisms and an inference network based on routing search. We demonstrate empirically that fitting autoregressive graph models via variational inference improves their qualitative and quantitative performance, and the improved model and inference network further boost the performance",
    "checked": true,
    "id": "aca43adabb6d19e08823ac7113b4b50600c349a0",
    "semantic_title": "fitting autoregressive graph generative models through maximum likelihood estimation",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0410.html": {
    "title": "An Analysis of Robustness of Non-Lipschitz Networks",
    "volume": "main",
    "abstract": "Despite significant advances, deep networks remain highly susceptible to adversarial attack. One fundamental challenge is that small input perturbations can often produce large movements in the network's final-layer feature space. In this paper, we define an attack model that abstracts this challenge, to help understand its intrinsic properties. In our model, the adversary may move data an arbitrary distance in feature space but only in random low-dimensional subspaces. We prove such adversaries can be quite powerful: defeating any algorithm that must classify any input it is given. However, by allowing the algorithm to abstain on unusual inputs, we show such adversaries can be overcome when classes are reasonably well-separated in feature space. We further provide strong theoretical guarantees for setting algorithm parameters to optimize over accuracy-abstention trade-offs using data-driven methods. Our results provide new robustness guarantees for nearest-neighbor style algorithms, and also have application to contrastive learning, where we empirically demonstrate the ability of such algorithms to obtain high robust accuracy with low abstention rates. Our model is also motivated by strategic classification, where entities being classified aim to manipulate their observable features to produce a preferred classification, and we provide new insights into that area as well",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0415.html": {
    "title": "Connectivity Matters: Neural Network Pruning Through the Lens of Effective Sparsity",
    "volume": "main",
    "abstract": "Neural network pruning is a fruitful area of research with surging interest in high sparsity regimes. Benchmarking in this domain heavily relies on faithful representation of the sparsity of subnetworks, which has been traditionally computed as the fraction of removed connections (direct sparsity). This definition, however, fails to recognize unpruned parameters that detached from input or output layers of the underlying subnetworks, potentially underestimating actual effective sparsity: the fraction of inactivated connections. While this effect might be negligible for moderately pruned networks (up to 10‚Äì100 compression rates), we find that it plays an increasing role for sparser subnetworks, greatly distorting comparison between different pruning algorithms. For example, we show that effective compression of a randomly pruned LeNet-300-100 can be orders of magnitude larger than its direct counterpart, while no discrepancy is ever observed when using SynFlow for pruning (Tanaka et al., 2020). In this work, we adopt the lens of effective sparsity to reevaluate several recent pruning algorithms on common benchmark architectures (e.g., LeNet-300-100, VGG-19, ResNet-18) and discover that their absolute and relative performance changes dramatically in this new, and as we argue, more appropriate framework. To aim for effective, rather than direct, sparsity, we develop a low-cost extension to most pruning algorithms. Further, equipped with effective sparsity as a reference frame, we partially reconfirm that random pruning with appropriate sparsity allocation across layers performs as well or better than more sophisticated algorithms for pruning at initialization (Su et al., 2020). In response to this observation, using an analogy of pressure distribution in coupled cylinders from thermodynamics, we design novel layerwise sparsity quotas that outperform all existing baselines in the context of random pruning",
    "checked": true,
    "id": "4ecb57ba76714ed4f14d11d3b30548225b2f15bc",
    "semantic_title": "connectivity matters: neural network pruning through the lens of effective sparsity",
    "citation_count": 9,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0440.html": {
    "title": "FedLab: A Flexible Federated Learning Framework",
    "volume": "MLOSS",
    "abstract": "FedLab is a lightweight open-source framework for the simulation of federated learning. The design of FedLab focuses on federated learning algorithm effectiveness and communication efficiency. It allows customization on server optimization, client optimization, communication agreement, and communication compression. Also, FedLab is scalable in different deployment scenarios with different computation and communication resources. We hope FedLab could provide flexible APIs as well as reliable baseline implementations and relieve the burden of implementing novel approaches for researchers in the FL community. The source code, tutorial, and documentation can be found at https://github.com/SMILELab-FL/FedLab",
    "checked": true,
    "id": "c0430712b4f12affbcc07d5b07942a16efca58ea",
    "semantic_title": "fedlab: a flexible federated learning framework",
    "citation_count": 32,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0503.html": {
    "title": "Inference for Gaussian Processes with Matern Covariogram on Compact Riemannian Manifolds",
    "volume": "main",
    "abstract": "Gaussian processes are widely employed as versatile modelling and predictive tools in spatial statistics, functional data analysis, computer modelling and diverse applications of machine learning. They have been widely studied over Euclidean spaces, where they are specified using covariance functions or covariograms for modelling complex dependencies. There is a growing literature on Gaussian processes over Riemannian manifolds in order to develop richer and more flexible inferential frameworks for non-Euclidean data. While numerical approximations through graph representations have been well studied for the Matern covariogram and heat kernel, the behaviour of asymptotic inference on the parameters of the covariogram has received relatively scant attention. We focus on asymptotic behaviour for Gaussian processes constructed over compact Riemannian manifolds. Building upon a recently introduced Matern covariogram on a compact Riemannian manifold, we employ formal notions and conditions for the equivalence of two Matern Gaussian random measures on compact manifolds to derive the parameter that is identifiable, also known as the microergodic parameter, and formally establish the consistency of the maximum likelihood estimate and the asymptotic optimality of the best linear unbiased predictor. The circle is studied as a specific example of compact Riemannian manifolds with numerical experiments to illustrate and corroborate the theory",
    "checked": false,
    "id": "9daf507ff265b4cf947f2b7c9abc6fbc46317376",
    "semantic_title": "inference for gaussian processes with mat√©rn covariogram on compact riemannian manifolds",
    "citation_count": 3,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0520.html": {
    "title": "Learning Optimal Group-structured Individualized Treatment Rules with Many Treatments",
    "volume": "main",
    "abstract": "Data driven individualized decision making problems have received a lot of attentions in recent years. In particular, decision makers aim to determine the optimal Individualized Treatment Rule (ITR) so that the expected specified outcome averaging over heterogeneous patient-specific characteristics is maximized. Many existing methods deal with binary or a moderate number of treatment arms and may not take potential treatment effect structure into account. However, the effectiveness of these methods may deteriorate when the number of treatment arms becomes large. In this article, we propose GRoup Outcome Weighted Learning (GROWL) to estimate the latent structure in the treatment space and the optimal group-structured ITRs through a single optimization. In particular, for estimating group-structured ITRs, we utilize the Reinforced Angle based Multicategory Support Vector Machines (RAMSVM) to learn group-based decision rules under the weighted angle based multi-class classification framework. Fisher consistency, the excess risk bound, and the convergence rate of the value function are established to provide a theoretical guarantee for GROWL. Extensive empirical results in simulation studies and real data analysis demonstrate that GROWL enjoys better performance than several other existing methods",
    "checked": true,
    "id": "67078e23f0317dd51229e3298d8a7de7d239adad",
    "semantic_title": "learning optimal group-structured individualized treatment rules with many treatments",
    "citation_count": 1,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0615.html": {
    "title": "Sparse Training with Lipschitz Continuous Loss Functions and a Weighted Group L0-norm Constraint",
    "volume": "main",
    "abstract": "This paper is motivated by structured sparsity for deep neural network training. We study a weighted group $l_0$-norm constraint, and present the projection and normal cone of this set. Using randomized smoothing, we develop zeroth and first-order algorithms for minimizing a Lipschitz continuous function constrained by any closed set which can be projected onto. Non-asymptotic convergence guarantees are proven in expectation for the proposed algorithms for two related convergence criteria which can be considered as approximate stationary points. Two further methods are given using the proposed algorithms: one with non-asymptotic convergence guarantees in high probability, and the other with asymptotic guarantees to a stationary point almost surely. We believe in particular that these are the first such non-asymptotic convergence results for constrained Lipschitz continuous loss functions",
    "checked": true,
    "id": "3849d4f3050188fa603278997da8cb9b0f78d6da",
    "semantic_title": "sparse training with lipschitz continuous loss functions and a weighted group l0-norm constraint",
    "citation_count": 1,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0627.html": {
    "title": "Intrinsic Gaussian Process on Unknown Manifolds with Probabilistic Metrics",
    "volume": "main",
    "abstract": "This article presents a novel approach to construct Intrinsic Gaussian Processes for regression on unknown manifolds with probabilistic metrics (GPUM) in point clouds. In many real world applications, one often encounters high dimensional data (e.g.‚Äòpoint cloud data') centered around some lower dimensional unknown manifolds. The geometry of manifold is in general different from the usual Euclidean geometry. Naively applying traditional smoothing methods such as Euclidean Gaussian Processes (GPs) to manifold-valued data and so ignoring the geometry of the space can potentially lead to highly misleading predictions and inferences. A manifold embedded in a high dimensional Euclidean space can be well described by a probabilistic mapping function and the corresponding latent space. We investigate the geometrical structure of the unknown manifolds using the Bayesian Gaussian Processes latent variable models(B-GPLVM) and Riemannian geometry. The distribution of the metric tensor is learned using B-GPLVM. The boundary of the resulting manifold is defined based on the uncertainty quantification of the mapping. We use the probabilistic metric tensor to simulate Brownian Motion paths on the unknown manifold. The heat kernel is estimated as the transition density of Brownian Motion and used as the covariance functions of GPUM. The applications of GPUM are illustrated in the simulation studies on the Swiss roll, high dimensional real datasets of WiFi signals and image data examples. Its performance is compared with the Graph Laplacian GP, Graph Mat\\'{e}rn GP and Euclidean GP",
    "checked": true,
    "id": "c0ebe62a5b733633c3b8196f66d9fb76902966b4",
    "semantic_title": "intrinsic gaussian process on unknown manifolds with probabilistic metrics",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-063.html": {
    "title": "Knowledge Hypergraph Embedding Meets Relational Algebra",
    "volume": "main",
    "abstract": "Relational databases are a successful model for data storage, and rely on query languages for information retrieval. Most of these query languages are based on relational algebra, a mathematical formalization at the core of relational models. Knowledge graphs are flexible data storage structures that allow for knowledge completion using machine learning techniques. Knowledge hypergraphs generalize knowledge graphs by allowing multi-argument relations. This work studies knowledge hypergraph completion through the lens of relational algebra and its core operations. We explore the space between relational algebra foundations and machine learning techniques for knowledge completion. We investigate whether such methods can capture high-level abstractions in terms of relational algebra operations. We propose a simple embedding-based model called Relational Algebra Embedding (ReAlE) that performs link prediction in knowledge hypergraphs. We show theoretically that ReAlE is fully expressive and can represent the relational algebra operations of renaming, projection, set union, selection, and set difference. We verify experimentally that ReAlE outperforms state-of-the-art models in knowledge hypergraph completion, and in representing each of these primitive relational algebra operations. For the latter experiment, we generate a synthetic knowledge hypergraph, for which we design an algorithm based on the Erdos-R'enyi model for generating random graphs",
    "checked": true,
    "id": "40cbdf5ac5dac74584d7080c0343b68833d30a82",
    "semantic_title": "knowledge hypergraph embedding meets relational algebra",
    "citation_count": 1,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0666.html": {
    "title": "Concentration analysis of multivariate elliptic diffusions",
    "volume": "main",
    "abstract": "We prove concentration inequalities and associated PAC bounds for both continuous- and discrete-time additive functionals for possibly unbounded functions of multivariate, nonreversible diffusion processes. Our analysis relies on an approach via the Poisson equation allowing us to consider a very broad class of subexponentially ergodic, multivariate diffusion processes. These results add to existing concentration inequalities for additive functionals of diffusion processes which have so far been only available for either bounded functions or for unbounded functions of processes from a significantly smaller class. We demonstrate the power of these exponential inequalities by two examples of very different areas. Considering a possibly high-dimensional, parametric, nonlinear drift model under sparsity constraints we apply the continuous-time concentration results to validate the restricted eigenvalue condition for Lasso estimation, which is fundamental for the derivation of oracle inequalities. The results for discrete additive functionals are applied for an investigation of the unadjusted Langevin MCMC algorithm for sampling of moderately heavy tailed densities $\\pi$. In particular, we provide PAC bounds for the sample Monte Carlo estimator of integrals $\\pi(f)$ for polynomially growing functions $f$ that quantify sufficient sample and step sizes for approximation within a prescribed margin with high probability",
    "checked": true,
    "id": "8033b002a8048ace294c7b4975025c11230ba4b8",
    "semantic_title": "concentration analysis of multivariate elliptic diffusions",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-067.html": {
    "title": "Risk Bounds for Positive-Unlabeled Learning Under the Selected At Random Assumption",
    "volume": "main",
    "abstract": "Positive-Unlabeled learning (PU learning) is a special case of semi-supervised binary classification where only a fraction of positive examples is labeled. The challenge is then to find the correct classifier despite this lack of information. Recently, new methodologies have been introduced to address the case where the probability of being labeled may depend on the covariates. In this paper, we are interested in establishing risk bounds for PU learning under this general assumption. In addition, we quantify the impact of label noise on PU learning compared to the standard classification setting. Finally, we provide a lower bound on the minimax risk proving that the upper bound is almost optimal",
    "checked": true,
    "id": "ab5dd2de983a00eb72097d8586c956ef62e4fb9b",
    "semantic_title": "risk bounds for positive-unlabeled learning under the selected at random assumption",
    "citation_count": 1,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0676.html": {
    "title": "Bayesian Calibration of Imperfect Computer Models using Physics-Informed Priors",
    "volume": "main",
    "abstract": "We introduce a computational efficient data-driven framework suitable for quantifying the uncertainty in physical parameters and model formulation of computer models, represented by differential equations. We construct physics-informed priors, which are multi-output GP priors that encode the model's structure in the covariance function. This is extended into a fully Bayesian framework that quantifies the uncertainty of physical parameters and model predictions. Since physical models often are imperfect descriptions of the real process, we allow the model to deviate from the observed data by considering a discrepancy function. For inference Hamiltonian Monte Carlo is used. Further, approximations for big data are developed that reduce the computational complexity from $\\mathcal{O}(N^3)$ to $\\mathcal{O}(N\\cdot m^2),$ where $m \\ll N.$ Our approach is demonstrated in simulation and real data case studies where the physics are described by time-dependent ODEs (cardiovascular models) and space-time dependent PDEs (heat equation). In the studies, it is shown that our modelling framework can recover the true parameters of the physical models in cases where 1) the reality is more complex than our modelling choice and 2) the data acquisition process is biased while also producing accurate predictions. Furthermore, it is demonstrated that our approach is computationally faster than traditional Bayesian calibration methods",
    "checked": true,
    "id": "0f2b6a40b60e4b628dee3231d122bcf5267dce2e",
    "semantic_title": "bayesian calibration of imperfect computer models using physics-informed priors",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0680.html": {
    "title": "Dimensionless machine learning: Imposing exact units equivariance",
    "volume": "main",
    "abstract": "Units equivariance (or units covariance) is the exact symmetry that follows from the requirement that relationships among measured quantities of physics relevance must obey self-consistent dimensional scalings. Here, we express this symmetry in terms of a (non-compact) group action, and we employ dimensional analysis and ideas from equivariant machine learning to provide a methodology for exactly units-equivariant machine learning: For any given learning task, we first construct a dimensionless version of its inputs using classic results from dimensional analysis and then perform inference in the dimensionless space. Our approach can be used to impose units equivariance across a broad range of machine learning methods that are equivariant to rotations and other groups. We discuss the in-sample and out-of-sample prediction accuracy gains one can obtain in contexts like symbolic regression and emulation, where symmetry is important. We illustrate our approach with simple numerical examples involving dynamical systems in physics and ecology",
    "checked": true,
    "id": "6ef8388468b3206b605475775b877f9ef073e729",
    "semantic_title": "dimensionless machine learning: imposing exact units equivariance",
    "citation_count": 10,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0689.html": {
    "title": "A General Theory for Federated Optimization with Asynchronous and Heterogeneous Clients Updates",
    "volume": "main",
    "abstract": "We propose a novel framework to study asynchronous federated learning optimization with delays in gradient updates. Our theoretical framework extends the standard FedAvg aggregation scheme by introducing stochastic aggregation weights to represent the variability of the clients update time, due for example to heterogeneous hardware capabilities. Our formalism applies to the general federated setting where clients have heterogeneous datasets and perform at least one step of stochastic gradient descent (SGD). We demonstrate convergence for such a scheme and provide sufficient conditions for the related minimum to be the optimum of the federated problem. We show that our general framework applies to existing optimization schemes including centralized learning, FedAvg, asynchronous FedAvg, and FedBuff. The theory here provided allows drawing meaningful guidelines for designing a federated learning experiment in heterogeneous conditions. In particular, we develop in this work FedFix, a novel extension of FedAvg enabling efficient asynchronous federated training while preserving the convergence stability of synchronous aggregation. We empirically demonstrate our theory on a series of experiments showing that asynchronous FedAvg leads to fast convergence at the expense of stability, and we finally demonstrate the improvements of FedFix over synchronous and asynchronous FedAvg",
    "checked": true,
    "id": "32786d4cbf3763606ba8a144dc8a88c124fc261c",
    "semantic_title": "a general theory for federated optimization with asynchronous and heterogeneous clients updates",
    "citation_count": 3,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0734.html": {
    "title": "FLIP: A Utility Preserving Privacy Mechanism for Time Series",
    "volume": "main",
    "abstract": "Guaranteeing privacy in released data is an important goal for data-producing agencies. There has been extensive research on developing suitable privacy mechanisms in recent years. Particularly notable is the idea of noise addition with the guarantee of differential privacy. There are, however, concerns about compromising data utility when very stringent privacy mechanisms are applied. Such compromises can be quite stark in correlated data, such as time series data. Adding white noise to a stochastic process may significantly change the correlation structure, a facet of the process that is essential to optimal prediction. We propose the use of all-pass filtering as a privacy mechanism for regularly sampled time series data, showing that this procedure preserves certain types of utility while also providing sufficient privacy guarantees to entity-level time series. Numerical studies explore the practical performance of the new method, and an empirical application to labor force data show the method's favorable utility properties in comparison to other competing privacy mechanisms",
    "checked": true,
    "id": "2da2a900f66be4f4ab161f6bd42091b14b7fc0de",
    "semantic_title": "flip: a utility preserving privacy mechanism for time series",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0744.html": {
    "title": "The Hyperspherical Geometry of Community Detection: Modularity as a Distance",
    "volume": "main",
    "abstract": "We introduce a metric space of clusterings, where clusterings are described by a binary vector indexed by the vertex-pairs. We extend this geometry to a hypersphere and prove that maximizing modularity is equivalent to minimizing the angular distance to some modularity vector over the set of clustering vectors. In that sense, modularity-based community detection methods can be seen as a subclass of a more general class of projection methods, which we define as the community detection methods that adhere to the following two-step procedure: first, mapping the network to a point on the hypersphere; second, projecting this point to the set of clustering vectors. We show that this class of projection methods contains many interesting community detection methods. Many of these new methods cannot be described in terms of null models and resolution parameters, as is customary for modularity-based methods. We provide a new characterization of such methods in terms of meridians and latitudes of the hypersphere. In addition, by relating the modularity resolution parameter to the latitude of the corresponding modularity vector, we obtain a new interpretation of the resolution limit that modularity maximization is known to suffer from",
    "checked": true,
    "id": "0b459f0451f555bd244902b7eb54aaf3233c843f",
    "semantic_title": "the hyperspherical geometry of community detection: modularity as a distance",
    "citation_count": 1,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0784.html": {
    "title": "The Implicit Bias of Benign Overfitting",
    "volume": "main",
    "abstract": "The phenomenon of benign overfitting, where a predictor perfectly fits noisy training data while attaining near-optimal expected loss, has received much attention in recent years, but still remains not fully understood beyond well-specified linear regression setups. In this paper, we provide several new results on when one can or cannot expect benign overfitting to occur, for both regression and classification tasks. We consider a prototypical and rather generic data model for benign overfitting of linear predictors, where an arbitrary input distribution of some fixed dimension $k$ is concatenated with a high-dimensional distribution. For linear regression which is not necessarily well-specified, we show that the minimum-norm interpolating predictor (that standard training methods converge to) is biased towards an inconsistent solution in general, hence benign overfitting will generally *not* occur. Moreover, we show how this can be extended beyond standard linear regression, by an argument proving how the existence of benign overfitting on some regression problems precludes its existence on other regression problems. We then turn to classification problems, and show that the situation there is much more favorable. Specifically, we prove that the max-margin predictor (to which standard training methods are known to converge in direction) is asymptotically biased towards minimizing a weighted squared hinge loss. This allows us to reduce the question of benign overfitting in classification to the simpler question of whether this loss is a good surrogate for the misclassification error, and use it to show benign overfitting in some new settings",
    "checked": true,
    "id": "6332429953b748de47d9615960a69f13947ab7f2",
    "semantic_title": "the implicit bias of benign overfitting",
    "citation_count": 27,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0866.html": {
    "title": "Generalization Bounds for Adversarial Contrastive Learning",
    "volume": "main",
    "abstract": "Deep networks are well-known to be fragile to adversarial attacks, and adversarial training is one of the most popular methods used to train a robust model. To take advantage of unlabeled data, recent works have applied adversarial training to contrastive learning (Adversarial Contrastive Learning; ACL for short) and obtain promising robust performance. However, the theory of ACL is not well understood. To fill this gap, we leverage the Rademacher omplexity to analyze the generalization performance of ACL, with a particular focus on linear models and multi-layer neural networks under $\\ell_p$ attack ($p \\ge 1$). Our theory shows that the average adversarial risk of the downstream tasks can be upper bounded by the adversarial unsupervised risk of the upstream task. The experimental results validate our theory",
    "checked": true,
    "id": "3990671bec3e7cb019dc7ae4a6d71ea0af1a49cb",
    "semantic_title": "generalization bounds for adversarial contrastive learning",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0917.html": {
    "title": "Learning Good State and Action Representations for Markov Decision Process via Tensor Decomposition",
    "volume": "main",
    "abstract": "The transition kernel of a continuous-state-action Markov decision process (MDP) admits a natural tensor structure. This paper proposes a tensor-inspired unsupervised learning method to identify meaningful low-dimensional state and action representations from empirical trajectories. The method exploits the MDP's tensor structure by kernelization, importance sampling and low-Tucker-rank approximation. This method can be further used to cluster states and actions respectively and find the best discrete MDP abstraction. We provide sharp statistical error bounds for tensor concentration and the preservation of diffusion distance after embedding. We further prove that the learned state/action abstractions provide accurate approximations to latent block structures if they exist, enabling function approximation in downstream tasks such as policy evaluation",
    "checked": true,
    "id": "9d08e6c635324aff2fdf44ca8cba6cc8bacc9814",
    "semantic_title": "learning good state and action representations for markov decision process via tensor decomposition",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1047.html": {
    "title": "SQLFlow: An Extensible Toolkit Integrating DB and AI",
    "volume": "MLOSS",
    "abstract": "Integrating AI algorithms into databases is an ongoing effort in both academia and industry. We introduce SQLFlow, a toolkit seamlessly combining data manipulations and AI operations that can be run locally or remotely. SQLFlow extends SQL syntax to support typical AI tasks including model training, inference, interpretation, and mathematical optimization. It is compatible with a variety of database management systems (DBMS) and AI engines, including MySQL, TiDB, MaxCompute, and Hive, as well as TensorFlow, scikit-learn, and XGBoost. Documentations and case studies are available at https://sqlflow.org. The source code and additional details can be found at https://github.com/sql-machine-learning/sqlflow",
    "checked": true,
    "id": "e49dd78b6731b1c5168694434a15b915e55e0f9b",
    "semantic_title": "sqlflow: an extensible toolkit integrating db and ai",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1065.html": {
    "title": "Deep linear networks can benignly overfit when shallow ones do",
    "volume": "main",
    "abstract": "We bound the excess risk of interpolating deep linear networks trained using gradient flow. In a setting previously used to establish risk bounds for the minimum $\\ell_2$-norm interpolant, we show that randomly initialized deep linear networks can closely approximate or even match known bounds for the minimum $\\ell_2$-norm interpolant. Our analysis also reveals that interpolating deep linear models have exactly the same conditional variance as the minimum $\\ell_2$-norm solution. Since the noise affects the excess risk only through the conditional variance, this implies that depth does not improve the algorithm's ability to \"hide the noise\". Our simulations verify that aspects of our bounds reflect typical behavior for simple data distributions. We also find that similar phenomena are seen in simulations with ReLU networks, although the situation there is more nuanced",
    "checked": true,
    "id": "69f325400c16670a490d1eb628223ef3b0b2d4e0",
    "semantic_title": "deep linear networks can benignly overfit when shallow ones do",
    "citation_count": 6,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1085.html": {
    "title": "A Unified Framework for Optimization-Based Graph Coarsening",
    "volume": "main",
    "abstract": "Graph coarsening is a widely used dimensionality reduction technique for approaching large-scale graph machine-learning problems. Given a large graph, graph coarsening aims to learn a smaller-tractable graph while preserving the properties of the originally given graph. Graph data consist of node features and graph matrix (e.g., adjacency and Laplacian). The existing graph coarsening methods ignore the node features and rely solely on a graph matrix to simplify graphs. In this paper, we introduce a novel optimization-based framework for graph dimensionality reduction. The proposed framework lies in the unification of graph learning and dimensionality reduction. It takes both the graph matrix and the node features as the input and learns the coarsen graph matrix and the coarsen feature matrix jointly while ensuring desired properties. The proposed optimization formulation is a multi-block non-convex optimization problem, which is solved efficiently by leveraging block majorization-minimization, $\\log$ determinant, Dirichlet energy, and regularization frameworks. The proposed algorithms are provably convergent and practically amenable to numerous tasks. It is also established that the learned coarsened graph is $\\epsilon\\in(0,1)$ similar to the original graph. Extensive experiments elucidate the efficacy of the proposed framework for real-world applications",
    "checked": true,
    "id": "1e5e079a19372155961cda3394c4691dd05ef98e",
    "semantic_title": "a unified framework for optimization-based graph coarsening",
    "citation_count": 1,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1138.html": {
    "title": "An Annotated Graph Model with Differential Degree Heterogeneity for Directed Networks",
    "volume": "main",
    "abstract": "Directed networks are conveniently represented as graphs in which ordered edges encode interactions between vertices. Despite their wide availability, there is a shortage of statistical models amenable for inference, specially when contextual information and degree heterogeneity are present. This paper presents an annotated graph model with parameters explicitly accounting for these features. To overcome the curse of dimensionality due to modelling degree heterogeneity, we introduce a sparsity assumption and propose a penalized likelihood approach with $\\ell_1$-regularization for parameter estimation. We study the estimation and selection consistency of this approach under a sparse network assumption, and show that inference on the covariate parameter is straightforward, thus bypassing the need for the kind of debiasing commonly employed in $\\ell_1$-penalized likelihood estimation. Simulation and data analysis corroborate our theoretical findings",
    "checked": true,
    "id": "11e4d10086b28b2b5629bb0b11868cabf76c1ac0",
    "semantic_title": "an annotated graph model with differential degree heterogeneity for directed networks",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1153.html": {
    "title": "Maximum likelihood estimation in Gaussian process regression is ill-posed",
    "volume": "main",
    "abstract": "Gaussian process regression underpins countless academic and industrial applications of machine learning and statistics, with maximum likelihood estimation routinely used to select appropriate parameters for the covariance kernel. However, it remains an open problem to establish the circumstances in which maximum likelihood estimation is well-posed, that is, when the predictions of the regression model are insensitive to small perturbations of the data. This article identifies scenarios where the maximum likelihood estimator fails to be well-posed, in that the predictive distributions are not Lipschitz in the data with respect to the Hellinger distance. These failure cases occur in the noiseless data setting, for any Gaussian process with a stationary covariance function whose lengthscale parameter is estimated using maximum likelihood. Although the failure of maximum likelihood estimation is part of Gaussian process folklore, these rigorous theoretical results appear to be the first of their kind. The implication of these negative results is that well-posedness may need to be assessed post-hoc, on a case-by-case basis, when maximum likelihood estimation is used to train a Gaussian process model",
    "checked": true,
    "id": "bf239aeebc2ab7edb18837d02f1ab06192aa7d6e",
    "semantic_title": "maximum likelihood estimation in gaussian process regression is ill-posed",
    "citation_count": 10,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1191.html": {
    "title": "Minimal Width for Universal Property of Deep RNN",
    "volume": "main",
    "abstract": "A recurrent neural network (RNN) is a widely used deep-learning network for dealing with sequential data. Imitating a dynamical system, an infinite-width RNN can approximate any open dynamical system in a compact domain. In general, deep narrow networks with bounded width and arbitrary depth are more effective than wide shallow networks with arbitrary width and bounded depth in practice; however, the universal approximation theorem for deep narrow structures has yet to be extensively studied. In this study, we prove the universality of deep narrow RNNs and show that the upper bound of the minimum width for universality can be independent of the length of the data. Specifically, we show a deep RNN with ReLU activation can approximate any continuous function or $L^p$ function with the widths $d_x+d_y+3$ and $\\max\\{d_x+1,d_y\\}$, respectively, where the target function maps a finite sequence of vectors in $\\mathbb{R}^{d_x}$ to a finite sequence of vectors in $\\mathbb{R}^{d_y}$. We also compute the additional width required if the activation function is sigmoid or more. In addition, we prove the universality of other recurrent networks, such as bidirectional RNNs. Bridging a multi-layer perceptron and an RNN, our theory and technique can shed light on further research on deep RNNs",
    "checked": true,
    "id": "eea7c0f878622be8fe437ed3c1f268439ba90b21",
    "semantic_title": "minimal width for universal property of deep rnn",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1208.html": {
    "title": "Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities",
    "volume": "main",
    "abstract": "Although deep learning has made great progress in recent years, the exploding economic and environmental costs of training neural networks are becoming unsustainable. To address this problem, there has been a great deal of research on *algorithmically-efficient deep learning*, which seeks to reduce training costs not at the hardware or implementation level, but through changes in the semantics of the training program. In this paper, we present a structured and comprehensive overview of the research in this field. First, we formalize the *algorithmic speedup* problem, then we use fundamental building blocks of algorithmically efficient training to develop a taxonomy. Our taxonomy highlights commonalities of seemingly disparate methods and reveals current research gaps. Next, we present evaluation best practices to enable comprehensive, fair, and reliable comparisons of speedup techniques. To further aid research and applications, we discuss common bottlenecks in the training pipeline (illustrated via experiments) and offer taxonomic mitigation strategies for them. Finally, we highlight some unsolved research challenges and present promising future directions",
    "checked": true,
    "id": "59fed7ca092c7e83583906456756abba8ce9295a",
    "semantic_title": "compute-efficient deep learning: algorithmic trends and opportunities",
    "citation_count": 16,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1398.html": {
    "title": "Benign overfitting in ridge regression",
    "volume": "main",
    "abstract": "In many modern applications of deep learning the neural network has many more parameters than the data points used for its training. Motivated by those practices, a large body of recent theoretical research has been devoted to studying overparameterized models. One of the central phenomena in this regime is the ability of the model to interpolate noisy data, but still have test error lower than the amount of noise in that data. arXiv:1906.11300 characterized for which covariance structure of the data such a phenomenon can happen in linear regression if one considers the interpolating solution with minimum $\\ell_2$-norm and the data has independent components: they gave a sharp bound on the variance term and showed that it can be small if and only if the data covariance has high effective rank in a subspace of small co-dimension. We strengthen and complete their results by eliminating the independence assumption and providing sharp bounds for the bias term. Thus, our results apply in a much more general setting than those of arXiv:1906.11300, e.g., kernel regression, and not only characterize how the noise is damped but also which part of the true signal is learned. Moreover, we extend the result to the setting of ridge regression, which allows us to explain another interesting phenomenon: we give general sufficient conditions under which the optimal regularization is negative",
    "checked": false,
    "id": "187a6fd505a90249359bc609bacdae93429d9fc7",
    "semantic_title": "dimension free ridge regression",
    "citation_count": 9,
    "authors": []
  },
  "https://jmlr.org/papers/v24/18-521.html": {
    "title": "HiGrad: Uncertainty Quantification for Online Learning and Stochastic Approximation",
    "volume": "main",
    "abstract": "Stochastic gradient descent (SGD) is an immensely popular approach for online learning in settings where data arrives in a stream or data sizes are very large. However, despite an ever-increasing volume of work on SGD, much less is known about the statistical inferential properties of SGD-based predictions. Taking a fully inferential viewpoint, this paper introduces a novel procedure termed HiGrad to conduct statistical inference for online learning, without incurring additional computational cost compared with SGD. The HiGrad procedure begins by performing SGD updates for a while and then splits the single thread into several threads, and this procedure hierarchically operates in this fashion along each thread. With predictions provided by multiple threads in place, a $t$-based confidence interval is constructed by decorrelating predictions using covariance structures given by a Donsker-style extension of the Ruppert--Polyak averaging scheme, which is a technical contribution of independent interest. Under certain regularity conditions, the HiGrad confidence interval is shown to attain asymptotically exact coverage probability. Finally, the performance of HiGrad is evaluated through extensive simulation studies and a real data example. An R package \\texttt{higrad} has been developed to implement the method",
    "checked": true,
    "id": "731fb221b411e0b916e86130aa22979f35197119",
    "semantic_title": "higrad: uncertainty quantification for online learning and stochastic approximation",
    "citation_count": 3,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1039.html": {
    "title": "Statistical Robustness of Empirical Risks in Machine Learning",
    "volume": "main",
    "abstract": "This paper studies convergence of empirical risks in reproducing kernel Hilbert spaces (RKHS). A conventional assumption in the existing research is that empirical training data are generated by the unknown true probability distribution but this may not be satisfied in some practical circumstances. Consequently the existing convergence results may not provide a guarantee as to whether the empirical risks are reliable or not when the data are potentially corrupted (generated by a distribution perturbed from the true). In this paper, we fill out the gap from robust statistics perspective (Kr√§tschmer, Schied and Z√§hle (2012); Kr√§tschmer, Schied and Z√§hle (2014); Guo and Xu (2020). First, we derive moderate sufficient conditions under which the expected risk changes stably (continuously) against small perturbation of the probability distributions of the underlying random variables and demonstrate how the cost function and kernel affect the stability. Second, we examine the difference between laws of the statistical estimators of the expected optimal loss based on pure data and contaminated data using Prokhorov metric and Kantorovich metric, and derive some asymptotic qualitative and non-asymptotic quantitative statistical robustness results. Third, we identify appropriate metrics under which the statistical estimators are uniformly asymptotically consistent. These results provide theoretical grounding for analysing asymptotic convergence and examining reliability of the statistical estimators in a number of regression models",
    "checked": false,
    "id": "37c4313c54e0b6511f6eec1114f3953fbbde62c0",
    "semantic_title": "statistical arbitrage risk premium by machine learning",
    "citation_count": 2,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1390.html": {
    "title": "Euler-Lagrange Analysis of Generative Adversarial Networks",
    "volume": "main",
    "abstract": "We consider Generative Adversarial Networks (GANs) and address the underlying functional optimization problem ab initio within a variational setting. Strictly speaking, the optimization of the generator and discriminator functions must be carried out in accordance with the Euler-Lagrange conditions, which become particularly relevant in scenarios where the optimization cost involves regularizers comprising the derivatives of these functions. Considering Wasserstein GANs (WGANs) with a gradient-norm penalty, we show that the optimal discriminator is the solution to a Poisson differential equation. In principle, the optimal discriminator can be obtained in closed form without having to train a neural network. We illustrate this by employing a Fourier-series approximation to solve the Poisson differential equation. Experimental results based on synthesized Gaussian data demonstrate superior convergence behavior of the proposed approach in comparison with the baseline WGAN variants that employ weight-clipping, gradient or Lipschitz penalties on the discriminator on low-dimensional data. We also analyze the truncation error of the Fourier-series approximation and the estimation error of the Fourier coefficients in a high-dimensional setting. We demonstrate applications to real-world images considering latent-space prior matching in Wasserstein autoencoders and present performance comparisons on benchmark datasets such as MNIST, SVHN, CelebA, CIFAR-10, and Ukiyo-E. We demonstrate that the proposed approach achieves comparable reconstruction error and Frechet inception distance with faster convergence and up to two-fold improvement in image sharpness",
    "checked": true,
    "id": "1fe312e8ee2f7ac393b8dc25b1bdfb6614bc1a8c",
    "semantic_title": "euler-lagrange analysis of generative adversarial networks",
    "citation_count": 3,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-998.html": {
    "title": "Graph Clustering with Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art results on many graph analysis tasks such as node classification and link prediction. However, important unsupervised problems on graphs, such as graph clustering, have proved more resistant to advances in GNNs. Graph clustering has the same overall goal as node pooling in GNNs‚Äîdoes this mean that GNN pooling methods do a good job at clustering graphs? Surprisingly, the answer is no‚Äîcurrent GNN pooling methods often fail to recover the cluster structure in cases where simple baselines, such as k-means applied on learned representations, work well. We investigate further by carefully designing a set of experiments to study different signal-to-noise scenarios both in graph structure and attribute data. To address these methods' poor performance in clustering, we introduce Deep Modularity Networks (DMoN), an unsupervised pooling method inspired by the modularity measure of clustering quality, and show how it tackles recovery of the challenging clustering structure of real-world graphs. Similarly, on real-world data, we show that DMoN produces high quality clusters which correlate strongly with ground truth labels, achieving state-of-the-art results with over 40% improvement over other pooling methods across different metrics",
    "checked": false,
    "id": "d8ac1d47be3bb24e1bd48f8c7ea8da7c1ff046d6",
    "semantic_title": "simplifying clustering with graph neural networks",
    "citation_count": 3,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0270.html": {
    "title": "An Eigenmodel for Dynamic Multilayer Networks",
    "volume": "main",
    "abstract": "Dynamic multilayer networks frequently represent the structure of multiple co-evolving relations; however, statistical models are not well-developed for this prevalent network type. Here, we propose a new latent space model for dynamic multilayer networks. The key feature of our model is its ability to identify common time-varying structures shared by all layers while also accounting for layer-wise variation and degree heterogeneity. We establish the identifiability of the model's parameters and develop a structured mean-field variational inference approach to estimate the model's posterior, which scales to networks previously intractable to dynamic latent space models. We demonstrate the estimation procedure's accuracy and scalability on simulated networks. We apply the model to two real-world problems: discerning regional conflicts in a data set of international relations and quantifying infectious disease spread throughout a school based on the student's daily contact patterns",
    "checked": true,
    "id": "7120d26424418c26722e7ceef31ed65fc235dff4",
    "semantic_title": "an eigenmodel for dynamic multilayer networks",
    "citation_count": 3,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0445.html": {
    "title": "A First Look into the Carbon Footprint of Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0449.html": {
    "title": "Combinatorial Optimization and Reasoning with Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0482.html": {
    "title": "A Rigorous Information-Theoretic Definition of Redundancy and Relevancy in Feature Selection Based on (Partial) Information Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0523.html": {
    "title": "Generalized Linear Models in Non-interactive Local Differential Privacy with Public Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0670.html": {
    "title": "Exploiting Discovered Regression Discontinuities to Debias Conditioned-on-observable Estimators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0699.html": {
    "title": "MARS: A Second-Order Reduction Algorithm for High-Dimensional Sparse Precision Matrices Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0745.html": {
    "title": "Sparse GCA and Thresholded Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0818.html": {
    "title": "Dimension Reduction in Contextual Online Learning via Nonparametric Variable Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0832.html": {
    "title": "Implicit Bias of Gradient Descent for Mean Squared Error Regression with Two-Layer Wide Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0841.html": {
    "title": "Asymptotics of Network Embeddings Learned via Subsampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0842.html": {
    "title": "Policy Gradient Methods Find the Nash Equilibrium in N-player General-sum Linear-quadratic Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0843.html": {
    "title": "Jump Interval-Learning for Individualized Decision Making with Continuous Treatments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1049.html": {
    "title": "Optimal Convergence Rates for Distributed Nystroem Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1095.html": {
    "title": "On Tilted Losses in Machine Learning: Theory and Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1254.html": {
    "title": "Large sample spectral analysis of graph-based multi-manifold clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1276.html": {
    "title": "Escaping The Curse of Dimensionality in Bayesian Model-Based Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1301.html": {
    "title": "Memory-Based Optimization Methods for Model-Agnostic Meta-Learning and Personalized Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1350.html": {
    "title": "Off-Policy Actor-Critic with Emphatic Weightings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1410.html": {
    "title": "Stochastic Optimization under Distributional Drift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1471.html": {
    "title": "Fast Objective & Duality Gap Convergence for Non-Convex Strongly-Concave Min-Max Problems with PL Condition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1516.html": {
    "title": "Controlling Wasserstein Distances by Kernel Norms with Application to Compressive Statistical Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0169.html": {
    "title": "MALib: A Parallel Framework for Population-based Multi-agent Reinforcement Learning",
    "volume": "MLOSS",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0367.html": {
    "title": "Generalization error bounds for multiclass sparse linear classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0371.html": {
    "title": "Selective inference for k-means clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0436.html": {
    "title": "Consistent Model-based Clustering using the Quasi-Bernoulli Stick-breaking Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0449.html": {
    "title": "Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0497.html": {
    "title": "Adaptive Data Depth via Multi-Armed Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0501.html": {
    "title": "Integrating Random Effects in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0522.html": {
    "title": "Restarted Nonconvex Accelerated Gradient Descent: No More Polylogarithmic Factor in the in the O(epsilon^(-7/4)) Complexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0555.html": {
    "title": "Asynchronous Iterations in Optimization: New Sequence Results and Sharper Algorithmic Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0582.html": {
    "title": "Infinite-dimensional optimization and Bayesian nonparametric learning of stochastic differential equations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0616.html": {
    "title": "Multivariate Soft Rank via Entropy-Regularized Optimal Transport: Sample Efficiency and Generative Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0755.html": {
    "title": "q-Learning in Continuous Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0799.html": {
    "title": "Flexible Model Aggregation for Quantile Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0882.html": {
    "title": "Preconditioned Gradient Descent for Overparameterized Nonconvex Burer--Monteiro Factorization with Global Optimality Certification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0937.html": {
    "title": "A Framework and Benchmark for Deep Batch Active Learning for Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0964.html": {
    "title": "Robust Methods for High-Dimensional Linear Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0983.html": {
    "title": "A Parameter-Free Conditional Gradient Method for Composite Minimization under H√∂lder Condition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1043.html": {
    "title": "Bilevel Optimization with a Lower-level Contraction: Optimal Sample Complexity without Warm-Start",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1122.html": {
    "title": "Inference on the Change Point under a High Dimensional Covariance Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1131.html": {
    "title": "DART: Distance Assisted Recursive Testing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1246.html": {
    "title": "Small Transformers Compute Universal Metric Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1395.html": {
    "title": "Incremental Learning in Diagonal Linear Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1488.html": {
    "title": "Beyond the Golden Ratio for Variational Inequality Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/23-0106.html": {
    "title": "From Classification Accuracy to Proper Scoring Rules: Elicitability of Probabilistic Top List Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1012.html": {
    "title": "Posterior Consistency for Bayesian Relevance Vector Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1131.html": {
    "title": "Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1287.html": {
    "title": "Evaluating Instrument Validity using the Principle of Independent Mechanisms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-1318.html": {
    "title": "Comprehensive Algorithm Portfolio Evaluation using Item Response Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/20-700.html": {
    "title": "F2A2: Flexible Fully-decentralized Approximate Actor-critic for Cooperative Multi-agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0169.html": {
    "title": "Variational Inference for Deblending Crowded Starfields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0377.html": {
    "title": "Dropout Training is Distributionally Robust Optimal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0434.html": {
    "title": "Factor Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0515.html": {
    "title": "Naive regression requires weaker assumptions than factor models to adjust for multiple cause confounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0579.html": {
    "title": "Quasi-Equivalence between Width and Depth of Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0599.html": {
    "title": "Metrizing Weak Convergence with Maximum Mean Discrepancies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0607.html": {
    "title": "On the Theoretical Equivalence of Several Trade-Off Curves Assessing Statistical Proximity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0742.html": {
    "title": "Learning an Explicit Hyper-parameter Prediction Function Conditioned on Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-082.html": {
    "title": "Quantifying Network Similarity using Graph Cumulants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0950.html": {
    "title": "The Proximal ID Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-0987.html": {
    "title": "Random Feature Neural Networks Learn Black-Scholes Type PDEs Without Curse of Dimensionality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1160.html": {
    "title": "Clustering with Tangles: Algorithmic Framework and Theoretical Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1170.html": {
    "title": "Insights into Ordinal Embedding Algorithms: A Systematic Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1250.html": {
    "title": "PAC-learning for Strategic Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1274.html": {
    "title": "Divide-and-Conquer Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1289.html": {
    "title": "MMD Aggregated Two-Sample Test",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1322.html": {
    "title": "Clustering and Structural Robustness in Causal Diagrams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1373.html": {
    "title": "Variational Gibbs Inference for Statistical Model Estimation from Incomplete Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1392.html": {
    "title": "Contrasting Identifying Assumptions of Average Causal Effects: Robustness and Semiparametric Efficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1436.html": {
    "title": "CodaLab Competitions: An Open Source Platform to Organize Scientific Challenges",
    "volume": "MLOSS",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1457.html": {
    "title": "Q-Learning for MDPs with General Spaces: Convergence and Near Optimality via Quantization under Weak Continuity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/21-1476.html": {
    "title": "Model-based Causal Discovery for Zero-Inflated Count Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0006.html": {
    "title": "Variational Inverting Network for Statistical Inverse Problems of Partial Differential Equations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0131.html": {
    "title": "Multiplayer Performative Prediction: Learning in Decision-Dependent Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0153.html": {
    "title": "A Non-parametric View of FedAvg and FedProx:Beyond Stationary Points",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0184.html": {
    "title": "Buffered Asynchronous SGD for Byzantine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0189.html": {
    "title": "L0Learn: A Scalable Package for Sparse Learning using L0 Regularization",
    "volume": "MLOSS",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0218.html": {
    "title": "Non-stationary Online Learning with Memory and Non-stochastic Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0305.html": {
    "title": "Augmented Sparsifiers for Generalized Hypergraph Cuts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0339.html": {
    "title": "Minimax Risk Classifiers with 0-1 Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0347.html": {
    "title": "LibMTL: A Python Library for Deep Multi-Task Learning",
    "volume": "MLOSS",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0364.html": {
    "title": "GFlowNet Foundations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0411.html": {
    "title": "Entropic Fictitious Play for Mean Field Optimization Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0491.html": {
    "title": "An Inexact Augmented Lagrangian Algorithm for Training Leaky ReLU Neural Network with Group Sparsity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0495.html": {
    "title": "Polynomial-Time Algorithms for Counting and Sampling Markov Equivalent DAGs with Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0496.html": {
    "title": "An Empirical Investigation of the Role of Pre-training in Lifelong Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0511.html": {
    "title": "Least Squares Model Averaging for Distributed Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0512.html": {
    "title": "Random Forests for Change Point Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0583.html": {
    "title": "GANs as Gradient Flows that Converge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0606.html": {
    "title": "Adaptation Augmented Model-based Policy Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0614.html": {
    "title": "Functional L-Optimality Subsampling for Functional Generalized Linear Models with Massive Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0630.html": {
    "title": "A Unified Framework for Factorizing Distributional Value Functions for Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0642.html": {
    "title": "Multi-source Learning via Completion of Block-wise Overlapping Noisy Matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0644.html": {
    "title": "Single Timescale Actor-Critic Method to Solve the Linear Quadratic Regulator with Convergence Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0657.html": {
    "title": "Conditional Distribution Function Estimation Using Neural Networks for Censored and Uncensored Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0712.html": {
    "title": "RankSEG: A Consistent Ranking-based Framework for Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0808.html": {
    "title": "Limits of Dense Simplicial Complexes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0809.html": {
    "title": "Merlion: End-to-End Machine Learning for Time Series",
    "volume": "MLOSS",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0845.html": {
    "title": "Autoregressive Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0850.html": {
    "title": "On the Optimality of Nuclear-norm-based Matrix Completion for Problems with Smooth Non-linear Structure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0880.html": {
    "title": "Interpretable and Fair Boolean Rule Sets via Column Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0881.html": {
    "title": "Sample Complexity for Distributionally Robust Learning under chi-square divergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0902.html": {
    "title": "Statistical Comparisons of Classifiers by Generalized Stochastic Dominance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0934.html": {
    "title": "Lifted Bregman Training of Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-0968.html": {
    "title": "Strategic Knowledge Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1021.html": {
    "title": "MultiZoo and MultiBench: A Standardized Toolkit for Multimodal Deep Learning",
    "volume": "MLOSS",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1053.html": {
    "title": "Tractable and Near-Optimal Adversarial Algorithms for Robust Estimation in Contaminated Gaussian Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1075.html": {
    "title": "Neural Q-learning for solving PDEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1081.html": {
    "title": "Scalable Computation of Causal Bounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1086.html": {
    "title": "Efficient Computation of Rankings from Pairwise Comparisons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1104.html": {
    "title": "Leaky Hockey Stick Loss: The First Negatively Divergent Margin-based Loss Function for Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1144.html": {
    "title": "PaLM: Scaling Language Modeling with Pathways",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1149.html": {
    "title": "Improved Powered Stochastic Optimization Algorithms for Large-Scale Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1154.html": {
    "title": "Sparse Graph Learning from Spatiotemporal Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1160.html": {
    "title": "Alpha-divergence Variational Inference Meets Importance Weighted Auto-Encoders: Methodology and Asymptotics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1176.html": {
    "title": "Selection by Prediction with Conformal p-values",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1217.html": {
    "title": "Confidence Intervals and Hypothesis Testing for High-dimensional Quantile Regression: Convolution Smoothing and Debiasing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-125.html": {
    "title": "Graph Attention Retrospective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1311.html": {
    "title": "Importance Sparsification for Sinkhorn Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1351.html": {
    "title": "Improving multiple-try Metropolis with local balancing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1468.html": {
    "title": "Unbiased Multilevel Monte Carlo Methods for Intractable Distributions: MLMC Meets MCMC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/22-1514.html": {
    "title": "Convex Reinforcement Learning in Finite Trials",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/23-0037.html": {
    "title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/23-0039.html": {
    "title": "Adaptive False Discovery Rate Control with Privacy Guarantee",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/23-0069.html": {
    "title": "Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/23-0112.html": {
    "title": "skrl: Modular and Flexible Library for Reinforcement Learning",
    "volume": "MLOSS",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/23-0300.html": {
    "title": "Torchhd: An Open Source Python Library to Support Research on Hyperdimensional Computing and Vector Symbolic Architectures",
    "volume": "MLOSS",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/23-0367.html": {
    "title": "Scalable Real-Time Recurrent Learning Using Columnar-Constructive Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://jmlr.org/papers/v24/23-0389.html": {
    "title": "Fairlearn: Assessing and Improving Fairness of AI Systems",
    "volume": "MLOSS",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  }
}