{
  "https://aclanthology.org/P19-1001": {
    "title": "One Time of Interaction May Not Be Enough: Go Deep with an Interaction-over-Interaction Network for Response Selection in Dialogues",
    "abstract": "Currently, researchers have paid great attention to retrieval-based dialogues in open-domain. In particular, people study the problem by investigating context-response matching for multi-turn response selection based on publicly recognized benchmark data sets. State-of-the-art methods require a response to interact with each utterance in a context from the beginning, but the interaction is performed in a shallow way. In this work, we let utterance-response interaction go deep by proposing an interaction-over-interaction network (IoI). The model performs matching by stacking multiple interaction blocks in which residual information from one time of interaction initiates the interaction process again. Thus, matching information within an utterance-response pair is extracted from the interaction of the pair in an iterative fashion, and the information flows along the chain of the blocks via representations. Evaluation results on three benchmark data sets indicate that IoI can significantly outperform state-of-the-art methods in terms of various matching metrics. Through further analysis, we also unveil how the depth of interaction affects the performance of IoI",
    "volume": "main",
    "checked": true,
    "id": "b69538f0a948206d46780272e5767b4ab57f40a0",
    "citation_count": 95
  },
  "https://aclanthology.org/P19-1002": {
    "title": "Incremental Transformer with Deliberation Decoder for Document Grounded Conversations",
    "abstract": "Document Grounded Conversations is a task to generate dialogue responses when chatting about the content of a given document. Obviously, document knowledge plays a critical role in Document Grounded Conversations, while existing dialogue models do not exploit this kind of knowledge effectively enough. In this paper, we propose a novel Transformer-based architecture for multi-turn document grounded conversations. In particular, we devise an Incremental Transformer to encode multi-turn utterances along with knowledge in related documents. Motivated by the human cognitive process, we design a two-pass decoder (Deliberation Decoder) to improve context coherence and knowledge correctness. Our empirical study on a real-world Document Grounded Dataset proves that responses generated by our model significantly outperform competitive baselines on both context coherence and knowledge relevance",
    "volume": "main",
    "checked": true,
    "id": "09ddd4719d7cb60fc5ea7a4c517eb854c321ddee",
    "citation_count": 78
  },
  "https://aclanthology.org/P19-1003": {
    "title": "Improving Multi-turn Dialogue Modelling with Utterance ReWriter",
    "abstract": "Recent research has achieved impressive results in single-turn dialogue modelling. In the multi-turn setting, however, current models are still far from satisfactory. One major challenge is the frequently occurred coreference and information omission in our daily conversation, making it hard for machines to understand the real intention. In this paper, we propose rewriting the human utterance as a pre-process to help multi-turn dialgoue modelling. Each utterance is first rewritten to recover all coreferred and omitted information. The next processing steps are then performed based on the rewritten utterance. To properly train the utterance rewriter, we collect a new dataset with human annotations and introduce a Transformer-based utterance rewriting architecture using the pointer network. We show the proposed architecture achieves remarkably good performance on the utterance rewriting task. The trained utterance rewriter can be easily integrated into online chatbots and brings general improvement over different domains",
    "volume": "main",
    "checked": true,
    "id": "847287e1920aa3092b56138b2ff395de4a13d63c",
    "citation_count": 66
  },
  "https://aclanthology.org/P19-1004": {
    "title": "Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study",
    "abstract": "Neural generative models have been become increasingly popular when building conversational agents. They offer flexibility, can be easily adapted to new domains, and require minimal domain engineering. A common criticism of these systems is that they seldom understand or use the available dialog history effectively. In this paper, we take an empirical approach to understanding how these models use the available dialog history by studying the sensitivity of the models to artificially introduced unnatural changes or perturbations to their context at test time. We experiment with 10 different types of perturbations on 4 multi-turn dialog datasets and find that commonly used neural dialog architectures like recurrent and transformer-based seq2seq models are rarely sensitive to most perturbations such as missing or reordering utterances, shuffling words, etc. Also, by open-sourcing our code, we believe that it will serve as a useful diagnostic tool for evaluating dialog systems in the future",
    "volume": "main",
    "checked": true,
    "id": "f7ba04ff84025fe7b74a18cf1d1d416c3ad4f424",
    "citation_count": 89
  },
  "https://aclanthology.org/P19-1005": {
    "title": "Boosting Dialog Response Generation",
    "abstract": "Neural models have become one of the most important approaches to dialog response generation. However, they still tend to generate the most common and generic responses in the corpus all the time. To address this problem, we designed an iterative training process and ensemble method based on boosting. We combined our method with different training and decoding paradigms as the base model, including mutual-information-based decoding and reward-augmented maximum likelihood learning. Empirical results show that our approach can significantly improve the diversity and relevance of the responses generated by all base models, backed by objective measurements and human evaluation",
    "volume": "main",
    "checked": true,
    "id": "f01ab0cc6d705196ff14dcca9d4507c09a23f5f4",
    "citation_count": 22
  },
  "https://aclanthology.org/P19-1006": {
    "title": "Constructing Interpretive Spatio-Temporal Features for Multi-Turn Responses Selection",
    "abstract": "Response selection plays an important role in fully automated dialogue systems. Given the dialogue context, the goal of response selection is to identify the best-matched next utterance (i.e., response) from multiple candidates. Despite the efforts of many previous useful models, this task remains challenging due to the huge semantic gap and also the large size of candidate set. To address these issues, we propose a Spatio-Temporal Matching network (STM) for response selection. In detail, soft alignment is first used to obtain the local relevance between the context and the response. And then, we construct spatio-temporal features by aggregating attention images in time dimension and make use of 3D convolution and pooling operations to extract matching information. Evaluation on two large-scale multi-turn response selection tasks has demonstrated that our proposed model significantly outperforms the state-of-the-art model. Particularly, visualization analysis shows that the spatio-temporal features enables matching information in segment pairs and time sequences, and have good interpretability for multi-turn text matching",
    "volume": "main",
    "checked": true,
    "id": "8139f574111db4a35574eba97f50d443ed3fb6fe",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-1007": {
    "title": "Semantic Parsing with Dual Learning",
    "abstract": "Semantic parsing converts natural language queries into structured logical forms. The lack of training data is still one of the most serious problems in this area. In this work, we develop a semantic parsing framework with the dual learning algorithm, which enables a semantic parser to make full use of data (labeled and even unlabeled) through a dual-learning game. This game between a primal model (semantic parsing) and a dual model (logical form to query) forces them to regularize each other, and can achieve feedback signals from some prior-knowledge. By utilizing the prior-knowledge of logical form structures, we propose a novel reward signal at the surface and semantic levels which tends to generate complete and reasonable logical forms. Experimental results show that our approach achieves new state-of-the-art performance on ATIS dataset and gets competitive performance on OVERNIGHT dataset",
    "volume": "main",
    "checked": true,
    "id": "5301f6320300ac87b42018da171f17e6b7842486",
    "citation_count": 45
  },
  "https://aclanthology.org/P19-1008": {
    "title": "Semantic Expressive Capacity with Bounded Memory",
    "abstract": "We investigate the capacity of mechanisms for compositional semantic parsing to describe relations between sentences and semantic representations. We prove that in order to represent certain relations, mechanisms which are syntactically projective must be able to remember an unbounded number of locations in the semantic representations, where nonprojective mechanisms need not. This is the first result of this kind, and has consequences both for grammar-based and for neural systems",
    "volume": "main",
    "checked": true,
    "id": "6d7e32df854a1b20788e6a2e3253258b115d939a",
    "citation_count": 0
  },
  "https://aclanthology.org/P19-1009": {
    "title": "AMR Parsing as Sequence-to-Graph Transduction",
    "abstract": "We propose an attention-based model that treats AMR parsing as sequence-to-graph transduction. Unlike most AMR parsers that rely on pre-trained aligners, external semantic resources, or data augmentation, our proposed parser is aligner-free, and it can be effectively trained with limited amounts of labeled AMR data. Our experimental results outperform all previously reported SMATCH scores, on both AMR 2.0 (76.3% on LDC2017T10) and AMR 1.0 (70.2% on LDC2014T12)",
    "volume": "main",
    "checked": true,
    "id": "aca856044b054a9b5c21a26aeb9575a3159c832c",
    "citation_count": 115
  },
  "https://aclanthology.org/P19-1010": {
    "title": "Generating Logical Forms from Graph Representations of Text and Entities",
    "abstract": "Structured information about entities is critical for many semantic parsing tasks. We present an approach that uses a Graph Neural Network (GNN) architecture to incorporate information about relevant entities and their relations during parsing. Combined with a decoder copy mechanism, this approach provides a conceptually simple mechanism to generate logical forms with entities. We demonstrate that this approach is competitive with the state-of-the-art across several tasks without pre-training, and outperforms existing approaches when combined with BERT pre-training",
    "volume": "main",
    "checked": true,
    "id": "8b53d0d64a118ac80936750dc77c9f633fe7a1a5",
    "citation_count": 31
  },
  "https://aclanthology.org/P19-1011": {
    "title": "Learning Compressed Sentence Representations for On-Device Text Processing",
    "abstract": "Vector representations of sentences, trained on massive text corpora, are widely used as generic sentence embeddings across a variety of NLP problems. The learned representations are generally assumed to be continuous and real-valued, giving rise to a large memory footprint and slow retrieval speed, which hinders their applicability to low-resource (memory and computation) platforms, such as mobile devices. In this paper, we propose four different strategies to transform continuous and generic sentence embeddings into a binarized form, while preserving their rich semantic information. The introduced methods are evaluated across a wide range of downstream tasks, where the binarized sentence embeddings are demonstrated to degrade performance by only about 2% relative to their continuous counterparts, while reducing the storage requirement by over 98%. Moreover, with the learned binary representations, the semantic relatedness of two sentences can be evaluated by simply calculating their Hamming distance, which is more computational efficient compared with the inner product operation between continuous embeddings. Detailed analysis and case study further validate the effectiveness of proposed methods",
    "volume": "main",
    "checked": true,
    "id": "a6d4e6b10d6eaf1184744804e802f9f1867aab03",
    "citation_count": 19
  },
  "https://aclanthology.org/P19-1012": {
    "title": "The (Non-)Utility of Structural Features in BiLSTM-based Dependency Parsers",
    "abstract": "Classical non-neural dependency parsers put considerable effort on the design of feature functions. Especially, they benefit from information coming from structural features, such as features drawn from neighboring tokens in the dependency tree. In contrast, their BiLSTM-based successors achieve state-of-the-art performance without explicit information about the structural context. In this paper we aim to answer the question: How much structural context are the BiLSTM representations able to capture implicitly? We show that features drawn from partial subtrees become redundant when the BiLSTMs are used. We provide a deep insight into information flow in transition- and graph-based neural architectures to demonstrate where the implicit information comes from when the parsers make their decisions. Finally, with model ablations we demonstrate that the structural context is not only present in the models, but it significantly influences their performance",
    "volume": "main",
    "checked": true,
    "id": "a6697651503207321c682966b1db746d602d6d2a",
    "citation_count": 17
  },
  "https://aclanthology.org/P19-1013": {
    "title": "Automatic Generation of High Quality CCGbanks for Parser Domain Adaptation",
    "abstract": "We propose a new domain adaptation method for Combinatory Categorial Grammar (CCG) parsing, based on the idea of automatic generation of CCG corpora exploiting cheaper resources of dependency trees. Our solution is conceptually simple, and not relying on a specific parser architecture, making it applicable to the current best-performing parsers. We conduct extensive parsing experiments with detailed discussion; on top of existing benchmark datasets on (1) biomedical texts and (2) question sentences, we create experimental datasets of (3) speech conversation and (4) math problems. When applied to the proposed method, an off-the-shelf CCG parser shows significant performance gains, improving from 90.7% to 96.6% on speech conversation, and from 88.5% to 96.8% on math problems",
    "volume": "main",
    "checked": true,
    "id": "aef3a762a6f9c130d787dba754d6a32bbbb3bd0c",
    "citation_count": 3
  },
  "https://aclanthology.org/P19-1014": {
    "title": "A Joint Named-Entity Recognizer for Heterogeneous Tag-sets Using a Tag Hierarchy",
    "abstract": "We study a variant of domain adaptation for named-entity recognition where multiple, heterogeneously tagged training sets are available. Furthermore, the test tag-set is not identical to any individual training tag-set. Yet, the relations between all tags are provided in a tag hierarchy, covering the test tags as a combination of training tags. This setting occurs when various datasets are created using different annotation schemes. This is also the case of extending a tag-set with a new tag by annotating only the new tag in a new dataset. We propose to use the given tag hierarchy to jointly learn a neural network that shares its tagging layer among all tag-sets. We compare this model to combining independent models and to a model based on the multitasking approach. Our experiments show the benefit of the tag-hierarchy model, especially when facing non-trivial consolidation of tag-sets",
    "volume": "main",
    "checked": true,
    "id": "95b42a217b6e02808e52c707db7087fa217b530c",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1015": {
    "title": "Massively Multilingual Transfer for NER",
    "abstract": "In cross-lingual transfer, NLP models over one or more source languages are applied to a low-resource target language. While most prior work has used a single source model or a few carefully selected models, here we consider a “massive” setting with many such models. This setting raises the problem of poor transfer, particularly from distant languages. We propose two techniques for modulating the transfer, suitable for zero-shot or few-shot learning, respectively. Evaluating on named entity recognition, we show that our techniques are much more effective than strong baselines, including standard ensembling, and our unsupervised method rivals oracle selection of the single best individual model",
    "volume": "main",
    "checked": true,
    "id": "e99e2bd4812b30e104db0feddb681f32acd88758",
    "citation_count": 125
  },
  "https://aclanthology.org/P19-1016": {
    "title": "Reliability-aware Dynamic Feature Composition for Name Tagging",
    "abstract": "Word embeddings are widely used on a variety of tasks and can substantially improve the performance. However, their quality is not consistent throughout the vocabulary due to the long-tail distribution of word frequency. Without sufficient contexts, rare word embeddings are usually less reliable than those of common words. However, current models typically trust all word embeddings equally regardless of their reliability and thus may introduce noise and hurt the performance. Since names often contain rare and uncommon words, this problem is particularly critical for name tagging. In this paper, we propose a novel reliability-aware name tagging model to tackle this issue. We design a set of word frequency-based reliability signals to indicate the quality of each word embedding. Guided by the reliability signals, the model is able to dynamically select and compose features such as word embedding and character-level representation using gating mechanisms. For example, if an input word is rare, the model relies less on its word embedding and assigns higher weights to its character and contextual features. Experiments on OntoNotes 5.0 show that our model outperforms the baseline model by 2.7% absolute gain in F-score. In cross-genre experiments on five genres in OntoNotes, our model improves the performance for most genre pairs and obtains up to 5% absolute F-score gain",
    "volume": "main",
    "checked": true,
    "id": "1d81900f6e06f97c433f0a559f51961d48cfd3dc",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1017": {
    "title": "Unsupervised Pivot Translation for Distant Languages",
    "abstract": "Unsupervised neural machine translation (NMT) has attracted a lot of attention recently. While state-of-the-art methods for unsupervised translation usually perform well between similar languages (e.g., English-German translation), they perform poorly between distant languages, because unsupervised alignment does not work well for distant languages. In this work, we introduce unsupervised pivot translation for distant languages, which translates a language to a distant language through multiple hops, and the unsupervised translation on each hop is relatively easier than the original direct translation. We propose a learning to route (LTR) method to choose the translation path between the source and target languages. LTR is trained on language pairs whose best translation path is available and is applied on the unseen language pairs for path selection. Experiments on 20 languages and 294 distant language pairs demonstrate the advantages of the unsupervised pivot translation for distant languages, as well as the effectiveness of the proposed LTR for path selection. Specifically, in the best case, LTR achieves an improvement of 5.58 BLEU points over the conventional direct unsupervised method",
    "volume": "main",
    "checked": true,
    "id": "85318466c38f91f21192da24f0ff747b75a39c21",
    "citation_count": 23
  },
  "https://aclanthology.org/P19-1018": {
    "title": "Bilingual Lexicon Induction with Semi-supervision in Non-Isometric Embedding Spaces",
    "abstract": "Recent work on bilingual lexicon induction (BLI) has frequently depended either on aligned bilingual lexicons or on distribution matching, often with an assumption about the isometry of the two spaces. We propose a technique to quantitatively estimate this assumption of the isometry between two embedding spaces and empirically show that this assumption weakens as the languages in question become increasingly etymologically distant. We then propose Bilingual Lexicon Induction with Semi-Supervision (BLISS) — a semi-supervised approach that relaxes the isometric assumption while leveraging both limited aligned bilingual lexicons and a larger set of unaligned word embeddings, as well as a novel hubness filtering technique. Our proposed method obtains state of the art results on 15 of 18 language pairs on the MUSE dataset, and does particularly well when the embedding spaces don’t appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision",
    "volume": "main",
    "checked": true,
    "id": "db1dafd0c356491cbbf53338b9984de324e7239c",
    "citation_count": 71
  },
  "https://aclanthology.org/P19-1019": {
    "title": "An Effective Approach to Unsupervised Machine Translation",
    "abstract": "While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through on-the-fly back-translation. Together, we obtain large improvements over the previous state-of-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014",
    "volume": "main",
    "checked": true,
    "id": "aecddd82840323e5bd43f9c73a32fed88ee93c8c",
    "citation_count": 116
  },
  "https://aclanthology.org/P19-1020": {
    "title": "Effective Adversarial Regularization for Neural Machine Translation",
    "abstract": "A regularization technique based on adversarial perturbation, which was initially developed in the field of image processing, has been successfully applied to text classification tasks and has yielded attractive improvements. We aim to further leverage this promising methodology into more sophisticated and critical neural models in the natural language processing field, i.e., neural machine translation (NMT) models. However, it is not trivial to apply this methodology to such models. Thus, this paper investigates the effectiveness of several possible configurations of applying the adversarial perturbation and reveals that the adversarial regularization technique can significantly and consistently improve the performance of widely used NMT models, such as LSTM-based and Transformer-based models",
    "volume": "main",
    "checked": true,
    "id": "4ddb924d2018f006324d930034a031b41af8c763",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-1021": {
    "title": "Revisiting Low-Resource Neural Machine Translation: A Case Study",
    "abstract": "It has been shown that the performance of neural machine translation (NMT) drops starkly in low-resource conditions, underperforming phrase-based statistical machine translation (PBSMT) and requiring large amounts of auxiliary data to achieve competitive results. In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. We discuss some pitfalls to be aware of when training low-resource NMT systems, and recent techniques that have shown to be especially helpful in low-resource settings, resulting in a set of best practices for low-resource NMT. In our experiments on German–English with different amounts of IWSLT14 training data, we show that, without the use of any auxiliary monolingual or multilingual data, an optimized NMT system can outperform PBSMT with far less data than previously claimed. We also apply these techniques to a low-resource Korean–English dataset, surpassing previously reported results by 4 BLEU",
    "volume": "main",
    "checked": true,
    "id": "10b9eee99b2632359d4d26f991e765bff8d91dee",
    "citation_count": 166
  },
  "https://aclanthology.org/P19-1022": {
    "title": "Domain Adaptive Inference for Neural Machine Translation",
    "abstract": "We investigate adaptive ensemble weighting for Neural Machine Translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. We adapt sequentially across two Spanish-English and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation. We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information, and report strong improvements across test domains without access to the domain label",
    "volume": "main",
    "checked": true,
    "id": "243eceaaf47d0d06d0b19e9fce1d1d1ab1a46fc4",
    "citation_count": 29
  },
  "https://aclanthology.org/P19-1023": {
    "title": "Neural Relation Extraction for Knowledge Base Enrichment",
    "abstract": "We study relation extraction for knowledge base (KB) enrichment. Specifically, we aim to extract entities and their relationships from sentences in the form of triples and map the elements of the extracted triples to an existing KB in an end-to-end manner. Previous studies focus on the extraction itself and rely on Named Entity Disambiguation (NED) to map triples into the KB space. This way, NED errors may cause extraction errors that affect the overall precision and recall.To address this problem, we propose an end-to-end relation extraction model for KB enrichment based on a neural encoder-decoder model. We collect high-quality training data by distant supervision with co-reference resolution and paraphrase detection. We propose an n-gram based attention model that captures multi-word entity names in a sentence. Our model employs jointly learned word and entity embeddings to support named entity disambiguation. Finally, our model uses a modified beam search and a triple classifier to help generate high-quality triples. Our model outperforms state-of-the-art baselines by 15.51% and 8.38% in terms of F1 score on two real-world datasets",
    "volume": "main",
    "checked": true,
    "id": "065d3fc4efd776f50ae27e7dd29b18495376525f",
    "citation_count": 73
  },
  "https://aclanthology.org/P19-1024": {
    "title": "Attention Guided Graph Convolutional Networks for Relation Extraction",
    "abstract": "Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text. However, how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question. Existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results. In this work, we propose Attention Guided Graph Convolutional Networks (AGGCNs), a novel model which directly takes full dependency trees as inputs. Our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches",
    "volume": "main",
    "checked": true,
    "id": "e4363d077a890c8d5c5e66b82fe69a1bbbdd5c80",
    "citation_count": 254
  },
  "https://aclanthology.org/P19-1025": {
    "title": "Spatial Aggregation Facilitates Discovery of Spatial Topics",
    "abstract": "Spatial aggregation refers to merging of documents created at the same spatial location. We show that by spatial aggregation of a large collection of documents and applying a traditional topic discovery algorithm on the aggregated data we can efficiently discover spatially distinct topics. By looking at topic discovery through matrix factorization lenses we show that spatial aggregation allows low rank approximation of the original document-word matrix, in which spatially distinct topics are preserved and non-spatial topics are aggregated into a single topic. Our experiments on synthetic data confirm this observation. Our experiments on 4.7 million tweets collected during the Sandy Hurricane in 2012 show that spatial and temporal aggregation allows rapid discovery of relevant spatial and temporal topics during that period. Our work indicates that different forms of document aggregation might be effective in rapid discovery of various types of distinct topics from large collections of documents",
    "volume": "main",
    "checked": true,
    "id": "38590470941cdbc8a0df64b3001f010f4c931ad7",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-1026": {
    "title": "Relation Embedding with Dihedral Group in Knowledge Graph",
    "abstract": "Link prediction is critical for the application of incomplete knowledge graph (KG) in the downstream tasks. As a family of effective approaches for link predictions, embedding methods try to learn low-rank representations for both entities and relations such that the bilinear form defined therein is a well-behaved scoring function. Despite of their successful performances, existing bilinear forms overlook the modeling of relation compositions, resulting in lacks of interpretability for reasoning on KG. To fulfill this gap, we propose a new model called DihEdral, named after dihedral symmetry group. This new model learns knowledge graph embeddings that can capture relation compositions by nature. Furthermore, our approach models the relation embeddings parametrized by discrete values, thereby decrease the solution space drastically. Our experiments show that DihEdral is able to capture all desired properties such as (skew-) symmetry, inversion and (non-) Abelian composition, and outperforms existing bilinear form based approach and is comparable to or better than deep learning models such as ConvE",
    "volume": "main",
    "checked": true,
    "id": "f7b42a7d2d1b7098362b2243d6f1c5650d683985",
    "citation_count": 48
  },
  "https://aclanthology.org/P19-1027": {
    "title": "Sequence Tagging with Contextual and Non-Contextual Subword Representations: A Multilingual Evaluation",
    "abstract": "Pretrained contextual and non-contextual subword embeddings have become available in over 250 languages, allowing massively multilingual NLP. However, while there is no dearth of pretrained embeddings, the distinct lack of systematic evaluations makes it difficult for practitioners to choose between them. In this work, we conduct an extensive evaluation comparing non-contextual subword embeddings, namely FastText and BPEmb, and a contextual representation method, namely BERT, on multilingual named entity recognition and part-of-speech tagging. We find that overall, a combination of BERT, BPEmb, and character representations works best across languages and tasks. A more detailed analysis reveals different strengths and weaknesses: Multilingual BERT performs well in medium- to high-resource languages, but is outperformed by non-contextual subword embeddings in a low-resource setting",
    "volume": "main",
    "checked": true,
    "id": "060a8d5a3c1e654e76e773129ce755bb1fe863fc",
    "citation_count": 28
  },
  "https://aclanthology.org/P19-1028": {
    "title": "Augmenting Neural Networks with First-order Logic",
    "abstract": "Today, the dominant paradigm for training neural networks involves minimizing task loss on a large dataset. Using world knowledge to inform a model, and yet retain the ability to perform end-to-end training remains an open question. In this paper, we present a novel framework for introducing declarative knowledge to neural network architectures in order to guide training and prediction. Our framework systematically compiles logical statements into computation graphs that augment a neural network without extra learnable parameters or manual redesign. We evaluate our modeling strategy on three tasks: machine comprehension, natural language inference, and text chunking. Our experiments show that knowledge-augmented networks can strongly improve over baselines, especially in low-data regimes",
    "volume": "main",
    "checked": true,
    "id": "c4b401e82d65cae80c851dd3f7b5a9dabb8cbc43",
    "citation_count": 74
  },
  "https://aclanthology.org/P19-1029": {
    "title": "Self-Regulated Interactive Sequence-to-Sequence Learning",
    "abstract": "Not all types of supervision signals are created equal: Different types of feedback have different costs and effects on learning. We show how self-regulation strategies that decide when to ask for which kind of feedback from a teacher (or from oneself) can be cast as a learning-to-learn problem leading to improved cost-aware sequence-to-sequence learning. In experiments on interactive neural machine translation, we find that the self-regulator discovers an 𝜖-greedy strategy for the optimal cost-quality trade-off by mixing different feedback types including corrections, error markups, and self-supervision. Furthermore, we demonstrate its robustness under domain shift and identify it as a promising alternative to active learning",
    "volume": "main",
    "checked": true,
    "id": "31947f1241fcf0e201fdc2d15ce071440ec350d7",
    "citation_count": 4
  },
  "https://aclanthology.org/P19-1030": {
    "title": "You Only Need Attention to Traverse Trees",
    "abstract": "In recent NLP research, a topic of interest is universal sentence encoding, sentence representations that can be used in any supervised task. At the word sequence level, fully attention-based models suffer from two problems: a quadratic increase in memory consumption with respect to the sentence length and an inability to capture and use syntactic information. Recursive neural nets can extract very good syntactic information by traversing a tree structure. To this end, we propose Tree Transformer, a model that captures phrase level syntax for constituency trees as well as word-level dependencies for dependency trees by doing recursive traversal only with attention. Evaluation of this model on four tasks gets noteworthy results compared to the standard transformer and LSTM-based models as well as tree-structured LSTMs. Ablation studies to find whether positional information is inherently encoded in the trees and which type of attention is suitable for doing the recursive traversal are provided",
    "volume": "main",
    "checked": true,
    "id": "988f2bc5fccbea00a23ecea2da112982d397a3dd",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1031": {
    "title": "Cross-Domain Generalization of Neural Constituency Parsers",
    "abstract": "Neural parsers obtain state-of-the-art results on benchmark treebanks for constituency parsing—but to what degree do they generalize to other domains? We present three results about the generalization of neural parsers in a zero-shot setting: training on trees from one corpus and evaluating on out-of-domain corpora. First, neural and non-neural parsers generalize comparably to new domains. Second, incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains, but does not give a larger relative improvement for out-of-domain treebanks. Finally, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. We analyze generalization on English and Chinese corpora, and in the process obtain state-of-the-art parsing results for the Brown, Genia, and English Web treebanks",
    "volume": "main",
    "checked": true,
    "id": "a2aa642db090b3aa28a44ccbc3c51fdb0be8335b",
    "citation_count": 19
  },
  "https://aclanthology.org/P19-1032": {
    "title": "Adaptive Attention Span in Transformers",
    "abstract": "We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters",
    "volume": "main",
    "checked": true,
    "id": "f4238bd2385a52413ccbacfd9e409a650235bd13",
    "citation_count": 193
  },
  "https://aclanthology.org/P19-1033": {
    "title": "Neural News Recommendation with Long- and Short-term User Representations",
    "abstract": "Personalized news recommendation is important to help users find their interested news and improve reading experience. A key problem in news recommendation is learning accurate user representations to capture their interests. Users usually have both long-term preferences and short-term interests. However, existing news recommendation methods usually learn single representations of users, which may be insufficient. In this paper, we propose a neural news recommendation approach which can learn both long- and short-term user representations. The core of our approach is a news encoder and a user encoder. In the news encoder, we learn representations of news from their titles and topic categories, and use attention network to select important words. In the user encoder, we propose to learn long-term user representations from the embeddings of their IDs.In addition, we propose to learn short-term user representations from their recently browsed news via GRU network. Besides, we propose two methods to combine long-term and short-term user representations. The first one is using the long-term user representation to initialize the hidden state of the GRU network in short-term user representation. The second one is concatenating both long- and short-term user representations as a unified user vector. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of neural news recommendation",
    "volume": "main",
    "checked": true,
    "id": "b7de4958c47666ccc064e354e37b9056c96ea8c2",
    "citation_count": 129
  },
  "https://aclanthology.org/P19-1034": {
    "title": "Automatic Domain Adaptation Outperforms Manual Domain Adaptation for Predicting Financial Outcomes",
    "abstract": "In this paper, we automatically create sentiment dictionaries for predicting financial outcomes. We compare three approaches: (i) manual adaptation of the domain-general dictionary H4N, (ii) automatic adaptation of H4N and (iii) a combination consisting of first manual, then automatic adaptation. In our experiments, we demonstrate that the automatically adapted sentiment dictionary outperforms the previous state of the art in predicting the financial outcomes excess return and volatility. In particular, automatic adaptation performs better than manual adaptation. In our analysis, we find that annotation based on an expert’s a priori belief about a word’s meaning can be incorrect – annotation should be performed based on the word’s contexts in the target domain instead",
    "volume": "main",
    "checked": true,
    "id": "8788339588c0e0175963d2e3928bc7c865fc2fa1",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-1035": {
    "title": "Manipulating the Difficulty of C-Tests",
    "abstract": "We propose two novel manipulation strategies for increasing and decreasing the difficulty of C-tests automatically. This is a crucial step towards generating learner-adaptive exercises for self-directed language learning and preparing language assessment tests. To reach the desired difficulty level, we manipulate the size and the distribution of gaps based on absolute and relative gap difficulty predictions. We evaluate our approach in corpus-based experiments and in a user study with 60 participants. We find that both strategies are able to generate C-tests with the desired difficulty level",
    "volume": "main",
    "checked": true,
    "id": "d463e409fc60a69cf3999d4f23ba9905ce503f61",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-1036": {
    "title": "Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings",
    "abstract": "Text classification aims at mapping documents into a set of predefined categories. Supervised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy. This is particularly true when the number of target categories is in the tens or the hundreds. In this work, we explore an unsupervised approach to classify documents into categories simply described by a label. The proposed method is inspired by the way a human proceeds in this situation: It draws on textual similarity between the most relevant words in each document and a dictionary of keywords for each category reflecting its semantics and lexical field. The novelty of our method hinges on the enrichment of the category labels through a combination of human expertise and language models, both generic and domain specific. Our experiments on 5 standard corpora show that the proposed method increases F1-score over relying solely on human expertise and can also be on par with simple supervised approaches. It thus provides a practical alternative to situations where low cost text categorization is needed, as we illustrate with our application to operational risk incidents classification",
    "volume": "main",
    "checked": true,
    "id": "a1a9bbe6fddc6c04dc48468755b1d780a49b812a",
    "citation_count": 17
  },
  "https://aclanthology.org/P19-1037": {
    "title": "Neural Text Simplification of Clinical Letters with a Domain Specific Phrase Table",
    "abstract": "Clinical letters are infamously impenetrable for the lay patient. This work uses neural text simplification methods to automatically improve the understandability of clinical letters for patients. We take existing neural text simplification software and augment it with a new phrase table that links complex medical terminology to simpler vocabulary by mining SNOMED-CT. In an evaluation task using crowdsourcing, we show that the results of our new system are ranked easier to understand (average rank 1.93) than using the original system (2.34) without our phrase table. We also show improvement against baselines including the original text (2.79) and using the phrase table without the neural text simplification software (2.94). Our methods can easily be transferred outside of the clinical domain by using domain-appropriate resources to provide effective neural text simplification for any domain without the need for costly annotation",
    "volume": "main",
    "checked": true,
    "id": "1332ec3fef2edf2d0be5a621a70528925eb1ee6f",
    "citation_count": 26
  },
  "https://aclanthology.org/P19-1038": {
    "title": "What You Say and How You Say It Matters: Predicting Stock Volatility Using Verbal and Vocal Cues",
    "abstract": "Predicting financial risk is an essential task in financial market. Prior research has shown that textual information in a firm’s financial statement can be used to predict its stock’s risk level. Nowadays, firm CEOs communicate information not only verbally through press releases and financial reports, but also nonverbally through investor meetings and earnings conference calls. There are anecdotal evidences that CEO’s vocal features, such as emotions and voice tones, can reveal the firm’s performance. However, how vocal features can be used to predict risk levels, and to what extent, is still unknown. To fill the gap, we obtain earnings call audio recordings and textual transcripts for S&P 500 companies in recent years. We propose a multimodal deep regression model (MDRM) that jointly model CEO’s verbal (from text) and vocal (from audio) information in a conference call. Empirical results show that our model that jointly considers verbal and vocal features achieves significant and substantial prediction error reduction. We also discuss several interesting findings and the implications to financial markets. The processed earnings conference calls data (text and audio) are released for readers who are interested in reproducing the results or designing trading strategy",
    "volume": "main",
    "checked": true,
    "id": "39799605dab71edba1417cd4bd632679b1813b34",
    "citation_count": 37
  },
  "https://aclanthology.org/P19-1039": {
    "title": "Detecting Concealed Information in Text and Speech",
    "abstract": "Motivated by infamous cheating scandals in the media industry, the wine industry, and political campaigns, we address the problem of detecting concealed information in technical settings. In this work, we explore acoustic-prosodic and linguistic indicators of information concealment by collecting a unique corpus of professionals practicing for oral exams while concealing information. We reveal subtle signs of concealing information in speech and text, compare and contrast them with those in deception detection literature, uncovering the link between concealing information and deception. We then present a series of experiments that automatically detect concealed information from text and speech. We compare the use of acoustic-prosodic, linguistic, and individual feature sets, using different machine learning models. Finally, we present a multi-task learning framework with acoustic, linguistic, and individual features, that outperforms human performance by over 15%",
    "volume": "main",
    "checked": true,
    "id": "225d1782d400406eadbba7af06e2d456599e76ff",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1040": {
    "title": "Evidence-based Trustworthiness",
    "abstract": "The information revolution brought with it information pollution. Information retrieval and extraction help us cope with abundant information from diverse sources. But some sources are of anonymous authorship, and some are of uncertain accuracy, so how can we determine what we should actually believe? Not all information sources are equally trustworthy, and simply accepting the majority view is often wrong. This paper develops a general framework for estimating the trustworthiness of information sources in an environment where multiple sources provide claims and supporting evidence, and each claim can potentially be produced by multiple sources. We consider two settings: one in which information sources directly assert claims, and a more realistic and challenging one, in which claims are inferred from evidence provided by sources, via (possibly noisy) NLP techniques. Our key contribution is to develop a family of probabilistic models that jointly estimate the trustworthiness of sources, and the credibility of claims they assert. This is done while accounting for the (possibly noisy) NLP needed to infer claims from evidence supplied by sources. We evaluate our framework on several datasets, showing strong results and significant improvement over baselines",
    "volume": "main",
    "checked": true,
    "id": "0e1b8f5f84f72c060a797d4ba59016a5e263df90",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-1041": {
    "title": "Disentangled Representation Learning for Non-Parallel Text Style Transfer",
    "abstract": "This paper tackles the problem of disentangling the latent representations of style and content in language models. We propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives, for style prediction and bag-of-words prediction, respectively. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning can be applied to style transfer on non-parallel corpora. We achieve high performance in terms of transfer accuracy, content preservation, and language fluency, in comparison to various previous approaches",
    "volume": "main",
    "checked": true,
    "id": "ea249d6793de488352858f11169bd718914731e1",
    "citation_count": 186
  },
  "https://aclanthology.org/P19-1042": {
    "title": "Cross-Sentence Grammatical Error Correction",
    "abstract": "Automatic grammatical error correction (GEC) research has made remarkable progress in the past decade. However, all existing approaches to GEC correct errors by considering a single sentence alone and ignoring crucial cross-sentence context. Some errors can only be corrected reliably using cross-sentence context and models can also benefit from the additional contextual information in correcting other errors. In this paper, we address this serious limitation of existing approaches and improve strong neural encoder-decoder models by appropriately modeling wider contexts. We employ an auxiliary encoder that encodes previous sentences and incorporate the encoding in the decoder via attention and gating mechanisms. Our approach results in statistically significant improvements in overall GEC performance over strong baselines across multiple test sets. Analysis of our cross-sentence GEC model on a synthetic dataset shows high performance in verb tense corrections that require cross-sentence context",
    "volume": "main",
    "checked": true,
    "id": "78a1a59b10570ad7b62d5f7675fd4f8b4c27dc2d",
    "citation_count": 21
  },
  "https://aclanthology.org/P19-1043": {
    "title": "This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation",
    "abstract": "Given the overwhelming number of emails, an effective subject line becomes essential to better inform the recipient of the email’s content. In this paper, we propose and study the task of email subject line generation: automatically generating an email subject line from the email body. We create the first dataset for this task and find that email subject line generation favor extremely abstractive summary which differentiates it from news headline generation or news single document summarization. We then develop a novel deep learning method and compare it to several baselines as well as recent state-of-the-art text summarization systems. We also investigate the efficacy of several automatic metrics based on correlations with human judgments and propose a new automatic evaluation metric. Our system outperforms competitive baselines given both automatic and human evaluations. To our knowledge, this is the first work to tackle the problem of effective email subject line generation",
    "volume": "main",
    "checked": true,
    "id": "03bde9c3babd77c69cad7c4f1ab819fa197bfd9f",
    "citation_count": 35
  },
  "https://aclanthology.org/P19-1044": {
    "title": "Time-Out: Temporal Referencing for Robust Modeling of Lexical Semantic Change",
    "abstract": "State-of-the-art models of lexical semantic change detection suffer from noise stemming from vector space alignment. We have empirically tested the Temporal Referencing method for lexical semantic change and show that, by avoiding alignment, it is less affected by this noise. We show that, trained on a diachronic corpus, the skip-gram with negative sampling architecture with temporal referencing outperforms alignment models on a synthetic task as well as a manual testset. We introduce a principled way to simulate lexical semantic change and systematically control for possible biases",
    "volume": "main",
    "checked": true,
    "id": "8948d2e7ef2cfd5cd9995eca4aaece7da67b31b4",
    "citation_count": 76
  },
  "https://aclanthology.org/P19-1045": {
    "title": "Adversarial Attention Modeling for Multi-dimensional Emotion Regression",
    "abstract": "In this paper, we propose a neural network-based approach, namely Adversarial Attention Network, to the task of multi-dimensional emotion regression, which automatically rates multiple emotion dimension scores for an input text. Especially, to determine which words are valuable for a particular emotion dimension, an attention layer is trained to weight the words in an input sequence. Furthermore, adversarial training is employed between two attention layers to learn better word weights via a discriminator. In particular, a shared attention layer is incorporated to learn public word weights between two emotion dimensions. Empirical evaluation on the EMOBANK corpus shows that our approach achieves notable improvements in r-values on both EMOBANK Reader’s and Writer’s multi-dimensional emotion regression tasks in all domains over the state-of-the-art baselines",
    "volume": "main",
    "checked": true,
    "id": "ef222e51a47c47e2c2c980348c87d9c45821c2ba",
    "citation_count": 25
  },
  "https://aclanthology.org/P19-1046": {
    "title": "Divide, Conquer and Combine: Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing",
    "abstract": "We propose a general strategy named ‘divide, conquer and combine’ for multimodal fusion. Instead of directly fusing features at holistic level, we conduct fusion hierarchically so that both local and global interactions are considered for a comprehensive interpretation of multimodal embeddings. In the ‘divide’ and ‘conquer’ stages, we conduct local fusion by exploring the interaction of a portion of the aligned feature vectors across various modalities lying within a sliding window, which ensures that each part of multimodal embeddings are explored sufficiently. On its basis, global fusion is conducted in the ‘combine’ stage to explore the interconnection across local interactions, via an Attentive Bi-directional Skip-connected LSTM that directly connects distant local interactions and integrates two levels of attention mechanism. In this way, local interactions can exchange information sufficiently and thus obtain an overall view of multimodal information. Our method achieves state-of-the-art performance on multimodal affective computing with higher efficiency",
    "volume": "main",
    "checked": true,
    "id": "9e43330f452a3ce3a946464214f2c3865eb7c7e4",
    "citation_count": 47
  },
  "https://aclanthology.org/P19-1047": {
    "title": "Modeling Financial Analysts' Decision Making via the Pragmatics and Semantics of Earnings Calls",
    "abstract": "Every fiscal quarter, companies hold earnings calls in which company executives respond to questions from analysts. After these calls, analysts often change their price target recommendations, which are used in equity re- search reports to help investors make deci- sions. In this paper, we examine analysts’ decision making behavior as it pertains to the language content of earnings calls. We identify a set of 20 pragmatic features of analysts’ questions which we correlate with analysts’ pre-call investor recommendations. We also analyze the degree to which semantic and pragmatic features from an earnings call complement market data in predicting analysts’ post-call changes in price targets. Our results show that earnings calls are moderately predictive of analysts’ decisions even though these decisions are influenced by a number of other factors including private communication with company executives and market conditions. A breakdown of model errors indicates disparate performance on calls from different market sectors",
    "volume": "main",
    "checked": true,
    "id": "f0c3de5686d859ad60bb872e150cf8b598f92c9a",
    "citation_count": 27
  },
  "https://aclanthology.org/P19-1048": {
    "title": "An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis",
    "abstract": "Aspect-based sentiment analysis produces a list of aspect terms and their corresponding sentiments for a natural language sentence. This task is usually done in a pipeline manner, with aspect term extraction performed first, followed by sentiment predictions toward the extracted aspect terms. While easier to develop, such an approach does not fully exploit joint information from the two subtasks and does not use all available sources of training information that might be helpful, such as document-level labeled sentiment corpus. In this paper, we propose an interactive multi-task learning network (IMN) which is able to jointly learn multiple related tasks simultaneously at both the token level as well as the document level. Unlike conventional multi-task learning methods that rely on learning common features for the different tasks, IMN introduces a message passing architecture where information is iteratively passed to different tasks through a shared set of latent variables. Experimental results demonstrate superior performance of the proposed method against multiple baselines on three benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "3f9a03b71216c202ac5a7bafba32e5e9773ed83f",
    "citation_count": 144
  },
  "https://aclanthology.org/P19-1049": {
    "title": "Decompositional Argument Mining: A General Purpose Approach for Argument Graph Construction",
    "abstract": "This work presents an approach decomposing propositions into four functional components and identify the patterns linking those components to determine argument structure. The entities addressed by a proposition are target concepts and the features selected to make a point about the target concepts are aspects. A line of reasoning is followed by providing evidence for the points made about the target concepts via aspects. Opinions on target concepts and opinions on aspects are used to support or attack the ideas expressed by target concepts and aspects. The relations between aspects, target concepts, opinions on target concepts and aspects are used to infer the argument relations. Propositions are connected iteratively to form a graph structure. The approach is generic in that it is not tuned for a specific corpus and evaluated on three different corpora from the literature: AAEC, AMT, US2016G1tv and achieved an F score of 0.79, 0.77 and 0.64, respectively",
    "volume": "main",
    "checked": true,
    "id": "0260c786e828ca3fe4d2c19012463ed32b2de68b",
    "citation_count": 12
  },
  "https://aclanthology.org/P19-1050": {
    "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
    "abstract": "Emotion recognition in conversations is a challenging task that has recently gained popularity due to its potential applications. Until now, however, a large-scale multimodal multi-party emotional conversational database containing more than two speakers per dialogue was missing. Thus, we propose the Multimodal EmotionLines Dataset (MELD), an extension and enhancement of EmotionLines. MELD contains about 13,000 utterances from 1,433 dialogues from the TV-series Friends. Each utterance is annotated with emotion and sentiment labels, and encompasses audio, visual and textual modalities. We propose several strong multimodal baselines and show the importance of contextual and multimodal information for emotion recognition in conversations. The full dataset is available for use at http://affective-meld.github.io",
    "volume": "main",
    "checked": true,
    "id": "f2d257625e8029f6f4998deb6279f97e07e2893c",
    "citation_count": 361
  },
  "https://aclanthology.org/P19-1051": {
    "title": "Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification",
    "abstract": "Open-domain targeted sentiment analysis aims to detect opinion targets along with their sentiment polarities from a sentence. Prior work typically formulates this task as a sequence tagging problem. However, such formulation suffers from problems such as huge search space and sentiment inconsistency. To address these problems, we propose a span-based extract-then-classify framework, where multiple opinion targets are directly extracted from the sentence under the supervision of target span boundaries, and corresponding polarities are then classified using their span representations. We further investigate three approaches under this framework, namely the pipeline, joint, and collapsed models. Experiments on three benchmark datasets show that our approach consistently outperforms the sequence tagging baseline. Moreover, we find that the pipeline model achieves the best performance compared with the other two models",
    "volume": "main",
    "checked": true,
    "id": "a2be5da4e0ba9a0968fb415c36a75ec6f5103817",
    "citation_count": 109
  },
  "https://aclanthology.org/P19-1052": {
    "title": "Transfer Capsule Network for Aspect Level Sentiment Classification",
    "abstract": "Aspect-level sentiment classification aims to determine the sentiment polarity of a sentence towards an aspect. Due to the high cost in annotation, the lack of aspect-level labeled data becomes a major obstacle in this area. On the other hand, document-level labeled data like reviews are easily accessible from online websites. These reviews encode sentiment knowledge in abundant contexts. In this paper, we propose a Transfer Capsule Network (TransCap) model for transferring document-level knowledge to aspect-level sentiment classification. To this end, we first develop an aspect routing approach to encapsulate the sentence-level semantic representations into semantic capsules from both the aspect-level and document-level data. We then extend the dynamic routing approach to adaptively couple the semantic capsules with the class capsules under the transfer learning framework. Experiments on SemEval datasets demonstrate the effectiveness of TransCap",
    "volume": "main",
    "checked": true,
    "id": "c1abff438508a80524b525b2e4b6bd4a5c40b101",
    "citation_count": 113
  },
  "https://aclanthology.org/P19-1053": {
    "title": "Progressive Self-Supervised Attention Learning for Aspect-Level Sentiment Analysis",
    "abstract": "In aspect-level sentiment classification (ASC), it is prevalent to equip dominant neural models with attention mechanisms, for the sake of acquiring the importance of each context word on the given aspect. However, such a mechanism tends to excessively focus on a few frequent words with sentiment polarities, while ignoring infrequent ones. In this paper, we propose a progressive self-supervised attention learning approach for neural ASC models, which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms. Specifically, we iteratively conduct sentiment predictions on all training instances. Particularly, at each iteration, the context word with the maximum attention weight is extracted as the one with active/misleading influence on the correct/incorrect prediction of every instance, and then the word itself is masked for subsequent iterations. Finally, we augment the conventional training objective with a regularization term, which enables ASC models to continue equally focusing on the extracted active context words while decreasing weights of those misleading ones. Experimental results on multiple datasets show that our proposed approach yields better attention mechanisms, leading to substantial improvements over the two state-of-the-art neural ASC models. Source code and trained models are available at https://github.com/DeepLearnXMU/PSSAttention",
    "volume": "main",
    "checked": true,
    "id": "db4da4ac33210c26bd440bfb894ea4e175c81238",
    "citation_count": 51
  },
  "https://aclanthology.org/P19-1054": {
    "title": "Classification and Clustering of Arguments with Contextualized Word Embeddings",
    "abstract": "We experiment with two recent contextualized word embedding methods (ELMo and BERT) in the context of open-domain argument search. For the first time, we show how to leverage the power of contextualized word embeddings to classify and cluster topic-dependent arguments, achieving impressive results on both tasks and across multiple datasets. For argument classification, we improve the state-of-the-art for the UKP Sentential Argument Mining Corpus by 20.8 percentage points and for the IBM Debater - Evidence Sentences dataset by 7.4 percentage points. For the understudied task of argument clustering, we propose a pre-training step which improves by 7.8 percentage points over strong baselines on a novel dataset, and by 12.3 percentage points for the Argument Facet Similarity (AFS) Corpus",
    "volume": "main",
    "checked": true,
    "id": "8c5ced1d6677eecf173ba895da11d736eb3c7a90",
    "citation_count": 102
  },
  "https://aclanthology.org/P19-1055": {
    "title": "Sentiment Tagging with Partial Labels using Modular Architectures",
    "abstract": "Many NLP learning tasks can be decomposed into several distinct sub-tasks, each associated with a partial label. In this paper we focus on a popular class of learning problems, sequence prediction applied to several sentiment analysis tasks, and suggest a modular learning approach in which different sub-tasks are learned using separate functional modules, combined to perform the final task while sharing information. Our experiments show this approach helps constrain the learning process and can alleviate some of the supervision efforts",
    "volume": "main",
    "checked": true,
    "id": "c9dd02d90e79d3d005a91bbe09e5b74a21810def",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1056": {
    "title": "DOER: Dual Cross-Shared RNN for Aspect Term-Polarity Co-Extraction",
    "abstract": "This paper focuses on two related subtasks of aspect-based sentiment analysis, namely aspect term extraction and aspect sentiment classification, which we call aspect term-polarity co-extraction. The former task is to extract aspects of a product or service from an opinion document, and the latter is to identify the polarity expressed in the document about these extracted aspects. Most existing algorithms address them as two separate tasks and solve them one by one, or only perform one task, which can be complicated for real applications. In this paper, we treat these two tasks as two sequence labeling problems and propose a novel Dual crOss-sharEd RNN framework (DOER) to generate all aspect term-polarity pairs of the input sentence simultaneously. Specifically, DOER involves a dual recurrent neural network to extract the respective representation of each task, and a cross-shared unit to consider the relationship between them. Experimental results demonstrate that the proposed framework outperforms state-of-the-art baselines on three benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "471b8f4592ac4c145a094dac5bd50fb712687acd",
    "citation_count": 67
  },
  "https://aclanthology.org/P19-1057": {
    "title": "A Corpus for Modeling User and Language Effects in Argumentation on Online Debating",
    "abstract": "Existing argumentation datasets have succeeded in allowing researchers to develop computational methods for analyzing the content, structure and linguistic features of argumentative text. They have been much less successful in fostering studies of the effect of “user” traits — characteristics and beliefs of the participants — on the debate/argument outcome as this type of user information is generally not available. This paper presents a dataset of 78,376 debates generated over a 10-year period along with surprisingly comprehensive participant profiles. We also complete an example study using the dataset to analyze the effect of selected user traits on the debate outcome in comparison to the linguistic features typically employed in studies of this kind",
    "volume": "main",
    "checked": true,
    "id": "084479defd15d7d808d7e516c4a6b450c73e2c0c",
    "citation_count": 14
  },
  "https://aclanthology.org/P19-1058": {
    "title": "Topic Tensor Network for Implicit Discourse Relation Recognition in Chinese",
    "abstract": "In the literature, most of the previous studies on English implicit discourse relation recognition only use sentence-level representations, which cannot provide enough semantic information in Chinese due to its unique paratactic characteristics. In this paper, we propose a topic tensor network to recognize Chinese implicit discourse relations with both sentence-level and topic-level representations. In particular, besides encoding arguments (discourse units) using a gated convolutional network to obtain sentence-level representations, we train a simplified topic model to infer the latent topic-level representations. Moreover, we feed the two pairs of representations to two factored tensor networks, respectively, to capture both the sentence-level interactions and topic-level relevance using multi-slice tensors. Experimentation on CDTB, a Chinese discourse corpus, shows that our proposed model significantly outperforms several state-of-the-art baselines in both micro and macro F1-scores",
    "volume": "main",
    "checked": true,
    "id": "53567c5d6531b885bcc9a2cb350c7553a940bf84",
    "citation_count": 17
  },
  "https://aclanthology.org/P19-1059": {
    "title": "Learning from Omission",
    "abstract": "Pragmatic reasoning allows humans to go beyond the literal meaning when interpret- ing language in context. Previous work has shown that such reasoning can improve the performance of already-trained language understanding systems. Here, we explore whether pragmatic reasoning during training can improve the quality of learned meanings. Our experiments on reference game data show that end-to-end pragmatic training produces more accurate utterance interpretation models, especially when data is sparse and language is complex",
    "volume": "main",
    "checked": true,
    "id": "9b8d9c83f06570e05f3f3fbef9c48c97638900fd",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1060": {
    "title": "Multi-Task Learning for Coherence Modeling",
    "abstract": "We address the task of assessing discourse coherence, an aspect of text quality that is essential for many NLP tasks, such as summarization and language assessment. We propose a hierarchical neural network trained in a multi-task fashion that learns to predict a document-level coherence score (at the network’s top layers) along with word-level grammatical roles (at the bottom layers), taking advantage of inductive transfer between the two tasks. We assess the extent to which our framework generalizes to different domains and prediction tasks, and demonstrate its effectiveness not only on standard binary evaluation coherence tasks, but also on real-world tasks involving the prediction of varying degrees of coherence, achieving a new state of the art",
    "volume": "main",
    "checked": true,
    "id": "451788cf6329f3258676118bc188c5bd69803d80",
    "citation_count": 17
  },
  "https://aclanthology.org/P19-1061": {
    "title": "Data Programming for Learning Discourse Structure",
    "abstract": "This paper investigates the advantages and limits of data programming for the task of learning discourse structure. The data programming paradigm implemented in the Snorkel framework allows a user to label training data using expert-composed heuristics, which are then transformed via the “generative step” into probability distributions of the class labels given the training candidates. These results are later generalized using a discriminative model. Snorkel’s attractive promise to create a large amount of annotated data from a smaller set of training data by unifying the output of a set of heuristics has yet to be used for computationally difficult tasks, such as that of discourse attachment, in which one must decide where a given discourse unit attaches to other units in a text in order to form a coherent discourse structure. Although approaching this problem using Snorkel requires significant modifications to the structure of the heuristics, we show that weak supervision methods can be more than competitive with classical supervised learning approaches to the attachment problem",
    "volume": "main",
    "checked": true,
    "id": "d023dfeff95bd0a50e611b18f09f863232ae8b2d",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-1062": {
    "title": "Evaluating Discourse in Structured Text Representations",
    "abstract": "Discourse structure is integral to understanding a text and is helpful in many NLP tasks. Learning latent representations of discourse is an attractive alternative to acquiring expensive labeled discourse data. Liu and Lapata (2018) propose a structured attention mechanism for text classification that derives a tree over a text, akin to an RST discourse tree. We examine this model in detail, and evaluate on additional discourse-relevant tasks and datasets, in order to assess whether the structured attention improves performance on the end task and whether it captures a text’s discourse structure. We find the learned latent trees have little to no structure and instead focus on lexical cues; even after obtaining more structured trees with proposed model modifications, the trees are still far from capturing discourse structure when compared to discourse dependency trees from an existing discourse parser. Finally, ablation studies show the structured attention provides little benefit, sometimes even hurting performance",
    "volume": "main",
    "checked": true,
    "id": "715c7f45f999acf7cfeac13894a520fab3cbea88",
    "citation_count": 25
  },
  "https://aclanthology.org/P19-1063": {
    "title": "Know What You Don't Know: Modeling a Pragmatic Speaker that Refers to Objects of Unknown Categories",
    "abstract": "Zero-shot learning in Language & Vision is the task of correctly labelling (or naming) objects of novel categories. Another strand of work in L&V aims at pragmatically informative rather than “correct” object descriptions, e.g. in reference games. We combine these lines of research and model zero-shot reference games, where a speaker needs to successfully refer to a novel object in an image. Inspired by models of “rational speech acts”, we extend a neural generator to become a pragmatic speaker reasoning about uncertain object categories. As a result of this reasoning, the generator produces fewer nouns and names of distractor categories as compared to a literal speaker. We show that this conversational strategy for dealing with novel objects often improves communicative success, in terms of resolution accuracy of an automatic listener",
    "volume": "main",
    "checked": true,
    "id": "d4e26768300d94850418de10484067707fec34ec",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-1064": {
    "title": "End-to-end Deep Reinforcement Learning Based Coreference Resolution",
    "abstract": "Recent neural network models have significantly advanced the task of coreference resolution. However, current neural coreference models are usually trained with heuristic loss functions that are computed over a sequence of local decisions. In this paper, we introduce an end-to-end reinforcement learning based coreference resolution model to directly optimize coreference evaluation metrics. Specifically, we modify the state-of-the-art higher-order mention ranking approach in Lee et al. (2018) to a reinforced policy gradient model by incorporating the reward associated with a sequence of coreference linking actions. Furthermore, we introduce maximum entropy regularization for adequate exploration to prevent the model from prematurely converging to a bad local optimum. Our proposed model achieves new state-of-the-art performance on the English OntoNotes v5.0 benchmark",
    "volume": "main",
    "checked": true,
    "id": "ca19fb1cc469a3e79b5216840b536a2eb94c0991",
    "citation_count": 33
  },
  "https://aclanthology.org/P19-1065": {
    "title": "Implicit Discourse Relation Identification for Open-domain Dialogues",
    "abstract": "Discourse relation identification has been an active area of research for many years, and the challenge of identifying implicit relations remains largely an unsolved task, especially in the context of an open-domain dialogue system. Previous work primarily relies on a corpora of formal text which is inherently non-dialogic, i.e., news and journals. This data however is not suitable to handle the nuances of informal dialogue nor is it capable of navigating the plethora of valid topics present in open-domain dialogue. In this paper, we designed a novel discourse relation identification pipeline specifically tuned for open-domain dialogue systems. We firstly propose a method to automatically extract the implicit discourse relation argument pairs and labels from a dataset of dialogic turns, resulting in a novel corpus of discourse relation pairs; the first of its kind to attempt to identify the discourse relations connecting the dialogic turns in open-domain discourse. Moreover, we have taken the first steps to leverage the dialogue features unique to our task to further improve the identification of such relations by performing feature ablation and incorporating dialogue features to enhance the state-of-the-art model",
    "volume": "main",
    "checked": true,
    "id": "b4338116d0e0af2698bec809f2c6ffbeb53e8abe",
    "citation_count": 12
  },
  "https://aclanthology.org/P19-1066": {
    "title": "Coreference Resolution with Entity Equalization",
    "abstract": "A key challenge in coreference resolution is to capture properties of entity clusters, and use those in the resolution process. Here we provide a simple and effective approach for achieving this, via an “Entity Equalization” mechanism. The Equalization approach represents each mention in a cluster via an approximation of the sum of all mentions in the cluster. We show how this can be done in a fully differentiable end-to-end manner, thus enabling high-order inferences in the resolution process. Our approach, which also employs BERT embeddings, results in new state-of-the-art results on the CoNLL-2012 coreference resolution task, improving average F1 by 3.6%",
    "volume": "main",
    "checked": true,
    "id": "9b4a06a38eba7c42fa8dcabf88e3cf38d4b57917",
    "citation_count": 82
  },
  "https://aclanthology.org/P19-1067": {
    "title": "A Cross-Domain Transferable Neural Coherence Model",
    "abstract": "Coherence is an important aspect of text quality and is crucial for ensuring its readability. One important limitation of existing coherence models is that training on one domain does not easily generalize to unseen categories of text. Previous work advocates for generative models for cross-domain generalization, because for discriminative models, the space of incoherent sentence orderings to discriminate against during training is prohibitively large. In this work, we propose a local discriminative neural model with a much smaller negative sampling space that can efficiently learn against incorrect orderings. The proposed coherence model is simple in structure, yet it significantly outperforms previous state-of-art methods on a standard benchmark dataset on the Wall Street Journal corpus, as well as in multiple new challenging settings of transfer to unseen categories of discourse on Wikipedia articles",
    "volume": "main",
    "checked": true,
    "id": "66a3ec99a9eef1d14c54b3e39b83a94a523a428a",
    "citation_count": 31
  },
  "https://aclanthology.org/P19-1068": {
    "title": "MOROCO: The Moldavian and Romanian Dialectal Corpus",
    "abstract": "In this work, we introduce the MOldavian and ROmanian Dialectal COrpus (MOROCO), which is freely available for download at https://github.com/butnaruandrei/MOROCO. The corpus contains 33564 samples of text (with over 10 million tokens) collected from the news domain. The samples belong to one of the following six topics: culture, finance, politics, science, sports and tech. The data set is divided into 21719 samples for training, 5921 samples for validation and another 5924 samples for testing. For each sample, we provide corresponding dialectal and category labels. This allows us to perform empirical studies on several classification tasks such as (i) binary discrimination of Moldavian versus Romanian text samples, (ii) intra-dialect multi-class categorization by topic and (iii) cross-dialect multi-class categorization by topic. We perform experiments using a shallow approach based on string kernels, as well as a novel deep approach based on character-level convolutional neural networks containing Squeeze-and-Excitation blocks. We also present and analyze the most discriminative features of our best performing model, before and after named entity removal",
    "volume": "main",
    "checked": true,
    "id": "ccee7c64e561780baaf45731b860aee619e7503c",
    "citation_count": 36
  },
  "https://aclanthology.org/P19-1069": {
    "title": "Just \"OneSeC\" for Producing Multilingual Sense-Annotated Data",
    "abstract": "The well-known problem of knowledge acquisition is one of the biggest issues in Word Sense Disambiguation (WSD), where annotated data are still scarce in English and almost absent in other languages. In this paper we formulate the assumption of One Sense per Wikipedia Category and present OneSeC, a language-independent method for the automatic extraction of hundreds of thousands of sentences in which a target word is tagged with its meaning. Our automatically-generated data consistently lead a supervised WSD model to state-of-the-art performance when compared with other automatic and semi-automatic methods. Moreover, our approach outperforms its competitors on multilingual and domain-specific settings, where it beats the existing state of the art on all languages and most domains. All the training data are available for research purposes at http://trainomatic.org/onesec",
    "volume": "main",
    "checked": true,
    "id": "278bcfde2417db1be5c565797cb3195086246890",
    "citation_count": 31
  },
  "https://aclanthology.org/P19-1070": {
    "title": "How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions",
    "abstract": "Cross-lingual word embeddings (CLEs) facilitate cross-lingual transfer of NLP models. Despite their ubiquitous downstream usage, increasingly popular projection-based CLE models are almost exclusively evaluated on bilingual lexicon induction (BLI). Even the BLI evaluations vary greatly, hindering our ability to correctly interpret performance and properties of different CLE models. In this work, we take the first step towards a comprehensive evaluation of CLE models: we thoroughly evaluate both supervised and unsupervised CLE models, for a large number of language pairs, on BLI and three downstream tasks, providing new insights concerning the ability of cutting-edge CLE models to support cross-lingual NLP. We empirically demonstrate that the performance of CLE models largely depends on the task at hand and that optimizing CLE models for BLI may hurt downstream performance. We indicate the most robust supervised and unsupervised CLE models and emphasize the need to reassess simple baselines, which still display competitive performance across the board. We hope our work catalyzes further research on CLE evaluation and model analysis",
    "volume": "main",
    "checked": true,
    "id": "e6bbc8d0e0c01c55be90bdc61c65bcb83119f0f9",
    "citation_count": 143
  },
  "https://aclanthology.org/P19-1071": {
    "title": "SP-10K: A Large-scale Evaluation Set for Selectional Preference Acquisition",
    "abstract": "Selectional Preference (SP) is a commonly observed language phenomenon and proved to be useful in many natural language processing tasks. To provide a better evaluation method for SP models, we introduce SP-10K, a large-scale evaluation set that provides human ratings for the plausibility of 10,000 SP pairs over five SP relations, covering 2,500 most frequent verbs, nouns, and adjectives in American English. Three representative SP acquisition methods based on pseudo-disambiguation are evaluated with SP-10K. To demonstrate the importance of our dataset, we investigate the relationship between SP-10K and the commonsense knowledge in ConceptNet5 and show the potential of using SP to represent the commonsense knowledge. We also use the Winograd Schema Challenge to prove that the proposed new SP relations are essential for the hard pronoun coreference resolution problem",
    "volume": "main",
    "checked": true,
    "id": "b14af42c8a3a3a2674780db395aa14410c7e319a",
    "citation_count": 17
  },
  "https://aclanthology.org/P19-1072": {
    "title": "A Wind of Change: Detecting and Evaluating Lexical Semantic Change across Times and Domains",
    "abstract": "We perform an interdisciplinary large-scale evaluation for detecting lexical semantic divergences in a diachronic and in a synchronic task: semantic sense changes across time, and semantic sense changes across domains. Our work addresses the superficialness and lack of comparison in assessing models of diachronic lexical change, by bringing together and extending benchmark models on a common state-of-the-art evaluation task. In addition, we demonstrate that the same evaluation task and modelling approaches can successfully be utilised for the synchronic detection of domain-specific sense divergences in the field of term extraction",
    "volume": "main",
    "checked": true,
    "id": "fb42361ee105c8fbd1beee5d0d2ec24603945ba3",
    "citation_count": 74
  },
  "https://aclanthology.org/P19-1073": {
    "title": "Errudite: Scalable, Reproducible, and Testable Error Analysis",
    "abstract": "Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs",
    "volume": "main",
    "checked": true,
    "id": "61c3c1d251c8d00a76c7513e06b24692d0b74284",
    "citation_count": 81
  },
  "https://aclanthology.org/P19-1074": {
    "title": "DocRED: A Large-Scale Document-Level Relation Extraction Dataset",
    "abstract": "Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED",
    "volume": "main",
    "checked": true,
    "id": "2745fc72e1dd53d1c30f17cf05841b163c2f63c9",
    "citation_count": 210
  },
  "https://aclanthology.org/P19-1075": {
    "title": "ChID: A Large-scale Chinese IDiom Dataset for Cloze Test",
    "abstract": "Cloze-style reading comprehension in Chinese is still limited due to the lack of various corpora. In this paper we propose a large-scale Chinese cloze test dataset ChID, which studies the comprehension of idiom, a unique language phenomenon in Chinese. In this corpus, the idioms in a passage are replaced by blank symbols and the correct answer needs to be chosen from well-designed candidate idioms. We carefully study how the design of candidate idioms and the representation of idioms affect the performance of state-of-the-art models. Results show that the machine accuracy is substantially worse than that of human, indicating a large space for further research",
    "volume": "main",
    "checked": true,
    "id": "e278e072774f23675266881750e20bca74804cb9",
    "citation_count": 45
  },
  "https://aclanthology.org/P19-1076": {
    "title": "Automatic Evaluation of Local Topic Quality",
    "abstract": "Topic models are typically evaluated with respect to the global topic distributions that they generate, using metrics such as coherence, but without regard to local (token-level) topic assignments. Token-level assignments are important for downstream tasks such as classification. Even recent models, which aim to improve the quality of these token-level topic assignments, have been evaluated only with respect to global metrics. We propose a task designed to elicit human judgments of token-level topic assignments. We use a variety of topic model types and parameters and discover that global metrics agree poorly with human assignments. Since human evaluation is expensive we propose a variety of automated metrics to evaluate topic models at a local level. Finally, we correlate our proposed metrics with human judgments from the task on several datasets. We show that an evaluation based on the percent of topic switches correlates most strongly with human judgment of local topic quality. We suggest that this new metric, which we call consistency, be adopted alongside global metrics such as topic coherence when evaluating new topic models",
    "volume": "main",
    "checked": true,
    "id": "030d9e7dd8cb73f13c47dfa30f473de21727a66a",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-1077": {
    "title": "Crowdsourcing and Aggregating Nested Markable Annotations",
    "abstract": "One of the key steps in language resource creation is the identification of the text segments to be annotated, or markables, which depending on the task may vary from nominal chunks for named entity resolution to (potentially nested) noun phrases in coreference resolution (or mentions) to larger text segments in text segmentation. Markable identification is typically carried out semi-automatically, by running a markable identifier and correcting its output by hand–which is increasingly done via annotators recruited through crowdsourcing and aggregating their responses. In this paper, we present a method for identifying markables for coreference annotation that combines high-performance automatic markable detectors with checking with a Game-With-A-Purpose (GWAP) and aggregation using a Bayesian annotation model. The method was evaluated both on news data and data from a variety of other genres and results in an improvement on F1 of mention boundaries of over seven percentage points when compared with a state-of-the-art, domain-independent automatic mention detector, and almost three points over an in-domain mention detector. One of the key contributions of our proposal is its applicability to the case in which markables are nested, as is the case with coreference markables; but the GWAP and several of the proposed markable detectors are task and language-independent and are thus applicable to a variety of other annotation scenarios",
    "volume": "main",
    "checked": true,
    "id": "d76d4aa1b35f6f1132de78fa2be9396130e6acab",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1078": {
    "title": "Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems",
    "abstract": "Over-dependence on domain ontology and lack of sharing knowledge across domains are two practical and yet less studied problems of dialogue state tracking. Existing approaches generally fall short when tracking unknown slot values during inference and often have difficulties in adapting to new domains. In this paper, we propose a Transferable Dialogue State Generator (TRADE) that generates dialogue states from utterances using copy mechanism, facilitating transfer when predicting (domain, slot, value) triplets not encountered during training. Our model is composed of an utterance encoder, a slot gate, and a state generator, which are shared across domains. Empirical results demonstrate that TRADE achieves state-of-the-art 48.62% joint goal accuracy for the five domains of MultiWOZ, a human-human dialogue dataset. In addition, we show the transferring ability by simulating zero-shot and few-shot dialogue state tracking for unseen domains. TRADE achieves 60.58% joint goal accuracy in one of the zero-shot domains, and is able to adapt to few-shot cases without forgetting already trained domains",
    "volume": "main",
    "checked": true,
    "id": "f13981668a98173bf6b49310f171a2093167a027",
    "citation_count": 296
  },
  "https://aclanthology.org/P19-1079": {
    "title": "Multi-Task Networks with Universe, Group, and Task Feature Learning",
    "abstract": "We present methods for multi-task learning that take advantage of natural groupings of related tasks. Task groups may be defined along known properties of the tasks, such as task domain or language. Such task groups represent supervised information at the inter-task level and can be encoded into the model. We investigate two variants of neural network architectures that accomplish this, learning different feature spaces at the levels of individual tasks, task groups, as well as the universe of all tasks: (1) parallel architectures encode each input simultaneously into feature spaces at different levels; (2) serial architectures encode each input successively into feature spaces at different levels in the task hierarchy. We demonstrate the methods on natural language understanding (NLU) tasks, where a grouping of tasks into different task domains leads to improved performance on ATIS, Snips, and a large in-house dataset",
    "volume": "main",
    "checked": true,
    "id": "68bf79d7233e58ec1126e6a02e8dc9dd79f70574",
    "citation_count": 16
  },
  "https://aclanthology.org/P19-1080": {
    "title": "Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue",
    "abstract": "Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Avenues like the E2E NLG Challenge have encouraged the development of neural approaches, particularly sequence-to-sequence (Seq2Seq) models for this problem. The semantic representations used, however, are often underspecified, which places a higher burden on the generation model for sentence planning, and also limits the extent to which generated responses can be controlled in a live system. In this paper, we (1) propose using tree-structured semantic representations, like those used in traditional rule-based NLG systems, for better discourse-level structuring and sentence-level planning; (2) introduce a challenging dataset using this representation for the weather domain; (3) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness; and (4) demonstrate promising results on our dataset and the E2E dataset",
    "volume": "main",
    "checked": true,
    "id": "8822e9f20502d4e15c838a44e39e0386969b03ec",
    "citation_count": 61
  },
  "https://aclanthology.org/P19-1081": {
    "title": "OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs",
    "abstract": "We study a conversational reasoning model that strategically traverses through a large-scale common fact knowledge graph (KG) to introduce engaging and contextually diverse entities and attributes. For this study, we collect a new Open-ended Dialog <-> KG parallel corpus called OpenDialKG, where each utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts. We then propose the DialKG Walker model that learns the symbolic transitions of dialog contexts as structured traversals over KG, and predicts natural entities to introduce given previous dialog contexts via a novel domain-agnostic, attention-based graph path decoder. Automatic and human evaluations show that our model can retrieve more natural and human-like responses than the state-of-the-art baselines or rule-based models, in both in-domain and cross-domain tasks. The proposed model also generates a KG walk path for each entity retrieved, providing a natural way to explain conversational reasoning",
    "volume": "main",
    "checked": true,
    "id": "0d3c68c207fc83fb402b7217811af22066300fc9",
    "citation_count": 184
  },
  "https://aclanthology.org/P19-1082": {
    "title": "Coupling Retrieval and Meta-Learning for Context-Dependent Semantic Parsing",
    "abstract": "In this paper, we present an approach to incorporate retrieved datapoints as supporting evidence for context-dependent semantic parsing, such as generating source code conditioned on the class environment. Our approach naturally combines a retrieval model and a meta-learner, where the former learns to find similar datapoints from the training data, and the latter considers retrieved datapoints as a pseudo task for fast adaptation. Specifically, our retriever is a context-aware encoder-decoder model with a latent variable which takes context environment into consideration, and our meta-learner learns to utilize retrieved datapoints in a model-agnostic meta-learning paradigm for fast adaptation. We conduct experiments on CONCODE and CSQA datasets, where the context refers to class environment in JAVA codes and conversational history, respectively. We use sequence-to-action model as the base semantic parser, which performs the state-of-the-art accuracy on both datasets. Results show that both the context-aware retriever and the meta-learning strategy improve accuracy, and our approach performs better than retrieve-and-edit baselines",
    "volume": "main",
    "checked": true,
    "id": "1f191dc4923df5fc53d5d375b05c0c32997348b9",
    "citation_count": 36
  },
  "https://aclanthology.org/P19-1083": {
    "title": "Knowledge-aware Pronoun Coreference Resolution",
    "abstract": "Resolving pronoun coreference requires knowledge support, especially for particular domains (e.g., medicine). In this paper, we explore how to leverage different types of knowledge to better resolve pronoun coreference with a neural model. To ensure the generalization ability of our model, we directly incorporate knowledge in the format of triplets, which is the most common format of modern knowledge graphs, instead of encoding it with features or rules as that in conventional approaches. Moreover, since not all knowledge is helpful in certain contexts, to selectively use them, we propose a knowledge attention module, which learns to select and use informative knowledge based on contexts, to enhance our model. Experimental results on two datasets from different domains prove the validity and effectiveness of our model, where it outperforms state-of-the-art baselines by a large margin. Moreover, since our model learns to use external knowledge rather than only fitting the training data, it also demonstrates superior performance to baselines in the cross-domain setting",
    "volume": "main",
    "checked": true,
    "id": "9f2d1fccd76d5b3a438e8e67cb717ec8b40a9e9e",
    "citation_count": 24
  },
  "https://aclanthology.org/P19-1084": {
    "title": "Don't Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference",
    "abstract": "Natural Language Inference (NLI) datasets often contain hypothesis-only biases—artifacts that allow models to achieve non-trivial performance without learning whether a premise entails a hypothesis. We propose two probabilistic methods to build models that are more robust to such biases and better transfer across datasets. In contrast to standard approaches to NLI, our methods predict the probability of a premise given a hypothesis and NLI label, discouraging models from ignoring the premise. We evaluate our methods on synthetic and existing NLI datasets by training on datasets containing biases and testing on datasets containing no (or different) hypothesis-only biases. Our results indicate that these methods can make NLI models more robust to dataset-specific artifacts, transferring better than a baseline architecture in 9 out of 12 NLI datasets. Additionally, we provide an extensive analysis of the interplay of our methods with known biases in NLI datasets, as well as the effects of encouraging models to ignore biases and fine-tuning on target datasets",
    "volume": "main",
    "checked": true,
    "id": "6540ee01a87c3b3435da73f4e3297489d525c151",
    "citation_count": 66
  },
  "https://aclanthology.org/P19-1085": {
    "title": "GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification",
    "abstract": "Fact verification (FV) is a challenging task which requires to retrieve relevant evidence from plain text and use the evidence to verify given claims. Many claims require to simultaneously integrate and reason over several pieces of evidence for verification. However, previous work employs simple models to extract information from evidence without letting evidence communicate with each other, e.g., merely concatenate the evidence for processing. Therefore, these methods are unable to grasp sufficient relational and logical information among the evidence. To alleviate this issue, we propose a graph-based evidence aggregating and reasoning (GEAR) framework which enables information to transfer on a fully-connected evidence graph and then utilizes different aggregators to collect multi-evidence information. We further employ BERT, an effective pre-trained language representation model, to improve the performance. Experimental results on a large-scale benchmark dataset FEVER have demonstrated that GEAR could leverage multi-evidence information for FV and thus achieves the promising result with a test FEVER score of 67.10%. Our code is available at https://github.com/thunlp/GEAR",
    "volume": "main",
    "checked": true,
    "id": "c3715947bbbf648dcf29a1aa4b35cfb68044f919",
    "citation_count": 120
  },
  "https://aclanthology.org/P19-1086": {
    "title": "SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for Evaluating Natural Language Inference",
    "abstract": "We present SherLIiC, a testbed for lexical inference in context (LIiC), consisting of 3985 manually annotated inference rule candidates (InfCands), accompanied by (i) ~960k unlabeled InfCands, and (ii) ~190k typed textual relations between Freebase entities extracted from the large entity-linked corpus ClueWeb09. Each InfCand consists of one of these relations, expressed as a lemmatized dependency path, and two argument placeholders, each linked to one or more Freebase types. Due to our candidate selection process based on strong distributional evidence, SherLIiC is much harder than existing testbeds because distributional evidence is of little utility in the classification of InfCands. We also show that, due to its construction, many of SherLIiC’s correct InfCands are novel and missing from existing rule bases. We evaluate a large number of strong baselines on SherLIiC, ranging from semantic vector space models to state of the art neural models of natural language inference (NLI). We show that SherLIiC poses a tough challenge to existing NLI systems",
    "volume": "main",
    "checked": true,
    "id": "832be79a6d98ca75984796647fd69ae4e5f36154",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-1087": {
    "title": "Extracting Symptoms and their Status from Clinical Conversations",
    "abstract": "This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span-attribute tagging (SA-T) model, trained using curriculum learning, and (2) a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter-rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F-score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models",
    "volume": "main",
    "checked": true,
    "id": "9929e732e938c9be0a4e431c7cb785ac03938ff6",
    "citation_count": 48
  },
  "https://aclanthology.org/P19-1088": {
    "title": "What Makes a Good Counselor? Learning to Distinguish between High-quality and Low-quality Counseling Conversations",
    "abstract": "The quality of a counseling intervention relies highly on the active collaboration between clients and counselors. In this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. Specifically, we address the differences between high-quality and low-quality counseling. Our approach examines participants’ turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. Our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. These features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%",
    "volume": "main",
    "checked": true,
    "id": "9b0c9d241269b98c80f65a14d5d65263d0688d70",
    "citation_count": 21
  },
  "https://aclanthology.org/P19-1089": {
    "title": "Finding Your Voice: The Linguistic Development of Mental Health Counselors",
    "abstract": "Mental health counseling is an enterprise with profound societal importance where conversations play a primary role. In order to acquire the conversational skills needed to face a challenging range of situations, mental health counselors must rely on training and on continued experience with actual clients. However, in the absence of large scale longitudinal studies, the nature and significance of this developmental process remain unclear. For example, prior literature suggests that experience might not translate into consequential changes in counselor behavior. This has led some to even argue that counseling is a profession without expertise. In this work, we develop a computational framework to quantify the extent to which individuals change their linguistic behavior with experience and to study the nature of this evolution. We use our framework to conduct a large longitudinal study of mental health counseling conversations, tracking over 3,400 counselors across their tenure. We reveal that overall, counselors do indeed change their conversational behavior to become more diverse across interactions, developing an individual voice that distinguishes them from other counselors. Furthermore, a finer-grained investigation shows that the rate and nature of this diversification vary across functionally different conversational components",
    "volume": "main",
    "checked": true,
    "id": "605cf5715d9507c27e95d0490f198ba87f2e31e3",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1090": {
    "title": "Towards Automating Healthcare Question Answering in a Noisy Multilingual Low-Resource Setting",
    "abstract": "We discuss ongoing work into automating a multilingual digital helpdesk service available via text messaging to pregnant and breastfeeding mothers in South Africa. Our anonymized dataset consists of short informal questions, often in low-resource languages, with unreliable language labels, spelling errors and code-mixing, as well as template answers with some inconsistencies. We explore cross-lingual word embeddings, and train parametric and non-parametric models on 90K samples for answer selection from a set of 126 templates. Preliminary results indicate that LSTMs trained end-to-end perform best, with a test accuracy of 62.13% and a recall@5 of 89.56%, and demonstrate that we can accelerate response time by several orders of magnitude",
    "volume": "main",
    "checked": true,
    "id": "e5823d286bc33ada7853669af97c4c4feac2776d",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1091": {
    "title": "Joint Entity Extraction and Assertion Detection for Clinical Text",
    "abstract": "Negative medical findings are prevalent in clinical reports, yet discriminating them from positive findings remains a challenging task for in-formation extraction. Most of the existing systems treat this task as a pipeline of two separate tasks, i.e., named entity recognition (NER)and rule-based negation detection. We consider this as a multi-task problem and present a novel end-to-end neural model to jointly extract entities and negations. We extend a standard hierarchical encoder-decoder NER model and first adopt a shared encoder followed by separate decoders for the two tasks. This architecture performs considerably better than the previous rule-based and machine learning-based systems. To overcome the problem of increased parameter size especially for low-resource settings, we propose the Conditional Softmax Shared Decoder architecture which achieves state-of-art results for NER and negation detection on the 2010 i2b2/VA challenge dataset and a proprietary de-identified clinical dataset",
    "volume": "main",
    "checked": true,
    "id": "9bad3940071c377bf1ff4946b5fc228c2f196c6a",
    "citation_count": 29
  },
  "https://aclanthology.org/P19-1092": {
    "title": "HEAD-QA: A Healthcare Dataset for Complex Reasoning",
    "abstract": "We present HEAD-QA, a multi-choice question answering testbed to encourage research on complex reasoning. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. We then consider monolingual (Spanish) and cross-lingual (to English) experiments with information retrieval and neural techniques. We show that: (i) HEAD-QA challenges current methods, and (ii) the results lag well behind human performance, demonstrating its usefulness as a benchmark for future work",
    "volume": "main",
    "checked": true,
    "id": "04234cd1cad396f76b96042227041abc9e525b0a",
    "citation_count": 23
  },
  "https://aclanthology.org/P19-1093": {
    "title": "Are You Convinced? Choosing the More Convincing Evidence with a Siamese Network",
    "abstract": "With the advancement in argument detection, we suggest to pay more attention to the challenging task of identifying the more convincing arguments. Machines capable of responding and interacting with humans in helpful ways have become ubiquitous. We now expect them to discuss with us the more delicate questions in our world, and they should do so armed with effective arguments. But what makes an argument more persuasive? What will convince you? In this paper, we present a new data set, IBM-EviConv, of pairs of evidence labeled for convincingness, designed to be more challenging than existing alternatives. We also propose a Siamese neural network architecture shown to outperform several baselines on both a prior convincingness data set and our own. Finally, we provide insights into our experimental results and the various kinds of argumentative value our method is capable of detecting",
    "volume": "main",
    "checked": true,
    "id": "8fd90cde74bc4349db1701107444c80db875f13a",
    "citation_count": 38
  },
  "https://aclanthology.org/P19-1094": {
    "title": "From Surrogacy to Adoption; From Bitcoin to Cryptocurrency: Debate Topic Expansion",
    "abstract": "When debating a controversial topic, it is often desirable to expand the boundaries of discussion. For example, we may consider the pros and cons of possible alternatives to the debate topic, make generalizations, or give specific examples. We introduce the task of Debate Topic Expansion - finding such related topics for a given debate topic, along with a novel annotated dataset for the task. We focus on relations between Wikipedia concepts, and show that they differ from well-studied lexical-semantic relations such as hypernyms, hyponyms and antonyms. We present algorithms for finding both consistent and contrastive expansions and demonstrate their effectiveness empirically. We suggest that debate topic expansion may have various use cases in argumentation mining",
    "volume": "main",
    "checked": true,
    "id": "e14ef4580eb8640493d5f983bc9c87c7bda6229a",
    "citation_count": 10
  },
  "https://aclanthology.org/P19-1095": {
    "title": "Multimodal and Multi-view Models for Emotion Recognition",
    "abstract": "Studies on emotion recognition (ER) show that combining lexical and acoustic information results in more robust and accurate models. The majority of the studies focus on settings where both modalities are available in training and evaluation. However, in practice, this is not always the case; getting ASR output may represent a bottleneck in a deployment pipeline due to computational complexity or privacy-related constraints. To address this challenge, we study the problem of efficiently combining acoustic and lexical modalities during training while still providing a deployable acoustic model that does not require lexical inputs. We first experiment with multimodal models and two attention mechanisms to assess the extent of the benefits that lexical information can provide. Then, we frame the task as a multi-view learning problem to induce semantic information from a multimodal model into our acoustic-only network using a contrastive loss function. Our multimodal model outperforms the previous state of the art on the USC-IEMOCAP dataset reported on lexical and acoustic information. Additionally, our multi-view-trained acoustic network significantly surpasses models that have been exclusively trained with acoustic features",
    "volume": "main",
    "checked": true,
    "id": "8658c2586b04918ef9f28d641e928b7e5da00b79",
    "citation_count": 21
  },
  "https://aclanthology.org/P19-1096": {
    "title": "Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts",
    "abstract": "Emotion cause extraction (ECE), the task aimed at extracting the potential causes behind certain emotions in text, has gained much attention in recent years due to its wide applications. However, it suffers from two shortcomings: 1) the emotion must be annotated before cause extraction in ECE, which greatly limits its applications in real-world scenarios; 2) the way to first annotate emotion and then extract the cause ignores the fact that they are mutually indicative. In this work, we propose a new task: emotion-cause pair extraction (ECPE), which aims to extract the potential pairs of emotions and corresponding causes in a document. We propose a 2-step approach to address this new ECPE task, which first performs individual emotion extraction and cause extraction via multi-task learning, and then conduct emotion-cause pairing and filtering. The experimental results on a benchmark emotion cause corpus prove the feasibility of the ECPE task as well as the effectiveness of our approach",
    "volume": "main",
    "checked": true,
    "id": "c75e65325995e2eff147af209f5dd0b5edf7ff38",
    "citation_count": 113
  },
  "https://aclanthology.org/P19-1097": {
    "title": "Argument Invention from First Principles",
    "abstract": "Competitive debaters often find themselves facing a challenging task – how to debate a topic they know very little about, with only minutes to prepare, and without access to books or the Internet? What they often do is rely on ”first principles”, commonplace arguments which are relevant to many topics, and which they have refined in past debates. In this work we aim to explicitly define a taxonomy of such principled recurring arguments, and, given a controversial topic, to automatically identify which of these arguments are relevant to the topic. As far as we know, this is the first time that this approach to argument invention is formalized and made explicit in the context of NLP. The main goal of this work is to show that it is possible to define such a taxonomy. While the taxonomy suggested here should be thought of as a ”first attempt” it is nonetheless coherent, covers well the relevant topics and coincides with what professional debaters actually argue in their speeches, and facilitates automatic argument invention for new topics",
    "volume": "main",
    "checked": true,
    "id": "2819c908cd3fb0a4135eccc9ff86dc952851a5c8",
    "citation_count": 14
  },
  "https://aclanthology.org/P19-1098": {
    "title": "Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization",
    "abstract": "The most important obstacles facing multi-document summarization include excessive redundancy in source descriptions and the looming shortage of training data. These obstacles prevent encoder-decoder models from being used directly, but optimization-based methods such as determinantal point processes (DPPs) are known to handle them well. In this paper we seek to strengthen a DPP-based method for extractive multi-document summarization by presenting a novel similarity measure inspired by capsule networks. The approach measures redundancy between a pair of sentences based on surface form and semantic information. We show that our DPP system with improved similarity measure performs competitively, outperforming strong summarization baselines on benchmark datasets. Our findings are particularly meaningful for summarizing documents created by multiple authors containing redundant yet lexically diverse expressions",
    "volume": "main",
    "checked": true,
    "id": "de4c7e5b7a5a4db4d28b30d3cc8df0601e18d37b",
    "citation_count": 42
  },
  "https://aclanthology.org/P19-1099": {
    "title": "Global Optimization under Length Constraint for Neural Text Summarization",
    "abstract": "We propose a global optimization method under length constraint (GOLC) for neural text summarization models. GOLC increases the probabilities of generating summaries that have high evaluation scores, ROUGE in this paper, within a desired length. We compared GOLC with two optimization methods, a maximum log-likelihood and a minimum risk training, on CNN/Daily Mail and a Japanese single document summarization data set of The Mainichi Shimbun Newspapers. The experimental results show that a state-of-the-art neural summarization model optimized with GOLC generates fewer overlength summaries while maintaining the fastest processing speed; only 6.70% overlength summaries on CNN/Daily and 7.8% on long summary of Mainichi, compared to the approximately 20% to 50% on CNN/Daily Mail and 10% to 30% on Mainichi with the other optimization methods. We also demonstrate the importance of the generation of in-length summaries for post-editing with the dataset Mainich that is created with strict length constraints. The ex- perimental results show approximately 30% to 40% improved post-editing time by use of in-length summaries",
    "volume": "main",
    "checked": true,
    "id": "a16faa446a831cf7ee9d3c8519aaf00053146b4f",
    "citation_count": 22
  },
  "https://aclanthology.org/P19-1100": {
    "title": "Searching for Effective Neural Extractive Summarization: What Works and What's Next",
    "abstract": "The recent years have seen remarkable success in the use of deep neural networks on text summarization. However, there is no clear understanding of why they perform so well, or how they might be improved. In this paper, we seek to better understand how neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Besides, we find an effective way to improve the current framework and achieve the state-of-the-art result on CNN/DailyMail by a large margin based on our observations and analysis. Hopefully, our work could provide more hints for future research on extractive summarization",
    "volume": "main",
    "checked": true,
    "id": "367c41f623f86e75d3154f6cab5b749cb7eb06b5",
    "citation_count": 104
  },
  "https://aclanthology.org/P19-1101": {
    "title": "A Simple Theoretical Model of Importance for Summarization",
    "abstract": "Research on summarization has mainly been driven by empirical approaches, crafting systems to perform well on standard datasets with the notion of information Importance remaining latent. We argue that establishing theoretical models of Importance will advance our understanding of the task and help to further improve summarization systems. To this end, we propose simple but rigorous definitions of several concepts that were previously used only intuitively in summarization: Redundancy, Relevance, and Informativeness. Importance arises as a single quantity naturally unifying these concepts. Additionally, we provide intuitions to interpret the proposed quantities and experiments to demonstrate the potential of the framework to inform and guide subsequent works",
    "volume": "main",
    "checked": true,
    "id": "14c371d359aed214b87b7a40bdbe7bfeb2968973",
    "citation_count": 55
  },
  "https://aclanthology.org/P19-1102": {
    "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    "abstract": "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting",
    "volume": "main",
    "checked": true,
    "id": "cc27ec53160d88c25fc5096c0df65536eb780de4",
    "citation_count": 255
  },
  "https://aclanthology.org/P19-1103": {
    "title": "Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency",
    "abstract": "We address the problem of adversarial attacks on text classification, which is rarely studied comparing to attacks on image classification. The challenge of this task is to generate adversarial examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a greedy algorithm called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using convolutional as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the models. At last, our method also exhibits a good transferability on the generated adversarial examples",
    "volume": "main",
    "checked": true,
    "id": "1adfa30bf112de20cb959014e44626d760aa8e4e",
    "citation_count": 334
  },
  "https://aclanthology.org/P19-1104": {
    "title": "Heuristic Authorship Obfuscation",
    "abstract": "Authorship verification is the task of determining whether two texts were written by the same author. We deal with the adversary task, called authorship obfuscation: preventing verification by altering a to-be-obfuscated text. Our new obfuscation approach (1) models writing style difference as the Jensen-Shannon distance between the character n-gram distributions of texts, and (2) manipulates an author’s subconsciously encoded writing style in a sophisticated manner using heuristic search. To obfuscate, we analyze the huge space of textual variants for a paraphrased version of the to-be-obfuscated text that has a sufficient Jensen-Shannon distance at minimal costs in terms of text quality. We analyze, quantify, and illustrate the rationale of this approach, define paraphrasing operators, derive obfuscation thresholds, and develop an effective obfuscation framework. Our authorship obfuscation approach defeats state-of-the-art verification approaches, including unmasking and compression models, while keeping text changes at a minimum",
    "volume": "main",
    "checked": true,
    "id": "7cc217b7338974ff69315a56667e9bdc051f139a",
    "citation_count": 13
  },
  "https://aclanthology.org/P19-1105": {
    "title": "Text Categorization by Learning Predominant Sense of Words as Auxiliary Task",
    "abstract": "Distributions of the senses of words are often highly skewed and give a strong influence of the domain of a document. This paper follows the assumption and presents a method for text categorization by leveraging the predominant sense of words depending on the domain, i.e., domain-specific senses. The key idea is that the features learned from predominant senses are possible to discriminate the domain of the document and thus improve the overall performance of text categorization. We propose multi-task learning framework based on the neural network model, transformer, which trains a model to simultaneously categorize documents and predicts a predominant sense for each word. The experimental results using four benchmark datasets show that our method is comparable to the state-of-the-art categorization approach, especially our model works well for categorization of multi-label documents",
    "volume": "main",
    "checked": true,
    "id": "078e5e45b4d5b8c2c0cc2d3b5d9baec15296b9bb",
    "citation_count": 12
  },
  "https://aclanthology.org/P19-1106": {
    "title": "DeepSentiPeer: Harnessing Sentiment in Review Texts to Recommend Peer Review Decisions",
    "abstract": "Automatically validating a research artefact is one of the frontiers in Artificial Intelligence (AI) that directly brings it close to competing with human intellect and intuition. Although criticised sometimes, the existing peer review system still stands as the benchmark of research validation. The present-day peer review process is not straightforward and demands profound domain knowledge, expertise, and intelligence of human reviewer(s), which is somewhat elusive with the current state of AI. However, the peer review texts, which contains rich sentiment information of the reviewer, reflecting his/her overall attitude towards the research in the paper, could be a valuable entity to predict the acceptance or rejection of the manuscript under consideration. Here in this work, we investigate the role of reviewer sentiment embedded within peer review texts to predict the peer review outcome. Our proposed deep neural architecture takes into account three channels of information: the paper, the corresponding reviews, and review’s polarity to predict the overall recommendation score as well as the final decision. We achieve significant performance improvement over the baselines (∼ 29% error reduction) proposed in a recently released dataset of peer reviews. An AI of this kind could assist the editors/program chairs as an additional layer of confidence, especially when non-responding/missing reviewers are frequent in present day peer review",
    "volume": "main",
    "checked": true,
    "id": "a1d1ab6ed9489367f50a9a49465ad969591bc7c6",
    "citation_count": 28
  },
  "https://aclanthology.org/P19-1107": {
    "title": "Gated Embeddings in End-to-End Speech Recognition for Conversational-Context Fusion",
    "abstract": "We present a novel conversational-context aware end-to-end speech recognizer based on a gated neural network that incorporates conversational-context/word/speech embeddings. Unlike conventional speech recognition models, our model learns longer conversational-context information that spans across sentences and is consequently better at recognizing long conversations. Specifically, we propose to use text-based external word and/or sentence embeddings (i.e., fastText, BERT) within an end-to-end framework, yielding significant improvement in word error rate with better conversational-context representation. We evaluated the models on the Switchboard conversational speech corpus and show that our model outperforms standard end-to-end speech recognition models",
    "volume": "main",
    "checked": true,
    "id": "efe4cae24065785c6727e815b941613b5d9ac0a0",
    "citation_count": 14
  },
  "https://aclanthology.org/P19-1108": {
    "title": "Figurative Usage Detection of Symptom Words to Improve Personal Health Mention Detection",
    "abstract": "Personal health mention detection deals with predicting whether or not a given sentence is a report of a health condition. Past work mentions errors in this prediction when symptom words, i.e., names of symptoms of interest, are used in a figurative sense. Therefore, we combine a state-of-the-art figurative usage detection with CNN-based personal health mention detection. To do so, we present two methods: a pipeline-based approach and a feature augmentation-based approach. The introduction of figurative usage detection results in an average improvement of 2.21% F-score of personal health mention detection, in the case of the feature augmentation-based approach. This paper demonstrates the promise of using figurative usage detection to improve personal health mention detection",
    "volume": "main",
    "checked": true,
    "id": "541780501b792ac875063b0bb18beecdee8bf484",
    "citation_count": 14
  },
  "https://aclanthology.org/P19-1109": {
    "title": "Complex Word Identification as a Sequence Labelling Task",
    "abstract": "Complex Word Identification (CWI) is concerned with detection of words in need of simplification and is a crucial first step in a simplification pipeline. It has been shown that reliable CWI systems considerably improve text simplification. However, most CWI systems to date address the task on a word-by-word basis, not taking the context into account. In this paper, we present a novel approach to CWI based on sequence modelling. Our system is capable of performing CWI in context, does not require extensive feature engineering and outperforms state-of-the-art systems on this task",
    "volume": "main",
    "checked": true,
    "id": "022b5364e02d21b9bc55b0bc0399cf81240508b4",
    "citation_count": 37
  },
  "https://aclanthology.org/P19-1110": {
    "title": "Neural News Recommendation with Topic-Aware News Representation",
    "abstract": "News recommendation can help users find interested news and alleviate information overload. The topic information of news is critical for learning accurate news and user representations for news recommendation. However, it is not considered in many existing news recommendation methods. In this paper, we propose a neural news recommendation approach with topic-aware news representations. The core of our approach is a topic-aware news encoder and a user encoder. In the news encoder we learn representations of news from their titles via CNN networks and apply attention networks to select important words. In addition, we propose to learn topic-aware news representations by jointly training the news encoder with an auxiliary topic classification task. In the user encoder we learn the representations of users from their browsed news and use attention networks to select informative news for user representation learning. Extensive experiments on a real-world dataset validate the effectiveness of our approach",
    "volume": "main",
    "checked": true,
    "id": "1c68c227249af06a26d95580f2f1a4f8538936bf",
    "citation_count": 45
  },
  "https://aclanthology.org/P19-1111": {
    "title": "Poetry to Prose Conversion in Sanskrit as a Linearisation Task: A Case for Low-Resource Languages",
    "abstract": "The word ordering in a Sanskrit verse is often not aligned with its corresponding prose order. Conversion of the verse to its corresponding prose helps in better comprehension of the construction. Owing to the resource constraints, we formulate this task as a word ordering (linearisation) task. In doing so, we completely ignore the word arrangement at the verse side. kāvya guru, the approach we propose, essentially consists of a pipeline of two pretraining steps followed by a seq2seq model. The first pretraining step learns task-specific token embeddings from pretrained embeddings. In the next step, we generate multiple possible hypotheses for possible word arrangements of the input %using another pretraining step. We then use them as inputs to a neural seq2seq model for the final prediction. We empirically show that the hypotheses generated by our pretraining step result in predictions that consistently outperform predictions based on the original order in the verse. Overall, kāvya guru outperforms current state of the art models in linearisation for the poetry to prose conversion task in Sanskrit",
    "volume": "main",
    "checked": true,
    "id": "753f881db881b05c81c91d0ffa6750276089c2ca",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-1112": {
    "title": "Learning Emphasis Selection for Written Text in Visual Media from Crowd-Sourced Label Distributions",
    "abstract": "In visual communication, text emphasis is used to increase the comprehension of written text to convey the author’s intent. We study the problem of emphasis selection, i.e. choosing candidates for emphasis in short written text, to enable automated design assistance in authoring. Without knowing the author’s intent and only considering the input text, multiple emphasis selections are valid. We propose a model that employs end-to-end label distribution learning (LDL) on crowd-sourced data and predicts a selection distribution, capturing the inter-subjectivity (common-sense) in the audience as well as the ambiguity of the input. We compare the model with several baselines in which the problem is transformed to single-label learning by mapping label distributions to absolute labels via majority voting",
    "volume": "main",
    "checked": true,
    "id": "d73e086991677e0001c739b4888772d962fe9553",
    "citation_count": 31
  },
  "https://aclanthology.org/P19-1113": {
    "title": "Rumor Detection by Exploiting User Credibility Information, Attention and Multi-task Learning",
    "abstract": "In this study, we propose a new multi-task learning approach for rumor detection and stance classification tasks. This neural network model has a shared layer and two task specific layers. We incorporate the user credibility information into the rumor detection layer, and we also apply attention mechanism in the rumor detection process. The attended information include not only the hidden states in the rumor detection layer, but also the hidden states from the stance detection layer. The experiments on two datasets show that our proposed model outperforms the state-of-the-art rumor detection approaches",
    "volume": "main",
    "checked": true,
    "id": "7882d5f0174342585987408d593cdb16748874a5",
    "citation_count": 81
  },
  "https://aclanthology.org/P19-1114": {
    "title": "Context-specific Language Modeling for Human Trafficking Detection from Online Advertisements",
    "abstract": "Human trafficking is a worldwide crisis. Traffickers exploit their victims by anonymously offering sexual services through online advertisements. These ads often contain clues that law enforcement can use to separate out potential trafficking cases from volunteer sex advertisements. The problem is that the sheer volume of ads is too overwhelming for manual processing. Ideally, a centralized semi-automated tool can be used to assist law enforcement agencies with this task. Here, we present an approach using natural language processing to identify trafficking ads on these websites. We propose a classifier by integrating multiple text feature sets, including the publicly available pre-trained textual language model Bi-directional Encoder Representation from transformers (BERT). In this paper, we demonstrate that a classifier using this composite feature set has significantly better performance compared to any single feature set alone",
    "volume": "main",
    "checked": true,
    "id": "7b762eed4875f192109b7f847ec109ed94dcf48e",
    "citation_count": 8
  },
  "https://aclanthology.org/P19-1115": {
    "title": "Self-Attentional Models for Lattice Inputs",
    "abstract": "Lattices are an efficient and effective method to encode ambiguity of upstream systems in natural language processing tasks, for example to compactly capture multiple speech recognition hypotheses, or to represent multiple linguistic analyses. Previous work has extended recurrent neural networks to model lattice inputs and achieved improvements in various tasks, but these models suffer from very slow computation speeds. This paper extends the recently proposed paradigm of self-attention to handle lattice inputs. Self-attention is a sequence modeling technique that relates inputs to one another by computing pairwise similarities and has gained popularity for both its strong results and its computational efficiency. To extend such models to handle lattices, we introduce probabilistic reachability masks that incorporate lattice structure into the model and support lattice scores if available. We also propose a method for adapting positional embeddings to lattice structures. We apply the proposed model to a speech translation task and find that it outperforms all examined baselines while being much faster to compute than previous neural lattice models during both training and inference",
    "volume": "main",
    "checked": true,
    "id": "309b2c75dcdafea19a053876e56cef9747d428fb",
    "citation_count": 33
  },
  "https://aclanthology.org/P19-1116": {
    "title": "When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion",
    "abstract": "Though machine translation errors caused by the lack of context beyond one sentence have long been acknowledged, the development of context-aware NMT systems is hampered by several problems. Firstly, standard metrics are not sensitive to improvements in consistency in document-level translations. Secondly, previous work on context-aware NMT assumed that the sentence-aligned parallel data consisted of complete documents while in most practical scenarios such document-level data constitutes only a fraction of the available parallel data. To address the first issue, we perform a human study on an English-Russian subtitles dataset and identify deixis, ellipsis and lexical cohesion as three main sources of inconsistency. We then create test sets targeting these phenomena. To address the second shortcoming, we consider a set-up in which a much larger amount of sentence-level data is available compared to that aligned at the document level. We introduce a model that is suitable for this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU",
    "volume": "main",
    "checked": true,
    "id": "1cf46a0f683f62e715840195fece4ac280a720cf",
    "citation_count": 122
  },
  "https://aclanthology.org/P19-1117": {
    "title": "A Compact and Language-Sensitive Multilingual Translation Method",
    "abstract": "Multilingual neural machine translation (Multi-NMT) with one encoder-decoder model has made remarkable progress due to its simple deployment. However, this multilingual translation paradigm does not make full use of language commonality and parameter sharing between encoder and decoder. Furthermore, this kind of paradigm cannot outperform the individual models trained on bilingual corpus in most cases. In this paper, we propose a compact and language-sensitive method for multilingual translation. To maximize parameter sharing, we first present a universal representor to replace both encoder and decoder models. To make the representor sensitive for specific languages, we further introduce language-sensitive embedding, attention, and discriminator with the ability to enhance model performance. We verify our methods on various translation scenarios, including one-to-many, many-to-many and zero-shot. Extensive experiments demonstrate that our proposed methods remarkably outperform strong standard multilingual translation systems on WMT and IWSLT datasets. Moreover, we find that our model is especially helpful in low-resource and zero-shot translation scenarios",
    "volume": "main",
    "checked": true,
    "id": "de413d73899369c8ae5002a5ed0dfd5f6da0402e",
    "citation_count": 29
  },
  "https://aclanthology.org/P19-1118": {
    "title": "Unsupervised Parallel Sentence Extraction with Parallel Segment Detection Helps Machine Translation",
    "abstract": "Mining parallel sentences from comparable corpora is important. Most previous work relies on supervised systems, which are trained on parallel data, thus their applicability is problematic in low-resource scenarios. Recent developments in building unsupervised bilingual word embeddings made it possible to mine parallel sentences based on cosine similarities of source and target language words. We show that relying only on this information is not enough, since sentences often have similar words but different meanings. We detect continuous parallel segments in sentence pair candidates and rely on them when mining parallel sentences. We show better mining accuracy on three language pairs in a standard shared task on artificial data. We also provide the first experiments showing that parallel sentences mined from real life sources improve unsupervised MT. Our code is available, we hope it will be used to support low-resource MT research",
    "volume": "main",
    "checked": true,
    "id": "07f4201c142567fd9c3e98ffc31b6caba4becfc9",
    "citation_count": 22
  },
  "https://aclanthology.org/P19-1119": {
    "title": "Unsupervised Bilingual Word Embedding Agreement for Unsupervised Neural Machine Translation",
    "abstract": "Unsupervised bilingual word embedding (UBWE), together with other technologies such as back-translation and denoising, has helped unsupervised neural machine translation (UNMT) achieve remarkable results in several language pairs. In previous methods, UBWE is first trained using non-parallel monolingual corpora and then this pre-trained UBWE is used to initialize the word embedding in the encoder and decoder of UNMT. That is, the training of UBWE and UNMT are separate. In this paper, we first empirically investigate the relationship between UBWE and UNMT. The empirical findings show that the performance of UNMT is significantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT",
    "volume": "main",
    "checked": true,
    "id": "58aff08ffa4db37f0a965b9f0e33c7c24cf34c16",
    "citation_count": 30
  },
  "https://aclanthology.org/P19-1120": {
    "title": "Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies",
    "abstract": "Transfer learning or multilingual model is essential for low-resource neural machine translation (NMT), but the applicability is limited to cognate languages by sharing their vocabularies. This paper shows effective techniques to transfer a pretrained NMT model to a new, unrelated language without shared vocabularies. We relieve the vocabulary mismatch by using cross-lingual word embedding, train a more language-agnostic encoder by injecting artificial noises, and generate synthetic data easily from the pretraining data without back-translation. Our methods do not require restructuring the vocabulary or retraining the model. We improve plain NMT transfer by up to +5.1% BLEU in five low-resource translation tasks, outperforming multilingual joint training by a large margin. We also provide extensive ablation studies on pretrained embedding, synthetic data, vocabulary size, and parameter freezing for a better understanding of NMT transfer",
    "volume": "main",
    "checked": true,
    "id": "7c2ce260d160c94aeeb4d711cf6757aadfde5d90",
    "citation_count": 62
  },
  "https://aclanthology.org/P19-1121": {
    "title": "Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations",
    "abstract": "Zero-shot translation, translating between language pairs on which a Neural Machine Translation (NMT) system has never been trained, is an emergent property when training the system in multilingual settings. However, naive training for zero-shot NMT easily fails, and is sensitive to hyper-parameter setting. The performance typically lags far behind the more conventional pivot-based approach which translates twice using a third language as a pivot. In this work, we address the degeneracy problem due to capturing spurious correlations by quantitatively analyzing the mutual information between language IDs of the source and decoded sentences. Inspired by this analysis, we propose to use two simple but effective approaches: (1) decoder pre-training; (2) back-translation. These methods show significant improvement (4 22 BLEU points) over the vanilla zero-shot translation on three challenging multilingual datasets, and achieve similar or better results than the pivot-based approach",
    "volume": "main",
    "checked": true,
    "id": "b3e3a2ec4e411e07a31972745e2eea537b9eb20c",
    "citation_count": 64
  },
  "https://aclanthology.org/P19-1122": {
    "title": "Syntactically Supervised Transformers for Faster Neural Machine Translation",
    "abstract": "Standard decoders for neural machine translation autoregressively generate a single target token per timestep, which slows inference especially for long outputs. While architectural advances such as the Transformer fully parallelize the decoder computations at training time, inference still proceeds sequentially. Recent developments in non- and semi-autoregressive decoding produce multiple tokens per timestep independently of the others, which improves inference speed but deteriorates translation quality. In this work, we propose the syntactically supervised Transformer (SynST), which first autoregressively predicts a chunked parse tree before generating all of the target tokens in one shot conditioned on the predicted parse. A series of controlled experiments demonstrates that SynST decodes sentences ~5x faster than the baseline autoregressive Transformer while achieving higher BLEU scores than most competing methods on En-De and En-Fr datasets",
    "volume": "main",
    "checked": true,
    "id": "d6e21619df572d04b2b2d97b4c5d1fd604f185fb",
    "citation_count": 44
  },
  "https://aclanthology.org/P19-1123": {
    "title": "Dynamically Composing Domain-Data Selection with Clean-Data Selection by \"Co-Curricular Learning\" for Neural Machine Translation",
    "abstract": "Noise and domain are important aspects of data quality for neural machine translation. Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined. This paper introduces a “co-curricular learning” method to compose dynamic domain-data selection with dynamic clean-data selection, for transfer learning across both capabilities. We apply an EM-style optimization procedure to further refine the “co-curriculum”. Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum",
    "volume": "main",
    "checked": true,
    "id": "e54e7f63d8e4deec9598ab765dc205c9de120343",
    "citation_count": 29
  },
  "https://aclanthology.org/P19-1124": {
    "title": "On the Word Alignment from Neural Machine Translation",
    "abstract": "Prior researches suggest that neural machine translation (NMT) captures word alignment through its attention mechanism, however, this paper finds attention may almost fail to capture word alignment for some NMT models. This paper thereby proposes two methods to induce word alignment which are general and agnostic to specific NMT models. Experiments show that both methods induce much better word alignment than attention. This paper further visualizes the translation through the word alignment induced by NMT. In particular, it analyzes the effect of alignment errors on translation errors at word level and its quantitative analysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics",
    "volume": "main",
    "checked": true,
    "id": "b1c7c5ab9ffce84ee3681011f09def462074f1ca",
    "citation_count": 54
  },
  "https://aclanthology.org/P19-1125": {
    "title": "Imitation Learning for Non-Autoregressive Neural Machine Translation",
    "abstract": "Non-autoregressive translation models (NAT) have achieved impressive inference speedup. A potential issue of the existing NAT algorithms, however, is that the decoding is conducted in parallel, without directly considering previous context. In this paper, we propose an imitation learning framework for non-autoregressive machine translation, which still enjoys the fast translation speed but gives comparable translation performance compared to its auto-regressive counterpart. We conduct experiments on the IWSLT16, WMT14 and WMT16 datasets. Our proposed model achieves a significant speedup over the autoregressive models, while keeping the translation quality comparable to the autoregressive models. By sampling sentence length in parallel at inference time, we achieve the performance of 31.85 BLEU on WMT16 Ro→En and 30.68 BLEU on IWSLT16 En→De",
    "volume": "main",
    "checked": true,
    "id": "df9e4fbe7f7a8d3a1d4804b2065308a24b8490ae",
    "citation_count": 41
  },
  "https://aclanthology.org/P19-1126": {
    "title": "Monotonic Infinite Lookback Attention for Simultaneous Machine Translation",
    "abstract": "Simultaneous machine translation begins to translate each source sentence before the source speaker is finished speaking, with applications to live and streaming scenarios. Simultaneous systems must carefully schedule their reading of the source sentence to balance quality against latency. We present the first simultaneous translation system to learn an adaptive schedule jointly with a neural machine translation (NMT) model that attends over all source tokens read thus far. We do so by introducing Monotonic Infinite Lookback (MILk) attention, which maintains both a hard, monotonic attention head to schedule the reading of the source sentence, and a soft attention head that extends from the monotonic head back to the beginning of the source. We show that MILk’s adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values",
    "volume": "main",
    "checked": true,
    "id": "8355c58a3e9b6254b88a7b5e684b612d71ea6019",
    "citation_count": 142
  },
  "https://aclanthology.org/P19-1127": {
    "title": "Global Textual Relation Embedding for Relational Understanding",
    "abstract": "Pre-trained embeddings such as word embeddings and sentence embeddings are fundamental tools facilitating a wide range of downstream NLP tasks. In this work, we investigate how to learn a general-purpose embedding of textual relations, defined as the shortest dependency path between entities. Textual relation embedding provides a level of knowledge between word/phrase level and sentence level, and we show that it can facilitate downstream tasks requiring relational understanding of the text. To learn such an embedding, we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase. We use global co-occurrence statistics between textual and knowledge base relations as the supervision signal to train the embedding. Evaluation on two relational understanding tasks demonstrates the usefulness of the learned textual relation embedding. The data and code can be found at https://github.com/czyssrs/GloREPlus",
    "volume": "main",
    "checked": true,
    "id": "3691d01a849c7256a9214b346b2fdeeb1c8a963e",
    "citation_count": 4
  },
  "https://aclanthology.org/P19-1128": {
    "title": "Graph Neural Networks with Generated Parameters for Relation Extraction",
    "abstract": "In this paper, we propose a novel graph neural network with generated parameters (GP-GNNs). The parameters in the propagation module, i.e. the transition matrices used in message passing procedure, are produced by a generator taking natural language sentences as inputs. We verify GP-GNNs in relation extraction from text, both on bag- and instance-settings. Experimental results on a human-annotated dataset and two distantly supervised datasets show that multi-hop reasoning mechanism yields significant improvements. We also perform a qualitative analysis to demonstrate that our model could discover more accurate relations by multi-hop relational reasoning",
    "volume": "main",
    "checked": true,
    "id": "352ac73b7d92afa915c06026a4336927d550cec3",
    "citation_count": 83
  },
  "https://aclanthology.org/P19-1129": {
    "title": "Entity-Relation Extraction as Multi-Turn Question Answering",
    "abstract": "In this paper, we propose a new paradigm for the task of entity-relation extraction. We cast the task as a multi-turn question answering problem, i.e., the extraction of entities and elations is transformed to the task of identifying answer spans from the context. This multi-turn QA formalization comes with several key advantages: firstly, the question query encodes important information for the entity/relation class we want to identify; secondly, QA provides a natural way of jointly modeling entity and relation; and thirdly, it allows us to exploit the well developed machine reading comprehension (MRC) models. Experiments on the ACE and the CoNLL04 corpora demonstrate that the proposed paradigm significantly outperforms previous best models. We are able to obtain the state-of-the-art results on all of the ACE04, ACE05 and CoNLL04 datasets, increasing the SOTA results on the three datasets to 49.6 (+1.2), 60.3 (+0.7) and 69.2 (+1.4), respectively. Additionally, we construct and will release a newly developed dataset RESUME, which requires multi-step reasoning to construct entity dependencies, as opposed to the single-step dependency extraction in the triplet exaction in previous datasets. The proposed multi-turn QA model also achieves the best performance on the RESUME dataset",
    "volume": "main",
    "checked": true,
    "id": "2c5ec74fb56fbfbceaa4cd5c8312ada4e2e19503",
    "citation_count": 205
  },
  "https://aclanthology.org/P19-1130": {
    "title": "Exploiting Entity BIO Tag Embeddings and Multi-task Learning for Relation Extraction with Imbalanced Data",
    "abstract": "In practical scenario, relation extraction needs to first identify entity pairs that have relation and then assign a correct relation class. However, the number of non-relation entity pairs in context (negative instances) usually far exceeds the others (positive instances), which negatively affects a model’s performance. To mitigate this problem, we propose a multi-task architecture which jointly trains a model to perform relation identification with cross-entropy loss and relation classification with ranking loss. Meanwhile, we observe that a sentence may have multiple entities and relation mentions, and the patterns in which the entities appear in a sentence may contain useful semantic information that can be utilized to distinguish between positive and negative instances. Thus we further incorporate the embeddings of character-wise/word-wise BIO tag from the named entity recognition task into character/word embeddings to enrich the input representation. Experiment results show that our proposed approach can significantly improve the performance of a baseline model with more than 10% absolute increase in F1-score, and outperform the state-of-the-art models on ACE 2005 Chinese and English corpus. Moreover, BIO tag embeddings are particularly effective and can be used to improve other models as well",
    "volume": "main",
    "checked": true,
    "id": "9fb38d45ae0ecb87389ef3f052a857ca397ebda0",
    "citation_count": 28
  },
  "https://aclanthology.org/P19-1131": {
    "title": "Joint Type Inference on Entities and Relations via Graph Convolutional Networks",
    "abstract": "We develop a new paradigm for the task of joint entity relation extraction. It first identifies entity spans, then performs a joint inference on entity types and relation types. To tackle the joint type inference task, we propose a novel graph convolutional network (GCN) running on an entity-relation bipartite graph. By introducing a binary relation classification task, we are able to utilize the structure of entity-relation bipartite graph in a more efficient and interpretable way. Experiments on ACE05 show that our model outperforms existing joint models in entity performance and is competitive with the state-of-the-art in relation performance",
    "volume": "main",
    "checked": true,
    "id": "7ce8ce2768907421fb1a6cbfe13a8a36992721a7",
    "citation_count": 68
  },
  "https://aclanthology.org/P19-1132": {
    "title": "Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers",
    "abstract": "Many approaches to extract multiple relations from a paragraph require multiple passes over the paragraph. In practice, multiple passes are computationally expensive and this makes difficult to scale to longer paragraphs and larger text corpora. In this work, we focus on the task of multiple relation extractions by encoding the paragraph only once. We build our solution upon the pre-trained self-attentive models (Transformer), where we first add a structured prediction layer to handle extraction between multiple entity pairs, then enhance the paragraph embedding to capture multiple relational information associated with each entity with entity-aware attention. We show that our approach is not only scalable but can also perform state-of-the-art on the standard benchmark ACE 2005",
    "volume": "main",
    "checked": true,
    "id": "85cc7ed9e6bca21ba5c414411421ae56a89df784",
    "citation_count": 78
  },
  "https://aclanthology.org/P19-1133": {
    "title": "Unsupervised Information Extraction: Regularizing Discriminative Approaches with Relation Distribution Losses",
    "abstract": "Unsupervised relation extraction aims at extracting relations between entities in text. Previous unsupervised approaches are either generative or discriminative. In a supervised setting, discriminative approaches, such as deep neural network classifiers, have demonstrated substantial improvement. However, these models are hard to train without supervision, and the currently proposed solutions are unstable. To overcome this limitation, we introduce a skewness loss which encourages the classifier to predict a relation with confidence given a sentence, and a distribution distance loss enforcing that all relations are predicted in average. These losses improve the performance of discriminative based models, and enable us to train deep neural networks satisfactorily, surpassing current state of the art on three different datasets",
    "volume": "main",
    "checked": true,
    "id": "104f60aca010f3e861af30ee9350efd0ece06b1d",
    "citation_count": 21
  },
  "https://aclanthology.org/P19-1134": {
    "title": "Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction",
    "abstract": "Distantly supervised relation extraction is widely used to extract relational facts from text, but suffers from noisy labels. Current relation extraction methods try to alleviate the noise by multi-instance learning and by providing supporting linguistic and contextual information to more efficiently guide the relation classification. While achieving state-of-the-art results, we observed these models to be biased towards recognizing a limited set of relations with high precision, while ignoring those in the long tail. To address this gap, we utilize a pre-trained language model, the OpenAI Generative Pre-trained Transformer (GPT) (Radford et al., 2018). The GPT and similar models have been shown to capture semantic and syntactic features, and also a notable amount of “common-sense” knowledge, which we hypothesize are important features for recognizing a more diverse set of relations. By extending the GPT to the distantly supervised setting, and fine-tuning it on the NYT10 dataset, we show that it predicts a larger set of distinct relation types with high confidence. Manual and automated evaluation of our model shows that it achieves a state-of-the-art AUC score of 0.422 on the NYT10 dataset, and performs especially well at higher recall levels",
    "volume": "main",
    "checked": true,
    "id": "68e686817f2c33cd09ba3805fa082348f18affd9",
    "citation_count": 82
  },
  "https://aclanthology.org/P19-1135": {
    "title": "ARNOR: Attention Regularization based Noise Reduction for Distant Supervision Relation Classification",
    "abstract": "Distant supervision is widely used in relation classification in order to create large-scale training data by aligning a knowledge base with an unlabeled corpus. However, it also introduces amounts of noisy labels where a contextual sentence actually does not express the labeled relation. In this paper, we propose ARNOR, a novel Attention Regularization based NOise Reduction framework for distant supervision relation classification. ARNOR assumes that a trustable relation label should be explained by the neural attention model. Specifically, our ARNOR framework iteratively learns an interpretable model and utilizes it to select trustable instances. We first introduce attention regularization to force the model to pay attention to the patterns which explain the relation labels, so as to make the model more interpretable. Then, if the learned model can clearly locate the relation patterns of a candidate instance in the training set, we will select it as a trustable instance for further training step. According to the experiments on NYT data, our ARNOR framework achieves significant improvements over state-of-the-art methods in both relation classification performance and noise reduction effect",
    "volume": "main",
    "checked": true,
    "id": "f160c69c428122e8fa7ba96f220b4ded5f8761f4",
    "citation_count": 36
  },
  "https://aclanthology.org/P19-1136": {
    "title": "GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction",
    "abstract": "In this paper, we present GraphRel, an end-to-end relation extraction model which uses graph convolutional networks (GCNs) to jointly learn named entities and relations. In contrast to previous baselines, we consider the interaction between named entities and relations via a 2nd-phase relation-weighted GCN to better extract relations. Linear and dependency structures are both used to extract both sequential and regional features of the text, and a complete word graph is further utilized to extract implicit features among all word pairs of the text. With the graph-based approach, the prediction for overlapping relations is substantially improved over previous sequential approaches. We evaluate GraphRel on two public datasets: NYT and WebNLG. Results show that GraphRel maintains high precision while increasing recall substantially. Also, GraphRel outperforms previous work by 3.2% and 5.8% (F1 score), achieving a new state-of-the-art for relation extraction",
    "volume": "main",
    "checked": true,
    "id": "6acacb9c81e67b50b218d173cb4022a1a5859191",
    "citation_count": 199
  },
  "https://aclanthology.org/P19-1137": {
    "title": "DIAG-NRE: A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction",
    "abstract": "Pattern-based labeling methods have achieved promising results in alleviating the inevitable labeling noises of distantly supervised neural relation extraction. However, these methods require significant expert labor to write relation-specific patterns, which makes them too sophisticated to generalize quickly. To ease the labor-intensive workload of pattern writing and enable the quick generalization to new relation types, we propose a neural pattern diagnosis framework, DIAG-NRE, that can automatically summarize and refine high-quality relational patterns from noise data with human experts in the loop. To demonstrate the effectiveness of DIAG-NRE, we apply it to two real-world datasets and present both significant and interpretable improvements over state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "96b4f3633d9544593aa6c50949e345d4016c8b48",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-1138": {
    "title": "Multi-grained Named Entity Recognition",
    "abstract": "This paper presents a novel framework, MGNER, for Multi-Grained Named Entity Recognition where multiple entities or entity mentions in a sentence could be non-overlapping or totally nested. Different from traditional approaches regarding NER as a sequential labeling task and annotate entities consecutively, MGNER detects and recognizes entities on multiple granularities: it is able to recognize named entities without explicitly assuming non-overlapping or totally nested structures. MGNER consists of a Detector that examines all possible word segments and a Classifier that categorizes entities. In addition, contextual information and a self-attention mechanism are utilized throughout the framework to improve the NER performance. Experimental results show that MGNER outperforms current state-of-the-art baselines up to 4.4% in terms of the F1 score among nested/non-overlapping NER tasks",
    "volume": "main",
    "checked": true,
    "id": "05325082c2a0f29a29886d6894f36328d328cd13",
    "citation_count": 50
  },
  "https://aclanthology.org/P19-1139": {
    "title": "ERNIE: Enhanced Language Representation with Informative Entities",
    "abstract": "Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future",
    "volume": "main",
    "checked": true,
    "id": "5f994dc8cae24ca9d1ed629e517fcc652660ddde",
    "citation_count": 765
  },
  "https://aclanthology.org/P19-1140": {
    "title": "Multi-Channel Graph Neural Network for Entity Alignment",
    "abstract": "Entity alignment typically suffers from the issues of structural heterogeneity and limited seed alignments. In this paper, we propose a novel Multi-channel Graph Neural Network model (MuGNN) to learn alignment-oriented knowledge graph (KG) embeddings by robustly encoding two KGs via multiple channels. Each channel encodes KGs via different relation weighting schemes with respect to self-attention towards KG completion and cross-KG attention for pruning exclusive entities respectively, which are further combined via pooling techniques. Moreover, we also infer and transfer rule knowledge for completing two KGs consistently. MuGNN is expected to reconcile the structural differences of two KGs, and thus make better use of seed alignments. Extensive experiments on five publicly available datasets demonstrate our superior performance (5% Hits@1 up on average). Source code and data used in the experiments can be accessed at https://github.com/thunlp/MuGNN",
    "volume": "main",
    "checked": true,
    "id": "cf0f3b886f4d4ab232a2fdf47819c2a89a5b1625",
    "citation_count": 138
  },
  "https://aclanthology.org/P19-1141": {
    "title": "A Neural Multi-digraph Model for Chinese NER with Gazetteers",
    "abstract": "Gazetteers were shown to be useful resources for named entity recognition (NER). Many existing approaches to incorporating gazetteers into machine learning based NER systems rely on manually defined selection strategies or handcrafted templates, which may not always lead to optimal effectiveness, especially when multiple gazetteers are involved. This is especially the case for the task of Chinese NER, where the words are not naturally tokenized, leading to additional ambiguities. To automatically learn how to incorporate multiple gazetteers into an NER system, we propose a novel approach based on graph neural networks with a multi-digraph structure that captures the information that the gazetteers offer. Experiments on various datasets show that our model is effective in incorporating rich gazetteer information while resolving ambiguities, outperforming previous approaches",
    "volume": "main",
    "checked": true,
    "id": "0fd26ed185aaf860f2db491c194884914fc29311",
    "citation_count": 72
  },
  "https://aclanthology.org/P19-1142": {
    "title": "Improved Language Modeling by Decoding the Past",
    "abstract": "Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling. We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method improves perplexity on the Penn Treebank dataset by up to 1.8 points and by up to 2.3 points on the WikiText-2 dataset, over strong regularized baselines using a single softmax. With a mixture-of-softmax model, we show gains of up to 1.0 perplexity points on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling",
    "volume": "main",
    "checked": true,
    "id": "f0ccb215faaeb1e9e86af5827b76c27a8d04e5a7",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1143": {
    "title": "Training Hybrid Language Models by Marginalizing over Segmentations",
    "abstract": "In this paper, we study the problem of hybrid language modeling, that is using models which can predict both characters and larger units such as character ngrams or words. Using such models, multiple potential segmentations usually exist for a given string, for example one using words and one using characters only. Thus, the probability of a string is the sum of the probabilities of all the possible segmentations. Here, we show how it is possible to marginalize over the segmentations efficiently, in order to compute the true probability of a sequence. We apply our technique on three datasets, comprising seven languages, showing improvements over a strong character level language model",
    "volume": "main",
    "checked": true,
    "id": "8a7e9f91d949764d6159c114d4feb961ec07b32d",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-1144": {
    "title": "Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",
    "abstract": "Common language models typically predict the next word given the context. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. Experiments have shown that our model outperformed several strong baseline models on different data sets. We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation",
    "volume": "main",
    "checked": true,
    "id": "9ae4aa0ccb45c564af422d2bc419f141c74eefd8",
    "citation_count": 8
  },
  "https://aclanthology.org/P19-1145": {
    "title": "Lightweight and Efficient Neural Natural Language Processing with Quaternion Networks",
    "abstract": "Many state-of-the-art neural models for NLP are heavily parameterized and thus memory inefficient. This paper proposes a series of lightweight and memory efficient neural architectures for a potpourri of natural language processing (NLP) tasks. To this end, our models exploit computation using Quaternion algebra and hypercomplex spaces, enabling not only expressive inter-component interactions but also significantly (75%) reduced parameter size due to lesser degrees of freedom in the Hamilton product. We propose Quaternion variants of models, giving rise to new architectures such as the Quaternion attention Model and Quaternion Transformer. Extensive experiments on a battery of NLP tasks demonstrates the utility of proposed Quaternion-inspired models, enabling up to 75% reduction in parameter size without significant loss in performance",
    "volume": "main",
    "checked": true,
    "id": "5240bad304d5e9dd6a7ab1e089e024119ae55567",
    "citation_count": 38
  },
  "https://aclanthology.org/P19-1146": {
    "title": "Sparse Sequence-to-Sequence Models",
    "abstract": "Sequence-to-sequence models are a powerful workhorse of NLP. Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. In this paper, we propose sparse sequence-to-sequence models, rooted in a new family of 𝛼-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any 𝛼 > 1. We provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. Experiments on morphological inflection and machine translation reveal consistent gains over dense models",
    "volume": "main",
    "checked": true,
    "id": "3cee801d10f410f0feb1a2390776a01ba2765001",
    "citation_count": 103
  },
  "https://aclanthology.org/P19-1147": {
    "title": "On the Robustness of Self-Attentive Models",
    "abstract": "This work examines the robustness of self-attentive neural networks against adversarial input perturbations. Specifically, we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis, entailment and machine translation under adversarial attacks. We also propose a novel attack algorithm for generating more natural adversarial examples that could mislead neural models but not humans. Experimental results show that, compared to recurrent neural models, self-attentive models are more robust against adversarial perturbation. In addition, we provide theoretical explanations for their superior robustness to support our claims",
    "volume": "main",
    "checked": true,
    "id": "b66a943dd745c0868d03144d60b7cd2aeb3c2ba7",
    "citation_count": 64
  },
  "https://aclanthology.org/P19-1148": {
    "title": "Exact Hard Monotonic Attention for Character-Level Transduction",
    "abstract": "Many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. With the help of dynamic programming, we are able to compute the exact marginalization over all alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer",
    "volume": "main",
    "checked": true,
    "id": "ec070add234e7ef8f7b80b91702e54a37d3be483",
    "citation_count": 51
  },
  "https://aclanthology.org/P19-1149": {
    "title": "A Lightweight Recurrent Network for Sequence Modeling",
    "abstract": "Recurrent networks have achieved great success on various sequential tasks with the assistance of complex recurrent units, but suffer from severe computational inefficiency due to weak parallelization. One direction to alleviate this issue is to shift heavy computations outside the recurrence. In this paper, we propose a lightweight recurrent network, or LRN. LRN uses input and forget gates to handle long-range dependencies as well as gradient vanishing and explosion, with all parameter related calculations factored outside the recurrence. The recurrence in LRN only manipulates the weight assigned to each token, tightly connecting LRN with self-attention networks. We apply LRN as a drop-in replacement of existing recurrent units in several neural sequential models. Extensive experiments on six NLP tasks show that LRN yields the best running efficiency with little or no loss in model performance",
    "volume": "main",
    "checked": true,
    "id": "60396a7643fd25fba52d73028bc563f5ad651bb6",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-1150": {
    "title": "Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications",
    "abstract": "Obstacles hindering the development of capsule networks for challenging NLP applications include poor scalability to large output spaces and less reliable routing processes. In this paper, we introduce: (i) an agreement score to evaluate the performance of routing processes at instance-level; (ii) an adaptive optimizer to enhance the reliability of routing; (iii) capsule compression and partial routing to improve the scalability of capsule networks. We validate our approach on two NLP tasks, namely: multi-label text classification and question answering. Experimental results show that our approach considerably improves over strong competitors on both tasks. In addition, we gain the best results in low-resource settings with few training instances",
    "volume": "main",
    "checked": true,
    "id": "8e93a072ac0324a7d012fbba6aa314d5b4b59b64",
    "citation_count": 103
  },
  "https://aclanthology.org/P19-1151": {
    "title": "Soft Representation Learning for Sparse Transfer",
    "abstract": "Transfer learning is effective for improving the performance of tasks that are related, and Multi-task learning (MTL) and Cross-lingual learning (CLL) are important instances. This paper argues that hard-parameter sharing, of hard-coding layers shared across different tasks or languages, cannot generalize well, when sharing with a loosely related task. Such case, which we call sparse transfer, might actually hurt performance, a phenomenon known as negative transfer. Our contribution is using adversarial training across tasks, to “soft-code” shared and private spaces, to avoid the shared space gets too sparse. In CLL, our proposed architecture considers another challenge of dealing with low-quality input",
    "volume": "main",
    "checked": true,
    "id": "87a29a8b35c3eaf8a659dd63f9f6a8bd78e00f9c",
    "citation_count": 3
  },
  "https://aclanthology.org/P19-1152": {
    "title": "Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization",
    "abstract": "There has been an increased interest in multimodal language processing including multimodal dialog, question answering, sentiment analysis, and speech recognition. However, naturally occurring multimodal data is often imperfect as a result of imperfect modalities, missing entries or noise corruption. To address these concerns, we present a regularization method based on tensor rank minimization. Our method is based on the observation that high-dimensional multimodal time series data often exhibit correlations across time and modalities which leads to low-rank tensor representations. However, the presence of noise or incomplete values breaks these correlations and results in tensor representations of higher rank. We design a model to learn such tensor representations and effectively regularize their rank. Experiments on multimodal language data show that our model achieves good results across various levels of imperfection",
    "volume": "main",
    "checked": true,
    "id": "93deae7cfaba34066afa05a2b05162891b13e769",
    "citation_count": 44
  },
  "https://aclanthology.org/P19-1153": {
    "title": "Towards Lossless Encoding of Sentences",
    "abstract": "A lot of work has been done in the field of image compression via machine learning, but not much attention has been given to the compression of natural language. Compressing text into lossless representations while making features easily retrievable is not a trivial task, yet has huge benefits. Most methods designed to produce feature rich sentence embeddings focus solely on performing well on downstream tasks and are unable to properly reconstruct the original sequence from the learned embedding. In this work, we propose a near lossless method for encoding long sequences of texts as well as all of their sub-sequences into feature rich representations. We test our method on sentiment analysis and show good performance across all sub-sentence and sentence embeddings",
    "volume": "main",
    "checked": true,
    "id": "d2d371224a716abdd45d3899633df9120de74ffe",
    "citation_count": 2
  },
  "https://aclanthology.org/P19-1154": {
    "title": "Open Vocabulary Learning for Neural Chinese Pinyin IME",
    "abstract": "Pinyin-to-character (P2C) conversion is the core component of pinyin-based Chinese input method engine (IME). However, the conversion is seriously compromised by the ambiguities of Chinese characters corresponding to pinyin as well as the predefined fixed vocabularies. To alleviate such inconveniences, we propose a neural P2C conversion model augmented by an online updated vocabulary with a sampling mechanism to support open vocabulary learning during IME working. Our experiments show that the proposed method outperforms commercial IMEs and state-of-the-art traditional models on standard corpus and true inputting history dataset in terms of multiple metrics and thus the online updated vocabulary indeed helps our IME effectively follows user inputting behavior",
    "volume": "main",
    "checked": true,
    "id": "dc477a26c162ddfa8ab36d4f7975f62f5ad04ab5",
    "citation_count": 18
  },
  "https://aclanthology.org/P19-1155": {
    "title": "Using LSTMs to Assess the Obligatoriness of Phonological Distinctive Features for Phonotactic Learning",
    "abstract": "To ascertain the importance of phonetic information in the form of phonological distinctive features for the purpose of segment-level phonotactic acquisition, we compare the performance of two recurrent neural network models of phonotactic learning: one that has access to distinctive features at the start of the learning process, and one that does not. Though the predictions of both models are significantly correlated with human judgments of non-words, the feature-naive model significantly outperforms the feature-aware one in terms of probability assigned to a held-out test set of English words, suggesting that distinctive features are not obligatory for learning phonotactic patterns at the segment level",
    "volume": "main",
    "checked": true,
    "id": "58710e8c0776bd677413e0a096ff39c91c66b91d",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1156": {
    "title": "Better Character Language Modeling through Morphology",
    "abstract": "We incorporate morphological supervision into character language models (CLMs) via multitasking and show that this addition improves bits-per-character (BPC) performance across 24 languages, even when the morphology data and language modeling data are disjoint. Analyzing the CLMs shows that inflected words benefit more from explicitly modeling morphology than uninflected words, and that morphological supervision improves performance even as the amount of language modeling data grows. We then transfer morphological supervision across languages to improve performance in the low-resource setting",
    "volume": "main",
    "checked": true,
    "id": "1d51b59fcf0297e8df931c8b614bd55165b24172",
    "citation_count": 8
  },
  "https://aclanthology.org/P19-1157": {
    "title": "Historical Text Normalization with Delayed Rewards",
    "abstract": "Training neural sequence-to-sequence models with simple token-level log-likelihood is now a standard approach to historical text normalization, albeit often outperformed by phrase-based models. Policy gradient training enables direct optimization for exact matches, and while the small datasets in historical text normalization are prohibitive of from-scratch reinforcement learning, we show that policy gradient fine-tuning leads to significant improvements across the board. Policy gradient training, in particular, leads to more accurate normalizations for long or unseen words",
    "volume": "main",
    "checked": true,
    "id": "26a238217321008cd1daaa649683d461e16e7574",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1158": {
    "title": "Stochastic Tokenization with a Language Model for Neural Text Classification",
    "abstract": "For unsegmented languages such as Japanese and Chinese, tokenization of a sentence has a significant impact on the performance of text classification. Sentences are usually segmented with words or subwords by a morphological analyzer or byte pair encoding and then encoded with word (or subword) representations for neural networks. However, segmentation is potentially ambiguous, and it is unclear whether the segmented tokens achieve the best performance for the target task. In this paper, we propose a method to simultaneously learn tokenization and text classification to address these problems. Our model incorporates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously. To make the model robust against infrequent tokens, we sampled segmentation for each sentence stochastically during training, which resulted in improved performance of text classification. We conducted experiments on sentiment analysis as a text classification task and show that our method achieves better performance than previous methods",
    "volume": "main",
    "checked": true,
    "id": "3504dd566b25f24f44cf647731370760a537b483",
    "citation_count": 10
  },
  "https://aclanthology.org/P19-1159": {
    "title": "Mitigating Gender Bias in Natural Language Processing: Literature Review",
    "abstract": "As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP",
    "volume": "main",
    "checked": true,
    "id": "493fac37cea49afb98c52c2f5dd75c303a325b25",
    "citation_count": 281
  },
  "https://aclanthology.org/P19-1160": {
    "title": "Gender-preserving Debiasing for Pre-trained Word Embeddings",
    "abstract": "Word embeddings learnt from massive text collections have demonstrated significant levels of discriminative biases such as gender, racial or ethnic biases, which in turn bias the down-stream NLP applications that use those word embeddings. Taking gender-bias as a working example, we propose a debiasing method that preserves non-discriminative gender-related information, while removing stereotypical discriminative gender biases from pre-trained word embeddings. Specifically, we consider four types of information: feminine, masculine, gender-neutral and stereotypical, which represent the relationship between gender vs. bias, and propose a debiasing method that (a) preserves the gender-related information in feminine and masculine words, (b) preserves the neutrality in gender-neutral words, and (c) removes the biases from stereotypical words. Experimental results on several previously proposed benchmark datasets show that our proposed method can debias pre-trained word embeddings better than existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information",
    "volume": "main",
    "checked": true,
    "id": "8ea6f7fcd75d7651b5c4f5c7cd121854f3369693",
    "citation_count": 75
  },
  "https://aclanthology.org/P19-1161": {
    "title": "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology",
    "abstract": "Gender stereotypes are manifest in most of the world’s languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality",
    "volume": "main",
    "checked": true,
    "id": "835ac3cbb41f2ec47718c5491211dd33b64f382b",
    "citation_count": 139
  },
  "https://aclanthology.org/P19-1162": {
    "title": "A Transparent Framework for Evaluating Unintended Demographic Bias in Word Embeddings",
    "abstract": "Word embedding models have gained a lot of traction in the Natural Language Processing community, however, they suffer from unintended demographic biases. Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test (WEAT). While these approaches offer great geometric insights into unintended biases in the embedding vector space, they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications. In this work, we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias. Our metric (Relative Negative Sentiment Bias, RNSB) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups. We show that our framework and metric enable useful analysis into the bias in word embeddings",
    "volume": "main",
    "checked": true,
    "id": "f007180553dd8e62b348a403ea8aafa36ec7d4ce",
    "citation_count": 34
  },
  "https://aclanthology.org/P19-1163": {
    "title": "The Risk of Racial Bias in Hate Speech Detection",
    "abstract": "We investigate how annotators’ insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet’s dialect they are significantly less likely to label the tweet as offensive",
    "volume": "main",
    "checked": true,
    "id": "8963317176fa81e185fd7a8f8cd001d7e11a4868",
    "citation_count": 454
  },
  "https://aclanthology.org/P19-1164": {
    "title": "Evaluating Gender Bias in Machine Translation",
    "abstract": "We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation (MT). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., “The doctor asked the nurse to help her in the operation”). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word “doctor”). Our analyses show that four popular industrial MT systems and two recent state-of-the-art academic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are publicly available at https://github.com/gabrielStanovsky/mt_gender",
    "volume": "main",
    "checked": true,
    "id": "1670a07b70f90cc4ddba71343e6a7ee4b5198595",
    "citation_count": 201
  },
  "https://aclanthology.org/P19-1165": {
    "title": "LSTMEmbed: Learning Word and Sense Representations from a Large Semantically Annotated Corpus with Long Short-Term Memories",
    "abstract": "While word embeddings are now a de facto standard representation of words in most NLP tasks, recently the attention has been shifting towards vector representations which capture the different meanings, i.e., senses, of words. In this paper we explore the capabilities of a bidirectional LSTM model to learn representations of word senses from semantically annotated corpora. We show that the utilization of an architecture that is aware of word order, like an LSTM, enables us to create better representations. We assess our proposed model on various standard benchmarks for evaluating semantic representations, reaching state-of-the-art performance on the SemEval-2014 word-to-sense similarity task. We release the code and the resulting word and sense embeddings at http://lcl.uniroma1.it/LSTMEmbed",
    "volume": "main",
    "checked": true,
    "id": "892d973ca542c60f8bec85922804d09ba1cb82a5",
    "citation_count": 14
  },
  "https://aclanthology.org/P19-1166": {
    "title": "Understanding Undesirable Word Embedding Associations",
    "abstract": "Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus",
    "volume": "main",
    "checked": true,
    "id": "72b642fada39be45dd7cf05b8520d58fe12b5e23",
    "citation_count": 88
  },
  "https://aclanthology.org/P19-1167": {
    "title": "Unsupervised Discovery of Gendered Language through Latent-Variable Modeling",
    "abstract": "Studying the ways in which language is gendered has long been an area of interest in sociolinguistics. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men",
    "volume": "main",
    "checked": true,
    "id": "d4eeb40b9bd06ed53a26282cd527609f71e6496f",
    "citation_count": 35
  },
  "https://aclanthology.org/P19-1168": {
    "title": "Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings",
    "abstract": "Given a small corpus D_T pertaining to a limited set of focused topics, our goal is to train embeddings that accurately capture the sense of words in the topic in spite of the limited size of D_T. These embeddings may be used in various tasks involving D_T. A popular strategy in limited data settings is to adapt pretrained embeddings E trained on a large corpus. To correct for sense drift, fine-tuning, regularization, projection, and pivoting have been proposed recently. Among these, regularization informed by a word’s corpus frequency performed well, but we improve upon it using a new regularizer based on the stability of its cooccurrence with other words. However, a thorough comparison across ten topics, spanning three tasks, with standardized settings of hyper-parameters, reveals that even the best embedding adaptation strategies provide small gains beyond well-tuned baselines, which many earlier comparisons ignored. In a bold departure from adapting pretrained embeddings, we propose using D_T to probe, attend to, and borrow fragments from any large, topic-rich source corpus (such as Wikipedia), which need not be the corpus used to pretrain embeddings. This step is made scalable and practical by suitable indexing. We reach the surprising conclusion that even limited corpus augmentation is more useful than adapting embeddings, which suggests that non-dominant sense information may be irrevocably obliterated from pretrained embeddings and cannot be salvaged by adaptation",
    "volume": "main",
    "checked": true,
    "id": "09997b1630ef2a63f617a4fec94400c5d3715c6e",
    "citation_count": 0
  },
  "https://aclanthology.org/P19-1169": {
    "title": "SphereRE: Distinguishing Lexical Relations with Hyperspherical Relation Embeddings",
    "abstract": "Lexical relations describe how meanings of terms relate to each other. Typical examples include hypernymy, synonymy, meronymy, etc. Automatic distinction of lexical relations is vital for NLP applications, and also challenging due to the lack of contextual signals to discriminate between such relations. In this work, we present a neural representation learning model to distinguish lexical relations among term pairs based on Hyperspherical Relation Embeddings (SphereRE). Rather than learning embeddings for individual terms, the model learns representations of relation triples by mapping them to the hyperspherical embedding space, where relation triples of different lexical relations are well separated. Experiments over several benchmarks confirm SphereRE outperforms state-of-the-arts",
    "volume": "main",
    "checked": true,
    "id": "8589293a1268cb44953709daafd0fa7aab8aa76a",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-1170": {
    "title": "Multilingual Factor Analysis",
    "abstract": "In this work we approach the task of learning multilingual word representations in an offline manner by fitting a generative latent variable model to a multilingual dictionary. We model equivalent words in different languages as different views of the same word generated by a common latent variable representing their latent lexical meaning. We explore the task of alignment by querying the fitted model for multilingual embeddings achieving competitive results across a variety of tasks. The proposed model is robust to noise in the embedding space making it a suitable method for distributed representations learned from noisy corpora",
    "volume": "main",
    "checked": true,
    "id": "0d28aec557460098967455aa00b007114f7c988e",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-1171": {
    "title": "Meaning to Form: Measuring Systematicity as Information",
    "abstract": "A longstanding debate in semiotics centers on the relationship between linguistic signs and their corresponding semantics: is there an arbitrary relationship between a word form and its meaning, or does some systematic phenomenon pervade? For instance, does the character bigram ‘gl’ have any systematic relationship to the meaning of words like ‘glisten’, ‘gleam’ and ‘glow’? In this work, we offer a holistic quantification of the systematicity of the sign using mutual information and recurrent neural networks. We employ these in a data-driven and massively multilingual approach to the question, examining 106 languages. We find a statistically significant reduction in entropy when modeling a word form conditioned on its semantic representation. Encouragingly, we also recover well-attested English examples of systematic affixes. We conclude with the meta-point: Our approximate effect size (measured in bits) is quite small—despite some amount of systematicity between form and meaning, an arbitrary relationship and its resulting benefits dominate human language",
    "volume": "main",
    "checked": true,
    "id": "73cf24d4c8ca809727553925a31253a649702582",
    "citation_count": 27
  },
  "https://aclanthology.org/P19-1172": {
    "title": "Learning Morphosyntactic Analyzers from the Bible via Iterative Annotation Projection across 26 Languages",
    "abstract": "A large percentage of computational tools are concentrated in a very small subset of the planet’s languages. Compounding the issue, many languages lack the high-quality linguistic annotation necessary for the construction of such tools with current machine learning methods. In this paper, we address both issues simultaneously: leveraging the high accuracy of English taggers and parsers, we project morphological information onto translations of the Bible in 26 varied test languages. Using an iterative discovery, constraint, and training process, we build inflectional lexica in the target languages. Through a combination of iteration, ensembling, and reranking, we see double-digit relative error reductions in lemmatization and morphological analysis over a strong initial system",
    "volume": "main",
    "checked": true,
    "id": "b2a891453f1e93877090b72a2d542e79baad7ebe",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1173": {
    "title": "Adversarial Multitask Learning for Joint Multi-Feature and Multi-Dialect Morphological Modeling",
    "abstract": "Morphological tagging is challenging for morphologically rich languages due to the large target space and the need for more training data to minimize model sparsity. Dialectal variants of morphologically rich languages suffer more as they tend to be more noisy and have less resources. In this paper we explore the use of multitask learning and adversarial training to address morphological richness and dialectal variations in the context of full morphological tagging. We use multitask learning for joint morphological modeling for the features within two dialects, and as a knowledge-transfer scheme for cross-dialectal modeling. We use adversarial training to learn dialect invariant features that can help the knowledge-transfer scheme from the high to low-resource variants. We work with two dialectal variants: Modern Standard Arabic (high-resource “dialect’”) and Egyptian Arabic (low-resource dialect) as a case study. Our models achieve state-of-the-art results for both. Furthermore, adversarial training provides more significant improvement when using smaller training datasets in particular",
    "volume": "main",
    "checked": true,
    "id": "59990123240261c5ea4520c4ff08f171fbdab17c",
    "citation_count": 19
  },
  "https://aclanthology.org/P19-1174": {
    "title": "Neural Machine Translation with Reordering Embeddings",
    "abstract": "The reordering model plays an important role in phrase-based statistical machine translation. However, there are few works that exploit the reordering information in neural machine translation. In this paper, we propose a reordering mechanism to learn the reordering embedding of a word based on its contextual information. These learned reordering embeddings are stacked together with self-attention networks to learn sentence representation for machine translation. The reordering mechanism can be easily integrated into both the encoder and the decoder in the Transformer translation system. Experimental results on WMT’14 English-to-German, NIST Chinese-to-English, and WAT Japanese-to-English translation tasks demonstrate that the proposed methods can significantly improve the performance of the Transformer",
    "volume": "main",
    "checked": true,
    "id": "957f53525b386d6f31077449c8ec23b83c19f506",
    "citation_count": 23
  },
  "https://aclanthology.org/P19-1175": {
    "title": "Neural Fuzzy Repair: Integrating Fuzzy Matches into Neural Machine Translation",
    "abstract": "We present a simple yet powerful data augmentation method for boosting Neural Machine Translation (NMT) performance by leveraging information retrieved from a Translation Memory (TM). We propose and test two methods for augmenting NMT training data with fuzzy TM matches. Tests on the DGT-TM data set for two language pairs show consistent and substantial improvements over a range of baseline systems. The results suggest that this method is promising for any translation environment in which a sizeable TM is available and a certain amount of repetition across translations is to be expected, especially considering its ease of implementation",
    "volume": "main",
    "checked": true,
    "id": "4ba55448a9a978e01a33638f5b294a9db1d5920b",
    "citation_count": 42
  },
  "https://aclanthology.org/P19-1176": {
    "title": "Learning Deep Transformer Models for Machine Translation",
    "abstract": "Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT’16 English-German and NIST OpenMT’12 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4-2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big",
    "volume": "main",
    "checked": true,
    "id": "a39398f68ae7e042f2ef5009e31b4e6a20fd5736",
    "citation_count": 333
  },
  "https://aclanthology.org/P19-1177": {
    "title": "Generating Diverse Translations with Sentence Codes",
    "abstract": "Users of machine translation systems may desire to obtain multiple candidates translated in different ways. In this work, we attempt to obtain diverse translations by using sentence codes to condition the sentence generation. We describe two methods to extract the codes, either with or without the help of syntax information. For diverse generation, we sample multiple candidates, each of which conditioned on a unique code. Experiments show that the sampled translations have much higher diversity scores when using reasonable sentence codes, where the translation quality is still on par with the baselines even under strong constraint imposed by the codes. In qualitative analysis, we show that our method is able to generate paraphrase translations with drastically different structures. The proposed approach can be easily adopted to existing translation systems as no modification to the model is required",
    "volume": "main",
    "checked": true,
    "id": "513926a7f7ae94e25053ca94f14b7a3639e09112",
    "citation_count": 28
  },
  "https://aclanthology.org/P19-1178": {
    "title": "Self-Supervised Neural Machine Translation",
    "abstract": "We present a simple new method where an emergent NMT system is used for simultaneously selecting training data and learning internal NMT representations. This is done in a self-supervised way without parallel data, in such a way that both tasks enhance each other during training. The method is language independent, introduces no additional hyper-parameters, and achieves BLEU scores of 29.21 (en2fr) and 27.36 (fr2en) on newstest2014 using English and French Wikipedia data for training",
    "volume": "main",
    "checked": true,
    "id": "a0467ad021e3b1f11d6f95c6e2df206455ca768b",
    "citation_count": 22
  },
  "https://aclanthology.org/P19-1179": {
    "title": "Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation",
    "abstract": "Previous work on end-to-end translation from speech has primarily used frame-level features as speech representations, which creates longer, sparser sequences than text. We show that a naive method to create compressed phoneme-like speech representations is far more effective and efficient for translation than traditional frame-level speech features. Specifically, we generate phoneme labels for speech frames and average consecutive frames with the same label to create shorter, higher-level source sequences for translation. We see improvements of up to 5 BLEU on both our high and low resource language pairs, with a reduction in training time of 60%. Our improvements hold across multiple data sizes and two language pairs",
    "volume": "main",
    "checked": true,
    "id": "fbd47a815c73a83e8a47ee2ed38826c82ffc0c2a",
    "citation_count": 27
  },
  "https://aclanthology.org/P19-1180": {
    "title": "Visually Grounded Neural Syntax Acquisition",
    "abstract": "We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach for learning syntactic representations and structures without any explicit supervision. The model learns by looking at natural images and reading paired captions. VG-NSL generates constituency parse trees of texts, recursively composes representations for constituents, and matches them with images. We define concreteness of constituents by their matching scores with images, and use it to guide the parsing of text. Experiments on the MSCOCO data set show that VG-NSL outperforms various unsupervised parsing approaches that do not use visual grounding, in terms of F1 scores against gold parse trees. We find that VGNSL is much more stable with respect to the choice of random initialization and the amount of training data. We also find that the concreteness acquired by VG-NSL correlates well with a similar measure defined by linguists. Finally, we also apply VG-NSL to multiple languages in the Multi30K data set, showing that our model consistently outperforms prior unsupervised approaches",
    "volume": "main",
    "checked": true,
    "id": "708f8c0eb5032edd6f31663a27febbb0529cbcf3",
    "citation_count": 55
  },
  "https://aclanthology.org/P19-1181": {
    "title": "Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation",
    "abstract": "Advances in learning and representations have reinvigorated work that connects language to other modalities. A particularly exciting direction is Vision-and-Language Navigation(VLN), in which agents interpret natural language instructions and visual scenes to move through environments and reach goals. Despite recent progress, current research leaves unclear how much of a role language under-standing plays in this task, especially because dominant evaluation metrics have focused on goal completion rather than the sequence of actions corresponding to the instructions. Here, we highlight shortcomings of current metrics for the Room-to-Room dataset (Anderson et al.,2018b) and propose a new metric, Coverage weighted by Length Score (CLS). We also show that the existing paths in the dataset are not ideal for evaluating instruction following because they are direct-to-goal shortest paths. We join existing short paths to form more challenging extended paths to create a new data set, Room-for-Room (R4R). Using R4R and CLS, we show that agents that receive rewards for instruction fidelity outperform agents that focus on goal completion",
    "volume": "main",
    "checked": true,
    "id": "68ccecb380ecfc0a4b294b84e3d0b6ff6884c4df",
    "citation_count": 91
  },
  "https://aclanthology.org/P19-1182": {
    "title": "Expressing Visual Relationships via Language",
    "abstract": "Describing images with text is a fundamental problem in vision-language research. Current studies in this domain mostly focus on single image captioning. However, in various real applications (e.g., image editing, difference interpretation, and retrieval), generating relational captions for two images, can also be very useful. This important problem has not been explored mostly due to lack of datasets and effective models. To push forward the research in this direction, we first introduce a new language-guided image editing dataset that contains a large number of real image pairs with corresponding editing instructions. We then propose a new relational speaker model based on an encoder-decoder architecture with static relational attention and sequential multi-head attention. We also extend the model with dynamic relational attention, which calculates visual alignment while decoding. Our models are evaluated on our newly collected and two public datasets consisting of image pairs annotated with relationship sentences. Experimental results, based on both automatic and human evaluation, demonstrate that our model outperforms all baselines and existing methods on all the datasets",
    "volume": "main",
    "checked": true,
    "id": "4eb2e2b9c22cb1da8561044ca0dc8fc0b13e3157",
    "citation_count": 22
  },
  "https://aclanthology.org/P19-1183": {
    "title": "Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video",
    "abstract": "In this paper, we address a novel task, namely weakly-supervised spatio-temporally grounding natural sentence in video. Specifically, given a natural sentence and a video, we localize a spatio-temporal tube in the video that semantically corresponds to the given sentence, with no reliance on any spatio-temporal annotations during training. First, a set of spatio-temporal tubes, referred to as instances, are extracted from the video. We then encode these instances and the sentence using our newly proposed attentive interactor which can exploit their fine-grained relationships to characterize their matching behaviors. Besides a ranking loss, a novel diversity loss is introduced to train our attentive interactor to strengthen the matching behaviors of reliable instance-sentence pairs and penalize the unreliable ones. We also contribute a dataset, called VID-sentence, based on the ImageNet video object detection dataset, to serve as a benchmark for our task. Results from extensive experiments demonstrate the superiority of our model over the baseline approaches",
    "volume": "main",
    "checked": true,
    "id": "7e158a7619c6d0d583a8bbdf00658b7d4bbe0374",
    "citation_count": 58
  },
  "https://aclanthology.org/P19-1184": {
    "title": "The PhotoBook Dataset: Building Common Ground through Visually-Grounded Dialogue",
    "abstract": "This paper introduces the PhotoBook dataset, a large-scale collection of visually-grounded, task-oriented dialogues in English designed to investigate shared dialogue history accumulating during conversation. Taking inspiration from seminal work on dialogue analysis, we propose a data-collection task formulated as a collaborative game prompting two online participants to refer to images utilising both their visual context as well as previously established referring expressions. We provide a detailed description of the task setup and a thorough analysis of the 2,500 dialogues collected. To further illustrate the novel features of the dataset, we propose a baseline model for reference resolution which uses a simple method to take into account shared information accumulated in a reference chain. Our results show that this information is particularly important to resolve later descriptions and underline the need to develop more sophisticated models of common ground in dialogue interaction",
    "volume": "main",
    "checked": true,
    "id": "76cb048a3571f71e191c9731c69644eec16eb1ed",
    "citation_count": 50
  },
  "https://aclanthology.org/P19-1185": {
    "title": "Continual and Multi-Task Architecture Search",
    "abstract": "Architecture search is the process of automatically learning the neural model or cell structure that best suits the given task. Recently, this approach has shown promising performance improvements (on language modeling and image classification) with reasonable training speed, using a weight sharing strategy called Efficient Neural Architecture Search (ENAS). In our work, we first introduce a novel continual architecture search (CAS) approach, so as to continually evolve the model parameters during the sequential training of several tasks, without losing performance on previously learned tasks (via block-sparsity and orthogonality constraints), thus enabling life-long learning. Next, we explore a multi-task architecture search (MAS) approach over ENAS for finding a unified, single cell structure that performs well across multiple tasks (via joint controller rewards), and hence allows more generalizable transfer of the cell structure knowledge to an unseen new task. We empirically show the effectiveness of our sequential continual learning and parallel multi-task learning based architecture search approaches on diverse sentence-pair classification tasks (GLUE) and multimodal-generation based video captioning tasks. Further, we present several ablations and analyses on the learned cell structures",
    "volume": "main",
    "checked": true,
    "id": "2ef851a47fc32d596883e08a5f655179b8c5b02d",
    "citation_count": 34
  },
  "https://aclanthology.org/P19-1186": {
    "title": "Semi-supervised Stochastic Multi-Domain Learning using Variational Inference",
    "abstract": "Supervised models of NLP rely on large collections of text which closely resemble the intended testing setting. Unfortunately matching text is often not available in sufficient quantity, and moreover, within any domain of text, data is often highly heterogenous. In this paper we propose a method to distill the important domain signal as part of a multi-domain learning system, using a latent variable model in which parts of a neural model are stochastically gated based on the inferred domain. We compare the use of discrete versus continuous latent variables, operating in a domain-supervised or a domain semi-supervised setting, where the domain is known only for a subset of training inputs. We show that our model leads to substantial performance improvements over competitive benchmark domain adaptation methods, including methods using adversarial learning",
    "volume": "main",
    "checked": true,
    "id": "5b60e5216b66cfc7f9e0c41de154915efe6e5ef3",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-1187": {
    "title": "Boosting Entity Linking Performance by Leveraging Unlabeled Documents",
    "abstract": "Modern entity linking systems rely on large collections of documents specifically annotated for the task (e.g., AIDA CoNLL). In contrast, we propose an approach which exploits only naturally occurring information: unlabeled documents and Wikipedia. Our approach consists of two stages. First, we construct a high recall list of candidate entities for each mention in an unlabeled document. Second, we use the candidate lists as weak supervision to constrain our document-level entity linking model. The model treats entities as latent variables and, when estimated on a collection of unlabelled texts, learns to choose entities relying both on local context of each mention and on coherence with other entities in the document. The resulting approach rivals fully-supervised state-of-the-art systems on standard test sets. It also approaches their performance in the very challenging setting: when tested on a test set sampled from the data used to estimate the supervised systems. By comparing to Wikipedia-only training of our model, we demonstrate that modeling unlabeled documents is beneficial",
    "volume": "main",
    "checked": true,
    "id": "6deb944e1d9294e3c3f121db5576ed5827520a88",
    "citation_count": 29
  },
  "https://aclanthology.org/P19-1188": {
    "title": "Pre-Learning Environment Representations for Data-Efficient Neural Instruction Following",
    "abstract": "We consider the problem of learning to map from natural language instructions to state transitions (actions) in a data-efficient manner. Our method takes inspiration from the idea that it should be easier to ground language to concepts that have already been formed through pre-linguistic observation. We augment a baseline instruction-following learner with an initial environment-learning phase that uses observations of language-free state transitions to induce a suitable latent representation of actions before processing the instruction-following training data. We show that mapping to pre-learned representations substantially improves performance over systems whose representations are learned from limited instructional data alone",
    "volume": "main",
    "checked": true,
    "id": "8723a153e64ec0aa354f5b8c75696747cfc4178a",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1189": {
    "title": "Reinforced Training Data Selection for Domain Adaptation",
    "abstract": "Supervised models suffer from the problem of domain shifting where distribution mismatch in the data across domains greatly affect model performance. To solve the problem, training data selection (TDS) has been proven to be a prospective solution for domain adaptation in leveraging appropriate data. However, conventional TDS methods normally requires a predefined threshold which is neither easy to set nor can be applied across tasks, and models are trained separately with the TDS process. To make TDS self-adapted to data and task, and to combine it with model training, in this paper, we propose a reinforcement learning (RL) framework that synchronously searches for training instances relevant to the target domain and learns better representations for them. A selection distribution generator (SDG) is designed to perform the selection and is updated according to the rewards computed from the selected data, where a predictor is included in the framework to ensure a task-specific model can be trained on the selected data and provides feedback to rewards. Experimental results from part-of-speech tagging, dependency parsing, and sentiment analysis, as well as ablation studies, illustrate that the proposed framework is not only effective in data selection and representation, but also generalized to accommodate different NLP tasks",
    "volume": "main",
    "checked": true,
    "id": "435799b6bb553b535f62d1fb796960a824cfc5cf",
    "citation_count": 24
  },
  "https://aclanthology.org/P19-1190": {
    "title": "Generating Long and Informative Reviews with Aspect-Aware Coarse-to-Fine Decoding",
    "abstract": "Generating long and informative review text is a challenging natural language generation task. Previous work focuses on word-level generation, neglecting the importance of topical and syntactic characteristics from natural languages. In this paper, we propose a novel review generation model by characterizing an elaborately designed aspect-aware coarse-to-fine generation process. First, we model the aspect transitions to capture the overall content flow. Then, to generate a sentence, an aspect-aware sketch will be predicted using an aspect-aware decoder. Finally, another decoder fills in the semantic slots by generating corresponding words. Our approach is able to jointly utilize aspect semantics, syntactic sketch, and context information. Extensive experiments results have demonstrated the effectiveness of the proposed model",
    "volume": "main",
    "checked": true,
    "id": "b432970be7e305a50575b86ba62f811cf56da0b0",
    "citation_count": 23
  },
  "https://aclanthology.org/P19-1191": {
    "title": "PaperRobot: Incremental Draft Generation of Scientific Ideas",
    "abstract": "We present a PaperRobot who performs as an automatic research assistant by (1) conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs (KGs); (2) creating new ideas by predicting links from the background KGs, by combining graph attention and contextual text attention; (3) incrementally writing some key elements of a new paper based on memory-attention networks: from the input title along with predicted related entities to generate a paper abstract, from the abstract to generate conclusion and future work, and finally from future work to generate a title for a follow-on paper. Turing Tests, where a biomedical domain expert is asked to compare a system output and a human-authored string, show PaperRobot generated abstracts, conclusion and future work sections, and new titles are chosen over human-written ones up to 30%, 24% and 12% of the time, respectively",
    "volume": "main",
    "checked": true,
    "id": "a6aed0c4e0f39a55edb407f492e41f178a62907f",
    "citation_count": 29
  },
  "https://aclanthology.org/P19-1192": {
    "title": "Rhetorically Controlled Encoder-Decoder for Modern Chinese Poetry Generation",
    "abstract": "Rhetoric is a vital element in modern poetry, and plays an essential role in improving its aesthetics. However, to date, it has not been considered in research on automatic poetry generation. In this paper, we propose a rhetorically controlled encoder-decoder for modern Chinese poetry generation. Our model relies on a continuous latent variable as a rhetoric controller to capture various rhetorical patterns in an encoder, and then incorporates rhetoric-based mixtures while generating modern Chinese poetry. For metaphor and personification, an automated evaluation shows that our model outperforms state-of-the-art baselines by a substantial margin, while human evaluation shows that our model generates better poems than baseline methods in terms of fluency, coherence, meaningfulness, and rhetorical aesthetics",
    "volume": "main",
    "checked": true,
    "id": "3b94efdce77e097d099de2528519059bab4b90df",
    "citation_count": 18
  },
  "https://aclanthology.org/P19-1193": {
    "title": "Enhancing Topic-to-Essay Generation with External Commonsense Knowledge",
    "abstract": "Automatic topic-to-essay generation is a challenging task since it requires generating novel, diverse, and topic-consistent paragraph-level text with a set of topics as input. Previous work tends to perform essay generation based solely on the given topics while ignoring massive commonsense knowledge. However, this commonsense knowledge provides additional background information, which can help to generate essays that are more novel and diverse. Towards filling this gap, we propose to integrate commonsense from the external knowledge base into the generator through dynamic memory mechanism. Besides, the adversarial training based on a multi-label discriminator is employed to further improve topic-consistency. We also develop a series of automatic evaluation metrics to comprehensively assess the quality of the generated essay. Experiments show that with external commonsense knowledge and adversarial training, the generated essays are more novel, diverse, and topic-consistent than existing methods in terms of both automatic and human evaluation",
    "volume": "main",
    "checked": true,
    "id": "4cc7c5aac8bf44265b985d5e0bf5bf65d3f2e0e6",
    "citation_count": 57
  },
  "https://aclanthology.org/P19-1194": {
    "title": "Towards Fine-grained Text Sentiment Transfer",
    "abstract": "In this paper, we focus on the task of fine-grained text sentiment transfer (FGST). This task aims to revise an input sequence to satisfy a given sentiment intensity, while preserving the original semantic content. Different from the conventional sentiment transfer task that only reverses the sentiment polarity (positive/negative) of text, the FTST task requires more nuanced and fine-grained control of sentiment. To remedy this, we propose a novel Seq2SentiSeq model. Specifically, the numeric sentiment intensity value is incorporated into the decoder via a Gaussian kernel layer to finely control the sentiment intensity of the output. Moreover, to tackle the problem of lacking parallel data, we propose a cycle reinforcement learning algorithm to guide the model training. In this framework, the elaborately designed rewards can balance both sentiment transformation and content preservation, while not requiring any ground truth output. Experimental results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation",
    "volume": "main",
    "checked": true,
    "id": "608a2a208e3ced44da309a8930ef713e4a06a2f2",
    "citation_count": 31
  },
  "https://aclanthology.org/P19-1195": {
    "title": "Data-to-text Generation with Entity Modeling",
    "abstract": "Recent approaches to data-to-text generation have shown great promise thanks to the use of large-scale datasets and the application of neural network architectures which are trained end-to-end. These models rely on representation learning to select content appropriately, structure it coherently, and verbalize it grammatically, treating entities as nothing more than vocabulary tokens. In this work we propose an entity-centric neural architecture for data-to-text generation. Our model creates entity-specific representations which are dynamically updated. Text is generated conditioned on the data input and entity memory representations using hierarchical attention at each time step. We present experiments on the RotoWire benchmark and a (five times larger) new dataset on the baseball domain which we create. Our results show that the proposed model outperforms competitive baselines in automatic and human evaluation",
    "volume": "main",
    "checked": true,
    "id": "416e3ffff2fe2d43343f6b721721a482829f882d",
    "citation_count": 82
  },
  "https://aclanthology.org/P19-1196": {
    "title": "Ensuring Readability and Data-fidelity using Head-modifier Templates in Deep Type Description Generation",
    "abstract": "A type description is a succinct noun compound which helps human and machines to quickly grasp the informative and distinctive information of an entity. Entities in most knowledge graphs (KGs) still lack such descriptions, thus calling for automatic methods to supplement such information. However, existing generative methods either overlook the grammatical structure or make factual mistakes in generated texts. To solve these problems, we propose a head-modifier template based method to ensure the readability and data fidelity of generated type descriptions. We also propose a new dataset and two metrics for this task. Experiments show that our method improves substantially compared with baselines and achieves state-of-the-art performance on both datasets",
    "volume": "main",
    "checked": true,
    "id": "de6219302bae6dfcabb5a4088777ee2afc97d6bc",
    "citation_count": 2
  },
  "https://aclanthology.org/P19-1197": {
    "title": "Key Fact as Pivot: A Two-Stage Model for Low Resource Table-to-Text Generation",
    "abstract": "Table-to-text generation aims to translate the structured data into the unstructured text. Most existing methods adopt the encoder-decoder framework to learn the transformation, which requires large-scale training samples. However, the lack of large parallel data is a major practical problem for many domains. In this work, we consider the scenario of low resource table-to-text generation, where only limited parallel data is available. We propose a novel model to separate the generation into two stages: key fact prediction and surface realization. It first predicts the key facts from the tables, and then generates the text with the key facts. The training of key fact prediction needs much fewer annotated data, while surface realization can be trained with pseudo parallel corpus. We evaluate our model on a biography generation dataset. Our model can achieve 27.34 BLEU score with only 1,000 parallel data, while the baseline model only obtain the performance of 9.71 BLEU score",
    "volume": "main",
    "checked": true,
    "id": "924ea1528b555ddfbfe6167f8fd18fd0e8b36479",
    "citation_count": 29
  },
  "https://aclanthology.org/P19-1198": {
    "title": "Unsupervised Neural Text Simplification",
    "abstract": "The paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabeled text corpora. The core framework is composed of a shared encoder and a pair of attentional-decoders, crucially assisted by discrimination-based losses and denoising. The framework is trained using unlabeled text collected from en-Wikipedia dump. Our analysis (both quantitative and qualitative involving human evaluators) on public test data shows that the proposed model can perform text-simplification at both lexical and syntactic levels, competitive to existing supervised methods. It also outperforms viable unsupervised baselines. Adding a few labeled pairs helps improve the performance further",
    "volume": "main",
    "checked": true,
    "id": "d8ad95d1918ed72a89fc3d8f842179ee9520cb19",
    "citation_count": 61
  },
  "https://aclanthology.org/P19-1199": {
    "title": "Syntax-Infused Variational Autoencoder for Text Generation",
    "abstract": "We present a syntax-infused variational autoencoder (SIVAE), that integrates sentences with their syntactic trees to improve the grammar of generated sentences. Distinct from existing VAE-based text generative models, SIVAE contains two separate latent spaces, for sentences and syntactic trees. The evidence lower bound objective is redesigned correspondingly, by optimizing a joint distribution that accommodates two encoders and two decoders. SIVAE works with long short-term memory architectures to simultaneously generate sentences and syntactic trees. Two versions of SIVAE are proposed: one captures the dependencies between the latent variables through a conditional prior network, and the other treats the latent variables independently such that syntactically-controlled sentence generation can be performed. Experimental results demonstrate the generative superiority of SIVAE on both reconstruction and targeted syntactic evaluations. Finally, we show that the proposed models can be used for unsupervised paraphrasing given different syntactic tree templates",
    "volume": "main",
    "checked": true,
    "id": "7c6b9df00d1c9c027063a1577b503a6355a5fc32",
    "citation_count": 32
  },
  "https://aclanthology.org/P19-1200": {
    "title": "Towards Generating Long and Coherent Text with Multi-Level Latent Variable Models",
    "abstract": "Variational autoencoders (VAEs) have received much attention recently as an end-to-end architecture for text generation with latent variables. However, previous works typically focus on synthesizing relatively short sentences (up to 20 words), and the posterior collapse issue has been widely identified in text-VAEs. In this paper, we propose to leverage several multi-level structures to learn a VAE model for generating long, and coherent text. In particular, a hierarchy of stochastic layers between the encoder and decoder networks is employed to abstract more informative and semantic-rich latent codes. Besides, we utilize a multi-level decoder structure to capture the coherent long-term structure inherent in long-form texts, by generating intermediate sentence representations as high-level plan vectors. Extensive experimental results demonstrate that the proposed multi-level VAE model produces more coherent and less repetitive long text compared to baselines as well as can mitigate the posterior-collapse issue",
    "volume": "main",
    "checked": true,
    "id": "39b4fb9f4bc29e380598623e5a5203f6e34b4be3",
    "citation_count": 42
  },
  "https://aclanthology.org/P19-1201": {
    "title": "Jointly Learning Semantic Parser and Natural Language Generator via Dual Information Maximization",
    "abstract": "Semantic parsing aims to transform natural language (NL) utterances into formal meaning representations (MRs), whereas an NL generator achieves the reverse: producing an NL description for some given MRs. Despite this intrinsic connection, the two tasks are often studied separately in prior work. In this paper, we model the duality of these two tasks via a joint learning framework, and demonstrate its effectiveness of boosting the performance on both tasks. Concretely, we propose a novel method of dual information maximization (DIM) to regularize the learning process, where DIM empirically maximizes the variational lower bounds of expected joint distributions of NL and MRs. We further extend DIM to a semi-supervision setup (SemiDIM), which leverages unlabeled data of both tasks. Experiments on three datasets of dialogue management and code generation (and summarization) show that performance on both semantic parsing and NL generation can be consistently improved by DIM, in both supervised and semi-supervised setups",
    "volume": "main",
    "checked": true,
    "id": "1fbaed00dbda975a6209761857dd1a78618c6585",
    "citation_count": 19
  },
  "https://aclanthology.org/P19-1202": {
    "title": "Learning to Select, Track, and Generate for Data-to-Text",
    "abstract": "We propose a data-to-text generation model with two modules, one for tracking and the other for text generation. Our tracking module selects and keeps track of salient information and memorizes which record has been mentioned. Our generation module generates a summary conditioned on the state of tracking module. Our proposed model is considered to simulate the human-like writing process that gradually selects the information by determining the intermediate variables while writing the summary. In addition, we also explore the effectiveness of the writer information for generations. Experimental results show that our proposed model outperforms existing models in all evaluation metrics even without writer information. Incorporating writer information further improves the performance, contributing to content planning and surface realization",
    "volume": "main",
    "checked": true,
    "id": "6fe2c143c5f12867bc55a43faac07c2ebb7919a4",
    "citation_count": 41
  },
  "https://aclanthology.org/P19-1203": {
    "title": "Reinforced Dynamic Reasoning for Conversational Question Generation",
    "abstract": "This paper investigates a new task named Conversational Question Generation (CQG) which is to generate a question based on a passage and a conversation history (i.e., previous turns of question-answer pairs). CQG is a crucial task for developing intelligent agents that can drive question-answering style conversations or test user understanding of a given passage. Towards that end, we propose a new approach named Reinforced Dynamic Reasoning network, which is based on the general encoder-decoder framework but incorporates a reasoning procedure in a dynamic manner to better understand what has been asked and what to ask next about the passage into the general encoder-decoder framework. To encourage producing meaningful questions, we leverage a popular question answering (QA) model to provide feedback and fine-tune the question generator using a reinforcement learning mechanism. Empirical results on the recently released CoQA dataset demonstrate the effectiveness of our method in comparison with various baselines and model variants. Moreover, to show the applicability of our method, we also apply it to create multi-turn question-answering conversations for passages in SQuAD",
    "volume": "main",
    "checked": true,
    "id": "d8cc00856eb2b503842cc806f420b1df2cfc6700",
    "citation_count": 27
  },
  "https://aclanthology.org/P19-1204": {
    "title": "TalkSumm: A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks",
    "abstract": "Currently, no large-scale training data is available for the task of scientific paper summarization. In this paper, we propose a novel method that automatically generates summaries for scientific papers, by utilizing videos of talks at scientific conferences. We hypothesize that such talks constitute a coherent and concise description of the papers’ content, and can form the basis for good summaries. We collected 1716 papers and their corresponding videos, and created a dataset of paper summaries. A model trained on this dataset achieves similar performance as models trained on a dataset of summaries created manually. In addition, we validated the quality of our summaries by human experts",
    "volume": "main",
    "checked": true,
    "id": "8490d23beacff4ae8003aa082a0b16b8fcce3e43",
    "citation_count": 43
  },
  "https://aclanthology.org/P19-1205": {
    "title": "Improving Abstractive Document Summarization with Salient Information Modeling",
    "abstract": "Comprehensive document encoding and salient information selection are two major difficulties for generating summaries with adequate salient information. To tackle the above difficulties, we propose a Transformer-based encoder-decoder framework with two novel extensions for abstractive document summarization. Specifically, (1) to encode the documents comprehensively, we design a focus-attention mechanism and incorporate it into the encoder. This mechanism models a Gaussian focal bias on attention scores to enhance the perception of local context, which contributes to producing salient and informative summaries. (2) To distinguish salient information precisely, we design an independent saliency-selection network which manages the information flow from encoder to decoder. This network effectively reduces the influences of secondary information on the generated summaries. Experimental results on the popular CNN/Daily Mail benchmark demonstrate that our model outperforms other state-of-the-art baselines on the ROUGE metrics",
    "volume": "main",
    "checked": true,
    "id": "1c5d2289e7db0a99f56badb081405f6f3bff6e38",
    "citation_count": 36
  },
  "https://aclanthology.org/P19-1206": {
    "title": "Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking",
    "abstract": "This paper focuses on the end-to-end abstractive summarization of a single product review without supervision. We assume that a review can be described as a discourse tree, in which the summary is the root, and the child sentences explain their parent in detail. By recursively estimating a parent from its children, our model learns the latent discourse tree without an external parser and generates a concise summary. We also introduce an architecture that ranks the importance of each sentence on the tree to support summary generation focusing on the main review point. The experimental results demonstrate that our model is competitive with or outperforms other unsupervised approaches. In particular, for relatively long reviews, it achieves a competitive or better performance than supervised models. The induced tree shows that the child sentences provide additional information about their parent, and the generated summary abstracts the entire review",
    "volume": "main",
    "checked": true,
    "id": "b0cd945f73e0f28a47cacb53f16d394917eea3a8",
    "citation_count": 32
  },
  "https://aclanthology.org/P19-1207": {
    "title": "BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization",
    "abstract": "The success of neural summarization models stems from the meticulous encodings of source articles. To overcome the impediments of limited and sometimes noisy training data, one promising direction is to make better use of the available training data by applying filters during summarization. In this paper, we propose a novel Bi-directional Selective Encoding with Template (BiSET) model, which leverages template discovered from training data to softly select key information from each source article to guide its summarization process. Extensive experiments on a standard summarization dataset are conducted and the results show that the template-equipped BiSET model manages to improve the summarization performance significantly with a new state of the art",
    "volume": "main",
    "checked": true,
    "id": "5fd37583f7aadaace5c7bbb3394d3c856bf6b9bb",
    "citation_count": 41
  },
  "https://aclanthology.org/P19-1208": {
    "title": "Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards",
    "abstract": "Generating keyphrases that summarize the main points of a document is a fundamental task in natural language processing. Although existing generative models are capable of predicting multiple keyphrases for an input document as well as determining the number of keyphrases to generate, they still suffer from the problem of generating too few keyphrases. To address this problem, we propose a reinforcement learning (RL) approach for keyphrase generation, with an adaptive reward function that encourages a model to generate both sufficient and accurate keyphrases. Furthermore, we introduce a new evaluation method that incorporates name variations of the ground-truth keyphrases using the Wikipedia knowledge base. Thus, our evaluation method can more robustly evaluate the quality of predicted keyphrases. Extensive experiments on five real-world datasets of different scales demonstrate that our RL approach consistently and significantly improves the performance of the state-of-the-art generative models with both conventional and new evaluation methods",
    "volume": "main",
    "checked": true,
    "id": "272730c72d6c40cf9cd6ca82664fcd273e032192",
    "citation_count": 51
  },
  "https://aclanthology.org/P19-1209": {
    "title": "Scoring Sentence Singletons and Pairs for Abstractive Summarization",
    "abstract": "When writing a summary, humans tend to choose content from one or two sentences and merge them into a single summary sentence. However, the mechanisms behind the selection of one or multiple source sentences remain poorly understood. Sentence fusion assumes multi-sentence input; yet sentence selection methods only work with single sentences and not combinations of them. There is thus a crucial gap between sentence selection and fusion to support summarizing by both compressing single sentences and fusing pairs. This paper attempts to bridge the gap by ranking sentence singletons and pairs together in a unified space. Our proposed framework attempts to model human methodology by selecting either a single sentence or a pair of sentences, then compressing or fusing the sentence(s) to produce a summary sentence. We conduct extensive experiments on both single- and multi-document summarization datasets and report findings on sentence selection and abstraction",
    "volume": "main",
    "checked": true,
    "id": "bc19e16762f8d37eb53d419b12be2b4b2cd937be",
    "citation_count": 78
  },
  "https://aclanthology.org/P19-1210": {
    "title": "Keep Meeting Summaries on Topic: Abstractive Multi-Modal Meeting Summarization",
    "abstract": "Transcripts of natural, multi-person meetings differ significantly from documents like news articles, which can make Natural Language Generation models for generating summaries unfocused. We develop an abstractive meeting summarizer from both videos and audios of meeting recordings. Specifically, we propose a multi-modal hierarchical attention across three levels: segment, utterance and word. To narrow down the focus into topically-relevant segments, we jointly model topic segmentation and summarization. In addition to traditional text features, we introduce new multi-modal features derived from visual focus of attention, based on the assumption that the utterance is more important if the speaker receives more attention. Experiments show that our model significantly outperforms the state-of-the-art with both BLEU and ROUGE measures",
    "volume": "main",
    "checked": true,
    "id": "8c2f5ed9efe3985ded8ee724dffc6ebf1f082493",
    "citation_count": 85
  },
  "https://aclanthology.org/P19-1211": {
    "title": "Adversarial Domain Adaptation Using Artificial Titles for Abstractive Title Generation",
    "abstract": "A common issue in training a deep learning, abstractive summarization model is lack of a large set of training summaries. This paper examines techniques for adapting from a labeled source domain to an unlabeled target domain in the context of an encoder-decoder model for text generation. In addition to adversarial domain adaptation (ADA), we introduce the use of artificial titles and sequential training to capture the grammatical style of the unlabeled target domain. Evaluation on adapting to/from news articles and Stack Exchange posts indicates that the use of these techniques can boost performance for both unsupervised adaptation as well as fine-tuning with limited target data",
    "volume": "main",
    "checked": true,
    "id": "7babff88d77beb97e38eb067d097d1522b3dfc49",
    "citation_count": 4
  },
  "https://aclanthology.org/P19-1212": {
    "title": "BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization",
    "abstract": "Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article’s global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Compared to existing summarization datasets, BIGPATENT has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research",
    "volume": "main",
    "checked": true,
    "id": "40345901fd28cbf65791c34671db6548b1089ed4",
    "citation_count": 102
  },
  "https://aclanthology.org/P19-1213": {
    "title": "Ranking Generated Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference",
    "abstract": "While recent progress on abstractive summarization has led to remarkably fluent summaries, factual errors in generated summaries still severely limit their use in practice. In this paper, we evaluate summaries produced by state-of-the-art models via crowdsourcing and show that such errors occur frequently, in particular with more abstractive models. We study whether textual entailment predictions can be used to detect such errors and if they can be reduced by reranking alternative predicted summaries. That leads to an interesting downstream application for entailment models. In our experiments, we find that out-of-the-box entailment models trained on NLI datasets do not yet offer the desired performance for the downstream task and we therefore release our annotations as additional test data for future extrinsic evaluations of NLI",
    "volume": "main",
    "checked": true,
    "id": "470c7c26f16bc52ea68a159c84f07b78d58fd02a",
    "citation_count": 159
  },
  "https://aclanthology.org/P19-1214": {
    "title": "Self-Supervised Learning for Contextualized Extractive Summarization",
    "abstract": "Existing models for extractive summarization are usually trained from scratch with a cross-entropy loss, which does not explicitly capture the global context at the document level. In this paper, we aim to improve this task by introducing three auxiliary pre-training tasks that learn to capture the document-level context in a self-supervised fashion. Experiments on the widely-used CNN/DM dataset validate the effectiveness of the proposed auxiliary tasks. Furthermore, we show that after pre-training, a clean model with simple building blocks is able to outperform previous state-of-the-art that are carefully designed",
    "volume": "main",
    "checked": true,
    "id": "c9144a7b01f6ff12a76fe7ec0202fae0b3e4bcce",
    "citation_count": 44
  },
  "https://aclanthology.org/P19-1215": {
    "title": "On the Summarization of Consumer Health Questions",
    "abstract": "Question understanding is one of the main challenges in question answering. In real world applications, users often submit natural language questions that are longer than needed and include peripheral information that increases the complexity of the question, leading to substantially more false positives in answer retrieval. In this paper, we study neural abstractive models for medical question summarization. We introduce the MeQSum corpus of 1,000 summarized consumer health questions. We explore data augmentation methods and evaluate state-of-the-art neural abstractive models on this new task. In particular, we show that semantic augmentation from question datasets improves the overall performance, and that pointer-generator networks outperform sequence-to-sequence attentional models on this task, with a ROUGE-1 score of 44.16%. We also present a detailed error analysis and discuss directions for improvement that are specific to question summarization",
    "volume": "main",
    "checked": true,
    "id": "2dad078c48278da520d5bd67ed2e4fca0ef85e83",
    "citation_count": 30
  },
  "https://aclanthology.org/P19-1216": {
    "title": "Unsupervised Rewriter for Multi-Sentence Compression",
    "abstract": "Multi-sentence compression (MSC) aims to generate a grammatical but reduced compression from multiple input sentences while retaining their key information. Previous dominating approach for MSC is the extraction-based word graph approach. A few variants further leveraged lexical substitution to yield more abstractive compression. However, two limitations exist. First, the word graph approach that simply concatenates fragments from multiple sentences may yield non-fluent or ungrammatical compression. Second, lexical substitution is often inappropriate without the consideration of context information. To tackle the above-mentioned issues, we present a neural rewriter for multi-sentence compression that does not need any parallel corpus. Empirical studies have shown that our approach achieves comparable results upon automatic evaluation and improves the grammaticality of compression based on human evaluation. A parallel corpus with more than 140,000 (sentence group, compression) pairs is also constructed as a by-product for future research",
    "volume": "main",
    "checked": true,
    "id": "225df0550811b9423535844abd6f09efff42c838",
    "citation_count": 13
  },
  "https://aclanthology.org/P19-1217": {
    "title": "Inferential Machine Comprehension: Answering Questions by Recursively Deducing the Evidence Chain from Text",
    "abstract": "This paper focuses on the topic of inferential machine comprehension, which aims to fully understand the meanings of given text to answer generic questions, especially the ones needed reasoning skills. In particular, we first encode the given document, question and options in a context aware way. We then propose a new network to solve the inference problem by decomposing it into a series of attention-based reasoning steps. The result of the previous step acts as the context of next step. To make each step can be directly inferred from the text, we design an operational cell with prior structure. By recursively linking the cells, the inferred results are synthesized together to form the evidence chain for reasoning, where the reasoning direction can be guided by imposing structural constraints to regulate interactions on the cells. Moreover, a termination mechanism is introduced to dynamically determine the uncertain reasoning depth, and the network is trained by reinforcement learning. Experimental results on 3 popular data sets, including MCTest, RACE and MultiRC, demonstrate the effectiveness of our approach",
    "volume": "main",
    "checked": true,
    "id": "43823cb40516b7f166dca3f42f2fe259c1e851af",
    "citation_count": 14
  },
  "https://aclanthology.org/P19-1218": {
    "title": "Token-level Dynamic Self-Attention Network for Multi-Passage Reading Comprehension",
    "abstract": "Multi-passage reading comprehension requires the ability to combine cross-passage information and reason over multiple passages to infer the answer. In this paper, we introduce the Dynamic Self-attention Network (DynSAN) for multi-passage reading comprehension task, which processes cross-passage information at token-level and meanwhile avoids substantial computational costs. The core module of the dynamic self-attention is a proposed gated token selection mechanism, which dynamically selects important tokens from a sequence. These chosen tokens will attend to each other via a self-attention mechanism to model long-range dependencies. Besides, convolutional layers are combined with the dynamic self-attention to enhance the model’s capacity of extracting local semantic. The experimental results show that the proposed DynSAN achieves new state-of-the-art performance on the SearchQA, Quasar-T and WikiHop datasets. Further ablation study also validates the effectiveness of our model components",
    "volume": "main",
    "checked": true,
    "id": "efe4902a39c8ef332058ae7d156a6bffcd3c1341",
    "citation_count": 16
  },
  "https://aclanthology.org/P19-1219": {
    "title": "Explicit Utilization of General Knowledge in Machine Reading Comprehension",
    "abstract": "To bridge the gap between Machine Reading Comprehension (MRC) models and human beings, which is mainly reflected in the hunger for data and the robustness to noise, in this paper, we explore how to integrate the neural networks of MRC models with the general knowledge of human beings. On the one hand, we propose a data enrichment method, which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage-question pair. On the other hand, we propose an end-to-end MRC model named as Knowledge Aided Reader (KAR), which explicitly uses the above extracted general knowledge to assist its attention mechanisms. Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them. When only a subset (20%-80%) of the training examples are available, KAR outperforms the state-of-the-art MRC models by a large margin, and is still reasonably robust to noise",
    "volume": "main",
    "checked": true,
    "id": "1c7ad3e8018864464441ee8da4069cf729bde1df",
    "citation_count": 38
  },
  "https://aclanthology.org/P19-1220": {
    "title": "Multi-style Generative Reading Comprehension",
    "abstract": "This study tackles generative reading comprehension (RC), which consists of answering questions based on textual evidence and natural language generation (NLG). We propose a multi-style abstractive summarization model for question answering, called Masque. The proposed model has two key characteristics. First, unlike most studies on RC that have focused on extracting an answer span from the provided passages, our model instead focuses on generating a summary from the question and multiple passages. This serves to cover various answer styles required for real-world applications. Second, whereas previous studies built a specific model for each answer style because of the difficulty of acquiring one general model, our approach learns multi-style answers within a model to improve the NLG capability for all styles involved. This also enables our model to give an answer in the target style. Experiments show that our model achieves state-of-the-art performance on the Q&A task and the Q&A + NLG task of MS MARCO 2.1 and the summary task of NarrativeQA. We observe that the transfer of the style-independent NLG capability to the target style is the key to its success",
    "volume": "main",
    "checked": true,
    "id": "4c09d30704c0ceb128bb31ee09f957ee58d5032c",
    "citation_count": 54
  },
  "https://aclanthology.org/P19-1221": {
    "title": "Retrieve, Read, Rerank: Towards End-to-End Multi-Document Reading Comprehension",
    "abstract": "This paper considers the reading comprehension task in which multiple documents are given as input. Prior work has shown that a pipeline of retriever, reader, and reranker can improve the overall performance. However, the pipeline system is inefficient since the input is re-encoded within each module, and is unable to leverage upstream components to help downstream training. In this work, we present RE3QA, a unified question answering model that combines context retrieving, reading comprehension, and answer reranking to predict the final answer. Unlike previous pipelined approaches, RE3QA shares contextualized text representation across different components, and is carefully designed to use high-quality upstream outputs (e.g., retrieved context or candidate answers) for directly supervising downstream modules (e.g., the reader or the reranker). As a result, the whole network can be trained end-to-end to avoid the context inconsistency problem. Experiments show that our model outperforms the pipelined baseline and achieves state-of-the-art results on two versions of TriviaQA and two variants of SQuAD",
    "volume": "main",
    "checked": true,
    "id": "c6460bf5763355e5e4cadea256825f575896b2a3",
    "citation_count": 41
  },
  "https://aclanthology.org/P19-1222": {
    "title": "Multi-Hop Paragraph Retrieval for Open-Domain Question Answering",
    "abstract": "This paper is concerned with the task of multi-hop open-domain Question Answering (QA). This task is particularly challenging since it requires the simultaneous performance of textual reasoning and efficient searching. We present a method for retrieving multiple supporting paragraphs, nested amidst a large knowledge base, which contain the necessary evidence to answer a given question. Our method iteratively retrieves supporting paragraphs by forming a joint vector representation of both a question and a paragraph. The retrieval is performed by considering contextualized sentence-level representations of the paragraphs in the knowledge source. Our method achieves state-of-the-art performance over two well-known datasets, SQuAD-Open and HotpotQA, which serve as our single- and multi-hop open-domain QA benchmarks, respectively",
    "volume": "main",
    "checked": true,
    "id": "03ffb4b19fef9ae483e526c2b6608ae8e1f65561",
    "citation_count": 71
  },
  "https://aclanthology.org/P19-1223": {
    "title": "E3: Entailment-driven Extracting and Editing for Conversational Machine Reading",
    "abstract": "Conversational machine reading systems help users answer high-level questions (e.g. determine if they qualify for particular government benefits) when they do not know the exact rules by which the determination is made (e.g. whether they need certain income levels or veteran status). The key challenge is that these rules are only provided in the form of a procedural text (e.g. guidelines from government website) which the system must read to figure out what to ask the user. We present a new conversational machine reading model that jointly extracts a set of decision rules from the procedural text while reasoning about which are entailed by the conversational history and which still need to be edited to create questions for the user. On the recently introduced ShARC conversational machine reading dataset, our Entailment-driven Extract and Edit network (E3) achieves a new state-of-the-art, outperforming existing systems as well as a new BERT-based baseline. In addition, by explicitly highlighting which information still needs to be gathered, E3 provides a more explainable alternative to prior work. We release source code for our models and experiments at https://github.com/vzhong/e3",
    "volume": "main",
    "checked": true,
    "id": "7a38f72c03a3a55ae0387dd8b526a5e16f01d426",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-1224": {
    "title": "Generating Question-Answer Hierarchies",
    "abstract": "The process of knowledge acquisition can be viewed as a question-answer game between a student and a teacher in which the student typically starts by asking broad, open-ended questions before drilling down into specifics (Hintikka, 1981; Hakkarainen and Sintonen, 2002). This pedagogical perspective motivates a new way of representing documents. In this paper, we present SQUASH (Specificity-controlled Question-Answer Hierarchies), a novel and challenging text generation task that converts an input document into a hierarchy of question-answer pairs. Users can click on high-level questions (e.g., “Why did Frodo leave the Fellowship?”) to reveal related but more specific questions (e.g., “Who did Frodo leave with?”). Using a question taxonomy loosely based on Lehnert (1978), we classify questions in existing reading comprehension datasets as either GENERAL or SPECIFIC . We then use these labels as input to a pipelined system centered around a conditional neural language model. We extensively evaluate the quality of the generated QA hierarchies through crowdsourced experiments and report strong empirical results",
    "volume": "main",
    "checked": true,
    "id": "8da992b611df508b1803f66ffa53bd1fb741a76c",
    "citation_count": 27
  },
  "https://aclanthology.org/P19-1225": {
    "title": "Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction",
    "abstract": "Question answering (QA) using textual sources for purposes such as reading comprehension (RC) has attracted much attention. This study focuses on the task of explainable multi-hop QA, which requires the system to return the answer with evidence sentences by reasoning and gathering disjoint pieces of the reference texts. It proposes the Query Focused Extractor (QFE) model for evidence extraction and uses multi-task learning with the QA model. QFE is inspired by extractive summarization models; compared with the existing method, which extracts each evidence sentence independently, it sequentially extracts evidence sentences by using an RNN with an attention mechanism on the question sentence. It enables QFE to consider the dependency among the evidence sentences and cover important information in the question sentence. Experimental results show that QFE with a simple RC baseline model achieves a state-of-the-art evidence extraction score on HotpotQA. Although designed for RC, it also achieves a state-of-the-art evidence extraction score on FEVER, which is a recognizing textual entailment task on a large textual database",
    "volume": "main",
    "checked": true,
    "id": "bed4621f62e3955b93285047a99013106998498c",
    "citation_count": 79
  },
  "https://aclanthology.org/P19-1226": {
    "title": "Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension",
    "abstract": "Machine reading comprehension (MRC) is a crucial and challenging task in NLP. Recently, pre-trained language models (LMs), especially BERT, have achieved remarkable success, presenting new state-of-the-art results in MRC. In this work, we investigate the potential of leveraging external knowledge bases (KBs) to further improve BERT for MRC. We introduce KT-NET, which employs an attention mechanism to adaptively select desired knowledge from KBs, and then fuses selected knowledge with BERT to enable context- and knowledge-aware predictions. We believe this would combine the merits of both deep LMs and curated KBs towards better MRC. Experimental results indicate that KT-NET offers significant and consistent improvements over BERT, outperforming competitive baselines on ReCoRD and SQuAD1.1 benchmarks. Notably, it ranks the 1st place on the ReCoRD leaderboard, and is also the best single model on the SQuAD1.1 leaderboard at the time of submission (March 4th, 2019)",
    "volume": "main",
    "checked": true,
    "id": "e7046bf945ad6326537a1ac78a96fd2f45acc900",
    "citation_count": 103
  },
  "https://aclanthology.org/P19-1227": {
    "title": "XQA: A Cross-lingual Open-domain Question Answering Dataset",
    "abstract": "Open-domain question answering (OpenQA) aims to answer questions through text retrieval and reading comprehension. Recently, lots of neural network-based models have been proposed and achieved promising results in OpenQA. However, the success of these models relies on a massive volume of training data (usually in English), which is not available in many other languages, especially for those low-resource languages. Therefore, it is essential to investigate cross-lingual OpenQA. In this paper, we construct a novel dataset XQA for cross-lingual OpenQA research. It consists of a training set in English as well as development and test sets in eight other languages. Besides, we provide several baseline systems for cross-lingual OpenQA, including two machine translation-based methods and one zero-shot cross-lingual method (multilingual BERT). Experimental results show that the multilingual BERT model achieves the best results in almost all target languages, while the performance of cross-lingual OpenQA is still much lower than that of English. Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are, but also how difficult the question set of the target language is. The XQA dataset is publicly available at http://github.com/thunlp/XQA",
    "volume": "main",
    "checked": true,
    "id": "f95f6392dd5aac975574ffd5ace4376927c79831",
    "citation_count": 57
  },
  "https://aclanthology.org/P19-1228": {
    "title": "Compound Probabilistic Context-Free Grammars for Grammar Induction",
    "abstract": "We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this context-dependent grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models",
    "volume": "main",
    "checked": true,
    "id": "7ea59779ffb392f099d5304680126b4299f43750",
    "citation_count": 102
  },
  "https://aclanthology.org/P19-1229": {
    "title": "Semi-supervised Domain Adaptation for Dependency Parsing",
    "abstract": "During the past decades, due to the lack of sufficient labeled data, most studies on cross-domain parsing focus on unsupervised domain adaptation, assuming there is no target-domain training data. However, unsupervised approaches make limited progress so far due to the intrinsic difficulty of both domain adaptation and parsing. This paper tackles the semi-supervised domain adaptation problem for Chinese dependency parsing, based on two newly-annotated large-scale domain-aware datasets. We propose a simple domain embedding approach to merge the source- and target-domain training data, which is shown to be more effective than both direct corpus concatenation and multi-task learning. In order to utilize unlabeled target-domain data, we employ the recent contextualized word representations and show that a simple fine-tuning procedure can further boost cross-domain parsing accuracy by large margin",
    "volume": "main",
    "checked": true,
    "id": "1bdd14d6cfceb033edcf84ce471a99abfd780754",
    "citation_count": 23
  },
  "https://aclanthology.org/P19-1230": {
    "title": "Head-Driven Phrase Structure Grammar Parsing on Penn Treebank",
    "abstract": "Head-driven phrase structure grammar (HPSG) enjoys a uniform formalism representing rich contextual syntactic and even semantic meanings. This paper makes the first attempt to formulate a simplified HPSG by integrating constituent and dependency formal representations into head-driven phrase structure. Then two parsing algorithms are respectively proposed for two converted tree representations, division span and joint span. As HPSG encodes both constituent and dependency structure information, the proposed HPSG parsers may be regarded as a sort of joint decoder for both types of structures and thus are evaluated in terms of extracted or converted constituent and dependency parsing trees. Our parser achieves new state-of-the-art performance for both parsing tasks on Penn Treebank (PTB) and Chinese Penn Treebank, verifying the effectiveness of joint learning constituent and dependency structures. In details, we report 95.84 F1 of constituent parsing and 97.00% UAS of dependency parsing on PTB",
    "volume": "main",
    "checked": true,
    "id": "72c0e5b7365ae4981db13cfa15ca808a8eb3a8a1",
    "citation_count": 106
  },
  "https://aclanthology.org/P19-1231": {
    "title": "Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning",
    "abstract": "In this work, we explore the way to perform named entity recognition (NER) using only unlabeled data and named entity dictionaries. To this end, we formulate the task as a positive-unlabeled (PU) learning problem and accordingly propose a novel PU learning algorithm to perform the task. We prove that the proposed algorithm can unbiasedly and consistently estimate the task loss as if there is fully labeled data. A key feature of the proposed method is that it does not require the dictionaries to label every entity within a sentence, and it even does not require the dictionaries to label all of the words constituting an entity. This greatly reduces the requirement on the quality of the dictionaries and makes our method generalize well with quite simple dictionaries. Empirical studies on four public NER datasets demonstrate the effectiveness of our proposed method. We have published the source code at https://github.com/v-mipeng/LexiconNER",
    "volume": "main",
    "checked": true,
    "id": "30964c12b5729fea0fad78ae1b03214e2632496c",
    "citation_count": 57
  },
  "https://aclanthology.org/P19-1232": {
    "title": "Multi-Task Semantic Dependency Parsing with Policy Gradient for Learning Easy-First Strategies",
    "abstract": "In Semantic Dependency Parsing (SDP), semantic relations form directed acyclic graphs, rather than trees. We propose a new iterative predicate selection (IPS) algorithm for SDP. Our IPS algorithm combines the graph-based and transition-based parsing approaches in order to handle multiple semantic head words. We train the IPS model using a combination of multi-task learning and task-specific policy gradient training. Trained this way, IPS achieves a new state of the art on the SemEval 2015 Task 18 datasets. Furthermore, we observe that policy gradient training learns an easy-first strategy",
    "volume": "main",
    "checked": true,
    "id": "57a219123a6541958210847cba9decd49269b3c6",
    "citation_count": 25
  },
  "https://aclanthology.org/P19-1233": {
    "title": "GCDT: A Global Context Enhanced Deep Transition Architecture for Sequence Labeling",
    "abstract": "Current state-of-the-art systems for sequence labeling are typically based on the family of Recurrent Neural Networks (RNNs). However, the shallow connections between consecutive hidden states of RNNs and insufficient modeling of global information restrict the potential performance of those models. In this paper, we try to address these issues, and thus propose a Global Context enhanced Deep Transition architecture for sequence labeling named GCDT. We deepen the state transition path at each position in a sentence, and further assign every token with a global representation learned from the entire sentence. Experiments on two standard sequence labeling tasks show that, given only training data and the ubiquitous word embeddings (Glove), our GCDT achieves 91.96 F1 on the CoNLL03 NER task and 95.43 F1 on the CoNLL2000 Chunking task, which outperforms the best reported results under the same settings. Furthermore, by leveraging BERT as an additional resource, we establish new state-of-the-art results with 93.47 F1 on NER and 97.30 F1 on Chunking",
    "volume": "main",
    "checked": true,
    "id": "f9c16fca2662a82428a977e09db0ea02a74911da",
    "citation_count": 61
  },
  "https://aclanthology.org/P19-1234": {
    "title": "Unsupervised Learning of PCFGs with Normalizing Flow",
    "abstract": "Unsupervised PCFG inducers hypothesize sets of compact context-free rules as explanations for sentences. PCFG induction not only provides tools for low-resource languages, but also plays an important role in modeling language acquisition (Bannard et al., 2009; Abend et al. 2017). However, current PCFG induction models, using word tokens as input, are unable to incorporate semantics and morphology into induction, and may encounter issues of sparse vocabulary when facing morphologically rich languages. This paper describes a neural PCFG inducer which employs context embeddings (Peters et al., 2018) in a normalizing flow model (Dinh et al., 2015) to extend PCFG induction to use semantic and morphological information. Linguistically motivated sparsity and categorical distance constraints are imposed on the inducer as regularization. Experiments show that the PCFG induction model with normalizing flow produces grammars with state-of-the-art accuracy on a variety of different languages. Ablation further shows a positive effect of normalizing flow, context embeddings and proposed regularizers",
    "volume": "main",
    "checked": true,
    "id": "5479d9f218bd1e7a9a4a13c225feedf69a02ac85",
    "citation_count": 24
  },
  "https://aclanthology.org/P19-1235": {
    "title": "Variance of Average Surprisal: A Better Predictor for Quality of Grammar from Unsupervised PCFG Induction",
    "abstract": "In unsupervised grammar induction, data likelihood is known to be only weakly correlated with parsing accuracy, especially at convergence after multiple runs. In order to find a better indicator for quality of induced grammars, this paper correlates several linguistically- and psycholinguistically-motivated predictors to parsing accuracy on a large multilingual grammar induction evaluation data set. Results show that variance of average surprisal (VAS) better correlates with parsing accuracy than data likelihood and that using VAS instead of data likelihood for model selection provides a significant accuracy boost. Further evidence shows VAS to be a better candidate than data likelihood for predicting word order typology classification. Analyses show that VAS seems to separate content words from function words in natural language grammars, and to better arrange words with different frequencies into separate classes that are more consistent with linguistic theory",
    "volume": "main",
    "checked": true,
    "id": "68223f28869f81517c87a66e235bfcfcc2bf1e50",
    "citation_count": 2
  },
  "https://aclanthology.org/P19-1236": {
    "title": "Cross-Domain NER using Cross-Domain Language Modeling",
    "abstract": "Due to limitation of labeled resources, cross-domain named entity recognition (NER) has been a challenging task. Most existing work considers a supervised setting, making use of labeled data for both the source and target domains. A disadvantage of such methods is that they cannot train for domains without NER data. To address this issue, we consider using cross-domain LM as a bridge cross-domains for NER domain adaptation, performing cross-domain and cross-task knowledge transfer by designing a novel parameter generation network. Results show that our method can effectively extract domain differences from cross-domain LM contrast, allowing unsupervised domain adaptation while also giving state-of-the-art results among supervised domain adaptation methods",
    "volume": "main",
    "checked": true,
    "id": "647f2ce03258c6963fbaa0e86e615a20c27e2c02",
    "citation_count": 72
  },
  "https://aclanthology.org/P19-1237": {
    "title": "Graph-based Dependency Parsing with Graph Neural Networks",
    "abstract": "We investigate the problem of efficiently incorporating high-order features into neural graph-based dependency parsing. Instead of explicitly extracting high-order features from intermediate parse trees, we develop a more powerful dependency tree node representation which captures high-order information concisely and efficiently. We use graph neural networks (GNNs) to learn the representations and discuss several new configurations of GNN’s updating and aggregation functions. Experiments on PTB show that our parser achieves the best UAS and LAS on PTB (96.0%, 94.3%) among systems without using any external resources",
    "volume": "main",
    "checked": true,
    "id": "ebc639bc6aea3e66241d4cab8a52a16efde920ea",
    "citation_count": 43
  },
  "https://aclanthology.org/P19-1238": {
    "title": "Wide-Coverage Neural A* Parsing for Minimalist Grammars",
    "abstract": "Minimalist Grammars (Stabler, 1997) are a computationally oriented, and rigorous formalisation of many aspects of Chomsky’s (1995) Minimalist Program. This paper presents the first ever application of this formalism to the task of realistic wide-coverage parsing. The parser uses a linguistically expressive yet highly constrained grammar, together with an adaptation of the A* search algorithm currently used in CCG parsing (Lewis and Steedman, 2014; Lewis et al., 2016), with supertag probabilities provided by a bi-LSTM neural network supertagger trained on MGbank, a corpus of MG derivation trees. We report on some promising initial experimental results for overall dependency recovery as well as on the recovery of certain unbounded long distance dependencies. Finally, although like other MG parsers, ours has a high order polynomial worst case time complexity, we show that in practice its expected time complexity is cubic in the length of the sentence. The parser is publicly available",
    "volume": "main",
    "checked": true,
    "id": "c20bf952d4e772760bafe435feee1114fc0671a6",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-1239": {
    "title": "Multi-Modal Sarcasm Detection in Twitter with Hierarchical Fusion Model",
    "abstract": "Sarcasm is a subtle form of language in which people express the opposite of what is implied. Previous works of sarcasm detection focused on texts. However, more and more social media platforms like Twitter allow users to create multi-modal messages, including texts, images, and videos. It is insufficient to detect sarcasm from multi-model messages based only on texts. In this paper, we focus on multi-modal sarcasm detection for tweets consisting of texts and images in Twitter. We treat text features, image features and image attributes as three modalities and propose a multi-modal hierarchical fusion model to address this task. Our model first extracts image features and attribute features, and then leverages attribute features and bidirectional LSTM network to extract text features. Features of three modalities are then reconstructed and fused into one feature vector for prediction. We create a multi-modal sarcasm detection dataset based on Twitter. Evaluation results on the dataset demonstrate the efficacy of our proposed model and the usefulness of the three modalities",
    "volume": "main",
    "checked": true,
    "id": "4fdd17e212bedd5cb288bf756e44e5af07ebc86c",
    "citation_count": 84
  },
  "https://aclanthology.org/P19-1240": {
    "title": "Topic-Aware Neural Keyphrase Generation for Social Media Language",
    "abstract": "A huge volume of user-generated content is daily produced on social media. To facilitate automatic language understanding, we study keyphrase prediction, distilling salient information from massive posts. While most existing methods extract words from source posts to form keyphrases, we propose a sequence-to-sequence (seq2seq) based neural keyphrase generation framework, enabling absent keyphrases to be created. Moreover, our model, being topic-aware, allows joint modeling of corpus-level latent topic representations, which helps alleviate data sparsity widely exhibited in social media language. Experiments on three datasets collected from English and Chinese social media platforms show that our model significantly outperforms both extraction and generation models without exploiting latent topics. Further discussions show that our model learns meaningful topics, which interprets its superiority in social media keyphrase generation",
    "volume": "main",
    "checked": true,
    "id": "10702199c8bd9edafa5be1ab68bd75b611af7ec7",
    "citation_count": 53
  },
  "https://aclanthology.org/P19-1241": {
    "title": "#YouToo? Detection of Personal Recollections of Sexual Harassment on Social Media",
    "abstract": "The availability of large-scale online social data, coupled with computational methods can help us answer fundamental questions relat- ing to our social lives, particularly our health and well-being. The #MeToo trend has led to people talking about personal experiences of harassment more openly. This work at- tempts to aggregate such experiences of sex- ual abuse to facilitate a better understanding of social media constructs and to bring about social change. It has been found that disclo- sure of abuse has positive psychological im- pacts. Hence, we contend that such informa- tion can leveraged to create better campaigns for social change by analyzing how users react to these stories and to obtain a better insight into the consequences of sexual abuse. We use a three part Twitter-Specific Social Media Lan- guage Model to segregate personal recollec- tions of sexual harassment from Twitter posts. An extensive comparison with state-of-the-art generic and specific models along with a de- tailed error analysis explores the merit of our proposed model",
    "volume": "main",
    "checked": true,
    "id": "5173d1a961e346de0c058b8dad64082f9ba4b850",
    "citation_count": 32
  },
  "https://aclanthology.org/P19-1242": {
    "title": "Multi-task Pairwise Neural Ranking for Hashtag Segmentation",
    "abstract": "Hashtags are often employed on social media and beyond to add metadata to a textual utterance with the goal of increasing discoverability, aiding search, or providing additional semantics. However, the semantic content of hashtags is not straightforward to infer as these represent ad-hoc conventions which frequently include multiple words joined together and can include abbreviations and unorthodox spellings. We build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for hashtag segmentation by framing it as a pairwise ranking problem between candidate segmentations. Our novel neural approaches demonstrate 24.6% error reduction in hashtag segmentation accuracy compared to the current state-of-the-art method. Finally, we demonstrate that a deeper understanding of hashtag semantics obtained through segmentation is useful for downstream applications such as sentiment analysis, for which we achieved a 2.6% increase in average recall on the SemEval 2017 sentiment analysis dataset",
    "volume": "main",
    "checked": true,
    "id": "987c77c910eb1bbe109d5a1d9f76cf8fb92e22cb",
    "citation_count": 10
  },
  "https://aclanthology.org/P19-1243": {
    "title": "Entity-Centric Contextual Affective Analysis",
    "abstract": "While contextualized word representations have improved state-of-the-art benchmarks in many NLP tasks, their potential usefulness for social-oriented tasks remains largely unexplored. We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. We evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. We find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. We ultimately use our method to examine differences in portrayals of men and women",
    "volume": "main",
    "checked": true,
    "id": "a1280728623e8fd605284b2b7cf536579b9e2cbf",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-1244": {
    "title": "Sentence-Level Evidence Embedding for Claim Verification with Hierarchical Attention Networks",
    "abstract": "Claim verification is generally a task of verifying the veracity of a given claim, which is critical to many downstream applications. It is cumbersome and inefficient for human fact-checkers to find consistent pieces of evidence, from which solid verdict could be inferred against the claim. In this paper, we propose a novel end-to-end hierarchical attention network focusing on learning to represent coherent evidence as well as their semantic relatedness with the claim. Our model consists of three main components: 1) A coherence-based attention layer embeds coherent evidence considering the claim and sentences from relevant articles; 2) An entailment-based attention layer attends on sentences that can semantically infer the claim on top of the first attention; and 3) An output layer predicts the verdict based on the embedded evidence. Experimental results on three public benchmark datasets show that our proposed model outperforms a set of state-of-the-art baselines",
    "volume": "main",
    "checked": true,
    "id": "e18c26b9faa94d03af5161cac103289b5f732dae",
    "citation_count": 55
  },
  "https://aclanthology.org/P19-1245": {
    "title": "Predicting Human Activities from User-Generated Content",
    "abstract": "The activities we do are linked to our interests, personality, political preferences, and decisions we make about the future. In this paper, we explore the task of predicting human activities from user-generated content. We collect a dataset containing instances of social media users writing about a range of everyday activities. We then use a state-of-the-art sentence embedding framework tailored to recognize the semantics of human activities and perform an automatic clustering of these activities. We train a neural network model to make predictions about which clusters contain activities that were performed by a given user based on the text of their previous posts and self-description. Additionally, we explore the degree to which incorporating inferred user traits into our model helps with this prediction task",
    "volume": "main",
    "checked": true,
    "id": "e299df2ecff33364ef4d799aa8ec89713bd7c5f1",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-1246": {
    "title": "You Write like You Eat: Stylistic Variation as a Predictor of Social Stratification",
    "abstract": "Inspired by Labov’s seminal work on stylisticvariation as a function of social stratification,we develop and compare neural models thatpredict a person’s presumed socio-economicstatus, obtained through distant supervision,from their writing style on social media. Thefocus of our work is on identifying the mostimportant stylistic parameters to predict socio-economic group. In particular, we show theeffectiveness of morpho-syntactic features aspredictors of style, in contrast to lexical fea-tures, which are good predictors of topic",
    "volume": "main",
    "checked": true,
    "id": "187b9b996285e06042a806462ce3a55c234e1b63",
    "citation_count": 3
  },
  "https://aclanthology.org/P19-1247": {
    "title": "Encoding Social Information with Graph Convolutional Networks forPolitical Perspective Detection in News Media",
    "abstract": "Identifying the political perspective shaping the way news events are discussed in the media is an important and challenging task. In this paper, we highlight the importance of contextualizing social information, capturing how this information is disseminated in social networks. We use Graph Convolutional Networks, a recently proposed neural architecture for representing relational information, to capture the documents’ social context. We show that social information can be used effectively as a source of distant supervision, and when direct supervision is available, even little social information can significantly improve performance",
    "volume": "main",
    "checked": true,
    "id": "cfca1faca73f5c310ff669f27024b3182d5731e3",
    "citation_count": 79
  },
  "https://aclanthology.org/P19-1248": {
    "title": "Fine-Grained Spoiler Detection from Large-Scale Review Corpora",
    "abstract": "This paper presents computational approaches for automatically detecting critical plot twists in reviews of media products. First, we created a large-scale book review dataset that includes fine-grained spoiler annotations at the sentence-level, as well as book and (anonymized) user information. Second, we carefully analyzed this dataset, and found that: spoiler language tends to be book-specific; spoiler distributions vary greatly across books and review authors; and spoiler sentences tend to jointly appear in the latter part of reviews. Third, inspired by these findings, we developed an end-to-end neural network architecture to detect spoiler sentences in review corpora. Quantitative and qualitative results demonstrate that the proposed method substantially outperforms existing baselines",
    "volume": "main",
    "checked": true,
    "id": "00fb281a201b0349fb6c59ef90a549120f6eddf7",
    "citation_count": 56
  },
  "https://aclanthology.org/P19-1249": {
    "title": "Celebrity Profiling",
    "abstract": "Celebrities are among the most prolific users of social media, promoting their personas and rallying followers. This activity is closely tied to genuine writing samples, which makes them worthy research subjects in many respects, not least profiling. With this paper we introduce the Webis Celebrity Corpus 2019. For its construction the Twitter feeds of 71,706 verified accounts have been carefully linked with their respective Wikidata items, crawling both. After cleansing, the resulting profiles contain an average of 29,968 words per profile and up to 239 pieces of personal information. A cross-evaluation that checked the correct association of Twitter account and Wikidata item revealed an error rate of only 0.6%, rendering the profiles highly reliable. Our corpus comprises a wide cross-section of local and global celebrities, forming a unique combination of scale, profile comprehensiveness, and label reliability. We further establish the state of the art’s profiling performance by evaluating the winning approaches submitted to the PAN gender prediction tasks in a transfer learning experiment. They are only outperformed by our own deep learning approach, which we also use to exemplify celebrity occupation prediction for the first time",
    "volume": "main",
    "checked": true,
    "id": "51e989831b29257d6d8c70554d4d330caa4fe3c8",
    "citation_count": 28
  },
  "https://aclanthology.org/P19-1250": {
    "title": "Dataset Creation for Ranking Constructive News Comments",
    "abstract": "Ranking comments on an online news service is a practically important task for the service provider, and thus there have been many studies on this task. However, most of them considered users’ positive feedback, such as “Like”-button clicks, as a quality measure. In this paper, we address directly evaluating the quality of comments on the basis of “constructiveness,” separately from user feedback. To this end, we create a new dataset including 100K+ Japanese comments with constructiveness scores (C-scores). Our experiments clarify that C-scores are not always related to users’ positive feedback, and the performance of pairwise ranking models tends to be enhanced by the variation of comments rather than articles",
    "volume": "main",
    "checked": true,
    "id": "81c23b7074afecaef54ca6a84c7e909fac32dc50",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1251": {
    "title": "Enhancing Air Quality Prediction with Social Media and Natural Language Processing",
    "abstract": "Accompanied by modern industrial developments, air pollution has already become a major concern for human health. Hence, air quality measures, such as the concentration of PM2.5, have attracted increasing attention. Even some studies apply historical measurements into air quality forecast, the changes of air quality conditions are still hard to monitor. In this paper, we propose to exploit social media and natural language processing techniques to enhance air quality prediction. Social media users are treated as social sensors with their findings and locations. After filtering noisy tweets using word selection and topic modeling, a deep learning model based on convolutional neural networks and over-tweet-pooling is proposed to enhance air quality prediction. We conduct experiments on 7-month real-world Twitter datasets in the five most heavily polluted states in the USA. The results show that our approach significantly improves air quality prediction over the baseline that does not use social media by 6.9% to 17.7% in macro-F1 scores",
    "volume": "main",
    "checked": true,
    "id": "659462bc113b506d442bf81b710ee75b90304685",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-1252": {
    "title": "Twitter Homophily: Network Based Prediction of User's Occupation",
    "abstract": "In this paper, we investigate the importance of social network information compared to content information in the prediction of a Twitter user’s occupational class. We show that the content information of a user’s tweets, the profile descriptions of a user’s follower/following community, and the user’s social network provide useful information for classifying a user’s occupational group. In our study, we extend an existing data set for this problem, and we achieve significantly better performance by using social network homophily that has not been fully exploited in previous work. In our analysis, we found that by using the graph convolutional network to exploit social homophily, we can achieve competitive performance on this data set with just a small fraction of the training data",
    "volume": "main",
    "checked": true,
    "id": "d2213fbe11928f89b287c0039395ac44262b1451",
    "citation_count": 13
  },
  "https://aclanthology.org/P19-1253": {
    "title": "Domain Adaptive Dialog Generation via Meta Learning",
    "abstract": "Domain adaptation is an essential task in dialog system building because there are so many new dialog tasks created for different needs every day. Collecting and annotating training data for these new tasks is costly since it involves real user interactions. We propose a domain adaptive dialog generation method based on meta-learning (DAML). DAML is an end-to-end trainable dialog system model that learns from multiple rich-resource tasks and then adapts to new domains with minimal training samples. We train a dialog system model using multiple rich-resource single-domain dialog data by applying the model-agnostic meta-learning algorithm to dialog domain. The model is capable of learning a competitive dialog system on a new domain with only a few training examples in an efficient manner. The two-step gradient updates in DAML enable the model to learn general features across multiple tasks. We evaluate our method on a simulated dialog dataset and achieve state-of-the-art performance, which is generalizable to new tasks",
    "volume": "main",
    "checked": true,
    "id": "104d77f2281a705deee73e1b5e4d064ab28d4561",
    "citation_count": 99
  },
  "https://aclanthology.org/P19-1254": {
    "title": "Strategies for Structuring Story Generation",
    "abstract": "Writers often rely on plans or sketches to write long stories, but most current language models generate word by word from left to right. We explore coarse-to-fine models for creating narrative texts of several hundred words, and introduce new models which decompose stories by abstracting over actions and entities. The model first generates the predicate-argument structure of the text, where different mentions of the same entity are marked with placeholder tokens. It then generates a surface realization of the predicate-argument structure, and finally replaces the entity placeholders with context-sensitive names and references. Human judges prefer the stories from our models to a wide range of previous approaches to hierarchical text generation. Extensive analysis shows that our methods can help improve the diversity and coherence of events and entities in generated stories",
    "volume": "main",
    "checked": true,
    "id": "2d6c0f7774d9d30d4972f5dba1d6e5389b3ddd2f",
    "citation_count": 150
  },
  "https://aclanthology.org/P19-1255": {
    "title": "Argument Generation with Retrieval, Planning, and Realization",
    "abstract": "Automatic argument generation is an appealing but challenging task. In this paper, we study the specific problem of counter-argument generation, and present a novel framework, CANDELA. It consists of a powerful retrieval system and a novel two-step generation model, where a text planning decoder first decides on the main talking points and a proper language style for each sentence, then a content realization decoder reflects the decisions and constructs an informative paragraph-level argument. Furthermore, our generation model is empowered by a retrieval system indexed with 12 million articles collected from Wikipedia and popular English news media, which provides access to high-quality content with diversity. Automatic evaluation on a large-scale dataset collected from Reddit shows that our model yields significantly higher BLEU, ROUGE, and METEOR scores than the state-of-the-art and non-trivial comparisons. Human evaluation further indicates that our system arguments are more appropriate for refutation and richer in content",
    "volume": "main",
    "checked": true,
    "id": "b7cc9fa44ac51a5fadf4528fcf0d276151ee7d17",
    "citation_count": 33
  },
  "https://aclanthology.org/P19-1256": {
    "title": "A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation",
    "abstract": "Recent neural language generation systems often hallucinate contents (i.e., producing irrelevant or contradicted facts), especially when trained on loosely corresponding pairs of the input structure and text. To mitigate this issue, we propose to integrate a language understanding module for data refinement with self-training iterations to effectively induce strong equivalence between the input data and the paired text. Experiments on the E2E challenge dataset show that our proposed framework can reduce more than 50% relative unaligned noise from the original data-text pairs. A vanilla sequence-to-sequence neural NLG model trained on the refined data has improved on content correctness compared with the current state-of-the-art ensemble generator",
    "volume": "main",
    "checked": true,
    "id": "8aa12409bc8ca1c18016c77d756ea2eae9ef9968",
    "citation_count": 55
  },
  "https://aclanthology.org/P19-1257": {
    "title": "Cross-Modal Commentator: Automatic Machine Commenting Based on Cross-Modal Information",
    "abstract": "Automatic commenting of online articles can provide additional opinions and facts to the reader, which improves user experience and engagement on social media platforms. Previous work focuses on automatic commenting based solely on textual content. However, in real-scenarios, online articles usually contain multiple modal contents. For instance, graphic news contains plenty of images in addition to text. Contents other than text are also vital because they are not only more attractive to the reader but also may provide critical information. To remedy this, we propose a new task: cross-model automatic commenting (CMAC), which aims to make comments by integrating multiple modal contents. We construct a large-scale dataset for this task and explore several representative methods. Going a step further, an effective co-attention model is presented to capture the dependency between textual and visual information. Evaluation results show that our proposed model can achieve better performance than competitive baselines",
    "volume": "main",
    "checked": true,
    "id": "cbf4d7e9274ba37f040aeeaad451abe98c569b88",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1258": {
    "title": "A Working Memory Model for Task-oriented Dialog Response Generation",
    "abstract": "Recently, to incorporate external Knowledge Base (KB) information, one form of world knowledge, several end-to-end task-oriented dialog systems have been proposed. These models, however, tend to confound the dialog history with KB tuples and simply store them into one memory. Inspired by the psychological studies on working memory, we propose a working memory model (WMM2Seq) for dialog response generation. Our WMM2Seq adopts a working memory to interact with two separated long-term memories, which are the episodic memory for memorizing dialog history and the semantic memory for storing KB tuples. The working memory consists of a central executive to attend to the aforementioned memories, and a short-term storage system to store the “activated” contents from the long-term memories. Furthermore, we introduce a context-sensitive perceptual process for the token representations of dialog history, and then feed them into the episodic memory. Extensive experiments on two task-oriented dialog datasets demonstrate that our WMM2Seq significantly outperforms the state-of-the-art results in several evaluation metrics",
    "volume": "main",
    "checked": true,
    "id": "ce6e1a9fd351e9abcdc39dc11629892435e27c83",
    "citation_count": 30
  },
  "https://aclanthology.org/P19-1259": {
    "title": "Cognitive Graph for Multi-Hop Reading Comprehension at Scale",
    "abstract": "We propose a new CogQA framework for multi-hop reading comprehension question answering in web-scale documents. Founded on the dual process theory in cognitive science, the framework gradually builds a cognitive graph in an iterative process by coordinating an implicit extraction module (System 1) and an explicit reasoning module (System 2). While giving accurate answers, our framework further provides explainable reasoning paths. Specifically, our implementation based on BERT and graph neural network efficiently handles millions of documents for multi-hop reasoning questions in the HotpotQA fullwiki dataset, achieving a winning joint F1 score of 34.9 on the leaderboard, compared to 23.1 of the best competitor",
    "volume": "main",
    "checked": true,
    "id": "ad8f43bfdb041fd97b3fea2eea23fff6e2221476",
    "citation_count": 154
  },
  "https://aclanthology.org/P19-1260": {
    "title": "Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs",
    "abstract": "Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance",
    "volume": "main",
    "checked": true,
    "id": "3b7d05fc6e1e0a622f9a8772f4557a166f811698",
    "citation_count": 101
  },
  "https://aclanthology.org/P19-1261": {
    "title": "Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension",
    "abstract": "Multi-hop reading comprehension requires the model to explore and connect relevant information from multiple sentences/documents in order to answer the question about the context. To achieve this, we propose an interpretable 3-module system called Explore-Propose-Assemble reader (EPAr). First, the Document Explorer iteratively selects relevant documents and represents divergent reasoning chains in a tree structure so as to allow assimilating information from all chains. The Answer Proposer then proposes an answer from every root-to-leaf path in the reasoning tree. Finally, the Evidence Assembler extracts a key sentence containing the proposed answer from every path and combines them to predict the final answer. Intuitively, EPAr approximates the coarse-to-fine-grained comprehension behavior of human readers when facing multiple long documents. We jointly optimize our 3 modules by minimizing the sum of losses from each stage conditioned on the previous stage’s output. On two multi-hop reading comprehension datasets WikiHop and MedHop, our EPAr model achieves significant improvements over the baseline and competitive results compared to the state-of-the-art model. We also present multiple reasoning-chain-recovery tests and ablation studies to demonstrate our system’s ability to perform interpretable and accurate reasoning",
    "volume": "main",
    "checked": true,
    "id": "451ca1dbbe935ddfb55f607b8d03b6ca932408cb",
    "citation_count": 36
  },
  "https://aclanthology.org/P19-1262": {
    "title": "Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development for Multi-Hop QA",
    "abstract": "Multi-hop question answering requires a model to connect multiple pieces of evidence scattered in a long context to answer the question. In this paper, we show that in the multi-hop HotpotQA (Yang et al., 2018) dataset, the examples often contain reasoning shortcuts through which models can directly locate the answer by word-matching the question with a sentence in the context. We demonstrate this issue by constructing adversarial documents that create contradicting answers to the shortcut but do not affect the validity of the original answer. The performance of strong baseline models drops significantly on our adversarial test, indicating that they are indeed exploiting the shortcuts rather than performing multi-hop reasoning. After adversarial training, the baseline’s performance improves but is still limited on the adversarial test. Hence, we use a control unit that dynamically attends to the question at different reasoning hops to guide the model’s multi-hop reasoning. We show that our 2-hop model trained on the regular data is more robust to the adversaries than the baseline. After adversarial training, it not only achieves significant improvements over its counterpart trained on regular data, but also outperforms the adversarially-trained baseline significantly. Finally, we sanity-check that these improvements are not obtained by exploiting potential new shortcuts in the adversarial data, but indeed due to robust multi-hop reasoning skills of the models",
    "volume": "main",
    "checked": true,
    "id": "0707f1e3791a6805bf4542605245cf4cdee3b9e0",
    "citation_count": 66
  },
  "https://aclanthology.org/P19-1263": {
    "title": "Exploiting Explicit Paths for Multi-hop Reading Comprehension",
    "abstract": "We propose a novel, path-based reasoning approach for the multi-hop reading comprehension task where a system needs to combine facts from multiple passages to answer a question. Although inspired by multi-hop reasoning over knowledge graphs, our proposed approach operates directly over unstructured text. It generates potential paths through passages and scores them without any direct path supervision. The proposed model, named PathNet, attempts to extract implicit relations from text through entity pair representations, and compose them to encode each path. To capture additional context, PathNet also composes the passage representations along each path to compute a passage-based representation. Unlike previous approaches, our model is then able to explain its reasoning via these explicit paths through the passages. We show that our approach outperforms prior models on the multi-hop Wikihop dataset, and also can be generalized to apply to the OpenBookQA dataset, matching state-of-the-art performance",
    "volume": "main",
    "checked": true,
    "id": "b1f0e5207f5b1605196c4d496376145a014169f0",
    "citation_count": 38
  },
  "https://aclanthology.org/P19-1264": {
    "title": "Sentence Mover's Similarity: Automatic Evaluation for Multi-Sentence Texts",
    "abstract": "For evaluating machine-generated texts, automatic methods hold the promise of avoiding collection of human judgments, which can be expensive and time-consuming. The most common automatic metrics, like BLEU and ROUGE, depend on exact word matching, an inflexible approach for measuring semantic similarity. We introduce methods based on sentence mover’s similarity; our automatic metrics evaluate text in a continuous space using word and sentence embeddings. We find that sentence-based metrics correlate with human judgments significantly better than ROUGE, both on machine-generated summaries (average length of 3.4 sentences) and human-authored essays (average length of 7.5). We also show that sentence mover’s similarity can be used as a reward when learning a generation model via reinforcement learning; we present both automatic and human evaluations of summaries learned in this way, finding that our approach outperforms ROUGE",
    "volume": "main",
    "checked": true,
    "id": "6ab3f185d9e4f0c8cfa0331810e15f5db09035c3",
    "citation_count": 106
  },
  "https://aclanthology.org/P19-1265": {
    "title": "Analysis of Automatic Annotation Suggestions for Hard Discourse-Level Tasks in Expert Domains",
    "abstract": "Many complex discourse-level tasks can aid domain experts in their work but require costly expert annotations for data creation. To speed up and ease annotations, we investigate the viability of automatically generated annotation suggestions for such tasks. As an example, we choose a task that is particularly hard for both humans and machines: the segmentation and classification of epistemic activities in diagnostic reasoning texts. We create and publish a new dataset covering two domains and carefully analyse the suggested annotations. We find that suggestions have positive effects on annotation speed and performance, while not introducing noteworthy biases. Envisioning suggestion models that improve with newly annotated texts, we contrast methods for continuous model adjustment and suggest the most effective setup for suggestions in future expert tasks",
    "volume": "main",
    "checked": true,
    "id": "66b32bdb57d67bc10cb11605b5ba9bad8741789f",
    "citation_count": 12
  },
  "https://aclanthology.org/P19-1266": {
    "title": "Deep Dominance - How to Properly Compare Deep Neural Models",
    "abstract": "Comparing between Deep Neural Network (DNN) models based on their performance on unseen data is crucial for the progress of the NLP field. However, these models have a large number of hyper-parameters and, being non-convex, their convergence point depends on the random values chosen at initialization and during training. Proper DNN comparison hence requires a comparison between their empirical score distributions on unseen data, rather than between single evaluation scores as is standard for more simple, convex models. In this paper, we propose to adapt to this problem a recently proposed test for the Almost Stochastic Dominance relation between two distributions. We define the criteria for a high quality comparison method between DNNs, and show, both theoretically and through analysis of extensive experimental results with leading DNN models for sequence tagging tasks, that the proposed test meets all criteria while previously proposed methods fail to do so. We hope the test we propose here will set a new working practice in the NLP community",
    "volume": "main",
    "checked": true,
    "id": "6a7769116c6733dffa347444b2835e50129e0143",
    "citation_count": 54
  },
  "https://aclanthology.org/P19-1267": {
    "title": "We Need to Talk about Standard Splits",
    "abstract": "It is standard practice in speech & language technology to rank systems according to their performance on a test set held out for evaluation. However, few researchers apply statistical tests to determine whether differences in performance are likely to arise by chance, and few examine the stability of system ranking across multiple training-testing splits. We conduct replication and reproduction experiments with nine part-of-speech taggers published between 2000 and 2018, each of which claimed state-of-the-art performance on a widely-used “standard split”. While we replicate results on the standard split, we fail to reliably reproduce some rankings when we repeat this analysis with randomly generated training-testing splits. We argue that randomly generated splits should be used in system evaluation",
    "volume": "main",
    "checked": true,
    "id": "94befec2a6d96e3a60fb8b77f2e161666743c1a5",
    "citation_count": 85
  },
  "https://aclanthology.org/P19-1268": {
    "title": "Aiming beyond the Obvious: Identifying Non-Obvious Cases in Semantic Similarity Datasets",
    "abstract": "Existing datasets for scoring text pairs in terms of semantic similarity contain instances whose resolution differs according to the degree of difficulty. This paper proposes to distinguish obvious from non-obvious text pairs based on superficial lexical overlap and ground-truth labels. We characterise existing datasets in terms of containing difficult cases and find that recently proposed models struggle to capture the non-obvious cases of semantic similarity. We describe metrics that emphasise cases of similarity which require more complex inference and propose that these are used for evaluating systems for semantic similarity",
    "volume": "main",
    "checked": true,
    "id": "bff61f928f488869907e48bb680ffcf7f78fb898",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-1269": {
    "title": "Putting Evaluation in Context: Contextual Embeddings Improve Machine Translation Evaluation",
    "abstract": "Accurate, automatic evaluation of machine translation is critical for system tuning, and evaluating progress in the field. We proposed a simple unsupervised metric, and additional supervised metrics which rely on contextual word embeddings to encode the translation and reference sentences. We find that these models rival or surpass all existing metrics in the WMT 2017 sentence-level and system-level tracks, and our trained model has a substantially higher correlation with human judgements than all existing metrics on the WMT 2017 to-English sentence level dataset",
    "volume": "main",
    "checked": true,
    "id": "dce91eb862d19a646b8f5171ec66e61a987f3b3c",
    "citation_count": 47
  },
  "https://aclanthology.org/P19-1270": {
    "title": "Joint Effects of Context and User History for Predicting Online Conversation Re-entries",
    "abstract": "As the online world continues its exponential growth, interpersonal communication has come to play an increasingly central role in opinion formation and change. In order to help users better engage with each other online, we study a challenging problem of re-entry prediction foreseeing whether a user will come back to a conversation they once participated in. We hypothesize that both the context of the ongoing conversations and the users’ previous chatting history will affect their continued interests in future engagement. Specifically, we propose a neural framework with three main layers, each modeling context, user history, and interactions between them, to explore how the conversation context and user chatting history jointly result in their re-entry behavior. We experiment with two large-scale datasets collected from Twitter and Reddit. Results show that our proposed framework with bi-attention achieves an F1 score of 61.1 on Twitter conversations, outperforming the state-of-the-art methods from previous work",
    "volume": "main",
    "checked": true,
    "id": "03a8e18d60ce93adf3dd3ebbe5e74d88601cb78b",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-1271": {
    "title": "CONAN - COunter NArratives through Nichesourcing: a Multilingual Dataset of Responses to Fight Online Hate Speech",
    "abstract": "Although there is an unprecedented effort to provide adequate responses in terms of laws and policies to hate content on social media platforms, dealing with hatred online is still a tough problem. Tackling hate speech in the standard way of content deletion or user suspension may be charged with censorship and overblocking. One alternate strategy, that has received little attention so far by the research community, is to actually oppose hate content with counter-narratives (i.e. informed textual responses). In this paper, we describe the creation of the first large-scale, multilingual, expert-based dataset of hate-speech/counter-narrative pairs. This dataset has been built with the effort of more than 100 operators from three different NGOs that applied their training and expertise to the task. Together with the collected data we also provide additional annotations about expert demographics, hate and response type, and data augmentation through translation and paraphrasing. Finally, we provide initial experiments to assess the quality of our data",
    "volume": "main",
    "checked": true,
    "id": "1dae97251a05320f5749355baa50387607318832",
    "citation_count": 96
  },
  "https://aclanthology.org/P19-1272": {
    "title": "Categorizing and Inferring the Relationship between the Text and Image of Twitter Posts",
    "abstract": "Text in social media posts is frequently accompanied by images in order to provide content, supply context, or to express feelings. This paper studies how the meaning of the entire tweet is composed through the relationship between its textual content and its image. We build and release a data set of image tweets annotated with four classes which express whether the text or the image provides additional information to the other modality. We show that by combining the text and image information, we can build a machine learning approach that accurately distinguishes between the relationship types. Further, we derive insights into how these relationships are materialized through text and image content analysis and how they are impacted by user demographic traits. These methods can be used in several downstream applications including pre-training image tagging models, collecting distantly supervised data for image captioning, and can be directly used in end-user applications to optimize screen estate",
    "volume": "main",
    "checked": true,
    "id": "54d92570201f72826e2a4bcc5b2a486e0b84049e",
    "citation_count": 28
  },
  "https://aclanthology.org/P19-1273": {
    "title": "Who Sides with Whom? Towards Computational Construction of Discourse Networks for Political Debates",
    "abstract": "Understanding the structures of political debates (which actors make what claims) is essential for understanding democratic political decision making. The vision of computational construction of such discourse networks from newspaper reports brings together political science and natural language processing. This paper presents three contributions towards this goal: (a) a requirements analysis, linking the task to knowledge base population; (b) an annotated pilot corpus of migration claims based on German newspaper reports; (c) initial modeling results",
    "volume": "main",
    "checked": true,
    "id": "e25c40a5cf137787bc995e646d5c4a1bb3375e49",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1274": {
    "title": "Analyzing Linguistic Differences between Owner and Staff Attributed Tweets",
    "abstract": "Research on social media has to date assumed that all posts from an account are authored by the same person. In this study, we challenge this assumption and study the linguistic differences between posts signed by the account owner or attributed to their staff. We introduce a novel data set of tweets posted by U.S. politicians who self-reported their tweets using a signature. We analyze the linguistic topics and style features that distinguish the two types of tweets. Predictive results show that we are able to predict owner and staff attributed tweets with good accuracy, even when not using any training data from that account",
    "volume": "main",
    "checked": true,
    "id": "7109362150c55836317155249a69ec3929e0c4ce",
    "citation_count": 3
  },
  "https://aclanthology.org/P19-1275": {
    "title": "Exploring Author Context for Detecting Intended vs Perceived Sarcasm",
    "abstract": "We investigate the impact of using author context on textual sarcasm detection. We define author context as the embedded representation of their historical posts on Twitter and suggest neural models that extract these representations. We experiment with two tweet datasets, one labelled manually for sarcasm, and the other via tag-based distant supervision. We achieve state-of-the-art performance on the second dataset, but not on the one labelled manually, indicating a difference between intended sarcasm, captured by distant supervision, and perceived sarcasm, captured by manual labelling",
    "volume": "main",
    "checked": true,
    "id": "932b3f3d814769a2eb4c6850805b2cc66961577b",
    "citation_count": 34
  },
  "https://aclanthology.org/P19-1276": {
    "title": "Open Domain Event Extraction Using Neural Latent Variable Models",
    "abstract": "We consider open domain event extraction, the task of extracting unconstraint types of events from news clusters. A novel latent variable neural model is constructed, which is scalable to very large corpus. A dataset is collected and manually annotated, with task-specific evaluation metrics being designed. Results show that the proposed unsupervised model gives better performance compared to the state-of-the-art method for event schema induction",
    "volume": "main",
    "checked": true,
    "id": "cf39ac1a08eee1099a9691c8f06808d5eef3dae8",
    "citation_count": 35
  },
  "https://aclanthology.org/P19-1277": {
    "title": "Multi-Level Matching and Aggregation Network for Few-Shot Relation Classification",
    "abstract": "This paper presents a multi-level matching and aggregation network (MLMAN) for few-shot relation classification. Previous studies on this topic adopt prototypical networks, which calculate the embedding vector of a query instance and the prototype vector of the support set for each relation candidate independently. On the contrary, our proposed MLMAN model encodes the query instance and each support set in an interactive way by considering their matching information at both local and instance levels. The final class prototype for each support set is obtained by attentive aggregation over the representations of support instances, where the weights are calculated using the query instance. Experimental results demonstrate the effectiveness of our proposed methods, which achieve a new state-of-the-art performance on the FewRel dataset",
    "volume": "main",
    "checked": true,
    "id": "c757e30c0abbfd5f11b1af1e109224c37819c1d6",
    "citation_count": 79
  },
  "https://aclanthology.org/P19-1278": {
    "title": "Quantifying Similarity between Relations with Fact Distribution",
    "abstract": "We introduce a conceptually simple and effective method to quantify the similarity between relations in knowledge bases. Specifically, our approach is based on the divergence between the conditional probability distributions over entity pairs. In this paper, these distributions are parameterized by a very simple neural network. Although computing the exact similarity is in-tractable, we provide a sampling-based method to get a good approximation. We empirically show the outputs of our approach significantly correlate with human judgments. By applying our method to various tasks, we also find that (1) our approach could effectively detect redundant relations extracted by open information extraction (Open IE) models, that (2) even the most competitive models for relational classification still make mistakes among very similar relations, and that (3) our approach could be incorporated into negative sampling and softmax classification to alleviate these mistakes",
    "volume": "main",
    "checked": true,
    "id": "4cf633d0893a1d3af97723ce1f2fae33c2a30043",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-1279": {
    "title": "Matching the Blanks: Distributional Similarity for Relation Learning",
    "abstract": "General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris’ distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task’s training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED",
    "volume": "main",
    "checked": true,
    "id": "4af09143735210777281b66997ec12994dbb43d4",
    "citation_count": 462
  },
  "https://aclanthology.org/P19-1280": {
    "title": "Fine-Grained Temporal Relation Extraction",
    "abstract": "We present a novel semantic framework for modeling temporal relations and event durations that maps pairs of events to real-valued scales. We use this framework to construct the largest temporal relations dataset to date, covering the entirety of the Universal Dependencies English Web Treebank. We use this dataset to train models for jointly predicting fine-grained temporal relations and event durations. We report strong results on our data and show the efficacy of a transfer-learning approach for predicting categorical relations",
    "volume": "main",
    "checked": true,
    "id": "1c3e6aeacb4d30c4e94ad3cd981fa5f38f1a5a79",
    "citation_count": 41
  },
  "https://aclanthology.org/P19-1281": {
    "title": "FIESTA: Fast IdEntification of State-of-The-Art models using adaptive bandit algorithms",
    "abstract": "We present FIESTA, a model selection approach that significantly reduces the computational resources required to reliably identify state-of-the-art performance from large collections of candidate models. Despite being known to produce unreliable comparisons, it is still common practice to compare model evaluations based on single choices of random seeds. We show that reliable model selection also requires evaluations based on multiple train-test splits (contrary to common practice in many shared tasks). Using bandit theory from the statistics literature, we are able to adaptively determine appropriate numbers of data splits and random seeds used to evaluate each model, focusing computational resources on the evaluation of promising models whilst avoiding wasting evaluations on models with lower performance. Furthermore, our user-friendly Python implementation produces confidence guarantees of correctly selecting the optimal model. We evaluate our algorithms by selecting between 8 target-dependent sentiment analysis methods using dramatically fewer model evaluations than current model selection approaches",
    "volume": "main",
    "checked": true,
    "id": "043c33b344fa19e04e0818788056d11d3ea739b1",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1282": {
    "title": "Is Attention Interpretable?",
    "abstract": "Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components’ representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components’ overall importance to a model, it is by no means a fail-safe indicator",
    "volume": "main",
    "checked": true,
    "id": "135112c7ba1762d65f39b1a61777f26ae4dfd8ad",
    "citation_count": 336
  },
  "https://aclanthology.org/P19-1283": {
    "title": "Correlating Neural and Symbolic Representations of Language",
    "abstract": "Analysis methods which enable us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach in NLP. Here we present two methods based on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which allow us to directly quantify how strongly the information encoded in neural activation patterns corresponds to information represented by symbolic structures such as syntax trees. We first validate our methods on the case of a simple synthetic language for arithmetic expressions with clearly defined syntax and semantics, and show that they exhibit the expected pattern of results. We then our methods to correlate neural representations of English sentences with their constituency parse trees",
    "volume": "main",
    "checked": true,
    "id": "59cfae5186900e8021ea31a6d3ce4f595f316ee5",
    "citation_count": 54
  },
  "https://aclanthology.org/P19-1284": {
    "title": "Interpretable Neural Predictions with Differentiable Binary Variables",
    "abstract": "The success of neural networks comes hand in hand with a desire for more interpretability. We focus on text classifiers and make them more interpretable by having them provide a justification–a rationale–for their predictions. We approach this problem by jointly training two neural network models: a latent model that selects a rationale (i.e. a short and informative part of the input text), and a classifier that learns from the words in the rationale alone. Previous work proposed to assign binary latent masks to input positions and to promote short selections via sparsity-inducing penalties such as L0 regularisation. We propose a latent model that mixes discrete and continuous behaviour allowing at the same time for binary selections and gradient-based training without REINFORCE. In our formulation, we can tractably compute the expected value of penalties such as L0, which allows us to directly optimise the model towards a pre-specified text selection rate. We show that our approach is competitive with previous work on rationale extraction, and explore further uses in attention mechanisms",
    "volume": "main",
    "checked": true,
    "id": "8c5465eb110d0cab951ca6858a0d51ae759d2f9c",
    "citation_count": 138
  },
  "https://aclanthology.org/P19-1285": {
    "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
    "abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch",
    "volume": "main",
    "checked": true,
    "id": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
    "citation_count": 2218
  },
  "https://aclanthology.org/P19-1286": {
    "title": "Domain Adaptation of Neural Machine Translation by Lexicon Induction",
    "abstract": "It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of NMT, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines",
    "volume": "main",
    "checked": true,
    "id": "fc5d79301a0876201c95954a764ec374b8eb236e",
    "citation_count": 48
  },
  "https://aclanthology.org/P19-1287": {
    "title": "Reference Network for Neural Machine Translation",
    "abstract": "Neural Machine Translation (NMT) has achieved notable success in recent years. Such a framework usually generates translations in isolation. In contrast, human translators often refer to reference data, either rephrasing the intricate sentence fragments with common terms in source language, or just accessing to the golden translation directly. In this paper, we propose a Reference Network to incorporate referring process into translation decoding of NMT. To construct a reference book, an intuitive way is to store the detailed translation history with extra memory, which is computationally expensive. Instead, we employ Local Coordinates Coding (LCC) to obtain global context vectors containing monolingual and bilingual contextual information for NMT decoding. Experimental results on Chinese-English and English-German tasks demonstrate that our proposed model is effective in improving the translation quality with lightweight computation cost",
    "volume": "main",
    "checked": true,
    "id": "a78c31494521dac965ca2f43e644ab1490121781",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-1288": {
    "title": "Retrieving Sequential Information for Non-Autoregressive Neural Machine Translation",
    "abstract": "Non-Autoregressive Transformer (NAT) aims to accelerate the Transformer model through discarding the autoregressive mechanism and generating target words independently, which fails to exploit the target sequential information. Over-translation and under-translation errors often occur for the above reason, especially in the long sentence translation scenario. In this paper, we propose two approaches to retrieve the target sequential information for NAT to enhance its translation ability while preserving the fast-decoding property. Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure. Secondly, we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder. Experimental results on three translation tasks show that the Reinforce-NAT surpasses the baseline NAT system by a significant margin on BLEU without decelerating the decoding speed and the FS-decoder achieves comparable translation performance to the autoregressive Transformer with considerable speedup",
    "volume": "main",
    "checked": true,
    "id": "2b68036936596af716d529a8752a5b42edbf7251",
    "citation_count": 33
  },
  "https://aclanthology.org/P19-1289": {
    "title": "STACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework",
    "abstract": "Simultaneous translation, which translates sentences before they are finished, is use- ful in many scenarios but is notoriously dif- ficult due to word-order differences. While the conventional seq-to-seq framework is only suitable for full-sentence translation, we pro- pose a novel prefix-to-prefix framework for si- multaneous translation that implicitly learns to anticipate in a single translation model. Within this framework, we present a very sim- ple yet surprisingly effective “wait-k” policy trained to generate the target sentence concur- rently with the source sentence, but always k words behind. Experiments show our strat- egy achieves low latency and reasonable qual- ity (compared to full-sentence translation) on 4 directions: zh↔en and de↔en",
    "volume": "main",
    "checked": true,
    "id": "9d3480e46cc506b73d5291387c6452998690fdd3",
    "citation_count": 167
  },
  "https://aclanthology.org/P19-1290": {
    "title": "Look Harder: A Neural Machine Translation Model with Hard Attention",
    "abstract": "Soft-attention based Neural Machine Translation (NMT) models have achieved promising results on several translation tasks. These models attend all the words in the source sequence for each target token, which makes them ineffective for long sequence translation. In this work, we propose a hard-attention based NMT model which selects a subset of source tokens for each target token to effectively handle long sequence translation. Due to the discrete nature of the hard-attention mechanism, we design a reinforcement learning algorithm coupled with reward shaping strategy to efficiently train it. Experimental results show that the proposed model performs better on long sequences and thereby achieves significant BLEU score improvement on English-German (EN-DE) and English-French (ENFR) translation tasks compared to the soft attention based NMT",
    "volume": "main",
    "checked": true,
    "id": "7d67237398986a6088c696df0bf57646c714508f",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-1291": {
    "title": "Robust Neural Machine Translation with Joint Textual and Phonetic Embedding",
    "abstract": "Neural machine translation (NMT) is notoriously sensitive to noises, but noises are almost inevitable in practice. One special kind of noise is the homophone noise, where words are replaced by other words with similar pronunciations. We propose to improve the robustness of NMT to homophone noises by 1) jointly embedding both textual and phonetic information of source sentences, and 2) augmenting the training dataset with homophone noises. Interestingly, to achieve better translation quality and more robustness, we found that most (though not all) weights should be put on the phonetic rather than textual information. Experiments show that our method not only significantly improves the robustness of NMT to homophone noises, but also surprisingly improves the translation quality on some clean test sets",
    "volume": "main",
    "checked": true,
    "id": "f53728f525d8b2eaa2ba21ff65ec98ece27f39d5",
    "citation_count": 47
  },
  "https://aclanthology.org/P19-1292": {
    "title": "A Simple and Effective Approach to Automatic Post-Editing with Transfer Learning",
    "abstract": "Automatic post-editing (APE) seeks to automatically refine the output of a black-box machine translation (MT) system through human post-edits. APE systems are usually trained by complementing human post-edited data with large, artificial data generated through back-translations, a time-consuming process often no easier than training a MT system from scratch. in this paper, we propose an alternative where we fine-tune pre-trained BERT models on both the encoder and decoder of an APE system, exploring several parameter sharing strategies. By only training on a dataset of 23K sentences for 3 hours on a single GPU we obtain results that are competitive with systems that were trained on 5M artificial sentences. When we add this artificial data our method obtains state-of-the-art results",
    "volume": "main",
    "checked": true,
    "id": "da26c653c89631b43eaff424a68dab59e9e5a8c5",
    "citation_count": 17
  },
  "https://aclanthology.org/P19-1293": {
    "title": "Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation",
    "abstract": "Given a rough, word-by-word gloss of a source language sentence, target language natives can uncover the latent, fully-fluent rendering of the translation. In this work we explore this intuition by breaking translation into a two step process: generating a rough gloss by means of a dictionary and then ‘translating’ the resulting pseudo-translation, or ‘Translationese’ into a fully fluent translation. We build our Translationese decoder once from a mish-mash of parallel data that has the target language in common and then can build dictionaries on demand using unsupervised techniques, resulting in rapidly generated unsupervised neural MT systems for many source languages. We apply this process to 14 test languages, obtaining better or comparable translation results on high-resource languages than previously published unsupervised MT studies, and obtaining good quality results for low-resource languages that have never been used in an unsupervised MT scenario",
    "volume": "main",
    "checked": true,
    "id": "760ac358f722c7bce41e57a9cee20bb433b9fdb9",
    "citation_count": 17
  },
  "https://aclanthology.org/P19-1294": {
    "title": "Training Neural Machine Translation to Apply Terminology Constraints",
    "abstract": "This paper proposes a novel method to inject custom terminology into neural machine translation at run time. Previous works have mainly proposed modifications to the decoding algorithm in order to constrain the output to include run-time-provided target terms. While being effective, these constrained decoding methods add, however, significant computational overhead to the inference step, and, as we show in this paper, can be brittle when tested in realistic conditions. In this paper we approach the problem by training a neural MT system to learn how to use custom terminology when provided with the input. Comparative experiments show that our method is not only more effective than a state-of-the-art implementation of constrained decoding, but is also as fast as constraint-free decoding",
    "volume": "main",
    "checked": true,
    "id": "9aa7b9c9c70ed68d6a345795c5249caf9b5d5fb7",
    "citation_count": 90
  },
  "https://aclanthology.org/P19-1295": {
    "title": "Leveraging Local and Global Patterns for Self-Attention Networks",
    "abstract": "Self-attention networks have received increasing research attention. By default, the hidden states of each word are hierarchically calculated by attending to all words in the sentence, which assembles global information. However, several studies pointed out that taking all signals into account may lead to overlooking neighboring information (e.g. phrase pattern). To address this argument, we propose a hybrid attention mechanism to dynamically leverage both of the local and global information. Specifically, our approach uses a gating scalar for integrating both sources of the information, which is also convenient for quantifying their contributions. Experiments on various neural machine translation tasks demonstrate the effectiveness of the proposed method. The extensive analyses verify that the two types of contexts are complementary to each other, and our method gives highly effective improvements in their integration",
    "volume": "main",
    "checked": true,
    "id": "2353f65b42e9f445425568088c5adef300a7f573",
    "citation_count": 32
  },
  "https://aclanthology.org/P19-1296": {
    "title": "Sentence-Level Agreement for Neural Machine Translation",
    "abstract": "The training objective of neural machine translation (NMT) is to minimize the loss between the words in the translated sentences and those in the references. In NMT, there is a natural correspondence between the source sentence and the target sentence. However, this relationship has only been represented using the entire neural network and the training objective is computed in word-level. In this paper, we propose a sentence-level agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance",
    "volume": "main",
    "checked": true,
    "id": "dfac457f4f688e9759a6e12acf96ef4b20e18c3d",
    "citation_count": 13
  },
  "https://aclanthology.org/P19-1297": {
    "title": "Multilingual Unsupervised NMT using Shared Encoder and Language-Specific Decoders",
    "abstract": "In this paper, we propose a multilingual unsupervised NMT scheme which jointly trains multiple languages with a shared encoder and multiple decoders. Our approach is based on denoising autoencoding of each language and back-translating between English and multiple non-English languages. This results in a universal encoder which can encode any language participating in training into an inter-lingual representation, and language-specific decoders. Our experiments using only monolingual corpora show that multilingual unsupervised model performs better than the separately trained bilingual models achieving improvement of up to 1.48 BLEU points on WMT test sets. We also observe that even if we do not train the network for all possible translation directions, the network is still able to translate in a many-to-many fashion leveraging encoder’s ability to generate interlingual representation",
    "volume": "main",
    "checked": true,
    "id": "2d2677460249bdcfad72bf3f95ca1d9a75dda502",
    "citation_count": 42
  },
  "https://aclanthology.org/P19-1298": {
    "title": "Lattice-Based Transformer Encoder for Neural Machine Translation",
    "abstract": "Neural machine translation (NMT) takes deterministic sequences for source representations. However, either word-level or subword-level segmentations have multiple choices to split a source sequence with different word segmentors or different subword vocabulary sizes. We hypothesize that the diversity in segmentations may affect the NMT performance. To integrate different segmentations with the state-of-the-art NMT model, Transformer, we propose lattice-based encoders to explore effective word or subword representation in an automatic way during training. We propose two methods: 1) lattice positional encoding and 2) lattice-aware self-attention. These two methods can be used together and show complementary to each other to further improve translation performance. Experiment results show superiorities of lattice-based encoders in word-level and subword-level representations over conventional Transformer encoder",
    "volume": "main",
    "checked": true,
    "id": "0ab0fda8774c303be8f8f8c8f684a890dcf5d455",
    "citation_count": 37
  },
  "https://aclanthology.org/P19-1299": {
    "title": "Multi-Source Cross-Lingual Model Transfer: Learning What to Share",
    "abstract": "Modern NLP applications have enjoyed a great boost utilizing neural networks models. Such deep neural models, however, are not applicable to most human languages due to the lack of annotated training data for various NLP tasks. Cross-lingual transfer learning (CLTL) is a viable method for building NLP models for a low-resource target language by leveraging labeled data from other (source) languages. In this work, we focus on the multilingual transfer setting where training data in multiple source languages is leveraged to further boost target language performance. Unlike most existing methods that rely only on language-invariant features for CLTL, our approach coherently utilizes both language-invariant and language-specific features at instance level. Our model leverages adversarial networks to learn language-invariant features, and mixture-of-experts models to dynamically exploit the similarity between the target language and each individual source language. This enables our model to learn effectively what to share between various languages in the multilingual setup. Moreover, when coupled with unsupervised multilingual embeddings, our model can operate in a zero-resource setting where neither target language training data nor cross-lingual resources are available. Our model achieves significant performance gains over prior art, as shown in an extensive set of experiments over multiple text classification and sequence tagging tasks including a large-scale industry dataset",
    "volume": "main",
    "checked": true,
    "id": "6ae057d72d608fe37f6c72f97750cdd7503bbd6b",
    "citation_count": 90
  },
  "https://aclanthology.org/P19-1300": {
    "title": "Unsupervised Multilingual Word Embedding with Limited Resources using Neural Language Models",
    "abstract": "Recently, a variety of unsupervised methods have been proposed that map pre-trained word embeddings of different languages into the same space without any parallel data. These methods aim to find a linear transformation based on the assumption that monolingual word embeddings are approximately isomorphic between languages. However, it has been demonstrated that this assumption holds true only on specific conditions, and with limited resources, the performance of these methods decreases drastically. To overcome this problem, we propose a new unsupervised multilingual embedding method that does not rely on such assumption and performs well under resource-poor scenarios, namely when only a small amount of monolingual data (i.e., 50k sentences) are available, or when the domains of monolingual data are different across languages. Our proposed model, which we call ‘Multilingual Neural Language Models’, shares some of the network parameters among multiple languages, and encodes sentences of multiple languages into the same space. The model jointly learns word embeddings of different languages in the same space, and generates multilingual embeddings without any parallel data or pre-training. Our experiments on word alignment tasks have demonstrated that, on the low-resource condition, our model substantially outperforms existing unsupervised and even supervised methods trained with 500 bilingual pairs of words. Our model also outperforms unsupervised methods given different-domain corpora across languages. Our code is publicly available",
    "volume": "main",
    "checked": true,
    "id": "cf4d68ddf6d288f23cf1b410b9f53da1adfc81b0",
    "citation_count": 16
  },
  "https://aclanthology.org/P19-1301": {
    "title": "Choosing Transfer Languages for Cross-Lingual Learning",
    "abstract": "Cross-lingual transfer, where a high-resource transfer language is used to improve the accuracy of a low-resource task language, is now an invaluable tool for improving performance of natural language processing (NLP) on low-resource languages. However, given a particular task language, it is not clear which language to transfer from, and the standard strategy is to select languages based on ad hoc criteria, usually the intuition of the experimenter. Since a large number of features contribute to the success of cross-lingual transfer (including phylogenetic similarity, typological properties, lexical overlap, or size of available data), even the most enlightened experimenter rarely considers all these factors for the particular task at hand. In this paper, we consider this task of automatically selecting optimal transfer languages as a ranking problem, and build models that consider the aforementioned features to perform this prediction. In experiments on representative NLP tasks, we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation, and glean insights on what features are most informative for each different NLP tasks, which may inform future ad hoc selection even without use of our method",
    "volume": "main",
    "checked": true,
    "id": "248824ec5d9b4ddf0c36cdc51b6b57af6e881328",
    "citation_count": 129
  },
  "https://aclanthology.org/P19-1302": {
    "title": "CogNet: A Large-Scale Cognate Database",
    "abstract": "This paper introduces CogNet, a new, large-scale lexical database that provides cognates -words of common origin and meaning- across languages. The database currently contains 3.1 million cognate pairs across 338 languages using 35 writing systems. The paper also describes the automated method by which cognates were computed from publicly available wordnets, with an accuracy evaluated to 94%. Finally, it presents statistics about the cognate data and some initial insights into it, hinting at a possible future exploitation of the resource by various fields of lingustics",
    "volume": "main",
    "checked": true,
    "id": "5b3c4e9305d392e0f160212cc288611776f9e3a4",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1303": {
    "title": "Neural Decipherment via Minimum-Cost Flow: From Ugaritic to Linear B",
    "abstract": "In this paper we propose a novel neural approach for automatic decipherment of lost languages. To compensate for the lack of strong supervision signal, our model design is informed by patterns in language change documented in historical linguistics. The model utilizes an expressive sequence-to-sequence model to capture character-level correspondences between cognates. To effectively train the model in unsupervised manner, we innovate the training procedure by formalizing it as a minimum-cost flow problem. When applied to decipherment of Ugaritic, we achieve 5% absolute improvement over state-of-the-art results. We also report first automatic results in deciphering Linear B, a syllabic language related to ancient Greek, where our model correctly translates 67.3% of cognates",
    "volume": "main",
    "checked": true,
    "id": "4d8c5370cf3f49d1afc5392928e4bf24aa73ea22",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-1304": {
    "title": "Cross-lingual Knowledge Graph Alignment via Graph Matching Neural Network",
    "abstract": "Previous cross-lingual knowledge graph (KG) alignment studies rely on entity embeddings derived only from monolingual KG structural information, which may fail at matching entities that have different facts in two KGs. In this paper, we introduce the topic entity graph, a local sub-graph of an entity, to represent entities with their contextual information in KG. From this view, the KB-alignment task can be formulated as a graph matching problem; and we further propose a graph-attention based solution, which first matches all entities in two topic entity graphs, and then jointly model the local matching information to derive a graph-level matching vector. Experiments show that our model outperforms previous state-of-the-art methods by a large margin",
    "volume": "main",
    "checked": true,
    "id": "a48bf0c4239c7db0d36bed98cd4f062f521fe01f",
    "citation_count": 149
  },
  "https://aclanthology.org/P19-1305": {
    "title": "Zero-Shot Cross-Lingual Abstractive Sentence Summarization through Teaching Generation and Attention",
    "abstract": "Abstractive Sentence Summarization (ASSUM) targets at grasping the core idea of the source sentence and presenting it as the summary. It is extensively studied using statistical models or neural models based on the large-scale monolingual source-summary parallel corpus. But there is no cross-lingual parallel corpus, whose source sentence language is different to the summary language, to directly train a cross-lingual ASSUM system. We propose to solve this zero-shot problem by using resource-rich monolingual ASSUM system to teach zero-shot cross-lingual ASSUM system on both summary word generation and attention. This teaching process is along with a back-translation process which simulates source-summary pairs. Experiments on cross-lingual ASSUM task show that our proposed method is significantly better than pipeline baselines and previous works, and greatly enhances the cross-lingual performances closer to the monolingual performances",
    "volume": "main",
    "checked": true,
    "id": "801168a3f084b385d418b38fc9d877bc57cc545a",
    "citation_count": 48
  },
  "https://aclanthology.org/P19-1306": {
    "title": "Improving Low-Resource Cross-lingual Document Retrieval by Reranking with Deep Bilingual Representations",
    "abstract": "In this paper, we propose to boost low-resource cross-lingual document retrieval performance with deep bilingual query-document representations. We match queries and documents in both source and target languages with four components, each of which is implemented as a term interaction-based deep neural network with cross-lingual word embeddings as input. By including query likelihood scores as extra features, our model effectively learns to rerank the retrieved documents by using a small number of relevance labels for low-resource language pairs. Due to the shared cross-lingual word embedding space, the model can also be directly applied to another language pair without any training label. Experimental results on the Material dataset show that our model outperforms the competitive translation-based baselines on English-Swahili, English-Tagalog, and English-Somali cross-lingual information retrieval tasks",
    "volume": "main",
    "checked": true,
    "id": "6aa2f7ef193856f97d26454a95e2583a3452abbe",
    "citation_count": 14
  },
  "https://aclanthology.org/P19-1307": {
    "title": "Are Girls Neko or Shōjo? Cross-Lingual Alignment of Non-Isomorphic Embeddings with Iterative Normalization",
    "abstract": "Cross-lingual word embeddings (CLWE) underlie many multilingual natural language processing systems, often through orthogonal transformations of pre-trained monolingual embeddings. However, orthogonal mapping only works on language pairs whose embeddings are naturally isomorphic. For non-isomorphic pairs, our method (Iterative Normalization) transforms monolingual embeddings to make orthogonal alignment easier by simultaneously enforcing that (1) individual word vectors are unit length, and (2) each language’s average vector is zero. Iterative Normalization consistently improves word translation accuracy of three CLWE methods, with the largest improvement observed on English-Japanese (from 2% to 44% test accuracy)",
    "volume": "main",
    "checked": true,
    "id": "f9ac14c9ac5ca413152f8b637986efc70a95faf6",
    "citation_count": 39
  },
  "https://aclanthology.org/P19-1308": {
    "title": "MAAM: A Morphology-Aware Alignment Model for Unsupervised Bilingual Lexicon Induction",
    "abstract": "The task of unsupervised bilingual lexicon induction (UBLI) aims to induce word translations from monolingual corpora in two languages. Previous work has shown that morphological variation is an intractable challenge for the UBLI task, where the induced translation in failure case is usually morphologically related to the correct translation. To tackle this challenge, we propose a morphology-aware alignment model for the UBLI task. The proposed model aims to alleviate the adverse effect of morphological variation by introducing grammatical information learned by the pre-trained denoising language model. Results show that our approach can substantially outperform several state-of-the-art unsupervised systems, and even achieves competitive performance compared to supervised methods",
    "volume": "main",
    "checked": true,
    "id": "3a0baafe548d11fd8dc08b0a0eab1ae0ef2f986f",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1309": {
    "title": "Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings",
    "abstract": "Machine translation is highly sensitive to the size and quality of the training data, which has led to an increasing interest in collecting and filtering large parallel corpora. In this paper, we propose a new method for this task based on multilingual sentence embeddings. In contrast to previous approaches, which rely on nearest neighbor retrieval with a hard threshold over cosine similarity, our proposed method accounts for the scale inconsistencies of this measure, considering the margin between a given sentence pair and its closest candidates instead. Our experiments show large improvements over existing methods. We outperform the best published results on the BUCC mining task and the UN reconstruction task by more than 10 F1 and 30 precision points, respectively. Filtering the English-German ParaCrawl corpus with our approach, we obtain 31.2 BLEU points on newstest2014, an improvement of more than one point over the best official filtered version",
    "volume": "main",
    "checked": true,
    "id": "30b09a853ab72e53078f1feefe6de5a847a2b169",
    "citation_count": 142
  },
  "https://aclanthology.org/P19-1310": {
    "title": "JW300: A Wide-Coverage Parallel Corpus for Low-Resource Languages",
    "abstract": "Viable cross-lingual transfer critically depends on the availability of parallel texts. Shortage of such resources imposes a development and evaluation bottleneck in multilingual processing. We introduce JW300, a parallel corpus of over 300 languages with around 100 thousand parallel sentences per language pair on average. In this paper, we present the resource and showcase its utility in experiments with cross-lingual word embedding induction and multi-source part-of-speech projection",
    "volume": "main",
    "checked": true,
    "id": "83ff258bdb027e38b75f3043b0789620c17b27f0",
    "citation_count": 99
  },
  "https://aclanthology.org/P19-1311": {
    "title": "Cross-Lingual Syntactic Transfer through Unsupervised Adaptation of Invertible Projections",
    "abstract": "Cross-lingual transfer is an effective way to build syntactic analysis tools in low-resource languages. However, transfer is difficult when transferring to typologically distant languages, especially when neither annotated target data nor parallel corpora are available. In this paper, we focus on methods for cross-lingual transfer to distant languages and propose to learn a generative model with a structured prior that utilizes labeled source data and unlabeled target data jointly. The parameters of source model and target model are softly shared through a regularized log likelihood objective. An invertible projection is employed to learn a new interlingual latent embedding space that compensates for imperfect cross-lingual word embedding input. We evaluate our method on two syntactic tasks: part-of-speech (POS) tagging and dependency parsing. On the Universal Dependency Treebanks, we use English as the only source corpus and transfer to a wide range of target languages. On the 10 languages in this dataset that are distant from English, our method yields an average of 5.2% absolute improvement on POS tagging and 8.3% absolute improvement on dependency parsing over a direct transfer method using state-of-the-art discriminative models",
    "volume": "main",
    "checked": true,
    "id": "6b7f2f30840b0d72484784a15b3be670868a9f95",
    "citation_count": 17
  },
  "https://aclanthology.org/P19-1312": {
    "title": "Unsupervised Joint Training of Bilingual Word Embeddings",
    "abstract": "State-of-the-art methods for unsupervised bilingual word embeddings (BWE) train a mapping function that maps pre-trained monolingual word embeddings into a bilingual space. Despite its remarkable results, unsupervised mapping is also well-known to be limited by the original dissimilarity between the word embedding spaces to be mapped. In this work, we propose a new approach that trains unsupervised BWE jointly on synthetic parallel data generated through unsupervised machine translation. We demonstrate that existing algorithms that jointly train BWE are very robust to noisy training data and show that unsupervised BWE jointly trained significantly outperform unsupervised mapped BWE in several cross-lingual NLP tasks",
    "volume": "main",
    "checked": true,
    "id": "f6602ea4135a2d10b7ebf5d4684e7ebae42d4c1a",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1313": {
    "title": "Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings",
    "abstract": "We consider the task of inferring “is-a” relationships from large text corpora. For this purpose, we propose a new method combining hyperbolic embeddings and Hearst patterns. This approach allows us to set appropriate constraints for inferring concept hierarchies from distributional contexts while also being able to predict missing “is-a”-relationships and to correct wrong extractions. Moreover – and in contrast with other methods – the hierarchical nature of hyperbolic space allows us to learn highly efficient representations and to improve the taxonomic consistency of the inferred hierarchies. Experimentally, we show that our approach achieves state-of-the-art performance on several commonly-used benchmarks",
    "volume": "main",
    "checked": true,
    "id": "5d894d8b35dce6f562f1ead09e77665d1593a793",
    "citation_count": 53
  },
  "https://aclanthology.org/P19-1314": {
    "title": "Is Word Segmentation Necessary for Deep Learning of Chinese Representations?",
    "abstract": "Segmenting a chunk of text into words is usually the first step of processing Chinese text, but its necessity has rarely been explored. In this paper, we ask the fundamental question of whether Chinese word segmentation (CWS) is necessary for deep learning-based Chinese Natural Language Processing. We benchmark neural word-based models which rely on word segmentation against neural char-based models which do not involve word segmentation in four end-to-end NLP benchmark tasks: language modeling, machine translation, sentence matching/paraphrase and text classification. Through direct comparisons between these two types of models, we find that char-based models consistently outperform word-based models. Based on these observations, we conduct comprehensive experiments to study why word-based models underperform char-based models in these deep learning-based NLP tasks. We show that it is because word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (OOV) words, and thus more prone to overfitting. We hope this paper could encourage researchers in the community to rethink the necessity of word segmentation in deep learning-based Chinese Natural Language Processing",
    "volume": "main",
    "checked": true,
    "id": "ae43556f2cc1cecb8b48762b4e09df319fbaa4d9",
    "citation_count": 95
  },
  "https://aclanthology.org/P19-1315": {
    "title": "Towards Understanding Linear Word Analogies",
    "abstract": "A surprising property of word vectors is that word analogies can often be solved with vector arithmetic. However, it is unclear why arithmetic operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a formal explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. Our theory has several implications. Past work has conjectured that linear substructures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel justification for the addition of SGNS word vectors by showing that it automatically down-weights the more frequent word, as weighting schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, justifying its use in capturing word dissimilarity",
    "volume": "main",
    "checked": true,
    "id": "86267d9fc54d0c054821a7dffd25f945a822d062",
    "citation_count": 61
  },
  "https://aclanthology.org/P19-1316": {
    "title": "On the Compositionality Prediction of Noun Phrases using Poincaré Embeddings",
    "abstract": "The compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their grammatical relations. Prediction of (non)-compositionality is a task that has been frequently addressed with distributional semantic models. We introduce a novel technique to blend hierarchical information with distributional information for predicting compositionality. In particular, we use hypernymy information of the multiword and its constituents encoded in the form of the recently introduced Poincaré embeddings in addition to the distributional information to detect compositionality for noun phrases. Using a weighted average of the distributional similarity and a Poincaré similarity function, we obtain consistent and substantial, statistically significant improvement across three gold standard datasets over state-of-the-art models based on distributional information only. Unlike traditional approaches that solely use an unsupervised setting, we have also framed the problem as a supervised task, obtaining comparable improvements. Further, we publicly release our Poincaré embeddings, which are trained on the output of handcrafted lexical-syntactic patterns on a large corpus",
    "volume": "main",
    "checked": true,
    "id": "d5357e312f1478c9f448054ec505bf5c64722c32",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-1317": {
    "title": "Robust Representation Learning of Biomedical Names",
    "abstract": "Biomedical concepts are often mentioned in medical documents under different name variations (synonyms). This mismatch between surface forms is problematic, resulting in difficulties pertaining to learning effective representations. Consequently, this has tremendous implications such as rendering downstream applications inefficacious and/or potentially unreliable. This paper proposes a new framework for learning robust representations of biomedical names and terms. The idea behind our approach is to consider and encode contextual meaning, conceptual meaning, and the similarity between synonyms during the representation learning process. Via extensive experiments, we show that our proposed method outperforms other baselines on a battery of retrieval, similarity and relatedness benchmarks. Moreover, our proposed method is also able to compute meaningful representations for unseen names, resulting in high practical utility in real-world applications",
    "volume": "main",
    "checked": true,
    "id": "3ce46859506cfdde6a34379e343f8ca0c058303c",
    "citation_count": 22
  },
  "https://aclanthology.org/P19-1318": {
    "title": "Relational Word Embeddings",
    "abstract": "While word embeddings have been shown to implicitly encode various forms of attributional knowledge, the extent to which they capture relational information is far more limited. In previous work, this limitation has been addressed by incorporating relational knowledge from external knowledge bases when learning the word embedding. Such strategies may not be optimal, however, as they are limited by the coverage of available resources and conflate similarity with other forms of relatedness. As an alternative, in this paper we propose to encode relational knowledge in a separate word embedding, which is aimed to be complementary to a given standard word embedding. This relational word embedding is still learned from co-occurrence statistics, and can thus be used even when no external knowledge base is available. Our analysis shows that relational word vectors do indeed capture information that is complementary to what is encoded in standard word embeddings",
    "volume": "main",
    "checked": true,
    "id": "da610485bdbaf3d66dc0392c6a9d8e6c84d4b794",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1319": {
    "title": "Unraveling Antonym's Word Vectors through a Siamese-like Network",
    "abstract": "Discriminating antonyms and synonyms is an important NLP task that has the difficulty that both, antonyms and synonyms, contains similar distributional information. Consequently, pairs of antonyms and synonyms may have similar word vectors. We present an approach to unravel antonymy and synonymy from word vectors based on a siamese network inspired approach. The model consists of a two-phase training of the same base network: a pre-training phase according to a siamese model supervised by synonyms and a training phase on antonyms through a siamese-like model that supports the antitransitivity present in antonymy. The approach makes use of the claim that the antonyms in common of a word tend to be synonyms. We show that our approach outperforms distributional and pattern-based approaches, relaying on a simple feed forward network as base network of the training phases",
    "volume": "main",
    "checked": true,
    "id": "7adf67f66d5dfbeea018add80d48df5f74a931b3",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1320": {
    "title": "Incorporating Syntactic and Semantic Information in Word Embeddings using Graph Convolutional Networks",
    "abstract": "Word embeddings have been widely adopted across several NLP applications. Most existing word embedding methods utilize sequential context of a word to learn its embedding. While there have been some attempts at utilizing syntactic context of a word, such methods result in an explosion of the vocabulary size. In this paper, we overcome this problem by proposing SynGCN, a flexible Graph Convolution based method for learning word embeddings. SynGCN utilizes the dependency context of a word without increasing the vocabulary size. Word embeddings learned by SynGCN outperform existing methods on various intrinsic and extrinsic tasks and provide an advantage when used with ELMo. We also propose SemGCN, an effective framework for incorporating diverse semantic knowledge for further enhancing learned word representations. We make the source code of both models available to encourage reproducible research",
    "volume": "main",
    "checked": true,
    "id": "ab571a354f0847677862da027a69db9531eb08e8",
    "citation_count": 62
  },
  "https://aclanthology.org/P19-1321": {
    "title": "Word and Document Embedding with vMF-Mixture Priors on Context Word Vectors",
    "abstract": "Word embedding models typically learn two types of vectors: target word vectors and context word vectors. These vectors are normally learned such that they are predictive of some word co-occurrence statistic, but they are otherwise unconstrained. However, the words from a given language can be organized in various natural groupings, such as syntactic word classes (e.g. nouns, adjectives, verbs) and semantic themes (e.g. sports, politics, sentiment). Our hypothesis in this paper is that embedding models can be improved by explicitly imposing a cluster structure on the set of context word vectors. To this end, our model relies on the assumption that context word vectors are drawn from a mixture of von Mises-Fisher (vMF) distributions, where the parameters of this mixture distribution are jointly optimized with the word vectors. We show that this results in word vectors which are qualitatively different from those obtained with existing word embedding models. We furthermore show that our embedding model can also be used to learn high-quality document representations",
    "volume": "main",
    "checked": true,
    "id": "667d4bedc542f69f06812bb68335c8d559064d44",
    "citation_count": 4
  },
  "https://aclanthology.org/P19-1322": {
    "title": "Delta Embedding Learning",
    "abstract": "Unsupervised word embeddings have become a popular approach of word representation in NLP tasks. However there are limitations to the semantics represented by unsupervised embeddings, and inadequate fine-tuning of embeddings can lead to suboptimal performance. We propose a novel learning technique called Delta Embedding Learning, which can be applied to general NLP tasks to improve performance by optimized tuning of the word embeddings. A structured regularization is applied to the embeddings to ensure they are tuned in an incremental way. As a result, the tuned word embeddings become better word representations by absorbing semantic information from supervision without “forgetting.” We apply the method to various NLP tasks and see a consistent improvement in performance. Evaluation also confirms the tuned word embeddings have better semantic properties",
    "volume": "main",
    "checked": true,
    "id": "e74fc1f12110d4a07ede318b903b9a26483bfdb2",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-1323": {
    "title": "Annotation and Automatic Classification of Aspectual Categories",
    "abstract": "We present the first annotated resource for the aspectual classification of German verb tokens in their clausal context. We use aspectual features compatible with the plurality of aspectual classifications in previous work and treat aspectual ambiguity systematically. We evaluate our corpus by using it to train supervised classifiers to automatically assign aspectual categories to verbs in context, permitting favourable comparisons to previous work",
    "volume": "main",
    "checked": true,
    "id": "881ab56454b228c15f0e48c252ae910ed9ec3a02",
    "citation_count": 2
  },
  "https://aclanthology.org/P19-1324": {
    "title": "Putting Words in Context: LSTM Language Models and Lexical Ambiguity",
    "abstract": "In neural network models of language, words are commonly represented using context-invariant representations (word embeddings) which are then put in context in the hidden layers. Since words are often ambiguous, representing the contextually relevant information is not trivial. We investigate how an LSTM language model deals with lexical ambiguity in English, designing a method to probe its hidden representations for lexical and contextual information about words. We find that both types of information are represented to a large extent, but also that there is room for improvement for contextual information",
    "volume": "main",
    "checked": true,
    "id": "2785d56159d01ad050b8014410d370e8270f9c1b",
    "citation_count": 29
  },
  "https://aclanthology.org/P19-1325": {
    "title": "Making Fast Graph-based Algorithms with Graph Metric Embeddings",
    "abstract": "Graph measures, such as node distances, are inefficient to compute. We explore dense vector representations as an effective way to approximate the same information. We introduce a simple yet efficient and effective approach for learning graph embeddings. Instead of directly operating on the graph structure, our method takes structural measures of pairwise node similarities into account and learns dense node representations reflecting user-defined graph distance measures, such as e.g. the shortest path distance or distance measures that take information beyond the graph structure into account. We demonstrate a speed-up of several orders of magnitude when predicting word similarity by vector operations on our embeddings as opposed to directly computing the respective path-based measures, while outperforming various other graph embeddings on semantic similarity and word sense disambiguation tasks",
    "volume": "main",
    "checked": true,
    "id": "686c4cacaa5cd2e88bfc71f5dc8aa3caa4b39607",
    "citation_count": 2
  },
  "https://aclanthology.org/P19-1326": {
    "title": "Embedding Imputation with Grounded Language Information",
    "abstract": "Due to the ubiquitous use of embeddings as input representations for a wide range of natural language tasks, imputation of embeddings for rare and unseen words is a critical problem in language processing. Embedding imputation involves learning representations for rare or unseen words during the training of an embedding model, often in a post-hoc manner. In this paper, we propose an approach for embedding imputation which uses grounded information in the form of a knowledge graph. This is in contrast to existing approaches which typically make use of vector space properties or subword information. We propose an online method to construct a graph from grounded information and design an algorithm to map from the resulting graphical structure to the space of the pre-trained embeddings. Finally, we evaluate our approach on a range of rare and unseen word tasks across various domains and show that our model can learn better representations. For example, on the Card-660 task our method improves Pearson’s and Spearman’s correlation coefficients upon the state-of-the-art by 11% and 17.8% respectively using GloVe embeddings",
    "volume": "main",
    "checked": true,
    "id": "f87485f3d997d3667d559e70e66126ef6ec8b196",
    "citation_count": 3
  },
  "https://aclanthology.org/P19-1327": {
    "title": "The Effectiveness of Simple Hybrid Systems for Hypernym Discovery",
    "abstract": "Hypernymy modeling has largely been separated according to two paradigms, pattern-based methods and distributional methods. However, recent works utilizing a mix of these strategies have yielded state-of-the-art results. This paper evaluates the contribution of both paradigms to hybrid success by evaluating the benefits of hybrid treatment of baseline models from each paradigm. Even with a simple methodology for each individual system, utilizing a hybrid approach establishes new state-of-the-art results on two domain-specific English hypernym discovery tasks and outperforms all non-hybrid approaches in a general English hypernym discovery task",
    "volume": "main",
    "checked": true,
    "id": "a1a8f35c9552f5b6666dfb79b2b0f572d71e1143",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1328": {
    "title": "BERT-based Lexical Substitution",
    "abstract": "Previous studies on lexical substitution tend to obtain substitute candidates by finding the target word’s synonyms from lexical resources (e.g., WordNet) and then rank the candidates based on its contexts. These approaches have two limitations: (1) They are likely to overlook good substitute candidates that are not the synonyms of the target words in the lexical resources; (2) They fail to take into account the substitution’s influence on the global context of the sentence. To address these issues, we propose an end-to-end BERT-based lexical substitution approach which can propose and validate substitute candidates without using any annotated data or manually curated resources. Our approach first applies dropout to the target word’s embedding for partially masking the word, allowing BERT to take balanced consideration of the target word’s semantics and contexts for proposing substitute candidates, and then validates the candidates based on their substitution’s influence on the global contextualized representation of the sentence. Experiments show our approach performs well in both proposing and ranking substitute candidates, achieving the state-of-the-art results in both LS07 and LS14 benchmarks",
    "volume": "main",
    "checked": true,
    "id": "40448ec376e3bc6f706f51bfc30a4a4cc0e7b43b",
    "citation_count": 49
  },
  "https://aclanthology.org/P19-1329": {
    "title": "Exploring Numeracy in Word Embeddings",
    "abstract": "Word embeddings are now pervasive across NLP subfields as the de-facto method of forming text representataions. In this work, we show that existing embedding models are inadequate at constructing representations that capture salient aspects of mathematical meaning for numbers, which is important for language understanding. Numbers are ubiquitous and frequently appear in text. Inspired by cognitive studies on how humans perceive numbers, we develop an analysis framework to test how well word embeddings capture two essential properties of numbers: magnitude (e.g. 3<4) and numeration (e.g. 3=three). Our experiments reveal that most models capture an approximate notion of magnitude, but are inadequate at capturing numeration. We hope that our observations provide a starting point for the development of methods which better capture numeracy in NLP systems",
    "volume": "main",
    "checked": true,
    "id": "a5dde0a71fb0465d33b95496c4bb64914b3d62d9",
    "citation_count": 41
  },
  "https://aclanthology.org/P19-1330": {
    "title": "HighRES: Highlight-based Reference-less Evaluation of Summarization",
    "abstract": "There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches",
    "volume": "main",
    "checked": true,
    "id": "bc494b9c6d9602a69b76ab9ea0e95d348a2fce19",
    "citation_count": 31
  },
  "https://aclanthology.org/P19-1331": {
    "title": "EditNTS: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing",
    "abstract": "We present the first sentence simplification model that learns explicit edit operations (ADD, DELETE, and KEEP) via a neural programmer-interpreter approach. Most current neural sentence simplification systems are variants of sequence-to-sequence models adopted from machine translation. These methods learn to simplify sentences as a byproduct of the fact that they are trained on complex-simple sentence pairs. By contrast, our neural programmer-interpreter is directly trained to predict explicit edit operations on targeted parts of the input sentence, resembling the way that humans perform simplification and revision. Our model outperforms previous state-of-the-art neural sentence simplification models (without external knowledge) by large margins on three benchmark text simplification corpora in terms of SARI (+0.95 WikiLarge, +1.89 WikiSmall, +1.41 Newsela), and is judged by humans to produce overall better and simpler output sentences",
    "volume": "main",
    "checked": true,
    "id": "340b59e6ee93d30c055b5e89a7cfbc88874c9958",
    "citation_count": 102
  },
  "https://aclanthology.org/P19-1332": {
    "title": "Decomposable Neural Paraphrase Generation",
    "abstract": "Paraphrasing exists at different granularity levels, such as lexical level, phrasal level and sentential level. This paper presents Decomposable Neural Paraphrase Generator (DNPG), a Transformer-based model that can learn and generate paraphrases of a sentence at different levels of granularity in a disentangled way. Specifically, the model is composed of multiple encoders and decoders with different structures, each of which corresponds to a specific granularity. The empirical study shows that the decomposition mechanism of DNPG makes paraphrase generation more interpretable and controllable. Based on DNPG, we further develop an unsupervised domain adaptation method for paraphrase generation. Experimental results show that the proposed model achieves competitive in-domain performance compared to state-of-the-art neural models, and significantly better performance when adapting to a new domain",
    "volume": "main",
    "checked": true,
    "id": "7094a778ec76b965aca40c8ef691eaa6f762c949",
    "citation_count": 61
  },
  "https://aclanthology.org/P19-1333": {
    "title": "Transforming Complex Sentences into a Semantic Hierarchy",
    "abstract": "We present an approach for recursively splitting and rephrasing complex English sentences into a novel semantic hierarchy of simplified sentences, with each of them presenting a more regular structure that may facilitate a wide variety of artificial intelligence tasks, such as machine translation (MT) or information extraction (IE). Using a set of hand-crafted transformation rules, input sentences are recursively transformed into a two-layered hierarchical representation in the form of core sentences and accompanying contexts that are linked via rhetorical relations. In this way, the semantic relationship of the decomposed constituents is preserved in the output, maintaining its interpretability for downstream applications. Both a thorough manual analysis and automatic evaluation across three datasets from two different domains demonstrate that the proposed syntactic simplification approach outperforms the state of the art in structural text simplification. Moreover, an extrinsic evaluation shows that when applying our framework as a preprocessing step the performance of state-of-the-art Open IE systems can be improved by up to 346% in precision and 52% in recall. To enable reproducible research, all code is provided online",
    "volume": "main",
    "checked": true,
    "id": "f488493c6a87ac55d4639f7641dbf360582b91d0",
    "citation_count": 24
  },
  "https://aclanthology.org/P19-1334": {
    "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
    "abstract": "A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area",
    "volume": "main",
    "checked": true,
    "id": "42ed4a9994e6121a9f325f5b901c5b3d7ce104f5",
    "citation_count": 728
  },
  "https://aclanthology.org/P19-1335": {
    "title": "Zero-Shot Entity Linking by Reading Entity Descriptions",
    "abstract": "We present the zero-shot entity linking task, where mentions must be linked to unseen entities without in-domain labeled data. The goal is to enable robust transfer to highly specialized domains, and so no metadata or alias tables are assumed. In this setting, entities are only identified by text descriptions, and models must rely strictly on language understanding to resolve the new entities. First, we show that strong reading comprehension models pre-trained on large unlabeled data can be used to generalize to unseen entities. Second, we propose a simple and effective adaptive pre-training strategy, which we term domain-adaptive pre-training (DAP), to address the domain shift problem associated with linking unseen entities in a new domain. We present experiments on a new dataset that we construct for this task and show that DAP improves over strong pre-training baselines, including BERT. The data and code are available at https://github.com/lajanugen/zeshel",
    "volume": "main",
    "checked": true,
    "id": "45fe966219595e3a6d771c15f273efa171a9f53a",
    "citation_count": 147
  },
  "https://aclanthology.org/P19-1336": {
    "title": "Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition",
    "abstract": "We propose a new neural transfer method termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER). Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are investigated to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD). Additionally, adversarial training is adopted to boost model generalization. In experiments, we examine the effects of different components in DATNet across domains and languages and show that significant improvement can be obtained especially for low-resource data, without augmenting any additional hand-crafted features and pre-trained language model",
    "volume": "main",
    "checked": true,
    "id": "e3ca15ccbec5ee2309031d8d90cec4d87ae11327",
    "citation_count": 67
  },
  "https://aclanthology.org/P19-1337": {
    "title": "Scalable Syntax-Aware Language Models Using Knowledge Distillation",
    "abstract": "Prior work has shown that, on small amounts of training data, syntactic neural language models learn structurally sensitive generalisations more successfully than sequential language models. However, their computational complexity renders scaling difficult, and it remains an open question whether structural biases are still necessary when sequential models have access to ever larger amounts of training data. To answer this question, we introduce an efficient knowledge distillation (KD) technique that transfers knowledge from a syntactic language model trained on a small corpus to an LSTM language model, hence enabling the LSTM to develop a more structurally sensitive representation of the larger training data it learns from. On targeted syntactic evaluations, we find that, while sequential LSTMs perform much better than previously reported, our proposed technique substantially improves on this baseline, yielding a new state of the art. Our findings and analysis affirm the importance of structural biases, even in models that learn from large amounts of data",
    "volume": "main",
    "checked": true,
    "id": "babf55e17591ea977e3f88d46dfe757a9ae0fdf2",
    "citation_count": 22
  },
  "https://aclanthology.org/P19-1338": {
    "title": "An Imitation Learning Approach to Unsupervised Parsing",
    "abstract": "Recently, there has been an increasing interest in unsupervised parsers that optimize semantically oriented objectives, typically using reinforcement learning. Unfortunately, the learned trees often do not match actual syntax trees well. Shen et al. (2018) propose a structured attention mechanism for language modeling (PRPN), which induces better syntactic structures but relies on ad hoc heuristics. Also, their model lacks interpretability as it is not grounded in parsing actions. In our work, we propose an imitation learning approach to unsupervised parsing, where we transfer the syntactic knowledge induced by PRPN to a Tree-LSTM model with discrete parsing actions. Its policy is then refined by Gumbel-Softmax training towards a semantically oriented objective. We evaluate our approach on the All Natural Language Inference dataset and show that it achieves a new state of the art in terms of parsing F-score, outperforming our base models, including PRPN",
    "volume": "main",
    "checked": true,
    "id": "31212862430352b8434bfceb1d4de29867c7a491",
    "citation_count": 17
  },
  "https://aclanthology.org/P19-1339": {
    "title": "Women's Syntactic Resilience and Men's Grammatical Luck: Gender-Bias in Part-of-Speech Tagging and Dependency Parsing",
    "abstract": "Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for part-of-speech tagging and dependency parsing have still not adapted to account for these differences. To address this, we annotate the Wall Street Journal part of the Penn Treebank with the gender information of the articles’ authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. Further analyses reveal numerous part-of-speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data. The results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. We release our data to the research community",
    "volume": "main",
    "checked": true,
    "id": "e85a50b523915b5fba3e3f1fdb743650f7d21bed",
    "citation_count": 40
  },
  "https://aclanthology.org/P19-1340": {
    "title": "Multilingual Constituency Parsing with Self-Attention and Pre-Training",
    "abstract": "We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2% relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1)",
    "volume": "main",
    "checked": true,
    "id": "526cae4863eb15b5bc39112449c2d5fdf1db85b2",
    "citation_count": 129
  },
  "https://aclanthology.org/P19-1341": {
    "title": "A Multilingual BPE Embedding Space for Universal Sentiment Lexicon Induction",
    "abstract": "We present a new method for sentiment lexicon induction that is designed to be applicable to the entire range of typological diversity of the world’s languages. We evaluate our method on Parallel Bible Corpus+ (PBC+), a parallel corpus of 1593 languages. The key idea is to use Byte Pair Encodings (BPEs) as basic units for multilingual embeddings. Through zero-shot transfer from English sentiment, we learn a seed lexicon for each language in the domain of PBC+. Through domain adaptation, we then generalize the domain-specific lexicon to a general one. We show – across typologically diverse languages in PBC+ – good quality of seed and general-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation. We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages",
    "volume": "main",
    "checked": true,
    "id": "8897df72881ad61f8d8a22bb1e1d78eeddf1ff1c",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-1342": {
    "title": "Tree Communication Models for Sentiment Analysis",
    "abstract": "Tree-LSTMs have been used for tree-based sentiment analysis over Stanford Sentiment Treebank, which allows the sentiment signals over hierarchical phrase structures to be calculated simultaneously. However, traditional tree-LSTMs capture only the bottom-up dependencies between constituents. In this paper, we propose a tree communication model using graph convolutional neural network and graph recurrent neural network, which allows rich information exchange between phrases constituent tree. Experiments show that our model outperforms existing work on bidirectional tree-LSTMs in both accuracy and efficiency, providing more consistent predictions on phrase-level sentiments",
    "volume": "main",
    "checked": true,
    "id": "002c086fcadb76e296cdd5accc29258b00ecf1cb",
    "citation_count": 24
  },
  "https://aclanthology.org/P19-1343": {
    "title": "Improved Sentiment Detection via Label Transfer from Monolingual to Synthetic Code-Switched Text",
    "abstract": "Multilingual writers and speakers often alternate between two languages in a single discourse. This practice is called “code-switching”. Existing sentiment detection methods are usually trained on sentiment-labeled monolingual text. Manually labeled code-switched text, especially involving minority languages, is extremely rare. Consequently, the best monolingual methods perform relatively poorly on code-switched text. We present an effective technique for synthesizing labeled code-switched text from labeled monolingual text, which is relatively readily available. The idea is to replace carefully selected subtrees of constituency parses of sentences in the resource-rich language with suitable token spans selected from automatic translations to the resource-poor language. By augmenting the scarce labeled code-switched text with plentiful synthetic labeled code-switched text, we achieve significant improvements in sentiment labeling accuracy (1.5%, 5.11% 7.20%) for three different language pairs (English-Hindi, English-Spanish and English-Bengali). The improvement is even significant in hatespeech detection whereby we achieve a 4% improvement using only synthetic code-switched data (6% with data augmentation)",
    "volume": "main",
    "checked": true,
    "id": "40d8ae568988abbf219d217cc164fbd571edf181",
    "citation_count": 12
  },
  "https://aclanthology.org/P19-1344": {
    "title": "Exploring Sequence-to-Sequence Learning in Aspect Term Extraction",
    "abstract": "Aspect term extraction (ATE) aims at identifying all aspect terms in a sentence and is usually modeled as a sequence labeling problem. However, sequence labeling based methods cannot make full use of the overall meaning of the whole sentence and have the limitation in processing dependencies between labels. To tackle these problems, we first explore to formalize ATE as a sequence-to-sequence (Seq2Seq) learning task where the source sequence and target sequence are composed of words and labels respectively. At the same time, to make Seq2Seq learning suit to ATE where labels correspond to words one by one, we design the gated unit networks to incorporate corresponding word representation into the decoder, and position-aware attention to pay more attention to the adjacent words of a target word. The experimental results on two datasets show that Seq2Seq learning is effective in ATE accompanied with our proposed gated unit networks and position-aware attention mechanism",
    "volume": "main",
    "checked": true,
    "id": "aae9475f1eb9e0a00490b3bd6bf298998508459e",
    "citation_count": 69
  },
  "https://aclanthology.org/P19-1345": {
    "title": "Aspect Sentiment Classification Towards Question-Answering with Reinforced Bidirectional Attention Network",
    "abstract": "In the literature, existing studies on aspect sentiment classification (ASC) focus on individual non-interactive reviews. This paper extends the research to interactive reviews and proposes a new research task, namely Aspect Sentiment Classification towards Question-Answering (ASC-QA), for real-world applications. This new task aims to predict sentiment polarities for specific aspects from interactive QA style reviews. In particular, a high-quality annotated corpus is constructed for ASC-QA to facilitate corresponding research. On this basis, a Reinforced Bidirectional Attention Network (RBAN) approach is proposed to address two inherent challenges in ASC-QA, i.e., semantic matching between question and answer, and data noise. Experimental results demonstrate the great advantage of the proposed approach to ASC-QA against several state-of-the-art baselines",
    "volume": "main",
    "checked": true,
    "id": "0ee7a8aac648161dd0fd8cc66f2cb952926944a7",
    "citation_count": 22
  },
  "https://aclanthology.org/P19-1346": {
    "title": "ELI5: Long Form Question Answering",
    "abstract": "We introduce the first large-scale corpus for long form question answering, a task requiring elaborate and in-depth answers to open-ended questions. The dataset comprises 270K threads from the Reddit forum “Explain Like I’m Five” (ELI5) where an online community provides answers to questions which are comprehensible by five year olds. Compared to existing datasets, ELI5 comprises diverse questions requiring multi-sentence answers. We provide a large set of web documents to help answer the question. Automatic and human evaluations show that an abstractive model trained with a multi-task objective outperforms conventional Seq2Seq, language modeling, as well as a strong extractive baseline.However, our best model is still far from human performance since raters prefer gold responses in over 86% of cases, leaving ample opportunity for future improvement",
    "volume": "main",
    "checked": true,
    "id": "ebf59587f8f170ff4241c42263bbfb9da5bd2135",
    "citation_count": 165
  },
  "https://aclanthology.org/P19-1347": {
    "title": "Textbook Question Answering with Multi-modal Context Graph Understanding and Self-supervised Open-set Comprehension",
    "abstract": "In this work, we introduce a novel algorithm for solving the textbook question answering (TQA) task which describes more realistic QA problems compared to other recent tasks. We mainly focus on two related issues with analysis of the TQA dataset. First, solving the TQA problems requires to comprehend multi-modal contexts in complicated input data. To tackle this issue of extracting knowledge features from long text lessons and merging them with visual features, we establish a context graph from texts and images, and propose a new module f-GCN based on graph convolutional networks (GCN). Second, scientific terms are not spread over the chapters and subjects are split in the TQA dataset. To overcome this so called ‘out-of-domain’ issue, before learning QA problems, we introduce a novel self-supervised open-set learning process without any annotations. The experimental results show that our model significantly outperforms prior state-of-the-art methods. Moreover, ablation studies validate that both methods of incorporating f-GCN for extracting knowledge from multi-modal contexts and our newly proposed self-supervised learning process are effective for TQA problems",
    "volume": "main",
    "checked": true,
    "id": "b7e3b4e36476519c27c7e433b768636fc23eca5b",
    "citation_count": 13
  },
  "https://aclanthology.org/P19-1348": {
    "title": "Generating Question Relevant Captions to Aid Visual Question Answering",
    "abstract": "Visual question answering (VQA) and image captioning require a shared body of general knowledge connecting language and vision. We present a novel approach to better VQA performance that exploits this connection by jointly generating captions that are targeted to help answer a specific visual question. The model is trained using an existing caption dataset by automatically determining question-relevant captions using an online gradient-based method. Experimental results on the VQA v2 challenge demonstrates that our approach obtains state-of-the-art VQA performance (e.g. 68.4% in the Test-standard set using a single model) by simultaneously generating question-relevant captions",
    "volume": "main",
    "checked": true,
    "id": "8faa293b037fb1dc32041b09faa93f7d1aa098e4",
    "citation_count": 33
  },
  "https://aclanthology.org/P19-1349": {
    "title": "Multi-grained Attention with Object-level Grounding for Visual Question Answering",
    "abstract": "Attention mechanisms are widely used in Visual Question Answering (VQA) to search for visual clues related to the question. Most approaches train attention models from a coarse-grained association between sentences and images, which tends to fail on small objects or uncommon concepts. To address this problem, this paper proposes a multi-grained attention method. It learns explicit word-object correspondence by two types of word-level attention complementary to the sentence-image association. Evaluated on the VQA benchmark, the multi-grained attention model achieves competitive performance with state-of-the-art models. And the visualized attention maps demonstrate that addition of object-level groundings leads to a better understanding of the images and locates the attended objects more precisely",
    "volume": "main",
    "checked": true,
    "id": "144f4d5dcd0b13935ff0d0890c2ec37aa40039b1",
    "citation_count": 16
  },
  "https://aclanthology.org/P19-1350": {
    "title": "Psycholinguistics Meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering",
    "abstract": "We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed VQA tasks, which differ by the types of questions involved (Wh-questions and polar questions). We test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates computational models. Our results show that dramatic forgetting is at play and that task difficulty and order matter. Two well-known current continual learning methods mitigate the problem only to a limiting degree",
    "volume": "main",
    "checked": true,
    "id": "aab85f2f1442233d6ded6229b7b299874a47353d",
    "citation_count": 27
  },
  "https://aclanthology.org/P19-1351": {
    "title": "Improving Visual Question Answering by Referring to Generated Paragraph Captions",
    "abstract": "Paragraph-style image captions describe diverse aspects of an image as opposed to the more common single-sentence captions that only provide an abstract description of the image. These paragraph captions can hence contain substantial information of the image for tasks such as visual question answering. Moreover, this textual information is complementary with visual information present in the image because it can discuss both more abstract concepts and more explicit, intermediate symbolic information about objects, events, and scenes that can directly be matched with the textual question and copied into the textual answer (i.e., via easier modality match). Hence, we propose a combined Visual and Textual Question Answering (VTQA) model which takes as input a paragraph caption as well as the corresponding image, and answers the given question based on both inputs. In our model, the inputs are fused to extract related information by cross-attention (early fusion), then fused again in the form of consensus (late fusion), and finally expected answers are given an extra score to enhance the chance of selection (later fusion). Empirical results show that paragraph captions, even when automatically generated (via an RL-based encoder-decoder model), help correctly answer more visual questions. Overall, our joint model, when trained on the Visual Genome dataset, significantly improves the VQA performance over a strong baseline model",
    "volume": "main",
    "checked": true,
    "id": "8501712706efa6f314438143de18507471781060",
    "citation_count": 18
  },
  "https://aclanthology.org/P19-1352": {
    "title": "Shared-Private Bilingual Word Embeddings for Neural Machine Translation",
    "abstract": "Word embedding is central to neural machine translation (NMT), which has attracted intensive research interest in recent years. In NMT, the source embedding plays the role of the entrance while the target embedding acts as the terminal. These layers occupy most of the model parameters for representation learning. Furthermore, they indirectly interface via a soft-attention mechanism, which makes them comparatively isolated. In this paper, we propose shared-private bilingual word embeddings, which give a closer relationship between the source and target embeddings, and which also reduce the number of model parameters. For similar source and target words, their embeddings tend to share a part of the features and they cooperatively learn these common representation units. Experiments on 5 language pairs belonging to 6 different language families and written in 5 different alphabets demonstrate that the proposed model provides a significant performance boost over the strong baselines with dramatically fewer model parameters",
    "volume": "main",
    "checked": true,
    "id": "d8fc035fd8ec5fd00c35aedbb0bf94eb0effff4a",
    "citation_count": 23
  },
  "https://aclanthology.org/P19-1353": {
    "title": "Literary Event Detection",
    "abstract": "In this work we present a new dataset of literary events—events that are depicted as taking place within the imagined space of a novel. While previous work has focused on event detection in the domain of contemporary news, literature poses a number of complications for existing systems, including complex narration, the depiction of a broad array of mental states, and a strong emphasis on figurative language. We outline the annotation decisions of this new dataset and compare several models for predicting events; the best performing model, a bidirectional LSTM with BERT token representations, achieves an F1 score of 73.9. We then apply this model to a corpus of novels split across two dimensions—prestige and popularity—and demonstrate that there are statistically significant differences in the distribution of events for prestige",
    "volume": "main",
    "checked": true,
    "id": "6fb09eb32ca16cd0e90cebf50c706a5d1b9466fe",
    "citation_count": 55
  },
  "https://aclanthology.org/P19-1354": {
    "title": "Assessing the Ability of Self-Attention Networks to Learn Word Order",
    "abstract": "Self-attention networks (SAN) have attracted a lot of interests due to their high parallelization and strong performance on a variety of NLP tasks, e.g. machine translation. Due to the lack of recurrence structure such as recurrent neural networks (RNN), SAN is ascribed to be weak at learning positional information of words for sequence modeling. However, neither this speculation has been empirically confirmed, nor explanations for their strong performances on machine translation tasks when “lacking positional information” have been explored. To this end, we propose a novel word reordering detection task to quantify how well the word order information learned by SAN and RNN. Specifically, we randomly move one word to another position, and examine whether a trained model can detect both the original and inserted positions. Experimental results reveal that: 1) SAN trained on word reordering detection indeed has difficulty learning the positional information even with the position embedding; and 2) SAN trained on machine translation learns better positional information than its RNN counterpart, in which position embedding plays a critical role. Although recurrence structure make the model more universally-effective on learning word order, learning objectives matter more in the downstream tasks such as machine translation",
    "volume": "main",
    "checked": true,
    "id": "f89d2da991935549b109d780be3351e0dda92a8f",
    "citation_count": 21
  },
  "https://aclanthology.org/P19-1355": {
    "title": "Energy and Policy Considerations for Deep Learning in NLP",
    "abstract": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice",
    "volume": "main",
    "checked": true,
    "id": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea",
    "citation_count": 1456
  },
  "https://aclanthology.org/P19-1356": {
    "title": "What Does BERT Learn about the Structure of Language?",
    "abstract": "BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT’s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures",
    "volume": "main",
    "checked": true,
    "id": "335613303ebc5eac98de757ed02a56377d99e03a",
    "citation_count": 734
  },
  "https://aclanthology.org/P19-1357": {
    "title": "A Just and Comprehensive Strategy for Using NLP to Address Online Abuse",
    "abstract": "Online abusive behavior affects millions and the NLP community has attempted to mitigate this problem by developing technologies to detect abuse. However, current methods have largely focused on a narrow definition of abuse to detriment of victims who seek both validation and solutions. In this position paper, we argue that the community needs to make three substantive changes: (1) expanding our scope of problems to tackle both more subtle and more serious forms of abuse, (2) developing proactive technologies that counter or inhibit abuse before it harms, and (3) reframing our effort within a framework of justice to promote healthy communities",
    "volume": "main",
    "checked": true,
    "id": "7472a40d5ac4f2312bf98f001906774dd45960e6",
    "citation_count": 88
  },
  "https://aclanthology.org/P19-1358": {
    "title": "Learning from Dialogue after Deployment: Feed Yourself, Chatbot!",
    "abstract": "The majority of conversations a dialogue agent sees over its lifetime occur after it has already been trained and deployed, leaving a vast store of potential training signal untapped. In this work, we propose the self-feeding chatbot, a dialogue agent with the ability to extract new training examples from the conversations it participates in. As our agent engages in conversation, it also estimates user satisfaction in its responses. When the conversation appears to be going well, the user’s responses become new training examples to imitate. When the agent believes it has made a mistake, it asks for feedback; learning to predict the feedback that will be given improves the chatbot’s dialogue abilities further. On the PersonaChat chit-chat dataset with over 131k training examples, we find that learning from dialogue with a self-feeding chatbot significantly improves performance, regardless of the amount of traditional supervision",
    "volume": "main",
    "checked": true,
    "id": "b0b96270a9bbeb9f3ec040e70114d565fbcaaed9",
    "citation_count": 122
  },
  "https://aclanthology.org/P19-1359": {
    "title": "Generating Responses with a Specific Emotion in Dialog",
    "abstract": "It is desirable for dialog systems to have capability to express specific emotions during a conversation, which has a direct, quantifiable impact on improvement of their usability and user satisfaction. After a careful investigation of real-life conversation data, we found that there are at least two ways to express emotions with language. One is to describe emotional states by explicitly using strong emotional words; another is to increase the intensity of the emotional experiences by implicitly combining neutral words in distinct ways. We propose an emotional dialogue system (EmoDS) that can generate the meaningful responses with a coherent structure for a post, and meanwhile express the desired emotion explicitly or implicitly within a unified framework. Experimental results showed EmoDS performed better than the baselines in BLEU, diversity and the quality of emotional expression",
    "volume": "main",
    "checked": true,
    "id": "c922e80acc086654ad2ca03f655ad60c848bdc68",
    "citation_count": 99
  },
  "https://aclanthology.org/P19-1360": {
    "title": "Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention",
    "abstract": "Semantically controlled neural response generation on limited-domain has achieved great performance. However, moving towards multi-domain large-scale scenarios are shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains. To alleviate such scalability issue, we exploit the structure of dialog acts to build a multi-layer hierarchical graph, where each act is represented as a root-to-leaf route on the graph. Then, we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network, where we disentangle attention heads to model designated nodes on the dialog act graph. By activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. On the large-scale Multi-Domain-WOZ dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics",
    "volume": "main",
    "checked": true,
    "id": "4d7df81767737c81890411c7f1685bb232921cb7",
    "citation_count": 92
  },
  "https://aclanthology.org/P19-1361": {
    "title": "Incremental Learning from Scratch for Task-Oriented Dialogue Systems",
    "abstract": "Clarifying user needs is essential for existing task-oriented dialogue systems. However, in real-world applications, developers can never guarantee that all possible user demands are taken into account in the design phase. Consequently, existing systems will break down when encountering unconsidered user needs. To address this problem, we propose a novel incremental learning framework to design task-oriented dialogue systems, or for short Incremental Dialogue System (IDS), without pre-defining the exhaustive list of user needs. Specifically, we introduce an uncertainty estimation module to evaluate the confidence of giving correct responses. If there is high confidence, IDS will provide responses to users. Otherwise, humans will be involved in the dialogue process, and IDS can learn from human intervention through an online learning module. To evaluate our method, we propose a new dataset which simulates unanticipated user needs in the deployment stage. Experiments show that IDS is robust to unconsidered user actions, and can update itself online by smartly selecting only the most effective training data, and hence attains better performance with less annotation cost",
    "volume": "main",
    "checked": true,
    "id": "4360aae9018a90a9d070deedcbc0a5524fe84d52",
    "citation_count": 13
  },
  "https://aclanthology.org/P19-1362": {
    "title": "ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation",
    "abstract": "In multi-turn dialogue generation, response is usually related with only a few contexts. Therefore, an ideal model should be able to detect these relevant contexts and produce a suitable response accordingly. However, the widely used hierarchical recurrent encoder-decoder models just treat all the contexts indiscriminately, which may hurt the following response generation process. Some researchers try to use the cosine similarity or the traditional attention mechanism to find the relevant contexts, but they suffer from either insufficient relevance assumption or position bias problem. In this paper, we propose a new model, named ReCoSa, to tackle this problem. Firstly, a word level LSTM encoder is conducted to obtain the initial representation of each context. Then, the self-attention mechanism is utilized to update both the context and masked response representation. Finally, the attention weights between each context and response representations are computed and used in the further decoding process. Experimental results on both Chinese customer services dataset and English Ubuntu dialogue dataset show that ReCoSa significantly outperforms baseline models, in terms of both metric-based and human evaluations. Further analysis on attention shows that the detected relevant contexts by ReCoSa are highly coherent with human’s understanding, validating the correctness and interpretability of ReCoSa",
    "volume": "main",
    "checked": true,
    "id": "67262b3ae544f51f7480650b064a740816e194c9",
    "citation_count": 87
  },
  "https://aclanthology.org/P19-1363": {
    "title": "Dialogue Natural Language Inference",
    "abstract": "Consistency is a long standing issue faced by dialogue models. In this paper, we frame the consistency of dialogue agents as natural language inference (NLI) and create a new natural language inference dataset called Dialogue NLI. We propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model, and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model’s consistency",
    "volume": "main",
    "checked": true,
    "id": "1dc04035b9926c46ded436e5762f3924ab29516e",
    "citation_count": 140
  },
  "https://aclanthology.org/P19-1364": {
    "title": "Budgeted Policy Learning for Task-Oriented Dialogue Systems",
    "abstract": "This paper presents a new approach that extends Deep Dyna-Q (DDQ) by incorporating a Budget-Conscious Scheduling (BCS) to best utilize a fixed, small amount of user interactions (budget) for learning task-oriented dialogue agents. BCS consists of (1) a Poisson-based global scheduler to allocate budget over different stages of training; (2) a controller to decide at each training step whether the agent is trained using real or simulated experiences; (3) a user goal sampling module to generate the experiences that are most effective for policy learning. Experiments on a movie-ticket booking task with simulated and real users show that our approach leads to significant improvements in success rate over the state-of-the-art baselines given the fixed budget",
    "volume": "main",
    "checked": true,
    "id": "371e23fe22d4313f784bbdc2fd18305b56b35fb3",
    "citation_count": 24
  },
  "https://aclanthology.org/P19-1365": {
    "title": "Comparison of Diverse Decoding Methods from Conditional Language Models",
    "abstract": "While conditional language models have greatly improved in their ability to output high quality natural language, many NLP applications benefit from being able to generate a diverse set of candidate sequences. Diverse decoding strategies aim to, within a given-sized candidate list, cover as much of the space of high-quality outputs as possible, leading to improvements for tasks that rerank and combine candidate outputs. Standard decoding methods, such as beam search, optimize for generating high likelihood sequences rather than diverse ones, though recent work has focused on increasing diversity in these methods. In this work, we perform an extensive survey of decoding-time strategies for generating diverse outputs from a conditional language model. In addition, we present a novel method where we over-sample candidates, then use clustering to remove similar sequences, thus achieving high diversity without sacrificing quality",
    "volume": "main",
    "checked": true,
    "id": "fd846869e6f25d9b1a524aef8b54a08b81a1b1fa",
    "citation_count": 63
  },
  "https://aclanthology.org/P19-1366": {
    "title": "Retrieval-Enhanced Adversarial Training for Neural Response Generation",
    "abstract": "Dialogue systems are usually built on either generation-based or retrieval-based approaches, yet they do not benefit from the advantages of different models. In this paper, we propose a Retrieval-Enhanced Adversarial Training (REAT) method for neural response generation. Distinct from existing approaches, the REAT method leverages an encoder-decoder framework in terms of an adversarial training paradigm, while taking advantage of N-best response candidates from a retrieval-based system to construct the discriminator. An empirical study on a large scale public available benchmark dataset shows that the REAT method significantly outperforms the vanilla Seq2Seq model as well as the conventional adversarial training approach",
    "volume": "main",
    "checked": true,
    "id": "e9a3bcba412f971bbc386f894287ef64113c7213",
    "citation_count": 32
  },
  "https://aclanthology.org/P19-1367": {
    "title": "Vocabulary Pyramid Network: Multi-Pass Encoding and Decoding with Multi-Level Vocabularies for Response Generation",
    "abstract": "We study the task of response generation. Conventional methods employ a fixed vocabulary and one-pass decoding, which not only make them prone to safe and general responses but also lack further refining to the first generated raw sequence. To tackle the above two problems, we present a Vocabulary Pyramid Network (VPN) which is able to incorporate multi-pass encoding and decoding with multi-level vocabularies into response generation. Specifically, the dialogue input and output are represented by multi-level vocabularies which are obtained from hierarchical clustering of raw words. Then, multi-pass encoding and decoding are conducted on the multi-level vocabularies. Since VPN is able to leverage rich encoding and decoding information with multi-level vocabularies, it has the potential to generate better responses. Experiments on English Twitter and Chinese Weibo datasets demonstrate that VPN remarkably outperforms strong baselines",
    "volume": "main",
    "checked": true,
    "id": "5d4bf12708fd0529fee136971b6fa0df8a351e78",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1368": {
    "title": "On-device Structured and Context Partitioned Projection Networks",
    "abstract": "A challenging problem in on-device text classification is to build highly accurate neural models that can fit in small memory footprint and have low latency. To address this challenge, we propose an on-device neural network SGNN++ which dynamically learns compact projection vectors from raw text using structured and context-dependent partition projections. We show that this results in accelerated inference and performance improvements. We conduct extensive evaluation on multiple conversational tasks and languages such as English, Japanese, Spanish and French. Our SGNN++ model significantly outperforms all baselines, improves upon existing on-device neural models and even surpasses RNN, CNN and BiLSTM models on dialog act and intent prediction. Through a series of ablation studies we show the impact of the partitioned projections and structured information leading to 10% improvement. We study the impact of the model size on accuracy and introduce quatization-aware training for SGNN++ to further reduce the model size while preserving the same quality. Finally, we show fast inference on mobile phones",
    "volume": "main",
    "checked": true,
    "id": "5d12770a43f7fecd17d8a58e4eedc48c41427cf4",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1369": {
    "title": "Proactive Human-Machine Conversation with Explicit Conversation Goal",
    "abstract": "Though great progress has been made for human-machine conversation, current dialogue system is still in its infancy: it usually converses passively and utters words more as a matter of response, rather than on its own initiatives. In this paper, we take a radical step towards building a human-like conversational agent: endowing it with the ability of proactively leading the conversation (introducing a new topic or maintaining the current topic). To facilitate the development of such conversation systems, we create a new dataset named Konv where one acts as a conversation leader and the other acts as the follower. The leader is provided with a knowledge graph and asked to sequentially change the discussion topics, following the given conversation goal, and meanwhile keep the dialogue as natural and engaging as possible. Konv enables a very challenging task as the model needs to both understand dialogue and plan over the given knowledge graph. We establish baseline results on this dataset (about 270K utterances and 30k dialogues) using several state-of-the-art models. Experimental results show that dialogue models that plan over the knowledge graph can make full use of related knowledge to generate more diverse multi-turn conversations. The baseline systems along with the dataset are publicly available",
    "volume": "main",
    "checked": true,
    "id": "6a58492882111f852ccf64bd64711bacfd0ffcda",
    "citation_count": 119
  },
  "https://aclanthology.org/P19-1370": {
    "title": "Learning a Matching Model with Co-teaching for Multi-turn Response Selection in Retrieval-based Dialogue Systems",
    "abstract": "We study learning of a matching model for response selection in retrieval-based dialogue systems. The problem is equally important with designing the architecture of a model, but is less explored in existing literature. To learn a robust matching model from noisy training data, we propose a general co-teaching framework with three specific teaching strategies that cover both teaching with loss functions and teaching with data curriculum. Under the framework, we simultaneously learn two matching models with independent training sets. In each iteration, one model transfers the knowledge learned from its training set to the other model, and at the same time receives the guide from the other model on how to overcome noise in training. Through being both a teacher and a student, the two models learn from each other and get improved together. Evaluation results on two public data sets indicate that the proposed learning approach can generally and significantly improve the performance of existing matching models",
    "volume": "main",
    "checked": true,
    "id": "2f414446bfba319200c94d601a29cb7c7e58884a",
    "citation_count": 22
  },
  "https://aclanthology.org/P19-1371": {
    "title": "Learning to Abstract for Memory-augmented Conversational Response Generation",
    "abstract": "Neural generative models for open-domain chit-chat conversations have become an active area of research in recent years. A critical issue with most existing generative models is that the generated responses lack informativeness and diversity. A few researchers attempt to leverage the results of retrieval models to strengthen the generative models, but these models are limited by the quality of the retrieval results. In this work, we propose a memory-augmented generative model, which learns to abstract from the training corpus and saves the useful information to the memory to assist the response generation. Our model clusters query-response samples, extracts characteristics of each cluster, and learns to utilize these characteristics for response generation. Experimental results show that our model outperforms other competitive baselines",
    "volume": "main",
    "checked": true,
    "id": "80e5d2fecfb51d0d5e03f1ea7c760ada944d1425",
    "citation_count": 27
  },
  "https://aclanthology.org/P19-1372": {
    "title": "Are Training Samples Correlated? Learning to Generate Dialogue Responses with Multiple References",
    "abstract": "Due to its potential applications, open-domain dialogue generation has become popular and achieved remarkable progress in recent years, but sometimes suffers from generic responses. Previous models are generally trained based on 1-to-1 mapping from an input query to its response, which actually ignores the nature of 1-to-n mapping in dialogue that there may exist multiple valid responses corresponding to the same query. In this paper, we propose to utilize the multiple references by considering the correlation of different valid responses and modeling the 1-to-n mapping with a novel two-step generation architecture. The first generation phase extracts the common features of different responses which, combined with distinctive features obtained in the second phase, can generate multiple diverse and appropriate responses. Experimental results show that our proposed model can effectively improve the quality of response and outperform existing neural dialogue models on both automatic and human evaluations",
    "volume": "main",
    "checked": true,
    "id": "29b125f6989759f687169ced8aece8aa2dda4f4f",
    "citation_count": 38
  },
  "https://aclanthology.org/P19-1373": {
    "title": "Pretraining Methods for Dialog Context Representation Learning",
    "abstract": "This paper examines various unsupervised pretraining objectives for learning dialog context representations. Two novel methods of pretraining dialog context encoders are proposed, and a total of four methods are examined. Each pretraining objective is fine-tuned and evaluated on a set of downstream dialog tasks using the MultiWoz dataset and strong performance improvement is observed. Further evaluation shows that our pretraining objectives result in not only better performance, but also better convergence, models that are less data hungry and have better domain generalizability",
    "volume": "main",
    "checked": true,
    "id": "535bbc71ef9f8a8c6180c599ded774ff6f4f8258",
    "citation_count": 62
  },
  "https://aclanthology.org/P19-1374": {
    "title": "A Large-Scale Corpus for Conversation Disentanglement",
    "abstract": "Disentangling conversations mixed together in a single stream of messages is a difficult task, made harder by the lack of large manually annotated datasets. We created a new dataset of 77,563 messages manually annotated with reply-structure graphs that both disentangle conversations and define internal conversation structure. Our data is 16 times larger than all previously released datasets combined, the first to include adjudication of annotation disagreements, and the first to include context. We use our data to re-examine prior work, in particular, finding that 89% of conversations in a widely used dialogue corpus are either missing messages or contain extra messages. Our manually-annotated data presents an opportunity to develop robust data-driven methods for conversation disentanglement, which will help advance dialogue research",
    "volume": "main",
    "checked": true,
    "id": "c59d36e79d573cc4a2440cb2a7154eada5c0ead2",
    "citation_count": 65
  },
  "https://aclanthology.org/P19-1375": {
    "title": "Self-Supervised Dialogue Learning",
    "abstract": "The sequential order of utterances is often meaningful in coherent dialogues, and the order changes of utterances could lead to low-quality and incoherent conversations. We consider the order information as a crucial supervised signal for dialogue learning, which, however, has been neglected by many previous dialogue systems. Therefore, in this paper, we introduce a self-supervised learning task, inconsistent order detection, to explicitly capture the flow of conversation in dialogues. Given a sampled utterance pair triple, the task is to predict whether it is ordered or misordered. Then we propose a sampling-based self-supervised network SSN to perform the prediction with sampled triple references from previous dialogue history. Furthermore, we design a joint learning framework where SSN can guide the dialogue systems towards more coherent and relevant dialogue learning through adversarial training. We demonstrate that the proposed methods can be applied to both open-domain and task-oriented dialogue scenarios, and achieve the new state-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking datasets",
    "volume": "main",
    "checked": true,
    "id": "f36f567c3a9d24a778f3fdefd0d411db10b942fc",
    "citation_count": 45
  },
  "https://aclanthology.org/P19-1376": {
    "title": "Are we there yet? Encoder-decoder neural networks as cognitive models of English past tense inflection",
    "abstract": "The cognitive mechanisms needed to account for the English past tense have long been a subject of debate in linguistics and cognitive science. Neural network models were proposed early on, but were shown to have clear flaws. Recently, however, Kirov and Cotterell (2018) showed that modern encoder-decoder (ED) models overcome many of these flaws. They also presented evidence that ED models demonstrate humanlike performance in a nonce-word task. Here, we look more closely at the behaviour of their model in this task. We find that (1) the model exhibits instability across multiple simulations in terms of its correlation with human data, and (2) even when results are aggregated across simulations (treating each simulation as an individual human participant), the fit to the human data is not strong—worse than an older rule-based model. These findings hold up through several alternative training regimes and evaluation measures. Although other neural architectures might do better, we conclude that there is still insufficient evidence to claim that neural nets are a good cognitive model for this task",
    "volume": "main",
    "checked": true,
    "id": "74c512bfb67d9cda71836bbd853bb68d4a82793e",
    "citation_count": 26
  },
  "https://aclanthology.org/P19-1377": {
    "title": "A Spreading Activation Framework for Tracking Conceptual Complexity of Texts",
    "abstract": "We propose an unsupervised approach for assessing conceptual complexity of texts, based on spreading activation. Using DBpedia knowledge graph as a proxy to long-term memory, mentioned concepts become activated and trigger further activation as the text is sequentially traversed. Drawing inspiration from psycholinguistic theories of reading comprehension, we model memory processes such as semantic priming, sentence wrap-up, and forgetting. We show that our models capture various aspects of conceptual text complexity and significantly outperform current state of the art",
    "volume": "main",
    "checked": true,
    "id": "9ff1d805955ca89dce2f6dab381ae2cd9e607699",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1378": {
    "title": "End-to-End Sequential Metaphor Identification Inspired by Linguistic Theories",
    "abstract": "End-to-end training with Deep Neural Networks (DNN) is a currently popular method for metaphor identification. However, standard sequence tagging models do not explicitly take advantage of linguistic theories of metaphor identification. We experiment with two DNN models which are inspired by two human metaphor identification procedures. By testing on three public datasets, we find that our models achieve state-of-the-art performance in end-to-end metaphor identification",
    "volume": "main",
    "checked": true,
    "id": "620261c2192ce5d96dd755b159ee4240521eed61",
    "citation_count": 60
  },
  "https://aclanthology.org/P19-1379": {
    "title": "Diachronic Sense Modeling with Deep Contextualized Word Embeddings: An Ecological View",
    "abstract": "Diachronic word embeddings have been widely used in detecting temporal changes. However, existing methods face the meaning conflation deficiency by representing a word as a single vector at each time period. To address this issue, this paper proposes a sense representation and tracking framework based on deep contextualized embeddings, aiming at answering not only what and when, but also how the word meaning changes. The experiments show that our framework is effective in representing fine-grained word senses, and it brings a significant improvement in word change detection task. Furthermore, we model the word change from an ecological viewpoint, and sketch two interesting sense behaviors in the process of language evolution, i.e. sense competition and sense cooperation",
    "volume": "main",
    "checked": true,
    "id": "d9f0648023c0d9b455ec2b8b52b49cbaa5e71899",
    "citation_count": 57
  },
  "https://aclanthology.org/P19-1380": {
    "title": "Miss Tools and Mr Fruit: Emergent Communication in Agents Learning about Object Affordances",
    "abstract": "Recent research studies communication emergence in communities of deep network agents assigned a joint task, hoping to gain insights on human language evolution. We propose here a new task capturing crucial aspects of the human environment, such as natural object affordances, and of human conversation, such as full symmetry among the participants. By conducting a thorough pragmatic and semantic analysis of the emergent protocol, we show that the agents solve the shared task through genuine bilateral, referential communication. However, the agents develop multiple idiolects, which makes us conclude that full symmetry is not a sufficient condition for a common language to emerge",
    "volume": "main",
    "checked": true,
    "id": "bae289e1d7a06d92b4856addd132d37849ec15f8",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1381": {
    "title": "CNNs found to jump around more skillfully than RNNs: Compositional Generalization in Seq2seq Convolutional Networks",
    "abstract": "Lake and Baroni (2018) introduced the SCAN dataset probing the ability of seq2seq models to capture compositional generalizations, such as inferring the meaning of “jump around” 0-shot from the component words. Recurrent networks (RNNs) were found to completely fail the most challenging generalization cases. We test here a convolutional network (CNN) on these tasks, reporting hugely improved performance with respect to RNNs. Despite the big improvement, the CNN has however not induced systematic rules, suggesting that the difference between compositional and non-compositional behaviour is not clear-cut",
    "volume": "main",
    "checked": true,
    "id": "2621323502fc779c79bca7ba112bc4d0c1db1d3f",
    "citation_count": 34
  },
  "https://aclanthology.org/P19-1382": {
    "title": "Uncovering Probabilistic Implications in Typological Knowledge Bases",
    "abstract": "The study of linguistic typology is rooted in the implications we find between linguistic features, such as the fact that languages with object-verb word ordering tend to have postpositions. Uncovering such implications typically amounts to time-consuming manual processing by trained and experienced linguists, which potentially leaves key linguistic universals unexplored. In this paper, we present a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further linguistic investigation. Our approach outperforms baselines previously used for this problem, as well as a strong baseline from knowledge base population",
    "volume": "main",
    "checked": true,
    "id": "15ecefd55389be637283377274630ba51c7315dc",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-1383": {
    "title": "Is Word Segmentation Child's Play in All Languages?",
    "abstract": "When learning language, infants need to break down the flow of input speech into minimal word-like units, a process best described as unsupervised bottom-up segmentation. Proposed strategies include several segmentation algorithms, but only cross-linguistically robust algorithms could be plausible candidates for human word learning, since infants have no initial knowledge of the ambient language. We report on the stability in performance of 11 conceptually diverse algorithms on a selection of 8 typologically distinct languages. The results consist evidence that some segmentation algorithms are cross-linguistically valid, thus could be considered as potential strategies employed by all infants",
    "volume": "main",
    "checked": true,
    "id": "1c85ac3e9da0645da050d413ec1a20b60202e7b8",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-1384": {
    "title": "On the Distribution of Deep Clausal Embeddings: A Large Cross-linguistic Study",
    "abstract": "Embedding a clause inside another (“the girl [who likes cars [that run fast]] has arrived”) is a fundamental resource that has been argued to be a key driver of linguistic expressiveness. As such, it plays a central role in fundamental debates on what makes human language unique, and how they might have evolved. Empirical evidence on the prevalence and the limits of embeddings has however been based on either laboratory setups or corpus data of relatively limited size. We introduce here a collection of large, dependency-parsed written corpora in 17 languages, that allow us, for the first time, to capture clausal embedding through dependency graphs and assess their distribution. Our results indicate that there is no evidence for hard constraints on embedding depth: the tail of depth distributions is heavy. Moreover, although deeply embedded clauses tend to be shorter, suggesting processing load issues, complex sentences with many embeddings do not display a bias towards less deep embeddings. Taken together, the results suggest that deep embeddings are not disfavoured in written language. More generally, our study illustrates how resources and methods from latest-generation big-data NLP can provide new perspectives on fundamental questions in theoretical linguistics",
    "volume": "main",
    "checked": true,
    "id": "3148268debf6c445953ae3389328f8801394e1ce",
    "citation_count": 3
  },
  "https://aclanthology.org/P19-1385": {
    "title": "Attention-based Conditioning Methods for External Knowledge Integration",
    "abstract": "In this paper, we present a novel approach for incorporating external knowledge in Recurrent Neural Networks (RNNs). We propose the integration of lexicon features into the self-attention mechanism of RNN-based architectures. This form of conditioning on the attention distribution, enforces the contribution of the most salient words for the task at hand. We introduce three methods, namely attentional concatenation, feature-based gating and affine transformation. Experiments on six benchmark datasets show the effectiveness of our methods. Attentional feature-based gating yields consistent performance improvement across tasks. Our approach is implemented as a simple add-on module for RNN-based models with minimal computational overhead and can be adapted to any deep neural architecture",
    "volume": "main",
    "checked": true,
    "id": "1648a8ee6e86c4118715f4b2e09762346cc5f54a",
    "citation_count": 18
  },
  "https://aclanthology.org/P19-1386": {
    "title": "The KnowRef Coreference Corpus: Removing Gender and Number Cues for Difficult Pronominal Anaphora Resolution",
    "abstract": "We introduce a new benchmark for coreference resolution and NLI, KnowRef, that targets common-sense understanding and world knowledge. Previous coreference resolution tasks can largely be solved by exploiting the number and gender of the antecedents, or have been handcrafted and do not reflect the diversity of naturally occurring text. We present a corpus of over 8,000 annotated text passages with ambiguous pronominal anaphora. These instances are both challenging and realistic. We show that various coreference systems, whether rule-based, feature-rich, or neural, perform significantly worse on the task than humans, who display high inter-annotator agreement. To explain this performance gap, we show empirically that state-of-the art models often fail to capture context, instead relying on the gender or number of candidate antecedents to make a decision. We then use problem-specific insights to propose a data-augmentation trick called antecedent switching to alleviate this tendency in models. Finally, we show that antecedent switching yields promising results on other tasks as well: we use it to achieve state-of-the-art results on the GAP coreference task",
    "volume": "main",
    "checked": true,
    "id": "91d422ddb9be5a1b70c36f20e18e164973c8a069",
    "citation_count": 35
  },
  "https://aclanthology.org/P19-1387": {
    "title": "StRE: Self Attentive Edit Quality Prediction in Wikipedia",
    "abstract": "Wikipedia can easily be justified as a behemoth, considering the sheer volume of content that is added or removed every minute to its several projects. This creates an immense scope, in the field of natural language processing toward developing automated tools for content moderation and review. In this paper we propose Self Attentive Revision Encoder (StRE) which leverages orthographic similarity of lexical units toward predicting the quality of new edits. In contrast to existing propositions which primarily employ features like page reputation, editor activity or rule based heuristics, we utilize the textual content of the edits which, we believe contains superior signatures of their quality. More specifically, we deploy deep encoders to generate representations of the edits from its text content, which we then leverage to infer quality. We further contribute a novel dataset containing ∼ 21M revisions across 32K Wikipedia pages and demonstrate that StRE outperforms existing methods by a significant margin – at least 17% and at most 103%. Our pre-trained model achieves such result after retraining on a set as small as 20% of the edits in a wikipage. This, to the best of our knowledge, is also the first attempt towards employing deep language models to the enormous domain of automated content moderation and review in Wikipedia",
    "volume": "main",
    "checked": true,
    "id": "e2988816a10bf5a50bd952601ac06026e5493782",
    "citation_count": 8
  },
  "https://aclanthology.org/P19-1388": {
    "title": "How Large Are Lions? Inducing Distributions over Quantitative Attributes",
    "abstract": "Most current NLP systems have little knowledge about quantitative attributes of objects and events. We propose an unsupervised method for collecting quantitative information from large amounts of web data, and use it to create a new, very large resource consisting of distributions over physical quantities associated with objects, adjectives, and verbs which we call Distributions over Quantitative (DoQ). This contrasts with recent work in this area which has focused on making only relative comparisons such as “Is a lion bigger than a wolf?”. Our evaluation shows that DoQ compares favorably with state of the art results on existing datasets for relative comparisons of nouns and adjectives, and on a new dataset we introduce",
    "volume": "main",
    "checked": true,
    "id": "d34e6c84119cef8eb9149a27d6b4903131407ea6",
    "citation_count": 40
  },
  "https://aclanthology.org/P19-1389": {
    "title": "Fine-Grained Sentence Functions for Short-Text Conversation",
    "abstract": "Sentence function is an important linguistic feature referring to a user’s purpose in uttering a specific sentence. The use of sentence function has shown promising results to improve the performance of conversation models. However, there is no large conversation dataset annotated with sentence functions. In this work, we collect a new Short-Text Conversation dataset with manually annotated SEntence FUNctions (STC-Sefun). Classification models are trained on this dataset to (i) recognize the sentence function of new data in a large corpus of short-text conversations; (ii) estimate a proper sentence function of the response given a test query. We later train conversation models conditioned on the sentence functions, including information retrieval-based and neural generative models. Experimental results demonstrate that the use of sentence functions can help improve the quality of the returned responses",
    "volume": "main",
    "checked": true,
    "id": "9ebe176991382916a07b3cb107b0d2ff27782308",
    "citation_count": 14
  },
  "https://aclanthology.org/P19-1390": {
    "title": "Give Me More Feedback II: Annotating Thesis Strength and Related Attributes in Student Essays",
    "abstract": "While the vast majority of existing work on automated essay scoring has focused on holistic scoring, researchers have recently begun work on scoring specific dimensions of essay quality. Nevertheless, progress on dimension-specific essay scoring is limited in part by the lack of annotated corpora. To facilitate advances in this area, we design a scoring rubric for scoring a core, yet unexplored dimension of persuasive essay quality, thesis strength, and annotate a corpus of essays with thesis strength scores. We additionally identify the attributes that could impact thesis strength and annotate the essays with the values of these attributes, which, when predicted by computational models, could provide further feedback to students on why her essay receives a particular thesis strength score",
    "volume": "main",
    "checked": true,
    "id": "2fc918a7da63d9c85ad37bf27d9174eae3c9de54",
    "citation_count": 12
  },
  "https://aclanthology.org/P19-1391": {
    "title": "Crowdsourcing and Validating Event-focused Emotion Corpora for German and English",
    "abstract": "Sentiment analysis has a range of corpora available across multiple languages. For emotion analysis, the situation is more limited, which hinders potential research on crosslingual modeling and the development of predictive models for other languages. In this paper, we fill this gap for German by constructing deISEAR, a corpus designed in analogy to the well-established English ISEAR emotion dataset. Motivated by Scherer’s appraisal theory, we implement a crowdsourcing experiment which consists of two steps. In step 1, participants create descriptions of emotional events for a given emotion. In step 2, five annotators assess the emotion expressed by the texts. We show that transferring an emotion classification model from the original English ISEAR to the German crowdsourced deISEAR via machine translation does not, on average, cause a performance drop",
    "volume": "main",
    "checked": true,
    "id": "f3c793d8bff8863ee0353f1d9c7a84182217c773",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-1392": {
    "title": "Pay Attention when you Pay the Bills. A Multilingual Corpus with Dependency-based and Semantic Annotation of Collocations",
    "abstract": "This paper presents a new multilingual corpus with semantic annotation of collocations in English, Portuguese, and Spanish. The whole resource contains 155k tokens and 1,526 collocations labeled in context. The annotated examples belong to three syntactic relations (adjective-noun, verb-object, and nominal compounds), and represent 58 lexical functions in the Meaning-Text Theory (e.g., Oper, Magn, Bon, etc.). Each collocation was annotated by three linguists and the final resource was revised by a team of experts. The resulting corpus can serve as a basis to evaluate different approaches for collocation identification, which in turn can be useful for different NLP tasks such as natural language understanding or natural language generation",
    "volume": "main",
    "checked": true,
    "id": "af26dcd38015b9dd0174cbc7ac8423edb5114ce2",
    "citation_count": 4
  },
  "https://aclanthology.org/P19-1393": {
    "title": "Does it Make Sense? And Why? A Pilot Study for Sense Making and Explanation",
    "abstract": "Introducing common sense to natural language understanding systems has received increasing research attention. It remains a fundamental question on how to evaluate whether a system has the sense-making capability. Existing benchmarks measure common sense knowledge indirectly or without reasoning. In this paper, we release a benchmark to directly test whether a system can differentiate natural language statements that make sense from those that do not make sense. In addition, a system is asked to identify the most crucial reason why a statement does not make sense. We evaluate models trained over large-scale language modeling tasks as well as human performance, showing that there are different challenges for system sense-making",
    "volume": "main",
    "checked": true,
    "id": "e5a400f9663b3d1f6784cd078f5a471e4f17535b",
    "citation_count": 77
  },
  "https://aclanthology.org/P19-1394": {
    "title": "Large Dataset and Language Model Fun-Tuning for Humor Recognition",
    "abstract": "The task of humor recognition has attracted a lot of attention recently due to the urge to process large amounts of user-generated texts and rise of conversational agents. We collected a dataset of jokes and funny dialogues in Russian from various online resources and complemented them carefully with unfunny texts with similar lexical properties. The dataset comprises of more than 300,000 short texts, which is significantly larger than any previous humor-related corpus. Manual annotation of 2,000 items proved the reliability of the corpus construction approach. Further, we applied language model fine-tuning for text classification and obtained an F1 score of 0.91 on a test set, which constitutes a considerable gain over baseline methods. The dataset is freely available for research community",
    "volume": "main",
    "checked": true,
    "id": "8cf7ada228bdabeb56f028c6762f1cf82f4acd7c",
    "citation_count": 30
  },
  "https://aclanthology.org/P19-1395": {
    "title": "Towards Language Agnostic Universal Representations",
    "abstract": "When a bilingual student learns to solve word problems in math, we expect the student to be able to solve these problem in both languages the student is fluent in, even if the math lessons were only taught in one language. However, current representations in machine learning are language dependent. In this work, we present a method to decouple the language from the problem by learning language agnostic representations and therefore allowing training a model in one language and applying to a different one in a zero shot fashion. We learn these representations by taking inspiration from linguistics, specifically the Universal Grammar hypothesis and learn universal latent representations that are language agnostic. We demonstrate the capabilities of these representations by showing that models trained on a single language using language agnostic representations achieve very similar accuracies in other languages",
    "volume": "main",
    "checked": true,
    "id": "c0affae8445bcd6fd5a84e34ed940bf03c7cb300",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-1396": {
    "title": "Leveraging Meta Information in Short Text Aggregation",
    "abstract": "Short texts such as tweets often contain insufficient word co-occurrence information for training conventional topic models. To deal with the insufficiency, we propose a generative model that aggregates short texts into clusters by leveraging the associated meta information. Our model can generate more interpretable topics as well as document clusters. We develop an effective Gibbs sampling algorithm favoured by the fully local conjugacy in the model. Extensive experiments demonstrate that our model achieves better performance in terms of document clustering and topic coherence",
    "volume": "main",
    "checked": true,
    "id": "1db09382df9f7495a71a0c9dd82bf9091e3f7f0d",
    "citation_count": 2
  },
  "https://aclanthology.org/P19-1397": {
    "title": "Exploiting Invertible Decoders for Unsupervised Sentence Representation Learning",
    "abstract": "Encoder-decoder models for unsupervised sentence representation learning using the distributional hypothesis effectively constrain the learnt representation of a sentence to only that needed to reproduce the next sentence. While the decoder is important to constrain the representation, these models tend to discard the decoder after training since only the encoder is needed to map the input sentence into a vector representation. However, parameters learnt in the decoder also contain useful information about the language. In order to utilise the decoder after learning, we present two types of decoding functions whose inverse can be easily derived without expensive inverse calculation. Therefore, the inverse of the decoding function serves as another encoder that produces sentence representations. We show that, with careful design of the decoding functions, the model learns good sentence representations, and the ensemble of the representations produced from the encoder and the inverse of the decoder demonstrate even better generalisation ability and solid transferability",
    "volume": "main",
    "checked": true,
    "id": "2a7edf7838a3f9246c4ca8368b12c2db32c7e379",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-1398": {
    "title": "Self-Attentive, Multi-Context One-Class Classification for Unsupervised Anomaly Detection on Text",
    "abstract": "There exist few text-specific methods for unsupervised anomaly detection, and for those that do exist, none utilize pre-trained models for distributed vector representations of words. In this paper we introduce a new anomaly detection method—Context Vector Data Description (CVDD)—which builds upon word embedding models to learn multiple sentence representations that capture multiple semantic contexts via the self-attention mechanism. Modeling multiple contexts enables us to perform contextual anomaly detection of sentences and phrases with respect to the multiple themes and concepts present in an unlabeled text corpus. These contexts in combination with the self-attention weights make our method highly interpretable. We demonstrate the effectiveness of CVDD quantitatively as well as qualitatively on the well-known Reuters, 20 Newsgroups, and IMDB Movie Reviews datasets",
    "volume": "main",
    "checked": true,
    "id": "92584dac09356c3c2e915932956bdc06f91d453f",
    "citation_count": 34
  },
  "https://aclanthology.org/P19-1399": {
    "title": "Hubless Nearest Neighbor Search for Bilingual Lexicon Induction",
    "abstract": "Bilingual Lexicon Induction (BLI) is the task of translating words from corpora in two languages. Recent advances in BLI work by aligning the two word embedding spaces. Following that, a key step is to retrieve the nearest neighbor (NN) in the target space given the source word. However, a phenomenon called hubness often degrades the accuracy of NN. Hubness appears as some data points, called hubs, being extra-ordinarily close to many of the other data points. Reducing hubness is necessary for retrieval tasks. One successful example is Inverted SoFtmax (ISF), recently proposed to improve NN. This work proposes a new method, Hubless Nearest Neighbor (HNN), to mitigate hubness. HNN differs from NN by imposing an additional equal preference assumption. Moreover, the HNN formulation explains why ISF works as well as it does. Empirical results demonstrate that HNN outperforms NN, ISF and other state-of-the-art. For reproducibility and follow-ups, we have published all code",
    "volume": "main",
    "checked": true,
    "id": "5292a00eff5f9864adc6a4d159591d3d476cba8c",
    "citation_count": 14
  },
  "https://aclanthology.org/P19-1400": {
    "title": "Distant Learning for Entity Linking with Automatic Noise Detection",
    "abstract": "Accurate entity linkers have been produced for domains and languages where annotated data (i.e., texts linked to a knowledge base) is available. However, little progress has been made for the settings where no or very limited amounts of labeled data are present (e.g., legal or most scientific domains). In this work, we show how we can learn to link mentions without having any labeled examples, only a knowledge base and a collection of unannotated texts from the corresponding domain. In order to achieve this, we frame the task as a multi-instance learning problem and rely on surface matching to create initial noisy labels. As the learning signal is weak and our surrogate labels are noisy, we introduce a noise detection component in our model: it lets the model detect and disregard examples which are likely to be noisy. Our method, jointly learning to detect noise and link entities, greatly outperforms the surface matching baseline. For a subset of entity categories, it even approaches the performance of supervised learning",
    "volume": "main",
    "checked": true,
    "id": "ae71b1f211a6591f45e82c3e06f81fa60386c808",
    "citation_count": 24
  },
  "https://aclanthology.org/P19-1401": {
    "title": "Learning How to Active Learn by Dreaming",
    "abstract": "Heuristic-based active learning (AL) methods are limited when the data distribution of the underlying learning problems vary. Recent data-driven AL policy learning methods are also restricted to learn from closely related domains. We introduce a new sample-efficient method that learns the AL policy directly on the target domain of interest by using wake and dream cycles. Our approach interleaves between querying the annotation of the selected datapoints to update the underlying student learner and improving AL policy using simulation where the current student learner acts as an imperfect annotator. We evaluate our method on cross-domain and cross-lingual text classification and named entity recognition tasks. Experimental results show that our dream-based AL policy training strategy is more effective than applying the pretrained policy without further fine-tuning and better than the existing strong baseline methods that use heuristics or reinforcement learning",
    "volume": "main",
    "checked": true,
    "id": "cd618944fa87f52f01d919facd392986a582dbb7",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-1402": {
    "title": "Few-Shot Representation Learning for Out-Of-Vocabulary Words",
    "abstract": "Existing approaches for learning word embedding often assume there are sufficient occurrences for each word in the corpus, such that the representation of words can be accurately estimated from their contexts. However, in real-world scenarios, out-of-vocabulary (a.k.a. OOV) words that do not appear in training corpus emerge frequently. How to learn accurate representations of these words to augment a pre-trained embedding by only a few observations is a challenging research problem. In this paper, we formulate the learning of OOV embedding as a few-shot regression problem by fitting a representation function to predict an oracle embedding vector (defined as embedding trained with abundant observations) based on limited contexts. Specifically, we propose a novel hierarchical attention network-based embedding framework to serve as the neural regression function, in which the context information of a word is encoded and aggregated from K observations. Furthermore, we propose to use Model-Agnostic Meta-Learning (MAML) for adapting the learned model to the new corpus fast and robustly. Experiments show that the proposed approach significantly outperforms existing methods in constructing an accurate embedding for OOV words and improves downstream tasks when the embedding is utilized",
    "volume": "main",
    "checked": true,
    "id": "23f40946f0be0c0428f85d7b48fac4c260e1a9cf",
    "citation_count": 51
  },
  "https://aclanthology.org/P19-1403": {
    "title": "Neural Temporality Adaptation for Document Classification: Diachronic Word Embeddings and Domain Adaptation Models",
    "abstract": "Language usage can change across periods of time, but document classifiers models are usually trained and tested on corpora spanning multiple years without considering temporal variations. This paper describes two complementary ways to adapt classifiers to shifts across time. First, we show that diachronic word embeddings, which were originally developed to study language change, can also improve document classification, and we show a simple method for constructing this type of embedding. Second, we propose a time-driven neural classification model inspired by methods for domain adaptation. Experiments on six corpora show how these methods can make classifiers more robust over time",
    "volume": "main",
    "checked": true,
    "id": "ed6793d15cf94945adec0dad8bcaf9290ecbae13",
    "citation_count": 23
  },
  "https://aclanthology.org/P19-1404": {
    "title": "Learning Transferable Feature Representations Using Neural Networks",
    "abstract": "Learning representations such that the source and target distributions appear as similar as possible has benefited transfer learning tasks across several applications. Generally it requires labeled data from the source and only unlabeled data from the target to learn such representations. While these representations act like a bridge to transfer knowledge learned in the source to the target; they may lead to negative transfer when the source specific characteristics detract their ability to represent the target data. We present a novel neural network architecture to simultaneously learn a two-part representation which is based on the principle of segregating source specific representation from the common representation. The first part captures the source specific characteristics while the second part captures the truly common representation. Our architecture optimizes an objective function which acts adversarial for the source specific part if it contributes towards the cross-domain learning. We empirically show that two parts of the representation, in different arrangements, outperforms existing learning algorithms on the source learning as well as cross-domain tasks on multiple datasets",
    "volume": "main",
    "checked": true,
    "id": "3ac4e5e444ad558575f51a4b547aefb756f3fe34",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-1405": {
    "title": "Bayes Test of Precision, Recall, and F1 Measure for Comparison of Two Natural Language Processing Models",
    "abstract": "Direct comparison on point estimation of the precision (P), recall (R), and F1 measure of two natural language processing (NLP) models on a common test corpus is unreasonable and results in less replicable conclusions due to a lack of a statistical test. However, the existing t-tests in cross-validation (CV) for model comparison are inappropriate because the distributions of P, R, F1 are skewed and an interval estimation of P, R, and F1 based on a t-test may exceed [0,1]. In this study, we propose to use a block-regularized 3×2 CV (3×2 BCV) in model comparison because it could regularize the difference in certain frequency distributions over linguistic units between training and validation sets and yield stable estimators of P, R, and F1. On the basis of the 3×2 BCV, we calibrate the posterior distributions of P, R, and F1 and derive an accurate interval estimation of P, R, and F1. Furthermore, we formulate the comparison into a hypothesis testing problem and propose a novel Bayes test. The test could directly compute the probabilities of the hypotheses on the basis of the posterior distributions and provide more informative decisions than the existing significance t-tests. Three experiments with regard to NLP chunking tasks are conducted, and the results illustrate the validity of the Bayes test",
    "volume": "main",
    "checked": true,
    "id": "26bd2ed1b00f99ea65012cb389e0b6a59847830b",
    "citation_count": 14
  },
  "https://aclanthology.org/P19-1406": {
    "title": "TIGS: An Inference Algorithm for Text Infilling with Gradient Search",
    "abstract": "Text infilling aims at filling in the missing part of a sentence or paragraph, which has been applied to a variety of real-world natural language generation scenarios. Given a well-trained sequential generative model, it is challenging for its unidirectional decoder to generate missing symbols conditioned on the past and future information around the missing part. In this paper, we propose an iterative inference algorithm based on gradient search, which could be the first inference algorithm that can be broadly applied to any neural sequence generative models for text infilling tasks. Extensive experimental comparisons show the effectiveness and efficiency of the proposed method on three different text infilling tasks with various mask ratios and different mask strategies, comparing with five state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "f4dbc630994cb71e16a5959431593583b3bd1574",
    "citation_count": 23
  },
  "https://aclanthology.org/P19-1407": {
    "title": "Keeping Notes: Conditional Natural Language Generation with a Scratchpad Encoder",
    "abstract": "We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output",
    "volume": "main",
    "checked": true,
    "id": "da7a1e2b3f0b0f1ccebcb10380e36e284b6eba9e",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1408": {
    "title": "Using Automatically Extracted Minimum Spans to Disentangle Coreference Evaluation from Boundary Detection",
    "abstract": "The common practice in coreference resolution is to identify and evaluate the maximum span of mentions. The use of maximum spans tangles coreference evaluation with the challenges of mention boundary detection like prepositional phrase attachment. To address this problem, minimum spans are manually annotated in smaller corpora. However, this additional annotation is costly and therefore, this solution does not scale to large corpora. In this paper, we propose the MINA algorithm for automatically extracting minimum spans to benefit from minimum span evaluation in all corpora. We show that the extracted minimum spans by MINA are consistent with those that are manually annotated by experts. Our experiments show that using minimum spans is in particular important in cross-dataset coreference evaluation, in which detected mention boundaries are noisier due to domain shift. We have integrated MINA into https://github.com/ns-moosavi/coval for reporting standard coreference scores based on both maximum and automatically detected minimum spans",
    "volume": "main",
    "checked": true,
    "id": "a30cb8d33dfeb79125d5e6cc4b17b169daf84326",
    "citation_count": 4
  },
  "https://aclanthology.org/P19-1409": {
    "title": "Revisiting Joint Modeling of Cross-document Entity and Event Coreference Resolution",
    "abstract": "Recognizing coreferring events and entities across multiple texts is crucial for many NLP applications. Despite the task’s importance, research focus was given mostly to within-document entity coreference, with rather little attention to the other variants. We propose a neural architecture for cross-document coreference resolution. Inspired by Lee et al. (2012), we jointly model entity and event coreference. We represent an event (entity) mention using its lexical span, surrounding context, and relation to entity (event) mentions via predicate-arguments structures. Our model outperforms the previous state-of-the-art event coreference model on ECB+, while providing the first entity coreference results on this corpus. Our analysis confirms that all our representation elements, including the mention span itself, its context, and the relation to other mentions contribute to the model’s success",
    "volume": "main",
    "checked": true,
    "id": "6cd9c11017595a2194cacfedb3ec4ecb30462287",
    "citation_count": 68
  },
  "https://aclanthology.org/P19-1410": {
    "title": "A Unified Linear-Time Framework for Sentence-Level Discourse Parsing",
    "abstract": "We propose an efficient neural framework for sentence-level discourse analysis in accordance with Rhetorical Structure Theory (RST). Our framework comprises a discourse segmenter to identify the elementary discourse units (EDU) in a text, and a discourse parser that constructs a discourse tree in a top-down fashion. Both the segmenter and the parser are based on Pointer Networks and operate in linear time. Our segmenter yields an F1 score of 95.4%, and our parser achieves an F1 score of 81.7% on the aggregated labeled (relation) metric, surpassing previous approaches by a good margin and approaching human agreement on both tasks (98.3 and 83.0 F1)",
    "volume": "main",
    "checked": true,
    "id": "a1f449ed8912d08876049bdbbb60d0b8a3416c33",
    "citation_count": 45
  },
  "https://aclanthology.org/P19-1411": {
    "title": "Employing the Correspondence of Relations and Connectives to Identify Implicit Discourse Relations via Label Embeddings",
    "abstract": "It has been shown that implicit connectives can be exploited to improve the performance of the models for implicit discourse relation recognition (IDRR). An important property of the implicit connectives is that they can be accurately mapped into the discourse relations conveying their functions. In this work, we explore this property in a multi-task learning framework for IDRR in which the relations and the connectives are simultaneously predicted, and the mapping is leveraged to transfer knowledge between the two prediction tasks via the embeddings of relations and connectives. We propose several techniques to enable such knowledge transfer that yield the state-of-the-art performance for IDRR on several settings of the benchmark dataset (i.e., the Penn Discourse Treebank dataset)",
    "volume": "main",
    "checked": true,
    "id": "961a810cada785735298c648141956bb7c37117d",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-1412": {
    "title": "Do You Know That Florence Is Packed with Visitors? Evaluating State-of-the-art Models of Speaker Commitment",
    "abstract": "When a speaker, Mary, asks “Do you know that Florence is packed with visitors?”, we take her to believe that Florence is packed with visitors, but not if she asks “Do you think that Florence is packed with visitors?”. Inferring speaker commitment (aka event factuality) is crucial for information extraction and question answering. Here, we explore the hypothesis that linguistic deficits drive the error patterns of existing speaker commitment models by analyzing the linguistic correlates of model error on a challenging naturalistic dataset. We evaluate two state-of-the-art speaker commitment models on the CommitmentBank, an English dataset of naturally occurring discourses. The CommitmentBank is annotated with speaker commitment towards the content of the complement (“Florence is packed with visitors” in our example) of clause-embedding verbs (“know”, “think”) under four entailment-canceling environments (negation, modal, question, conditional). A breakdown of items by linguistic features reveals asymmetrical error patterns: while the models achieve good performance on some classes (e.g., negation), they fail to generalize to the diverse linguistic constructions (e.g., conditionals) in natural language, highlighting directions for improvement",
    "volume": "main",
    "checked": true,
    "id": "56ed530013dfc77bef2564d2b8f72a92e474cfb3",
    "citation_count": 16
  },
  "https://aclanthology.org/P19-1413": {
    "title": "Multi-Relational Script Learning for Discourse Relations",
    "abstract": "Modeling script knowledge can be useful for a wide range of NLP tasks. Current statistical script learning approaches embed the events, such that their relationships are indicated by their similarity in the embedding. While intuitive, these approaches fall short of representing nuanced relations, needed for downstream tasks. In this paper, we suggest to view learning event embedding as a multi-relational problem, which allows us to capture different aspects of event pairs. We model a rich set of event relations, such as Cause and Contrast, derived from the Penn Discourse Tree Bank. We evaluate our model on three types of tasks, the popular Mutli-Choice Narrative Cloze and its variants, several multi-relational prediction tasks, and a related downstream task—implicit discourse sense classification",
    "volume": "main",
    "checked": true,
    "id": "333286c1c58d7a876eb74cf74f8a684648c17c22",
    "citation_count": 25
  },
  "https://aclanthology.org/P19-1414": {
    "title": "Open-Domain Why-Question Answering with Adversarial Learning to Encode Answer Texts",
    "abstract": "In this paper, we propose a method for why-question answering (why-QA) that uses an adversarial learning framework. Existing why-QA methods retrieve “answer passages” that usually consist of several sentences. These multi-sentence passages contain not only the reason sought by a why-question and its connection to the why-question, but also redundant and/or unrelated parts. We use our proposed “Adversarial networks for Generating compact-answer Representation” (AGR) to generate from a passage a vector representation of the non-redundant reason sought by a why-question and exploit the representation for judging whether the passage actually answers the why-question. Through a series of experiments using Japanese why-QA datasets, we show that these representations improve the performance of our why-QA neural model as well as that of a BERT-based why-QA model. We show that they also improve a state-of-the-art distantly supervised open-domain QA (DS-QA) method on publicly available English datasets, even though the target task is not a why-QA",
    "volume": "main",
    "checked": true,
    "id": "76bf1ee608dcf35753d52c7fa5076d90f30c2e69",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1415": {
    "title": "Learning to Ask Unanswerable Questions for Machine Reading Comprehension",
    "abstract": "Machine reading comprehension with unanswerable questions is a challenging task. In this work, we propose a data augmentation technique by automatically generating relevant unanswerable questions according to an answerable question paired with its corresponding paragraph that contains the answer. We introduce a pair-to-sequence model for unanswerable question generation, which effectively captures the interactions between the question and the paragraph. We also present a way to construct training data for our question generation models by leveraging the existing reading comprehension dataset. Experimental results show that the pair-to-sequence model performs consistently better compared with the sequence-to-sequence baseline. We further use the automatically generated unanswerable questions as a means of data augmentation on the SQuAD 2.0 dataset, yielding 1.9 absolute F1 improvement with BERT-base model and 1.7 absolute F1 improvement with BERT-large model",
    "volume": "main",
    "checked": true,
    "id": "02dbc43fb947eda8f3b83bc085b4deb0f07010f5",
    "citation_count": 28
  },
  "https://aclanthology.org/P19-1416": {
    "title": "Compositional Questions Do Not Necessitate Multi-hop Reasoning",
    "abstract": "Multi-hop reading comprehension (RC) questions are challenging because they require reading and reasoning over multiple paragraphs. We argue that it can be difficult to construct large multi-hop RC datasets. For example, even highly compositional questions can be answered with a single hop if they target specific entity types, or the facts needed to answer them are redundant. Our analysis is centered on HotpotQA, where we show that single-hop reasoning can solve much more of the dataset than previously thought. We introduce a single-hop BERT-based RC model that achieves 67 F1—comparable to state-of-the-art multi-hop models. We also design an evaluation setting where humans are not shown all of the necessary paragraphs for the intended multi-hop reasoning but can still answer over 80% of questions. Together with detailed error analysis, these results suggest there should be an increasing focus on the role of evidence in multi-hop reasoning and possibly even a shift towards information retrieval style evaluations with large and diverse evidence collections",
    "volume": "main",
    "checked": true,
    "id": "95f2aedef1453e5d88af9d5fddb79e0e56223cb0",
    "citation_count": 113
  },
  "https://aclanthology.org/P19-1417": {
    "title": "Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader",
    "abstract": "We propose a new end-to-end question answering model, which learns to aggregate answer evidence from an incomplete knowledge base (KB) and a set of retrieved text snippets.Under the assumptions that structured data is easier to query and the acquired knowledge can help the understanding of unstructured text, our model first accumulates knowledge ofKB entities from a question-related KB sub-graph; then reformulates the question in the latent space and reads the text with the accumulated entity knowledge at hand. The evidence from KB and text are finally aggregated to predict answers. On the widely-used KBQA benchmark WebQSP, our model achieves consistent improvements across settings with different extents of KB incompleteness",
    "volume": "main",
    "checked": true,
    "id": "750fee51c6498ed73729c72a4ffad46d98bfbc31",
    "citation_count": 72
  },
  "https://aclanthology.org/P19-1418": {
    "title": "AdaNSP: Uncertainty-driven Adaptive Decoding in Neural Semantic Parsing",
    "abstract": "Neural semantic parsers utilize the encoder-decoder framework to learn an end-to-end model for semantic parsing that transduces a natural language sentence to the formal semantic representation. To keep the model aware of the underlying grammar in target sequences, many constrained decoders were devised in a multi-stage paradigm, which decode to the sketches or abstract syntax trees first, and then decode to target semantic tokens. We instead to propose an adaptive decoding method to avoid such intermediate representations. The decoder is guided by model uncertainty and automatically uses deeper computations when necessary. Thus it can predict tokens adaptively. Our model outperforms the state-of-the-art neural models and does not need any expertise like predefined grammar or sketches in the meantime",
    "volume": "main",
    "checked": true,
    "id": "421f9f3c19e2a6e99149280ae6259990ce98803a",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1419": {
    "title": "The Language of Legal and Illegal Activity on the Darknet",
    "abstract": "The non-indexed parts of the Internet (the Darknet) have become a haven for both legal and illegal anonymous activity. Given the magnitude of these networks, scalably monitoring their activity necessarily relies on automated tools, and notably on NLP tools. However, little is known about what characteristics texts communicated through the Darknet have, and how well do off-the-shelf NLP tools do on this domain. This paper tackles this gap and performs an in-depth investigation of the characteristics of legal and illegal text in the Darknet, comparing it to a clear net website with similar content as a control condition. Taking drugs-related websites as a test case, we find that texts for selling legal and illegal drugs have several linguistic characteristics that distinguish them from one another, as well as from the control condition, among them the distribution of POS tags, and the coverage of their named entities in Wikipedia",
    "volume": "main",
    "checked": true,
    "id": "786c27b75eec2e9bdfc6d49cf80d2c5e6bcd251a",
    "citation_count": 10
  },
  "https://aclanthology.org/P19-1420": {
    "title": "Eliciting Knowledge from Experts: Automatic Transcript Parsing for Cognitive Task Analysis",
    "abstract": "Cognitive task analysis (CTA) is a type of analysis in applied psychology aimed at eliciting and representing the knowledge and thought processes of domain experts. In CTA, often heavy human labor is involved to parse the interview transcript into structured knowledge (e.g., flowchart for different actions). To reduce human efforts and scale the process, automated CTA transcript parsing is desirable. However, this task has unique challenges as (1) it requires the understanding of long-range context information in conversational text; and (2) the amount of labeled data is limited and indirect—i.e., context-aware, noisy, and low-resource. In this paper, we propose a weakly-supervised information extraction framework for automated CTA transcript parsing. We partition the parsing process into a sequence labeling task and a text span-pair relation extraction task, with distant supervision from human-curated protocol files. To model long-range context information for extracting sentence relations, neighbor sentences are involved as a part of input. Different types of models for capturing context dependency are then applied. We manually annotate real-world CTA transcripts to facilitate the evaluation of the parsing tasks",
    "volume": "main",
    "checked": true,
    "id": "b9c40949ddccf968fbbf2353beff6d7e5a400d34",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-1421": {
    "title": "Course Concept Expansion in MOOCs with External Knowledge and Interactive Game",
    "abstract": "As Massive Open Online Courses (MOOCs) become increasingly popular, it is promising to automatically provide extracurricular knowledge for MOOC users. Suffering from semantic drifts and lack of knowledge guidance, existing methods can not effectively expand course concepts in complex MOOC environments. In this paper, we first build a novel boundary during searching for new concepts via external knowledge base and then utilize heterogeneous features to verify the high-quality results. In addition, to involve human efforts in our model, we design an interactive optimization mechanism based on a game. Our experiments on the four datasets from Coursera and XuetangX show that the proposed method achieves significant improvements(+0.19 by MAP) over existing methods",
    "volume": "main",
    "checked": true,
    "id": "c316aff2bfb25a3066960e511f82a7a8a910a516",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1422": {
    "title": "Towards Near-imperceptible Steganographic Text",
    "abstract": "We show that the imperceptibility of several existing linguistic steganographic systems (Fang et al., 2017; Yang et al., 2018) relies on implicit assumptions on statistical behaviors of fluent text. We formally analyze them and empirically evaluate these assumptions. Furthermore, based on these observations, we propose an encoding algorithm called patient-Huffman with improved near-imperceptible guarantees",
    "volume": "main",
    "checked": true,
    "id": "81ba3a1fccdf87bfa2860cfc5916cf75f2d48449",
    "citation_count": 19
  },
  "https://aclanthology.org/P19-1423": {
    "title": "Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network",
    "abstract": "Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction",
    "volume": "main",
    "checked": true,
    "id": "358ca777d9992bdc06fdcc1940e3b18a8da68878",
    "citation_count": 110
  },
  "https://aclanthology.org/P19-1424": {
    "title": "Neural Legal Judgment Prediction in English",
    "abstract": "Legal judgment prediction is the task of automatically predicting the outcome of a court case, given a text describing the case’s facts. Previous work on using neural models for this task has focused on Chinese; only feature-based models (e.g., using bags of words and topics) have been considered in English. We release a new English legal judgment prediction dataset, containing cases from the European Court of Human Rights. We evaluate a broad variety of neural models on the new dataset, establishing strong baselines that surpass previous feature-based models in three tasks: (1) binary violation classification; (2) multi-label classification; (3) case importance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT’s length limitation",
    "volume": "main",
    "checked": true,
    "id": "aca16f64ddbf187f8944118c8f72777c3d682521",
    "citation_count": 115
  },
  "https://aclanthology.org/P19-1425": {
    "title": "Robust Neural Machine Translation with Doubly Adversarial Inputs",
    "abstract": "Neural machine translation (NMT) often suffers from the vulnerability to noisy perturbations in the input. We propose an approach to improving the robustness of NMT models, which consists of two parts: (1) attack the translation model with adversarial source examples; (2) defend the translation model with adversarial target inputs to improve its robustness against the adversarial source inputs. For the generation of adversarial inputs, we propose a gradient-based method to craft adversarial examples informed by the translation loss over the clean inputs. Experimental results on Chinese-English and English-German translation tasks demonstrate that our approach achieves significant improvements (2.8 and 1.6 BLEU points) over Transformer on standard clean benchmarks as well as exhibiting higher robustness on noisy data",
    "volume": "main",
    "checked": true,
    "id": "e364a32064235297e6bcf7b86aeb73679527222c",
    "citation_count": 172
  },
  "https://aclanthology.org/P19-1426": {
    "title": "Bridging the Gap between Training and Inference for Neural Machine Translation",
    "abstract": "Neural Machine Translation (NMT) generates target words sequentially in the way of predicting the next word conditioned on the context words. At training time, it predicts with the ground truth words as context while at inference it has to generate the entire sequence from scratch. This discrepancy of the fed context leads to error accumulation among the way. Furthermore, word-level training requires strict matching between the generated sequence and the ground truth sequence which leads to overcorrection over different but reasonable translations. In this paper, we address these issues by sampling context words not only from the ground truth sequence but also from the predicted sequence by the model during training, where the predicted sequence is selected with a sentence-level optimum. Experiment results on Chinese->English and WMT’14 English->German translation tasks demonstrate that our approach can achieve significant improvements on multiple datasets",
    "volume": "main",
    "checked": true,
    "id": "31f531484e963da69c21f0d72ae0d59142d9e7db",
    "citation_count": 165
  },
  "https://aclanthology.org/P19-1427": {
    "title": "Beyond BLEU:Training Neural Machine Translation with Semantic Similarity",
    "abstract": "While most neural machine translation (NMT)systems are still trained using maximum likelihood estimation, recent work has demonstrated that optimizing systems to directly improve evaluation metrics such as BLEU can significantly improve final translation accuracy. However, training with BLEU has some limitations: it doesn’t assign partial credit, it has a limited range of output values, and it can penalize semantically correct hypotheses if they differ lexically from the reference. In this paper, we introduce an alternative reward function for optimizing NMT systems that is based on recent work in semantic similarity. We evaluate on four disparate languages trans-lated to English, and find that training with our proposed metric results in better translations as evaluated by BLEU, semantic similarity, and human evaluation, and also that the optimization procedure converges faster. Analysis suggests that this is because the proposed metric is more conducive to optimization, assigning partial credit and providing more diversity in scores than BLEU",
    "volume": "main",
    "checked": true,
    "id": "d1d23675d2e65cd734f2955c10ec1028b1139b5b",
    "citation_count": 70
  },
  "https://aclanthology.org/P19-1428": {
    "title": "AutoML Strategy Based on Grammatical Evolution: A Case Study about Knowledge Discovery from Text",
    "abstract": "The process of extracting knowledge from natural language text poses a complex problem that requires both a combination of machine learning techniques and proper feature selection. Recent advances in Automatic Machine Learning (AutoML) provide effective tools to explore large sets of algorithms, hyper-parameters and features to find out the most suitable combination of them. This paper proposes a novel AutoML strategy based on probabilistic grammatical evolution, which is evaluated on the health domain by facing the knowledge discovery challenge in Spanish text documents. Our approach achieves state-of-the-art results and provides interesting insights into the best combination of parameters and algorithms to use when dealing with this challenge. Source code is provided for the research community",
    "volume": "main",
    "checked": true,
    "id": "596c5016b4c6541419c20d72b7e7d4d238402bfe",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-1429": {
    "title": "Distilling Discrimination and Generalization Knowledge for Event Detection via Delta-Representation Learning",
    "abstract": "Event detection systems rely on discrimination knowledge to distinguish ambiguous trigger words and generalization knowledge to detect unseen/sparse trigger words. Current neural event detection approaches focus on trigger-centric representations, which work well on distilling discrimination knowledge, but poorly on learning generalization knowledge. To address this problem, this paper proposes a Delta-learning approach to distill discrimination and generalization knowledge by effectively decoupling, incrementally learning and adaptively fusing event representation. Experiments show that our method significantly outperforms previous approaches on unseen/sparse trigger words, and achieves state-of-the-art performance on both ACE2005 and KBP2017 datasets",
    "volume": "main",
    "checked": true,
    "id": "c1bf827abaca1131d7120fec1cff25a2ec4e82ca",
    "citation_count": 29
  },
  "https://aclanthology.org/P19-1430": {
    "title": "Chinese Relation Extraction with Multi-Grained Information and External Linguistic Knowledge",
    "abstract": "Chinese relation extraction is conducted using neural networks with either character-based or word-based inputs, and most existing methods typically suffer from segmentation errors and ambiguity of polysemy. To address the issues, we propose a multi-grained lattice framework (MG lattice) for Chinese relation extraction to take advantage of multi-grained language information and external linguistic knowledge. In this framework, (1) we incorporate word-level information into character sequence inputs so that segmentation errors can be avoided. (2) We also model multiple senses of polysemous words with the help of external linguistic knowledge, so as to alleviate polysemy ambiguity. Experiments on three real-world datasets in distinct domains show consistent and significant superiority and robustness of our model, as compared with other baselines. We will release the source code of this paper in the future",
    "volume": "main",
    "checked": true,
    "id": "7334f45c06555d4b6bf7e6b4437574c11369697e",
    "citation_count": 48
  },
  "https://aclanthology.org/P19-1431": {
    "title": "A2N: Attending to Neighbors for Knowledge Graph Inference",
    "abstract": "State-of-the-art models for knowledge graph completion aim at learning a fixed embedding representation of entities in a multi-relational graph which can generalize to infer unseen entity relationships at test time. This can be sub-optimal as it requires memorizing and generalizing to all possible entity relationships using these fixed representations. We thus propose a novel attention-based method to learn query-dependent representation of entities which adaptively combines the relevant graph neighborhood of an entity leading to more accurate KG completion. The proposed method is evaluated on two benchmark datasets for knowledge graph completion, and experimental results show that the proposed model performs competitively or better than existing state-of-the-art, including recent methods for explicit multi-hop reasoning. Qualitative probing offers insight into how the model can reason about facts involving multiple hops in the knowledge graph, through the use of neighborhood attention",
    "volume": "main",
    "checked": true,
    "id": "634c747829bb4529370775180cc254883e418ae9",
    "citation_count": 64
  },
  "https://aclanthology.org/P19-1432": {
    "title": "Graph based Neural Networks for Event Factuality Prediction using Syntactic and Semantic Structures",
    "abstract": "Event factuality prediction (EFP) is the task of assessing the degree to which an event mentioned in a sentence has happened. For this task, both syntactic and semantic information are crucial to identify the important context words. The previous work for EFP has only combined these information in a simple way that cannot fully exploit their coordination. In this work, we introduce a novel graph-based neural network for EFP that can integrate the semantic and syntactic information more effectively. Our experiments demonstrate the advantage of the proposed model for EFP",
    "volume": "main",
    "checked": true,
    "id": "37d8e064df4e921bcb7b80b6d4c3ee7488a027e0",
    "citation_count": 33
  },
  "https://aclanthology.org/P19-1433": {
    "title": "Embedding Time Expressions for Deep Temporal Ordering Models",
    "abstract": "Data-driven models have demonstrated state-of-the-art performance in inferring the temporal ordering of events in text. However, these models often overlook explicit temporal signals, such as dates and time windows. Rule-based methods can be used to identify the temporal links between these time expressions (timexes), but they fail to capture timexes’ interactions with events and are hard to integrate with the distributed representations of neural net models. In this paper, we introduce a framework to infuse temporal awareness into such models by learning a pre-trained model to embed timexes. We generate synthetic data consisting of pairs of timexes, then train a character LSTM to learn embeddings and classify the timexes’ temporal relation. We evaluate the utility of these embeddings in the context of a strong neural model for event temporal ordering, and show a small increase in performance on the MATRES dataset and more substantial gains on an automatically collected dataset with more frequent event-timex interactions",
    "volume": "main",
    "checked": true,
    "id": "4075a87634d541c52ac83c2729c3f3ae93c6abf6",
    "citation_count": 13
  },
  "https://aclanthology.org/P19-1434": {
    "title": "Episodic Memory Reader: Learning What to Remember for Question Answering from Streaming Data",
    "abstract": "We consider a novel question answering (QA) task where the machine needs to read from large streaming data (long documents or videos) without knowing when the questions will be given, which is difficult to solve with existing QA methods due to their lack of scalability. To tackle this problem, we propose a novel end-to-end deep network model for reading comprehension, which we refer to as Episodic Memory Reader (EMR) that sequentially reads the input contexts into an external memory, while replacing memories that are less important for answering unseen questions. Specifically, we train an RL agent to replace a memory entry when the memory is full, in order to maximize its QA accuracy at a future timepoint, while encoding the external memory using either the GRU or the Transformer architecture to learn representations that considers relative importance between the memory entries. We validate our model on a synthetic dataset (bAbI) as well as real-world large-scale textual QA (TriviaQA) and video QA (TVQA) datasets, on which it achieves significant improvements over rule based memory scheduling policies or an RL based baseline that independently learns the query-specific importance of each memory",
    "volume": "main",
    "checked": true,
    "id": "4b47e7376666a87374b1254469723603039333fa",
    "citation_count": 8
  },
  "https://aclanthology.org/P19-1435": {
    "title": "Selection Bias Explorations and Debias Methods for Natural Language Sentence Matching Datasets",
    "abstract": "Natural Language Sentence Matching (NLSM) has gained substantial attention from both academics and the industry, and rich public datasets contribute a lot to this process. However, biased datasets can also hurt the generalization performance of trained models and give untrustworthy evaluation results. For many NLSM datasets, the providers select some pairs of sentences into the datasets, and this sampling procedure can easily bring unintended pattern, i.e., selection bias. One example is the QuoraQP dataset, where some content-independent naive features are unreasonably predictive. Such features are the reflection of the selection bias and termed as the “leakage features.” In this paper, we investigate the problem of selection bias on six NLSM datasets and find that four out of them are significantly biased. We further propose a training and evaluation framework to alleviate the bias. Experimental results on QuoraQP suggest that the proposed framework can improve the generalization ability of trained models, and give more trustworthy evaluation results for real-world adoptions",
    "volume": "main",
    "checked": true,
    "id": "d296414c6dcec961e56eabadfc6ee6cce56d4605",
    "citation_count": 25
  },
  "https://aclanthology.org/P19-1436": {
    "title": "Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index",
    "abstract": "Existing open-domain question answering (QA) models are not suitable for real-time usage because they need to process several long documents on-demand for every input query, which is computationally prohibitive. In this paper, we introduce query-agnostic indexable representations of document phrases that can drastically speed up open-domain QA. In particular, our dense-sparse phrase encoding effectively captures syntactic, semantic, and lexical information of the phrases and eliminates the pipeline filtering of context documents. Leveraging strategies for optimizing training and inference time, our model can be trained and deployed even in a single 4-GPU server. Moreover, by representing phrases as pointers to their start and end tokens, our model indexes phrases in the entire English Wikipedia (up to 60 billion phrases) using under 2TB. Our experiments on SQuAD-Open show that our model is on par with or more accurate than previous models with 6000x reduced computational cost, which translates into at least 68x faster end-to-end inference benchmark on CPUs. Code and demo are available at nlp.cs.washington.edu/denspi",
    "volume": "main",
    "checked": true,
    "id": "b29db655a18e7417e1188ba392a06b6314f0cb87",
    "citation_count": 103
  },
  "https://aclanthology.org/P19-1437": {
    "title": "Language Modeling with Shared Grammar",
    "abstract": "Sequential recurrent neural networks have achieved superior performance on language modeling, but overlook the structure information in natural language. Recent works on structure-aware models have shown promising results on language modeling. However, how to incorporate structure knowledge on corpus without syntactic annotations remains an open problem. In this work, we propose neural variational language model (NVLM), which enables the sharing of grammar knowledge among different corpora. Experimental results demonstrate the effectiveness of our framework on two popular benchmark datasets. With the help of shared grammar, our language model converges significantly faster to a lower perplexity on new training corpus",
    "volume": "main",
    "checked": true,
    "id": "7284e1b540a5c6e368c6616866d5068516c33eeb",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-1438": {
    "title": "Zero-Shot Semantic Parsing for Instructions",
    "abstract": "We consider a zero-shot semantic parsing task: parsing instructions into compositional logical forms, in domains that were not seen during training. We present a new dataset with 1,390 examples from 7 application domains (e.g. a calendar or a file manager), each example consisting of a triplet: (a) the application’s initial state, (b) an instruction, to be carried out in the context of that state, and (c) the state of the application after carrying out the instruction. We introduce a new training algorithm that aims to train a semantic parser on examples from a set of source domains, so that it can effectively parse instructions from an unknown target domain. We integrate our algorithm into the floating parser of Pasupat and Liang (2015), and further augment the parser with features and a logical form candidate filtering logic, to support zero-shot adaptation. Our experiments with various zero-shot adaptation setups demonstrate substantial performance gains over a non-adapted parser",
    "volume": "main",
    "checked": true,
    "id": "ac82cc36382db4722170a053720c4fe33f1e6c87",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-1439": {
    "title": "Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling",
    "abstract": "Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target tasks. In addition, fine-tuning BERT on an intermediate task often negatively impacts downstream transfer. In a more positive trend, we see modest gains from multitask training, suggesting the development of more sophisticated multitask and transfer learning techniques as an avenue for further research",
    "volume": "main",
    "checked": true,
    "id": "06a1bf4a7333bbc78dbd7470666b33bd9e26882b",
    "citation_count": 82
  },
  "https://aclanthology.org/P19-1440": {
    "title": "Complex Question Decomposition for Semantic Parsing",
    "abstract": "In this work, we focus on complex question semantic parsing and propose a novel Hierarchical Semantic Parsing (HSP) method, which utilizes the decompositionality of complex questions for semantic parsing. Our model is designed within a three-stage parsing architecture based on the idea of decomposition-integration. In the first stage, we propose a question decomposer which decomposes a complex question into a sequence of sub-questions. In the second stage, we design an information extractor to derive the type and predicate information of these questions. In the last stage, we integrate the generated information from previous stages and generate a logical form for the complex question. We conduct experiments on COMPLEXWEBQUESTIONS which is a large scale complex question semantic parsing dataset, results show that our model achieves significant improvement compared to state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "6d5ae956f59071a9f3a0003f793e056791376752",
    "citation_count": 19
  },
  "https://aclanthology.org/P19-1441": {
    "title": "Multi-Task Deep Neural Networks for Natural Language Understanding",
    "abstract": "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available",
    "volume": "main",
    "checked": true,
    "id": "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac",
    "citation_count": 841
  },
  "https://aclanthology.org/P19-1442": {
    "title": "DisSent: Learning Sentence Representations from Explicit Discourse Relations",
    "abstract": "Learning effective representations of sentences is one of the core missions of natural language understanding. Existing models either train on a vast amount of text, or require costly, manually curated sentence relation datasets. We show that with dependency parsing and rule-based rubrics, we can curate a high quality sentence relation task by leveraging explicit discourse relations. We show that our curated dataset provides an excellent signal for learning vector representations of sentence meaning, representing relations that can only be determined when the meanings of two sentences are combined. We demonstrate that the automatically curated corpus allows a bidirectional LSTM sentence encoder to yield high quality sentence embeddings and can serve as a supervised fine-tuning dataset for larger models such as BERT. Our fixed sentence embeddings achieve high performance on a variety of transfer tasks, including SentEval, and we achieve state-of-the-art results on Penn Discourse Treebank’s implicit relation prediction task",
    "volume": "main",
    "checked": true,
    "id": "eaea866271ce0bdf1b9bf8e11581a66ec58806c6",
    "citation_count": 66
  },
  "https://aclanthology.org/P19-1443": {
    "title": "SParC: Cross-Domain Semantic Parsing in Context",
    "abstract": "We present SParC, a dataset for cross-domainSemanticParsing inContext that consists of 4,298 coherent question sequences (12k+ individual questions annotated with SQL queries). It is obtained from controlled user interactions with 200 complex databases over 138 domains. We provide an in-depth analysis of SParC and show that it introduces new challenges compared to existing datasets. SParC demonstrates complex contextual dependencies, (2) has greater semantic diversity, and (3) requires generalization to unseen domains due to its cross-domain nature and the unseen databases at test time. We experiment with two state-of-the-art text-to-SQL models adapted to the context-dependent, cross-domain setup. The best model obtains an exact match accuracy of 20.2% over all questions and less than10% over all interaction sequences, indicating that the cross-domain setting and the con-textual phenomena of the dataset present significant challenges for future research. The dataset, baselines, and leaderboard are released at https://yale-lily.github.io/sparc",
    "volume": "main",
    "checked": true,
    "id": "f79af5e9e96f9c777e8759d791e33e9da83ffa65",
    "citation_count": 88
  },
  "https://aclanthology.org/P19-1444": {
    "title": "Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation",
    "abstract": "We present a neural approach called IRNet for complex and cross-domain Text-to-SQL. IRNet aims to address two challenges: 1) the mismatch between intents expressed in natural language (NL) and the implementation details in SQL; 2) the challenge in predicting columns caused by the large number of out-of-domain words. Instead of end-to-end synthesizing a SQL query, IRNet decomposes the synthesis process into three phases. In the first phase, IRNet performs a schema linking over a question and a database schema. Then, IRNet adopts a grammar-based neural model to synthesize a SemQL query which is an intermediate representation that we design to bridge NL and SQL. Finally, IRNet deterministically infers a SQL query from the synthesized SemQL query with domain knowledge. On the challenging Text-to-SQL benchmark Spider, IRNet achieves 46.7% accuracy, obtaining 19.5% absolute improvement over previous state-of-the-art approaches. At the time of writing, IRNet achieves the first position on the Spider leaderboard",
    "volume": "main",
    "checked": true,
    "id": "3c89068551a84430a0b1fdfeb8d963e2b2fc7ecc",
    "citation_count": 205
  },
  "https://aclanthology.org/P19-1445": {
    "title": "EigenSent: Spectral sentence embeddings using higher-order Dynamic Mode Decomposition",
    "abstract": "Distributed representation of words, or word embeddings, have motivated methods for calculating semantic representations of word sequences such as phrases, sentences and paragraphs. Most of the existing methods to do so either use algorithms to learn such representations, or improve on calculating weighted averages of the word vectors. In this work, we experiment with spectral methods of signal representation and summarization as mechanisms for constructing such word-sequence embeddings in an unsupervised fashion. In particular, we explore an algorithm rooted in fluid-dynamics, known as higher-order Dynamic Mode Decomposition, which is designed to capture the eigenfrequencies, and hence the fundamental transition dynamics, of periodic and quasi-periodic systems. It is empirically observed that this approach, which we call EigenSent, can summarize transitions in a sequence of words and generate an embedding that can represent well the sequence itself. To the best of the authors’ knowledge, this is the first application of a spectral decomposition and signal summarization technique on text, to create sentence embeddings. We test the efficacy of this algorithm in creating sentence embeddings on three public datasets, where it performs appreciably well. Moreover it is also shown that, due to the positive combination of their complementary properties, concatenating the embeddings generated by EigenSent with simple word vector averaging achieves state-of-the-art results",
    "volume": "main",
    "checked": true,
    "id": "658d82710cff5afd00147275a02f075ec28aef05",
    "citation_count": 8
  },
  "https://aclanthology.org/P19-1446": {
    "title": "SemBleu: A Robust Metric for AMR Parsing Evaluation",
    "abstract": "Evaluating AMR parsing accuracy involves comparing pairs of AMR graphs. The major evaluation metric, SMATCH (Cai and Knight, 2013), searches for one-to-one mappings between the nodes of two AMRs with a greedy hill-climbing algorithm, which leads to search errors. We propose SEMBLEU, a robust metric that extends BLEU (Papineni et al., 2002) to AMRs. It does not suffer from search errors and considers non-local correspondences in addition to local ones. SEMBLEU is fully content-driven and punishes situations where a system’s output does not preserve most information from the input. Preliminary experiments on both sentence and corpus levels show that SEMBLEU has slightly higher consistency with human judgments than SMATCH. Our code is available at http://github.com/ freesunshine0316/sembleu",
    "volume": "main",
    "checked": true,
    "id": "a0c0e757dad75934d02febef25d978e097092571",
    "citation_count": 22
  },
  "https://aclanthology.org/P19-1447": {
    "title": "Reranking for Neural Semantic Parsing",
    "abstract": "Semantic parsing considers the task of transducing natural language (NL) utterances into machine executable meaning representations (MRs). While neural network-based semantic parsers have achieved impressive improvements over previous methods, results are still far from perfect, and cursory manual inspection can easily identify obvious problems such as lack of adequacy or coherence of the generated MRs. This paper presents a simple approach to quickly iterate and improve the performance of an existing neural semantic parser by reranking an n-best list of predicted MRs, using features that are designed to fix observed problems with baseline models. We implement our reranker in a competitive neural semantic parser and test on four semantic parsing (GEO, ATIS) and Python code generation (Django, CoNaLa) tasks, improving the strong baseline parser by up to 5.7% absolute in BLEU (CoNaLa) and 2.9% in accuracy (Django), outperforming the best published neural parser results on all four datasets",
    "volume": "main",
    "checked": true,
    "id": "3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c",
    "citation_count": 44
  },
  "https://aclanthology.org/P19-1448": {
    "title": "Representing Schema Structure with Graph Neural Networks for Text-to-SQL Parsing",
    "abstract": "Research on parsing language to SQL has largely ignored the structure of the database (DB) schema, either because the DB was very simple, or because it was observed at both training and test time. In spider, a recently-released text-to-SQL dataset, new and complex DBs are given at test time, and so the structure of the DB schema can inform the predicted SQL query. In this paper, we present an encoder-decoder semantic parser, where the structure of the DB schema is encoded with a graph neural network, and this representation is later used at both encoding and decoding time. Evaluation shows that encoding the schema structure improves our parser accuracy from 33.8% to 39.4%, dramatically above the current state of the art, which is at 19.7%",
    "volume": "main",
    "checked": true,
    "id": "c3e8686609e5bfc250e3785cba9979c1d0a2db04",
    "citation_count": 113
  },
  "https://aclanthology.org/P19-1449": {
    "title": "Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark",
    "abstract": "The GLUE benchmark (Wang et al., 2019b) is a suite of language understanding tasks which has seen dramatic progress in the past year, with average performance moving from 70.0 at launch to 83.9, state of the art at the time of writing (May 24, 2019). Here, we measure human performance on the benchmark, in order to learn whether significant headroom remains for further progress. We provide a conservative estimate of human performance on the benchmark through crowdsourcing: Our annotators are non-experts who must learn each task from a brief set of instructions and 20 examples. In spite of limited training, these annotators robustly outperform the state of the art on six of the nine GLUE tasks and achieve an average score of 87.1. Given the fast pace of progress however, the headroom we observe is quite limited. To reproduce the data-poor setting that our annotators must learn in, we also train the BERT model (Devlin et al., 2019) in limited-data regimes, and conclude that low-resource sentence classification remains a challenge for modern neural network approaches to text understanding",
    "volume": "main",
    "checked": true,
    "id": "4888102774ad93140391f3a26af0f54cfba5ec34",
    "citation_count": 37
  },
  "https://aclanthology.org/P19-1450": {
    "title": "Compositional Semantic Parsing across Graphbanks",
    "abstract": "Most semantic parsers that map sentences to graph-based meaning representations are hand-designed for specific graphbanks. We present a compositional neural semantic parser which achieves, for the first time, competitive accuracies across a diverse range of graphbanks. Incorporating BERT embeddings and multi-task learning improves the accuracy further, setting new states of the art on DM, PAS, PSD, AMR 2015 and EDS",
    "volume": "main",
    "checked": true,
    "id": "671a05535da65f9fc22800b5aa94795fc670ac45",
    "citation_count": 44
  },
  "https://aclanthology.org/P19-1451": {
    "title": "Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning",
    "abstract": "Our work involves enriching the Stack-LSTM transition-based AMR parser (Ballesteros and Al-Onaizan, 2017) by augmenting training with Policy Learning and rewarding the Smatch score of sampled graphs. In addition, we also combined several AMR-to-text alignments with an attention mechanism and we supplemented the parser with pre-processed concept identification, named entities and contextualized embeddings. We achieve a highly competitive performance that is comparable to the best published results. We show an in-depth study ablating each of the new components of the parser",
    "volume": "main",
    "checked": true,
    "id": "8cc369961ce9cd04f300ee0a81646556ee0626c3",
    "citation_count": 49
  },
  "https://aclanthology.org/P19-1452": {
    "title": "BERT Rediscovers the Classical NLP Pipeline",
    "abstract": "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations",
    "volume": "main",
    "checked": true,
    "id": "97906df07855b029b7aae7c2a1c6c5e8df1d531c",
    "citation_count": 843
  },
  "https://aclanthology.org/P19-1453": {
    "title": "Simple and Effective Paraphrastic Similarity from Parallel Translations",
    "abstract": "We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the time-consuming intermediate step of creating para-phrase corpora. Further, we show that the resulting model can be applied to cross lingual tasks where it both outperforms and is orders of magnitude faster than more complex state-of-the-art baselines",
    "volume": "main",
    "checked": true,
    "id": "86d84c1c9b0a500f930696ab27c83a4b30477560",
    "citation_count": 36
  },
  "https://aclanthology.org/P19-1454": {
    "title": "Second-Order Semantic Dependency Parsing with End-to-End Neural Networks",
    "abstract": "Semantic dependency parsing aims to identify semantic relationships between words in a sentence that form a graph. In this paper, we propose a second-order semantic dependency parser, which takes into consideration not only individual dependency edges but also interactions between pairs of edges. We show that second-order parsing can be approximated using mean field (MF) variational inference or loopy belief propagation (LBP). We can unfold both algorithms as recurrent layers of a neural network and therefore can train the parser in an end-to-end manner. Our experiments show that our approach achieves state-of-the-art performance",
    "volume": "main",
    "checked": true,
    "id": "ad1298322b3e4b75f63d3d08bf2387305f0254fa",
    "citation_count": 42
  },
  "https://aclanthology.org/P19-1455": {
    "title": "Towards Multimodal Sarcasm Detection (An _Obviously_ Perfect Paper)",
    "abstract": "Sarcasm is often expressed through several verbal and non-verbal cues, e.g., a change of tone, overemphasis in a word, a drawn-out syllable, or a straight looking face. Most of the recent work in sarcasm detection has been carried out on textual data. In this paper, we argue that incorporating multimodal cues can improve the automatic classification of sarcasm. As a first step towards enabling the development of multimodal approaches for sarcasm detection, we propose a new sarcasm dataset, Multimodal Sarcasm Detection Dataset (MUStARD), compiled from popular TV shows. MUStARD consists of audiovisual utterances annotated with sarcasm labels. Each utterance is accompanied by its context of historical utterances in the dialogue, which provides additional information on the scenario where the utterance occurs. Our initial results show that the use of multimodal information can reduce the relative error rate of sarcasm detection by up to 12.9% in F-score when compared to the use of individual modalities. The full dataset is publicly available for use at https://github.com/soujanyaporia/MUStARD",
    "volume": "main",
    "checked": true,
    "id": "6a040a5dcc223538b4c8dc775df891c97c9b4fbd",
    "citation_count": 95
  },
  "https://aclanthology.org/P19-1456": {
    "title": "Determining Relative Argument Specificity and Stance for Complex Argumentative Structures",
    "abstract": "Systems for automatic argument generation and debate require the ability to (1) determine the stance of any claims employed in the argument and (2) assess the specificity of each claim relative to the argument context. Existing work on understanding claim specificity and stance, however, has been limited to the study of argumentative structures that are relatively shallow, most often consisting of a single claim that directly supports or opposes the argument thesis. In this paper, we tackle these tasks in the context of complex arguments on a diverse set of topics. In particular, our dataset consists of manually curated argument trees for 741 controversial topics covering 95,312 unique claims; lines of argument are generally of depth 2 to 6. We find that as the distance between a pair of claims increases along the argument path, determining the relative specificity of a pair of claims becomes easier and determining their relative stance becomes harder",
    "volume": "main",
    "checked": true,
    "id": "33cc6d93cd86a488b00224cfbec1f31723d0693b",
    "citation_count": 10
  },
  "https://aclanthology.org/P19-1457": {
    "title": "Latent Variable Sentiment Grammar",
    "abstract": "Neural models have been investigated for sentiment classification over constituent trees. They learn phrase composition automatically by encoding tree structures but do not explicitly model sentiment composition, which requires to encode sentiment class labels. To this end, we investigate two formalisms with deep sentiment representations that capture sentiment subtype expressions by latent variables and Gaussian mixture vectors, respectively. Experiments on Stanford Sentiment Treebank (SST) show the effectiveness of sentiment grammar over vanilla neural encoders. Using ELMo embeddings, our method gives the best results on this benchmark",
    "volume": "main",
    "checked": true,
    "id": "498ef0cb6265817db4cfa65c2c5e5a7f50dcca44",
    "citation_count": 4
  },
  "https://aclanthology.org/P19-1458": {
    "title": "An Investigation of Transfer Learning-Based Sentiment Analysis in Japanese",
    "abstract": "Text classification approaches have usually required task-specific model architectures and huge labeled datasets. Recently, thanks to the rise of text-based transfer learning techniques, it is possible to pre-train a language model in an unsupervised manner and leverage them to perform effective on downstream tasks. In this work we focus on Japanese and show the potential use of transfer learning techniques in text classification. Specifically, we perform binary and multi-class sentiment classification on the Rakuten product review and Yahoo movie review datasets. We show that transfer learning-based approaches perform better than task-specific models trained on 3 times as much data. Furthermore, these approaches perform just as well for language modeling pre-trained on only 1/30 of the data. We release our pre-trained models and code as open source",
    "volume": "main",
    "checked": true,
    "id": "82d40215de43fbc2aa3b0f8c6ebba73f35e64c9b",
    "citation_count": 17
  },
  "https://aclanthology.org/P19-1459": {
    "title": "Probing Neural Network Comprehension of Natural Language Arguments",
    "abstract": "We are surprised to find that BERT’s peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work",
    "volume": "main",
    "checked": true,
    "id": "f3b89e9a2b8ce1b6058e6984c3556bc2dded0938",
    "citation_count": 320
  },
  "https://aclanthology.org/P19-1460": {
    "title": "Recognising Agreement and Disagreement between Stances with Reason Comparing Networks",
    "abstract": "We identify agreement and disagreement between utterances that express stances towards a topic of discussion. Existing methods focus mainly on conversational settings, where dialogic features are used for (dis)agreement inference. We extend this scope and seek to detect stance (dis)agreement in a broader setting, where independent stance-bearing utterances, which prevail in many stance corpora and real-world scenarios, are compared. To cope with such non-dialogic utterances, we find that the reasons uttered to back up a specific stance can help predict stance (dis)agreements. We propose a reason comparing network (RCN) to leverage reason information for stance comparison. Empirical results on a well-known stance corpus show that our method can discover useful reason information, enabling it to outperform several baselines in stance (dis)agreement detection",
    "volume": "main",
    "checked": true,
    "id": "1a235fdd2cebc44de8951f5d76221fe0b99f5f07",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-1461": {
    "title": "Toward Comprehensive Understanding of a Sentiment Based on Human Motives",
    "abstract": "In sentiment detection, the natural language processing community has focused on determining holders, facets, and valences, but has paid little attention to the reasons for sentiment decisions. Our work considers human motives as the driver for human sentiments and addresses the problem of motive detection as the first step. Following a study in psychology, we define six basic motives that cover a wide range of topics appearing in review texts, annotate 1,600 texts in restaurant and laptop domains with the motives, and report the performance of baseline methods on this new dataset. We also show that cross-domain transfer learning boosts detection performance, which indicates that these universal motives exist across different domains",
    "volume": "main",
    "checked": true,
    "id": "ce7fe36af1b382c6e544bbc09932edb4287295be",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1462": {
    "title": "Context-aware Embedding for Targeted Aspect-based Sentiment Analysis",
    "abstract": "Attention-based neural models were employed to detect the different aspects and sentiment polarities of the same target in targeted aspect-based sentiment analysis (TABSA). However, existing methods do not specifically pre-train reasonable embeddings for targets and aspects in TABSA. This may result in targets or aspects having the same vector representations in different contexts and losing the context-dependent information. To address this problem, we propose a novel method to refine the embeddings of targets and aspects. Such pivotal embedding refinement utilizes a sparse coefficient vector to adjust the embeddings of target and aspect from the context. Hence the embeddings of targets and aspects can be refined from the highly correlative words instead of using context-independent or randomly initialized vectors. Experiment results on two benchmark datasets show that our approach yields the state-of-the-art performance in TABSA task",
    "volume": "main",
    "checked": true,
    "id": "6924733e36dd1b63ddb7babc9a636b801d1c9983",
    "citation_count": 34
  },
  "https://aclanthology.org/P19-1463": {
    "title": "Yes, we can! Mining Arguments in 50 Years of US Presidential Campaign Debates",
    "abstract": "Political debates offer a rare opportunity for citizens to compare the candidates’ positions on the most controversial topics of the campaign. Thus they represent a natural application scenario for Argument Mining. As existing research lacks solid empirical investigation of the typology of argument components in political debates, we fill this gap by proposing an Argument Mining approach to political debates. We address this task in an empirical manner by annotating 39 political debates from the last 50 years of US presidential campaigns, creating a new corpus of 29k argument components, labeled as premises and claims. We then propose two tasks: (1) identifying the argumentative components in such debates, and (2) classifying them as premises and claims. We show that feature-rich SVM learners and Neural Network architectures outperform standard baselines in Argument Mining over such complex data. We release the new corpus USElecDeb60To16 and the accompanying software under free licenses to the research community",
    "volume": "main",
    "checked": true,
    "id": "400c5201ac5f01a1dd9a1d27fb4d4a126bb3ff4a",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1464": {
    "title": "An Empirical Study of Span Representations in Argumentation Structure Parsing",
    "abstract": "For several natural language processing (NLP) tasks, span representation design is attracting considerable attention as a promising new technique; a common basis for an effective design has been established. With such basis, exploring task-dependent extensions for argumentation structure parsing (ASP) becomes an interesting research direction. This study investigates (i) span representation originally developed for other NLP tasks and (ii) a simple task-dependent extension for ASP. Our extensive experiments and analysis show that these representations yield high performance for ASP and provide some challenging types of instances to be parsed",
    "volume": "main",
    "checked": true,
    "id": "a31efa6fc92eae8f387f1dc9a292bc91817310a3",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-1465": {
    "title": "Simple and Effective Text Matching with Richer Alignment Features",
    "abstract": "In this paper, we present a fast and strong neural approach for general purpose text matching applications. We explore what is sufficient to build a fast and well-performed text matching model and propose to keep three key features available for inter-sequence alignment: original point-wise features, previous aligned features, and contextual features while simplifying all the remaining components. We conduct experiments on four well-studied benchmark datasets across tasks of natural language inference, paraphrase identification and answer selection. The performance of our model is on par with the state-of-the-art on all datasets with much fewer parameters and the inference speed is at least 6 times faster compared with similarly performed ones",
    "volume": "main",
    "checked": true,
    "id": "7076b396c2cc8fe031de62138522051728bd5292",
    "citation_count": 98
  },
  "https://aclanthology.org/P19-1466": {
    "title": "Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs",
    "abstract": "The recent proliferation of knowledge graphs (KGs) coupled with incomplete or partial information, in the form of missing relations (links) between entities, has fueled a lot of research on knowledge base completion (also known as relation prediction). Several recent works suggest that convolutional neural network (CNN) based models generate richer and more expressive feature embeddings and hence also perform well on relation prediction. However, we observe that these KG embeddings treat triples independently and thus fail to cover the complex and hidden information that is inherently implicit in the local neighborhood surrounding a triple. To this effect, our paper proposes a novel attention-based feature embedding that captures both entity and relation features in any given entity’s neighborhood. Additionally, we also encapsulate relation clusters and multi-hop relations in our model. Our empirical study offers insights into the efficacy of our attention-based model and we show marked performance gains in comparison to state-of-the-art methods on all datasets",
    "volume": "main",
    "checked": true,
    "id": "fddd3dab90c243ab7fc038bc6449ef62c0e06037",
    "citation_count": 271
  },
  "https://aclanthology.org/P19-1467": {
    "title": "Neural Network Alignment for Sentential Paraphrases",
    "abstract": "We present a monolingual alignment system for long, sentence- or clause-level alignments, and demonstrate that systems designed for word- or short phrase-based alignment are ill-suited for these longer alignments. Our system is capable of aligning semantically similar spans of arbitrary length. We achieve significantly higher recall on aligning phrases of four or more words and outperform state-of-the- art aligners on the long alignments in the MSR RTE corpus",
    "volume": "main",
    "checked": true,
    "id": "3372c2bdc3aa8cf59bad64e7edaba243e26d4d88",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-1468": {
    "title": "Duality of Link Prediction and Entailment Graph Induction",
    "abstract": "Link prediction and entailment graph induction are often treated as different problems. In this paper, we show that these two problems are actually complementary. We train a link prediction model on a knowledge graph of assertions extracted from raw text. We propose an entailment score that exploits the new facts discovered by the link prediction model, and then form entailment graphs between relations. We further use the learned entailments to predict improved link prediction scores. Our results show that the two tasks can benefit from each other. The new entailment score outperforms prior state-of-the-art results on a standard entialment dataset and the new link prediction scores show improvements over the raw link prediction scores",
    "volume": "main",
    "checked": true,
    "id": "e5529a223176cbb19026a0ee7ae25521d9c39a88",
    "citation_count": 13
  },
  "https://aclanthology.org/P19-1469": {
    "title": "A Cross-Sentence Latent Variable Model for Semi-Supervised Text Sequence Matching",
    "abstract": "We present a latent variable model for predicting the relationship between a pair of text sequences. Unlike previous auto-encoding–based approaches that consider each sequence separately, our proposed framework utilizes both sequences within a single model by generating a sequence that has a given relationship with a source sequence. We further extend the cross-sentence generating framework to facilitate semi-supervised training. We also define novel semantic constraints that lead the decoder network to generate semantically plausible and diverse sequences. We demonstrate the effectiveness of the proposed model from quantitative and qualitative experiments, while achieving state-of-the-art results on semi-supervised natural language inference and paraphrase identification",
    "volume": "main",
    "checked": true,
    "id": "0b2c31a4f8c70873f5dc775bbb3064eff25b5cb1",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1470": {
    "title": "COMET: Commonsense Transformers for Automatic Knowledge Graph Construction",
    "abstract": "We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5% (ATOMIC) and 91.7% (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods",
    "volume": "main",
    "checked": true,
    "id": "f48ae425e2567be2d993efcaaf74c2274fc9d7c5",
    "citation_count": 508
  },
  "https://aclanthology.org/P19-1471": {
    "title": "Detecting Subevents using Discourse and Narrative Features",
    "abstract": "Recognizing the internal structure of events is a challenging language processing task of great importance for text understanding. We present a supervised model for automatically identifying when one event is a subevent of another. Building on prior work, we introduce several novel features, in particular discourse and narrative features, that significantly improve upon prior state-of-the-art performance. Error analysis further demonstrates the utility of these features. We evaluate our model on the only two annotated corpora with event hierarchies: HiEve and the Intelligence Community corpus. No prior system has been evaluated on both corpora. Our model outperforms previous systems on both corpora, achieving 0.74 BLANC F1 on the Intelligence Community corpus and 0.70 F1 on the HiEve corpus, respectively a 15 and 5 percentage point improvement over previous models",
    "volume": "main",
    "checked": true,
    "id": "2d6f6f2cdcf81af1702dc23c2947f10e749a79ae",
    "citation_count": 16
  },
  "https://aclanthology.org/P19-1472": {
    "title": "HellaSwag: Can a Machine Really Finish Your Sentence?",
    "abstract": "Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as “A woman sits at a piano,” a machine must select the most likely followup: “She sets her fingers on the keys.” With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical ‘Goldilocks’ zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges",
    "volume": "main",
    "checked": true,
    "id": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
    "citation_count": 304
  },
  "https://aclanthology.org/P19-1473": {
    "title": "Unified Semantic Parsing with Weak Supervision",
    "abstract": "Semantic parsing over multiple knowledge bases enables a parser to exploit structural similarities of programs across the multiple domains. However, the fundamental challenge lies in obtaining high-quality annotations of (utterance, program) pairs across various domains needed for training such models. To overcome this, we propose a novel framework to build a unified multi-domain enabled semantic parser trained only with weak supervision (denotations). Weakly supervised training is particularly arduous as the program search space grows exponentially in a multi-domain setting. To solve this, we incorporate a multi-policy distillation mechanism in which we first train domain-specific semantic parsers (teachers) using weak supervision in the absence of the ground truth programs, followed by training a single unified parser (student) from the domain specific policies obtained from these teachers. The resultant semantic parser is not only compact but also generalizes better, and generates more accurate programs. It further does not require the user to provide a domain label while querying. On the standard Overnight dataset (containing multiple domains), we demonstrate that the proposed model improves performance by 20% in terms of denotation accuracy in comparison to baseline techniques",
    "volume": "main",
    "checked": true,
    "id": "c195fcb97032ebd32443bfa5603abcf950dbc61a",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1474": {
    "title": "Every Child Should Have Parents: A Taxonomy Refinement Algorithm Based on Hyperbolic Term Embeddings",
    "abstract": "We introduce the use of Poincaré embeddings to improve existing state-of-the-art approaches to domain-specific taxonomy induction from text as a signal for both relocating wrong hyponym terms within a (pre-induced) taxonomy as well as for attaching disconnected terms in a taxonomy. This method substantially improves previous state-of-the-art results on the SemEval-2016 Task 13 on taxonomy extraction. We demonstrate the superiority of Poincaré embeddings over distributional semantic representations, supporting the hypothesis that they can better capture hierarchical lexical-semantic relationships than embeddings in the Euclidean space",
    "volume": "main",
    "checked": true,
    "id": "e3d1c2b392f19d197268981e5d021ffe40bfe728",
    "citation_count": 32
  },
  "https://aclanthology.org/P19-1475": {
    "title": "Learning to Rank for Plausible Plausibility",
    "abstract": "Researchers illustrate improvements in contextual encoding strategies via resultant performance on a battery of shared Natural Language Understanding (NLU) tasks. Many of these tasks are of a categorical prediction variety: given a conditioning context (e.g., an NLI premise), provide a label based on an associated prompt (e.g., an NLI hypothesis). The categorical nature of these tasks has led to common use of a cross entropy log-loss objective during training. We suggest this loss is intuitively wrong when applied to plausibility tasks, where the prompt by design is neither categorically entailed nor contradictory given the context. Log-loss naturally drives models to assign scores near 0.0 or 1.0, in contrast to our proposed use of a margin-based loss. Following a discussion of our intuition, we describe a confirmation study based on an extreme, synthetically curated task derived from MultiNLI. We find that a margin-based loss leads to a more plausible model of plausibility. Finally, we illustrate improvements on the Choice Of Plausible Alternative (COPA) task through this change in loss",
    "volume": "main",
    "checked": true,
    "id": "1b04f7bc98c3710953d9012bd3ab04d218b8959c",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1476": {
    "title": "Generalized Tuning of Distributional Word Vectors for Monolingual and Cross-Lingual Lexical Entailment",
    "abstract": "Lexical entailment (LE; also known as hyponymy-hypernymy or is-a relation) is a core asymmetric lexical relation that supports tasks like taxonomy induction and text generation. In this work, we propose a simple and effective method for fine-tuning distributional word vectors for LE. Our Generalized Lexical ENtailment model (GLEN) is decoupled from the word embedding model and applicable to any distributional vector space. Yet – unlike existing retrofitting models – it captures a general specialization function allowing for LE-tuning of the entire distributional space and not only the vectors of words seen in lexical constraints. Coupled with a multilingual embedding space, GLEN seamlessly enables cross-lingual LE detection. We demonstrate the effectiveness of GLEN in graded LE and report large improvements (over 20% in accuracy) over state-of-the-art in cross-lingual LE detection",
    "volume": "main",
    "checked": true,
    "id": "82e451933e15c9e2d725be87c0df2f41c0f9e57a",
    "citation_count": 13
  },
  "https://aclanthology.org/P19-1477": {
    "title": "Attention Is (not) All You Need for Commonsense Reasoning",
    "abstract": "The recently introduced BERT model exhibits strong performance on several language understanding benchmarks. In this paper, we describe a simple re-implementation of BERT for commonsense reasoning. We show that the attentions produced by BERT can be directly utilized for tasks such as the Pronoun Disambiguation Problem and Winograd Schema Challenge. Our proposed attention-guided commonsense reasoning method is conceptually simple yet empirically powerful. Experimental analysis on multiple datasets demonstrates that our proposed system performs remarkably well on all cases while outperforming the previously reported state of the art by a margin. While results suggest that BERT seems to implicitly learn to establish complex relationships between entities, solving commonsense reasoning tasks might require more than unsupervised models learned from huge text corpora",
    "volume": "main",
    "checked": true,
    "id": "8e82dd83df5023df86868c59a03fd7872fb5931e",
    "citation_count": 31
  },
  "https://aclanthology.org/P19-1478": {
    "title": "A Surprisingly Robust Trick for the Winograd Schema Challenge",
    "abstract": "The Winograd Schema Challenge (WSC) dataset WSC273 and its inference counterpart WNLI are popular benchmarks for natural language understanding and commonsense reasoning. In this paper, we show that the performance of three language models on WSC273 consistently and robustly improves when fine-tuned on a similar pronoun disambiguation problem dataset (denoted WSCR). We additionally generate a large unsupervised WSC-like dataset. By fine-tuning the BERT language model both on the introduced and on the WSCR dataset, we achieve overall accuracies of 72.5% and 74.7% on WSC273 and WNLI, improving the previous state-of-the-art solutions by 8.8% and 9.6%, respectively. Furthermore, our fine-tuned models are also consistently more accurate on the “complex” subsets of WSC273, introduced by Trichelair et al. (2018)",
    "volume": "main",
    "checked": true,
    "id": "c57298fe3faf87f9f24414821b0df7ebb7634320",
    "citation_count": 88
  },
  "https://aclanthology.org/P19-1479": {
    "title": "Coherent Comments Generation for Chinese Articles with a Graph-to-Sequence Model",
    "abstract": "Automatic article commenting is helpful in encouraging user engagement on online news platforms. However, the news documents are usually too long for models under traditional encoder-decoder frameworks, which often results in general and irrelevant comments. In this paper, we propose to generate comments with a graph-to-sequence model that models the input news as a topic interaction graph. By organizing the article into graph structure, our model can better understand the internal structure of the article and the connection between topics, which makes it better able to generate coherent and informative comments. We collect and release a large scale news-comment corpus from a popular Chinese online news platform Tencent Kuaibao. Extensive experiment results show that our model can generate much more coherent and informative comments compared with several strong baseline models",
    "volume": "main",
    "checked": true,
    "id": "b2125d912941244c243a33e31b01e34467cea457",
    "citation_count": 30
  },
  "https://aclanthology.org/P19-1480": {
    "title": "Interconnected Question Generation with Coreference Alignment and Conversation Flow Modeling",
    "abstract": "We study the problem of generating interconnected questions in question-answering style conversations. Compared with previous works which generate questions based on a single sentence (or paragraph), this setting is different in two major aspects: (1) Questions are highly conversational. Almost half of them refer back to conversation history using coreferences. (2) In a coherent conversation, questions have smooth transitions between turns. We propose an end-to-end neural model with coreference alignment and conversation flow modeling. The coreference alignment modeling explicitly aligns coreferent mentions in conversation history with corresponding pronominal references in generated questions, which makes generated questions interconnected to conversation history. The conversation flow modeling builds a coherent conversation by starting questioning on the first few sentences in a text passage and smoothly shifting the focus to later parts. Extensive experiments show that our system outperforms several baselines and can generate highly conversational questions. The code implementation is released at https://github.com/Evan-Gao/conversaional-QG",
    "volume": "main",
    "checked": true,
    "id": "561564941c60a58bb277841090a7e2ac8c2309c4",
    "citation_count": 28
  },
  "https://aclanthology.org/P19-1481": {
    "title": "Cross-Lingual Training for Automatic Question Generation",
    "abstract": "Automatic question generation (QG) is a challenging problem in natural language understanding. QG systems are typically built assuming access to a large number of training instances where each instance is a question and its corresponding answer. For a new language, such training instances are hard to obtain making the QG problem even more challenging. Using this as our motivation, we study the reuse of an available large QG dataset in a secondary language (e.g. English) to learn a QG model for a primary language (e.g. Hindi) of interest. For the primary language, we assume access to a large amount of monolingual text but only a small QG dataset. We propose a cross-lingual QG model which uses the following training regime: (i) Unsupervised pretraining of language models in both primary and secondary languages and (ii) joint supervised training for QG in both languages. We demonstrate the efficacy of our proposed approach using two different primary languages, Hindi and Chinese. Our proposed framework clearly outperforms a number of baseline models, including a fully-supervised transformer-based model trained on the QG datasets in the primary language. We also create and release a new question answering dataset for Hindi consisting of 6555 sentences",
    "volume": "main",
    "checked": true,
    "id": "2dd84aa297426ff5d78760d1358c9167e84e944b",
    "citation_count": 35
  },
  "https://aclanthology.org/P19-1482": {
    "title": "A Hierarchical Reinforced Sequence Operation Method for Unsupervised Text Style Transfer",
    "abstract": "Unsupervised text style transfer aims to alter text styles while preserving the content, without aligned data for supervision. Existing seq2seq methods face three challenges: 1) the transfer is weakly interpretable, 2) generated outputs struggle in content preservation, and 3) the trade-off between content and style is intractable. To address these challenges, we propose a hierarchical reinforced sequence operation method, named Point-Then-Operate (PTO), which consists of a high-level agent that proposes operation positions and a low-level agent that alters the sentence. We provide comprehensive training objectives to control the fluency, style, and content of the outputs and a mask-based inference algorithm that allows for multi-step revision based on the single-step trained agents. Experimental results on two text style transfer datasets show that our method significantly outperforms recent methods and effectively addresses the aforementioned challenges",
    "volume": "main",
    "checked": true,
    "id": "6d25f338954a0705616d232cd2ca2c6b975fbeaf",
    "citation_count": 45
  },
  "https://aclanthology.org/P19-1483": {
    "title": "Handling Divergent Reference Texts when Evaluating Table-to-Text Generation",
    "abstract": "Automatically constructed datasets for generating text from semi-structured data (tables), such as WikiBio, often contain reference texts that diverge from the information in the corresponding semi-structured data. We show that metrics which rely solely on the reference texts, such as BLEU and ROUGE, show poor correlation with human judgments when those references diverge. We propose a new metric, PARENT, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall. Through a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed by Wiseman et al (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge",
    "volume": "main",
    "checked": true,
    "id": "02cbb0db288af2c83b48a023f245812bd22a2408",
    "citation_count": 94
  },
  "https://aclanthology.org/P19-1484": {
    "title": "Unsupervised Question Answering by Cloze Translation",
    "abstract": "Obtaining training data for Question Answering (QA) is time-consuming and resource-intensive, and existing QA datasets are only available for limited domains and languages. In this work, we explore to what extent high quality training data is actually required for Extractive QA, and investigate the possibility of unsupervised Extractive QA. We approach this problem by first learning to generate context, question and answer triples in an unsupervised manner, which we then use to synthesize Extractive QA training data automatically. To generate such triples, we first sample random context paragraphs from a large corpus of documents and then random noun phrases or Named Entity mentions from these paragraphs as answers. Next we convert answers in context to “fill-in-the-blank” cloze questions and finally translate them into natural questions. We propose and compare various unsupervised ways to perform cloze-to-natural question translation, including training an unsupervised NMT model using non-aligned corpora of natural questions and cloze questions as well as a rule-based approach. We find that modern QA models can learn to answer human questions surprisingly well using only synthetic training data. We demonstrate that, without using the SQuAD training data at all, our approach achieves 56.4 F1 on SQuAD v1 (64.5 F1 when the answer is a Named Entity mention), outperforming early supervised models",
    "volume": "main",
    "checked": true,
    "id": "b2821ea94b1a7645a8befabce3a161473eb2e965",
    "citation_count": 95
  },
  "https://aclanthology.org/P19-1485": {
    "title": "MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension",
    "abstract": "A large number of reading comprehension (RC) datasets has been created recently, but little analysis has been done on whether they generalize to one another, and the extent to which existing datasets can be leveraged for improving performance on new ones. In this paper, we conduct such an investigation over ten RC datasets, training on one or more source RC datasets, and evaluating generalization, as well as transfer to a target RC dataset. We analyze the factors that contribute to generalization, and show that training on a source RC dataset and transferring to a target dataset substantially improves performance, even in the presence of powerful contextual representations from BERT (Devlin et al., 2019). We also find that training on multiple source RC datasets leads to robust generalization and transfer, and can reduce the cost of example collection for a new RC dataset. Following our analysis, we propose MultiQA, a BERT-based model, trained on multiple RC datasets, which leads to state-of-the-art performance on five RC datasets. We share our infrastructure for the benefit of the research community",
    "volume": "main",
    "checked": true,
    "id": "636904d91d9dd1a641a595d9578ba7640f35aa74",
    "citation_count": 134
  },
  "https://aclanthology.org/P19-1486": {
    "title": "Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives",
    "abstract": "This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. We propose a curriculum learning (CL) based Pointer-Generator framework for reading/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. This can be interpreted as a form of domain randomization and/or generative pretraining during training. To this end, the usage of the Pointer-Generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. Additionally, we propose a new Introspective Alignment Layer (IAL), which reasons over decomposed alignments using block-based self-attention. We evaluate our proposed method on the NarrativeQA reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by 51% relative improvement on BLEU-4 and 17% relative improvement on Rouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and CL components",
    "volume": "main",
    "checked": true,
    "id": "e1f037f47d96d10af5c1ab390a246238dd8e1057",
    "citation_count": 56
  },
  "https://aclanthology.org/P19-1487": {
    "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning",
    "abstract": "Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning",
    "volume": "main",
    "checked": true,
    "id": "874e9318c09c711ecd48a903b3824a3a03e2cd62",
    "citation_count": 276
  },
  "https://aclanthology.org/P19-1488": {
    "title": "Interpretable Question Answering on Knowledge Bases and Text",
    "abstract": "Interpretability of machine learning (ML) models becomes more relevant with their increasing adoption. In this work, we address the interpretability of ML based question answering (QA) models on a combination of knowledge bases (KB) and text documents. We adapt post hoc explanation methods such as LIME and input perturbation (IP) and compare them with the self-explanatory attention mechanism of the model. For this purpose, we propose an automatic evaluation paradigm for explanation methods in the context of QA. We also conduct a study with human annotators to evaluate whether explanations help them identify better QA models. Our results suggest that IP provides better explanations than LIME or attention, according to both automatic and human evaluation. We obtain the same ranking of methods in both experiments, which supports the validity of our automatic evaluation paradigm",
    "volume": "main",
    "checked": true,
    "id": "dee97bca27dee7217fc1248b8fdcb3133686b30d",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-1489": {
    "title": "A Resource-Free Evaluation Metric for Cross-Lingual Word Embeddings Based on Graph Modularity",
    "abstract": "Cross-lingual word embeddings encode the meaning of words from different languages into a shared low-dimensional space. An important requirement for many downstream tasks is that word similarity should be independent of language—i.e., word vectors within one language should not be more similar to each other than to words in another language. We measure this characteristic using modularity, a network measurement that measures the strength of clusters in a graph. Modularity has a moderate to strong correlation with three downstream tasks, even though modularity is based only on the structure of embeddings and does not require any external resources. We show through experiments that modularity can serve as an intrinsic validation metric to improve unsupervised cross-lingual word embeddings, particularly on distant language pairs in low-resource settings",
    "volume": "main",
    "checked": true,
    "id": "46150ac82c49f5a7994ea831f88a25a9cde96d90",
    "citation_count": 13
  },
  "https://aclanthology.org/P19-1490": {
    "title": "Multilingual and Cross-Lingual Graded Lexical Entailment",
    "abstract": "Grounded in cognitive linguistics, graded lexical entailment (GR-LE) is concerned with fine-grained assertions regarding the directional hierarchical relationships between concepts on a continuous scale. In this paper, we present the first work on cross-lingual generalisation of GR-LE relation. Starting from HyperLex, the only available GR-LE dataset in English, we construct new monolingual GR-LE datasets for three other languages, and combine those to create a set of six cross-lingual GR-LE datasets termed CL-HYPERLEX. We next present a novel method dubbed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel) for effectively capturing graded (and binary) LE, both monolingually in different languages as well as across languages (i.e., on CL-HYPERLEX). Coupled with a bilingual dictionary, CLEAR leverages taxonomic LE knowledge in a resource-rich language (e.g., English) and propagates it to other languages. Supported by cross-lingual LE transfer, CLEAR sets competitive baseline performance on three new monolingual GR-LE datasets and six cross-lingual GR-LE datasets. In addition, we show that CLEAR outperforms current state-of-the-art on binary cross-lingual LE detection by a wide margin for diverse language pairs",
    "volume": "main",
    "checked": true,
    "id": "09e1277e555ef87d53648be72454e2e6bc6150c8",
    "citation_count": 12
  },
  "https://aclanthology.org/P19-1491": {
    "title": "What Kind of Language Is Hard to Language-Model?",
    "abstract": "How language-agnostic are current state-of-the-art NLP tools? Are there some types of language that are easier to model with current methods? In prior work (Cotterell et al., 2018) we attempted to address this question for language modeling, and observed that recurrent neural network language models do not perform equally well over all the high-resource European languages found in the Europarl corpus. We speculated that inflectional morphology may be the primary culprit for the discrepancy. In this paper, we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual Bible corpus. Methodologically, we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora. In other words, the model is aware of inter-sentence variation and can handle missing data. Exploiting this model, we show that “translationese” is not any easier to model than natively written language in a fair comparison. Trying to answer the question of what features difficult languages have in common, we try and fail to reproduce our earlier (Cotterell et al., 2018) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample",
    "volume": "main",
    "checked": true,
    "id": "98050eeda3b79a18f555480881e1f3bc7d447882",
    "citation_count": 45
  },
  "https://aclanthology.org/P19-1492": {
    "title": "Analyzing the Limitations of Cross-lingual Word Embedding Mappings",
    "abstract": "Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations. While several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure, it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings. So as to answer this question, we experiment with parallel corpora, which allows us to compare offline mapping to an extension of skip-gram that jointly learns both embedding spaces. We observe that, under these ideal conditions, joint learning yields to more isomorphic embeddings, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction. We thus conclude that current mapping methods do have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal",
    "volume": "main",
    "checked": true,
    "id": "114bdb2f620963db4b2fb637cbe212cfe1267d08",
    "citation_count": 47
  },
  "https://aclanthology.org/P19-1493": {
    "title": "How Multilingual is Multilingual BERT?",
    "abstract": "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs",
    "volume": "main",
    "checked": true,
    "id": "809cc93921e4698bde891475254ad6dfba33d03b",
    "citation_count": 787
  },
  "https://aclanthology.org/P19-1494": {
    "title": "Bilingual Lexicon Induction through Unsupervised Machine Translation",
    "abstract": "A recent research line has obtained strong results on bilingual lexicon induction by aligning independently trained word embeddings in two languages and using the resulting cross-lingual embeddings to induce word translation pairs through nearest neighbor or related retrieval methods. In this paper, we propose an alternative approach to this problem that builds on the recent work on unsupervised machine translation. This way, instead of directly inducing a bilingual lexicon from cross-lingual embeddings, we use them to build a phrase-table, combine it with a language model, and use the resulting machine translation system to generate a synthetic parallel corpus, from which we extract the bilingual lexicon using statistical word alignment techniques. As such, our method can work with any word embedding and cross-lingual mapping technique, and it does not require any additional resource besides the monolingual corpus used to train the embeddings. When evaluated on the exact same cross-lingual embeddings, our proposed method obtains an average improvement of 6 accuracy points over nearest neighbor and 4 points over CSLS retrieval, establishing a new state-of-the-art in the standard MUSE dataset",
    "volume": "main",
    "checked": true,
    "id": "cce3ae8c53fc57f45800659586149ced9856a8fe",
    "citation_count": 40
  },
  "https://aclanthology.org/P19-1495": {
    "title": "Automatically Identifying Complaints in Social Media",
    "abstract": "Complaining is a basic speech act regularly used in human and computer mediated communication to express a negative mismatch between reality and expectations in a particular situation. Automatically identifying complaints in social media is of utmost importance for organizations or brands to improve the customer experience or in developing dialogue systems for handling and responding to complaints. In this paper, we introduce the first systematic analysis of complaints in computational linguistics. We collect a new annotated data set of written complaints expressed on Twitter. We present an extensive linguistic analysis of complaining as a speech act in social media and train strong feature-based and neural models of complaints across nine domains achieving a predictive performance of up to 79 F1 using distant supervision",
    "volume": "main",
    "checked": true,
    "id": "ddb79133c9ad4b99c7427b20a8900e73e5025b66",
    "citation_count": 31
  },
  "https://aclanthology.org/P19-1496": {
    "title": "TWEETQA: A Social Media Focused Question Answering Dataset",
    "abstract": "With social media becoming increasingly popular on which lots of news and real-time events are reported, developing automated question answering systems is critical to the effective-ness of many applications that rely on real-time knowledge. While previous datasets have concentrated on question answering (QA) for formal text like news and Wikipedia, we present the first large-scale dataset for QA over social media data. To ensure that the tweets we collected are useful, we only gather tweets used by journalists to write news articles. We then ask human annotators to write questions and answers upon these tweets. Unlike otherQA datasets like SQuAD in which the answers are extractive, we allow the answers to be abstractive. We show that two recently proposed neural models that perform well on formal texts are limited in their performance when applied to our dataset. In addition, even the fine-tuned BERT model is still lagging behind human performance with a large margin. Our results thus point to the need of improved QA systems targeting social media text",
    "volume": "main",
    "checked": true,
    "id": "5b78c2476984452c372e7fce6ac8246d15c97efa",
    "citation_count": 41
  },
  "https://aclanthology.org/P19-1497": {
    "title": "Asking the Crowd: Question Analysis, Evaluation and Generation for Open Discussion on Online Forums",
    "abstract": "Teaching machines to ask questions is an important yet challenging task. Most prior work focused on generating questions with fixed answers. As contents are highly limited by given answers, these questions are often not worth discussing. In this paper, we take the first step on teaching machines to ask open-answered questions from real-world news for open discussion (openQG). To generate high-qualified questions, effective ways for question evaluation are required. We take the perspective that the more answers a question receives, the better it is for open discussion, and analyze how language use affects the number of answers. Compared with other factors, e.g. topic and post time, linguistic factors keep our evaluation from being domain-specific. We carefully perform variable control on 11.5M questions from online forums to get a dataset, OQRanD, and further perform question analysis. Based on these conclusions, several models are built for question evaluation. For openQG task, we construct OQGenD, the first dataset as far as we know, and propose a model based on conditional generative adversarial networks and our question evaluation model. Experiments show that our model can generate questions with higher quality compared with commonly-used text generation methods",
    "volume": "main",
    "checked": true,
    "id": "266903d0e9e3fe0eea8bd0e2e31614e5675a0ddf",
    "citation_count": 0
  },
  "https://aclanthology.org/P19-1498": {
    "title": "Tree LSTMs with Convolution Units to Predict Stance and Rumor Veracity in Social Media Conversations",
    "abstract": "Learning from social-media conversations has gained significant attention recently because of its applications in areas like rumor detection. In this research, we propose a new way to represent social-media conversations as binarized constituency trees that allows comparing features in source-posts and their replies effectively. Moreover, we propose to use convolution units in Tree LSTMs that are better at learning patterns in features obtained from the source and reply posts. Our Tree LSTM models employ multi-task (stance + rumor) learning and propagate the useful stance signal up in the tree for rumor classification at the root node. The proposed models achieve state-of-the-art performance, outperforming the current best model by 12% and 15% on F1-macro for rumor-veracity classification and stance classification tasks respectively",
    "volume": "main",
    "checked": true,
    "id": "5bc141975d48b87bbf43fb630d536e6e8958cd2e",
    "citation_count": 63
  },
  "https://aclanthology.org/P19-1499": {
    "title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization",
    "abstract": "Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels, which are created heuristically using rule-based methods. Training the hierarchical encoder with these inaccurate labels is challenging. Inspired by the recent work on pre-training transformer sentence encoders (Devlin et al., 2018), we propose Hibert (as shorthand for HIerachical Bidirectional Encoder Representations from Transformers) for document encoding and a method to pre-train it using unlabeled data. We apply the pre-trained Hibert to our summarization model and it outperforms its randomly initialized counterpart by 1.25 ROUGE on the CNN/Dailymail dataset and by 2.0 ROUGE on a version of New York Times dataset. We also achieve the state-of-the-art performance on these two datasets",
    "volume": "main",
    "checked": true,
    "id": "203b543bfa1e564bb80ff4229b43174d7c71b0c0",
    "citation_count": 269
  },
  "https://aclanthology.org/P19-1500": {
    "title": "Hierarchical Transformers for Multi-Document Summarization",
    "abstract": "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines",
    "volume": "main",
    "checked": true,
    "id": "7cc730da554003dda77796d2cb4f06da5dfd5592",
    "citation_count": 196
  },
  "https://aclanthology.org/P19-1501": {
    "title": "Abstractive Text Summarization Based on Deep Learning and Semantic Content Generalization",
    "abstract": "This work proposes a novel framework for enhancing abstractive text summarization based on the combination of deep learning techniques along with semantic data transformations. Initially, a theoretical model for semantic-based text generalization is introduced and used in conjunction with a deep encoder-decoder architecture in order to produce a summary in generalized form. Subsequently, a methodology is proposed which transforms the aforementioned generalized summary into human-readable form, retaining at the same time important informational aspects of the original text and addressing the problem of out-of-vocabulary or rare words. The overall approach is evaluated on two popular datasets with encouraging results",
    "volume": "main",
    "checked": true,
    "id": "50ce1595e8daf95ea031b91237d0ed21e1525787",
    "citation_count": 22
  },
  "https://aclanthology.org/P19-1502": {
    "title": "Studying Summarization Evaluation Metrics in the Appropriate Scoring Range",
    "abstract": "In summarization, automatic evaluation metrics are usually compared based on their ability to correlate with human judgments. Unfortunately, the few existing human judgment datasets have been created as by-products of the manual evaluations performed during the DUC/TAC shared tasks. However, modern systems are typically better than the best systems submitted at the time of these shared tasks. We show that, surprisingly, evaluation metrics which behave similarly on these datasets (average-scoring range) strongly disagree in the higher-scoring range in which current systems now operate. It is problematic because metrics disagree yet we can’t decide which one to trust. This is a call for collecting human judgments for high-scoring summaries as this would resolve the debate over which metrics to trust. This would also be greatly beneficial to further improve summarization systems and metrics alike",
    "volume": "main",
    "checked": true,
    "id": "cca42b7e53a2d5e5c0f995735043e84a3b1cc89c",
    "citation_count": 39
  },
  "https://aclanthology.org/P19-1503": {
    "title": "Simple Unsupervised Summarization by Contextual Matching",
    "abstract": "We propose an unsupervised method for sentence summarization using only language modeling. The approach employs two language models, one that is generic (i.e. pretrained), and the other that is specific to the target domain. We show that by using a product-of-experts criteria these are enough for maintaining continuous contextual matching while maintaining output fluency. Experiments on both abstractive and extractive sentence summarization data sets show promising results of our method without being exposed to any paired data",
    "volume": "main",
    "checked": true,
    "id": "513084a723ffcbd6546b5a84f1f01ecaa7dfeddd",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-1504": {
    "title": "Generating Summaries with Topic Templates and Structured Convolutional Decoders",
    "abstract": "Existing neural generation approaches create multi-sentence text as a single sequence. In this paper we propose a structured convolutional decoder that is guided by the content structure of target summaries. We compare our model with existing sequential decoders on three data sets representing different domains. Automatic and human evaluation demonstrate that our summaries have better content coverage",
    "volume": "main",
    "checked": true,
    "id": "3e2d30e8ae9ff584b5922310ba71bf277bcd8317",
    "citation_count": 33
  },
  "https://aclanthology.org/P19-1505": {
    "title": "Morphological Irregularity Correlates with Frequency",
    "abstract": "We present a study of morphological irregularity. Following recent work, we define an information-theoretic measure of irregularity based on the predictability of forms in a language. Using a neural transduction model, we estimate this quantity for the forms in 28 languages. We first present several validatory and exploratory analyses of irregularity. We then show that our analyses provide evidence for a correlation between irregularity and frequency: higher frequency items are more likely to be irregular and irregular items are more likely be highly frequent. To our knowledge, this result is the first of its breadth and confirms longstanding proposals from the linguistics literature. The correlation is more robust when aggregated at the level of whole paradigms—providing support for models of linguistic structure in which inflected forms are unified by abstract underlying stems or lexemes",
    "volume": "main",
    "checked": true,
    "id": "6a2daefcae320eece9aa56cb8cb6c55503c59eb2",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-1506": {
    "title": "Like a Baby: Visually Situated Neural Language Acquisition",
    "abstract": "We examine the benefits of visual context in training neural language models to perform next-word prediction. A multi-modal neural architecture is introduced that outperform its equivalent trained on language alone with a 2% decrease in perplexity, even when no visual context is available at test. Fine-tuning the embeddings of a pre-trained state-of-the-art bidirectional language model (BERT) in the language modeling framework yields a 3.5% improvement. The advantage for training with visual context when testing without is robust across different languages (English, German and Spanish) and different models (GRU, LSTM, Delta-RNN, as well as those that use BERT embeddings). Thus, language models perform better when they learn like a baby, i.e, in a multi-modal environment. This finding is compatible with the theory of situated cognition: language is inseparable from its physical context",
    "volume": "main",
    "checked": true,
    "id": "a9c9b81f05d8bc274182d6c6f3cc0b70d5c73cfa",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1507": {
    "title": "Relating Simple Sentence Representations in Deep Neural Networks and the Brain",
    "abstract": "What is the relationship between sentence representations learned by deep recurrent models against those encoded by the brain? Is there any correspondence between hidden layers of these recurrent models and brain regions when processing sentences? Can these deep models be used to synthesize brain data which can then be utilized in other extrinsic tasks? We investigate these questions using sentences with simple syntax and semantics (e.g., The bone was eaten by the dog.). We consider multiple neural network architectures, including recently proposed ELMo and BERT. We use magnetoencephalography (MEG) brain recording data collected from human subjects when they were reading these simple sentences. Overall, we find that BERT’s activations correlate the best with MEG brain data. We also find that the deep network representation can be used to generate brain data from new sentences to augment existing brain data. To the best of our knowledge, this is the first work showing that the MEG brain recording when reading a word in a sentence can be used to distinguish earlier words in the sentence. Our exploration is also the first to use deep neural network representations to generate synthetic brain data and to show that it helps in improving subsequent stimuli decoding task accuracy",
    "volume": "main",
    "checked": true,
    "id": "25b82cbfe4d917a8d21ac0338906cbff33ebbb46",
    "citation_count": 12
  },
  "https://aclanthology.org/P19-1508": {
    "title": "Modeling Affirmative and Negated Action Processing in the Brain with Lexical and Compositional Semantic Models",
    "abstract": "Recent work shows that distributional semantic models can be used to decode patterns of brain activity associated with individual words and sentence meanings. However, it is yet unclear to what extent such models can be used to study and decode fMRI patterns associated with specific aspects of semantic composition such as the negation function. In this paper, we apply lexical and compositional semantic models to decode fMRI patterns associated with negated and affirmative sentences containing hand-action verbs. Our results show reduced decoding (correlation) of sentences where the verb is in the negated context, as compared to the affirmative one, within brain regions implicated in action-semantic processing. This supports behavioral and brain imaging studies, suggesting that negation involves reduced access to aspects of the affirmative mental representation. The results pave the way for testing alternate semantic models of negation against human semantic processing in the brain",
    "volume": "main",
    "checked": true,
    "id": "48ff6ccc7eda117ae68797f6a206a44972172184",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1509": {
    "title": "Word-order Biases in Deep-agent Emergent Communication",
    "abstract": "Sequence-processing neural networks led to remarkable progress on many NLP tasks. As a consequence, there has been increasing interest in understanding to what extent they process language as humans do. We aim here to uncover which biases such models display with respect to “natural” word-order constraints. We train models to communicate about paths in a simple gridworld, using miniature languages that reflect or violate various natural language trends, such as the tendency to avoid redundancy or to minimize long-distance dependencies. We study how the controlled characteristics of our miniature languages affect individual learning and their stability across multiple network generations. The results draw a mixed picture. On the one hand, neural networks show a strong tendency to avoid long-distance dependencies. On the other hand, there is no clear preference for the efficient, non-redundant encoding of information that is widely attested in natural language. We thus suggest inoculating a notion of “effort” into neural networks, as a possible way to make their linguistic behavior more human-like",
    "volume": "main",
    "checked": true,
    "id": "e24e44515b15e1326dd25ab092a152a067c63fc1",
    "citation_count": 19
  },
  "https://aclanthology.org/P19-1510": {
    "title": "NNE: A Dataset for Nested Named Entity Recognition in English Newswire",
    "abstract": "Named entity recognition (NER) is widely used in natural language processing applications and downstream tasks. However, most NER tools target flat annotation from popular datasets, eschewing the semantic information available in nested entity mentions. We describe NNE—a fine-grained, nested named entity dataset over the full Wall Street Journal portion of the Penn Treebank (PTB). Our annotation comprises 279,795 mentions of 114 entity types with up to 6 layers of nesting. We hope the public release of this large dataset for English newswire will encourage development of new techniques for nested NER",
    "volume": "main",
    "checked": true,
    "id": "48d010d4d39d07c7dd7d110049044dac4b557a56",
    "citation_count": 37
  },
  "https://aclanthology.org/P19-1511": {
    "title": "Sequence-to-Nuggets: Nested Entity Mention Detection via Anchor-Region Networks",
    "abstract": "Sequential labeling-based NER approaches restrict each word belonging to at most one entity mention, which will face a serious problem when recognizing nested entity mentions. In this paper, we propose to resolve this problem by modeling and leveraging the head-driven phrase structures of entity mentions, i.e., although a mention can nest other mentions, they will not share the same head word. Specifically, we propose Anchor-Region Networks (ARNs), a sequence-to-nuggets architecture for nested mention detection. ARNs first identify anchor words (i.e., possible head words) of all mentions, and then recognize the mention boundaries for each anchor word by exploiting regular phrase structures. Furthermore, we also design Bag Loss, an objective function which can train ARNs in an end-to-end manner without using any anchor word annotation. Experiments show that ARNs achieve the state-of-the-art performance on three standard nested entity mention detection benchmarks",
    "volume": "main",
    "checked": true,
    "id": "f9a5048c9cf069d8345eabe9afe2b5d3029f5f4d",
    "citation_count": 73
  },
  "https://aclanthology.org/P19-1512": {
    "title": "Improving Textual Network Embedding with Global Attention via Optimal Transport",
    "abstract": "Constituting highly informative network embeddings is an essential tool for network analysis. It encodes network topology, along with other useful side information, into low dimensional node-based feature representations that can be exploited by statistical modeling. This work focuses on learning context-aware network embeddings augmented with text data. We reformulate the network embedding problem, and present two novel strategies to improve over traditional attention mechanisms: (i) a content-aware sparse attention module based on optimal transport; and (ii) a high-level attention parsing module. Our approach yields naturally sparse and self-normalized relational inference. It can capture long-term interactions between sequences, thus addressing the challenges faced by existing textual network embedding schemes. Extensive experiments are conducted to demonstrate our model can consistently outperform alternative state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "ac21ec0b6f632999bb42ab14f5acc573c2efc5f4",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1513": {
    "title": "Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction",
    "abstract": "While the fast-paced inception of novel tasks and new datasets helps foster active research in a community towards interesting directions, keeping track of the abundance of research activity in different areas on different datasets is likely to become increasingly difficult. The community could greatly benefit from an automatic system able to summarize scientific results, e.g., in the form of a leaderboard. In this paper we build two datasets and develop a framework (TDMS-IE) aimed at automatically extracting task, dataset, metric and score from NLP papers, towards the automatic construction of leaderboards. Experiments show that our model outperforms several baselines by a large margin. Our model is a first step towards automatic leaderboard construction, e.g., in the NLP domain",
    "volume": "main",
    "checked": true,
    "id": "115617d09807325bd18c42deedd6cab435f90563",
    "citation_count": 44
  },
  "https://aclanthology.org/P19-1514": {
    "title": "Scaling up Open Tagging from Tens to Thousands: Comprehension Empowered Attribute Value Extraction from Product Title",
    "abstract": "Supplementing product information by extracting attribute values from title is a crucial task in e-Commerce domain. Previous studies treat each attribute only as an entity type and build one set of NER tags (e.g., BIO) for each of them, leading to a scalability issue which unfits to the large sized attribute system in real world e-Commerce. In this work, we propose a novel approach to support value extraction scaling up to thousands of attributes without losing performance: (1) We propose to regard attribute as a query and adopt only one global set of BIO tags for any attributes to reduce the burden of attribute tag or model explosion; (2) We explicitly model the semantic representations for attribute and title, and develop an attention mechanism to capture the interactive semantic relations in-between to enforce our framework to be attribute comprehensive. We conduct extensive experiments in real-life datasets. The results show that our model not only outperforms existing state-of-the-art NER tagging models, but also is robust and generates promising results for up to 8,906 attributes",
    "volume": "main",
    "checked": true,
    "id": "80db9287663af88a83f757ad78bad37bd9c44483",
    "citation_count": 35
  },
  "https://aclanthology.org/P19-1515": {
    "title": "Incorporating Linguistic Constraints into Keyphrase Generation",
    "abstract": "Keyphrases, that concisely describe the high-level topics discussed in a document, are very useful for a wide range of natural language processing tasks. Though existing keyphrase generation methods have achieved remarkable performance on this task, they generate many overlapping phrases (including sub-phrases or super-phrases) of keyphrases. In this paper, we propose the parallel Seq2Seq network with the coverage attention to alleviate the overlapping phrase problem. Specifically, we integrate the linguistic constraints of keyphrase into the basic Seq2Seq network on the source side, and employ the multi-task learning framework on the target side. In addition, in order to prevent from generating overlapping phrases of keyphrases with correct syntax, we introduce the coverage vector to keep track of the attention history and to decide whether the parts of source text have been covered by existing generated keyphrases. Experimental results show that our method can outperform the state-of-the-art CopyRNN on scientific datasets, and is also more effective in news domain",
    "volume": "main",
    "checked": true,
    "id": "3f03c4c337ca4103e7b01657e1065739d00ddcd1",
    "citation_count": 30
  },
  "https://aclanthology.org/P19-1516": {
    "title": "A Unified Multi-task Adversarial Learning Framework for Pharmacovigilance Mining",
    "abstract": "The mining of adverse drug reaction (ADR) has a crucial role in the pharmacovigilance. The traditional ways of identifying ADR are reliable but time-consuming, non-scalable and offer a very limited amount of ADR relevant information. With the unprecedented growth of information sources in the forms of social media texts (Twitter, Blogs, Reviews etc.), biomedical literature, and Electronic Medical Records (EMR), it has become crucial to extract the most pertinent ADR related information from these free-form texts. In this paper, we propose a neural network inspired multi- task learning framework that can simultaneously extract ADRs from various sources. We adopt a novel adversarial learning-based approach to learn features across multiple ADR information sources. Unlike the other existing techniques, our approach is capable to extracting fine-grained information (such as ‘Indications’, ‘Symptoms’, ‘Finding’, ‘Disease’, ‘Drug’) which provide important cues in pharmacovigilance. We evaluate our proposed approach on three publicly available real- world benchmark pharmacovigilance datasets, a Twitter dataset from PSB 2016 Social Me- dia Shared Task, CADEC corpus and Medline ADR corpus. Experiments show that our unified framework achieves state-of-the-art performance on individual tasks associated with the different benchmark datasets. This establishes the fact that our proposed approach is generic, which enables it to achieve high performance on the diverse datasets",
    "volume": "main",
    "checked": true,
    "id": "461958790c53adc29b5124e0e3468b5bf0bb759e",
    "citation_count": 12
  },
  "https://aclanthology.org/P19-1517": {
    "title": "Quantity Tagger: A Latent-Variable Sequence Labeling Approach to Solving Addition-Subtraction Word Problems",
    "abstract": "An arithmetic word problem typically includes a textual description containing several constant quantities. The key to solving the problem is to reveal the underlying mathematical relations (such as addition and subtraction) among quantities, and then generate equations to find solutions. This work presents a novel approach, Quantity Tagger, that automatically discovers such hidden relations by tagging each quantity with a sign corresponding to one type of mathematical operation. For each quantity, we assume there exists a latent, variable-sized quantity span surrounding the quantity token in the text, which conveys information useful for determining its sign. Empirical results show that our method achieves 5 and 8 points of accuracy gains on two datasets respectively, compared to prior approaches",
    "volume": "main",
    "checked": true,
    "id": "5435f997d98754f68492334eeb87d027047e60cb",
    "citation_count": 10
  },
  "https://aclanthology.org/P19-1518": {
    "title": "A Deep Reinforced Sequence-to-Set Model for Multi-Label Classification",
    "abstract": "Multi-label classification (MLC) aims to predict a set of labels for a given instance. Based on a pre-defined label order, the sequence-to-sequence (Seq2Seq) model trained via maximum likelihood estimation method has been successfully applied to the MLC task and shows powerful ability to capture high-order correlations between labels. However, the output labels are essentially an unordered set rather than an ordered sequence. This inconsistency tends to result in some intractable problems, e.g., sensitivity to the label order. To remedy this, we propose a simple but effective sequence-to-set model. The proposed model is trained via reinforcement learning, where reward feedback is designed to be independent of the label order. In this way, we can reduce the dependence of the model on the label order, as well as capture high-order correlations between labels. Extensive experiments show that our approach can substantially outperform competitive baselines, as well as effectively reduce the sensitivity to the label order",
    "volume": "main",
    "checked": true,
    "id": "4ab5778866f04d5bdb2a1e69e252c43d513d66de",
    "citation_count": 25
  },
  "https://aclanthology.org/P19-1519": {
    "title": "Joint Slot Filling and Intent Detection via Capsule Neural Networks",
    "abstract": "Being able to recognize words as slots and detect the intent of an utterance has been a keen issue in natural language understanding. The existing works either treat slot filling and intent detection separately in a pipeline manner, or adopt joint models which sequentially label slots while summarizing the utterance-level intent without explicitly preserving the hierarchical relationship among words, slots, and intents. To exploit the semantic hierarchy for effective modeling, we propose a capsule-based neural network model which accomplishes slot filling and intent detection via a dynamic routing-by-agreement schema. A re-routing schema is proposed to further synergize the slot filling performance using the inferred intent representation. Experiments on two real-world datasets show the effectiveness of our model when compared with other alternative model architectures, as well as existing natural language understanding services",
    "volume": "main",
    "checked": true,
    "id": "5d64f735b104f64d35717092a1f3fba94e6e3bec",
    "citation_count": 156
  },
  "https://aclanthology.org/P19-1520": {
    "title": "Neural Aspect and Opinion Term Extraction with Mined Rules as Weak Supervision",
    "abstract": "Lack of labeled training data is a major bottleneck for neural network based aspect and opinion term extraction on product reviews. To alleviate this problem, we first propose an algorithm to automatically mine extraction rules from existing training examples based on dependency parsing results. The mined rules are then applied to label a large amount of auxiliary data. Finally, we study training procedures to train a neural model which can learn from both the data automatically labeled by the rules and a small amount of data accurately annotated by human. Experimental results show that although the mined rules themselves do not perform well due to their limited flexibility, the combination of human annotated data and rule labeled auxiliary data can improve the neural model and allow it to achieve performance better than or comparable with the current state-of-the-art",
    "volume": "main",
    "checked": true,
    "id": "51371746aff20df9357595407ec35731f26407ab",
    "citation_count": 64
  },
  "https://aclanthology.org/P19-1521": {
    "title": "Cost-sensitive Regularization for Label Confusion-aware Event Detection",
    "abstract": "In supervised event detection, most of the mislabeling occurs between a small number of confusing type pairs, including trigger-NIL pairs and sibling sub-types of the same coarse type. To address this label confusion problem, this paper proposes cost-sensitive regularization, which can force the training procedure to concentrate more on optimizing confusing type pairs. Specifically, we introduce a cost-weighted term into the training loss, which penalizes more on mislabeling between confusing label pairs. Furthermore, we also propose two estimators which can effectively measure such label confusion based on instance-level or population-level statistics. Experiments on TAC-KBP 2017 datasets demonstrate that the proposed method can significantly improve the performances of different models in both English and Chinese event detection",
    "volume": "main",
    "checked": true,
    "id": "ad77c9d46b4877c75c9cd990f4f9ec5a5c48643d",
    "citation_count": 16
  },
  "https://aclanthology.org/P19-1522": {
    "title": "Exploring Pre-trained Language Models for Event Extraction and Generation",
    "abstract": "Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%",
    "volume": "main",
    "checked": true,
    "id": "36b79362d2927824e0daa864dd32cf0de7ca35f9",
    "citation_count": 162
  },
  "https://aclanthology.org/P19-1523": {
    "title": "Improving Open Information Extraction via Iterative Rank-Aware Learning",
    "abstract": "Open information extraction (IE) is the task of extracting open-domain assertions from natural language sentences. A key step in open IE is confidence modeling, ranking the extractions based on their estimated quality to adjust precision and recall of extracted assertions. We found that the extraction likelihood, a confidence measure used by current supervised open IE systems, is not well calibrated when comparing the quality of assertions extracted from different sentences. We propose an additional binary classification loss to calibrate the likelihood to make it more globally comparable, and an iterative learning process, where extractions generated by the open IE model are incrementally included as training samples to help the model learn from trial and error. Experiments on OIE2016 demonstrate the effectiveness of our method. Code and data are available at https://github.com/jzbjyb/oie_rank",
    "volume": "main",
    "checked": true,
    "id": "8147a495b9a933742f06458244f7c5df00767c4e",
    "citation_count": 8
  },
  "https://aclanthology.org/P19-1524": {
    "title": "Towards Improving Neural Named Entity Recognition with Gazetteers",
    "abstract": "Most of the recently proposed neural models for named entity recognition have been purely data-driven, with a strong emphasis on getting rid of the efforts for collecting external resources or designing hand-crafted features. This could increase the chance of overfitting since the models cannot access any supervision signal beyond the small amount of annotated data, limiting their power to generalize beyond the annotated entities. In this work, we show that properly utilizing external gazetteers could benefit segmental neural NER models. We add a simple module on the recently proposed hybrid semi-Markov CRF architecture and observe some promising results",
    "volume": "main",
    "checked": true,
    "id": "de82ac9c443ace8f3552ca3d2fcb7387d8199092",
    "citation_count": 65
  },
  "https://aclanthology.org/P19-1525": {
    "title": "Span-Level Model for Relation Extraction",
    "abstract": "Relation Extraction is the task of identifying entity mention spans in raw text and then identifying relations between pairs of the entity mentions. Recent approaches for this span-level task have been token-level models which have inherent limitations. They cannot easily define and implement span-level features, cannot model overlapping entity mentions and have cascading errors due to the use of sequential decoding. To address these concerns, we present a model which directly models all possible spans and performs joint entity mention detection and relation extraction. We report a new state-of-the-art performance of 62.83 F1 (prev best was 60.49) on the ACE2005 dataset",
    "volume": "main",
    "checked": true,
    "id": "103f465212a3270b9b6ed957564054d4322d25aa",
    "citation_count": 47
  },
  "https://aclanthology.org/P19-1526": {
    "title": "Enhancing Unsupervised Generative Dependency Parser with Contextual Information",
    "abstract": "Most of the unsupervised dependency parsers are based on probabilistic generative models that learn the joint distribution of the given sentence and its parse. Probabilistic generative models usually explicit decompose the desired dependency tree into factorized grammar rules, which lack the global features of the entire sentence. In this paper, we propose a novel probabilistic model called discriminative neural dependency model with valence (D-NDMV) that generates a sentence and its parse from a continuous latent representation, which encodes global contextual information of the generated sentence. We propose two approaches to model the latent representation: the first deterministically summarizes the representation from the sentence and the second probabilistically models the representation conditioned on the sentence. Our approach can be regarded as a new type of autoencoder model to unsupervised dependency parsing that combines the benefits of both generative and discriminative techniques. In particular, our approach breaks the context-free independence assumption in previous generative approaches and therefore becomes more expressive. Our extensive experimental results on seventeen datasets from various sources show that our approach achieves competitive accuracy compared with both generative and discriminative state-of-the-art unsupervised dependency parsers",
    "volume": "main",
    "checked": true,
    "id": "827c9f7e1c03bc25cb474ad119ff0eeda70a0298",
    "citation_count": 13
  },
  "https://aclanthology.org/P19-1527": {
    "title": "Neural Architectures for Nested NER through Linearization",
    "abstract": "We propose two neural network architectures for nested named entity recognition (NER), a setting in which named entities may overlap and also be labeled with more than one label. We encode the nested labels using a linearized scheme. In our first proposed approach, the nested labels are modeled as multilabels corresponding to the Cartesian product of the nested labels in a standard LSTM-CRF architecture. In the second one, the nested NER is viewed as a sequence-to-sequence problem, in which the input sequence consists of the tokens and output sequence of the labels, using hard attention on the word whose label is being predicted. The proposed methods outperform the nested NER state of the art on four corpora: ACE-2004, ACE-2005, GENIA and Czech CNEC. We also enrich our architectures with the recently published contextual embeddings: ELMo, BERT and Flair, reaching further improvements for the four nested entity corpora. In addition, we report flat NER state-of-the-art results for CoNLL-2002 Dutch and Spanish and for CoNLL-2003 English",
    "volume": "main",
    "checked": true,
    "id": "806f13809ea4293ff5505b5af66909cb2284ab48",
    "citation_count": 166
  },
  "https://aclanthology.org/P19-1528": {
    "title": "Online Infix Probability Computation for Probabilistic Finite Automata",
    "abstract": "Probabilistic finite automata (PFAs) are com- mon statistical language model in natural lan- guage and speech processing. A typical task for PFAs is to compute the probability of all strings that match a query pattern. An impor- tant special case of this problem is computing the probability of a string appearing as a pre- fix, suffix, or infix. These problems find use in many natural language processing tasks such word prediction and text error correction. Recently, we gave the first incremental algorithm to efficiently compute the infix probabilities of each prefix of a string (Cognetta et al., 2018). We develop an asymptotic improvement of that algorithm and solve the open problem of computing the infix probabilities of PFAs from streaming data, which is crucial when process- ing queries online and is the ultimate goal of the incremental approach",
    "volume": "main",
    "checked": true,
    "id": "1fa6db5a4b4a1aaf0cff85c8d4c5b0e5c8bbd35f",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-1529": {
    "title": "How to Best Use Syntax in Semantic Role Labelling",
    "abstract": "There are many different ways in which external information might be used in a NLP task. This paper investigates how external syntactic information can be used most effectively in the Semantic Role Labeling (SRL) task. We evaluate three different ways of encoding syntactic parses and three different ways of injecting them into a state-of-the-art neural ELMo-based SRL sequence labelling model. We show that using a constituency representation as input features improves performance the most, achieving a new state-of-the-art for non-ensemble SRL models on the in-domain CoNLL’05 and CoNLL’12 benchmarks",
    "volume": "main",
    "checked": true,
    "id": "31a3bdd716c175846011180467be9f2911cc4bd8",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-1530": {
    "title": "PTB Graph Parsing with Tree Approximation",
    "abstract": "The Penn Treebank (PTB) represents syntactic structures as graphs due to nonlocal dependencies. This paper proposes a method that approximates PTB graph-structured representations by trees. By our approximation method, we can reduce nonlocal dependency identification and constituency parsing into single tree-based parsing. An experimental result demonstrates that our approximation method with an off-the-shelf tree-based constituency parser significantly outperforms the previous methods in nonlocal dependency identification",
    "volume": "main",
    "checked": true,
    "id": "0d61a6b67c327017e0c466269fcb7077b32b60f9",
    "citation_count": 2
  },
  "https://aclanthology.org/P19-1531": {
    "title": "Sequence Labeling Parsing by Learning across Representations",
    "abstract": "We use parsing as sequence labeling as a common framework to learn across constituency and dependency syntactic abstractions.To do so, we cast the problem as multitask learning (MTL). First, we show that adding a parsing paradigm as an auxiliary loss consistently improves the performance on the other paradigm. Secondly, we explore an MTL sequence labeling model that parses both representations, at almost no cost in terms of performance and speed. The results across the board show that on average MTL models with auxiliary losses for constituency parsing outperform single-task ones by 1.05 F1 points, and for dependency parsing by 0.62 UAS points",
    "volume": "main",
    "checked": true,
    "id": "ca8435bb02dbf7209c5058af062c16d531e1f7cd",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1532": {
    "title": "A Prism Module for Semantic Disentanglement in Name Entity Recognition",
    "abstract": "Natural Language Processing has been perplexed for many years by the problem that multiple semantics are mixed inside a word, even with the help of context. To solve this problem, we propose a prism module to disentangle the semantic aspects of words and reduce noise at the input layer of a model. In the prism module, some words are selectively replaced with task-related semantic aspects, then these denoised word representations can be fed into downstream tasks to make them easier. Besides, we also introduce a structure to train this module jointly with the downstream model without additional data. This module can be easily integrated into the downstream model and significantly improve the performance of baselines on named entity recognition (NER) task. The ablation analysis demonstrates the rationality of the method. As a side effect, the proposed method also provides a way to visualize the contribution of each word",
    "volume": "main",
    "checked": true,
    "id": "8f049ef3339bd14cb17b2086b016327b5346b6f4",
    "citation_count": 2
  },
  "https://aclanthology.org/P19-1533": {
    "title": "Label-Agnostic Sequence Labeling by Copying Nearest Neighbors",
    "abstract": "Retrieve-and-edit based approaches to structured prediction, where structures associated with retrieved neighbors are edited to form new structures, have recently attracted increased interest. However, much recent work merely conditions on retrieved structures (e.g., in a sequence-to-sequence framework), rather than explicitly manipulating them. We show we can perform accurate sequence labeling by explicitly (and only) copying labels from retrieved neighbors. Moreover, because this copying is label-agnostic, we can achieve impressive performance in zero-shot sequence-labeling tasks. We additionally consider a dynamic programming approach to sequence labeling in the presence of retrieved neighbors, which allows for controlling the number of distinct (copied) segments used to form a prediction, and leads to both more interpretable and accurate predictions",
    "volume": "main",
    "checked": true,
    "id": "ad92d28648cd8136b3bbb888ee38d92845775d2e",
    "citation_count": 37
  },
  "https://aclanthology.org/P19-1534": {
    "title": "Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset",
    "abstract": "One challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly, a key communicative skill. While it is straightforward for humans to recognize and acknowledge others’ feelings in a conversation, this is a significant challenge for AI systems due to the paucity of suitable publicly-available datasets for training and evaluation. This work proposes a new benchmark for empathetic dialogue generation and EmpatheticDialogues, a novel dataset of 25k conversations grounded in emotional situations. Our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators, compared to models merely trained on large-scale Internet conversation data. We also present empirical comparisons of dialogue model adaptations for empathetic responding, leveraging existing models or datasets without requiring lengthy re-training of the full model",
    "volume": "main",
    "checked": true,
    "id": "a33a06ddc762fb855b6954c08d5aca603080b011",
    "citation_count": 414
  },
  "https://aclanthology.org/P19-1535": {
    "title": "Know More about Each Other: Evolving Dialogue Strategy via Compound Assessment",
    "abstract": "In this paper, a novel Generation-Evaluation framework is developed for multi-turn conversations with the objective of letting both participants know more about each other. For the sake of rational knowledge utilization and coherent conversation flow, a dialogue strategy which controls knowledge selection is instantiated and continuously adapted via reinforcement learning. Under the deployed strategy, knowledge grounded conversations are conducted with two dialogue agents. The generated dialogues are comprehensively evaluated on aspects like informativeness and coherence, which are aligned with our objective and human instinct. These assessments are integrated as a compound reward to guide the evolution of dialogue strategy via policy gradient. Comprehensive experiments have been carried out on the publicly available dataset, demonstrating that the proposed method outperforms the other state-of-the-art approaches significantly",
    "volume": "main",
    "checked": true,
    "id": "1be5a8868feddcf11b9b61a4219153ab4fdeb3a7",
    "citation_count": 13
  },
  "https://aclanthology.org/P19-1536": {
    "title": "Training Neural Response Selection for Task-Oriented Dialogue Systems",
    "abstract": "Despite their popularity in the chatbot literature, retrieval-based models have had modest impact on task-oriented dialogue systems, with the main obstacle to their application being the low-data regime of most task-oriented dialogue tasks. Inspired by the recent success of pretraining in language modelling, we propose an effective method for deploying response selection in task-oriented dialogue. To train response selection models for task-oriented dialogue tasks, we propose a novel method which: 1) pretrains the response selection model on large general-domain conversational corpora; and then 2) fine-tunes the pretrained model for the target dialogue domain, relying only on the small in-domain dataset to capture the nuances of the given dialogue domain. Our evaluation on five diverse application domains, ranging from e-commerce to banking, demonstrates the effectiveness of the proposed training method",
    "volume": "main",
    "checked": true,
    "id": "9b431609d961466e7a972f568ccdb2d675adb34f",
    "citation_count": 83
  },
  "https://aclanthology.org/P19-1537": {
    "title": "Collaborative Dialogue in Minecraft",
    "abstract": "We wish to develop interactive agents that can communicate with humans to collaboratively solve tasks in grounded scenarios. Since computer games allow us to simulate such tasks without the need for physical robots, we define a Minecraft-based collaborative building task in which one player (A, the Architect) is shown a target structure and needs to instruct the other player (B, the Builder) to build this structure. Both players interact via a chat interface. A can observe B but cannot place blocks. We present the Minecraft Dialogue Corpus, a collection of 509 conversations and game logs. As a first step towards our goal of developing fully interactive agents for this task, we consider the subtask of Architect utterance generation, and show how challenging it is",
    "volume": "main",
    "checked": true,
    "id": "4aab6559007292d1d5823a5abbe4b75a7576c180",
    "citation_count": 43
  },
  "https://aclanthology.org/P19-1538": {
    "title": "Neural Response Generation with Meta-words",
    "abstract": "We present open domain dialogue generation with meta-words. A meta-word is a structured record that describes attributes of a response, and thus allows us to explicitly model the one-to-many relationship within open domain dialogues and perform response generation in an explainable and controllable manner. To incorporate meta-words into generation, we propose a novel goal-tracking memory network that formalizes meta-word expression as a goal in response generation and manages the generation process to achieve the goal with a state memory panel and a state controller. Experimental results from both automatic evaluation and human judgment on two large-scale data sets indicate that our model can significantly outperform state-of-the-art generation models in terms of response relevance, response diversity, and accuracy of meta-word expression",
    "volume": "main",
    "checked": true,
    "id": "dd1e4c7fb44be613cde2ce9766b219c037a952e4",
    "citation_count": 35
  },
  "https://aclanthology.org/P19-1539": {
    "title": "Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading",
    "abstract": "Although neural conversational models are effective in learning how to produce fluent responses, their primary challenge lies in knowing what to say to make the conversation contentful and non-vacuous. We present a new end-to-end approach to contentful neural conversation that jointly models response generation and on-demand machine reading. The key idea is to provide the conversation model with relevant long-form text on the fly as a source of external knowledge. The model performs QA-style reading comprehension on this text in response to each conversational turn, thereby allowing for more focused integration of external knowledge than has been possible in prior approaches. To support further research on knowledge-grounded conversation, we introduce a new large-scale conversation dataset grounded in external web pages (2.8M turns, 7.4M sentences of grounding). Both human evaluation and automated metrics show that our approach results in more contentful responses compared to a variety of previous methods, improving both the informativeness and diversity of generated output",
    "volume": "main",
    "checked": true,
    "id": "27033b8f72bf8cb7662c9f92b3ccb3c476db7135",
    "citation_count": 93
  },
  "https://aclanthology.org/P19-1540": {
    "title": "Ordinal and Attribute Aware Response Generation in a Multimodal Dialogue System",
    "abstract": "Multimodal dialogue systems have opened new frontiers in the traditional goal-oriented dialogue systems. The state-of-the-art dialogue systems are primarily based on unimodal sources, predominantly the text, and hence cannot capture the information present in the other sources such as videos, audios, images etc. With the availability of large scale multimodal dialogue dataset (MMD) (Saha et al., 2018) on the fashion domain, the visual appearance of the products is essential for understanding the intention of the user. Without capturing the information from both the text and image, the system will be incapable of generating correct and desirable responses. In this paper, we propose a novel position and attribute aware attention mechanism to learn enhanced image representation conditioned on the user utterance. Our evaluation shows that the proposed model can generate appropriate responses while preserving the position and attribute information. Experimental results also prove that our proposed approach attains superior performance compared to the baseline models, and outperforms the state-of-the-art approaches on text similarity based evaluation metrics",
    "volume": "main",
    "checked": true,
    "id": "314b6a975373f69f7628f2bc8647084181c84ac9",
    "citation_count": 16
  },
  "https://aclanthology.org/P19-1541": {
    "title": "Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference",
    "abstract": "Dialogue contexts are proven helpful in the spoken language understanding (SLU) system and they are typically encoded with explicit memory representations. However, most of the previous models learn the context memory with only one objective to maximizing the SLU performance, leaving the context memory under-exploited. In this paper, we propose a new dialogue logistic inference (DLI) task to consolidate the context memory jointly with SLU in the multi-task framework. DLI is defined as sorting a shuffled dialogue session into its original logical order and shares the same memory encoder and retrieval mechanism as the SLU model. Our experimental results show that various popular contextual SLU models can benefit from our approach, and improvements are quite impressive, especially in slot filling",
    "volume": "main",
    "checked": true,
    "id": "5fd491aab4884bc3adc34fcbacef9cd898b322c6",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-1542": {
    "title": "Personalizing Dialogue Agents via Meta-Learning",
    "abstract": "Existing personalized dialogue models use human designed persona descriptions to improve dialogue consistency. Collecting such descriptions from existing dialogues is expensive and requires hand-crafted feature designs. In this paper, we propose to extend Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) to personalized dialogue learning without using any persona descriptions. Our model learns to quickly adapt to new personas by leveraging only a few dialogue samples collected from the same user, which is fundamentally different from conditioning the response on the persona descriptions. Empirical results on Persona-chat dataset (Zhang et al., 2018) indicate that our solution outperforms non-meta-learning baselines using automatic evaluation metrics, and in terms of human-evaluated fluency and consistency",
    "volume": "main",
    "checked": true,
    "id": "bbc1fa2c9c54d8916469f413fdceb6d4087267a4",
    "citation_count": 114
  },
  "https://aclanthology.org/P19-1543": {
    "title": "Reading Turn by Turn: Hierarchical Attention Architecture for Spoken Dialogue Comprehension",
    "abstract": "Comprehending multi-turn spoken conversations is an emerging research area, presenting challenges different from reading comprehension of passages due to the interactive nature of information exchange from at least two speakers. Unlike passages, where sentences are often the default semantic modeling unit, in multi-turn conversations, a turn is a topically coherent unit embodied with immediately relevant context, making it a linguistically intuitive segment for computationally modeling verbal interactions. Therefore, in this work, we propose a hierarchical attention neural network architecture, combining turn-level and word-level attention mechanisms, to improve spoken dialogue comprehension performance. Experiments are conducted on a multi-turn conversation dataset, where nurses inquire and discuss symptom information with patients. We empirically show that the proposed approach outperforms standard attention baselines, achieves more efficient learning outcomes, and is more robust to lengthy and out-of-distribution test samples",
    "volume": "main",
    "checked": true,
    "id": "0e4be7a58d3e65a20b3deec411c4f0ebbe93fccc",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1544": {
    "title": "A Novel Bi-directional Interrelated Model for Joint Intent Detection and Slot Filling",
    "abstract": "A spoken language understanding (SLU) system includes two main tasks, slot filling (SF) and intent detection (ID). The joint model for the two tasks is becoming a tendency in SLU. But the bi-directional interrelated connections between the intent and slots are not established in the existing joint models. In this paper, we propose a novel bi-directional interrelated model for joint intent detection and slot filling. We introduce an SF-ID network to establish direct connections for the two tasks to help them promote each other mutually. Besides, we design an entirely new iteration mechanism inside the SF-ID network to enhance the bi-directional interrelated connections. The experimental results show that the relative improvement in the sentence-level semantic frame accuracy of our model is 3.79% and 5.42% on ATIS and Snips datasets, respectively, compared to the state-of-the-art model",
    "volume": "main",
    "checked": true,
    "id": "52919d9a881eee8645a1ff796d75137d5d8d9383",
    "citation_count": 154
  },
  "https://aclanthology.org/P19-1545": {
    "title": "Dual Supervised Learning for Natural Language Understanding and Generation",
    "abstract": "Natural language understanding (NLU) and natural language generation (NLG) are both critical research topics in the NLP and dialogue fields. Natural language understanding is to extract the core semantic meaning from the given utterances, while natural language generation is opposite, of which the goal is to construct corresponding sentences based on the given semantics. However, such dual relationship has not been investigated in literature. This paper proposes a novel learning framework for natural language understanding and generation on top of dual supervised learning, providing a way to exploit the duality. The preliminary experiments show that the proposed approach boosts the performance for both tasks, demonstrating the effectiveness of the dual relationship",
    "volume": "main",
    "checked": true,
    "id": "d833a5a729107ad1fbcd6d49f80f5677717bf048",
    "citation_count": 21
  },
  "https://aclanthology.org/P19-1546": {
    "title": "SUMBT: Slot-Utterance Matching for Universal and Scalable Belief Tracking",
    "abstract": "In goal-oriented dialog systems, belief trackers estimate the probability distribution of slot-values at every dialog turn. Previous neural approaches have modeled domain- and slot-dependent belief trackers, and have difficulty in adding new slot-values, resulting in lack of flexibility of domain ontology configurations. In this paper, we propose a new approach to universal and scalable belief tracker, called slot-utterance matching belief tracker (SUMBT). The model learns the relations between domain-slot-types and slot-values appearing in utterances through attention mechanisms based on contextual semantic vectors. Furthermore, the model predicts slot-value labels in a non-parametric way. From our experiments on two dialog corpora, WOZ 2.0 and MultiWOZ, the proposed model showed performance improvement in comparison with slot-dependent methods and achieved the state-of-the-art joint accuracy",
    "volume": "main",
    "checked": true,
    "id": "1368c8d46c68a0bb6bfa8b4c3c12e3c49051a3dc",
    "citation_count": 118
  },
  "https://aclanthology.org/P19-1547": {
    "title": "Robust Zero-Shot Cross-Domain Slot Filling with Example Values",
    "abstract": "Task-oriented dialog systems increasingly rely on deep learning-based slot filling models, usually needing extensive labeled training data for target domains. Often, however, little to no target domain training data may be available, or the training and target domain schemas may be misaligned, as is common for web forms on similar websites. Prior zero-shot slot filling models use slot descriptions to learn concepts, but are not robust to misaligned schemas. We propose utilizing both the slot description and a small number of examples of slot values, which may be easily available, to learn semantic representations of slots which are transferable across domains and robust to misaligned schemas. Our approach outperforms state-of-the-art models on two multi-domain datasets, especially in the low-data setting",
    "volume": "main",
    "checked": true,
    "id": "7eb9c6c45244653f902b45744c866734e1cd4bf7",
    "citation_count": 62
  },
  "https://aclanthology.org/P19-1548": {
    "title": "Deep Unknown Intent Detection with Margin Loss",
    "abstract": "Identifying the unknown (novel) user intents that have never appeared in the training set is a challenging task in the dialogue system. In this paper, we present a two-stage method for detecting unknown intents. We use bidirectional long short-term memory (BiLSTM) network with the margin loss as the feature extractor. With margin loss, we can learn discriminative deep features by forcing the network to maximize inter-class variance and to minimize intra-class variance. Then, we feed the feature vectors to the density-based novelty detection algorithm, local outlier factor (LOF), to detect unknown intents. Experiments on two benchmark datasets show that our method can yield consistent improvements compared with the baseline methods",
    "volume": "main",
    "checked": true,
    "id": "90521586562ef699a295ff3f33ef1cb1f67b5b3c",
    "citation_count": 80
  },
  "https://aclanthology.org/P19-1549": {
    "title": "Modeling Semantic Relationship in Multi-turn Conversations with Hierarchical Latent Variables",
    "abstract": "Multi-turn conversations consist of complex semantic structures, and it is still a challenge to generate coherent and diverse responses given previous utterances. It’s practical that a conversation takes place under a background, meanwhile, the query and response are usually most related and they are consistent in topic but also different in content. However, little work focuses on such hierarchical relationship among utterances. To address this problem, we propose a Conversational Semantic Relationship RNN (CSRR) model to construct the dependency explicitly. The model contains latent variables in three hierarchies. The discourse-level one captures the global background, the pair-level one stands for the common topic information between query and response, and the utterance-level ones try to represent differences in content. Experimental results show that our model significantly improves the quality of responses in terms of fluency, coherence, and diversity compared to baseline methods",
    "volume": "main",
    "checked": true,
    "id": "fae43f08450e6aacb85d654cf9a2d6a964000e65",
    "citation_count": 26
  },
  "https://aclanthology.org/P19-1550": {
    "title": "Rationally Reappraising ATIS-based Dialogue Systems",
    "abstract": "The Air Travel Information Service (ATIS) corpus has been the most common benchmark for evaluating Spoken Language Understanding (SLU) tasks for more than three decades since it was released. Recent state-of-the-art neural models have obtained F1-scores near 98% on the task of slot filling. We developed a rule-based grammar for the ATIS domain that achieves a 95.82% F1-score on our evaluation set. In the process, we furthermore discovered numerous shortcomings in the ATIS corpus annotation, which we have fixed. This paper presents a detailed account of these shortcomings, our proposed repairs, our rule-based grammar and the neural slot-filling architectures associated with ATIS. We also rationally reappraise the motivations for choosing a neural architecture in view of this account. Fixing the annotation errors results in a relative error reduction of between 19.4 and 52% across all architectures. We nevertheless argue that neural models must play a different role in ATIS dialogues because of the latter’s lack of variety",
    "volume": "main",
    "checked": true,
    "id": "5f881357bc6fd42742f3c2ee746cbe5a855abda8",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1551": {
    "title": "Learning Latent Trees with Stochastic Perturbations and Differentiable Dynamic Programming",
    "abstract": "We treat projective dependency trees as latent variables in our probabilistic model and induce them in such a way as to be beneficial for a downstream task, without relying on any direct tree supervision. Our approach relies on Gumbel perturbations and differentiable dynamic programming. Unlike previous approaches to latent tree learning, we stochastically sample global structures and our parser is fully differentiable. We illustrate its effectiveness on sentiment analysis and natural language inference tasks. We also study its properties on a synthetic structure induction task. Ablation studies emphasize the importance of both stochasticity and constraining latent structures to be projective trees",
    "volume": "main",
    "checked": true,
    "id": "0f45215ddad707f955ab08e90397fbc5c2a2298c",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1552": {
    "title": "Neural-based Chinese Idiom Recommendation for Enhancing Elegance in Essay Writing",
    "abstract": "Although the proper use of idioms can enhance the elegance of writing, the active use of various expressions is a challenge because remembering idioms is difficult. In this study, we address the problem of idiom recommendation by leveraging a neural machine translation framework, in which we suppose that idioms are written with one pseudo target language. Two types of real-life datasets are collected to support this study. Experimental results show that the proposed approach achieves promising performance compared with other baseline methods",
    "volume": "main",
    "checked": true,
    "id": "0f8dfb28f55b6e2e5ecc97869b08001e1d81d2c3",
    "citation_count": 10
  },
  "https://aclanthology.org/P19-1553": {
    "title": "Better Exploiting Latent Variables in Text Modeling",
    "abstract": "We show that sampling latent variables multiple times at a gradient step helps in improving a variational autoencoder and propose a simple and effective method to better exploit these latent variables through hidden state averaging. Consistent gains in performance on two different datasets, Penn Treebank and Yahoo, indicate the generalizability of our method",
    "volume": "main",
    "checked": true,
    "id": "4077ef4c259accbb75a6f1fd8a3c0095fd2caf4b",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-1554": {
    "title": "Misleading Failures of Partial-input Baselines",
    "abstract": "Recent work establishes dataset difficulty and removes annotation artifacts via partial-input baselines (e.g., hypothesis-only model for SNLI or question-only model for VQA). A successful partial-input baseline indicates that the dataset is cheatable. But the converse is not necessarily true: failures of partial-input baselines do not mean the dataset is free of artifacts. We first design artificial datasets to illustrate how the trivial patterns that are only visible in the full input can evade any partial-input baseline. Next, we identify such artifacts in the SNLI dataset—a hypothesis-only model augmented with trivial patterns in the premise can solve 15% of previously-thought “hard” examples. Our work provides a caveat for the use and creation of partial-input baselines for datasets",
    "volume": "main",
    "checked": true,
    "id": "a9daf0c137f182763a6ec001f4f2d41a517fe595",
    "citation_count": 24
  },
  "https://aclanthology.org/P19-1555": {
    "title": "Soft Contextual Data Augmentation for Neural Machine Translation",
    "abstract": "While data augmentation is an important trick to boost the accuracy of deep learning methods in computer vision tasks, its study in natural language tasks is still very limited. In this paper, we present a novel data augmentation method for neural machine translation.Different from previous augmentation methods that randomly drop, swap or replace words with other words in a sentence, we softly augment a randomly chosen word in a sentence by its contextual mixture of multiple related words. More accurately, we replace the one-hot representation of a word by a distribution (provided by a language model) over the vocabulary, i.e., replacing the embedding of this word by a weighted combination of multiple semantically similar words. Since the weights of those words depend on the contextual information of the word to be replaced,the newly generated sentences capture much richer information than previous augmentation methods. Experimental results on both small scale and large scale machine translation data sets demonstrate the superiority of our method over strong baselines",
    "volume": "main",
    "checked": true,
    "id": "258e92bd6ceaeb78e7384eeea57b4c7a2c356cfa",
    "citation_count": 85
  },
  "https://aclanthology.org/P19-1556": {
    "title": "Reversing Gradients in Adversarial Domain Adaptation for Question Deduplication and Textual Entailment Tasks",
    "abstract": "Adversarial domain adaptation has been recently proposed as an effective technique for textual matching tasks, such as question deduplication. Here we investigate the use of gradient reversal on adversarial domain adaptation to explicitly learn both shared and unshared (domain specific) representations between two textual domains. In doing so, gradient reversal learns features that explicitly compensate for domain mismatch, while still distilling domain specific knowledge that can improve target domain accuracy. We evaluate reversing gradients for adversarial adaptation on multiple domains, and demonstrate that it significantly outperforms other methods on question deduplication as well as on recognizing textual entailment (RTE) tasks, achieving up to 7% absolute boost in base model accuracy on some datasets",
    "volume": "main",
    "checked": true,
    "id": "38a9765ca11f3141b944df482a8d882bd277141f",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1557": {
    "title": "Towards Integration of Statistical Hypothesis Tests into Deep Neural Networks",
    "abstract": "We report our ongoing work about a new deep architecture working in tandem with a statistical test procedure for jointly training texts and their label descriptions for multi-label and multi-class classification tasks. A statistical hypothesis testing method is used to extract the most informative words for each given class. These words are used as a class description for more label-aware text classification. Intuition is to help the model to concentrate on more informative words rather than more frequent ones. The model leverages the use of label descriptions in addition to the input text to enhance text classification performance. Our method is entirely data-driven, has no dependency on other sources of information than the training data, and is adaptable to different classification problems by providing appropriate training data without major hyper-parameter tuning. We trained and tested our system on several publicly available datasets, where we managed to improve the state-of-the-art on one set with a high margin and to obtain competitive results on all other ones",
    "volume": "main",
    "checked": true,
    "id": "46b1b44ca723be1a1da2677bd755fd3505d3d6e6",
    "citation_count": 2
  },
  "https://aclanthology.org/P19-1558": {
    "title": "Depth Growing for Neural Machine Translation",
    "abstract": "While very deep neural networks have shown effectiveness for computer vision and text classification applications, how to increase the network depth of the neural machine translation (NMT) models for better translation quality remains a challenging problem. Directly stacking more blocks to the NMT model results in no improvement and even drop in performance. In this work, we propose an effective two-stage approach with three specially designed components to construct deeper NMT models, which result in significant improvements over the strong Transformer baselines on WMT14 English→German and English→French translation tasks",
    "volume": "main",
    "checked": true,
    "id": "e146050bfe5e00063e55d467125a86daa45d7e1e",
    "citation_count": 34
  },
  "https://aclanthology.org/P19-1559": {
    "title": "Generating Fluent Adversarial Examples for Natural Languages",
    "abstract": "Efficiently building an adversarial attacker for natural language processing (NLP) tasks is a real challenge. Firstly, as the sentence space is discrete, it is difficult to make small perturbations along the direction of gradients. Secondly, the fluency of the generated examples cannot be guaranteed. In this paper, we propose MHA, which addresses both problems by performing Metropolis-Hastings sampling, whose proposal is designed with the guidance of gradients. Experiments on IMDB and SNLI show that our proposed MHAoutperforms the baseline model on attacking capability. Adversarial training with MHA also leads to better robustness and performance",
    "volume": "main",
    "checked": true,
    "id": "afd975a296886e89722891ad13c8dba0d26b1ed2",
    "citation_count": 107
  },
  "https://aclanthology.org/P19-1560": {
    "title": "Towards Explainable NLP: A Generative Explanation Framework for Text Classification",
    "abstract": "Building explainable systems is a critical problem in the field of Natural Language Processing (NLP), since most machine learning models provide no explanations for the predictions. Existing approaches for explainable machine learning systems tend to focus on interpreting the outputs or the connections between inputs and outputs. However, the fine-grained information (e.g. textual explanations for the labels) is often ignored, and the systems do not explicitly generate the human-readable explanations. To solve this problem, we propose a novel generative explanation framework that learns to make classification decisions and generate fine-grained explanations at the same time. More specifically, we introduce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations. We construct two new datasets that contain summaries, rating scores, and fine-grained reasons. We conduct experiments on both datasets, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time",
    "volume": "main",
    "checked": true,
    "id": "b59646ddc6c102da27d42097d99f1deada65c84a",
    "citation_count": 83
  },
  "https://aclanthology.org/P19-1561": {
    "title": "Combating Adversarial Misspellings with Robust Word Recognition",
    "abstract": "To combat adversarial spelling mistakes, we propose placing a word recognition model in front of the downstream classifier. Our word recognition models build upon the RNN semi-character architecture, introducing several new backoff strategies for handling rare and unseen words. Trained to recognize words corrupted by random adds, drops, swaps, and keyboard mistakes, our method achieves 32% relative (and 3.3% absolute) error reduction over the vanilla semi-character model. Notably, our pipeline confers robustness on the downstream classifier, outperforming both adversarial training and off-the-shelf spell checkers. Against a BERT model fine-tuned for sentiment analysis, a single adversarially-chosen character attack lowers accuracy from 90.3% to 45.8%. Our defense restores accuracy to 75%. Surprisingly, better word recognition does not always entail greater robustness. Our analysis reveals that robustness also depends upon a quantity that we denote the sensitivity",
    "volume": "main",
    "checked": true,
    "id": "43c844c30765f3fa25bfabd83490ef826b9ceca1",
    "citation_count": 201
  },
  "https://aclanthology.org/P19-1562": {
    "title": "An Empirical Investigation of Structured Output Modeling for Graph-based Neural Dependency Parsing",
    "abstract": "In this paper, we investigate the aspect of structured output modeling for the state-of-the-art graph-based neural dependency parser (Dozat and Manning, 2017). With evaluations on 14 treebanks, we empirically show that global output-structured models can generally obtain better performance, especially on the metric of sentence-level Complete Match. However, probably because neural models already learn good global views of the inputs, the improvement brought by structured output modeling is modest",
    "volume": "main",
    "checked": true,
    "id": "eec491a4d722b2a23f0b1cc288cefa7478a4ffd3",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1563": {
    "title": "Observing Dialogue in Therapy: Categorizing and Forecasting Behavioral Codes",
    "abstract": "Automatically analyzing dialogue can help understand and guide behavior in domains such as counseling, where interactions are largely mediated by conversation. In this paper, we study modeling behavioral codes used to asses a psychotherapy treatment style called Motivational Interviewing (MI), which is effective for addressing substance abuse and related problems. Specifically, we address the problem of providing real-time guidance to therapists with a dialogue observer that (1) categorizes therapist and client MI behavioral codes and, (2) forecasts codes for upcoming utterances to help guide the conversation and potentially alert the therapist. For both tasks, we define neural network models that build upon recent successes in dialogue modeling. Our experiments demonstrate that our models can outperform several baselines for both tasks. We also report the results of a careful analysis that reveals the impact of the various network design tradeoffs for modeling therapy dialogue",
    "volume": "main",
    "checked": true,
    "id": "f1e192ec0574eece6e7fcc5c1def2c358dc47e73",
    "citation_count": 21
  },
  "https://aclanthology.org/P19-1564": {
    "title": "Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems",
    "abstract": "Developing Video-Grounded Dialogue Systems (VGDS), where a dialogue is conducted based on visual and audio aspects of a given video, is significantly more challenging than traditional image or text-grounded dialogue systems because (1) feature space of videos span across multiple picture frames, making it difficult to obtain semantic information; and (2) a dialogue agent must perceive and process information from different modalities (audio, video, caption, etc.) to obtain a comprehensive understanding. Most existing work is based on RNNs and sequence-to-sequence architectures, which are not very effective for capturing complex long-term dependencies (like in videos). To overcome this, we propose Multimodal Transformer Networks (MTN) to encode videos and incorporate information from different modalities. We also propose query-aware attention through an auto-encoder to extract query-aware features from non-text modalities. We develop a training procedure to simulate token-level decoding to improve the quality of generated responses during inference. We get state of the art performance on Dialogue System Technology Challenge 7 (DSTC7). Our model also generalizes to another multimodal visual-grounded dialogue task, and obtains promising performance",
    "volume": "main",
    "checked": true,
    "id": "594ad264d6b92afb9d13cb56ad8ffadba94a9f7a",
    "citation_count": 82
  },
  "https://aclanthology.org/P19-1565": {
    "title": "Target-Guided Open-Domain Conversation",
    "abstract": "Many real-world open-domain conversation applications have specific goals to achieve during open-ended chats, such as recommendation, psychotherapy, education, etc. We study the problem of imposing conversational goals on open-domain chat agents. In particular, we want a conversational system to chat naturally with human and proactively guide the conversation to a designated target subject. The problem is challenging as no public data is available for learning such a target-guided strategy. We propose a structured approach that introduces coarse-grained keywords to control the intended content of system responses. We then attain smooth conversation transition through turn-level supervised learning, and drive the conversation towards the target with discourse-level constraints. We further derive a keyword-augmented conversation dataset for the study. Quantitative and human evaluations show our system can produce meaningful and effective conversations, significantly improving over other approaches",
    "volume": "main",
    "checked": true,
    "id": "06236fa6fd407f4b629be41049c880a8569c20ab",
    "citation_count": 75
  },
  "https://aclanthology.org/P19-1566": {
    "title": "Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good",
    "abstract": "Developing intelligent persuasive conversational agents to change people’s opinions and actions for social good is the frontier in advancing the ethical development of automated dialogue systems. To do so, the first step is to understand the intricate organization of strategic disclosures and appeals employed in human persuasion conversations. We designed an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. We collected a large dataset with 1,017 dialogues and annotated emerging persuasion strategies from a subset. Based on the annotation, we built a baseline classifier with context information and sentence-level features to predict the 10 persuasion strategies used in the corpus. Furthermore, to develop an understanding of personalized persuasion processes, we analyzed the relationships between individuals’ demographic and psychological backgrounds including personality, morality, value systems, and their willingness for donation. Then, we analyzed which types of persuasion strategies led to a greater amount of donation depending on the individuals’ personal backgrounds. This work lays the ground for developing a personalized persuasive dialogue system",
    "volume": "main",
    "checked": true,
    "id": "3d52d429b4d83d096dd354e8470bf3655e8b67bc",
    "citation_count": 111
  },
  "https://aclanthology.org/P19-1567": {
    "title": "Improving Neural Conversational Models with Entropy-Based Data Filtering",
    "abstract": "Current neural network-based conversational models lack diversity and generate boring responses to open-ended utterances. Priors such as persona, emotion, or topic provide additional information to dialog models to aid response generation, but annotating a dataset with priors is expensive and such annotations are rarely available. While previous methods for improving the quality of open-domain response generation focused on either the underlying model or the training objective, we present a method of filtering dialog datasets by removing generic utterances from training data using a simple entropy-based approach that does not require human supervision. We conduct extensive experiments with different variations of our method, and compare dialog models across 17 evaluation metrics to show that training on datasets filtered this way results in better conversational quality as chatbots learn to output more diverse responses",
    "volume": "main",
    "checked": true,
    "id": "6f729336e4150f2468a6f7587d823046f7c0419c",
    "citation_count": 37
  },
  "https://aclanthology.org/P19-1568": {
    "title": "Zero-shot Word Sense Disambiguation using Sense Definition Embeddings",
    "abstract": "Word Sense Disambiguation (WSD) is a long-standing but open problem in Natural Language Processing (NLP). WSD corpora are typically small in size, owing to an expensive annotation process. Current supervised WSD methods treat senses as discrete labels and also resort to predicting the Most-Frequent-Sense (MFS) for words unseen during training. This leads to poor performance on rare and unseen senses. To overcome this challenge, we propose Extended WSD Incorporating Sense Embeddings (EWISE), a supervised model to perform WSD by predicting over a continuous sense embedding space as opposed to a discrete label space. This allows EWISE to generalize over both seen and unseen senses, thus achieving generalized zero-shot learning. To obtain target sense embeddings, EWISE utilizes sense definitions. EWISE learns a novel sentence encoder for sense definitions by using WordNet relations and also ConvE, a recently proposed knowledge graph embedding method. We also compare EWISE against other sentence encoders pretrained on large corpora to generate definition embeddings. EWISE achieves new state-of-the-art WSD performance",
    "volume": "main",
    "checked": true,
    "id": "2bd840084332c78af7c19363197db50a61074c12",
    "citation_count": 79
  },
  "https://aclanthology.org/P19-1569": {
    "title": "Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation",
    "abstract": "Contextual embeddings represent a new generation of semantic representations learned from Neural Language Modelling (NLM) that addresses the issue of meaning conflation hampering traditional word embeddings. In this work, we show that contextual embeddings can be used to achieve unprecedented gains in Word Sense Disambiguation (WSD) tasks. Our approach focuses on creating sense-level embeddings with full-coverage of WordNet, and without recourse to explicit knowledge of sense distributions or task-specific modelling. As a result, a simple Nearest Neighbors (k-NN) method using our representations is able to consistently surpass the performance of previous systems using powerful neural sequencing models. We also analyse the robustness of our approach when ignoring part-of-speech and lemma features, requiring disambiguation against the full sense inventory, and revealing shortcomings to be improved. Finally, we explore applications of our sense embeddings for concept-level analyses of contextual embeddings and their respective NLMs",
    "volume": "main",
    "checked": true,
    "id": "b85ec2b911b7b5e4bc5519d503d265b6c9c9d0c2",
    "citation_count": 102
  },
  "https://aclanthology.org/P19-1570": {
    "title": "Word2Sense: Sparse Interpretable Word Embeddings",
    "abstract": "We present an unsupervised method to generate Word2Sense word embeddings that are interpretable — each dimension of the embedding space corresponds to a fine-grained sense, and the non-negative value of the embedding along the j-th dimension represents the relevance of the j-th sense to the word. The underlying LDA-based generative model can be extended to refine the representation of a polysemous word in a short context, allowing us to use the embedings in contextual tasks. On computational NLP tasks, Word2Sense embeddings compare well with other word embeddings generated by unsupervised methods. Across tasks such as word similarity, entailment, sense induction, and contextual interpretation, Word2Sense is competitive with the state-of-the-art method for that task. Word2Sense embeddings are at least as sparse and fast to compute as prior art",
    "volume": "main",
    "checked": true,
    "id": "a01f3039dd2ef75b3db08e4cedd0fcf7139f465c",
    "citation_count": 38
  },
  "https://aclanthology.org/P19-1571": {
    "title": "Modeling Semantic Compositionality with Sememe Knowledge",
    "abstract": "Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to model SC while few works consider external knowledge in models. In this paper, we verify the effectiveness of sememes, the minimum semantic units of human languages, in modeling SC by a confirmatory experiment. Furthermore, we make the first attempt to incorporate sememe knowledge into SC models, and employ the sememe-incorporated models in learning representations of multiword expressions, a typical task of SC. In experiments, we implement our models by incorporating knowledge from a famous sememe knowledge base HowNet and perform both intrinsic and extrinsic evaluations. Experimental results show that our models achieve significant performance boost as compared to the baseline methods without considering sememe knowledge. We further conduct quantitative analysis and case studies to demonstrate the effectiveness of applying sememe knowledge in modeling SC.All the code and data of this paper can be obtained on https://github.com/thunlp/Sememe-SC",
    "volume": "main",
    "checked": true,
    "id": "01e5b087b3aa2ef4bf51f68dd61003b1af9156d6",
    "citation_count": 25
  },
  "https://aclanthology.org/P19-1572": {
    "title": "Predicting Humorousness and Metaphor Novelty with Gaussian Process Preference Learning",
    "abstract": "The inability to quantify key aspects of creative language is a frequent obstacle to natural language understanding. To address this, we introduce novel tasks for evaluating the creativeness of language—namely, scoring and ranking text by humorousness and metaphor novelty. To sidestep the difficulty of assigning discrete labels or numeric scores, we learn from pairwise comparisons between texts. We introduce a Bayesian approach for predicting humorousness and metaphor novelty using Gaussian process preference learning (GPPL), which achieves a Spearman’s ρ of 0.56 against gold using word embeddings and linguistic features. Our experiments show that given sparse, crowdsourced annotation data, ranking using GPPL outperforms best–worst scaling. We release a new dataset for evaluating humour containing 28,210 pairwise comparisons of 4,030 texts, and make our software freely available",
    "volume": "main",
    "checked": true,
    "id": "3f9bb59e8bac1b06f21a0da8685bafdce0b0f7c3",
    "citation_count": 16
  },
  "https://aclanthology.org/P19-1573": {
    "title": "Empirical Linguistic Study of Sentence Embeddings",
    "abstract": "The purpose of the research is to answer the question whether linguistic information is retained in vector representations of sentences. We introduce a method of analysing the content of sentence embeddings based on universal probing tasks, along with the classification datasets for two contrasting languages. We perform a series of probing and downstream experiments with different types of sentence embeddings, followed by a thorough analysis of the experimental results. Aside from dependency parser-based embeddings, linguistic information is retained best in the recently proposed LASER sentence embeddings",
    "volume": "main",
    "checked": true,
    "id": "0c4f8dd922530471410e43ae13d9f2a3e572cb1a",
    "citation_count": 19
  },
  "https://aclanthology.org/P19-1574": {
    "title": "Probing for Semantic Classes: Diagnosing the Meaning Content of Word Embeddings",
    "abstract": "Word embeddings typically represent different meanings of a word in a single conflated vector. Empirical analysis of embeddings of ambiguous words is currently limited by the small size of manually annotated resources and by the fact that word senses are treated as unrelated individual concepts. We present a large dataset based on manual Wikipedia annotations and word senses, where word senses from different words are related by semantic classes. This is the basis for novel diagnostic tests for an embedding’s content: we probe word embeddings for semantic classes and analyze the embedding space by classifying embeddings into semantic classes. Our main findings are: (i) Information about a sense is generally represented well in a single-vector embedding – if the sense is frequent. (ii) A classifier can accurately predict whether a word is single-sense or multi-sense, based only on its embedding. (iii) Although rare senses are not well represented in single-vector embeddings, this does not have negative impact on an NLP application whose performance depends on frequent senses",
    "volume": "main",
    "checked": true,
    "id": "4158285df6a2b0f5a0de567573d995328ccbbb38",
    "citation_count": 25
  },
  "https://aclanthology.org/P19-1575": {
    "title": "Deep Neural Model Inspection and Comparison via Functional Neuron Pathways",
    "abstract": "We introduce a general method for the interpretation and comparison of neural models. The method is used to factor a complex neural model into its functional components, which are comprised of sets of co-firing neurons that cut across layers of the network architecture, and which we call neural pathways. The function of these pathways can be understood by identifying correlated task level and linguistic heuristics in such a way that this knowledge acts as a lens for approximating what the network has learned to apply to its intended task. As a case study for investigating the utility of these pathways, we present an examination of pathways identified in models trained for two standard tasks, namely Named Entity Recognition and Recognizing Textual Entailment",
    "volume": "main",
    "checked": true,
    "id": "1aba2e7139161fc277e54265e8543539b3c2ca75",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-1576": {
    "title": "Collocation Classification with Unsupervised Relation Vectors",
    "abstract": "Lexical relation classification is the task of predicting whether a certain relation holds between a given pair of words. In this paper, we explore to which extent the current distributional landscape based on word embeddings provides a suitable basis for classification of collocations, i.e., pairs of words between which idiosyncratic lexical relations hold. First, we introduce a novel dataset with collocations categorized according to lexical functions. Second, we conduct experiments on a subset of this benchmark, comparing it in particular to the well known DiffVec dataset. In these experiments, in addition to simple word vector arithmetic operations, we also investigate the role of unsupervised relation vectors as a complementary input. While these relation vectors indeed help, we also show that lexical function classification poses a greater challenge than the syntactic and semantic relations that are typically used for benchmarks in the literature",
    "volume": "main",
    "checked": true,
    "id": "526769fbf692d144b46159326a42a9730c1fa679",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-1577": {
    "title": "Corpus-based Check-up for Thesaurus",
    "abstract": "In this paper we discuss the usefulness of applying a checking procedure to existing thesauri. The procedure is based on the analysis of discrepancies of corpus-based and thesaurus-based word similarities. We applied the procedure to more than 30 thousand words of the Russian wordnet and found some serious errors in word sense description, including inaccurate relationships and missing senses of ambiguous words",
    "volume": "main",
    "checked": true,
    "id": "5ada2b04ece587802d4ce27f5cfaa8e64dcaed67",
    "citation_count": 10
  },
  "https://aclanthology.org/P19-1578": {
    "title": "Confusionset-guided Pointer Networks for Chinese Spelling Check",
    "abstract": "This paper proposes Confusionset-guided Pointer Networks for Chinese Spell Check (CSC) task. More concretely, our approach utilizes the off-the-shelf confusionset for guiding the character generation. To this end, our novel Seq2Seq model jointly learns to copy a correct character from an input sentence through a pointer network, or generate a character from the confusionset rather than the entire vocabulary. We conduct experiments on three human-annotated datasets, and results demonstrate that our proposed generative model outperforms all competitor models by a large margin of up to 20% F1 score, achieving state-of-the-art performance on three datasets",
    "volume": "main",
    "checked": true,
    "id": "c07181117ef987628ca2895a576db29c4b9221cc",
    "citation_count": 43
  },
  "https://aclanthology.org/P19-1579": {
    "title": "Generalized Data Augmentation for Low-Resource Translation",
    "abstract": "Low-resource language pairs with a paucity of parallel data pose challenges for machine translation in terms of both adequacy and fluency. Data augmentation utilizing a large amount of monolingual data is regarded as an effective way to alleviate the problem. In this paper, we propose a general framework of data augmentation for low-resource machine translation not only using target-side monolingual data, but also by pivoting through a related high-resource language. Specifically, we experiment with a two-step pivoting method to convert high-resource data to the low-resource language, making best use of available resources to better approximate the true distribution of the low-resource language. First, we inject low-resource words into high-resource sentences through an induced bilingual dictionary. Second, we further edit the high-resource data injected with low-resource words using a modified unsupervised machine translation framework. Extensive experiments on four low-resource datasets show that under extreme low-resource settings, our data augmentation techniques improve translation quality by up to 1.5 to 8 BLEU points compared to supervised back-translation baselines",
    "volume": "main",
    "checked": true,
    "id": "595306f993993e44e2c2f674367103f44df03d9b",
    "citation_count": 65
  },
  "https://aclanthology.org/P19-1580": {
    "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
    "abstract": "Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU",
    "volume": "main",
    "checked": true,
    "id": "07a64686ce8e43ac475a8d820a8a9f1d87989583",
    "citation_count": 549
  },
  "https://aclanthology.org/P19-1581": {
    "title": "Better OOV Translation with Bilingual Terminology Mining",
    "abstract": "Unseen words, also called out-of-vocabulary words (OOVs), are difficult for machine translation. In neural machine translation, byte-pair encoding can be used to represent OOVs, but they are still often incorrectly translated. We improve the translation of OOVs in NMT using easy-to-obtain monolingual data. We look for OOVs in the text to be translated and translate them using simple-to-construct bilingual word embeddings (BWEs). In our MT experiments we take the 5-best candidates, which is motivated by intrinsic mining experiments. Using all five of the proposed target language words as queries we mine target-language sentences. We then back-translate, forcing the back-translation of each of the five proposed target-language OOV-translation-candidates to be the original source-language OOV. We show that by using this synthetic data to fine-tune our system the translation of OOVs can be dramatically improved. In our experiments we use a system trained on Europarl and mine sentences containing medical terms from monolingual data",
    "volume": "main",
    "checked": true,
    "id": "c008595042423ddb167065b3f4d1216218e50da4",
    "citation_count": 14
  },
  "https://aclanthology.org/P19-1582": {
    "title": "Simultaneous Translation with Flexible Policy via Restricted Imitation Learning",
    "abstract": "Simultaneous translation is widely useful but remains one of the most difficult tasks in NLP. Previous work either uses fixed-latency policies, or train a complicated two-staged model using reinforcement learning. We propose a much simpler single model that adds a “delay” token to the target vocabulary, and design a restricted dynamic oracle to greatly simplify training. Experiments on Chinese <-> English simultaneous translation show that our work leads to flexible policies that achieve better BLEU scores and lower latencies compared to both fixed and RL-learned policies",
    "volume": "main",
    "checked": true,
    "id": "310050f7bf5bd5d630e7971114fbccbd6198a393",
    "citation_count": 57
  },
  "https://aclanthology.org/P19-1583": {
    "title": "Target Conditioned Sampling: Optimizing Data Selection for Multilingual Neural Machine Translation",
    "abstract": "To improve low-resource Neural Machine Translation (NMT) with multilingual corpus, training on the most related high-resource language only is generally more effective than us- ing all data available (Neubig and Hu, 2018). However, it remains a question whether a smart data selection strategy can further improve low-resource NMT with data from other auxiliary languages. In this paper, we seek to construct a sampling distribution over all multilingual data, so that it minimizes the training loss of the low-resource language. Based on this formulation, we propose and efficient algorithm, (TCS), which first samples a target sentence, and then conditionally samples its source sentence. Experiments show TCS brings significant gains of up to 2 BLEU improvements on three of four languages we test, with minimal training overhead",
    "volume": "main",
    "checked": true,
    "id": "a0cbaf59f563580f68523ab6839a436e38b6db18",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-1584": {
    "title": "Adversarial Learning of Privacy-Preserving Text Representations for De-Identification of Medical Records",
    "abstract": "De-identification is the task of detecting protected health information (PHI) in medical text. It is a critical step in sanitizing electronic health records (EHR) to be shared for research. Automatic de-identification classifiers can significantly speed up the sanitization process. However, obtaining a large and diverse dataset to train such a classifier that works well across many types of medical text poses a challenge as privacy laws prohibit the sharing of raw medical records. We introduce a method to create privacy-preserving shareable representations of medical text (i.e. they contain no PHI) that does not require expensive manual pseudonymization. These representations can be shared between organizations to create unified datasets for training de-identification models. Our representation allows training a simple LSTM-CRF de-identification model to an F1 score of 97.4%, which is comparable to a strong baseline that exposes private information in its representation. A robust, widely available de-identification classifier based on our representation could potentially enable studies for which de-identification would otherwise be too costly",
    "volume": "main",
    "checked": true,
    "id": "3d4325ed280bacdde69735d45421bacea35304a6",
    "citation_count": 28
  },
  "https://aclanthology.org/P19-1585": {
    "title": "Merge and Label: A Novel Neural Network Architecture for Nested NER",
    "abstract": "Named entity recognition (NER) is one of the best studied tasks in natural language processing. However, most approaches are not capable of handling nested structures which are common in many applications. In this paper we introduce a novel neural network architecture that first merges tokens and/or entities into entities forming nested structures, and then labels each of them independently. Unlike previous work, our merge and label approach predicts real-valued instead of discrete segmentation structures, which allow it to combine word and nested entity embeddings while maintaining differentiability. We evaluate our approach using the ACE 2005 Corpus, where it achieves state-of-the-art F1 of 74.6, further improved with contextual embeddings (BERT) to 82.4, an overall improvement of close to 8 F1 points over previous approaches trained on the same data. Additionally we compare it against BiLSTM-CRFs, the dominant approach for flat NER structures, demonstrating that its ability to predict nested structures does not impact performance in simpler cases",
    "volume": "main",
    "checked": true,
    "id": "d1952ce95709e714e6e468e52f1140941f7cbb47",
    "citation_count": 51
  },
  "https://aclanthology.org/P19-1586": {
    "title": "Low-resource Deep Entity Resolution with Transfer and Active Learning",
    "abstract": "Entity resolution (ER) is the task of identifying different representations of the same real-world entities across databases. It is a key step for knowledge base creation and text mining. Recent adaptation of deep learning methods for ER mitigates the need for dataset-specific feature engineering by constructing distributed representations of entity records. While these methods achieve state-of-the-art performance over benchmark data, they require large amounts of labeled data, which are typically unavailable in realistic ER applications. In this paper, we develop a deep learning-based method that targets low-resource settings for ER through a novel combination of transfer learning and active learning. We design an architecture that allows us to learn a transferable model from a high-resource setting to a low-resource one. To further adapt to the target dataset, we incorporate active learning that carefully selects a few informative examples to fine-tune the transferred model. Empirical evaluation demonstrates that our method achieves comparable, if not better, performance compared to state-of-the-art learning-based methods while using an order of magnitude fewer labels",
    "volume": "main",
    "checked": true,
    "id": "c57757597d0664a0f66d40108e50bf696044c9fe",
    "citation_count": 90
  },
  "https://aclanthology.org/P19-1587": {
    "title": "A Semi-Markov Structured Support Vector Machine Model for High-Precision Named Entity Recognition",
    "abstract": "Named entity recognition (NER) is the backbone of many NLP solutions. F1 score, the harmonic mean of precision and recall, is often used to select/evaluate the best models. However, when precision needs to be prioritized over recall, a state-of-the-art model might not be the best choice. There is little in literature that directly addresses training-time modifications to achieve higher precision information extraction. In this paper, we propose a neural semi-Markov structured support vector machine model that controls the precision-recall trade-off by assigning weights to different types of errors in the loss-augmented inference during training. The semi-Markov property provides more accurate phrase-level predictions, thereby improving performance. We empirically demonstrate the advantage of our model when high precision is required by comparing against strong baselines based on CRF. In our experiments with the CoNLL 2003 dataset, our model achieves a better precision-recall trade-off at various precision levels",
    "volume": "main",
    "checked": true,
    "id": "47797d3830507ac0f75bf01bb29c895ae99c2ef5",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1588": {
    "title": "Using Human Attention to Extract Keyphrase from Microblog Post",
    "abstract": "This paper studies automatic keyphrase extraction on social media. Previous works have achieved promising results on it, but they neglect human reading behavior during keyphrase annotating. The human attention is a crucial element of human reading behavior. It reveals the relevance of words to the main topics of the target text. Thus, this paper aims to integrate human attention into keyphrase extraction models. First, human attention is represented by the reading duration estimated from eye-tracking corpus. Then, we merge human attention with neural network models by an attention mechanism. In addition, we also integrate human attention into unsupervised models. To the best of our knowledge, we are the first to utilize human attention on keyphrase extraction tasks. The experimental results show that our models have significant improvements on two Twitter datasets",
    "volume": "main",
    "checked": true,
    "id": "827f15e8fb5f1ea047bd4503498265125807aab6",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1589": {
    "title": "Model-Agnostic Meta-Learning for Relation Classification with Limited Supervision",
    "abstract": "In this paper we frame the task of supervised relation classification as an instance of meta-learning. We propose a model-agnostic meta-learning protocol for training relation classifiers to achieve enhanced predictive performance in limited supervision settings. During training, we aim to not only learn good parameters for classifying relations with sufficient supervision, but also learn model parameters that can be fine-tuned to enhance predictive performance for relations with limited supervision. In experiments conducted on two relation classification datasets, we demonstrate that the proposed meta-learning approach improves the predictive performance of two state-of-the-art supervised relation classification models",
    "volume": "main",
    "checked": true,
    "id": "e962c301df1d33bc12d8115f4c82093103c94eeb",
    "citation_count": 43
  },
  "https://aclanthology.org/P19-1590": {
    "title": "Variational Pretraining for Semi-supervised Text Classification",
    "abstract": "We introduce VAMPIRE, a lightweight pretraining framework for effective text classification when data and computing resources are limited. We pretrain a unigram document model as a variational autoencoder on in-domain, unlabeled data and use its internal states as features in a downstream classifier. Empirically, we show the relative strength of VAMPIRE against computationally expensive contextual embeddings and other popular semi-supervised baselines under low resource settings. We also find that fine-tuning to in-domain data is crucial to achieving decent performance from contextual embeddings when working with limited supervision. We accompany this paper with code to pretrain and use VAMPIRE embeddings in downstream tasks",
    "volume": "main",
    "checked": true,
    "id": "3b3f47170ec5c4fabac510585b33aeb87b384396",
    "citation_count": 66
  },
  "https://aclanthology.org/P19-1591": {
    "title": "Task Refinement Learning for Improved Accuracy and Stability of Unsupervised Domain Adaptation",
    "abstract": "Pivot Based Language Modeling (PBLM) (Ziser and Reichart, 2018a), combining LSTMs with pivot-based methods, has yielded significant progress in unsupervised domain adaptation. However, this approach is still challenged by the large pivot detection problem that should be solved, and by the inherent instability of LSTMs. In this paper we propose a Task Refinement Learning (TRL) approach, in order to solve these problems. Our algorithms iteratively train the PBLM model, gradually increasing the information exposed about each pivot. TRL-PBLM achieves stateof- the-art accuracy in six domain adaptation setups for sentiment classification. Moreover, it is much more stable than plain PBLM across model configurations, making the model much better fitted for practical use",
    "volume": "main",
    "checked": true,
    "id": "1ffb3e5a366ffe80e1dd706fb2bf929be685e2a8",
    "citation_count": 29
  },
  "https://aclanthology.org/P19-1592": {
    "title": "Optimal Transport-based Alignment of Learned Character Representations for String Similarity",
    "abstract": "String similarity models are vital for record linkage, entity resolution, and search. In this work, we present STANCE–a learned model for computing the similarity of two strings. Our approach encodes the characters of each string, aligns the encodings using Sinkhorn Iteration (alignment is posed as an instance of optimal transport) and scores the alignment with a convolutional neural network. We evaluate STANCE’s ability to detect whether two strings can refer to the same entity–a task we term alias detection. We construct five new alias detection datasets (and make them publicly available). We show that STANCE (or one of its variants) outperforms both state-of-the-art and classic, parameter-free similarity models on four of the five datasets. We also demonstrate STANCE’s ability to improve downstream tasks by applying it to an instance of cross-document coreference and show that it leads to a 2.8 point improvement in Bˆ3 F1 over the previous state-of-the-art approach",
    "volume": "main",
    "checked": true,
    "id": "4e89daf0e57921d0e42bc0d224f4ba2982812a18",
    "citation_count": 12
  },
  "https://aclanthology.org/P19-1593": {
    "title": "The Referential Reader: A Recurrent Entity Network for Anaphora Resolution",
    "abstract": "We present a new architecture for storing and accessing entity mentions during online text processing. While reading the text, entity references are identified, and may be stored by either updating or overwriting a cell in a fixed-length memory. The update operation implies coreference with the other mentions that are stored in the same cell; the overwrite operation causes these mentions to be forgotten. By encoding the memory operations as differentiable gates, it is possible to train the model end-to-end, using both a supervised anaphora resolution objective as well as a supplementary language modeling objective. Evaluation on a dataset of pronoun-name anaphora demonstrates strong performance with purely incremental text processing",
    "volume": "main",
    "checked": true,
    "id": "8af29a137e8fea8a9560a86a8e2ec8074bbd1dec",
    "citation_count": 12
  },
  "https://aclanthology.org/P19-1594": {
    "title": "Interpolated Spectral NGram Language Models",
    "abstract": "Spectral models for learning weighted non-deterministic automata have nice theoretical and algorithmic properties. Despite this, it has been challenging to obtain competitive results in language modeling tasks, for two main reasons. First, in order to capture long-range dependencies of the data, the method must use statistics from long substrings, which results in very large matrices that are difficult to decompose. The second is that the loss function behind spectral learning, based on moment matching, differs from the probabilistic metrics used to evaluate language models. In this work we employ a technique for scaling up spectral learning, and use interpolated predictions that are optimized to maximize perplexity. Our experiments in character-based language modeling show that our method matches the performance of state-of-the-art ngram models, while being very fast to train",
    "volume": "main",
    "checked": true,
    "id": "ba98c63fc81d5547f003f18363a91ecd89298758",
    "citation_count": 2
  },
  "https://aclanthology.org/P19-1595": {
    "title": "BAM! Born-Again Multi-Task Networks for Natural Language Understanding",
    "abstract": "It can be challenging to train multi-task neural networks that outperform or even match their single-task counterparts. To help address this, we propose using knowledge distillation where single-task models teach a multi-task model. We enhance this training with teacher annealing, a novel method that gradually transitions the model from distillation to supervised learning, helping the multi-task model surpass its single-task teachers. We evaluate our approach by multi-task fine-tuning BERT on the GLUE benchmark. Our method consistently improves over standard single-task and multi-task training",
    "volume": "main",
    "checked": true,
    "id": "ef6948edae12eba6f1d486b8600108b9762f36ab",
    "citation_count": 164
  },
  "https://aclanthology.org/P19-1596": {
    "title": "Curate and Generate: A Corpus and Method for Joint Control of Semantics and Style in Neural NLG",
    "abstract": "Neural natural language generation (NNLG) from structured meaning representations has become increasingly popular in recent years. While we have seen progress with generating syntactically correct utterances that preserve semantics, various shortcomings of NNLG systems are clear: new tasks require new training data which is not available or straightforward to acquire, and model outputs are simple and may be dull and repetitive. This paper addresses these two critical challenges in NNLG by: (1) scalably (and at no cost) creating training datasets of parallel meaning representations and reference texts with rich style markup by using data from freely available and naturally descriptive user reviews, and (2) systematically exploring how the style markup enables joint control of semantic and stylistic aspects of neural model output. We present YelpNLG, a corpus of 300,000 rich, parallel meaning representations and highly stylistically varied reference texts spanning different restaurant attributes, and describe a novel methodology that can be scalably reused to generate NLG datasets for other domains. The experiments show that the models control important aspects, including lexical choice of adjectives, output length, and sentiment, allowing the models to successfully hit multiple style targets without sacrificing semantics",
    "volume": "main",
    "checked": true,
    "id": "62b45c9eeae4c89ec1a4ad071eebb54438be741b",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-1597": {
    "title": "Automated Chess Commentator Powered by Neural Chess Engine",
    "abstract": "In this paper, we explore a new approach for automated chess commentary generation, which aims to generate chess commentary texts in different categories (e.g., description, comparison, planning, etc.). We introduce a neural chess engine into text generation models to help with encoding boards, predicting moves, and analyzing situations. By jointly training the neural chess engine and the generation models for different categories, the models become more effective. We conduct experiments on 5 categories in a benchmark Chess Commentary dataset and achieve inspiring results in both automatic and human evaluations",
    "volume": "main",
    "checked": true,
    "id": "35958f4359456db9a535a14a8c5a3872000fe252",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1598": {
    "title": "Barack's Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling",
    "abstract": "Modeling human language requires the ability to not only generate fluent text but also encode factual knowledge. However, traditional language models are only capable of remembering facts seen at training time, and often have difficulty recalling them. To address this, we introduce the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. We also introduce the Linked WikiText-2 dataset, a corpus of annotated text aligned to the Wikidata knowledge graph whose contents (roughly) match the popular WikiText-2 benchmark. In experiments, we demonstrate that the KGLM achieves significantly better performance than a strong baseline language model. We additionally compare different language model’s ability to complete sentences requiring factual knowledge, showing that the KGLM outperforms even very large language models in generating facts",
    "volume": "main",
    "checked": true,
    "id": "d6a13d8d168936a8947101d76fe060704d2f26ec",
    "citation_count": 134
  },
  "https://aclanthology.org/P19-1599": {
    "title": "Controllable Paraphrase Generation with a Syntactic Exemplar",
    "abstract": "Prior work on controllable text generation usually assumes that the controlled attribute can take on one of a small set of values known a priori. In this work, we propose a novel task, where the syntax of a generated sentence is controlled rather by a sentential exemplar. To evaluate quantitatively with standard metrics, we create a novel dataset with human annotations. We also develop a variational model with a neural module specifically designed for capturing syntactic knowledge and several multitask training objectives to promote disentangled representation learning. Empirically, the proposed model is observed to achieve improvements over baselines and learn to capture desirable characteristics",
    "volume": "main",
    "checked": true,
    "id": "0e5346ecb85569995ddd926fe21458f35afc7cf5",
    "citation_count": 77
  },
  "https://aclanthology.org/P19-1600": {
    "title": "Towards Comprehensive Description Generation from Factual Attribute-value Tables",
    "abstract": "The comprehensive descriptions for factual attribute-value tables, which should be accurate, informative and loyal, can be very helpful for end users to understand the structured data in this form. However previous neural generators might suffer from key attributes missing, less informative and groundless information problems, which impede the generation of high-quality comprehensive descriptions for tables. To relieve these problems, we first propose force attention (FA) method to encourage the generator to pay more attention to the uncovered attributes to avoid potential key attributes missing. Furthermore, we propose reinforcement learning for information richness to generate more informative as well as more loyal descriptions for tables. In our experiments, we utilize the widely used WIKIBIO dataset as a benchmark. Besides, we create WB-filter based on WIKIBIO to test our model in the simulated user-oriented scenarios, in which the generated descriptions should accord with particular user interests. Experimental results show that our model outperforms the state-of-the-art baselines on both automatic and human evaluation",
    "volume": "main",
    "checked": true,
    "id": "07da5f1b0089d080a1ed9295676ded47505ca25a",
    "citation_count": 21
  },
  "https://aclanthology.org/P19-1601": {
    "title": "Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation",
    "abstract": "Disentangling the content and style in the latent space is prevalent in unpaired text style transfer. However, two major issues exist in most of the current neural models. 1) It is difficult to completely strip the style information from the semantics for a sentence. 2) The recurrent neural network (RNN) based encoder and decoder, mediated by the latent representation, cannot well deal with the issue of the long-term dependency, resulting in poor preservation of non-stylistic semantic content. In this paper, we propose the Style Transformer, which makes no assumption about the latent representation of source sentence and equips the power of attention mechanism in Transformer to achieve better style transfer and better content preservation",
    "volume": "main",
    "checked": true,
    "id": "ae60d15ec4f6c3839df2db19154ee5971a1e9640",
    "citation_count": 127
  },
  "https://aclanthology.org/P19-1602": {
    "title": "Generating Sentences from Disentangled Syntactic and Semantic Spaces",
    "abstract": "Variational auto-encoders (VAEs) are widely used in natural language generation due to the regularization of the latent space. However, generating sentences from the continuous latent space does not explicitly model the syntactic information. In this paper, we propose to generate sentences from disentangled syntactic and semantic spaces. Our proposed method explicitly models syntactic information in the VAE’s latent space by using the linearized tree sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntax transfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work",
    "volume": "main",
    "checked": true,
    "id": "b636795b33eb155670953c472e5b07d7e1935895",
    "citation_count": 74
  },
  "https://aclanthology.org/P19-1603": {
    "title": "Learning to Control the Fine-grained Sentiment for Story Ending Generation",
    "abstract": "Automatic story ending generation is an interesting and challenging task in natural language generation. Previous studies are mainly limited to generate coherent, reasonable and diversified story endings, and few works focus on controlling the sentiment of story endings. This paper focuses on generating a story ending which meets the given fine-grained sentiment intensity. There are two major challenges to this task. First is the lack of story corpus which has fine-grained sentiment labels. Second is the difficulty of explicitly controlling sentiment intensity when generating endings. Therefore, we propose a generic and novel framework which consists of a sentiment analyzer and a sentimental generator, respectively addressing the two challenges. The sentiment analyzer adopts a series of methods to acquire sentiment intensities of the story dataset. The sentimental generator introduces the sentiment intensity into decoder via a Gaussian Kernel Layer to control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better",
    "volume": "main",
    "checked": true,
    "id": "2dd5a8b04be7199f7d8bb3b727705f7de66ed6c8",
    "citation_count": 40
  },
  "https://aclanthology.org/P19-1604": {
    "title": "Self-Attention Architectures for Answer-Agnostic Neural Question Generation",
    "abstract": "Neural architectures based on self-attention, such as Transformers, recently attracted interest from the research community, and obtained significant improvements over the state of the art in several tasks. We explore how Transformers can be adapted to the task of Neural Question Generation without constraining the model to focus on a specific answer passage. We study the effect of several strategies to deal with out-of-vocabulary words such as copy mechanisms, placeholders, and contextual word embeddings. We report improvements obtained over the state-of-the-art on the SQuAD dataset according to automated metrics (BLEU, ROUGE), as well as qualitative human assessments of the system outputs",
    "volume": "main",
    "checked": true,
    "id": "7cb8b70ab4a45b0d52e17d243cb880158bc4b528",
    "citation_count": 36
  },
  "https://aclanthology.org/P19-1605": {
    "title": "Unsupervised Paraphrasing without Translation",
    "abstract": "Paraphrasing is an important task demonstrating the ability to abstract semantic content from its surface form. Recent literature on automatic paraphrasing is dominated by methods leveraging machine translation as an intermediate step. This contrasts with humans, who can paraphrase without necessarily being bilingual. This work proposes to learn paraphrasing models only from a monolingual corpus. To that end, we propose a residual variant of vector-quantized variational auto-encoder. Our experiments consider paraphrase identification, and paraphrasing for training set augmentation, comparing to supervised and unsupervised translation-based approaches. Monolingual paraphrasing is shown to outperform unsupervised translation in all contexts. The comparison with supervised MT is more mixed: monolingual paraphrasing is interesting for identification and augmentation but supervised MT is superior for generation",
    "volume": "main",
    "checked": true,
    "id": "4a70df03329fe23dc454273e87a95fa39396dd88",
    "citation_count": 47
  },
  "https://aclanthology.org/P19-1606": {
    "title": "Storyboarding of Recipes: Grounded Contextual Generation",
    "abstract": "Information need of humans is essentially multimodal in nature, enabling maximum exploitation of situated context. We introduce a dataset for sequential procedural (how-to) text generation from images in cooking domain. The dataset consists of 16,441 cooking recipes with 160,479 photos associated with different steps. We setup a baseline motivated by the best performing model in terms of human evaluation for the Visual Story Telling (ViST) task. In addition, we introduce two models to incorporate high level structure learnt by a Finite State Machine (FSM) in neural sequential generation process by: (1) Scaffolding Structure in Decoder (SSiD) (2) Scaffolding Structure in Loss (SSiL). Our best performing model (SSiL) achieves a METEOR score of 0.31, which is an improvement of 0.6 over the baseline model. We also conducted human evaluation of the generated grounded recipes, which reveal that 61% found that our proposed (SSiL) model is better than the baseline model in terms of overall recipes. We also discuss analysis of the output highlighting key important NLP issues for prospective directions",
    "volume": "main",
    "checked": true,
    "id": "710d1b5d2662fd78dfb7904e64a1edbe1a671567",
    "citation_count": 24
  },
  "https://aclanthology.org/P19-1607": {
    "title": "Negative Lexically Constrained Decoding for Paraphrase Generation",
    "abstract": "Paraphrase generation can be regarded as monolingual translation. Unlike bilingual machine translation, paraphrase generation rewrites only a limited portion of an input sentence. Hence, previous methods based on machine translation often perform conservatively to fail to make necessary rewrites. To solve this problem, we propose a neural model for paraphrase generation that first identifies words in the source sentence that should be paraphrased. Then, these words are paraphrased by the negative lexically constrained decoding that avoids outputting these words as they are. Experiments on text simplification and formality transfer show that our model improves the quality of paraphrasing by making necessary rewrites to an input sentence",
    "volume": "main",
    "checked": true,
    "id": "c1f552dac0ffde4f6d6a32fd755bfb22ca123ae4",
    "citation_count": 27
  },
  "https://aclanthology.org/P19-1608": {
    "title": "Large-Scale Transfer Learning for Natural Language Generation",
    "abstract": "Large-scale pretrained language models define state of the art in natural language processing, achieving outstanding performance on a variety of tasks. We study how these architectures can be applied and adapted for natural language generation, comparing a number of architectural and training schemes. We focus in particular on open-domain dialog as a typical high entropy generation task, presenting and comparing different architectures for adapting pretrained models with state of the art results",
    "volume": "main",
    "checked": true,
    "id": "791c3c30f2af10ac06f4fbc5b1e8960064aacbc7",
    "citation_count": 59
  },
  "https://aclanthology.org/P19-1609": {
    "title": "Automatic Grammatical Error Correction for Sequence-to-sequence Text Generation: An Empirical Study",
    "abstract": "Sequence-to-sequence (seq2seq) models have achieved tremendous success in text generation tasks. However, there is no guarantee that they can always generate sentences without grammatical errors. In this paper, we present a preliminary empirical study on whether and how much automatic grammatical error correction can help improve seq2seq text generation. We conduct experiments across various seq2seq text generation tasks including machine translation, formality style transfer, sentence compression and simplification. Experiments show the state-of-the-art grammatical error correction system can improve the grammaticality of generated text and can bring task-oriented improvements in the tasks where target sentences are in a formal style",
    "volume": "main",
    "checked": true,
    "id": "b864ecde785c816bc922e65ca5e1e2c88649975a",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-1610": {
    "title": "Improving the Robustness of Question Answering Systems to Question Paraphrasing",
    "abstract": "Despite the advancement of question answering (QA) systems and rapid improvements on held-out test sets, their generalizability is a topic of concern. We explore the robustness of QA models to question paraphrasing by creating two test sets consisting of paraphrased SQuAD questions. Paraphrased questions from the first test set are very similar to the original questions designed to test QA models’ over-sensitivity, while questions from the second test set are paraphrased using context words near an incorrect answer candidate in an attempt to confuse QA models. We show that both paraphrased test sets lead to significant decrease in performance on multiple state-of-the-art QA models. Using a neural paraphrasing model trained to generate multiple paraphrased questions for a given source question and a set of paraphrase suggestions, we propose a data augmentation approach that requires no human intervention to re-train the models for improved robustness to question paraphrasing",
    "volume": "main",
    "checked": true,
    "id": "676f8086e68011198e3ff89250158f8338b7c50c",
    "citation_count": 62
  },
  "https://aclanthology.org/P19-1611": {
    "title": "RankQA: Neural Question Answering with Answer Re-Ranking",
    "abstract": "The conventional paradigm in neural question answering (QA) for narrative content is limited to a two-stage process: first, relevant text passages are retrieved and, subsequently, a neural network for machine comprehension extracts the likeliest answer. However, both stages are largely isolated in the status quo and, hence, information from the two phases is never properly fused. In contrast, this work proposes RankQA: RankQA extends the conventional two-stage process in neural QA with a third stage that performs an additional answer re-ranking. The re-ranking leverages different features that are directly extracted from the QA pipeline, i.e., a combination of retrieval and comprehension features. While our intentionally simple design allows for an efficient, data-sparse estimation, it nevertheless outperforms more complex QA systems by a significant margin: in fact, RankQA achieves state-of-the-art performance on 3 out of 4 benchmark datasets. Furthermore, its performance is especially superior in settings where the size of the corpus is dynamic. Here the answer re-ranking provides an effective remedy against the underlying noise-information trade-off due to a variable corpus size. As a consequence, RankQA represents a novel, powerful, and thus challenging baseline for future research in content-based QA",
    "volume": "main",
    "checked": true,
    "id": "c3e37d91f76bffde82db6ae7765017e942e2e5f4",
    "citation_count": 30
  },
  "https://aclanthology.org/P19-1612": {
    "title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering",
    "abstract": "Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match",
    "volume": "main",
    "checked": true,
    "id": "a81874b4a651a740fffbfc47ef96515e8c7f782f",
    "citation_count": 539
  },
  "https://aclanthology.org/P19-1613": {
    "title": "Multi-hop Reading Comprehension through Question Decomposition and Rescoring",
    "abstract": "Multi-hop Reading Comprehension (RC) requires reasoning and aggregation across several paragraphs. We propose a system for multi-hop RC that decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models. Since annotations for such decomposition are expensive, we recast subquestion generation as a span prediction problem and show that our method, trained using only 400 labeled examples, generates sub-questions that are as effective as human-authored sub-questions. We also introduce a new global rescoring approach that considers each decomposition (i.e. the sub-questions and their answers) to select the best final answer, greatly improving overall performance. Our experiments on HotpotQA show that this approach achieves the state-of-the-art results, while providing explainable evidence for its decision making in the form of sub-questions",
    "volume": "main",
    "checked": true,
    "id": "b9372e972997c5056bb79c70526230baed2e372b",
    "citation_count": 138
  },
  "https://aclanthology.org/P19-1614": {
    "title": "Combining Knowledge Hunting and Neural Language Models to Solve the Winograd Schema Challenge",
    "abstract": "Winograd Schema Challenge (WSC) is a pronoun resolution task which seems to require reasoning with commonsense knowledge. The needed knowledge is not present in the given text. Automatic extraction of the needed knowledge is a bottleneck in solving the challenge. The existing state-of-the-art approach uses the knowledge embedded in their pre-trained language model. However, the language models only embed part of the knowledge, the ones related to frequently co-existing concepts. This limits the performance of such models on the WSC problems. In this work, we build-up on the language model based methods and augment them with a commonsense knowledge hunting (using automatic extraction from text) module and an explicit reasoning module. Our end-to-end system built in such a manner improves on the accuracy of two of the available language model based approaches by 5.53% and 7.7% respectively. Overall our system achieves the state-of-the-art accuracy of 71.06% on the WSC dataset, an improvement of 7.36% over the previous best",
    "volume": "main",
    "checked": true,
    "id": "f3093b694f443a2f6f1f7e684ffe13d57806eb5a",
    "citation_count": 8
  },
  "https://aclanthology.org/P19-1615": {
    "title": "Careful Selection of Knowledge to Solve Open Book Question Answering",
    "abstract": "Open book question answering is a type of natural language based QA (NLQA) where questions are expected to be answered with respect to a given set of open book facts, and common knowledge about a topic. Recently a challenge involving such QA, OpenBookQA, has been proposed. Unlike most other NLQA that focus on linguistic understanding, OpenBookQA requires deeper reasoning involving linguistic understanding as well as reasoning with common knowledge. In this paper we address QA with respect to the OpenBookQA dataset and combine state of the art language models with abductive information retrieval (IR), information gain based re-ranking, passage selection and weighted scoring to achieve 72.0% accuracy, an 11.6% improvement over the current state of the art",
    "volume": "main",
    "checked": true,
    "id": "a4804394c362e7129312ae20fc094e60f56db3b0",
    "citation_count": 42
  },
  "https://aclanthology.org/P19-1616": {
    "title": "Learning Representation Mapping for Relation Detection in Knowledge Base Question Answering",
    "abstract": "Relation detection is a core step in many natural language process applications including knowledge base question answering. Previous efforts show that single-fact questions could be answered with high accuracy. However, one critical problem is that current approaches only get high accuracy for questions whose relations have been seen in the training data. But for unseen relations, the performance will drop rapidly. The main reason for this problem is that the representations for unseen relations are missing. In this paper, we propose a simple mapping method, named representation adapter, to learn the representation mapping for both seen and unseen relations based on previously learned relation embedding. We employ the adversarial objective and the reconstruction objective to improve the mapping performance. We re-organize the popular SimpleQuestion dataset to reveal and evaluate the problem of detecting unseen relations. Experiments show that our method can greatly improve the performance of unseen relations while the performance for those seen part is kept comparable to the state-of-the-art",
    "volume": "main",
    "checked": true,
    "id": "e75f5a2e3fcee6e38fc725f4f7fbace96998ef90",
    "citation_count": 19
  },
  "https://aclanthology.org/P19-1617": {
    "title": "Dynamically Fused Graph Network for Multi-hop Reasoning",
    "abstract": "Text-based question answering (TBQA) has been studied extensively in recent years. Most existing approaches focus on finding the answer to a question within a single paragraph. However, many difficult questions require multiple supporting evidence from scattered text among two or more documents. In this paper, we propose Dynamically Fused Graph Network (DFGN), a novel method to answer those questions requiring multiple scattered evidence and reasoning over them. Inspired by human’s step-by-step reasoning behavior, DFGN includes a dynamic fusion layer that starts from the entities mentioned in the given query, explores along the entity graph dynamically built from the text, and gradually finds relevant supporting entities from the given documents. We evaluate DFGN on HotpotQA, a public TBQA dataset requiring multi-hop reasoning. DFGN achieves competitive results on the public board. Furthermore, our analysis shows DFGN produces interpretable reasoning chains",
    "volume": "main",
    "checked": true,
    "id": "df2ce576eca0b3db177c83bc6cf0f9fe2c7714f0",
    "citation_count": 138
  },
  "https://aclanthology.org/P19-1618": {
    "title": "NLProlog: Reasoning with Weak Unification for Question Answering in Natural Language",
    "abstract": "Rule-based models are attractive for various tasks because they inherently lead to interpretable and explainable decisions and can easily incorporate prior knowledge. However, such systems are difficult to apply to problems involving natural language, due to its large linguistic variability. In contrast, neural models can cope very well with ambiguity by learning distributed representations of words and their composition from data, but lead to models that are difficult to interpret. In this paper, we describe a model combining neural networks with logic programming in a novel manner for solving multi-hop reasoning tasks over natural language. Specifically, we propose to use an Prolog prover which we extend to utilize a similarity function over pretrained sentence encoders. We fine-tune the representations for the similarity function via backpropagation. This leads to a system that can apply rule-based reasoning to natural language, and induce domain-specific natural language rules from training data. We evaluate the proposed system on two different question answering tasks, showing that it outperforms two baselines – BiDAF (Seo et al., 2016a) and FastQA( Weissenborn et al., 2017) on a subset of the WikiHop corpus and achieves competitive results on the MedHop data set (Welbl et al., 2017)",
    "volume": "main",
    "checked": true,
    "id": "2163dbd2c06f0aa326995b59c226e40553c4c63b",
    "citation_count": 71
  },
  "https://aclanthology.org/P19-1619": {
    "title": "Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions",
    "abstract": "Several deep learning models have been proposed for solving math word problems (MWPs) automatically. Although these models have the ability to capture features without manual efforts, their approaches to capturing features are not specifically designed for MWPs. To utilize the merits of deep learning models with simultaneous consideration of MWPs’ specific features, we propose a group attention mechanism to extract global features, quantity-related features, quantity-pair features and question-related features in MWPs respectively. The experimental results show that the proposed approach performs significantly better than previous state-of-the-art methods, and boost performance from 66.9% to 69.5% on Math23K with training-test split, from 65.8% to 66.9% on Math23K with 5-fold cross-validation and from 69.2% to 76.1% on MAWPS",
    "volume": "main",
    "checked": true,
    "id": "865f5167c4353d2b120f0469ed1c298bc92794fa",
    "citation_count": 55
  },
  "https://aclanthology.org/P19-1620": {
    "title": "Synthetic QA Corpora Generation with Roundtrip Consistency",
    "abstract": "We introduce a novel method of generating synthetic question answering corpora by combining models of question generation and answer extraction, and by filtering the results to ensure roundtrip consistency. By pretraining on the resulting corpora we obtain significant improvements on SQuAD2 and NQ, establishing a new state-of-the-art on the latter. Our synthetic data generation models, for both question generation and answer extraction, can be fully reproduced by finetuning a publicly available BERT model on the extractive subsets of SQuAD2 and NQ. We also describe a more powerful variant that does full sequence-to-sequence pretraining for question generation, obtaining exact match and F1 at less than 0.1% and 0.4% from human performance on SQuAD2",
    "volume": "main",
    "checked": true,
    "id": "8eb11d7e23cebe93ea58ddf25c8229e9f3a5a909",
    "citation_count": 149
  },
  "https://aclanthology.org/P19-1621": {
    "title": "Are Red Roses Red? Evaluating Consistency of Question-Answering Models",
    "abstract": "Although current evaluation of question-answering systems treats predictions in isolation, we need to consider the relationship between predictions to measure true understanding. A model should be penalized for answering “no” to “Is the rose red?” if it answers “red” to “What color is the rose?”. We propose a method to automatically extract such implications for instances from two QA datasets, VQA and SQuAD, which we then use to evaluate the consistency of models. Human evaluation shows these generated implications are well formed and valid. Consistency evaluation provides crucial insights into gaps in existing models, while retraining with implication-augmented data improves consistency on both synthetic and human-generated implications",
    "volume": "main",
    "checked": true,
    "id": "2723f54b4f5ed2d3f3a47c1b6749bbf5d8c660fd",
    "citation_count": 77
  },
  "https://aclanthology.org/P19-1622": {
    "title": "MCˆ2: Multi-perspective Convolutional Cube for Conversational Machine Reading Comprehension",
    "abstract": "Conversational machine reading comprehension (CMRC) extends traditional single-turn machine reading comprehension (MRC) by multi-turn interactions, which requires machines to consider the history of conversation. Most of models simply combine previous questions for conversation understanding and only employ recurrent neural networks (RNN) for reasoning. To comprehend context profoundly and efficiently from different perspectives, we propose a novel neural network model, Multi-perspective Convolutional Cube (MCˆ2). We regard each conversation as a cube. 1D and 2D convolutions are integrated with RNN in our model. To avoid models previewing the next turn of conversation, we also extend causal convolution partially to 2D. Experiments on the Conversational Question Answering (CoQA) dataset show that our model achieves state-of-the-art results",
    "volume": "main",
    "checked": true,
    "id": "c5c5d588612a0d3e86ecc993a79711c3da4aaec7",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-1623": {
    "title": "Reducing Word Omission Errors in Neural Machine Translation: A Contrastive Learning Approach",
    "abstract": "While neural machine translation (NMT) has achieved remarkable success, NMT systems are prone to make word omission errors. In this work, we propose a contrastive learning approach to reducing word omission errors in NMT. The basic idea is to enable the NMT model to assign a higher probability to a ground-truth translation and a lower probability to an erroneous translation, which is automatically constructed from the ground-truth translation by omitting words. We design different types of negative examples depending on the number of omitted words, word frequency, and part of speech. Experiments on Chinese-to-English, German-to-English, and Russian-to-English translation tasks show that our approach is effective in reducing word omission errors and achieves better translation performance than three baseline methods",
    "volume": "main",
    "checked": true,
    "id": "1ec5f3b55a90c1b32ad74dbe4423019d006b6bd3",
    "citation_count": 39
  },
  "https://aclanthology.org/P19-1624": {
    "title": "Exploiting Sentential Context for Neural Machine Translation",
    "abstract": "In this work, we present novel approaches to exploit sentential context for neural machine translation (NMT). Specifically, we show that a shallow sentential context extracted from the top encoder layer only, can improve translation performance via contextualizing the encoding representations of individual words. Next, we introduce a deep sentential context, which aggregates the sentential context representations from all of the internal layers of the encoder to form a more comprehensive context representation. Experimental results on the WMT14 English-German and English-French benchmarks show that our model consistently improves performance over the strong Transformer model, demonstrating the necessity and effectiveness of exploiting sentential context for NMT",
    "volume": "main",
    "checked": true,
    "id": "512894bf03ec8c41bc73205cec9e648147837432",
    "citation_count": 21
  },
  "https://aclanthology.org/P19-1625": {
    "title": "Wetin dey with these comments? Modeling Sociolinguistic Factors Affecting Code-switching Behavior in Nigerian Online Discussions",
    "abstract": "Multilingual individuals code switch between languages as a part of a complex communication process. However, most computational studies have examined only one or a handful of contextual factors predictive of switching. Here, we examine Naija-English code switching in a rich contextual environment to understand the social and topical factors eliciting a switch. We introduce a new corpus of 330K articles and accompanying 389K comments labeled for code switching behavior. In modeling whether a comment will switch, we show that topic-driven variation, tribal affiliation, emotional valence, and audience design all play complementary roles in behavior",
    "volume": "main",
    "checked": true,
    "id": "6ef8f996fed2fa51479057ea378362d96bed26ec",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-1626": {
    "title": "Accelerating Sparse Matrix Operations in Neural Networks on Graphics Processing Units",
    "abstract": "Graphics Processing Units (GPUs) are commonly used to train and evaluate neural networks efficiently. While previous work in deep learning has focused on accelerating operations on dense matrices/tensors on GPUs, efforts have concentrated on operations involving sparse data structures. Operations using sparse structures are common in natural language models at the input and output layers, because these models operate on sequences over discrete alphabets. We present two new GPU algorithms: one at the input layer, for multiplying a matrix by a few-hot vector (generalizing the more common operation of multiplication by a one-hot vector) and one at the output layer, for a fused softmax and top-N selection (commonly used in beam search). Our methods achieve speedups over state-of-the-art parallel GPU baselines of up to 7x and 50x, respectively. We also illustrate how our methods scale on different GPU architectures",
    "volume": "main",
    "checked": true,
    "id": "a339bd43dc2fa2376f2193dfef87cf8b3d151e9a",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-1627": {
    "title": "An Automated Framework for Fast Cognate Detection and Bayesian Phylogenetic Inference in Computational Historical Linguistics",
    "abstract": "We present a fully automated workflow for phylogenetic reconstruction on large datasets, consisting of two novel methods, one for fast detection of cognates and one for fast Bayesian phylogenetic inference. Our results show that the methods take less than a few minutes to process language families that have so far required large amounts of time and computational power. Moreover, the cognates and the trees inferred from the method are quite close, both to gold standard cognate judgments and to expert language family trees. Given its speed and ease of application, our framework is specifically useful for the exploration of very large datasets in historical linguistics",
    "volume": "main",
    "checked": true,
    "id": "070d21bbef52cd426efbe346cc5e68901beb6427",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-1628": {
    "title": "Sentence Centrality Revisited for Unsupervised Summarization",
    "abstract": "Single document summarization has enjoyed renewed interest in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. In this paper we develop an unsupervised approach arguing that it is unrealistic to expect large-scale and high-quality training data to be available or created for different types of summaries, domains, or languages. We revisit a popular graph-based ranking algorithm and modify how node (aka sentence) centrality is computed in two ways: (a) we employ BERT, a state-of-the-art neural representation learning model to better capture sentential meaning and (b) we build graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin",
    "volume": "main",
    "checked": true,
    "id": "5ccfbddcfd8684a97fe1b693b8b510526936f553",
    "citation_count": 114
  },
  "https://aclanthology.org/P19-1629": {
    "title": "Discourse Representation Parsing for Sentences and Documents",
    "abstract": "We introduce a novel semantic parsing task based on Discourse Representation Theory (DRT; Kamp and Reyle 1993). Our model operates over Discourse Representation Tree Structures which we formally define for sentences and documents. We present a general framework for parsing discourse structures of arbitrary length and granularity. We achieve this with a neural model equipped with a supervised hierarchical attention mechanism and a linguistically-motivated copy strategy. Experimental results on sentence- and document-level benchmarks show that our model outperforms competitive baselines by a wide margin",
    "volume": "main",
    "checked": true,
    "id": "db53e9926d7092d7c839c38123be85e84840192a",
    "citation_count": 17
  },
  "https://aclanthology.org/P19-1630": {
    "title": "Inducing Document Structure for Aspect-based Summarization",
    "abstract": "Automatic summarization is typically treated as a 1-to-1 mapping from document to summary. Documents such as news articles, however, are structured and often cover multiple topics or aspects; and readers may be interested in only some of them. We tackle the task of aspect-based summarization, where, given a document and a target aspect, our models generate a summary centered around the aspect. We induce latent document structure jointly with an abstractive summarization objective, and train our models in a scalable synthetic setup. In addition to improvements in summarization over topic-agnostic baselines, we demonstrate the benefit of the learnt document structure: we show that our models (a) learn to accurately segment documents by aspect; (b) can leverage the structure to produce both abstractive and extractive aspect-based summaries; and (c) that structure is particularly advantageous for summarizing long documents. All results transfer from synthetic training documents to natural news articles from CNN/Daily Mail and RCV1",
    "volume": "main",
    "checked": true,
    "id": "761e606b19c48d03b077d5b9c37652260d18f073",
    "citation_count": 28
  },
  "https://aclanthology.org/P19-1631": {
    "title": "Incorporating Priors with Feature Attribution on Text Classification",
    "abstract": "Feature attribution methods, proposed recently, help users interpret the predictions of complex models. Our approach integrates feature attributions into the objective function to allow machine learning practitioners to incorporate priors in model building. To demonstrate the effectiveness our technique, we apply it to two tasks: (1) mitigating unintended bias in text classifiers by neutralizing identity terms; (2) improving classifier performance in scarce data setting by forcing model to focus on toxic terms. Our approach adds an L2 distance loss between feature attributions and task-specific prior values to the objective. Our experiments show that i) a classifier trained with our technique reduces undesired model biases without a tradeoff on the original task; ii) incorporating prior helps model performance in scarce data settings",
    "volume": "main",
    "checked": true,
    "id": "742b501ce3cee83ef610aec9e51a889f8fd2418a",
    "citation_count": 74
  },
  "https://aclanthology.org/P19-1632": {
    "title": "Matching Article Pairs with Graphical Decomposition and Convolutions",
    "abstract": "Identifying the relationship between two articles, e.g., whether two articles published from different sources describe the same breaking news, is critical to many document understanding tasks. Existing approaches for modeling and matching sentence pairs do not perform well in matching longer documents, which embody more complex interactions between the enclosed entities than a sentence does. To model article pairs, we propose the Concept Interaction Graph to represent an article as a graph of concepts. We then match a pair of articles by comparing the sentences that enclose the same concept vertex through a series of encoding techniques, and aggregate the matching signals through a graph convolutional network. To facilitate the evaluation of long article matching, we have created two datasets, each consisting of about 30K pairs of breaking news articles covering diverse topics in the open domain. Extensive evaluations of the proposed methods on the two datasets demonstrate significant improvements over a wide range of state-of-the-art methods for natural language matching",
    "volume": "main",
    "checked": true,
    "id": "c0fdddc750f58373ad6b1e30660812ef9903b7fe",
    "citation_count": 30
  },
  "https://aclanthology.org/P19-1633": {
    "title": "Hierarchical Transfer Learning for Multi-label Text Classification",
    "abstract": "Multi-Label Hierarchical Text Classification (MLHTC) is the task of categorizing documents into one or more topics organized in an hierarchical taxonomy. MLHTC can be formulated by combining multiple binary classification problems with an independent classifier for each category. We propose a novel transfer learning based strategy, HTrans, where binary classifiers at lower levels in the hierarchy are initialized using parameters of the parent classifier and fine-tuned on the child category classification task. In HTrans, we use a Gated Recurrent Unit (GRU)-based deep learning architecture coupled with attention. Compared to binary classifiers trained from scratch, our HTrans approach results in significant improvements of 1% on micro-F1 and 3% on macro-F1 on the RCV1 dataset. Our experiments also show that binary classifiers trained from scratch are significantly better than single multi-label models",
    "volume": "main",
    "checked": true,
    "id": "1eed88ce728bee99bd20641ef4271a208d52aed4",
    "citation_count": 42
  },
  "https://aclanthology.org/P19-1634": {
    "title": "Bias Analysis and Mitigation in the Evaluation of Authorship Verification",
    "abstract": "The PAN series of shared tasks is well known for its continuous and high quality research in the field of digital text forensics. Among others, PAN contributions include original corpora, tailored benchmarks, and standardized experimentation platforms. In this paper we review, theoretically and practically, the authorship verification task and conclude that the underlying experiment design cannot guarantee pushing forward the state of the art—in fact, it allows for top benchmarking with a surprisingly straightforward approach. In this regard, we present a “Basic and Fairly Flawed” (BAFF) authorship verifier that is on a par with the best approaches submitted so far, and that illustrates sources of bias that should be eliminated. We pinpoint these sources in the evaluation chain and present a refined authorship corpus as effective countermeasure",
    "volume": "main",
    "checked": true,
    "id": "bd369585cff800306e1c5df558d66e3c8c2ccdd0",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-1635": {
    "title": "Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments",
    "abstract": "In this paper, we attempt to answer the question of whether neural network models can learn numeracy, which is the ability to predict the magnitude of a numeral at some specific position in a text description. A large benchmark dataset, called Numeracy-600K, is provided for the novel task. We explore several neural network models including CNN, GRU, BiGRU, CRNN, CNN-capsule, GRU-capsule, and BiGRU-capsule in the experiments. The results show that the BiGRU model gets the best micro-averaged F1 score of 80.16%, and the GRU-capsule model gets the best macro-averaged F1 score of 64.71%. Besides discussing the challenges through comprehensive experiments, we also present an important application scenario, i.e., detecting exaggerated information, for the task",
    "volume": "main",
    "checked": true,
    "id": "11bb73c6cdec9c5ce579194c26ad74c722e32f95",
    "citation_count": 32
  },
  "https://aclanthology.org/P19-1636": {
    "title": "Large-Scale Multi-Label Text Classification on EU Legislation",
    "abstract": "We consider Large-Scale Multi-Label Text Classification (LMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR-LEX, annotated with ∼4.3k EUROVOC labels, which is suitable for LMTC, few- and zero-shot learning. Experimenting with several neural classifiers, we show that BIGRUs with label-wise attention perform better than other current state of the art methods. Domain-specific WORD2VEC and context-sensitive ELMO embeddings further improve performance. We also find that considering only particular zones of the documents is sufficient. This allows us to bypass BERT’s maximum text length limit and fine-tune BERT, obtaining the best results in all but zero-shot learning cases",
    "volume": "main",
    "checked": true,
    "id": "12ae4d4428963d98346e8b16d76c8e165e33a094",
    "citation_count": 105
  },
  "https://aclanthology.org/P19-1637": {
    "title": "Why Didn't You Listen to Me? Comparing User Control of Human-in-the-Loop Topic Models",
    "abstract": "To address the lack of comparative evaluation of Human-in-the-Loop Topic Modeling (HLTM) systems, we implement and evaluate three contrasting HLTM modeling approaches using simulation experiments. These approaches extend previously proposed frameworks, including constraints and informed prior-based methods. Users should have a sense of control in HLTM systems, so we propose a control metric to measure whether refinement operations’ results match users’ expectations. Informed prior-based methods provide better control than constraints, but constraints yield higher quality topics",
    "volume": "main",
    "checked": true,
    "id": "4e7843b7e6dc69ce27ee054f0d59de33c0df755e",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-1638": {
    "title": "Encouraging Paragraph Embeddings to Remember Sentence Identity Improves Classification",
    "abstract": "While paragraph embedding models are remarkably effective for downstream classification tasks, what they learn and encode into a single vector remains opaque. In this paper, we investigate a state-of-the-art paragraph embedding method proposed by Zhang et al. (2017) and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not. We formulate a sentence content task to probe for this basic linguistic property and find that even a much simpler bag-of-words method has no trouble solving it. This result motivates us to replace the reconstruction-based objective of Zhang et al. (2017) with our sentence content probe objective in a semi-supervised setting. Despite its simplicity, our objective improves over paragraph reconstruction in terms of (1) downstream classification accuracies on benchmark datasets, (2) faster training, and (3) better generalization ability",
    "volume": "main",
    "checked": true,
    "id": "8be39979cb2eb1aeaba15b57e1e4bc712eb962cb",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-1639": {
    "title": "A Multi-Task Architecture on Relevance-based Neural Query Translation",
    "abstract": "We describe a multi-task learning approach to train a Neural Machine Translation (NMT) model with a Relevance-based Auxiliary Task (RAT) for search query translation. The translation process for Cross-lingual Information Retrieval (CLIR) task is usually treated as a black box and it is performed as an independent step. However, an NMT model trained on sentence-level parallel data is not aware of the vocabulary distribution of the retrieval corpus. We address this problem and propose a multi-task learning architecture that achieves 16% improvement over a strong baseline on Italian-English query-document dataset. We show using both quantitative and qualitative analysis that our model generates balanced and precise translations with the regularization effect it achieves from multi-task learning paradigm",
    "volume": "main",
    "checked": true,
    "id": "3bfa016b5b951ce7d3858c7f36ca4d205fec6a24",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-1640": {
    "title": "Topic Modeling with Wasserstein Autoencoders",
    "abstract": "We propose a novel neural topic model in the Wasserstein autoencoders (WAE) framework. Unlike existing variational autoencoder based models, we directly enforce Dirichlet prior on the latent document-topic vectors. We exploit the structure of the latent space and apply a suitable kernel in minimizing the Maximum Mean Discrepancy (MMD) to perform distribution matching. We discover that MMD performs much better than the Generative Adversarial Network (GAN) in matching high dimensional Dirichlet distribution. We further discover that incorporating randomness in the encoder output during training leads to significantly more coherent topics. To measure the diversity of the produced topics, we propose a simple topic uniqueness metric. Together with the widely used coherence measure NPMI, we offer a more wholistic evaluation of topic quality. Experiments on several real datasets show that our model produces significantly better topics than existing topic models",
    "volume": "main",
    "checked": true,
    "id": "7eca2934825184908e57619b354c327a6eaaf7cc",
    "citation_count": 57
  },
  "https://aclanthology.org/P19-1641": {
    "title": "Dense Procedure Captioning in Narrated Instructional Videos",
    "abstract": "Understanding narrated instructional videos is important for both research and real-world web applications. Motivated by video dense captioning, we propose a model to generate procedure captions from narrated instructional videos which are a sequence of step-wise clips with description. Previous works on video dense captioning learn video segments and generate captions without considering transcripts. We argue that transcripts in narrated instructional videos can enhance video representation by providing fine-grained complimentary and semantic textual information. In this paper, we introduce a framework to (1) extract procedures by a cross-modality module, which fuses video content with the entire transcript; and (2) generate captions by encoding video frames as well as a snippet of transcripts within each extracted procedure. Experiments show that our model can achieve state-of-the-art performance in procedure extraction and captioning, and the ablation studies demonstrate that both the video frames and the transcripts are important for the task",
    "volume": "main",
    "checked": true,
    "id": "ea57ffa3e13400cad53dc061887a6fbbd45e7f12",
    "citation_count": 32
  },
  "https://aclanthology.org/P19-1642": {
    "title": "Latent Variable Model for Multi-modal Translation",
    "abstract": "In this work, we propose to model the interaction between visual and textual features for multi-modal neural machine translation (MMT) through a latent variable model. This latent variable can be seen as a multi-modal stochastic embedding of an image and its description in a foreign language. It is used in a target-language decoder and also to predict image features. Importantly, our model formulation utilises visual and textual inputs during training but does not require that images be available at test time. We show that our latent variable MMT formulation improves considerably over strong baselines, including a multi-task learning approach (Elliott and Kadar, 2017) and a conditional variational auto-encoder approach (Toyama et al., 2016). Finally, we show improvements due to (i) predicting image features in addition to only conditioning on them, (ii) imposing a constraint on the KL term to promote models with non-negligible mutual information between inputs and latent variable, and (iii) by training on additional target-language image descriptions (i.e. synthetic data)",
    "volume": "main",
    "checked": true,
    "id": "e4e769b2b51349c91e9f7f67218a042ed3080d44",
    "citation_count": 43
  },
  "https://aclanthology.org/P19-1643": {
    "title": "Identifying Visible Actions in Lifestyle Vlogs",
    "abstract": "We consider the task of identifying human actions visible in online videos. We focus on the widely spread genre of lifestyle vlogs, which consist of videos of people performing actions while verbally describing them. Our goal is to identify if actions mentioned in the speech description of a video are visually present. We construct a dataset with crowdsourced manual annotations of visible actions, and introduce a multimodal algorithm that leverages information derived from visual and linguistic clues to automatically infer which actions are visible in a video",
    "volume": "main",
    "checked": true,
    "id": "0fe96f493334987589f8ccd55f2d1209df258449",
    "citation_count": 10
  },
  "https://aclanthology.org/P19-1644": {
    "title": "A Corpus for Reasoning about Natural Language Grounded in Photographs",
    "abstract": "We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. The data contains 107,292 examples of English sentences paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. Qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge",
    "volume": "main",
    "checked": true,
    "id": "cf336d272a30d6ad6141db67faa64deb8791cd61",
    "citation_count": 279
  },
  "https://aclanthology.org/P19-1645": {
    "title": "Learning to Discover, Ground and Use Words with Segmental Neural Language Models",
    "abstract": "We propose a segmental neural language model that combines the generalization power of neural networks with the ability to discover word-like units that are latent in unsegmented character sequences. In contrast to previous segmentation models that treat word segmentation as an isolated task, our model unifies word discovery, learning how words fit together to form sentences, and, by conditioning the model on visual context, how words’ meanings ground in representations of nonlinguistic modalities. Experiments show that the unconditional model learns predictive distributions better than character LSTM models, discovers words competitively with nonparametric Bayesian word segmentation models, and that modeling language conditional on visual context improves performance on both",
    "volume": "main",
    "checked": true,
    "id": "a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f",
    "citation_count": 23
  },
  "https://aclanthology.org/P19-1646": {
    "title": "What Should I Ask? Using Conversationally Informative Rewards for Goal-oriented Visual Dialog",
    "abstract": "The ability to engage in goal-oriented conversations has allowed humans to gain knowledge, reduce uncertainty, and perform tasks more efficiently. Artificial agents, however, are still far behind humans in having goal-driven conversations. In this work, we focus on the task of goal-oriented visual dialogue, aiming to automatically generate a series of questions about an image with a single objective. This task is challenging since these questions must not only be consistent with a strategy to achieve a goal, but also consider the contextual information in the image. We propose an end-to-end goal-oriented visual dialogue system, that combines reinforcement learning with regularized information gain. Unlike previous approaches that have been proposed for the task, our work is motivated by the Rational Speech Act framework, which models the process of human inquiry to reach a goal. We test the two versions of our model on the GuessWhat?! dataset, obtaining significant results that outperform the current state-of-the-art models in the task of generating questions to find an undisclosed object in an image",
    "volume": "main",
    "checked": true,
    "id": "133d4278069bd3eba627611c51a388879e9eae46",
    "citation_count": 32
  },
  "https://aclanthology.org/P19-1647": {
    "title": "Symbolic Inductive Bias for Visually Grounded Learning of Spoken Language",
    "abstract": "A widespread approach to processing spoken language is to first automatically transcribe it into text. An alternative is to use an end-to-end approach: recent works have proposed to learn semantic embeddings of spoken language from images with spoken captions, without an intermediate transcription step. We propose to use multitask learning to exploit existing transcribed speech within the end-to-end setting. We describe a three-task architecture which combines the objectives of matching spoken captions with corresponding images, speech with text, and text with images. We show that the addition of the speech/text task leads to substantial performance improvements on image retrieval when compared to training the speech/image task in isolation. We conjecture that this is due to a strong inductive bias transcribed speech provides to the model, and offer supporting evidence for this",
    "volume": "main",
    "checked": true,
    "id": "19463ba394386ade82e446674e7756b72178e8ae",
    "citation_count": 24
  },
  "https://aclanthology.org/P19-1648": {
    "title": "Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog",
    "abstract": "This paper presents a new model for visual dialog, Recurrent Dual Attention Network (ReDAN), using multi-step reasoning to answer a series of questions about an image. In each question-answering turn of a dialog, ReDAN infers the answer progressively through multiple reasoning steps. In each step of the reasoning process, the semantic representation of the question is updated based on the image and the previous dialog history, and the recurrently-refined representation is used for further reasoning in the subsequent step. On the VisDial v1.0 dataset, the proposed ReDAN model achieves a new state-of-the-art of 64.47% NDCG score. Visualization on the reasoning process further demonstrates that ReDAN can locate context-relevant visual and textual clues via iterative refinement, which can lead to the correct answer step-by-step",
    "volume": "main",
    "checked": true,
    "id": "8d9f1eaac344b7e6cc76396f576c5dc0bd0b34f4",
    "citation_count": 85
  },
  "https://aclanthology.org/P19-1649": {
    "title": "Lattice Transformer for Speech Translation",
    "abstract": "Recent advances in sequence modeling have highlighted the strengths of the transformer architecture, especially in achieving state-of-the-art machine translation results. However, depending on the up-stream systems, e.g., speech recognition, or word segmentation, the input to translation system can vary greatly. The goal of this work is to extend the attention mechanism of the transformer to naturally consume the lattice in addition to the traditional sequential input. We first propose a general lattice transformer for speech translation where the input is the output of the automatic speech recognition (ASR) which contains multiple paths and posterior scores. To leverage the extra information from the lattice structure, we develop a novel controllable lattice attention mechanism to obtain latent representations. On the LDC Spanish-English speech translation corpus, our experiments show that lattice transformer generalizes significantly better and outperforms both a transformer baseline and a lattice LSTM. Additionally, we validate our approach on the WMT 2017 Chinese-English translation task with lattice inputs from different BPE segmentations. In this task, we also observe the improvements over strong baselines",
    "volume": "main",
    "checked": true,
    "id": "a4d29e912ef6014e355649c1f9f822b9bf3d14e3",
    "citation_count": 30
  },
  "https://aclanthology.org/P19-1650": {
    "title": "Informative Image Captioning with External Sources of Information",
    "abstract": "An image caption should fluently present the essential information in a given image, including informative, fine-grained entity mentions and the manner in which these entities interact. However, current captioning models are usually trained to generate captions that only contain common object names, thus falling short on an important “informativeness” dimension. We present a mechanism for integrating image information together with fine-grained labels (assumed to be generated by some upstream models) into a caption that describes the image in a fluent and informative manner. We introduce a multimodal, multi-encoder model based on Transformer that ingests both image features and multiple sources of entity labels. We demonstrate that we can learn to control the appearance of these entity labels in the output, resulting in captions that are both fluent and informative",
    "volume": "main",
    "checked": true,
    "id": "68490e9e0bca2f0b1ae2ca636effcd8fc63d2008",
    "citation_count": 30
  },
  "https://aclanthology.org/P19-1651": {
    "title": "CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication",
    "abstract": "In this work, we propose a goal-driven collaborative task that combines language, perception, and action. Specifically, we develop a Collaborative image-Drawing game between two agents, called CoDraw. Our game is grounded in a virtual world that contains movable clip art objects. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. The two players communicate with each other using natural language. We collect the CoDraw dataset of ~10K dialogs consisting of ~138K messages exchanged between human players. We define protocols and metrics to evaluate learned agents in this testbed, highlighting the need for a novel “crosstalk” evaluation condition which pairs agents trained independently on disjoint subsets of the training data. We present models for our task and benchmark them using both fully automated evaluation and by having them play the game live with humans",
    "volume": "main",
    "checked": true,
    "id": "58641a3a4b4653b5d63e57dc6dfe3935b866d78f",
    "citation_count": 51
  },
  "https://aclanthology.org/P19-1652": {
    "title": "Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning",
    "abstract": "Image Captioning aims at generating a short description for an image. Existing research usually employs the architecture of CNN-RNN that views the generation as a sequential decision-making process and the entire dataset vocabulary is used as decoding space. They suffer from generating high frequent n-gram with irrelevant words. To tackle this problem, we propose to construct an image-grounded vocabulary, based on which, captions are generated with limitation and guidance. In specific, a novel hierarchical structure is proposed to construct the vocabulary incorporating both visual information and relations among words. For generation, we propose a word-aware RNN cell incorporating vocabulary information into the decoding process directly. Reinforce algorithm is employed to train the generator using constraint vocabulary as action space. Experimental results on MS COCO and Flickr30k show the effectiveness of our framework compared to some state-of-the-art models",
    "volume": "main",
    "checked": true,
    "id": "aaf5e3afd61d0df6c483ca32faf8e7a9198b1557",
    "citation_count": 18
  },
  "https://aclanthology.org/P19-1653": {
    "title": "Distilling Translations with Visual Awareness",
    "abstract": "Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language",
    "volume": "main",
    "checked": true,
    "id": "da54a4f2a1e8d1358a832b176f1a6c97e230e037",
    "citation_count": 51
  },
  "https://aclanthology.org/P19-1654": {
    "title": "VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions",
    "abstract": "We address the task of evaluating image description generation systems. We propose a novel image-aware metric for this task: VIFIDEL. It estimates the faithfulness of a generated caption with respect to the content of the actual image, based on the semantic similarity between labels of objects depicted in images and words in the description. The metric is also able to take into account the relative importance of objects mentioned in human reference descriptions during evaluation. Even if these human reference descriptions are not available, VIFIDEL can still reliably evaluate system descriptions. The metric achieves high correlation with human judgments on two well-known datasets and is competitive with metrics that depend on and rely exclusively on human references",
    "volume": "main",
    "checked": true,
    "id": "fbcea7715f62395fd5a83423d57556e8acca9a62",
    "citation_count": 18
  },
  "https://aclanthology.org/P19-1655": {
    "title": "Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation",
    "abstract": "Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task",
    "volume": "main",
    "checked": true,
    "id": "b37d073109cfcf913cf53aded3872e6158e828a0",
    "citation_count": 69
  },
  "https://aclanthology.org/P19-1656": {
    "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
    "abstract": "Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT",
    "volume": "main",
    "checked": true,
    "id": "949fef650da4c41afe6049a183b504b3cc91f4bd",
    "citation_count": 424
  },
  "https://aclanthology.org/P19-1657": {
    "title": "Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-ray Reports",
    "abstract": "Chest X-Ray (CXR) images are commonly used for clinical screening and diagnosis. Automatically writing reports for these images can considerably lighten the workload of radiologists for summarizing descriptive findings and conclusive impressions. The complex structures between and within sections of the reports pose a great challenge to the automatic report generation. Specifically, the section Impression is a diagnostic summarization over the section Findings; and the appearance of normality dominates each section over that of abnormality. Existing studies rarely explore and consider this fundamental structure information. In this work, we propose a novel framework which exploits the structure information between and within report sections for generating CXR imaging reports. First, we propose a two-stage strategy that explicitly models the relationship between Findings and Impression. Second, we design a novel co-operative multi-agent system that implicitly captures the imbalanced distribution between abnormality and normality. Experiments on two CXR report datasets show that our method achieves state-of-the-art performance in terms of various evaluation metrics. Our results expose that the proposed approach is able to generate high-quality medical reports through integrating the structure information",
    "volume": "main",
    "checked": true,
    "id": "f08b8d6f2df54675c9b83fb115e63df763ea32fb",
    "citation_count": 61
  },
  "https://aclanthology.org/P19-1658": {
    "title": "Visual Story Post-Editing",
    "abstract": "We introduce the first dataset for human edits of machine-generated visual stories and explore how these collected edits may be used for the visual story post-editing task. The dataset ,VIST-Edit, includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions. We establish baselines for the task, showing how a relatively small set of human edits can be leveraged to boost the performance of large visual storytelling models. We also discuss the weak correlation between automatic evaluation scores and human ratings, motivating the need for new automatic metrics",
    "volume": "main",
    "checked": true,
    "id": "37fa5e2ab18e4c69b060b66bd602b55adddc0ba2",
    "citation_count": 12
  },
  "https://aclanthology.org/P19-1659": {
    "title": "Multimodal Abstractive Summarization for How2 Videos",
    "abstract": "In this paper, we study abstractive summarization for open-domain videos. Unlike the traditional text news summarization, the goal is less to “compress” text information but rather to provide a fluent textual summary of information that has been collected and fused from different source modalities, in our case video and audio transcripts (or text). We show how a multi-source sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, compare various models trained with different modalities and present pilot experiments on the How2 corpus of instructional videos. We also propose a new evaluation metric (Content F1) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries, which is covered by metrics like ROUGE and BLEU",
    "volume": "main",
    "checked": true,
    "id": "37cc2c54cc60ee1301baca2d95bf003c76dd07d5",
    "citation_count": 47
  },
  "https://aclanthology.org/P19-1660": {
    "title": "Learning to Relate from Captions and Bounding Boxes",
    "abstract": "In this work, we propose a novel approach that predicts the relationships between various entities in an image in a weakly supervised manner by relying on image captions and object bounding box annotations as the sole source of supervision. Our proposed approach uses a top-down attention mechanism to align entities in captions to objects in the image, and then leverage the syntactic structure of the captions to align the relations. We use these alignments to train a relation classification network, thereby obtaining both grounded captions and dense relationships. We demonstrate the effectiveness of our model on the Visual Genome dataset by achieving a recall@50 of 15% and recall@100 of 25% on the relationships present in the image. We also show that the model successfully predicts relations that are not present in the corresponding captions",
    "volume": "main",
    "checked": true,
    "id": "20e582dd2afab1778ad3fa9438a6451d5ebe1af2",
    "citation_count": 3
  },
  "https://aclanthology.org/P19-2001": {
    "title": "Distributed Knowledge Based Clinical Auto-Coding System",
    "abstract": "Codification of free-text clinical narratives have long been recognised to be beneficial for secondary uses such as funding, insurance claim processing and research. In recent years, many researchers have studied the use of Natural Language Processing (NLP), related Machine Learning (ML) methods and techniques to resolve the problem of manual coding of clinical narratives. Most of the studies are focused on classification systems relevant to the U.S and there is a scarcity of studies relevant to Australian classification systems such as ICD-10-AM and ACHI. Therefore, we aim to develop a knowledge-based clinical auto-coding system, that utilise appropriate NLP and ML techniques to assign ICD-10-AM and ACHI codes to clinical records, while adhering to both local coding standards (Australian Coding Standard) and international guidelines that get updated and validated continuously",
    "volume": "student",
    "checked": true,
    "id": "9950834c6517c7dd60f228f1f986dc190b602b58",
    "citation_count": 4
  },
  "https://aclanthology.org/P19-2002": {
    "title": "Robust to Noise Models in Natural Language Processing Tasks",
    "abstract": "There are a lot of noise texts surrounding a person in modern life. The traditional approach is to use spelling correction, yet the existing solutions are far from perfect. We propose robust to noise word embeddings model, which outperforms existing commonly used models, like fasttext and word2vec in different tasks. In addition, we investigate the noise robustness of current models in different natural language processing tasks. We propose extensions for modern models in three downstream tasks, i.e. text classification, named entity recognition and aspect extraction, which shows improvement in noise robustness over existing solutions",
    "volume": "student",
    "checked": true,
    "id": "3f420d9c3d017c5ab84bba4a68c6ee9a442a5548",
    "citation_count": 8
  },
  "https://aclanthology.org/P19-2003": {
    "title": "A Computational Linguistic Study of Personal Recovery in Bipolar Disorder",
    "abstract": "Mental health research can benefit increasingly fruitfully from computational linguistics methods, given the abundant availability of language data in the internet and advances of computational tools. This interdisciplinary project will collect and analyse social media data of individuals diagnosed with bipolar disorder with regard to their recovery experiences. Personal recovery - living a satisfying and contributing life along symptoms of severe mental health issues - so far has only been investigated qualitatively with structured interviews and quantitatively with standardised questionnaires with mainly English-speaking participants inWestern countries. Complementary to this evidence, computational linguistic methods allow us to analyse first-person accounts shared online in large quantities, representing unstructured settings and a more heterogeneous, multilingual population, to draw a more complete picture of the aspects and mechanisms of personal recovery in bipolar disorder",
    "volume": "student",
    "checked": true,
    "id": "e94cfbc3df26b2826b939f33a343c86542d14dcc",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-2004": {
    "title": "Measuring the Value of Linguistics: A Case Study from St. Lawrence Island Yupik",
    "abstract": "The adaptation of neural approaches to NLP is a landmark achievement that has called into question the utility of linguistics in the development of computational systems. This research proposal consequently explores this question in the context of a neural morphological analyzer for a polysynthetic language, St. Lawrence Island Yupik. It asks whether incorporating elements of Yupik linguistics into the implementation of the analyzer can improve performance, both in low-resource settings and in high-resource settings, where rich quantities of data are readily available",
    "volume": "student",
    "checked": true,
    "id": "d300cb5e34a60945cc9cf00d1e18ef7cb33a5e53",
    "citation_count": 0
  },
  "https://aclanthology.org/P19-2005": {
    "title": "Not All Reviews Are Equal: Towards Addressing Reviewer Biases for Opinion Summarization",
    "abstract": "Consumers read online reviews for insights which help them to make decisions. Given the large volumes of reviews, succinct review summaries are important for many applications. Existing research has focused on mining for opinions from only review texts and largely ignores the reviewers. However, reviewers have biases and may write lenient or harsh reviews; they may also have preferences towards some topics over others. Therefore, not all reviews are equal. Ignoring the biases in reviews can generate misleading summaries. We aim for summarization of reviews to include balanced opinions from reviewers of different biases and preferences. We propose to model reviewer biases from their review texts and rating distributions, and learn a bias-aware opinion representation. We further devise an approach for balanced opinion summarization of reviews using our bias-aware opinion representation",
    "volume": "student",
    "checked": true,
    "id": "d3fec2b47051ab07e43af6462c62566e053df033",
    "citation_count": 3
  },
  "https://aclanthology.org/P19-2006": {
    "title": "Towards Turkish Abstract Meaning Representation",
    "abstract": "Using rooted, directed and labeled graphs, Abstract Meaning Representation (AMR) abstracts away from syntactic features such as word order and does not annotate every constituent in a sentence. AMR has been specified for English and was not supposed to be an Interlingua. However, several studies strived to overcome divergences in the annotations between English AMRs and those of their target languages by refining the annotation specification. Following this line of research, we have started to build the first Turkish AMR corpus by hand-annotating 100 sentences of the Turkish translation of the novel “The Little Prince” and comparing the results with the English AMRs available for the same corpus. The next step is to prepare the Turkish AMR annotation specification for training future annotators",
    "volume": "student",
    "checked": true,
    "id": "54e64d663a8bfb0d2835ca2a149bccc313a08345",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-2007": {
    "title": "Gender Stereotypes Differ between Male and Female Writings",
    "abstract": "Written language often contains gender stereotypes, typically conveyed unintentionally by the author. To study the difference in how female and male authors portray people of different genders, we quantitatively evaluate and analyze the gender stereotypes in their writings on two different datasets and from multiple aspects. We show that writings by females on average have lower gender stereotype scores. We plan to study and interpret the distributions of gender stereotype scores of individual words, and how they differ between male and female writings. We also plan on using more datasets over the past century to study how the stereotypes in female and male writings evolved over time",
    "volume": "student",
    "checked": true,
    "id": "3f24e48ee0206f4244e2b1dd40c0b6adb54d0976",
    "citation_count": 4
  },
  "https://aclanthology.org/P19-2008": {
    "title": "Question Answering in the Biomedical Domain",
    "abstract": "Question answering techniques have mainly been investigated in open domains. However, there are particular challenges in extending these open-domain techniques to extend into the biomedical domain. Question answering focusing on patients is less studied. We find that there are some challenges in patient question answering such as limited annotated data, lexical gap and quality of answer spans. We aim to address some of these gaps by extending and developing upon the literature to design a question answering system that can decide on the most appropriate answers for patients attempting to self-diagnose while including the ability to abstain from answering when confidence is low",
    "volume": "student",
    "checked": true,
    "id": "04a8716cf84b439e40fbe6b70c3e23e84a2f712e",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-2009": {
    "title": "Knowledge Discovery and Hypothesis Generation from Online Patient Forums: A Research Proposal",
    "abstract": "The unprompted patient experiences shared on patient forums contain a wealth of unexploited knowledge. Mining this knowledge and cross-linking it with biomedical literature, could expose novel insights, which could subsequently provide hypotheses for further clinical research. As of yet, automated methods for open knowledge discovery on patient forum text are lacking. Thus, in this research proposal, we outline future research into methods for mining, aggregating and cross-linking patient knowledge from online forums. Additionally, we aim to address how one could measure the credibility of this extracted knowledge",
    "volume": "student",
    "checked": true,
    "id": "ad7451663bae61968a67b8652bbbd5bfb5d01110",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-2010": {
    "title": "Automated Cross-language Intelligibility Analysis of Parkinson's Disease Patients Using Speech Recognition Technologies",
    "abstract": "Speech deficits are common symptoms amongParkinson’s Disease (PD) patients. The automatic assessment of speech signals is promising for the evaluation of the neurological state and the speech quality of the patients. Recently, progress has been made in applying machine learning and computational methods to automatically evaluate the speech of PD patients. In the present study, we plan to analyze the speech signals of PD patients and healthy control (HC) subjects in three different languages: German, Spanish, and Czech, with the aim to identify biomarkers to discriminate between PD patients and HC subjects and to evaluate the neurological state of the patients. Therefore, the main contribution of this study is the automatic classification of PD patients and HC subjects in different languages with focusing on phonation, articulation, and prosody. We will focus on an intelligibility analysis based on automatic speech recognition systems trained on these three languages. This is one of the first studies done that considers the evaluation of the speech of PD patients in different languages. The purpose of this research proposal is to build a model that can discriminate PD and HC subjects even when the language used for train and test is different",
    "volume": "student",
    "checked": true,
    "id": "e4e20e7d7f1d488dc98c9c827052eca071d04565",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-2011": {
    "title": "Natural Language Generation: Recently Learned Lessons, Directions for Semantic Representation-based Approaches, and the Case of Brazilian Portuguese Language",
    "abstract": "This paper presents a more recent literature review on Natural Language Generation. In particular, we highlight the efforts for Brazilian Portuguese in order to show the available resources and the existent approaches for this language. We also focus on the approaches for generation from semantic representations (emphasizing the Abstract Meaning Representation formalism) as well as their advantages and limitations, including possible future directions",
    "volume": "student",
    "checked": true,
    "id": "e6c9afbf608b64c0afd07e632d8ebf5e0c52cdf1",
    "citation_count": 0
  },
  "https://aclanthology.org/P19-2012": {
    "title": "Long-Distance Dependencies Don't Have to Be Long: Simplifying through Provably (Approximately) Optimal Permutations",
    "abstract": "Neural models at the sentence level often operate on the constituent words/tokens in a way that encodes the inductive bias of processing the input in a similar fashion to how humans do. However, there is no guarantee that the standard ordering of words is computationally efficient or optimal. To help mitigate this, we consider a dependency parse as a proxy for the inter-word dependencies in a sentence and simplify the sentence with respect to combinatorial objectives imposed on the sentence-parse pair. The associated optimization results in permuted sentences that are provably (approximately) optimal with respect to minimizing dependency parse lengths and that are demonstrably simpler. We evaluate our general-purpose permutations within a fine-tuning schema for the downstream task of subjectivity analysis. Our fine-tuned baselines reflect a new state of the art for the SUBJ dataset and the permutations we introduce lead to further improvements with a 2.0% increase in classification accuracy (absolute) and a 45% reduction in classification error (relative) over the previous state of the art",
    "volume": "student",
    "checked": true,
    "id": "f5d267e0485c8286966ec2c0a1031faccde0c470",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-2013": {
    "title": "Predicting the Outcome of Deliberative Democracy: A Research Proposal",
    "abstract": "As liberal states across the world face a decline in political participation by citizens, deliberative democracy is a promising solution for the public’s decreasing confidence and apathy towards the democratic process. Deliberative dialogue is method of public interaction that is fundamental to the concept of deliberative democracy. The ability to identify and predict consensus in the dialogues could bring greater accessibility and transparency to the face-to-face participatory process. The paper sets out a research plan for the first steps at automatically identifying and predicting consensus in a corpus of German language debates on hydraulic fracking. It proposes the use of a unique combination of lexical, sentiment, durational and further ‘derivative’ features of adjacency pairs to train traditional classification models. In addition to this, the use of deep learning techniques to improve the accuracy of the classification and prediction tasks is also discussed. Preliminary results at the classification of utterances are also presented, with an F1 between 0.61 and 0.64 demonstrating that the task of recognising agreement is demanding but possible",
    "volume": "student",
    "checked": true,
    "id": "3da18c4ba2133bb500550a57d56a8dabc7aa3c08",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-2014": {
    "title": "Active Reading Comprehension: A Dataset for Learning the Question-Answer Relationship Strategy",
    "abstract": "Reading comprehension (RC) through question answering is a useful method for evaluating if a reader understands a text. Standard accuracy metrics are used for evaluation, where high accuracy is taken as indicative of a good understanding. However, literature in quality learning suggests that task performance should also be evaluated on the undergone process to answer. The Question-Answer Relationship (QAR) is one of the strategies for evaluating a reader’s understanding based on their ability to select different sources of information depending on the question type. We propose the creation of a dataset to learn the QAR strategy with weak supervision. We expect to complement current work on reading comprehension by introducing a new setup for evaluation",
    "volume": "student",
    "checked": true,
    "id": "c990c3d7293e392c94523f798a4ce8d94a27221a",
    "citation_count": 0
  },
  "https://aclanthology.org/P19-2015": {
    "title": "Paraphrases as Foreign Languages in Multilingual Neural Machine Translation",
    "abstract": "Paraphrases, rewordings of the same semantic meaning, are useful for improving generalization and translation. Unlike previous works that only explore paraphrases at the word or phrase level, we use different translations of the whole training data that are consistent in structure as paraphrases at the corpus level. We treat paraphrases as foreign languages, tag source sentences with paraphrase labels, and train on parallel paraphrases in the style of multilingual Neural Machine Translation (NMT). Our multi-paraphrase NMT that trains only on two languages outperforms the multilingual baselines. Adding paraphrases improves the rare word translation and increases entropy and diversity in lexical choice. Adding the source paraphrases boosts performance better than adding the target ones, while adding both lifts performance further. We achieve a BLEU score of 57.2 for French-to-English translation using 24 corpus-level paraphrases of the Bible, which outperforms the multilingual baselines and is +34.7 above the single-source single-target NMT baseline",
    "volume": "student",
    "checked": true,
    "id": "21b59845c4a1d08a65d451cf7165706622b80a0f",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-2016": {
    "title": "Improving Mongolian-Chinese Neural Machine Translation with Morphological Noise",
    "abstract": "For the translation of agglutinative language such as typical Mongolian, unknown (UNK) words not only come from the quite restricted vocabulary, but also mostly from misunderstanding of the translation model to the morphological changes. In this study, we introduce a new adversarial training model to alleviate the UNK problem in Mongolian-Chinese machine translation. The training process can be described as three adversarial sub models (generator, value screener and discriminator), playing a win-win game. In this game, the added screener plays the role of emphasizing that the discriminator pays attention to the added Mongolian morphological noise in the form of pseudo-data and improving the training efficiency. The experimental results show that the newly emerged Mongolian-Chinese task is state-of-the-art. Under this premise, the training time is greatly shortened",
    "volume": "student",
    "checked": true,
    "id": "f62954d7570a79bfcfab0e06a8693a4679848b62",
    "citation_count": 2
  },
  "https://aclanthology.org/P19-2017": {
    "title": "Unsupervised Pretraining for Neural Machine Translation Using Elastic Weight Consolidation",
    "abstract": "This work presents our ongoing research of unsupervised pretraining in neural machine translation (NMT). In our method, we initialize the weights of the encoder and decoder with two language models that are trained with monolingual data and then fine-tune the model on parallel data using Elastic Weight Consolidation (EWC) to avoid forgetting of the original language modeling task. We compare the regularization by EWC with the previous work that focuses on regularization by language modeling objectives. The positive result is that using EWC with the decoder achieves BLEU scores similar to the previous work. However, the model converges 2-3 times faster and does not require the original unlabeled training data during the fine-tuning stage. In contrast, the regularization using EWC is less effective if the original and new tasks are not closely related. We show that initializing the bidirectional NMT encoder with a left-to-right language model and forcing the model to remember the original left-to-right language modeling task limits the learning capacity of the encoder for the whole bidirectional context",
    "volume": "student",
    "checked": true,
    "id": "3c541136849e575bd8a81e0f1e3c89214132df54",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-2018": {
    "title": "Māori Loanwords: A Corpus of New Zealand English Tweets",
    "abstract": "Māori loanwords are widely used in New Zealand English for various social functions by New Zealanders within and outside of the Māori community. Motivated by the lack of linguistic resources for studying how Māori loanwords are used in social media, we present a new corpus of New Zealand English tweets. We collected tweets containing selected Māori words that are likely to be known by New Zealanders who do not speak Māori. Since over 30% of these words turned out to be irrelevant, we manually annotated a sample of our tweets into relevant and irrelevant categories. This data was used to train machine learning models to automatically filter out irrelevant tweets",
    "volume": "student",
    "checked": true,
    "id": "5652ddff761974a48018c22e91226cf79a673b65",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-2019": {
    "title": "Ranking of Potential Questions",
    "abstract": "Questions are an integral part of discourse. They provide structure and support the exchange of information. One linguistic theory, the Questions Under Discussion model, takes question structures as integral to the functioning of a coherent discourse. This theory has not been tested on the count of its validity for predicting observations in real dialogue data, however. In this submission, a system for ranking explicit and implicit questions by their appropriateness in a dialogue is presented. This system implements constraints and principles put forward in the linguistic literature",
    "volume": "student",
    "checked": true,
    "id": "79889e529cfe44db8a507bc18988104cdbc419b9",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-2020": {
    "title": "Controlling Grammatical Error Correction Using Word Edit Rate",
    "abstract": "When professional English teachers correct grammatically erroneous sentences written by English learners, they use various methods. The correction method depends on how much corrections a learner requires. In this paper, we propose a method for neural grammar error correction (GEC) that can control the degree of correction. We show that it is possible to actually control the degree of GEC by using new training data annotated with word edit rate. Thereby, diverse corrected sentences is obtained from a single erroneous sentence. Moreover, compared to a GEC model that does not use information on the degree of correction, the proposed method improves correction accuracy",
    "volume": "student",
    "checked": true,
    "id": "cad1d81d9fa4f0ae671014b2decfe88254b7c29d",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-2021": {
    "title": "From Brain Space to Distributional Space: The Perilous Journeys of fMRI Decoding",
    "abstract": "Recent work in cognitive neuroscience has introduced models for predicting distributional word meaning representations from brain imaging data. Such models have great potential, but the quality of their predictions has not yet been thoroughly evaluated from a computational linguistics point of view. Due to the limited size of available brain imaging datasets, standard quality metrics (e.g. similarity judgments and analogies) cannot be used. Instead, we investigate the use of several alternative measures for evaluating the predicted distributional space against a corpus-derived distributional space. We show that a state-of-the-art decoder, while performing impressively on metrics that are commonly used in cognitive neuroscience, performs unexpectedly poorly on our metrics. To address this, we propose strategies for improving the model’s performance. Despite returning promising results, our experiments also demonstrate that much work remains to be done before distributional representations can reliably be predicted from brain data",
    "volume": "student",
    "checked": true,
    "id": "9a2c532c085e6436bfad958a972686850f4cd2a3",
    "citation_count": 3
  },
  "https://aclanthology.org/P19-2022": {
    "title": "Towards Incremental Learning of Word Embeddings Using Context Informativeness",
    "abstract": "In this paper, we investigate the task of learning word embeddings from very sparse data in an incremental, cognitively-plausible way. We focus on the notion of ‘informativeness’, that is, the idea that some content is more valuable to the learning process than other. We further highlight the challenges of online learning and argue that previous systems fall short of implementing incrementality. Concretely, we incorporate informativeness in a previously proposed model of nonce learning, using it for context selection and learning rate modulation. We test our system on the task of learning new words from definitions, as well as on the task of learning new words from potentially uninformative contexts. We demonstrate that informativeness is crucial to obtaining state-of-the-art performance in a truly incremental setup",
    "volume": "student",
    "checked": true,
    "id": "edcad0b6b0afac8812425f39e1edc3d3c03c2402",
    "citation_count": 8
  },
  "https://aclanthology.org/P19-2023": {
    "title": "A Strong and Robust Baseline for Text-Image Matching",
    "abstract": "We review the current schemes of text-image matching models and propose improvements for both training and inference. First, we empirically show limitations of two popular loss (sum and max-margin loss) widely used in training text-image embeddings and propose a trade-off: a kNN-margin loss which 1) utilizes information from hard negatives and 2) is robust to noise as all K-most hardest samples are taken into account, tolerating pseudo negatives and outliers. Second, we advocate the use of Inverted Softmax (IS) and Cross-modal Local Scaling (CSLS) during inference to mitigate the so-called hubness problem in high-dimensional embedding space, enhancing scores of all metrics by a large margin",
    "volume": "student",
    "checked": true,
    "id": "edd200e1e18794202c3a55a810717e87be7b7dba",
    "citation_count": 12
  },
  "https://aclanthology.org/P19-2024": {
    "title": "Incorporating Textual Information on User Behavior for Personality Prediction",
    "abstract": "Several recent studies have shown that textual information of user posts and user behaviors such as liking and sharing the specific posts are useful for predicting the personality of social media users. However, less attention has been paid to the textual information derived from the user behaviors. In this paper, we investigate the effect of textual information on user behaviors for personality prediction. Our experiments on the personality prediction of Twitter users show that the textual information of user behaviors is more useful than the co-occurrence information of the user behaviors. They also show that taking user behaviors into account is crucial for predicting the personality of users who do not post frequently",
    "volume": "student",
    "checked": true,
    "id": "7d24953e8bf3fa776ca56b3f026948d03244ebf5",
    "citation_count": 14
  },
  "https://aclanthology.org/P19-2025": {
    "title": "Corpus Creation and Analysis for Named Entity Recognition in Telugu-English Code-Mixed Social Media Data",
    "abstract": "Named Entity Recognition(NER) is one of the important tasks in Natural Language Processing(NLP) and also is a subtask of Information Extraction. In this paper we present our work on NER in Telugu-English code-mixed social media data. Code-Mixing, a progeny of multilingualism is a way in which multilingual people express themselves on social media by using linguistics units from different languages within a sentence or speech context. Entity Extraction from social media data such as tweets(twitter) is in general difficult due to its informal nature, code-mixed data further complicates the problem due to its informal, unstructured and incomplete information. We present a Telugu-English code-mixed corpus with the corresponding named entity tags. The named entities used to tag data are Person(‘Per’), Organization(‘Org’) and Location(‘Loc’). We experimented with the machine learning models Conditional Random Fields(CRFs), Decision Trees and BiLSTMs on our corpus which resulted in a F1-score of 0.96, 0.94 and 0.95 respectively",
    "volume": "student",
    "checked": true,
    "id": "55d518e167518557bce2b6040bbaa59a659bf2d8",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-2026": {
    "title": "Joint Learning of Named Entity Recognition and Entity Linking",
    "abstract": "Named entity recognition (NER) and entity linking (EL) are two fundamentally related tasks, since in order to perform EL, first the mentions to entities have to be detected. However, most entity linking approaches disregard the mention detection part, assuming that the correct mentions have been previously detected. In this paper, we perform joint learning of NER and EL to leverage their relatedness and obtain a more robust and generalisable system. For that, we introduce a model inspired by the Stack-LSTM approach. We observe that, in fact, doing multi-task learning of NER and EL improves the performance in both tasks when comparing with models trained with individual objectives. Furthermore, we achieve results competitive with the state-of-the-art in both NER and EL",
    "volume": "student",
    "checked": true,
    "id": "ba7096622583fe4ecee35cdff48fa31a2cba70f8",
    "citation_count": 59
  },
  "https://aclanthology.org/P19-2027": {
    "title": "Dialogue-Act Prediction of Future Responses Based on Conversation History",
    "abstract": "Sequence-to-sequence models are a common approach to develop a chatbot. They can train a conversational model in an end-to-end manner. One significant drawback of such a neural network based approach is that the response generation process is a black-box, and how a specific response is generated is unclear. To tackle this problem, an interpretable response generation mechanism is desired. As a step toward this direction, we focus on dialogue-acts (DAs) that may provide insight to understand the response generation process. In particular, we propose a method to predict a DA of the next response based on the history of previous utterances and their DAs. Experiments using a Switch Board Dialogue Act corpus show that compared to the baseline considering only a single utterance, our model achieves 10.8% higher F1-score and 3.0% higher accuracy on DA prediction",
    "volume": "student",
    "checked": true,
    "id": "2157905fa30f6b8e5fcbed811a9f4efd9e4aef9a",
    "citation_count": 8
  },
  "https://aclanthology.org/P19-2028": {
    "title": "Computational Ad Hominem Detection",
    "abstract": "Fallacies like the personal attack—also known as the ad hominem attack—are introduced in debates as an easy win, even though they provide no rhetorical contribution. Although their importance in argumentation mining is acknowledged, automated mining and analysis is still lacking. We show TF-IDF approaches are insufficient to detect the ad hominem attack. Therefore we present a machine learning approach for information extraction, which has a recall of 80% for a social media data source. We also demonstrate our approach with an application that uses online learning",
    "volume": "student",
    "checked": true,
    "id": "f4d92ef6c1a40f6e70ab861003cbfa87039d8c15",
    "citation_count": 6
  },
  "https://aclanthology.org/P19-2029": {
    "title": "Multiple Character Embeddings for Chinese Word Segmentation",
    "abstract": "Chinese word segmentation (CWS) is often regarded as a character-based sequence labeling task in most current works which have achieved great success with the help of powerful neural networks. However, these works neglect an important clue: Chinese characters incorporate both semantic and phonetic meanings. In this paper, we introduce multiple character embeddings including Pinyin Romanization and Wubi Input, both of which are easily accessible and effective in depicting semantics of characters. We propose a novel shared Bi-LSTM-CRF model to fuse linguistic features efficiently by sharing the LSTM network during the training procedure. Extensive experiments on five corpora show that extra embeddings help obtain a significant improvement in labeling accuracy. Specifically, we achieve the state-of-the-art performance in AS and CityU corpora with F1 scores of 96.9 and 97.3, respectively without leveraging any external lexical resources",
    "volume": "student",
    "checked": true,
    "id": "0ef66969ef82466765379795b57218be0072da16",
    "citation_count": 16
  },
  "https://aclanthology.org/P19-2030": {
    "title": "Attention over Heads: A Multi-Hop Attention for Neural Machine Translation",
    "abstract": "In this paper, we propose a multi-hop attention for the Transformer. It refines the attention for an output symbol by integrating that of each head, and consists of two hops. The first hop attention is the scaled dot-product attention which is the same attention mechanism used in the original Transformer. The second hop attention is a combination of multi-layer perceptron (MLP) attention and head gate, which efficiently increases the complexity of the model by adding dependencies between heads. We demonstrate that the translation accuracy of the proposed multi-hop attention outperforms the baseline Transformer significantly, +0.85 BLEU point for the IWSLT-2017 German-to-English task and +2.58 BLEU point for the WMT-2017 German-to-English task. We also find that the number of parameters required for a multi-hop attention is smaller than that for stacking another self-attention layer and the proposed model converges significantly faster than the original Transformer",
    "volume": "student",
    "checked": true,
    "id": "dffed23fd7f2de433073f2453ab65ebc2954863a",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-2031": {
    "title": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function",
    "abstract": "Gender bias exists in natural language datasets, which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach and show that it outperforms existing strategies in all bias evaluation metrics",
    "volume": "student",
    "checked": true,
    "id": "623b1c61aa36048a38485a44551cb3fdcbcc827b",
    "citation_count": 41
  },
  "https://aclanthology.org/P19-2032": {
    "title": "Automatic Generation of Personalized Comment Based on User Profile",
    "abstract": "Comments on social media are very diverse, in terms of content, style and vocabulary, which make generating comments much more challenging than other existing natural language generation (NLG) tasks. Besides, since different user has different expression habits, it is necessary to take the user’s profile into consideration when generating comments. In this paper, we introduce the task of automatic generation of personalized comment (AGPC) for social media. Based on tens of thousands of users’ real comments and corresponding user profiles on weibo, we propose Personalized Comment Generation Network (PCGN) for AGPC. The model utilizes user feature embedding with a gated memory and attends to user description to model personality of users. In addition, external user representation is taken into consideration during the decoding to enhance the comments generation. Experimental results show that our model can generate natural, human-like and personalized comments",
    "volume": "student",
    "checked": true,
    "id": "46f152301641eb1400eac915c518d637fada00d8",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-2033": {
    "title": "From Bilingual to Multilingual Neural Machine Translation by Incremental Training",
    "abstract": "Multilingual Neural Machine Translation approaches are based on the use of task specific models and the addition of one more language can only be done by retraining the whole system. In this work, we propose a new training schedule that allows the system to scale to more languages without modification of the previous components based on joint training and language-independent encoder/decoder modules allowing for zero-shot translation. This work in progress shows close results to state-of-the-art in the WMT task",
    "volume": "student",
    "checked": true,
    "id": "98b127e4bb1f0ac304bca251a056384786222fe5",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-2034": {
    "title": "STRASS: A Light and Effective Method for Extractive Summarization Based on Sentence Embeddings",
    "abstract": "This paper introduces STRASS: Summarization by TRAnsformation Selection and Scoring. It is an extractive text summarization method which leverages the semantic information in existing sentence embedding spaces. Our method creates an extractive summary by selecting the sentences with the closest embeddings to the document embedding. The model earns a transformation of the document embedding to minimize the similarity between the extractive summary and the ground truth summary. As the transformation is only composed of a dense layer, the training can be done on CPU, therefore, inexpensive. Moreover, inference time is short and linear according to the number of sentences. As a second contribution, we introduce the French CASS dataset, composed of judgments from the French Court of cassation and their corresponding summaries. On this dataset, our results show that our method performs similarly to the state of the art extractive methods with effective training and inferring time",
    "volume": "student",
    "checked": true,
    "id": "81c9a5a8eff0ac28320bfd39c9578d54c4588376",
    "citation_count": 10
  },
  "https://aclanthology.org/P19-2035": {
    "title": "Attention and Lexicon Regularized LSTM for Aspect-based Sentiment Analysis",
    "abstract": "Abstract Attention based deep learning systems have been demonstrated to be the state of the art approach for aspect-level sentiment analysis, however, end-to-end deep neural networks lack flexibility as one can not easily adjust the network to fix an obvious problem, especially when more training data is not available: e.g. when it always predicts positive when seeing the word disappointed. Meanwhile, it is less stressed that attention mechanism is likely to “over-focus” on particular parts of a sentence, while ignoring positions which provide key information for judging the polarity. In this paper, we describe a simple yet effective approach to leverage lexicon information so that the model becomes more flexible and robust. We also explore the effect of regularizing attention vectors to allow the network to have a broader “focus” on different parts of the sentence. The experimental results demonstrate the effectiveness of our approach",
    "volume": "student",
    "checked": true,
    "id": "d628cec30692a818d9129b0caf7115eee5db88c1",
    "citation_count": 35
  },
  "https://aclanthology.org/P19-2036": {
    "title": "Controllable Text Simplification with Lexical Constraint Loss",
    "abstract": "We propose a method to control the level of a sentence in a text simplification task. Text simplification is a monolingual translation task translating a complex sentence into a simpler and easier to understand the alternative. In this study, we use the grade level of the US education system as the level of the sentence. Our text simplification method succeeds in translating an input into a specific grade level by considering levels of both sentences and words. Sentence level is considered by adding the target grade level as input. By contrast, the word level is considered by adding weights to the training loss based on words that frequently appear in sentences of the desired grade level. Although existing models that consider only the sentence level may control the syntactic complexity, they tend to generate words beyond the target level. Our approach can control both the lexical and syntactic complexity and achieve an aggressive rewriting. Experiment results indicate that the proposed method improves the metrics of both BLEU and SARI",
    "volume": "student",
    "checked": true,
    "id": "82e39e2d8d6d7ce84a65f934f378a8d19b8ac0db",
    "citation_count": 33
  },
  "https://aclanthology.org/P19-2037": {
    "title": "Normalizing Non-canonical Turkish Texts Using Machine Translation Approaches",
    "abstract": "With the growth of the social web, user-generated text data has reached unprecedented sizes. Non-canonical text normalization provides a way to exploit this as a practical source of training data for language processing systems. The state of the art in Turkish text normalization is composed of a token level pipeline of modules, heavily dependent on external linguistic resources and manually defined rules. Instead, we propose a fully automated, context-aware machine translation approach with fewer stages of processing. Experiments with various implementations of our approach show that we are able to surpass the current best-performing system by a large margin",
    "volume": "student",
    "checked": true,
    "id": "56aaab14a16ef1b83d12a9589a46d838dc33f28f",
    "citation_count": 10
  },
  "https://aclanthology.org/P19-2038": {
    "title": "ARHNet - Leveraging Community Interaction for Detection of Religious Hate Speech in Arabic",
    "abstract": "The rapid widespread of social media has lead to some undesirable consequences like the rapid increase of hateful content and offensive language. Religious Hate Speech, in particular, often leads to unrest and sometimes aggravates to violence against people on the basis of their religious affiliations. The richness of the Arabic morphology and the limited available resources makes this task especially challenging. The current state-of-the-art approaches to detect hate speech in Arabic rely entirely on textual (lexical and semantic) cues. Our proposed methodology contends that leveraging Community-Interaction can better help us profile hate speech content on social media. Our proposed ARHNet (Arabic Religious Hate Speech Net) model incorporates both Arabic Word Embeddings and Social Network Graphs for the detection of religious hate speech",
    "volume": "student",
    "checked": true,
    "id": "016a3216fb4b66336331795b948953d394c53896",
    "citation_count": 17
  },
  "https://aclanthology.org/P19-2039": {
    "title": "Investigating Political Herd Mentality: A Community Sentiment Based Approach",
    "abstract": "Analyzing polarities and sentiments inherent in political speeches and debates poses an important problem today. This experiment aims to address this issue by analyzing publicly-available Hansard transcripts of the debates conducted in the UK Parliament. Our proposed approach, which uses community-based graph information to augment hand-crafted features based on topic modeling and emotion detection on debate transcripts, currently surpasses the benchmark results on the same dataset. Such sentiment classification systems could prove to be of great use in today’s politically turbulent times, for public knowledge of politicians’ stands on various relevant issues proves vital for good governance and citizenship. The experiments also demonstrate that continuous feature representations learned from graphs can improve performance on sentiment classification tasks significantly",
    "volume": "student",
    "checked": true,
    "id": "89cc165d61699e3a55e2c4fbdb34a63bd1b165ba",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-2040": {
    "title": "Transfer Learning Based Free-Form Speech Command Classification for Low-Resource Languages",
    "abstract": "Current state-of-the-art speech-based user interfaces use data intense methodologies to recognize free-form speech commands. However, this is not viable for low-resource languages, which lack speech data. This restricts the usability of such interfaces to a limited number of languages. In this paper, we propose a methodology to develop a robust domain-specific speech command classification system for low-resource languages using speech data of a high-resource language. In this transfer learning-based approach, we used a Convolution Neural Network (CNN) to identify a fixed set of intents using an ASR-based character probability map. We were able to achieve significant results for Sinhala and Tamil datasets using an English based ASR, which attests the robustness of the proposed approach",
    "volume": "student",
    "checked": true,
    "id": "9ad68df478b6571ca9dd70deddeb0fb80f7b0850",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-2041": {
    "title": "Embedding Strategies for Specialized Domains: Application to Clinical Entity Recognition",
    "abstract": "Using pre-trained word embeddings in conjunction with Deep Learning models has become the “de facto” approach in Natural Language Processing (NLP). While this usually yields satisfactory results, off-the-shelf word embeddings tend to perform poorly on texts from specialized domains such as clinical reports. Moreover, training specialized word representations from scratch is often either impossible or ineffective due to the lack of large enough in-domain data. In this work, we focus on the clinical domain for which we study embedding strategies that rely on general-domain resources only. We show that by combining off-the-shelf contextual embeddings (ELMo) with static word2vec embeddings trained on a small in-domain corpus built from the task data, we manage to reach and sometimes outperform representations learned from a large corpus in the medical domain",
    "volume": "student",
    "checked": true,
    "id": "d8d7e338e528bdf89c589ab5926fc9ec4339c78b",
    "citation_count": 13
  },
  "https://aclanthology.org/P19-2042": {
    "title": "Enriching Neural Models with Targeted Features for Dementia Detection",
    "abstract": "Alzheimers disease is an irreversible brain disease that slowly destroys memory skills andthinking skills leading to the need for full-time care. Early detection of Alzheimer’s dis-ease is fundamental to slow down the progress of the disease. In this work we are developing Natural Language Processing techniques to detect linguistic characteristics of patients suffering Alzheimer’s Disease and related Dementias. We are proposing a neural model based on a CNN-LSTM architecture that is able to take in consideration both long language samples and hand-crafted linguistic features to distinguish between dementia affected and healthy patients. We are exploring the effects of the introduction of an attention mechanism on both our model and the actual state of the art. Our approach is able to set a new state-of-the art on the DementiaBank dataset achieving an F1 Score of 0.929 in the Dementia patients classification Supplementary material include code to run the experiments",
    "volume": "student",
    "checked": true,
    "id": "a67e59ba96d86a0b609b84363dc212f3dfef97dd",
    "citation_count": 23
  },
  "https://aclanthology.org/P19-2043": {
    "title": "English-Indonesian Neural Machine Translation for Spoken Language Domains",
    "abstract": "In this work, we conduct a study on Neural Machine Translation (NMT) for English-Indonesian (EN-ID) and Indonesian-English (ID-EN). We focus on spoken language domains, namely colloquial and speech languages. We build NMT systems using the Transformer model for both translation directions and implement domain adaptation, in which we train our pre-trained NMT systems on speech language (in-domain) data. Moreover, we conduct an evaluation on how the domain-adaptation method in our EN-ID system can result in more formal translation outputs",
    "volume": "student",
    "checked": true,
    "id": "dfc78778309562c9f767cfda900ec291eb919d5f",
    "citation_count": 2
  },
  "https://aclanthology.org/P19-2044": {
    "title": "Improving Neural Entity Disambiguation with Graph Embeddings",
    "abstract": "Entity Disambiguation (ED) is the task of linking an ambiguous entity mention to a corresponding entry in a knowledge base. Current methods have mostly focused on unstructured text data to learn representations of entities, however, there is structured information in the knowledge base itself that should be useful to disambiguate entities. In this work, we propose a method that uses graph embeddings for integrating structured information from the knowledge base with unstructured information from text-based representations. Our experiments confirm that graph embeddings trained on a graph of hyperlinks between Wikipedia articles improve the performances of simple feed-forward neural ED model and a state-of-the-art neural ED system",
    "volume": "student",
    "checked": true,
    "id": "540a140e4b8576e0b4edaefd5cee9d9c55da0e1d",
    "citation_count": 23
  },
  "https://aclanthology.org/P19-2045": {
    "title": "Hierarchical Multi-label Classification of Text with Capsule Networks",
    "abstract": "Capsule networks have been shown to demonstrate good performance on structured data in the area of visual inference. In this paper we apply and compare simple shallow capsule networks for hierarchical multi-label text classification and show that they can perform superior to other neural networks, such as CNNs and LSTMs, and non-neural network architectures such as SVMs. For our experiments, we use the established Web of Science (WOS) dataset and introduce a new real-world scenario dataset, the BlurbGenreCollection (BGC). Our results confirm the hypothesis that capsule networks are especially advantageous for rare events and structurally diverse categories, which we attribute to their ability to combine latent encoded information",
    "volume": "student",
    "checked": true,
    "id": "58203813610b866483ffc2bd1181f616ae38107c",
    "citation_count": 46
  },
  "https://aclanthology.org/P19-2046": {
    "title": "Convolutional Neural Networks for Financial Text Regression",
    "abstract": "Forecasting financial volatility of a publicly-traded company from its annual reports has been previously defined as a text regression problem. Recent studies use a manually labeled lexicon to filter the annual reports by keeping sentiment words only. In order to remove the lexicon dependency without decreasing the performance, we replace bag-of-words model word features by word embedding vectors. Using word vectors increases the number of parameters. Considering the increase in number of parameters and excessive lengths of annual reports, a convolutional neural network model is proposed and transfer learning is applied. Experimental results show that the convolutional neural network model provides more accurate volatility predictions than lexicon based models",
    "volume": "student",
    "checked": true,
    "id": "8d4d1f67f81cb8e12e0206f35ff13429e686b610",
    "citation_count": 10
  },
  "https://aclanthology.org/P19-2047": {
    "title": "Sentiment Analysis on Naija-Tweets",
    "abstract": "Examining sentiments in social media poses a challenge to natural language processing because of the intricacy and variability in the dialect articulation, noisy terms in form of slang, abbreviation, acronym, emoticon, and spelling error coupled with the availability of real-time content. Moreover, most of the knowledge-based approaches for resolving slang, abbreviation, and acronym do not consider the issue of ambiguity that evolves in the usage of these noisy terms. This research work proposes an improved framework for social media feed pre-processing that leverages on the combination of integrated local knowledge bases and adapted Lesk algorithm to facilitate pre-processing of social media feeds. The results from the experimental evaluation revealed an improvement over existing methods when applied to supervised learning algorithms in the task of extracting sentiments from Nigeria-origin tweets with an accuracy of 99.17%",
    "volume": "student",
    "checked": true,
    "id": "75754fea78f04685637be758d93032c094a508b7",
    "citation_count": 4
  },
  "https://aclanthology.org/P19-2048": {
    "title": "Fact or Factitious? Contextualized Opinion Spam Detection",
    "abstract": "In this paper we perform an analytic comparison of a number of techniques used to detect fake and deceptive online reviews. We apply a number machine learning approaches found to be effective, and introduce our own approach by fine-tuning state of the art contextualised embeddings. The results we obtain show the potential of contextualised embeddings for fake review detection, and lay the groundwork for future research in this area",
    "volume": "student",
    "checked": true,
    "id": "e959b7ea12c82c956041beccf0c09dce3df3af8f",
    "citation_count": 21
  },
  "https://aclanthology.org/P19-2049": {
    "title": "Scheduled Sampling for Transformers",
    "abstract": "Scheduled sampling is a technique for avoiding one of the known problems in sequence-to-sequence generation: exposure bias. It consists of feeding the model a mix of the teacher forced embeddings and the model predictions from the previous step in training time. The technique has been used for improving model performance with recurrent neural networks (RNN). In the Transformer model, unlike the RNN, the generation of a new word attends to the full sentence generated so far, not only to the last word, and it is not straightforward to apply the scheduled sampling technique. We propose some structural changes to allow scheduled sampling to be applied to Transformer architectures, via a two-pass decoding strategy. Experiments on two language pairs achieve performance close to a teacher-forcing baseline and show that this technique is promising for further exploration",
    "volume": "student",
    "checked": true,
    "id": "6b6befbff611ddc98ef268b3c51353593bb07e77",
    "citation_count": 36
  },
  "https://aclanthology.org/P19-2050": {
    "title": "BREAKING! Presenting Fake News Corpus for Automated Fact Checking",
    "abstract": "Popular fake news articles spread faster than mainstream articles on the same topic which renders manual fact checking inefficient. At the same time, creating tools for automatic detection is as challenging due to lack of dataset containing articles which present fake or manipulated stories as compelling facts. In this paper, we introduce manually verified corpus of compelling fake and questionable news articles on the USA politics, containing around 700 articles from Aug-Nov, 2016. We present various analyses on this corpus and finally implement classification model based on linguistic features. This work is still in progress as we plan to extend the dataset in the future and use it for our approach towards automated fake news detection",
    "volume": "student",
    "checked": true,
    "id": "74ca163326c1e8ca076bfd7d1355f58c9a772777",
    "citation_count": 20
  },
  "https://aclanthology.org/P19-2051": {
    "title": "Cross-domain and Cross-lingual Abusive Language Detection: A Hybrid Approach with Deep Learning and a Multilingual Lexicon",
    "abstract": "The development of computational methods to detect abusive language in social media within variable and multilingual contexts has recently gained significant traction. The growing interest is confirmed by the large number of benchmark corpora for different languages developed in the latest years. However, abusive language behaviour is multifaceted and available datasets are featured by different topical focuses. This makes abusive language detection a domain-dependent task, and building a robust system to detect general abusive content a first challenge. Moreover, most resources are available for English, which makes detecting abusive language in low-resource languages a further challenge. We address both challenges by considering ten publicly available datasets across different domains and languages. A hybrid approach with deep learning and a multilingual lexicon to cross-domain and cross-lingual detection of abusive content is proposed and compared with other simpler models. We show that training a system on general abusive language datasets will produce a cross-domain robust system, which can be used to detect other more specific types of abusive content. We also found that using the domain-independent lexicon HurtLex is useful to transfer knowledge between domains and languages. In the cross-lingual experiment, we demonstrate the effectiveness of our jointlearning model also in out-domain scenarios",
    "volume": "student",
    "checked": true,
    "id": "4af68fb572b518edc7b3c04fb025c5b40eeca9ad",
    "citation_count": 65
  },
  "https://aclanthology.org/P19-2052": {
    "title": "De-Mixing Sentiment from Code-Mixed Text",
    "abstract": "Code-mixing is the phenomenon of mixing the vocabulary and syntax of multiple languages in the same sentence. It is an increasingly common occurrence in today’s multilingual society and poses a big challenge when encountered in different downstream tasks. In this paper, we present a hybrid architecture for the task of Sentiment Analysis of English-Hindi code-mixed data. Our method consists of three components, each seeking to alleviate different issues. We first generate subword level representations for the sentences using a CNN architecture. The generated representations are used as inputs to a Dual Encoder Network which consists of two different BiLSTMs - the Collective and Specific Encoder. The Collective Encoder captures the overall sentiment of the sentence, while the Specific Encoder utilizes an attention mechanism in order to focus on individual sentiment-bearing sub-words. This, combined with a Feature Network consisting of orthographic features and specially trained word embeddings, achieves state-of-the-art results - 83.54% accuracy and 0.827 F1 score - on a benchmark dataset",
    "volume": "student",
    "checked": true,
    "id": "d1d944267c66fd903af230bec4d5a92952aec34b",
    "citation_count": 43
  },
  "https://aclanthology.org/P19-2053": {
    "title": "Unsupervised Learning of Discourse-Aware Text Representation for Essay Scoring",
    "abstract": "Existing document embedding approaches mainly focus on capturing sequences of words in documents. However, some document classification and regression tasks such as essay scoring need to consider discourse structure of documents. Although some prior approaches consider this issue and utilize discourse structure of text for document classification, these approaches are dependent on computationally expensive parsers. In this paper, we propose an unsupervised approach to capture discourse structure in terms of coherence and cohesion for document embedding that does not require any expensive parser or annotation. Extrinsic evaluation results show that the document representation obtained from our approach improves the performance of essay Organization scoring and Argument Strength scoring",
    "volume": "student",
    "checked": true,
    "id": "f201648bb50df1a66825d79c2b510ad469c80ee8",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-2054": {
    "title": "Multimodal Logical Inference System for Visual-Textual Entailment",
    "abstract": "A large amount of research about multimodal inference across text and vision has been recently developed to obtain visually grounded word and sentence representations. In this paper, we use logic-based representations as unified meaning representations for texts and images and present an unsupervised multimodal logical inference system that can effectively prove entailment relations between them. We show that by combining semantic parsing and theorem proving, the system can handle semantically complex sentences for visual-textual inference",
    "volume": "student",
    "checked": true,
    "id": "5e59b5dedafb4f647f9cf5976b1f99eb1de12b77",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-2055": {
    "title": "Deep Neural Models for Medical Concept Normalization in User-Generated Texts",
    "abstract": "In this work, we consider the medical concept normalization problem, i.e., the problem of mapping a health-related entity mention in a free-form text to a concept in a controlled vocabulary, usually to the standard thesaurus in the Unified Medical Language System (UMLS). This is a challenging task since medical terminology is very different when coming from health care professionals or from the general public in the form of social media texts. We approach it as a sequence learning problem with powerful neural networks such as recurrent neural networks and contextualized word representation models trained to obtain semantic representations of social media expressions. Our experimental evaluation over three different benchmarks shows that neural architectures leverage the semantic meaning of the entity mention and significantly outperform existing state of the art models",
    "volume": "student",
    "checked": true,
    "id": "04c31edb2a74e078dd4580a46a366dab79c1e7e0",
    "citation_count": 37
  },
  "https://aclanthology.org/P19-2056": {
    "title": "Using Semantic Similarity as Reward for Reinforcement Learning in Sentence Generation",
    "abstract": "Traditional model training for sentence generation employs cross-entropy loss as the loss function. While cross-entropy loss has convenient properties for supervised learning, it is unable to evaluate sentences as a whole, and lacks flexibility. We present the approach of training the generation model using the estimated semantic similarity between the output and reference sentences to alleviate the problems faced by the training with cross-entropy loss. We use the BERT-based scorer fine-tuned to the Semantic Textual Similarity (STS) task for semantic similarity estimation, and train the model with the estimated scores through reinforcement learning (RL). Our experiments show that reinforcement learning with semantic similarity reward improves the BLEU scores from the baseline LSTM NMT model",
    "volume": "student",
    "checked": true,
    "id": "6a53a38e1b160ab70f4a0f84ceff906ac84d9b12",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-2057": {
    "title": "Sentiment Classification Using Document Embeddings Trained with Cosine Similarity",
    "abstract": "In document-level sentiment classification, each document must be mapped to a fixed length vector. Document embedding models map each document to a dense, low-dimensional vector in continuous vector space. This paper proposes training document embeddings using cosine similarity instead of dot product. Experiments on the IMDB dataset show that accuracy is improved when using cosine similarity compared to using dot product, while using feature combination with Naive Bayes weighted bag of n-grams achieves a competitive accuracy of 93.68%. Code to reproduce all experiments is available at https://github.com/tanthongtan/dv-cosine",
    "volume": "student",
    "checked": true,
    "id": "2a7e68459d79fcfdc147781593a0da9b1e93ab00",
    "citation_count": 70
  },
  "https://aclanthology.org/P19-2058": {
    "title": "Detecting Adverse Drug Reactions from Biomedical Texts with Neural Networks",
    "abstract": "Detection of adverse drug reactions in postapproval periods is a crucial challenge for pharmacology. Social media and electronic clinical reports are becoming increasingly popular as a source for obtaining health related information. In this work, we focus on extraction information of adverse drug reactions from various sources of biomedical textbased information, including biomedical literature and social media. We formulate the problem as a binary classification task and compare the performance of four state-of-the-art attention-based neural networks in terms of the F-measure. We show the effectiveness of these methods on four different benchmarks",
    "volume": "student",
    "checked": true,
    "id": "abd3228149748c61d614b9e095da1a206e552671",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-2059": {
    "title": "Annotating and Analyzing Semantic Role of Elementary Units and Relations in Online Persuasive Arguments",
    "abstract": "For analyzing online persuasions, one of the important goals is to semantically understand how people construct comments to persuade others. However, analyzing the semantic role of arguments for online persuasion has been less emphasized. Therefore, in this study, we propose a novel annotation scheme that captures the semantic role of arguments in a popular online persuasion forum, so-called ChangeMyView. Through this study, we have made the following contributions: (i) proposing a scheme that includes five types of elementary units (EUs) and two types of relations. (ii) annotating ChangeMyView which results in 4612 EUs and 2713 relations in 345 posts. (iii) analyzing the semantic role of persuasive arguments. Our analyses captured certain characteristic phenomena for online persuasion",
    "volume": "student",
    "checked": true,
    "id": "143b8a35356b1b7f9e590d983b52719b15497d2b",
    "citation_count": 15
  },
  "https://aclanthology.org/P19-2060": {
    "title": "A Japanese Word Segmentation Proposal",
    "abstract": "Current Japanese word segmentation methods, that use a morpheme-based approach, may produce different segmentations for the same strings. This occurs when these strings appear in different sentences. The cause is the influence of different contexts around these strings affecting the probabilistic models used in segmentation algorithms. This paper presents an alternative to the current morpheme-based scheme for Japanese word segmentation. The proposed scheme focuses on segmenting inflections as single words instead of separating the auxiliary verbs and other morphemes from the stems. Some morphological segmentation rules are presented for each type of word and these rules are implemented in a program which is properly described. The program is used to generate a segmentation of a sentence corpus, whose consistency is calculated and compared with the current morpheme-based segmentation of the same corpus. The experiments show that this method produces a much more consistent segmentation than the morpheme-based one",
    "volume": "student",
    "checked": true,
    "id": "f38bfb99cc9d4795f5aa82513f5a45968075d059",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-3001": {
    "title": "Sakura: Large-scale Incorrect Example Retrieval System for Learners of Japanese as a Second Language",
    "abstract": "This study develops an incorrect example retrieval system, called Sakura, using a large-scale Lang-8 dataset for Japanese language learners. Existing example retrieval systems do not include grammatically incorrect examples or present only a few examples, if any. If a retrieval system has a wide coverage of incorrect examples along with the correct counterpart, learners can revise their composition themselves. Considering the usability of retrieving incorrect examples, our proposed system uses a large-scale corpus to expand the coverage of incorrect examples and presents correct expressions along with incorrect expressions. Our intrinsic and extrinsic evaluations indicate that our system is more useful than a previous system",
    "volume": "demo",
    "checked": true,
    "id": "835e93e5abecad38d7bd4a678d38ade0e5bb4e62",
    "citation_count": 0
  },
  "https://aclanthology.org/P19-3002": {
    "title": "SLATE: A Super-Lightweight Annotation Tool for Experts",
    "abstract": "Many annotation tools have been developed, covering a wide variety of tasks and providing features like user management, pre-processing, and automatic labeling. However, all of these tools use Graphical User Interfaces, and often require substantial effort to install and configure. This paper presents a new annotation tool that is designed to fill the niche of a lightweight interface for users with a terminal-based workflow. SLATE supports annotation at different scales (spans of characters, tokens, and lines, or a document) and of different types (free text, labels, and links), with easily customisable keybindings, and unicode support. In a user study comparing with other tools it was consistently the easiest to install and use. SLATE fills a need not met by existing systems, and has already been used to annotate two corpora, one of which involved over 250 hours of annotation effort",
    "volume": "demo",
    "checked": true,
    "id": "f7b9c89f98fded757aa1d6f919eb87346c1fb085",
    "citation_count": 21
  },
  "https://aclanthology.org/P19-3004": {
    "title": "SARAL: A Low-Resource Cross-Lingual Domain-Focused Information Retrieval System for Effective Rapid Document Triage",
    "abstract": "With the increasing democratization of electronic media, vast information resources are available in less-frequently-taught languages such as Swahili or Somali. That information, which may be crucially important and not available elsewhere, can be difficult for monolingual English speakers to effectively access. In this paper we present an end-to-end cross-lingual information retrieval (CLIR) and summarization system for low-resource languages that 1) enables English speakers to search foreign language repositories of text and audio using English queries, 2) summarizes the retrieved documents in English with respect to a particular information need, and 3) provides complete transcriptions and translations as needed. The SARAL system achieved the top end-to-end performance in the most recent IARPA MATERIAL CLIR+summarization evaluations. Our demonstration system provides end-to-end open query retrieval and summarization capability, and presents the original source text or audio, speech transcription, and machine translation, for two low resource languages",
    "volume": "demo",
    "checked": true,
    "id": "36526300cca6abedb83a9ef758a466031c1628a9",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-3005": {
    "title": "Jiuge: A Human-Machine Collaborative Chinese Classical Poetry Generation System",
    "abstract": "Research on the automatic generation of poetry, the treasure of human culture, has lasted for decades. Most existing systems, however, are merely model-oriented, which input some user-specified keywords and directly complete the generation process in one pass, with little user participation. We believe that the machine, being a collaborator or an assistant, should not replace human beings in poetic creation. Therefore, we proposed Jiuge, a human-machine collaborative Chinese classical poetry generation system. Unlike previous systems, Jiuge allows users to revise the unsatisfied parts of a generated poem draft repeatedly. According to the revision, the poem will be dynamically updated and regenerated. After the revision and modification procedure, the user can write a satisfying poem together with Jiuge system collaboratively. Besides, Jiuge can accept multi-modal inputs, such as keywords, plain text or images. By exposing the options of poetry genres, styles and revision modes, Jiuge, acting as a professional assistant, allows constant and active participation of users in poetic creation",
    "volume": "demo",
    "checked": true,
    "id": "5a6da697099dae0e0002b830b9988b45c85a6a58",
    "citation_count": 26
  },
  "https://aclanthology.org/P19-3006": {
    "title": "Rapid Customization for Event Extraction",
    "abstract": "Extracting events in the form of who is involved in what at when and where from text, is one of the core information extraction tasks that has many applications such as web search and question answering. We present a system for rapidly customizing event extraction capability to find new event types (what happened) and their arguments (who, when, and where). To enable extracting events of new types, we develop a novel approach to allow a user to find, expand and filter event triggers by exploring an unannotated development corpus. The system will then generate mention level event annotation automatically and train a neural network model for finding the corresponding events. To enable extracting arguments for new event types, the system makes novel use of the ACE annotation dataset to train a generic argument attachment model for extracting Actor, Place, and Time. We demonstrate that with less than 10 minutes of human effort per event type, the system achieves good performance for 67 novel event types. Experiments also show that the generic argument attachment model performs well on the novel event types. Our system (code, UI, documentation, demonstration video) is released as open source",
    "volume": "demo",
    "checked": true,
    "id": "dcf24ed256d5fc8f56878ee03efb8fadad48918e",
    "citation_count": 19
  },
  "https://aclanthology.org/P19-3007": {
    "title": "A Multiscale Visualization of Attention in the Transformer Model",
    "abstract": "The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior",
    "volume": "demo",
    "checked": true,
    "id": "0de0a44b859a3719d11834479112314b4caba669",
    "citation_count": 320
  },
  "https://aclanthology.org/P19-3008": {
    "title": "PostAc : A Visual Interactive Search, Exploration, and Analysis Platform for PhD Intensive Job Postings",
    "abstract": "Over 60% of Australian PhD graduates land their first job after graduation outside academia, but this job market remains largely hidden to these job seekers. Employers’ low awareness and interest in attracting PhD graduates means that the term “PhD” is rarely used as a keyword in job advertisements; 80% of companies looking to employ similar researchers do not specifically ask for a PhD qualification. As a result, typing in “PhD” to a job search engine tends to return mostly academic jobs. We set out to make the market for advanced research skills more visible to job seekers. In this paper, we present PostAc, an online platform of authentic job postings that helps PhD graduates sharpen their career thinking. The platform is underpinned by research on the key factors that identify what an employer is looking for when they want to hire a highly skilled researcher. Its ranking model leverages the free-form text embedded in the job description to quantify the most sought-after PhD skills and educate information seekers about the Australian job-market appetite for PhD skills. The platform makes visible the geographic location, industry sector, job title, working hours, continuity, and wage of the research intensive jobs. This is the first data-driven exploration in this field. Both empirical results and online platform will be presented in this paper",
    "volume": "demo",
    "checked": true,
    "id": "144e597ae4aeef5dc65f72b3937135f3d3d8245f",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-3009": {
    "title": "An adaptable task-oriented dialog system for stand-alone embedded devices",
    "abstract": "This paper describes a spoken-language end-to-end task-oriented dialogue system for small embedded devices such as home appliances. While the current system implements a smart alarm clock with advanced calendar scheduling functionality, the system is designed to make it easy to port to other application domains (e.g., the dialogue component factors out domain-specific execution from domain-general actions such as requesting and updating slot values). The system does not require internet connectivity because all components, including speech recognition, natural language understanding, dialogue management, execution and text-to-speech, run locally on the embedded device (our demo uses a Raspberry Pi). This simplifies deployment, minimizes server costs and most importantly, eliminates user privacy risks. The demo video in alarm domain is here youtu.be/N3IBMGocvHU",
    "volume": "demo",
    "checked": true,
    "id": "ea2ad7e330070aed3e909a2e263ae88e320663b3",
    "citation_count": 4
  },
  "https://aclanthology.org/P19-3010": {
    "title": "AlpacaTag: An Active Learning-based Crowd Annotation Framework for Sequence Tagging",
    "abstract": "We introduce an open-source web-based data annotation framework (AlpacaTag) for sequence tagging tasks such as named-entity recognition (NER). The distinctive advantages of AlpacaTag are three-fold. 1) Active intelligent recommendation: dynamically suggesting annotations and sampling the most informative unlabeled instances with a back-end active learned model; 2) Automatic crowd consolidation: enhancing real-time inter-annotator agreement by merging inconsistent labels from multiple annotators; 3) Real-time model deployment: users can deploy their models in downstream systems while new annotations are being made. AlpacaTag is a comprehensive solution for sequence labeling tasks, ranging from rapid tagging with recommendations powered by active learning and auto-consolidation of crowd annotations to real-time model deployment",
    "volume": "demo",
    "checked": true,
    "id": "0acbdcac9edf74cc2c1e98bd59e301c9300977d0",
    "citation_count": 21
  },
  "https://aclanthology.org/P19-3011": {
    "title": "ConvLab: Multi-Domain End-to-End Dialog System Platform",
    "abstract": "We present ConvLab, an open-source multi-domain end-to-end dialog system platform, that enables researchers to quickly set up experiments with reusable components and compare a large set of different approaches, ranging from conventional pipeline systems to end-to-end neural models, in common environments. ConvLab offers a set of fully annotated datasets and associated pre-trained reference models. As a showcase, we extend the MultiWOZ dataset with user dialog act annotations to train all component models and demonstrate how ConvLab makes it easy and effortless to conduct complicated experiments in multi-domain end-to-end dialog settings",
    "volume": "demo",
    "checked": true,
    "id": "0654b7fa0a25356ad1638844c9aa887e801d3c93",
    "citation_count": 83
  },
  "https://aclanthology.org/P19-3012": {
    "title": "Demonstration of a Neural Machine Translation System with Online Learning for Translators",
    "abstract": "We present a demonstration of our system, which implements online learning for neural machine translation in a production environment. These techniques allow the system to continuously learn from the corrections provided by the translators. We implemented an end-to-end platform integrating our machine translation servers to one of the most common user interfaces for professional translators: SDL Trados Studio. We pretend to save post-editing effort as the machine is continuously learning from its mistakes and adapting the models to a specific domain or user style",
    "volume": "demo",
    "checked": true,
    "id": "137787a92eca254910f3ce05262d31364f890e97",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-3013": {
    "title": "FASTDial: Abstracting Dialogue Policies for Fast Development of Task Oriented Agents",
    "abstract": "We present a novel abstraction framework called FASTDial for designing task oriented dialogue agents, built on top of the OpenDial toolkit. This framework is meant to facilitate prototyping and development of dialogue systems from scratch also by non tech savvy especially when limited training data is available. To this end, we use a generic and simple frame-slots data-structure with pre-defined dialogue policies that allows for fast design and implementation at the price of some flexibility reduction. Moreover, it allows for minimizing programming effort and domain expert training time, by hiding away many implementation details. We provide a system demonstration screencast video in the following link: https://vimeo.com/329840716",
    "volume": "demo",
    "checked": true,
    "id": "de05b957b37f4c7d995313691480794a1303238f",
    "citation_count": 0
  },
  "https://aclanthology.org/P19-3014": {
    "title": "A Neural, Interactive-predictive System for Multimodal Sequence to Sequence Tasks",
    "abstract": "We present a demonstration of a neural interactive-predictive system for tackling multimodal sequence to sequence tasks. The system generates text predictions to different sequence to sequence tasks: machine translation, image and video captioning. These predictions are revised by a human agent, who introduces corrections in the form of characters. The system reacts to each correction, providing alternative hypotheses, compelling with the feedback provided by the user. The final objective is to reduce the human effort required during this correction process. This system is implemented following a client-server architecture. For accessing the system, we developed a website, which communicates with the neural model, hosted in a local server. From this website, the different tasks can be tackled following the interactive–predictive framework. We open-source all the code developed for building this system. The demonstration in hosted in http://casmacat.prhlt.upv.es/interactive-seq2seq",
    "volume": "demo",
    "checked": true,
    "id": "0997497ee345083379e320233415f1f388f6aa25",
    "citation_count": 4
  },
  "https://aclanthology.org/P19-3015": {
    "title": "NeuralClassifier: An Open-source Neural Hierarchical Multi-label Text Classification Toolkit",
    "abstract": "In this paper, we introduce NeuralClassifier, a toolkit for neural hierarchical multi-label text classification. NeuralClassifier is designed for quick implementation of neural models for hierarchical multi-label classification task, which is more challenging and common in real-world scenarios. A salient feature is that NeuralClassifier currently provides a variety of text encoders, such as FastText, TextCNN, TextRNN, RCNN, VDCNN, DPCNN, DRNN, AttentiveConvNet and Transformer encoder, etc. It also supports other text classification scenarios, including binary-class and multi-class classification. Built on PyTorch, the core operations are calculated in batch, making the toolkit efficient with the acceleration of GPU. Experiments show that models built in our toolkit achieve comparable performance with reported results in the literature",
    "volume": "demo",
    "checked": true,
    "id": "83e5da76e30ca783cf5888752a3dd9076c1f9cc2",
    "citation_count": 21
  },
  "https://aclanthology.org/P19-3016": {
    "title": "ADVISER: A Dialog System Framework for Education & Research",
    "abstract": "In this paper, we present ADVISER - an open source dialog system framework for education and research purposes. This system supports multi-domain task-oriented conversations in two languages. It additionally provides a flexible architecture in which modules can be arbitrarily combined or exchanged - allowing for easy switching between rules-based and neural network based implementations. Furthermore, ADVISER offers a transparent, user-friendly framework designed for interdisciplinary collaboration: from a flexible back end, allowing easy integration of new features, to an intuitive graphical user interface supporting nontechnical users",
    "volume": "demo",
    "checked": true,
    "id": "4a9c79b4bdaa44187af4a511d9baffa87740a429",
    "citation_count": 3
  },
  "https://aclanthology.org/P19-3017": {
    "title": "KCAT: A Knowledge-Constraint Typing Annotation Tool",
    "abstract": "In this paper, we propose an efficient Knowledge Constraint Fine-grained Entity Typing Annotation Tool, which further improves the entity typing process through entity linking together with some practical functions",
    "volume": "demo",
    "checked": true,
    "id": "18b0cb949ffd9205ac73035a234d668e6bc8ce8f",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-3018": {
    "title": "An Environment for Relational Annotation of Political Debates",
    "abstract": "This paper describes the MARDY corpus annotation environment developed for a collaboration between political science and computational linguistics. The tool realizes the complete workflow necessary for annotating a large newspaper text collection with rich information about claims (demands) raised by politicians and other actors, including claim and actor spans, relations, and polarities. In addition to the annotation GUI, the tool supports the identification of relevant documents, text pre-processing, user management, integration of external knowledge bases, annotation comparison and merging, statistical analysis, and the incorporation of machine learning models as “pseudo-annotators”",
    "volume": "demo",
    "checked": true,
    "id": "f9cabdb068d2a9f2088dc3a92eab79b7a5fa729a",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-3019": {
    "title": "GLTR: Statistical Detection and Visualization of Generated Text",
    "abstract": "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs",
    "volume": "demo",
    "checked": true,
    "id": "867db5097ad6aaef098c60b0845785b440eca49a",
    "citation_count": 111
  },
  "https://aclanthology.org/P19-3020": {
    "title": "OpenKiwi: An Open Source Framework for Quality Estimation",
    "abstract": "We introduce OpenKiwi, a Pytorch-based open source framework for translation quality estimation. OpenKiwi supports training and testing of word-level and sentence-level quality estimation systems, implementing the winning systems of the WMT 2015–18 quality estimation campaigns. We benchmark OpenKiwi on two datasets from WMT 2018 (English-German SMT and NMT), yielding state-of-the-art performance on the word-level tasks and near state-of-the-art in the sentence-level tasks",
    "volume": "demo",
    "checked": true,
    "id": "95bcee172f94c7ce7201d3cc3d221ec4a4a9d706",
    "citation_count": 89
  },
  "https://aclanthology.org/P19-3021": {
    "title": "Microsoft Icecaps: An Open-Source Toolkit for Conversation Modeling",
    "abstract": "The Intelligent Conversation Engine: Code and Pre-trained Systems (Microsoft Icecaps) is an upcoming open-source natural language processing repository. Icecaps wraps TensorFlow functionality in a modular component-based architecture, presenting an intuitive and flexible paradigm for constructing sophisticated learning setups. Capabilities include multitask learning between models with shared parameters, upgraded language model decoding features, a range of built-in architectures, and a user-friendly data processing pipeline. The system is targeted toward conversational tasks, exploring diverse response generation, coherence, and knowledge grounding. Icecaps also provides pre-trained conversational models that can be either used directly or loaded for fine-tuning or bootstrapping other models; these models power an online demo of our framework",
    "volume": "demo",
    "checked": true,
    "id": "898841b9f005ce3fe82c070d9be123251d1fddd3",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-3022": {
    "title": "PerspectroScope: A Window to the World of Diverse Perspectives",
    "abstract": "This work presents PerspectroScope, a web-based system which lets users query a discussion-worthy natural language claim, and extract and visualize various perspectives in support or against the claim, along with evidence supporting each perspective. The system thus lets users explore various perspectives that could touch upon aspects of the issue at hand.The system is built as a combination of retrieval engines and learned textual-entailment-like classifiers built using a few recent developments in natural language understanding. To make the system more adaptive, expand its coverage, and improve its decisions over time, our platform employs various mechanisms to get corrections from the users. PerspectroScope is available at github.com/CogComp/perspectroscope Web demo link: http://orwell.seas.upenn.edu:4002/ Link to demo video: https://www.youtube.com/watch?v=MXBTR1Sp3Bs",
    "volume": "demo",
    "checked": true,
    "id": "c4aa27cb03eabc1d9675671ed7be5eafc1408653",
    "citation_count": 10
  },
  "https://aclanthology.org/P19-3023": {
    "title": "HEIDL: Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop",
    "abstract": "While the role of humans is increasingly recognized in machine learning community, representation of and interaction with models in current human-in-the-loop machine learning (HITL-ML) approaches are too low-level and far-removed from human’s conceptual models. We demonstrate HEIDL, a prototype HITL-ML system that exposes the machine-learned model through high-level, explainable linguistic expressions formed of predicates representing semantic structure of text. In HEIDL, human’s role is elevated from simply evaluating model predictions to interpreting and even updating the model logic directly by enabling interaction with rule predicates themselves. Raising the currency of interaction to such semantic levels calls for new interaction paradigms between humans and machines that result in improved productivity for text analytics model development process. Moreover, by involving humans in the process, the human-machine co-created models generalize better to unseen data as domain experts are able to instill their expertise by extrapolating from what has been learned by automated algorithms from few labelled data",
    "volume": "demo",
    "checked": true,
    "id": "3bb3a018bf8c0defd7eec3d98001aa9103fcb24a",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-3024": {
    "title": "My Turn To Read: An Interleaved E-book Reading Tool for Developing and Struggling Readers",
    "abstract": "Literacy is crucial for functioning in modern society. It underpins everything from educational attainment and employment opportunities to health outcomes. We describe My Turn To Read, an app that uses interleaved reading to help developing and struggling readers improve reading skills while reading for meaning and pleasure. We hypothesize that the longer-term impact of the app will be to help users become better, more confident readers with an increased stamina for extended reading. We describe the technology and present preliminary evidence in support of this hypothesis",
    "volume": "demo",
    "checked": true,
    "id": "8a4bde0d03ee3d65c25f48e1cb56916a557865e1",
    "citation_count": 4
  },
  "https://aclanthology.org/P19-3025": {
    "title": "GrapAL: Connecting the Dots in Scientific Literature",
    "abstract": "We introduce GrapAL (Graph database of Academic Literature), a versatile tool for exploring and investigating a knowledge base of scientific literature that was semi-automatically constructed using NLP methods. GrapAL fills many informational needs expressed by researchers. At the core of GrapAL is a Neo4j graph database with an intuitive schema and a simple query language. In this paper, we describe the basic elements of GrapAL, how to use it, and several use cases such as finding experts on a given topic for peer reviewing, discovering indirect connections between biomedical entities, and computing citation-based metrics. We open source the demo code to help other researchers develop applications that build on GrapAL",
    "volume": "demo",
    "checked": true,
    "id": "0cc1befaf72ca96c2a5034d004132da1b7bbc58b",
    "citation_count": 10
  },
  "https://aclanthology.org/P19-3026": {
    "title": "ClaimPortal: Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on Twitter",
    "abstract": "We present ClaimPortal, a web-based platform for monitoring, searching, checking, and analyzing English factual claims on Twitter from the American political domain. We explain the architecture of ClaimPortal, its components and functions, and the user interface. While the last several years have witnessed a substantial growth in interests and efforts in the area of computational fact-checking, ClaimPortal is a novel infrastructure in that fact-checkers have largely skipped factual claims in tweets. It can be a highly powerful tool to both general web users and fact-checkers. It will also be an educational resource in helping cultivate a society that is less susceptible to falsehoods",
    "volume": "demo",
    "checked": true,
    "id": "16c7a8fff344033e8f0d3abb0f047ff4af89f01a",
    "citation_count": 9
  },
  "https://aclanthology.org/P19-3027": {
    "title": "Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation",
    "abstract": "We introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks that transform any inputs into natural language, such as machine translation, summarization, dialog, content manipulation, and so forth. With the design goals of modularity, versatility, and extensibility in mind, Texar extracts common patterns underlying the diverse tasks and methodologies, creates a library of highly reusable modules and functionalities, and allows arbitrary model architectures and algorithmic paradigms. In Texar, model architecture, inference, and learning processes are properly decomposed. Modules at a high concept level can be freely assembled or plugged in/swapped out. Texar is thus particularly suitable for researchers and practitioners to do fast prototyping and experimentation. The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https://www.texar.io",
    "volume": "demo",
    "checked": true,
    "id": "75f90cbbf3c27a8b27567d6a9c8c4538743c8fff",
    "citation_count": 37
  },
  "https://aclanthology.org/P19-3028": {
    "title": "Parallax: Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae",
    "abstract": "Embeddings are a fundamental component of many modern machine learning and natural language processing models. Understanding them and visualizing them is essential for gathering insights about the information they capture and the behavior of the models. In this paper, we introduce Parallax, a tool explicitly designed for this task. Parallax allows the user to use both state-of-the-art embedding analysis methods (PCA and t-SNE) and a simple yet effective task-oriented approach where users can explicitly define the axes of the projection through algebraic formulae. %consists in projecting them in two-dimensional planes without any interpretable semantics associated to the axes of the projection, which makes detailed analyses and comparison among multiple sets of embeddings challenging. In this approach, embeddings are projected into a semantically meaningful subspace, which enhances interpretability and allows for more fine-grained analysis. We demonstrate the power of the tool and the proposed methodology through a series of case studies and a user study",
    "volume": "demo",
    "checked": true,
    "id": "9767b1d458495e80cb0bd04d8366d680d43bdbf9",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-3029": {
    "title": "Flambé: A Customizable Framework for Machine Learning Experiments",
    "abstract": "Flambé is a machine learning experimentation framework built to accelerate the entire research life cycle. Flambé’s main objective is to provide a unified interface for prototyping models, running experiments containing complex pipelines, monitoring those experiments in real-time, reporting results, and deploying a final model for inference. Flambé achieves both flexibility and simplicity by allowing users to write custom code but instantly include that code as a component in a larger system which is represented by a concise configuration file format. We demonstrate the application of the framework through a cutting-edge multistage use case: fine-tuning and distillation of a state of the art pretrained language model used for text classification",
    "volume": "demo",
    "checked": true,
    "id": "bfa3c693e5a0f7e0bce562eb400e63d02ac991e7",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-3030": {
    "title": "A Modular Tool for Automatic Summarization",
    "abstract": "This paper introduces the first fine-grained modular tool for automatic summarization. Open source and written in Java, it is designed to be as straightforward as possible for end-users. Its modular architecture is meant to ease its maintenance and the development and integration of new modules. We hope that it will ease the work of researchers in automatic summarization by providing a reliable baseline for future works as well as an easy way to evaluate methods on different corpora",
    "volume": "demo",
    "checked": true,
    "id": "6d51d8a0f5ad5ac1dce9afd321aee2d85349c5b0",
    "citation_count": 1
  },
  "https://aclanthology.org/P19-3031": {
    "title": "TARGER: Neural Argument Mining at Your Fingertips",
    "abstract": "We present TARGER, an open source neural argument mining framework for tagging arguments in free input texts and for keyword-based retrieval of arguments from an argument-tagged web-scale corpus. The currently available models are pre-trained on three recent argument mining datasets and enable the use of neural argument mining without any reproducibility effort on the user’s side. The open source code ensures portability to other domains and use cases",
    "volume": "demo",
    "checked": true,
    "id": "310a9381b35a6e3187a5a0ff3ec50f65cc0211f2",
    "citation_count": 63
  },
  "https://aclanthology.org/P19-3032": {
    "title": "MoNoise: A Multi-lingual and Easy-to-use Lexical Normalization Tool",
    "abstract": "In this paper, we introduce and demonstrate the online demo as well as the command line interface of a lexical normalization system (MoNoise) for a variety of languages. We further improve this model by using features from the original word for every normalization candidate. For comparison with future work, we propose the bundling of seven datasets in six languages to form a new benchmark, together with a novel evaluation metric which is particularly suitable for cross-dataset comparisons. MoNoise reaches a new state-of-art performance for six out of seven of these datasets. Furthermore, we allow the user to tune the ‘aggressiveness’ of the normalization, and show how the model can be made more efficient with only a small loss in performance. The online demo can be found on: http://www.robvandergoot.com/monoise and the corresponding code on: https://bitbucket.org/robvanderg/monoise/",
    "volume": "demo",
    "checked": true,
    "id": "28f966be276910ffbad55639862c7baf33c54d91",
    "citation_count": 19
  },
  "https://aclanthology.org/P19-3033": {
    "title": "Level-Up: Learning to Improve Proficiency Level of Essays",
    "abstract": "We introduce a method for generating suggestions on a given sentence for improving the proficiency level. In our approach, the sentence is transformed into a sequence of grammatical elements aimed at providing suggestions of more advanced grammar elements based on originals. The method involves parsing the sentence, identifying grammatical elements, and ranking related elements to recommend a higher level of grammatical element. We present a prototype tutoring system, Level-Up, that applies the method to English learners’ essays in order to assist them in writing and reading. Evaluation on a set of essays shows that our method does assist user in writing",
    "volume": "demo",
    "checked": true,
    "id": "2050b14d503c7f41c347457a46103ae6e74cf9e1",
    "citation_count": 0
  },
  "https://aclanthology.org/P19-3034": {
    "title": "Learning to Link Grammar and Encyclopedic Information of Assist ESL Learners",
    "abstract": "We introduce a system aimed at improving and expanding second language learners’ English vocabulary. In addition to word definitions, we provide rich lexical information such as collocations and grammar patterns for target words. We present Linggle Booster that takes an article, identifies target vocabulary, provides lexical information, and generates a quiz on target words. Linggle Booster also links named-entity to corresponding Wikipedia pages. Evaluation on a set of target words shows that the method have reasonably good performance in terms of generating useful and information for learning vocabulary",
    "volume": "demo",
    "checked": true,
    "id": "ebcb425ed1a51e8f1a6eca422882abd454fe04f2",
    "citation_count": 0
  },
  "https://aclanthology.org/P19-4001": {
    "title": "Latent Structure Models for Natural Language Processing",
    "abstract": "Latent structure models are a powerful tool for modeling compositional data, discovering linguistic structure, and building NLP pipelines. They are appealing for two main reasons: they allow incorporating structural bias during training, leading to more accurate models; and they allow discovering hidden linguistic structure, which provides better interpretability. This tutorial will cover recent advances in discrete latent structure models. We discuss their motivation, potential, and limitations, then explore in detail three strategies for designing such models: gradient approximation, reinforcement learning, and end-to-end differentiable methods. We highlight connections among all these methods, enumerating their strengths and weaknesses. The models we present and analyze have been applied to a wide variety of NLP tasks, including sentiment analysis, natural language inference, language modeling, machine translation, and semantic parsing. Examples and evaluation will be covered throughout. After attending the tutorial, a practitioner will be better informed about which method is best suited for their problem",
    "volume": "tutorial",
    "checked": true,
    "id": "a8abe785f46ebdeec6768ab741162830ba66b4ec",
    "citation_count": 7
  },
  "https://aclanthology.org/P19-4002": {
    "title": "Graph-Based Meaning Representations: Design and Processing",
    "abstract": "This tutorial is on representing and processing sentence meaning in the form of labeled directed graphs. The tutorial will (a) briefly review relevant background in formal and linguistic semantics; (b) semi-formally define a unified abstract view on different flavors of semantic graphs and associated terminology; (c) survey common frameworks for graph-based meaning representation and available graph banks; and (d) offer a technical overview of a representative selection of different parsing approaches",
    "volume": "tutorial",
    "checked": true,
    "id": "5fd6339f299304a7541c805c5ee443fbfb0bac3c",
    "citation_count": 24
  },
  "https://aclanthology.org/P19-4003": {
    "title": "Discourse Analysis and Its Applications",
    "abstract": "Discourse processing is a suite of Natural Language Processing (NLP) tasks to uncover linguistic structures from texts at several levels, which can support many downstream applications. This involves identifying the topic structure, the coherence structure, the coreference structure, and the conversation structure for conversational discourse. Taken together, these structures can inform text summarization, machine translation, essay scoring, sentiment analysis, information extraction, question answering, and thread recovery. The tutorial starts with an overview of basic concepts in discourse analysis – monologue vs. conversation, synchronous vs. asynchronous conversation, and key linguistic structures in discourse analysis. We also give an overview of linguistic structures and corresponding discourse analysis tasks that discourse researchers are generally interested in, as well as key applications on which these discourse structures have an impact",
    "volume": "tutorial",
    "checked": true,
    "id": "b478a2136ff93437ad293f8d73d3cc6b45203f57",
    "citation_count": 17
  },
  "https://aclanthology.org/P19-4004": {
    "title": "Computational Analysis of Political Texts: Bridging Research Efforts Across Communities",
    "abstract": "In the last twenty years, political scientists started adopting and developing natural language processing (NLP) methods more actively in order to exploit text as an additional source of data in their analyses. Over the last decade the usage of computational methods for analysis of political texts has drastically expanded in scope, allowing for a sustained growth of the text-as-data community in political science. In political science, NLP methods have been extensively used for a number of analyses types and tasks, including inferring policy position of actors from textual evidence, detecting topics in political texts, and analyzing stylistic aspects of political texts (e.g., assessing the role of language ambiguity in framing the political agenda). Just like in numerous other domains, much of the work on computational analysis of political texts has been enabled and facilitated by the development of resources such as, the topically coded electoral programmes (e.g., the Manifesto Corpus) or topically coded legislative texts (e.g., the Comparative Agenda Project). Political scientists created resources and used available NLP methods to process textual data largely in isolation from the NLP community. At the same time, NLP researchers addressed closely related tasks such as election prediction, ideology classification, and stance detection. In other words, these two communities have been largely agnostic of one another, with NLP researchers mostly unaware of interesting applications in political science and political scientists not applying cutting-edge NLP methodology to their problems. The main goal of this tutorial is to systematize and analyze the body of research work on political texts from both communities. We aim to provide a gentle, all-round introduction to methods and tasks related to computational analysis of political texts. Our vision is to bring the two research communities closer to each other and contribute to faster and more significant developments in this interdisciplinary research area",
    "volume": "tutorial",
    "checked": true,
    "id": "4dea8e7bbc8f59b0307879121f5f8eab0848dd06",
    "citation_count": 11
  },
  "https://aclanthology.org/P19-4005": {
    "title": "Wikipedia as a Resource for Text Analysis and Retrieval",
    "abstract": "This tutorial examines the role of Wikipedia in tasks related to text analysis and retrieval. Text analysis tasks, which take advantage of Wikipedia, include coreference resolution, word sense and entity disambiguation and information extraction. In information retrieval, a better understanding of the structure and meaning of queries helps in matching queries against documents, clustering search results, answer and entity retrieval and retrieving knowledge panels for queries asking about popular entities",
    "volume": "tutorial",
    "checked": true,
    "id": "ceaadd41b7ac98b69acb2c4c12d896789a200d9d",
    "citation_count": 0
  },
  "https://aclanthology.org/P19-4006": {
    "title": "Deep Bayesian Natural Language Processing",
    "abstract": "This introductory tutorial addresses the advances in deep Bayesian learning for natural language with ubiquitous applications ranging from speech recognition to document summarization, text classification, text segmentation, information extraction, image caption generation, sentence generation, dialogue control, sentiment classification, recommendation system, question answering and machine translation, to name a few. Traditionally, “deep learning” is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The “semantic structure” in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The “distribution function” in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process, Chinese restaurant process, hierarchical Pitman-Yor process, Indian buffet process, recurrent neural network, long short-term memory, sequence-to-sequence model, variational auto-encoder, generative adversarial network, attention mechanism, memory-augmented neural network, skip neural network, stochastic neural network, predictive state neural network and policy neural network. We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language. The variational inference and sampling method are formulated to tackle the optimization for complicated models. The word and sentence embeddings, clustering and co-clustering are merged with linguistic and semantic constraints. A series of case studies and domain applications are presented to tackle different issues in deep Bayesian processing, learning and understanding. At last, we will point out a number of directions and outlooks for future studies",
    "volume": "tutorial",
    "checked": true,
    "id": "d9287c88e6a1b47a10828d5eb0130fd5c2c4514b",
    "citation_count": 26
  },
  "https://aclanthology.org/P19-4007": {
    "title": "Unsupervised Cross-Lingual Representation Learning",
    "abstract": "In this tutorial, we provide a comprehensive survey of the exciting recent work on cutting-edge weakly-supervised and unsupervised cross-lingual word representations. After providing a brief history of supervised cross-lingual word representations, we focus on: 1) how to induce weakly-supervised and unsupervised cross-lingual word representations in truly resource-poor settings where bilingual supervision cannot be guaranteed; 2) critical examinations of different training conditions and requirements under which unsupervised algorithms can and cannot work effectively; 3) more robust methods for distant language pairs that can mitigate instability issues and low performance for distant language pairs; 4) how to comprehensively evaluate such representations; and 5) diverse applications that benefit from cross-lingual word representations (e.g., MT, dialogue, cross-lingual sequence labeling and structured prediction applications, cross-lingual IR)",
    "volume": "tutorial",
    "checked": true,
    "id": "b21b96671cbf85f7cdf1692ac3ddc9a30efce951",
    "citation_count": 85
  },
  "https://aclanthology.org/P19-4008": {
    "title": "Advances in Argument Mining",
    "abstract": "This course aims to introduce students to an exciting and dynamic area that has witnessed remarkable growth over the past 36 months. Argument mining builds on opinion mining, sentiment analysis and related to tasks to automatically extract not just *what* people think, but *why* they hold the opinions they do. From being largely beyond the state of the art barely five years ago, there are now many hundreds of papers on the topic, millions of dollars of commercial and research investment, and the 6th ACL workshop on the topic will be in Florence in 2019. The tutors have delivered tutorials on argument mining at ACL 2016, at IJCAI 2016 and at ESSLLI 2017; for ACL 2019, we have developed a tutorial that provides a synthesis of the major advances in the area over the past three years",
    "volume": "tutorial",
    "checked": true,
    "id": "c5f49a7dfbc115c381ab96abf30fa63fafa59274",
    "citation_count": 4
  },
  "https://aclanthology.org/P19-4009": {
    "title": "Storytelling from Structured Data and Knowledge Graphs : An NLG Perspective",
    "abstract": "In this tutorial, we wish to cover the foundational, methodological, and system development aspects of translating structured data (such as data in tabular form) and knowledge bases (such as knowledge graphs) into natural language. The attendees of the tutorial will be able to take away from this tutorial, (1) the basic ideas around how modern NLP and NLG techniques could be applied to describe and summarize textual data in format that is non-linguistic in nature or has some structure, and (2) a few interesting open-ended questions, which could lead to significant research contributions in future. The tutorial aims to convey challenges and nuances in structured data translation, data representation techniques, and domain adaptable solutions for translation of the data into natural language form. Various solutions, starting from traditional rule based/heuristic driven and modern data-driven and ultra-modern deep-neural style architectures will be discussed, followed by a brief discussion on evaluation and quality estimation. A significant portion of the tutorial will be dedicated towards unsupervised, scalable, and adaptable solutions, given that systems for such an important task will never naturally enjoy sustainable large scale domain independent labeled (parallel) data",
    "volume": "tutorial",
    "checked": true,
    "id": "d6b531782d2a00ba859a5bce1acfff4680915c7e",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-3201": {
    "title": "Extracting Kinship from Obituary to Enhance Electronic Health Records for Genetic Research",
    "abstract": "Claims database and electronic health records database do not usually capture kinship or family relationship information, which is imperative for genetic research. We identify online obituaries as a new data source and propose a special named entity recognition and relation extraction solution to extract names and kinships from online obituaries. Built on 1,809 annotated obituaries and a novel tagging scheme, our joint neural model achieved macro-averaged precision, recall and F measure of 72.69%, 78.54% and 74.93%, and micro-averaged precision, recall and F measure of 95.74%, 98.25% and 96.98% using 57 kinships with 10 or more examples in a 10-fold cross-validation experiment. The model performance improved dramatically when trained with 34 kinships with 50 or more examples. Leveraging additional information such as age, death date, birth date and residence mentioned by obituaries, we foresee a promising future of supplementing EHR databases with comprehensive and accurate kinship information for genetic research",
    "volume": "workshop",
    "checked": true,
    "id": "2c75545d594e5544a1538aa05eb641e91e1bf45f",
    "citation_count": 11
  },
  "https://aclanthology.org/W19-3202": {
    "title": "Lexical Normalization of User-Generated Medical Text",
    "abstract": "In the medical domain, user-generated social media text is increasingly used as a valuable complementary knowledge source to scientific medical literature. The extraction of this knowledge is complicated by colloquial language use and misspellings. Yet, lexical normalization of such data has not been addressed properly. This paper presents an unsupervised, data-driven spelling correction module for medical social media. Our method outperforms state-of-the-art spelling correction and can detect mistakes with an F0.5 of 0.888. Additionally, we present a novel corpus for spelling mistake detection and correction on a medical patient forum",
    "volume": "workshop",
    "checked": true,
    "id": "0ccedf9d1d996627a914ff1037d3aadb957b2c09",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-3203": {
    "title": "Overview of the Fourth Social Media Mining for Health (SMM4H) Shared Tasks at ACL 2019",
    "abstract": "The number of users of social media continues to grow, with nearly half of adults worldwide and two-thirds of all American adults using social networking. Advances in automated data processing, machine learning and NLP present the possibility of utilizing this massive data source for biomedical and public health applications, if researchers address the methodological challenges unique to this media. We present the Social Media Mining for Health Shared Tasks collocated with the ACL at Florence in 2019, which address these challenges for health monitoring and surveillance, utilizing state of the art techniques for processing noisy, real-world, and substantially creative language expressions from social media users. For the fourth execution of this challenge, we proposed four different tasks. Task 1 asked participants to distinguish tweets reporting an adverse drug reaction (ADR) from those that do not. Task 2, a follow-up to Task 1, asked participants to identify the span of text in tweets reporting ADRs. Task 3 is an end-to-end task where the goal was to first detect tweets mentioning an ADR and then map the extracted colloquial mentions of ADRs in the tweets to their corresponding standard concept IDs in the MedDRA vocabulary. Finally, Task 4 asked participants to classify whether a tweet contains a personal mention of one’s health, a more general discussion of the health issue, or is an unrelated mention. A total of 34 teams from around the world registered and 19 teams from 12 countries submitted a system run. We summarize here the corpora for this challenge which are freely available at https://competitions.codalab.org/competitions/22521, and present an overview of the methods and the results of the competing systems",
    "volume": "workshop",
    "checked": true,
    "id": "8dd234dde9c319174fcbe380164d0886d67bf92e",
    "citation_count": 75
  },
  "https://aclanthology.org/W19-3204": {
    "title": "MedNorm: A Corpus and Embeddings for Cross-terminology Medical Concept Normalisation",
    "abstract": "The medical concept normalisation task aims to map textual descriptions to standard terminologies such as SNOMED-CT or MedDRA. Existing publicly available datasets annotated using different terminologies cannot be simply merged and utilised, and therefore become less valuable when developing machine learning-based concept normalisation systems. To address that, we designed a data harmonisation pipeline and engineered a corpus of 27,979 textual descriptions simultaneously mapped to both MedDRA and SNOMED-CT, sourced from five publicly available datasets across biomedical and social media domains. The pipeline can be used in the future to integrate new datasets into the corpus and also could be applied in relevant data curation tasks. We also described a method to merge different terminologies into a single concept graph preserving their relations and demonstrated that representation learning approach based on random walks on a graph can efficiently encode both hierarchical and equivalent relations and capture semantic similarities not only between concepts inside a given terminology but also between concepts from different terminologies. We believe that making a corpus and embeddings for cross-terminology medical concept normalisation available to the research community would contribute to a better understanding of the task",
    "volume": "workshop",
    "checked": true,
    "id": "23e8d6a5b55046c8c11a8509a4abad29f05ea485",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3205": {
    "title": "Passive Diagnosis Incorporating the PHQ-4 for Depression and Anxiety",
    "abstract": "Depression and anxiety are the two most prevalent mental health disorders worldwide, impacting the lives of millions of people each year. In this work, we develop and evaluate a multilabel, multidimensional deep neural network designed to predict PHQ-4 scores based on individuals written text. Our system outperforms random baseline metrics and provides a novel approach to how we can predict psychometric scores from written text. Additionally, we explore how this architecture can be applied to analyse social media data",
    "volume": "workshop",
    "checked": true,
    "id": "be321ca71124629315e1fce8ec0ae6e8ff19baa5",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3206": {
    "title": "HITSZ-ICRC: A Report for SMM4H Shared Task 2019-Automatic Classification and Extraction of Adverse Effect Mentions in Tweets",
    "abstract": "This is the system description of the Harbin Institute of Technology Shenzhen (HITSZ) team for the first and second subtasks of the fourth Social Media Mining for Health Applications (SMM4H) shared task in 2019. The two subtasks are automatic classification and extraction of adverse effect mentions in tweets. The systems for the two subtasks are based on bidirectional encoder representations from transformers (BERT), and achieves promising results. Among the systems we developed for subtask1, the best F1-score was 0.6457, for subtask2, the best relaxed F1-score and the best strict F1-score were 0.614 and 0.407 respectively. Our system ranks first among all systems on subtask1",
    "volume": "workshop",
    "checked": true,
    "id": "eca0b777498d258fa8b647db6735d1432ffaaa6f",
    "citation_count": 14
  },
  "https://aclanthology.org/W19-3207": {
    "title": "KFU NLP Team at SMM4H 2019 Tasks: Want to Extract Adverse Drugs Reactions from Tweets? BERT to The Rescue",
    "abstract": "This paper describes a system developed for the Social Media Mining for Health (SMM4H) 2019 shared tasks. Specifically, we participated in three tasks. The goals of the first two tasks are to classify whether a tweet contains mentions of adverse drug reactions (ADR) and extract these mentions, respectively. The objective of the third task is to build an end-to-end solution: first, detect ADR mentions and then map these entities to concepts in a controlled vocabulary. We investigate the use of a language representation model BERT trained to obtain semantic representations of social media texts. Our experiments on a dataset of user reviews showed that BERT is superior to state-of-the-art models based on recurrent neural networks. The BERT-based system for Task 1 obtained an F1 of 57.38%, with improvements up to +7.19% F1 over a score averaged across all 43 submissions. The ensemble of neural networks with a voting scheme for named entity recognition ranked first among 9 teams at the SMM4H 2019 Task 2 and obtained a relaxed F1 of 65.8%. The end-to-end model based on BERT for ADR normalization ranked first at the SMM4H 2019 Task 3 and obtained a relaxed F1 of 43.2%",
    "volume": "workshop",
    "checked": true,
    "id": "d1068ceb10f20fed47754269cda27a9f212da1b0",
    "citation_count": 21
  },
  "https://aclanthology.org/W19-3208": {
    "title": "Approaching SMM4H with Merged Models and Multi-task Learning",
    "abstract": "We describe our submissions to the 4th edition of the Social Media Mining for Health Applications (SMM4H) shared task. Our team (UZH) participated in two sub-tasks: Automatic classifications of adverse effects mentions in tweets (Task 1) and Generalizable identification of personal health experience mentions (Task 4). For our submissions, we exploited ensembles based on a pre-trained language representation with a neural transformer architecture (BERT) (Tasks 1 and 4) and a CNN-BiLSTM(-CRF) network within a multi-task learning scenario (Task 1). These systems are placed on top of a carefully crafted pipeline of domain-specific preprocessing steps",
    "volume": "workshop",
    "checked": true,
    "id": "1869f628409c629d607c1348998cd804409ae960",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-3209": {
    "title": "Identifying Adverse Drug Events Mentions in Tweets Using Attentive, Collocated, and Aggregated Medical Representation",
    "abstract": "Identifying mentions of medical concepts in social media is challenging because of high variability in free text. In this paper, we propose a novel neural network architecture, the Collocated LSTM with Attentive Pooling and Aggregated representation (CLAPA), that integrates a bidirectional LSTM model with attention and pooling strategy and utilizes the collocation information from training data to improve the representation of medical concepts. The collocation and aggregation layers improve the model performance on the task of identifying mentions of adverse drug events (ADE) in tweets. Using the dataset made available as part of the workshop shared task, we show that careful selection of neighborhood contexts can help uncover useful local information and improve the overall medical concept representation",
    "volume": "workshop",
    "checked": true,
    "id": "9141767653308bbfd1389d3e6a579c110c6f3362",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3210": {
    "title": "Correlating Twitter Language with Community-Level Health Outcomes",
    "abstract": "We study how language on social media is linked to mortal diseases such as atherosclerotic heart disease (AHD), diabetes and various types of cancer. Our proposed model leverages state-of-the-art sentence embeddings, followed by a regression model and clustering, without the need of additional labelled data. It allows to predict community-level medical outcomes from language, and thereby potentially translate these to the individual level. The method is applicable to a wide range of target variables and allows us to discover known and potentially novel correlations of medical outcomes with life-style aspects and other socioeconomic risk factors",
    "volume": "workshop",
    "checked": true,
    "id": "9ad6f24fe887cda854a07d386f17e557876b3604",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3211": {
    "title": "Affective Behaviour Analysis of On-line User Interactions: Are On-line Support Groups More Therapeutic than Twitter?",
    "abstract": "The increase in the prevalence of mental health problems has coincided with a growing popularity of health related social networking sites. Regardless of their therapeutic potential, on-line support groups (OSGs) can also have negative effects on patients. In this work we propose a novel methodology to automatically verify the presence of therapeutic factors in social networking websites by using Natural Language Processing (NLP) techniques. The methodology is evaluated on on-line asynchronous multi-party conversations collected from an OSG and Twitter. The results of the analysis indicate that therapeutic factors occur more frequently in OSG conversations than in Twitter conversations. Moreover, the analysis of OSG conversations reveals that the users of that platform are supportive, and interactions are likely to lead to the improvement of their emotional state. We believe that our method provides a stepping stone towards automatic analysis of emotional states of users of online platforms. Possible applications of the method include provision of guidelines that highlight potential implications of using such platforms on users' mental health, and/or support in the analysis of their impact on specific individuals",
    "volume": "workshop",
    "checked": true,
    "id": "4c62c84659e45afde520cc3d67fd783405106e51",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-3212": {
    "title": "Transfer Learning for Health-related Twitter Data",
    "abstract": "Transfer learning is promising for many NLP applications, especially in tasks with limited labeled data. This paper describes the methods developed by team TMRLeiden for the 2019 Social Media Mining for Health Applications (SMM4H) Shared Task. Our methods use state-of-the-art transfer learning methods to classify, extract and normalise adverse drug effects (ADRs) and to classify personal health mentions from health-related tweets. The code and fine-tuned models are publicly available",
    "volume": "workshop",
    "checked": true,
    "id": "84a97e7ce30751c83de2953ed2b658146ac164ab",
    "citation_count": 15
  },
  "https://aclanthology.org/W19-3213": {
    "title": "NLP@UNED at SMM4H 2019: Neural Networks Applied to Automatic Classifications of Adverse Effects Mentions in Tweets",
    "abstract": "This paper describes a system for automatically classifying adverse effects mentions in tweets developed for the task 1 at Social Media Mining for Health Applications (SMM4H) Shared Task 2019. We have developed a system based on LSTM neural networks inspired by the excellent results obtained by deep learning classifiers in the last edition of this task. The network is trained along with Twitter GloVe pre-trained word embeddings",
    "volume": "workshop",
    "checked": true,
    "id": "bff914857406a3dd856d36d3f4967c64c13fc264",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-3214": {
    "title": "Detecting and Extracting of Adverse Drug Reaction Mentioning Tweets with Multi-Head Self Attention",
    "abstract": "This paper describes our system for the first and second shared tasks of the fourth Social Media Mining for Health Applications (SMM4H) workshop. We enhance tweet representation with a language model and distinguish the importance of different words with Multi-Head Self-Attention. In addition, transfer learning is exploited to make up for the data shortage. Our system achieved competitive results on both tasks with an F1-score of 0.5718 for task 1 and 0.653 (overlap) / 0.357 (strict) for task 2",
    "volume": "workshop",
    "checked": true,
    "id": "9651dad9362ca7ce3f16c852f0462d45fc7755be",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-3215": {
    "title": "Deep Learning for Identification of Adverse Effect Mentions In Twitter Data",
    "abstract": "Social Media Mining for Health Applications (SMM4H) Adverse Effect Mentions Shared Task challenges participants to accurately identify spans of text within a tweet that correspond to Adverse Effects (AEs) resulting from medication usage (Weissenbacher et al., 2019). This task features a training data set of 2,367 tweets, in addition to a 1,000 tweet evaluation data set. The solution presented here features a bidirectional Long Short-term Memory Network (bi-LSTM) for the generation of character-level embeddings. It uses a second bi-LSTM trained on both character and token level embeddings to feed a Conditional Random Field (CRF) which provides the final classification. This paper further discusses the deep learning algorithms used in our solution",
    "volume": "workshop",
    "checked": true,
    "id": "9179abe4cb6c6de682b61786bcd589768b592f71",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3216": {
    "title": "Using Machine Learning and Deep Learning Methods to Find Mentions of Adverse Drug Reactions in Social Media",
    "abstract": "Over time the use of social networks is becoming very popular platforms for sharing health related information. Social Media Mining for Health Applications (SMM4H) provides tasks such as those described in this document to help manage information in the health domain. This document shows the first participation of the SINAI group. We study approaches based on machine learning and deep learning to extract adverse drug reaction mentions from Twitter. The results obtained in the tasks are encouraging, we are close to the average of all participants and even above in some cases",
    "volume": "workshop",
    "checked": true,
    "id": "47ba27ff507db9962948bc52b4c7e58a409e22ab",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-3217": {
    "title": "Towards Text Processing Pipelines to Identify Adverse Drug Events-related Tweets: University of Michigan @ SMM4H 2019 Task 1",
    "abstract": "We participated in Task 1 of the Social Media Mining for Health Applications (SMM4H) 2019 Shared Tasks on detecting mentions of adverse drug events (ADEs) in tweets. Our approach relied on a text processing pipeline for tweets, and training traditional machine learning and deep learning models. Our submitted runs performed above average for the task",
    "volume": "workshop",
    "checked": true,
    "id": "89e02f50c17e947a0827042c919661d5475f0b03",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-3218": {
    "title": "Neural Network to Identify Personal Health Experience Mention in Tweets Using BioBERT Embeddings",
    "abstract": "This paper describes the system developed by team ASU-NLP for the Social Media Mining for Health Applications(SMM4H) shared task 4. We extract feature embeddings from the BioBERT (Lee et al., 2019) model which has been fine-tuned on the training dataset and use that as inputs to a dense fully connected neural network. We achieve above average scores among the participant systems with the overall F1-score, accuracy, precision, recall as 0.8036, 0.8456, 0.9783, 0.6818 respectively",
    "volume": "workshop",
    "checked": true,
    "id": "fb0268c6c2a5337d42166d68f947b49c9dc62466",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-3219": {
    "title": "Give It a Shot: Few-shot Learning to Normalize ADR Mentions in Social Media Posts",
    "abstract": "This paper describes the system that team MYTOMORROWS-TU DELFT developed for the 2019 Social Media Mining for Health Applications (SMM4H) Shared Task 3, for the end-to-end normalization of ADR tweet mentions to their corresponding MEDDRA codes. For the first two steps, we reuse a state-of-the art approach, focusing our contribution on the final entity-linking step. For that we propose a simple Few-Shot learning approach, based on pre-trained word embeddings and data from the UMLS, combined with the provided training data. Our system (relaxed F1: 0.337-0.345) outperforms the average (relaxed F1 0.2972) of the participants in this task, demonstrating the potential feasibility of few-shot learning in the context of medical text normalization",
    "volume": "workshop",
    "checked": true,
    "id": "68e02fb968f31c4dadd4c7d62e46edbfa2e52ccd",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3220": {
    "title": "BIGODM System in the Social Media Mining for Health Applications Shared Task 2019",
    "abstract": "In this study, we describe our methods to automatically classify Twitter posts conveying events of adverse drug reaction (ADR). Based on our previous experience in tackling the ADR classification task, we empirically applied the vote-based under-sampling ensemble approach along with linear support vector machine (SVM) to develop our classifiers as part of our participation in ACL 2019 Social Media Mining for Health Applications (SMM4H) shared task 1. The best-performed model on the test sets were trained on a merged corpus consisting of the datasets released by SMM4H 2017 and 2019. By using VUE, the corpus was randomly under-sampled with 2:1 ratio between the negative and positive classes to create an ensemble using the linear kernel trained with features including bag-of-word, domain knowledge, negation and word embedding. The best performing model achieved an F-measure of 0.551 which is about 5% higher than the average F-scores of 16 teams",
    "volume": "workshop",
    "checked": true,
    "id": "9fd0ed1b1e6388aa5049b40e638da9f3d2bff69e",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-3221": {
    "title": "Detection of Adverse Drug Reaction Mentions in Tweets Using ELMo",
    "abstract": "This paper describes the models used by our team in SMM4H 2019 shared task (Weissenbacher et al., 2019). We submitted results for subtasks 1 and 2. For task 1 which aims to detect tweets with Adverse Drug Reaction (ADR) mentions we used ELMo embeddings which is a deep contextualized word representation able to capture both syntactic and semantic characteristics. For task 2, which focuses on extraction of ADR mentions, first the same architecture as task 1 was used to identify whether or not a tweet contains ADR. Then, for tweets positively classified as mentioning ADR, the relevant text span was identified by similarity matching with 3 different lexicon sets",
    "volume": "workshop",
    "checked": true,
    "id": "e655bdb808c9f4fefa72db7385e92671bba6fd8d",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-3222": {
    "title": "Adverse Drug Effect and Personalized Health Mentions, CLaC at SMM4H 2019, Tasks 1 and 4",
    "abstract": "CLaC labs participated in Task 1 and 4 of SMM4H 2019. We pursed two main objectives in our submission. First we tried to use some textual features in a deep net framework, and second, the potential use of more than one word embedding was tested. The results seem positively affected by the proposed architectures",
    "volume": "workshop",
    "checked": true,
    "id": "3b91c2c8646514a12b1521f70dea940bfac37802",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3223": {
    "title": "MIDAS@SMM4H-2019: Identifying Adverse Drug Reactions and Personal Health Experience Mentions from Twitter",
    "abstract": "In this paper, we present our approach and the system description for the Social Media Mining for Health Applications (SMM4H) Shared Task 1,2 and 4 (2019). Our main contribution is to show the effectiveness of Transfer Learning approaches like BERT and ULMFiT, and how they generalize for the classification tasks like identification of adverse drug reaction mentions and reporting of personal health problems in tweets. We show the use of stacked embeddings combined with BLSTM+CRF tagger for identifying spans mentioning adverse drug reactions in tweets. We also show that these approaches perform well even with imbalanced dataset in comparison to undersampling and oversampling",
    "volume": "workshop",
    "checked": true,
    "id": "1df248d54df1bacb5d3ef217e10077ab6a48862e",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-3224": {
    "title": "Detection of Adverse Drug Reaction in Tweets Using a Combination of Heterogeneous Word Embeddings",
    "abstract": "This paper details our approach to the task of detecting reportage of adverse drug reaction in tweets as part of the 2019 social media mining for healthcare applications shared task. We employed a combination of three types of word representations as input to a LSTM model. With this approach, we achieved an F1 score of 0.5209",
    "volume": "workshop",
    "checked": true,
    "id": "38b961f717c6d6f450153ed12ed442f2f65a04dc",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-3225": {
    "title": "Identification of Adverse Drug Reaction Mentions in Tweets – SMM4H Shared Task 2019",
    "abstract": "Analyzing social media posts can offer insights into a wide range of topics that are commonly discussed online, providing valuable information for studying various health-related phenomena reported online. The outcome of this work can offer insights into pharmacovigilance research to monitor the adverse effects of medications. This research specifically looks into mentions of adverse drug reactions (ADRs) in Twitter data through the Social Media Mining for Health Applications (SMM4H) Shared Task 2019. Adverse drug reactions are undesired harmful effects which can arise from medication or other methods of treatment. The goal of this research is to build accurate models using natural language processing techniques to detect reports of adverse drug reactions in Twitter data and extract these words or phrases",
    "volume": "workshop",
    "checked": true,
    "id": "4f2228e60a380f49fcc453166af98ce9e2df6201",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3301": {
    "title": "Cross-Linguistic Semantic Annotation: Reconciling the Language-Specific and the Universal",
    "abstract": "Developers of cross-linguistic semantic annotation schemes face a number of issues not encountered in monolingual annotation. This paper discusses four such issues, related to the establishment of annotation labels, and the treatment of languages with more fine-grained, more coarse-grained, and cross-cutting categories. We propose that a lattice-like architecture of the annotation categories can adequately handle all four issues, and at the same time remain both intuitive for annotators and faithful to typological insights. This position is supported by a brief annotation experiment",
    "volume": "workshop",
    "checked": true,
    "id": "1b6d485240853aef44305082c68ada80949014bd",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-3302": {
    "title": "Thirty Musts for Meaning Banking",
    "abstract": "Meaning banking—creating a semantically annotated corpus for the purpose of semantic parsing or generation—is a challenging task. It is quite simple to come up with a complex meaning representation, but it is hard to design a simple meaning representation that captures many nuances of meaning. This paper lists some lessons learned in nearly ten years of meaning annotation during the development of the Groningen Meaning Bank (Bos et al., 2017) and the Parallel Meaning Bank (Abzianidze et al., 2017). The paper's format is rather unconventional: there is no explicit related work, no methodology section, no results, and no discussion (and the current snippet is not an abstract but actually an introductory preface). Instead, its structure is inspired by work of Traum (2000) and Bender (2013). The list starts with a brief overview of the existing meaning banks (Section 1) and the rest of the items are roughly divided into three groups: corpus collection (Section 2 and 3, annotation methods (Section 4–11), and design of meaning representations (Section 12–30). We hope this overview will give inspiration and guidance in creating improved meaning banks in the future",
    "volume": "workshop",
    "checked": true,
    "id": "f0f0e7c39dfdb81513484c2cfbe86a0e4c4cbaeb",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-3303": {
    "title": "Modeling Quantification and Scope in Abstract Meaning Representations",
    "abstract": "In this paper, we propose an extension to Abstract Meaning Representations (AMRs) to encode scope information of quantifiers and negation, in a way that overcomes the semantic gaps of the schema while maintaining its cognitive simplicity. Specifically, we address three phenomena not previously part of the AMR specification: quantification, negation (generally), and modality. The resulting representation, which we call \"Uniform Meaning Representation\" (UMR), adopts the predicative core of AMR and embeds it under a \"scope\" graph when appropriate. UMR representations differ from other treatments of quantification and modal scope phenomena in two ways: (a) they are more transparent; and (b) they specify default scope when possible.'",
    "volume": "workshop",
    "checked": true,
    "id": "402a7ab2b96250f6f96bb189a9e60cac350e8b15",
    "citation_count": 14
  },
  "https://aclanthology.org/W19-3304": {
    "title": "Parsing Meaning Representations: Is Easier Always Better?",
    "abstract": "The parsing accuracy varies a great deal for different meaning representations. In this paper, we compare the parsing performances between Abstract Meaning Representation (AMR) and Minimal Recursion Semantics (MRS), and provide an in-depth analysis of what factors contributed to the discrepancy in their parsing accuracy. By crystalizing the trade-off between representation expressiveness and ease of automatic parsing, we hope our results can help inform the design of the next-generation meaning representations",
    "volume": "workshop",
    "checked": true,
    "id": "4f4124f77e1f7eb4350c7e140c165943352808c0",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-3305": {
    "title": "GKR: Bridging the Gap between Symbolic/structural and Distributional Meaning Representations",
    "abstract": "Three broad approaches have been attempted to combine distributional and structural/symbolic aspects to construct meaning representations: a) injecting linguistic features into distributional representations, b) injecting distributional features into symbolic representations or c) combining structural and distributional features in the final representation. This work focuses on an example of the third and less studied approach: it extends the Graphical Knowledge Representation (GKR) to include distributional features and proposes a division of semantic labour between the distributional and structural/symbolic features. We propose two extensions of GKR that clearly show this division and empirically test one of the proposals on an NLI dataset with hard compositional pairs",
    "volume": "workshop",
    "checked": true,
    "id": "0950c25baa11655a569a68a9c99744d829b96349",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3306": {
    "title": "Generating Discourse Inferences from Unscoped Episodic Logical Formulas",
    "abstract": "Abstract Unscoped episodic logical form (ULF) is a semantic representation capturing the predicate-argument structure of English within the episodic logic formalism in relation to the syntactic structure, while leaving scope, word sense, and anaphora unresolved. We describe how ULF can be used to generate natural language inferences that are grounded in the semantic and syntactic structure through a small set of rules defined over interpretable predicates and transformations on ULFs. The semantic restrictions placed by ULF semantic types enables us to ensure that the inferred structures are semantically coherent while the nearness to syntax enables accurate mapping to English. We demonstrate these inferences on four classes of conversationally-oriented inferences in a mixed genre dataset with 68.5% precision from human judgments",
    "volume": "workshop",
    "checked": true,
    "id": "57046209bac432d1af853ceb47d1824afab3f97f",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-3307": {
    "title": "A Plea for Information Structure as a Part of Meaning Representation",
    "abstract": "The view that the representation of information structure (IS) should be a part of (any type of) representation of meaning is based on the fact that IS is a semantically relevant phenomenon. In the contribution, three arguments supporting this view are briefly summarized, namely, the relation of IS to the interpretation of negation and presupposition, the relevance of IS to the understanding of discourse connectivity and for the establishment and interpretation of coreference relations. Afterwards, possible integration of the description of the main ingredient of IS into a meaning representation is illustrated",
    "volume": "workshop",
    "checked": true,
    "id": "d2103e7bc8cb67f33faaf5ef85641f6d90666ef5",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3308": {
    "title": "TCL - a Lexicon of Turkish Discourse Connectives",
    "abstract": "It is known that discourse connectives are the most salient indicators of discourse relations. State-of-the-art parsers being developed to predict explicit discourse connectives exploit annotated discourse corpora but a lexicon of discourse connectives is also needed to enable further research in discourse structure and support the development of language technologies that use these structures for text understanding. This paper presents a lexicon of Turkish discourse connectives built by automatic means. The lexicon has the format of the German connective lexicon, DiMLex, where for each discourse connective, information about the connective's orthographic variants, syntactic category and senses are provided along with sample relations. In this paper, we describe the data sources we used and the development steps of the lexicon",
    "volume": "workshop",
    "checked": true,
    "id": "a349954408e9b3b2f3fdfaff7d1e1ea8fead71fb",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3309": {
    "title": "Meta-Semantic Representation for Early Detection of Alzheimer's Disease",
    "abstract": "This paper presents a new task-oriented meaning representation called meta-semantics, that is designed to detect patients with early symptoms of Alzheimer's disease by analyzing their language beyond a syntactic or semantic level. Meta-semantic representation consists of three parts, entities, predicate argument structures, and discourse attributes, that derive rich knowledge graphs. For this study, 50 controls and 50 patients with mild cognitive impairment (MCI) are selected, and meta-semantic representation is annotated on their speeches transcribed in text. Inter-annotator agreement scores of 88%, 82%, and 89% are achieved for the three types of annotation, respectively. Five analyses are made using this annotation, depicting clear distinctions between the control and MCI groups. Finally, a neural model is trained on features extracted from those analyses to classify MCI patients from normal controls, showing a high accuracy of 82% that is very promising",
    "volume": "workshop",
    "checked": true,
    "id": "68dc06c13b134a3f92b0d6e6315d7529acba82d9",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3310": {
    "title": "Ellipsis in Chinese AMR Corpus",
    "abstract": "Ellipsis is very common in language. It's necessary for natural language processing to restore the elided elements in a sentence. However, there's only a few corpora annotating the ellipsis, which draws back the automatic detection and recovery of the ellipsis. This paper introduces the annotation of ellipsis in Chinese sentences, using a novel graph-based representation Abstract Meaning Representation (AMR), which has a good mechanism to restore the elided elements manually. We annotate 5,000 sentences selected from Chinese TreeBank (CTB). We find that 54.98% of sentences have ellipses. 92% of the ellipses are restored by copying the antecedents' concepts. and 12.9% of them are the new added concepts. In addition, we find that the elided element is a word or phrase in most cases, but sometimes only the head of a phrase or parts of a phrase, which is rather hard for the automatic recovery of ellipsis",
    "volume": "workshop",
    "checked": true,
    "id": "cae67720fce16ffd09193592d27584373bc58a00",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3311": {
    "title": "Event Structure Representation: Between Verbs and Argument Structure Constructions",
    "abstract": "This paper proposes a novel representation of event structure by separating verbal semantics and the meaning of argument structure constructions that verbs occur in. Our model demonstrates how the two meaning representations interact. Our model thus effectively deals with various verb construals in different argument structure constructions, unlike purely verb-based approaches. However, unlike many constructionally-based approaches, we also provide a richer representation of the event structure evoked by the verb meaning",
    "volume": "workshop",
    "checked": true,
    "id": "119f82e631efd6b65f5f836cbada278e90e80015",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-3312": {
    "title": "Distributional Semantics Meets Construction Grammar. towards a Unified Usage-Based Model of Grammar and Meaning",
    "abstract": "In this paper, we propose a new type of semantic representation of Construction Grammar that combines constructions with the vector representations used in Distributional Semantics. We introduce a new framework, Distributional Construction Grammar, where grammar and meaning are systematically modeled from language use, and finally, we discuss the kind of contributions that distributional models can provide to CxG representation from a linguistic and cognitive perspective",
    "volume": "workshop",
    "checked": true,
    "id": "4ca34ad219ac5fd103be323bc72dde7176391c25",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-3313": {
    "title": "Meaning Representation of Null Instantiated Semantic Roles in FrameNet",
    "abstract": "Humans have the unique ability to infer information about participants in a scene, even if they are not mentioned in a text about that scene. Computer systems cannot do so without explicit information about those participants. This paper addresses the linguistic phenomenon of null-instantiated frame elements, i.e., implicit semantic roles, and their representation in FrameNet (FN). It motivates FN's annotation practice, and illustrates three types of null-instantiated arguments that FrameNet tracks, noting that other lexical resources do not record such semantic-pragmatic information, despite its need in natural language understanding (NLU), and the elaborate efforts to create new datasets. It challenges the community to appeal to FN data to develop more sophisticated techniques for recognizing implicit semantic roles, and creating needed datasets. Although the annotation of null-instantiated roles was lexicographically motivated, FN provides useful information for text processing, and therefore must be considered in the design of any meaning representation for natural language understanding",
    "volume": "workshop",
    "checked": true,
    "id": "4813e3c3316d0d40e5b4d33a0e8f72fa592d112a",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-3314": {
    "title": "Copula and Case-Stacking Annotations for Korean AMR",
    "abstract": "This paper concerns the application of Abstract Meaning Representation (AMR) to Korean. In this regard, it focuses on the copula construction and its negation and the case-stacking phenomenon thereof. To illustrate this clearly, we reviewed the :domain annotation scheme from various perspectives. In this process, the existing annotation guidelines were improved to devise annotation schemes for each issue under the principle of pursuing consistency and efficiency of annotation without distorting the characteristics of Korean",
    "volume": "workshop",
    "checked": true,
    "id": "466aba05910495aa16236d3fa97d9707ecfd93cf",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-3315": {
    "title": "ClearTAC: Verb Tense, Aspect, and Form Classification Using Neural Nets",
    "abstract": "This paper proposes using a Bidirectional LSTM-CRF model in order to identify the tense and aspect of verbs. The information that this classifier outputs can be useful for ordering events and can provide a pre-processing step to improve efficiency of annotating this type of information. This neural network architecture has been successfully employed for other sequential labeling tasks, and we show that it significantly outperforms the rule-based tool TMV-annotator on the Propbank I dataset",
    "volume": "workshop",
    "checked": true,
    "id": "1da41d04495ddb315224ef741fb7d5b45d4636ff",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-3316": {
    "title": "Preparing SNACS for Subjects and Objects",
    "abstract": "Research on adpositions and possessives in multiple languages has led to a small inventory of general-purpose meaning classes that disambiguate tokens. Importantly, that work has argued for a principled separation of the semantic role in a scene from the function coded by morphosyntax. Here, we ask whether this approach can be generalized beyond adpositions and possessives to cover all scene participants—including subjects and objects—directly, without reference to a frame lexicon. We present new guidelines for English and the results of an interannotator agreement study",
    "volume": "workshop",
    "checked": true,
    "id": "8bb255c4afea4a0ccd07d30b8a442d5d00770586",
    "citation_count": 11
  },
  "https://aclanthology.org/W19-3317": {
    "title": "A Case Study on Meaning Representation for Vietnamese",
    "abstract": "This paper presents a case study on meaning representation for Vietnamese. Having introduced several existing semantic representation schemes for different languages, we select as basis for our work on Vietnamese AMR (Abstract Meaning Representation). From it, we define a meaning representation label set by adapting the English schema and taking into account the specific characteristics of Vietnamese",
    "volume": "workshop",
    "checked": true,
    "id": "891ad79b140b585e2ea9d3896bd150e10fe50d51",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-3318": {
    "title": "VerbNet Representations: Subevent Semantics for Transfer Verbs",
    "abstract": "This paper announces the release of a new version of the English lexical resource VerbNet with substantially revised semantic representations designed to facilitate computer planning and reasoning based on human language. We use the transfer of possession and transfer of information event representations to illustrate both the general framework of the representations and the types of nuances the new representations can capture. These representations use a Generative Lexicon-inspired subevent structure to track attributes of event participants across time, highlighting oppositions and temporal and causal relations among the subevents",
    "volume": "workshop",
    "checked": true,
    "id": "309cdda11f69f68ed5c9f080c18a065ff86c92ae",
    "citation_count": 17
  },
  "https://aclanthology.org/W19-3319": {
    "title": "Semantically Constrained Multilayer Annotation: The Case of Coreference",
    "abstract": "We propose a coreference annotation scheme as a layer on top of the Universal Conceptual Cognitive Annotation foundational layer, treating units in predicate-argument structure as a basis for entity and event mentions. We argue that this allows coreference annotators to sidestep some of the challenges faced in other schemes, which do not enforce consistency with predicate-argument structure and vary widely in what kinds of mentions they annotate and how. The proposed approach is examined with a pilot annotation study and compared with annotations from other schemes",
    "volume": "workshop",
    "checked": true,
    "id": "72e2256d678b2fbdbb3fe72039b7bea76a995fc1",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-3320": {
    "title": "Towards Universal Semantic Representation",
    "abstract": "Natural language understanding at the semantic level and independent of language variations is of great practical value. Existing approaches such as semantic role labeling (SRL) and abstract meaning representation (AMR) still have features related to the peculiarities of the particular language. In this work we describe various challenges and possible solutions in designing a semantic representation that is universal across a variety of languages",
    "volume": "workshop",
    "checked": true,
    "id": "d74e7c3dc138ced47812b7feca130abf8953bc23",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-3321": {
    "title": "A Dependency Structure Annotation for Modality",
    "abstract": "This paper presents an annotation scheme for modality that employs a dependency structure. Events and sources (here, conceivers) are represented as nodes and epistemic strength relations characterize the edges. The epistemic strength values are largely based on Saurí and Pustejovsky's (2009) FactBank, while the dependency structure mirrors Zhang and Xue's (2018b) approach to temporal relations. Six documents containing 377 events have been annotated by two expert annotators with high levels of agreement",
    "volume": "workshop",
    "checked": true,
    "id": "5208c1a16abee136f60b24083773dff57541985a",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-3322": {
    "title": "Augmenting Abstract Meaning Representation for Human-Robot Dialogue",
    "abstract": "We detail refinements made to Abstract Meaning Representation (AMR) that make the representation more suitable for supporting a situated dialogue system, where a human remotely controls a robot for purposes of search and rescue and reconnaissance. We propose 36 augmented AMRs that capture speech acts, tense and aspect, and spatial information. This linguistic information is vital for representing important distinctions, for example whether the robot has moved, is moving, or will move. We evaluate two existing AMR parsers for their performance on dialogue data. We also outline a model for graph-to-graph conversion, in which output from AMR parsers is converted into our refined AMRs. The design scheme presented here, though task-specific, is extendable for broad coverage of speech acts using AMR in future task-independent work",
    "volume": "workshop",
    "checked": true,
    "id": "c912337dd5aeb0872217949e3b8db6dfc25ad3f9",
    "citation_count": 14
  },
  "https://aclanthology.org/W19-3401": {
    "title": "Composing a Picture Book by Automatic Story Understanding and Visualization",
    "abstract": "Pictures can enrich storytelling experiences. We propose a framework that can automatically compose a picture book by understanding story text and visualizing it with painting elements, i.e., characters and backgrounds. For story understanding, we extract key information from a story on both sentence level and paragraph level, including characters, scenes and actions. These concepts are organized and visualized in a way that depicts the development of a story. We collect a set of Chinese stories for children and apply our approach to compose pictures for stories. Extensive experiments are conducted towards story event extraction for visualization to demonstrate the effectiveness of our method",
    "volume": "workshop",
    "checked": true,
    "id": "88c2391eff2e08d5644fd076859e2c0f3b139425",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3402": {
    "title": "\"My Way of Telling a Story\": Persona based Grounded Story Generation",
    "abstract": "Visual storytelling is the task of generating stories based on a sequence of images. Inspired by the recent works in neural generation focusing on controlling the form of text, this paper explores the idea of generating these stories in different personas. However, one of the main challenges of performing this task is the lack of a dataset of visual stories in different personas. Having said that, there are independent datasets for both visual storytelling and annotated sentences for various persona. In this paper we describe an approach to overcome this by getting labelled persona data from a different task and leveraging those annotations to perform persona based story generation. We inspect various ways of incorporating personality in both the encoder and the decoder representations to steer the generation in the target direction. To this end, we propose five models which are incremental extensions to the baseline model to perform the task at hand. In our experiments we use five different personas to guide the generation process. We find that the models based on our hypotheses perform better at capturing words while generating stories in the target persona",
    "volume": "workshop",
    "checked": true,
    "id": "9e05c31fa936f6bbb30633597168b63b60d3f3e0",
    "citation_count": 19
  },
  "https://aclanthology.org/W19-3403": {
    "title": "Using Functional Schemas to Understand Social Media Narratives",
    "abstract": "We propose a novel take on understanding narratives in social media, focusing on learning \"functional story schemas\", which consist of sets of stereotypical functional structures. We develop an unsupervised pipeline to extract schemas and apply our method to Reddit posts to detect schematic structures that are characteristic of different subreddits. We validate our schemas through human interpretation and evaluate their utility via a text classification task. Our experiments show that extracted schemas capture distinctive structural patterns in different subreddits, improving classification performance of several models by 2.4% on average. We also observe that these schemas serve as lenses that reveal community norms",
    "volume": "workshop",
    "checked": true,
    "id": "0ec7c1c06ed79445a6e0cfeed188f6893fff24e6",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-3404": {
    "title": "A Hybrid Model for Globally Coherent Story Generation",
    "abstract": "Automatically generating globally coherent stories is a challenging problem. Neural text generation models have been shown to perform well at generating fluent sentences from data, but they usually fail to keep track of the overall coherence of the story after a couple of sentences. Existing work that incorporates a text planning module succeeded in generating recipes and dialogues, but appears quite data-demanding. We propose a novel story generation approach that generates globally coherent stories from a fairly small corpus. The model exploits a symbolic text planning module to produce text plans, thus reducing the demand of data; a neural surface realization module then generates fluent text conditioned on the text plan. Human evaluation showed that our model outperforms various baselines by a wide margin and generates stories which are fluent as well as globally coherent",
    "volume": "workshop",
    "checked": true,
    "id": "ad44ff82f824c3d91001c47ce413d64a61521ae8",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-3405": {
    "title": "Guided Neural Language Generation for Automated Storytelling",
    "abstract": "Neural network based approaches to automated story plot generation attempt to learn how to generate novel plots from a corpus of natural language plot summaries. Prior work has shown that a semantic abstraction of sentences called events improves neural plot generation and and allows one to decompose the problem into: (1) the generation of a sequence of events (event-to-event) and (2) the transformation of these events into natural language sentences (event-to-sentence). However, typical neural language generation approaches to event-to-sentence can ignore the event details and produce grammatically-correct but semantically-unrelated sentences. We present an ensemble-based model that generates natural language guided by events. Our method outperforms the baseline sequence-to-sequence model. Additionally, we provide results for a full end-to-end automated story generation system, demonstrating how our model works with existing systems designed for the event-to-event problem",
    "volume": "workshop",
    "checked": true,
    "id": "54e40f94d886247183f03ef6bd6b212c1bdcde8a",
    "citation_count": 17
  },
  "https://aclanthology.org/W19-3406": {
    "title": "An Analysis of Emotion Communication Channels in Fan-Fiction: Towards Emotional Storytelling",
    "abstract": "Centrality of emotion for the stories told by humans is underpinned by numerous studies in literature and psychology. The research in automatic storytelling has recently turned towards emotional storytelling, in which characters' emotions play an important role in the plot development (Theune et al., 2004; y Perez, 2007; Mendez et al., 2016). However, these studies mainly use emotion to generate propositional statements in the form \"A feels affection towards B\" or \"A confronts B\". At the same time, emotional behavior does not boil down to such propositional descriptions, as humans display complex and highly variable patterns in communicating their emotions, both verbally and non-verbally. In this paper, we analyze how emotions are expressed non-verbally in a corpus of fan fiction short stories. Our analysis shows that stories written by humans convey character emotions along various non-verbal channels. We find that some non-verbal channels, such as facial expressions and voice characteristics of the characters, are more strongly associated with joy, while gestures and body postures are more likely to occur with trust. Based on our analysis, we argue that automatic storytelling systems should take variability of emotion into account when generating descriptions of characters' emotions",
    "volume": "workshop",
    "checked": true,
    "id": "9eac761c028b1785bb843ed36f8d043df4037dd2",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-3407": {
    "title": "Narrative Generation in the Wild: Methods from NaNoGenMo",
    "abstract": "In text generation, generating long stories is still a challenge. Coherence tends to decrease rapidly as the output length increases. Especially for generated stories, coherence of the narrative is an important quality aspect of the output text. In this paper we examine how narrative coherence is attained in the submissions of NaNoGenMo 2018, an online text generation event where participants are challenged to generate a 50,000 word novel. We list the main approaches that were used to generate coherent narratives and link them to scientific literature. Finally, we give recommendations on when to use which approach",
    "volume": "workshop",
    "checked": true,
    "id": "d744edc7e5b2e01220b34ba0bd8240abdb3c36f6",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-3408": {
    "title": "Lexical concreteness in narrative",
    "abstract": "This study explores the relation between lexical concreteness and narrative text quality. We present a methodology to quantitatively measure lexical concreteness of a text. We apply it to a corpus of student stories, scored according to writing evaluation rubrics. Lexical concreteness is weakly-to-moderately related to story quality, depending on story-type. The relation is mostly borne by adjectives and nouns, but also found for adverbs and verbs",
    "volume": "workshop",
    "checked": true,
    "id": "491f3b6833694f3b8612eb1d06d73489eb5df3e6",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3409": {
    "title": "A Simple Approach to Classify Fictional and Non-Fictional Genres",
    "abstract": "In this work, we deploy a logistic regression classifier to ascertain whether a given document belongs to the fiction or non-fiction genre. For genre identification, previous work had proposed three classes of features, viz., low-level (character-level and token counts), high-level (lexical and syntactic information) and derived features (type-token ratio, average word length or average sentence length). Using the Recursive feature elimination with cross-validation (RFECV) algorithm, we perform feature selection experiments on an exhaustive set of nineteen features (belonging to all the classes mentioned above) extracted from Brown corpus text. As a result, two simple features viz., the ratio of the number of adverbs to adjectives and the number of adjectives to pronouns turn out to be the most significant. Subsequently, our classification experiments aimed towards genre identification of documents from the Brown and Baby BNC corpora demonstrate that the performance of a classifier containing just the two aforementioned features is at par with that of a classifier containing the exhaustive feature set",
    "volume": "workshop",
    "checked": true,
    "id": "ecdc4948b9c373a3b3d24777a3af280fb87bc826",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3410": {
    "title": "Detecting Everyday Scenarios in Narrative Texts",
    "abstract": "Script knowledge consists of detailed information on everyday activities. Such information is often taken for granted in text and needs to be inferred by readers. Therefore, script knowledge is a central component to language comprehension. Previous work on representing scripts is mostly based on extensive manual work or limited to scenarios that can be found with sufficient redundancy in large corpora. We introduce the task of scenario detection, in which we identify references to scripts. In this task, we address a wide range of different scripts (200 scenarios) and we attempt to identify all references to them in a collection of narrative texts. We present a first benchmark data set and a baseline model that tackles scenario detection using techniques from topic segmentation and text classification",
    "volume": "workshop",
    "checked": true,
    "id": "21eb3dfefd118dd819f18eae26c8a5277d0730bb",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3411": {
    "title": "Personality Traits Recognition in Literary Texts",
    "abstract": "Interesting stories often are built around interesting characters. Finding and detailing what makes an interesting character is a real challenge, but certainly a significant cue is the character personality traits. Our exploratory work tests the adaptability of the current personality traits theories to literal characters, focusing on the analysis of utterances in theatre scripts. And, at the opposite, we try to find significant traits for interesting characters. The preliminary results demonstrate that our approach is reasonable. Using machine learning for gaining insight into the personality traits of fictional characters can make sense",
    "volume": "workshop",
    "checked": true,
    "id": "5ef191197c79c34e8b3f44a55e384fda1a2cc455",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3412": {
    "title": "Winter is here: Summarizing Twitter Streams related to Pre-Scheduled Events",
    "abstract": "Pre-scheduled events, such as TV shows and sports games, usually garner considerable attention from the public. Twitter captures large volumes of discussions and messages related to these events, in real-time. Twitter streams related to pre-scheduled events are characterized by the following: (1) spikes in the volume of published tweets reflect the highlights of the event and (2) some of the published tweets make reference to the characters involved in the event, in the context in which they are currently portrayed in a subevent. In this paper, we take advantage of these characteristics to identify the highlights of pre-scheduled events from tweet streams and we demonstrate a method to summarize these highlights. We evaluate our algorithm on tweets collected around 2 episodes of a popular TV show, Game of Thrones, Season 7",
    "volume": "workshop",
    "checked": true,
    "id": "41c8bcfb6301d2bd15d19d95c4b30c886f997486",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-3413": {
    "title": "WriterForcing: Generating more interesting story endings",
    "abstract": "We study the problem of generating interesting endings for stories. Neural generative models have shown promising results for various text generation problems. Sequence to Sequence (Seq2Seq) models are typically trained to generate a single output sequence for a given input sequence. However, in the context of a story, multiple endings are possible. Seq2Seq models tend to ignore the context and generate generic and dull responses. Very few works have studied generating diverse and interesting story endings for the same story context. In this paper, we propose models which generate more diverse and interesting outputs by 1) training models to focus attention on important keyphrases of the story, and 2) promoting generating nongeneric words. We show that the combination of the two leads to more interesting endings",
    "volume": "workshop",
    "checked": true,
    "id": "226b6be8c7bab0dca80084d31e5e6719c45f1242",
    "citation_count": 14
  },
  "https://aclanthology.org/W19-3414": {
    "title": "Prediction of a Movie's Success From Plot Summaries Using Deep Learning Models",
    "abstract": "As the size of investment for movie production grows bigger, the need for predicting a movie's success in early stages has increased. To address this need, various approaches have been proposed, mostly relying on movie reviews, trailer movie clips, and SNS postings. However, all of these are available only after a movie is produced and released. To enable a more earlier prediction of a movie's performance, we propose a deep-learning based approach to predict the success of a movie using only its plot summary text. This paper reports the results evaluating the efficacy of the proposed method and concludes with discussions and future work",
    "volume": "workshop",
    "checked": true,
    "id": "f144b55d0ad0b2d370ed78e4401ff129b15cb7f0",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-3501": {
    "title": "Subversive Toxicity Detection using Sentiment Information",
    "abstract": "The presence of toxic content has become a major problem for many online communities. Moderators try to limit this problem by implementing more and more refined comment filters, but toxic users are constantly finding new ways to circumvent them. Our hypothesis is that while modifying toxic content and keywords to fool filters can be easy, hiding sentiment is harder. In this paper, we explore various aspects of sentiment detection and their correlation to toxicity, and use our results to implement a toxicity detection tool. We then test how adding the sentiment information helps detect toxicity in three different real-world datasets, and incorporate subversion to these datasets to simulate a user trying to circumvent the system. Our results show sentiment information has a positive impact on toxicity detection",
    "volume": "workshop",
    "checked": true,
    "id": "43d421e44ea1ee2c2e31b342e64618b7282edfee",
    "citation_count": 19
  },
  "https://aclanthology.org/W19-3502": {
    "title": "Exploring Deep Multimodal Fusion of Text and Photo for Hate Speech Classification",
    "abstract": "Interactions among users on social network platforms are usually positive, constructive and insightful. However, sometimes people also get exposed to objectionable content such as hate speech, bullying, and verbal abuse etc. Most social platforms have explicit policy against hate speech because it creates an environment of intimidation and exclusion, and in some cases may promote real-world violence. As users' interactions on today's social networks involve multiple modalities, such as texts, images and videos, in this paper we explore the challenge of automatically identifying hate speech with deep multimodal technologies, extending previous research which mostly focuses on the text signal alone. We present a number of fusion approaches to integrate text and photo signals. We show that augmenting text with image embedding information immediately leads to a boost in performance, while applying additional attention fusion methods brings further improvement",
    "volume": "workshop",
    "checked": true,
    "id": "ee8ea1a02a7cea51ae9191e0c0dad7ab080741d8",
    "citation_count": 48
  },
  "https://aclanthology.org/W19-3503": {
    "title": "Detecting harassment in real-time as conversations develop",
    "abstract": "We developed a machine-learning-based method to detect video game players that harass teammates or opponents in chat earlier in the conversation. This real-time technology would allow gaming companies to intervene during games, such as issue warnings or muting or banning a player. In a proof-of-concept experiment on League of Legends data we compute and visualize evaluation metrics for a machine learning classifier as conversations unfold, and observe that the optimal precision and recall of detecting toxic players at each moment in the conversation depends on the confidence threshold of the classifier: the threshold should start low, and increase as the conversation unfolds. How fast this sliding threshold should increase depends on the training set size",
    "volume": "workshop",
    "checked": true,
    "id": "bfdfcbd8e71b651feb39d61772556524e18df390",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-3504": {
    "title": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    "abstract": "Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect",
    "volume": "workshop",
    "checked": true,
    "id": "5af1e8b2f546ab8dbac7a35e89e5a2b2af7968d7",
    "citation_count": 236
  },
  "https://aclanthology.org/W19-3505": {
    "title": "Automated Identification of Verbally Abusive Behaviors in Online Discussions",
    "abstract": "Discussion forum participation represents one of the crucial factors for learning and often the only way of supporting social interactions in online settings. However, as much as sharing new ideas or asking thoughtful questions contributes learning, verbally abusive behaviors, such as expressing negative emotions in online discussions, could have disproportionate detrimental effects. To provide means for mitigating the potential negative effects on course participation and learning, we developed an automated classifier for identifying communication that show linguistic patterns associated with hostility in online forums. In so doing, we employ several well-established automated text analysis tools and build on the common practices for handling highly imbalanced datasets and reducing the sensitivity to overfitting. Although still in its infancy, our approach shows promising results (ROC AUC .73) towards establishing a robust detector of abusive behaviors. We further provide an overview of the classification (linguistic and contextual) features most indicative of online aggression",
    "volume": "workshop",
    "checked": true,
    "id": "a7cefcda3d2e90f1f1549a9bf274007d5af43faf",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-3506": {
    "title": "Multi-label Hate Speech and Abusive Language Detection in Indonesian Twitter",
    "abstract": "Hate speech and abusive language spreading on social media need to be detected automatically to avoid conflict between citizen. Moreover, hate speech has a target, category, and level that also needs to be detected to help the authority in prioritizing which hate speech must be addressed immediately. This research discusses multi-label text classification for abusive language and hate speech detection including detecting the target, category, and level of hate speech in Indonesian Twitter using machine learning approach with Support Vector Machine (SVM), Naive Bayes (NB), and Random Forest Decision Tree (RFDT) classifier and Binary Relevance (BR), Label Power-set (LP), and Classifier Chains (CC) as the data transformation method. We used several kinds of feature extractions which are term frequency, orthography, and lexicon features. Our experiment results show that in general RFDT classifier using LP as the transformation method gives the best accuracy with fast computational time",
    "volume": "workshop",
    "checked": true,
    "id": "ad3b9d3136037eeaade13580050cbb16640aa084",
    "citation_count": 84
  },
  "https://aclanthology.org/W19-3507": {
    "title": "The Discourse of Online Content Moderation: Investigating Polarized User Responses to Changes in Reddit's Quarantine Policy",
    "abstract": "Recent concerns over abusive behavior on their platforms have pressured social media companies to strengthen their content moderation policies. However, user opinions on these policies have been relatively understudied. In this paper, we present an analysis of user responses to a September 27, 2018 announcement about the quarantine policy on Reddit as a case study of to what extent the discourse on content moderation is polarized by users' ideological viewpoint. We introduce a novel partitioning approach for characterizing user polarization based on their distribution of participation across interest subreddits. We then use automated techniques for capturing framing to examine how users with different viewpoints discuss moderation issues, finding that right-leaning users invoked censorship while left-leaning users highlighted inconsistencies on how content policies are applied. Overall, we argue for a more nuanced approach to moderation by highlighting the intersection of behavior and ideology in considering how abusive language is defined and regulated",
    "volume": "workshop",
    "checked": true,
    "id": "23a8f088d29246ec9ed03f704e5308e732dd99df",
    "citation_count": 25
  },
  "https://aclanthology.org/W19-3508": {
    "title": "Pay \"Attention\" to your Context when Classifying Abusive Language",
    "abstract": "The goal of any social media platform is to facilitate healthy and meaningful interactions among its users. But more often than not, it has been found that it becomes an avenue for wanton attacks. We propose an experimental study that has three aims: 1) to provide us with a deeper understanding of current data sets that focus on different types of abusive language, which are sometimes overlapping (racism, sexism, hate speech, offensive language, and personal attacks); 2) to investigate what type of attention mechanism (contextual vs. self-attention) is better for abusive language detection using deep learning architectures; and 3) to investigate whether stacked architectures provide an advantage over simple architectures for this task",
    "volume": "workshop",
    "checked": true,
    "id": "95e5e3e22dddc943ff111acab411f079487dc587",
    "citation_count": 18
  },
  "https://aclanthology.org/W19-3509": {
    "title": "Challenges and frontiers in abusive content detection",
    "abstract": "Online abusive content detection is an inherently difficult task. It has received considerable attention from academia, particularly within the computational linguistics community, and performance appears to have improved as the field has matured. However, considerable challenges and unaddressed frontiers remain, spanning technical, social and ethical dimensions. These issues constrain the performance, efficiency and generalizability of abusive content detection systems. In this article we delineate and clarify the main challenges and frontiers in the field, critically evaluate their implications and discuss potential solutions. We also highlight ways in which social scientific insights can advance research. We discuss the lack of support given to researchers working with abusive content and provide guidelines for ethical research",
    "volume": "workshop",
    "checked": true,
    "id": "5e0d31034ad5df02c937f354efa3297a41b08d30",
    "citation_count": 117
  },
  "https://aclanthology.org/W19-3510": {
    "title": "A Hierarchically-Labeled Portuguese Hate Speech Dataset",
    "abstract": "Over the past years, the amount of online offensive speech has been growing steadily. To successfully cope with it, machine learning are applied. However, ML-based techniques require sufficiently large annotated datasets. In the last years, different datasets were published, mainly for English. In this paper, we present a new dataset for Portuguese, which has not been in focus so far. The dataset is composed of 5,668 tweets. For its annotation, we defined two different schemes used by annotators with different levels of expertise. Firstly, non-experts annotated the tweets with binary labels ('hate' vs. 'no-hate'). Secondly, expert annotators classified the tweets following a fine-grained hierarchical multiple label scheme with 81 hate speech categories in total. The inter-annotator agreement varied from category to category, which reflects the insight that some types of hate speech are more subtle than others and that their detection depends on personal perception. This hierarchical annotation scheme is the main contribution of the presented work, as it facilitates the identification of different types of hate speech and their intersections. To demonstrate the usefulness of our dataset, we carried a baseline classification experiment with pre-trained word embeddings and LSTM on the binary classified data, with a state-of-the-art outcome",
    "volume": "workshop",
    "checked": true,
    "id": "58b8fa5915703c4cd32206ecc23f8d0fb57dac73",
    "citation_count": 79
  },
  "https://aclanthology.org/W19-3511": {
    "title": "A System to Monitor Cyberbullying based on Message Classification and Social Network Analysis",
    "abstract": "Social media platforms like Twitter and Instagram face a surge in cyberbullying phenomena against young users and need to develop scalable computational methods to limit the negative consequences of this kind of abuse. Despite the number of approaches recently proposed in the Natural Language Processing (NLP) research area for detecting different forms of abusive language, the issue of identifying cyberbullying phenomena at scale is still an unsolved problem. This is because of the need to couple abusive language detection on textual message with network analysis, so that repeated attacks against the same person can be identified. In this paper, we present a system to monitor cyberbullying phenomena by combining message classification and social network analysis. We evaluate the classification module on a data set built on Instagram messages, and we describe the cyberbullying monitoring user interface",
    "volume": "workshop",
    "checked": true,
    "id": "e5bec8e2202ca9ee139bf1ccc9a0dca2e50c46e8",
    "citation_count": 20
  },
  "https://aclanthology.org/W19-3512": {
    "title": "L-HSAB: A Levantine Twitter Dataset for Hate Speech and Abusive Language",
    "abstract": "Hate speech and abusive language have become a common phenomenon on Arabic social media. Automatic hate speech and abusive detection systems can facilitate the prohibition of toxic textual contents. The complexity, informality and ambiguity of the Arabic dialects hindered the provision of the needed resources for Arabic abusive/hate speech detection research. In this paper, we introduce the first publicly-available Levantine Hate Speech and Abusive (L-HSAB) Twitter dataset with the objective to be a benchmark dataset for automatic detection of online Levantine toxic contents. We, further, provide a detailed review of the data collection steps and how we design the annotation guidelines such that a reliable dataset annotation is guaranteed. This has been later emphasized through the comprehensive evaluation of the annotations as the annotation agreement metrics of Cohen's Kappa (k) and Krippendorff's alpha (α) indicated the consistency of the annotations",
    "volume": "workshop",
    "checked": true,
    "id": "22563d036d203e9644baa1f7a6ddc201de8e3053",
    "citation_count": 80
  },
  "https://aclanthology.org/W19-3513": {
    "title": "At the Lower End of Language—Exploring the Vulgar and Obscene Side of German",
    "abstract": "In this paper, we describe a workflow for the data-driven acquisition and semantic scaling of a lexicon that covers lexical items from the lower end of the German language register—terms typically considered as rough, vulgar or obscene. Since the fine semantic representation of grades of obscenity can only inadequately be captured at the categorical level (e.g., obscene vs. non-obscene, or rough vs. vulgar), our main contribution lies in applying best-worst scaling, a rating methodology that has already been shown to be useful for emotional language, to capture the relative strength of obscenity of lexical items. We describe the empirical foundations for bootstrapping such a low-end lexicon for German by starting from manually supplied lexicographic categorizations of a small seed set of rough and vulgar lexical items and automatically enlarging this set by means of distributional semantics. We then determine the degrees of obscenity for the full set of all acquired lexical items by letting crowdworkers comparatively assess their pejorative grade using best-worst scaling. This semi-automatically enriched lexicon already comprises 3,300 lexical items and incorporates 33,000 vulgarity ratings. Using it as a seed lexicon for fully automatic lexical acquisition, we were able to raise its coverage up to slightly more than 11,000 entries",
    "volume": "workshop",
    "checked": true,
    "id": "48390f00d114c6948a894e24c78c538db3acf568",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-3514": {
    "title": "Preemptive Toxic Language Detection in Wikipedia Comments Using Thread-Level Context",
    "abstract": "We address the task of automatically detecting toxic content in user generated texts. We fo cus on exploring the potential for preemptive moderation, i.e., predicting whether a particular conversation thread will, in the future, incite a toxic comment. Moreover, we perform preliminary investigation of whether a model that jointly considers all comments in a conversation thread outperforms a model that considers only individual comments. Using an existing dataset of conversations among Wikipedia contributors as a starting point, we compile a new large-scale dataset for this task consisting of labeled comments and comments from their conversation threads",
    "volume": "workshop",
    "checked": true,
    "id": "77e2ac1f155842b05261ab8523c6f90ac55e55df",
    "citation_count": 19
  },
  "https://aclanthology.org/W19-3515": {
    "title": "Neural Word Decomposition Models for Abusive Language Detection",
    "abstract": "The text we see in social media suffers from lots of undesired characterstics like hatespeech, abusive language, insults etc. The nature of this text is also very different compared to the traditional text we see in news with lots of obfuscated words, intended typos. This poses several robustness challenges to many natural language processing (NLP) techniques developed for traditional text. Many techniques proposed in the recent times such as charecter encoding models, subword models, byte pair encoding to extract subwords can aid in dealing with few of these nuances. In our work, we analyze the effectiveness of each of the above techniques, compare and contrast various word decomposition techniques when used in combination with others. We experiment with recent advances of finetuning pretrained language models, and demonstrate their robustness to domain shift. We also show our approaches achieve state of the art performance on Wikipedia attack, toxicity datasets, and Twitter hatespeech dataset",
    "volume": "workshop",
    "checked": true,
    "id": "659ce190ece2fe577153e5c36beb6db18a6f9c85",
    "citation_count": 21
  },
  "https://aclanthology.org/W19-3516": {
    "title": "A Platform Agnostic Dual-Strand Hate Speech Detector",
    "abstract": "Hate speech detectors must be applicable across a multitude of services and platforms, and there is hence a need for detection approaches that do not depend on any information specific to a given platform. For instance, the information stored about the text's author may differ between services, and so using such data would reduce a system's general applicability. The paper thus focuses on using exclusively text-based input in the detection, in an optimised architecture combining Convolutional Neural Networks and Long Short-Term Memory-networks. The hate speech detector merges two strands with character n-grams and word embeddings to produce the final classification, and is shown to outperform comparable previous approaches",
    "volume": "workshop",
    "checked": true,
    "id": "023a95a80dd080194c3210b6dd1e13389be9c796",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-3517": {
    "title": "Detecting Aggression and Toxicity using a Multi Dimension Capsule Network",
    "abstract": "In the era of social media, hate speech, trolling and verbal abuse have become a common issue. We present an approach to automatically classify such statements, using a new deep learning architecture. Our model comprises of a Multi Dimension Capsule Network that generates the representation of sentences which we use for classification. We further provide an analysis of our model's interpretation of such statements. We compare the results of our model with state-of-art classification algorithms and demonstrate our model's ability. It also has the capability to handle comments that are written in both Hindi and English, which are provided in the TRAC dataset. We also compare results on Kaggle's Toxic comment classification dataset",
    "volume": "workshop",
    "checked": true,
    "id": "7ff58034b10130a57186fe0764655d8ddc12ca09",
    "citation_count": 15
  },
  "https://aclanthology.org/W19-3518": {
    "title": "An Impossible Dialogue! Nominal Utterances and Populist Rhetoric in an Italian Twitter Corpus of Hate Speech against Immigrants",
    "abstract": "The paper proposes an investigation on the role of populist themes and rhetoric in an Italian Twitter corpus of hate speech against immigrants. The corpus had been annotated with four new layers of analysis: Nominal Utterances, that can be seen as consistent with populist rhetoric; In-out-group rhetoric, a very common populist strategy to polarize public opinion; Slogan-like nominal utterances, that may convey the call for severe illiberal policies against immigrants; News, to recognize the role of newspapers (headlines or reference to articles) in the Twitter political discourse on immigration featured by hate speech",
    "volume": "workshop",
    "checked": true,
    "id": "6402a29abf75c23d0e242aa41a5c6984aca2551d",
    "citation_count": 77
  },
  "https://aclanthology.org/W19-3519": {
    "title": "\"Condescending, Rude, Assholes\": Framing gender and hostility on Stack Overflow",
    "abstract": "The disciplines of Gender Studies and Data Science are incompatible. This is conventional wisdom, supported by how many computational studies simplify gender into an immutable binary categorization that appears crude to the critical social researcher. I argue that the characterization of gender norms is context specific and may prove valuable in constructing useful models. I show how gender can be framed in computational studies as a stylized repetition of acts mediated by a social structure, and not a possessed biological category. By conducting a review of existing work, I show how gender should be explored in multiplicity in computational research through clustering techniques, and layout how this is being achieved in a study in progress on gender hostility on Stack Overflow",
    "volume": "workshop",
    "checked": true,
    "id": "3080efd9bd5c92fabbe14ffb8523c5246e2a5679",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-3520": {
    "title": "Online aggression from a sociological perspective: An integrative view on determinants and possible countermeasures",
    "abstract": "The present paper introduces a theoretical model for explaining aggressive online comments from a sociological perspective. It is innovative as it combines individual, situational, and social-structural determinants of online aggression and tries to theoretically derive their interplay. Moreover, the paper suggests an empirical strategy for testing the model. The main contribution will be to match online commenting data with survey data containing rich background data of non- /aggressive online commentators",
    "volume": "workshop",
    "checked": true,
    "id": "b0f02471c8147b1053cda177bfbcfe2e74f018ed",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3601": {
    "title": "Development of a General Purpose Sentiment Lexicon for Igbo Language",
    "abstract": "There are publicly available general purpose sentiment lexicons in some high resource languages but very few exist in the low resource languages. This makes it difficult to directly perform sentiment analysis tasks in such languages. The objective of this work is to create a general purpose sentiment lexicon for Igbo language that can determine the sentiment of documents written in Igbo language without having to translate it to English language. The material used was an automatically translated Liu's lexicon and manual addition of Igbo native words. The result of this work is a general purpose lexicon – IgboSentilex. The performance was tested on the BBC Igbo news channel. It returned an average polarity agreement of 95% with other general purpose sentiment lexicons",
    "volume": "workshop",
    "checked": true,
    "id": "f5108d7f19d9c6169055f1572bdd51b9882c4139",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3602": {
    "title": "Towards a Resource Grammar for Runyankore and Rukiga",
    "abstract": "Currently, there is a lack of computational grammar resources for many under-resourced languages which limits the ability to develop Natural Language Processing (NLP) tools and applications such as Multilingual Document Authoring, Computer-Assisted Language Learning (CALL) and Low-Coverage Machine Translation (MT) for these languages. In this paper, we present our attempt to formalise the grammar of two such languages: Runyankore and Rukiga. For this formalisation we use the Grammatical Framework (GF) and its Resource Grammar Library (GF-RGL)",
    "volume": "workshop",
    "checked": true,
    "id": "bae7e69df3b2511681613c4cbec0d61b9d3cae22",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3603": {
    "title": "Speech Recognition for Tigrinya language Using Deep Neural Network Approach",
    "abstract": "This work presents a speech recognition model for Tigrinya language .The Deep Neural Network is used to make the recognition model. The Long Short-Term Memory Network (LSTM), which is a special kind of Recurrent Neural Network composed of Long Short-Term Memory blocks, is the primary layer of our neural network model. The 40-dimensional features are MFCC-LDA-MLLT-fMLLR with CMN were used. The acoustic models are trained on features that are obtained by projecting down to 40 dimensions using linear discriminant analysis (LDA). Moreover, speaker adaptive training (SAT) is done using a single feature-space maximum likelihood linear regression (FMLLR) transform estimated per speaker. We train and compare LSTM and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models. Finally, the accuracy of the model is evaluated based on the recognition rate",
    "volume": "workshop",
    "checked": true,
    "id": "250690cbc1fa79fff486270c5f5ef958daabfcd0",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3604": {
    "title": "Knowledge-Based Word Sense Disambiguation with Distributional Semantic Expansion",
    "abstract": "In this paper, we presented a WSD system that uses LDA topics for semantic expansion of document words. Our system also uses sense frequency information from SemCor to give higher priority to the senses which are more probable to happen",
    "volume": "workshop",
    "checked": true,
    "id": "aa7ada75b0424e12109169736c5a40e285bfb6b2",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3605": {
    "title": "AspeRa: Aspect-Based Rating Prediction Based on User Reviews",
    "abstract": "We propose a novel Aspect-based Rating Prediction model (AspeRa) that estimates user rating based on review texts for the items. It is based on aspect extraction with neural networks and combines the advantages of deep learning and topic modeling. It is mainly designed for recommendations, but an important secondary goal of AspeRa is to discover coherent aspects of reviews that can be used to explain predictions or for user profiling. We conduct a comprehensive empirical study of AspeRa, showing that it outperforms state-of-the-art models in terms of recommendation quality and produces interpretable aspects. This paper is an abridged version of our work (Nikolenko et al., 2019)",
    "volume": "workshop",
    "checked": true,
    "id": "eee91eb3c88e7008b1eb536d3f31a35e9811b1e3",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-3606": {
    "title": "Recognizing Arrow Of Time In The Short Stories",
    "abstract": "Recognizing the arrow of time in the context of paragraphs in short stories is a challenging task. i.e., given only two paragraphs (excerpted from a random position in a short story), determining which comes first and which comes next is a difficult task even for humans. In this paper, we have collected and curated a novel dataset for tackling this challenging task. We have shown that a pre-trained BERT architecture achieves reasonable accuracy on the task, and outperforms RNN-based architectures",
    "volume": "workshop",
    "checked": true,
    "id": "73b62f5db3becaa829a594652411ffe326258966",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3607": {
    "title": "Amharic Word Sequence Prediction",
    "abstract": "The significance of computers and handheld devices are not deniable in the modern world of today. Texts are entered to these devices using word processing programs as well as other techniques and word prediction is one of the techniques. Word Prediction is the action of guessing or forecasting what word comes after, based on some current information, and it is the main focus of this study. Even though Amharic is used by a large number of populations, no significant work is done on the topic of word sequence prediction. In this study, Amharic word sequence prediction model is developed with statistical methods using Hidden Markov Model by incorporating detailed Part of speech tag. Evaluation of the model is performed using developed prototype and keystroke savings (KSS) as a metrics. According to our experiment, prediction result using a bi-gram with detailed Part of Speech tag model has higher KSS and it is better compared to tri-gram model and better than those without Part of Speech tag. Therefore, statistical approach with Detailed POS has quite good potential on word sequence prediction for Amharic language. This research deals with designing word sequence prediction model in Amharic language. It is a language that is spoken in eastern Africa. One of the needs for Amharic word sequence prediction for mobile use and other digital devices is in order to facilitate data entry and communication in our language. Word sequence prediction is a challenging task for inflected languages. (Arora, 2007) These kinds of languages are morphologically rich and have enormous word forms. i.e. one word can have different forms. As Amharic language is highly inflected language and morphologically rich it shares this problem. (prediction, 2008) This problem makes word prediction system much more difficult and results poor performance. Due to this reason storing all forms in dictionary won't solve the problem as in English and other less inflected languages. But considering other techniques that could help the predictor to suggest the next word like a POS based prediction should be used. Previous researches used dictionary approach with no consideration of context information. Hence storing all forms of words in dictionary for inflected languages such as Amharic language has been less effective. The main goal of this thesis is to implement Amharic word prediction model that works with better prediction speed and with narrowed search space as much as possible. We introduced two models; tags and words and linear interpolation that use part of speech tag information in addition to word n-grams in order to maximize the likelihood of syntactic appropriateness of the suggestions. We believe the results found reflect this. Amharic word sequence prediction using bi-gram model with higher POS weight and detailed Part of speech tag gave better keystroke savings in all scenarios of our experiment. The study followed Design Science Research Methodology (DSRM). Since DSRM includes approaches, techniques, tools, algorithms and evaluation mechanisms in the process, we followed statistical approach with statistical language modeling and built Amharic prediction model based on information from Part of Speech tagger. The statistics included in the systems varies from single word frequencies to part-of-speech tag n-grams. That means it included the statistics of Word frequencies, Word sequence frequencies, Part-of-speech sequence frequencies and other important information. Later on the system was evaluated using Keystroke Savings. (Lindh, 011). Linux mint was used as the main Operation System during the frame work design. We used corpus of 680,000 tagged words that has 31 tag sets, python programming language and its libraries for both the part of speech tagger and the predictor module. Other Tool that was used is the SRILIM (The SRI language modeling toolkit) in order to generate unigram bigram and trigram count as an input for the language model. SRILIM is toolkit that uses to build and apply statistical language modeling. This thesis presented Amharic word sequence prediction model using the statistical approach. We described a combined statistical and lexical word prediction system for handling inflected languages by making use of POS tags to build the language model. We developed Amharic language models of bigram and trigram for the training purpose. We obtained 29% of KSS using bigram model with detailed part ofspeech tag. Hence, Based on the experiments carried out for this study and the results obtained, the following conclusions were made. We concluded that employing syntactic information in the form of Part-of-Speech (POS) n-grams promises more effective predictions. We also can conclude data quantity, performance of POS tagger and data quality highly affects the keystroke savings. Here in our study the tests were done on a small collection of 100 phrases. According to our evaluation better Keystroke saving (KSS) is achieved when using bi-gram model than the tri-gram models. We believe the results obtained using the experiment of detailed Part of speech tags were effective Since speed and search space are the basic issues in word sequence prediction",
    "volume": "workshop",
    "checked": true,
    "id": "02996b230117b7840eaa578f17bf56f5e6883d80",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3608": {
    "title": "A Framework for Relation Extraction Across Multiple Datasets in Multiple Domains",
    "abstract": "In this work, we aim to build a unifying framework for relation extraction (RE), applying this on 3 highly used datasets with the ability to be extendable to new datasets. At the moment, the domain suffers from lack of reproducibility as well as a lack of consensus on generalizable techniques. Our framework will be open-sourced and will aid in performing systematic exploration on the effect of different modeling techniques, pre-processing, training methodologies and evaluation metrics on the 3 datasets to help establish a consensus",
    "volume": "workshop",
    "checked": true,
    "id": "0cd833457c13f2d0bb9ae2109425aace1a756e7e",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3609": {
    "title": "Learning and Understanding Different Categories of Sexism Using Convolutional Neural Network's Filters",
    "abstract": "Sexism is very common in social media and makes the boundaries of free speech tighter for female users. Automatically flagging and removing sexist content requires niche identification and description of the categories. In this study, inspired by social science work, we propose three categories of sexism toward women as follows: \"Indirect sexism\", \"Sexual sexism\" and \"Physical sexism\". We build classifiers such as Convolutional Neural Network (CNN) to automatically detect different types of sexism and address problems of annotation. Even though inherent non-interpretability of CNN is a challenge for users who detect sexism, as the reason classifying a given speech instance with regard to sexism is difficult to glance from a CNN. However, recent research developed interpretable CNN filters for text data. In a CNN, filters followed by different activation patterns along with global max-pooling can help us tease apart the most important ngrams from the rest. In this paper, we interpret a CNN model trained to classify sexism in order to understand different categories of sexism by detecting semantic categories of ngrams and clustering them. Then, these ngrams in each category are used to improve the performance of the classification task. It is a preliminary work using machine learning and natural language techniques to learn the concept of sexism and distinguishes itself by looking at more precise categories of sexism in social media along with an in-depth investigation of CNN's filters",
    "volume": "workshop",
    "checked": true,
    "id": "1e87ab9ef48fef34de8f17257a3796f01516016f",
    "citation_count": 11
  },
  "https://aclanthology.org/W19-3610": {
    "title": "Modeling Five Sentence Quality Representations by Finding Latent Spaces Produced with Deep Long Short-Memory Models",
    "abstract": "We present a study in which we train neural models that approximate rules that assess the quality of English sentences. We modeled five rules using deep LSTMs trained over a dataset of sentences whose quality is evaluated under such rules. Preliminary results suggest the neural architecture can model such rules to high accuracy",
    "volume": "workshop",
    "checked": true,
    "id": "202f10a50fee695ad33a9d88ce4c2d52b0cf4e75",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3611": {
    "title": "English-Ethiopian Languages Statistical Machine Translation",
    "abstract": "In this paper, we describe an attempt towards the development of parallel corpora for English and Ethiopian Languages, such as Amharic, Tigrigna, Afan-Oromo, Wolaytta and Ge'ez. The corpora are used for conducting bi-directional SMT experiments. The BLEU scores of the bi-directional SMT systems show a promising result. The morphological richness of the Ethiopian languages has a great impact on the performance of SMT especially when the targets are Ethiopian languages",
    "volume": "workshop",
    "checked": true,
    "id": "e1835650d9ce78cb7ca5c563859e8d6c4e59b62e",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3612": {
    "title": "An automatic discourse relation alignment experiment on TED-MDB",
    "abstract": "This paper describes an automatic discourse relation alignment experiment as an empirical justification of the planned annotation projection approach to enlarge the 3600-word multilingual corpus of TED Multilingual Discourse Bank (TED-MDB). The experiment is carried out on a single language pair (English-Turkish) included in TED-MDB. The paper first describes the creation of a large corpus of English-Turkish bi-sentences, then it presents a sense-based experiment that automatically aligns the relations in the English sentences of TED-MDB with the Turkish sentences. The results are very close to the results obtained from an earlier semi-automatic post-annotation alignment experiment validated by human annotators and are encouraging for future annotation projection tasks",
    "volume": "workshop",
    "checked": true,
    "id": "a2ad4db38fba4c4bc19a158e3d946e0419e53186",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-3613": {
    "title": "The Design and Construction of the Corpus of China English",
    "abstract": "The paper describes the development a corpus of an English variety, i.e. China English, in or-der to provide a linguistic resource for researchers in the field of China English. The Corpus of China English (CCE) was built with due consideration given to its representativeness and authenticity. It was composed of more than 13,962,102 tokens in 15,333 texts evenly divided between the following four genres: newspapers, magazines, fiction and academic writings. The texts cover a wide range of domains, such as news, financial, politics, environment, social, culture, technology, sports, education, philosophy, literary, etc. It is a helpful resource for research on China English, computational linguistics, natural language processing, corpus linguistics and English language education",
    "volume": "workshop",
    "checked": true,
    "id": "e99bf2b3714b8c552ffc13d170757e0712115bf4",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3614": {
    "title": "Learning Trilingual Dictionaries for Urdu – Roman Urdu – English",
    "abstract": "In this paper, we present an effort to generate a joint Urdu, Roman Urdu and English trilingual lexicon using automated methods. We make a case for using statistical machine translation approaches and parallel corpora for dictionary creation. To this purpose, we use word alignment tools on the corpus and evaluate translations using human evaluators. Despite different writing script and considerable noise in the corpus our results show promise with over 85% accuracy of Roman Urdu–Urdu and 45% English–Urdu pairs",
    "volume": "workshop",
    "checked": true,
    "id": "9292114ad24a5e93bd395bd94e981ef10765196f",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3615": {
    "title": "Joint Inference on Bilingual Parse Trees for PP-attachment Disambiguation",
    "abstract": "Prepositional Phrase (PP) attachment is a classical problem in NLP for languages like English, which suffer from structural ambiguity. In this work, we solve this problem with the help of another language free from such ambiguities, using the parse tree of the parallel sentence in the other language, and word alignments. We formulate an optimization framework that encourages agreement between the parse trees for two languages, and solve it using a novel Dual Decomposition (DD) based algorithm. Experiments on the English-Hindi language pair show promising improvements over the baseline",
    "volume": "workshop",
    "checked": true,
    "id": "10e717e0f42f1393404d616a71c5cdf6aca49496",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3616": {
    "title": "Using Attention-based Bidirectional LSTM to Identify Different Categories of Offensive Language Directed Toward Female Celebrities",
    "abstract": "Social media posts reflect the emotions, intentions and mental state of the users. Twitter users who harass famous female figures may do so with different intentions and intensities. Recent studies have published datasets focusing on different types of online harassment, vulgar language, and emotional intensities. We trained, validate and test our proposed model, attention-based bidirectional neural network, on the three datasets:\"online harassment\", \"vulgar language\" and \"valance\" and achieved state of the art performance in two of the datasets. We report F1 score for each dataset separately along with the final precision, recall and macro-averaged F1 score. In addition, we identify ten female figures from different professions and racial backgrounds who have experienced harassment on Twitter. We tested the trained models on ten collected corpuses each related to one famous female figure to predict the type of harassing language, the type of vulgar language and the degree of intensity of language occurring on their social platforms. Interestingly, the achieved results show different patterns of linguistic use targeting different racial background and occupations. The contribution of this study is two-fold. From the technical perspective, our proposed methodology is shown to be effective with a good margin in comparison to the previous state-of-the-art results on one of the two available datasets. From the social perspective, we introduce a methodology which can unlock facts about the nature of offensive language targeting women on online social platforms. The collected dataset will be shared publicly for further investigation",
    "volume": "workshop",
    "checked": true,
    "id": "7d8dc4ec7d4c005d33cacac0d5bbce273767b6bb",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-3617": {
    "title": "Sentiment Analysis Model for Opinionated Awngi Text: Case of Music Reviews",
    "abstract": "Abstract The analysis of sentiments is imperative to make a decision for individuals, organizations, and governments. Due to the rapid growth of Awngi (Agew) text on the web, there is no available corpus annotated for sentiment analysis. In this paper, we present a SA model for the Awngi language spoken in Ethiopia, by using a supervised machine learning approach. We developed our corpus by collecting around 1500 posts from online sources. This research is begun to build and evaluate the model for opinionated Awngi music reviews. Thus, pre-processing techniques have been employed to clean the data, to convert transliterations to the native Ethiopic script for accessibility and convenience to typing and to change the words to their base form by removing the inflectional morphemes. After pre-processing, the corpus is manually annotated by three the language professional for giving polarity, and rate, their level of confidence in their selection and sentiment intensity scale values. To improve the calculation method of feature selection and weighting and proposed a more suitable SA algorithm for feature extraction named CHI and weight calculation named TF IDF, increasing the proportion and weight of sentiment words in the feature words. We employed Support Vector Machines (SVM), Naïve Bayes (NB) and Maximum Entropy (MxEn) machine learning algorithms. Generally, the results are encouraging, despite the morphological challenge in Awngi, the data cleanness and small size of data. We are believed that the results could improve further with a larger corpus",
    "volume": "workshop",
    "checked": true,
    "id": "74493d528e2a8214d30e62c83a528d09c95d08f1",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3618": {
    "title": "A compositional view of questions",
    "abstract": "We present a research on compositional treatment of questions in neo-davidsonian event semantics style. Our work is based on (Champollion, 2011) where only declarative sentences were considered. Our research is based on complex formal examples, paving the way towards further research in this domain and further testing on real-life corpora",
    "volume": "workshop",
    "checked": true,
    "id": "e638993eb8633a413893f569ed79ecfe027bcc60",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3619": {
    "title": "Controlling the Specificity of Clarification Question Generation",
    "abstract": "Unlike comprehension-style questions, clarification questions look for some missing information in a given context. However, without guidance, neural models for question generation, similar to dialog generation models, lead to generic and bland questions that cannot elicit useful information. We argue that controlling the level of specificity of the generated questions can have useful applications and propose a neural clarification question generation model for the same. We first train a classifier that annotates a clarification question with its level of specificity (generic or specific) to the given context. Our results on the Amazon questions dataset demonstrate that training a clarification question generation model on specificity annotated data can generate questions with varied levels of specificity to the given context",
    "volume": "workshop",
    "checked": true,
    "id": "fd3a2e61d0cb1ff2d63a757c8571a2859709a10b",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-3620": {
    "title": "Non-Monotonic Sequential Text Generation",
    "abstract": "Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy's own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order while achieving competitive performance with conventional left-to-right generation",
    "volume": "workshop",
    "checked": true,
    "id": "a9a4a12900d0ac1063505bb28d5e0180d78a39a0",
    "citation_count": 95
  },
  "https://aclanthology.org/W19-3621": {
    "title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them",
    "abstract": "Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between \"gender-neutralized\" words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling",
    "volume": "workshop",
    "checked": true,
    "id": "94cf3f2c4410fcb06a90abebd99f7113c69e1ed9",
    "citation_count": 362
  },
  "https://aclanthology.org/W19-3622": {
    "title": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?",
    "abstract": "Many natural languages assign grammatical gender also to inanimate nouns in the language. In such languages, words that relate to the gender-marked nouns are inflected to agree with the noun's gender. We show that this affects the word representations of inanimate nouns, resulting in nouns with the same gender being closer to each other than nouns with different gender. While \"embedding debiasing\" methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words' context when training word embeddings is effective in removing it. Fixing the grammatical gender bias results in a positive effect on the quality of the resulting word embeddings, both in monolingual and cross lingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results",
    "volume": "workshop",
    "checked": true,
    "id": "d5ff69bdd782101bc4ad11633d02af5ec8f9387c",
    "citation_count": 15
  },
  "https://aclanthology.org/W19-3623": {
    "title": "Automatic Product Categorization for Official Statistics",
    "abstract": "The North American Product Classification System (NAPCS) is a comprehensive, hierarchical classification system for products (goods and services) that is consistent across the three North American countries. Beginning in 2017, the Economic Census will use NAPCS to produce economy-wide product tabulations. Respondents are asked to report data from a long, pre-specified list of potential products in a given industry, with some lists containing more than 50 potential products. Businesses have expressed the desire to alternatively supply Universal Product Codes (UPC) to the U. S. Census Bureau. Much work has been done around the categorization of products using product descriptions. No study has applied these efforts for the calculation of official statistics (statistics published by government agencies) using only the text of UPC product descriptions. The question we address in this paper is: Given UPC codes and their associated product descriptions, can we accurately predict NAPCS? We tested the feasibility of businesses submitting a spreadsheet with Universal Product Codes and their associated text descriptions. This novel strategy classified text with very high accuracy rates, all of our algorithms surpassed over 90 percent",
    "volume": "workshop",
    "checked": true,
    "id": "2904f6eb45bbf232b90451007a8db71e3aeea279",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3624": {
    "title": "An Online Topic Modeling Framework with Topics Automatically Labeled",
    "abstract": "In this paper, we propose a novel online topic tracking framework, named IEDL, for tracking the topic changes related to deep learning techniques on Stack Exchange and automatically interpreting each identified topic. The proposed framework combines the prior topic distributions in a time window during inferring the topics in current time slice, and introduces a new ranking scheme to select most representative phrases and sentences for the inferred topics in each time slice. Experiments on 7,076 Stack Exchange posts show the effectiveness of IEDL in tracking topic changes and labeling topics",
    "volume": "workshop",
    "checked": true,
    "id": "820eb27a5ec7c44faf5cbe656aa917e4fa525dee",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3625": {
    "title": "Construction and Alignment of Multilingual Entailment Graphs for Semantic Inference",
    "abstract": "This paper presents ongoing work on the construction and alignment of predicate entailment graphs in English and German. We extract predicate-argument pairs from large corpora of monolingual English and German news text and construct monolingual paraphrase clusters and entailment graphs. We use an aligned subset of entities to derive the bilingual alignment of entities and relations, and achieve better than baseline results on a translated subset of a predicate entailment data set (Levy and Dagan, 2016) and the German portion of XNLI (Conneau et al., 2018)",
    "volume": "workshop",
    "checked": true,
    "id": "4e5ada875d3933082bff6393e7fcc4604e01f1f4",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3626": {
    "title": "KB-NLG: From Knowledge Base to Natural Language Generation",
    "abstract": "We perform the natural language generation (NLG) task by mapping sets of Resource Description Framework (RDF) triples into text. First we investigate the impact of increasing the number of entity types in delexicalisaiton on the generation quality. Second we conduct different experiments to evaluate two widely applied language generation systems, encoder-decoder with attention and the Transformer model on a large benchmark dataset. We evaluate different models on automatic metrics, as well as the training time. To our knowledge, we are the first to apply Transformer model to this task",
    "volume": "workshop",
    "checked": true,
    "id": "d5010c762c8dea4c55aeb5e9fd7b3675aa7480df",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-3627": {
    "title": "Acoustic Characterization of Singaporean Children's English: Comparisons to American and British Counterparts",
    "abstract": "We investigate English pronunciation patterns in Singaporean children in relation to their American and British counterparts by conducting archetypal analysis on selected vowel pairs. Given that Singapore adopts British English as the institutional standard, one might expect Singaporean children to follow British pronunciation patterns, but we observe that Singaporean children also present similar patterns to Americans for TRAP-BATH spilt vowels: (1) British and Singaporean children both produce these vowels with a relatively lowered tongue height. (2) These vowels are more fronted for American and Singaporean children (p < 0.001). In addition, when comparing /æ/ and /ε/ productions, British speakers show the clearest distinction between the two vowels; Singaporean and American speakers exhibit a higher and more fronted tongue position for /æ/ (p < 0.001), causing /æ/ to be acoustically more similar to /ε/",
    "volume": "workshop",
    "checked": true,
    "id": "6839bb929551487cd1ac5ca427ac15a8b51cf78e",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3628": {
    "title": "Rethinking Phonotactic Complexity",
    "abstract": "In this work, we propose the use of phone-level language models to estimate phonotactic complexity—measured in bits per phoneme—which makes cross-linguistic comparison straightforward. We compare the entropy across languages using this simple measure, gaining insight on how complex different language's phonotactics are. Finally, we show a very strong negative correlation between phonotactic complexity and the average length of words—Spearman rho=-0.744—when analysing a collection of 106 languages with 1016 basic concepts each",
    "volume": "workshop",
    "checked": true,
    "id": "57c84316638959fd862bd71aeb40b49a3fec8ba2",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3629": {
    "title": "Implementing a Multi-lingual Chatbot for Positive Reinforcement in Young Learners",
    "abstract": "This is a humanitarian work –a counter-terrorism effort. The presentation describes the experiences of developing a multi-lingua, interactive chatbot trained on the corpus of two Nigerian Languages (Hausa and Fulfude), with simultaneous translation to a third (Kanuri), to stimulate conversations, deliver tailored contents to the users thereby aiding in the detection of the probability and degree of radicalization in young learners through data analysis of the games moves and vocabularies. As chatbots have the ability to simulate a human conversation based on rhetorical behavior, the system is able to learn the need of individual user through constant interaction and deliver tailored contents that promote good behavior in Hausa, Fulfulde and Kanuri languages",
    "volume": "workshop",
    "checked": true,
    "id": "121ec203aa16e93c5408d9f39711d7eba15520d8",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3630": {
    "title": "A Deep Learning Approach to Language-independent Gender Prediction on Twitter",
    "abstract": "This work presents a set of experiments conducted to predict the gender of Twitter users based on language-independent features extracted from the text of the users' tweets. The experiments were performed on a version of TwiSty dataset including tweets written by the users of six different languages: Portuguese, French, Dutch, English, German, and Italian. Logistic regression (LR), and feed-forward neural networks (FFNN) with back-propagation were used to build models in two different settings: Inter-Lingual (IL) and Cross-Lingual (CL). In the IL setting, the training and testing were performed on the same language whereas in the CL, Italian and German datasets were set aside and only used as test sets and the rest were combined to compose training and development sets. In the IL, the highest accuracy score belongs to LR whereas, in the CL, FFNN with three hidden layers yields the highest score. The results show that neural network based models underperform traditional models when the size of the training set is small; however, they beat traditional models by a non-trivial margin, when they are fed with large enough data. Finally, the feature analysis confirms that men and women have different writing styles independent of their language",
    "volume": "workshop",
    "checked": true,
    "id": "ea05643249b5972098f1f4977b4b8fa8fbae4f22",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-3631": {
    "title": "Isolating the Effects of Modeling Recursive Structures: A Case Study in Pronunciation Prediction of Chinese Characters",
    "abstract": "Finding that explicitly modeling structures leads to better generalization, we consider the task of predicting Cantonese pronunciations of logographs (Chinese characters) using logographs' recursive structures. This task is a suitable case study for two reasons. First, logographs' pronunciations depend on structures (i.e. the hierarchies of sub-units in logographs) Second, the quality of logographic structures is consistent since the structures are constructed automatically using a set of rules. Thus, this task is less affected by confounds such as varying quality between annotators. Empirical results show that modeling structures explicitly using treeLSTM outperforms LSTM baseline, reducing prediction error by 6.0% relative",
    "volume": "workshop",
    "checked": true,
    "id": "708e4d8dedf497e371c112a8f5b8ccde0fdc2485",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3632": {
    "title": "Benchmarking Neural Machine Translation for Southern African Languages",
    "abstract": "Unlike major Western languages, most African languages are very low-resourced. Furthermore, the resources that do exist are often scattered and difficult to obtain and discover. As a result, the data and code for existing research has rarely been shared, meaning researchers struggle to reproduce reported results, and almost no publicly available benchmarks or leaderboards for African machine translation models exist. To start to address these problems, we trained neural machine translation models for a subset of Southern African languages on publicly-available datasets. We provide the code for training the models and evaluate the models on a newly released evaluation set, with the aim of starting a leaderboard for Southern African languages and spur future research in the field",
    "volume": "workshop",
    "checked": true,
    "id": "b5c9a84f0cd6085d0e691804635e1cff4a3a3f6f",
    "citation_count": 16
  },
  "https://aclanthology.org/W19-3633": {
    "title": "OCR Quality and NLP Preprocessing",
    "abstract": "We present initial experiments to evaluate the performance of tasks such as Part of Speech Tagging on data corrupted by Optical Character Recognition (OCR). Our results, based on English and German data, using artificial experiments as well as initial real OCRed data indicate that already a small drop in OCR quality considerably increases the error rates, which would have a significant impact on subsequent processing steps",
    "volume": "workshop",
    "checked": true,
    "id": "b9c37532fb0466dd09a204f530c792c6139aad4f",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-3634": {
    "title": "Developing a Fine-grained Corpus for a Less-resourced Language: the case of Kurdish",
    "abstract": "Kurdish is a less-resourced language consisting of different dialects written in various scripts. Approximately 30 million people in different countries speak the language. The lack of corpora is one of the main obstacles in Kurdish language processing. In this paper, we present KTC-the Kurdish Textbooks Corpus, which is composed of 31 K-12 textbooks in Sorani dialect. The corpus is normalized and categorized into 12 educational subjects containing 693,800 tokens (110,297 types). Our resource is publicly available for non-commercial use under the CC BY-NC-SA 4.0 license",
    "volume": "workshop",
    "checked": true,
    "id": "43da7985fafaeb1588652f1c8f41ed475b8a6039",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-3635": {
    "title": "Amharic Question Answering for Biography, Definition, and Description Questions",
    "abstract": null,
    "volume": "workshop",
    "checked": true,
    "id": "1bd1e266a1eaa3aeafcf94d6bef51a985175ad7f",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-3636": {
    "title": "Polysemous Language in Child Directed Speech",
    "abstract": "Polysemous Language in Child Directed Speech Learning the meaning of words is one of the fundamental building blocks of verbal communication. Models of child language acquisition have generally made the simplifying assumption that each word appears in child-directed speech with a single meaning. To understand naturalistic word learning during childhood, it is essential to know whether children hear input that is in fact constrained to single meaning per word, or whether the environment naturally contains multiple senses.In this study, we use a topic modeling approach to automatically induce word senses from child-directed speech. Our results confirm the plausibility of our automated analysis approach and reveal an increasing rate of using multiple senses in child-directed speech, starting with corpora from children as early as the first year of life",
    "volume": "workshop",
    "checked": true,
    "id": "84b55acdb5194d5bb2b68b136eb09ed2bdef12ac",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3637": {
    "title": "Principled Frameworks for Evaluating Ethics in NLP Systems",
    "abstract": "We critique recent work on ethics in natural language processing. Those discussions have focused on data collection, experimental design, and interventions in modeling. But we argue that we ought to first understand the frameworks of ethics that are being used to evaluate the fairness and justice of algorithmic systems. Here, we begin that discussion by outlining deontological and consequentialist ethics, and make predictions on the research agenda prioritized by each",
    "volume": "workshop",
    "checked": true,
    "id": "255739eb8e68e99a6523380e1b7838813fa1bfcb",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-3638": {
    "title": "Understanding the Shades of Sexism in Popular TV Series",
    "abstract": "[Multiple-submission] In the midst of a generation widely exposed to and influenced by media entertainment, the NLP research community has shown relatively little attention on the sexist comments in popular TV series. To understand sexism in TV series, we propose a way of collecting distant supervision dataset using Character Persona information with the psychological theories on sexism. We assume that sexist characters from TV shows are more prone to making sexist comments when talking about women, and show that this hypothesis is valid through experiment. Finally, we conduct an interesting analysis on popular TV show characters and successfully identify different shades of sexism that is often overlooked",
    "volume": "workshop",
    "checked": true,
    "id": "ebc1561be5dc53460fe2cdcd8a6ea0f6168ae830",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3639": {
    "title": "Evaluating Ways of Adapting Word Similarity",
    "abstract": "People judge pairwise similarity by deciding which aspects of the words' meanings are relevant for the comparison of the given pair. However, computational representations of meaning rely on dimensions of the vector representation for similarity comparisons, without considering the specific pairing at hand. Prior work has adapted computational similarity judgments by using the softmax function in order to address this limitation by capturing asymmetry in human judgments. We extend this analysis by showing that a simple modification of cosine similarity offers a better correlation with human judgments over a comprehensive dataset. The modification performs best when the similarity between two words is calculated with reference to other words that are most similar and dissimilar to the pair",
    "volume": "workshop",
    "checked": true,
    "id": "449b4eb26fb27b654626215467598ad2b881fdd6",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3640": {
    "title": "Exploring the Use of Lexicons to aid Deep Learning towards the Detection of Abusive Language",
    "abstract": "Detecting abusive language is a significant research topic, which has received a lot of attention recently. Our work focused on detecting personal attacks in online conversations. State-of-the-art research on this task has largely used deep learning with word embeddings. We explored the use of sentiment lexicons as well as semantic lexicons towards improving the accuracy of the baseline Convolutional Neural Network (CNN) using regular word embeddings. This is a work in progress, limited by time constraints and appropriate infrastructure. Our preliminary results showed promise for utilizing lexicons, especially semantic lexicons, for the task of detecting abusive language",
    "volume": "workshop",
    "checked": true,
    "id": "84c98ed014cf5ae77fdbb615061bebdceedd819a",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-3641": {
    "title": "Entity-level Classification of Adverse Drug Reactions: a Comparison of Neural Network Models",
    "abstract": "This paper presents our experimental work on exploring the potential of neural network models developed for aspect-based sentiment analysis for entity-level adverse drug reaction (ADR) classification. Our goal is to explore how to represent local context around ADR mentions and learn an entity representation, interacting with its context. We conducted extensive experiments on various sources of text-based information, including social media, electronic health records, and abstracts of scientific articles from PubMed. The results show that Interactive Attention Neural Network (IAN) outperformed other models on four corpora in terms of macro F-measure. This work is an abridged version of our recent paper accepted to Programming and Computer Software journal in 2019",
    "volume": "workshop",
    "checked": true,
    "id": "9635a103b81eed92d883ecf4adaf83be5040d724",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-3642": {
    "title": "Context Effects on Human Judgments of Similarity",
    "abstract": "The semantic similarity of words forms the basis of many natural language processing methods. These computational similarity measures are often based on a mathematical comparison of vector representations of word meanings, while human judgments of similarity differ in lacking geometrical properties, e.g., symmetric similarity and triangular similarity. In this study, we propose a novel task design to further explore human behavior by asking whether a pair of words is deemed more similar depending on an immediately preceding judgment. Results from a crowdsourcing experiment show that people consistently judge words as more similar when primed by a judgment that evokes a relevant relationship. Our analysis further shows that word2vec similarity correlated significantly better with the out-of-context judgments, thus confirming the methodological differences in human-computer judgments, and offering a new testbed for probing the differences",
    "volume": "workshop",
    "checked": true,
    "id": "97da663ff2e123e3b85b9d7064572285235d717d",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3643": {
    "title": "NLP Automation to Read Radiological Reports to Detect the Stage of Cancer Among Lung Cancer Patients",
    "abstract": "A common challenge in the healthcare industry today is physicians have access to massive amounts of healthcare data but have little time and no appropriate tools. For instance, the risk prediction model generated by logistic regression could predict the probability of diseases occurrence and thus prioritizing patients' waiting list for further investigations. However, many medical reports available in current clinical practice system are not yet ready for analysis using either statistics or machine learning as they are in unstructured text format. The complexity of medical information makes the annotation or validation of data very challenging and thus acts as a bottleneck to apply machine learning techniques in medical data. This study is therefore conducted to create such annotations automatically where the computer can read radiological reports for oncologists and mark the staging of lung cancer. This staging information is obtained using the rule-based method implemented using the standards of Tumor Node Metastasis (TNM) staging along with deep learning technology called Long Short Term Memory (LSTM) to extract clinical information from the Computed Tomography (CT) text report. The empirical experiment shows promising results being the accuracy of up to 85%",
    "volume": "workshop",
    "checked": true,
    "id": "47c257eae71a49d8959c2a0ac539c1f87fd1ae10",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-3644": {
    "title": "Augmenting Named Entity Recognition with Commonsense Knowledge",
    "abstract": "Commonsense can be vital in some applications like Natural Language Understanding (NLU), where it is often required to resolve ambiguity arising from implicit knowledge and underspecification. In spite of the remarkable success of neural network approaches on a variety of Natural Language Processing tasks, many of them struggle to react effectively in cases that require commonsense knowledge. In the present research, we take advantage of the availability of the open multilingual knowledge graph ConceptNet, by using it as an additional external resource in Named Entity Recognition (NER). Our proposed architecture involves BiLSTM layers combined with a CRF layer that was augmented with some features such as pre-trained word embedding layers and dropout layers. Moreover, apart from using word representations, we used also character-based representation to capture the morphological and the orthographic information. Our experiments and evaluations showed an improvement in the overall performance with +2.86 in the F1-measure. Commonsense reasonnig has been employed in other studies and NLP tasks but to the best of our knowledge, there is no study relating the integration of a commonsense knowledge base in NER",
    "volume": "workshop",
    "checked": true,
    "id": "c80fecc35eb0f26bd1e8af02e53234ac830b63ff",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-3645": {
    "title": "Pardon the Interruption: Automatic Analysis of Gender and Competitive Turn-Taking in United States Supreme Court Hearings",
    "abstract": "The United States Supreme Court plays a key role in defining the legal basis for gender discrimination throughout the country, yet there are few checks on gender bias within the court itself. In conversational turn-taking, interruptions have been documented as a marker of bias between speakers of different genders. The goal of this study is to automatically differentiate between respectful and disrespectful conversational turns taken during official hearings, which could help in detecting bias and finding remediation techniques for discourse in the courtroom. In this paper, I present a corpus of turns annotated by legal professionals, and describe the design of a semi-supervised classifier that will use acoustic and lexical features to analyze turn-taking at scale. On completion of annotations, this classifier will be trained to extract the likelihood that turns are respectful or disrespectful for use in studies of speech trends",
    "volume": "workshop",
    "checked": true,
    "id": "a7e1c57ecf37c774810ea121b079cfeb996bdbfd",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-3646": {
    "title": "Evaluating Coherence in Dialogue Systems using Entailment",
    "abstract": "Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses",
    "volume": "workshop",
    "checked": true,
    "id": "5e63fbf6e2051b6bc279eb246ac9263f2b47bce7",
    "citation_count": 57
  },
  "https://aclanthology.org/W19-3647": {
    "title": "Exploiting machine algorithms in vocalic quantification of African English corpora",
    "abstract": "Towards procedural fidelity in the processing of African English speech corpora, this work demonstrates how the adaptation of machine-assisted segmentation of phonemes and automatic extraction of acoustic values can significantly speed up the processing of naturalistic data and make the vocalic analysis of the varieties less impressionistic. Research in African English phonology has, till date, been least data-driven – much less the use of comparative corpora for cross-varietal assessments. Using over 30 hours of naturalistic data (from 28 speakers in 5 Nigerian cities), the procedures for segmenting audio files into phonemic units via the Munich Automatic Segmentation System (MAUS), and the extraction of their spectral values in Praat are explained. Evidence from the speech corpora supports a more complex vocalic inventory than attested in previous auditory/manual-based accounts – thus reinforcing the resourcefulness of the algorithms for the current data and cognate varieties. Keywords: machine algorithms; naturalistic data; African English phonology; vowel segmentation",
    "volume": "workshop",
    "checked": true,
    "id": "3233df2d157c880223775426fc6fbf00fefc1576",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3648": {
    "title": "Assessing the Ability of Neural Machine Translation Models to Perform Syntactic Rewriting",
    "abstract": "We describe work in progress for evaluating performance of sequence-to-sequence neural networks on the task of syntax-based reordering for rules applicable to simultaneous machine translation. We train models that attempt to rewrite English sentences using rules that are commonly used by human interpreters. We examine the performance of these models to determine which forms of rewriting are more difficult for them to learn and which architectures are the best at learning them",
    "volume": "workshop",
    "checked": true,
    "id": "244b3be1e1fc6880a95ae4e52581c74942bb750b",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3649": {
    "title": "Authorship Recognition with Short-Text using Graph-based Techniques",
    "abstract": "In recent years, studies of authorship recognition has aroused great interest in graph-based analysis. Modeling the writing style of each author using a network of co-occurrence words. However, short texts can generate some changes in the topology of network that cause impact on techniques of feature extraction based on graph topology. In this work, we evaluate the robustness of global-strategy and local-strategy based on complex network measurements comparing with graph2vec a graph embedding technique based on skip-gram model. The experiment consists of evaluating how each modification in the length of text affects the accuracy of authorship recognition on both techniques using cross-validation and machine learning techniques",
    "volume": "workshop",
    "checked": true,
    "id": "63c00c40acfc668383a5be9370d2f69846c94078",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3650": {
    "title": "A Parallel Corpus Mixtec-Spanish",
    "abstract": "This work is about the compilation process of parallel documents Spanish-Mixtec. There are not many Spanish-Mixec parallel texts and most of the sources are non-digital books. Due to this, we need to face the errors when digitizing the sources and difficulties in sentence alignment, as well as the fact that does not exist a standard orthography. Our parallel corpus consists of sixty texts coming from books and digital repositories. These documents belong to different domains: history, traditional stories, didactic material, recipes, ethnographical descriptions of each town and instruction manuals for disease prevention. We have classified this material in five major categories: didactic (6 texts), educative (6 texts), interpretative (7 texts), narrative (39 texts), and poetic (2 texts). The final total of tokens is 49,814 Spanish words and 47,774 Mixtec words. The texts belong to the states of Oaxaca (48 texts), Guerrero (9 texts) and Puebla (3 texts). According to this data, we see that the corpus is unbalanced in what refers to the representation of the different territories. While 55% of speakers are in Oaxaca, 80% of texts come from this region. Guerrero has the 30% of speakers and the 15% of texts and Puebla, with the 15% of the speakers has a representation of the 5% in the corpus",
    "volume": "workshop",
    "checked": true,
    "id": "2dc24d359b47e8dbf3cc81a4ba2f845769f6b2fd",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3651": {
    "title": "Emoji Usage Across Platforms: A Case Study for the Charlottesville Event",
    "abstract": "We study emoji usage patterns across two social media platforms, one of them considered a fringe community called Gab, and the other Twitter. We find that Gab tends to comparatively use more emotionally charged emoji, but also seems more apathetic towards the violence during the event, while Twitter takes a more empathetic approach to the event",
    "volume": "workshop",
    "checked": true,
    "id": "793f9e6d17f89d5144f96de082ad7dde4878ec1d",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3652": {
    "title": "Reading KITTY: Pitch Range as an Indicator of Reading Skill",
    "abstract": "While affective outcomes are generally positive for the use of eBooks and computer-based reading tutors in teaching children to read, learning outcomes are often poorer (Korat and Shamir, 2004). We describe the first iteration of Reading Kitty, an iOS application that uses NLP and speech processing to focus children's time on close reading and prosody in oral reading, while maintaining an emphasis on creativity and artifact creation. We also share preliminary results demonstrating that pitch range can be used to automatically predict readers' skill level",
    "volume": "workshop",
    "checked": true,
    "id": "dff6b19d1338b4c9d151904b170d6ad742c90f0c",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4824": {
    "title": "Adversarial Attack on Sentiment Classification",
    "abstract": "In this paper, we propose a white-box attack algorithm called \"Global Search\" method and compare it with a simple misspelling noise and a more sophisticated and common white-box attack approach called \"Greedy Search\". The attack methods are evaluated on the Convolutional Neural Network (CNN) sentiment classifier trained on the IMDB movie review dataset. The attack success rate is used to evaluate the effectiveness of the attack methods and the perplexity of the sentences is used to measure the degree of distortion of the generated adversarial examples. The experiment results show that the proposed \"Global Search\" method generates more powerful adversarial examples with less distortion or less modification to the source text",
    "volume": "workshop",
    "checked": true,
    "id": "9ff7f170db0ef2fc7ffbbc6776f55dcf24edfa85",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-3654": {
    "title": "CSI Peru News: finding the culprit, victim and location in news articles",
    "abstract": "We introduce a shift on the DS method over the domain of crime-related news from Peru, attempting to find the culprit, victim and location of a crime description from a RE perspective. Obtained results are highly promising and show that proposed modifications are effective in non-traditional domains",
    "volume": "workshop",
    "checked": true,
    "id": "43697257cf6e2c06b41b3f9fdf6ad93ba4b0aa04",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3655": {
    "title": "Exploring Social Bias in Chatbots using Stereotype Knowledge",
    "abstract": "Exploring social bias in chatbot is an important, yet relatively unexplored problem. In this paper, we propose an approach to understand social bias in chatbots by leveraging stereotype knowledge. It allows interesting comparison of bias between chatbots and humans, and provides intuitive analysis of existing chatbots by borrowing the finer-grain concepts of sexism and racism",
    "volume": "workshop",
    "checked": true,
    "id": "eac7c7dc2f2682af5ddc1093d516ca6a5f62c891",
    "citation_count": 25
  },
  "https://aclanthology.org/W19-3656": {
    "title": "Cross-Sentence Transformations in Text Simplification",
    "abstract": "Current approaches to Text Simplification focus on simplifying sentences individually. However, certain simplification transformations span beyond single sentences (e.g. joining and re-ordering sentences). In this paper, we motivate the need for modelling the simplification task at the document level, and assess the performance of sequence-to-sequence neural models in this setup. We analyse parallel original-simplified documents created by professional editors and show that there are frequent rewriting transformations that are not restricted to sentence boundaries. We also propose strategies to automatically evaluate the performance of a simplification model on these cross-sentence transformations. Our experiments show the inability of standard sequence-to-sequence neural models to learn these transformations, and suggest directions towards document-level simplification",
    "volume": "workshop",
    "checked": true,
    "id": "72b8431703196f156bf707ce0209e7637ccd079b",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3701": {
    "title": "Unsupervised Induction of Ukrainian Morphological Paradigms for the New Lexicon: Extending Coverage for Named Entities and Neologisms using Inflection Tables and Unannotated Corpora",
    "abstract": "The paper presents an unsupervised method for quickly extending a Ukrainian lexicon by generating paradigms and morphological feature structures for new Named Entities and neologisms, which are not covered by existing static morphological resources. This approach addresses a practical problem of modelling paradigms for entities created by the dynamic processes in the lexicon: this problem is especially serious for highly-inflected languages in domains with specialised or quickly changing lexicon. The method uses an unannotated Ukrainian corpus and a small fixed set of inflection tables, which can be found in traditional grammar textbooks. The advantage of the proposed approach is that updating the morphological lexicon does not require training or linguistic annotation, allowing fast knowledge-light extension of an existing static lexicon to improve morphological coverage on a specific corpus. The method is implemented in an open-source package on a GitHub repository. It can be applied to other low-resourced inflectional languages which have internet corpora and linguistic descriptions of their inflection system, following the example of inflection tables for Ukrainian. Evaluation results shows consistent improvements in coverage for Ukrainian corpora of different corpus types",
    "volume": "workshop",
    "checked": true,
    "id": "be6925495e795cc1ee0e6287dbbddbd4dbc6036d",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3702": {
    "title": "Multiple Admissibility: Judging Grammaticality using Unlabeled Data in Language Learning",
    "abstract": "We present our work on the problem of Multiple Admissibility (MA) in language learning. Multiple Admissibility occurs in many languages when more than one grammatical form of a word fits syntactically and semantically in a given context. In second language (L2) education - in particular, in intelligent tutoring systems/computer-aided language learning (ITS/CALL) systems, which generate exercises automatically - this implies that multiple alternative answers are possible. We treat the problem as a grammaticality judgement task. We train a neural network with an objective to label sentences as grammatical or ungrammatical, using a \"simulated learner corpus\": a dataset with correct text, and with artificial errors generated automatically. While MA occurs commonly in many languages, this paper focuses on learning Russian. We present a detailed classification of the types of constructions in Russian, in which MA is possible, and evaluate the model using a test set built from answers provided by the users of a running language learning system",
    "volume": "workshop",
    "checked": true,
    "id": "bb02aa0f596dc05aed4b6b1eca9059a27c37d308",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-3703": {
    "title": "Numbers Normalisation in the Inflected Languages: a Case Study of Polish",
    "abstract": "Text normalisation in Text-to-Speech systems is a process of converting written expressions to their spoken forms. This task is complicated because in many cases the normalised form depends on the context. Furthermore, when we analysed languages like Croatian, Lithuanian, Polish, Russian or Slovak there is additional difficulty related to their inflected nature. In this paper we want to show how to deal with this problem for one of these languages: Polish, without having a large dedicated data set and using solutions prepared for other NLP tasks. We limited our study to only numbers expressions, which are the most common non-standard words to normalise. The proposed solution is a combination of morphological tagger and transducer supported by a dictionary of numbers in their spoken forms. The data set used for evaluation is based on the part of 1-million word subset of the National Corpus of Polish. The accuracy of the described approach is presented with a comparison to a simple baseline and two commercial systems: Google Cloud Text-to-Speech and Amazon Polly",
    "volume": "workshop",
    "checked": true,
    "id": "23b44a81e1b101ed289074c62433bf674431748a",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3704": {
    "title": "What does Neural Bring? Analysing Improvements in Morphosyntactic Annotation and Lemmatisation of Slovenian, Croatian and Serbian",
    "abstract": "We present experiments on Slovenian, Croatian and Serbian morphosyntactic annotation and lemmatisation between the former state-of-the-art for these three languages and one of the best performing systems at the CoNLL 2018 shared task, the Stanford NLP neural pipeline. Our experiments show significant improvements in morphosyntactic annotation, especially on categories where either semantic knowledge is needed, available through word embeddings, or where long-range dependencies have to be modelled. On the other hand, on the task of lemmatisation no improvements are obtained with the neural solution, mostly due to the heavy dependence of the task on the lookup in an external lexicon, but also due to obvious room for improvements in the Stanford NLP pipeline's lemmatisation",
    "volume": "workshop",
    "checked": true,
    "id": "bf986c618856bfc229651a222234cf5ef9402374",
    "citation_count": 17
  },
  "https://aclanthology.org/W19-3705": {
    "title": "AGRR 2019: Corpus for Gapping Resolution in Russian",
    "abstract": "This paper provides a comprehensive overview of the gapping dataset for Russian that consists of 7.5k sentences with gapping (as well as 15k relevant negative sentences) and comprises data from various genres: news, fiction, social media and technical texts. The dataset was prepared for the Automatic Gapping Resolution Shared Task for Russian (AGRR-2019) - a competition aimed at stimulating the development of NLP tools and methods for processing of ellipsis. In this paper, we pay special attention to the gapping resolution methods that were introduced within the shared task as well as an alternative test set that illustrates that our corpus is a diverse and representative subset of Russian language gapping sufficient for effective utilization of machine learning techniques",
    "volume": "workshop",
    "checked": true,
    "id": "f5c5f617b324e19c555480027913deee721e3aa6",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-3706": {
    "title": "Creating a Corpus for Russian Data-to-Text Generation Using Neural Machine Translation and Post-Editing",
    "abstract": "In this paper, we propose an approach for semi-automatically creating a data-to-text (D2T) corpus for Russian that can be used to learn a D2T natural language generation model. An error analysis of the output of an English-to-Russian neural machine translation system shows that 80% of the automatically translated sentences contain an error and that 53% of all translation errors bear on named entities (NE). We therefore focus on named entities and introduce two post-editing techniques for correcting wrongly translated NEs",
    "volume": "workshop",
    "checked": true,
    "id": "84281e999465fad51945e0fa3b482418f6b70683",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-3707": {
    "title": "Data Set for Stance and Sentiment Analysis from User Comments on Croatian News",
    "abstract": "Nowadays it is becoming more important than ever to find new ways of extracting useful information from the evergrowing amount of user-generated data available online. In this paper, we describe the creation of a data set that contains news articles and corresponding comments from Croatian news outlet 24 sata. Our annotation scheme is specifically tailored for the task of detecting stances and sentiment from user comments as well as assessing if commentator claims are verifiable. Through this data, we hope to get a better understanding of the publics viewpoint on various events. In addition, we also explore the potential of applying supervised machine learning models toautomate annotation of more data",
    "volume": "workshop",
    "checked": true,
    "id": "dbf6fd2018010d5969c571be3e8d956d3bb2322e",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-3708": {
    "title": "A Dataset for Noun Compositionality Detection for a Slavic Language",
    "abstract": "This paper presents the first gold-standard resource for Russian annotated with compositionality information of noun compounds. The compound phrases are collected from the Universal Dependency treebanks according to part of speech patterns, such as ADJ+NOUN or NOUN+NOUN, using the gold-standard annotations. Each compound phrase is annotated by two experts and a moderator according to the following schema: the phrase can be either compositional, non-compositional, or ambiguous (i.e., depending on the context it can be interpreted both as compositional or non-compositional). We conduct an experimental evaluation of models and methods for predicting compositionality of noun compounds in unsupervised and supervised setups. We show that methods from previous work evaluated on the proposed Russian-language resource achieve the performance comparable with results on English corpora",
    "volume": "workshop",
    "checked": true,
    "id": "6cd13ede65fd3125ab1c8e9b4dcd80b183465e54",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-3709": {
    "title": "The Second Cross-Lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic Languages",
    "abstract": "We describe the Second Multilingual Named Entity Challenge in Slavic languages. The task is recognizing mentions of named entities in Web documents, their normalization, and cross-lingual linking. The Challenge was organized as part of the 7th Balto-Slavic Natural Language Processing Workshop, co-located with the ACL-2019 conference. Eight teams participated in the competition, which covered four languages and five entity types. Performance for the named entity recognition task reached 90% F-measure, much higher than reported in the first edition of the Challenge. Seven teams covered all four languages, and five teams participated in the cross-lingual entity linking task. Detailed evaluation information is available on the shared task web page",
    "volume": "workshop",
    "checked": true,
    "id": "90fffe9299d6b44c90cac918a4fb808dc514b841",
    "citation_count": 24
  },
  "https://aclanthology.org/W19-3710": {
    "title": "BSNLP2019 Shared Task Submission: Multisource Neural NER Transfer",
    "abstract": "This paper describes the Cognitive Computation (CogComp) Group's submissions to the multilingual named entity recognition shared task at the Balto-Slavic Natural Language Processing (BSNLP) Workshop. The final model submitted is a multi-source neural NER system with multilingual BERT embeddings, trained on the concatenation of training data in various Slavic languages (as well as English). The performance of our system on the official testing data suggests that multi-source approaches consistently outperform single-source approaches for this task, even with the noise of mismatching tagsets",
    "volume": "workshop",
    "checked": true,
    "id": "48437237cf6daa0530d4d38ef106a8930f8a8a5f",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-3711": {
    "title": "TLR at BSNLP2019: A Multilingual Named Entity Recognition System",
    "abstract": "This paper presents our participation at the shared task on multilingual named entity recognition at BSNLP2019. Our strategy is based on a standard neural architecture for sequence labeling. In particular, we use a mixed model which combines multilingualcontextual and language-specific embeddings. Our only submitted run is based on a voting schema using multiple models, one for each of the four languages of the task (Bulgarian, Czech, Polish, and Russian) and another for English. Results for named entity recognition are encouraging for all languages, varying from 60% to 83% in terms of Strict and Relaxed metrics, respectively",
    "volume": "workshop",
    "checked": true,
    "id": "d9465f0eabd156c4cda6210ea1c55fc24de78732",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-3712": {
    "title": "Tuning Multilingual Transformers for Language-Specific Named Entity Recognition",
    "abstract": "Our paper addresses the problem of multilingual named entity recognition on the material of 4 languages: Russian, Bulgarian, Czech and Polish. We solve this task using the BERT model. We use a hundred languages multilingual model as base for transfer to the mentioned Slavic languages. Unsupervised pre-training of the BERT model on these 4 languages allows to significantly outperform baseline neural approaches and multilingual BERT. Additional improvement is achieved by extending BERT with a word-level CRF layer. Our system was submitted to BSNLP 2019 Shared Task on Multilingual Named Entity Recognition and demonstrated top performance in multilingual setting for two competition metrics. We open-sourced NER models and BERT model pre-trained on the four Slavic languages",
    "volume": "workshop",
    "checked": true,
    "id": "1cd8167b2a6be4fdc0f2bfeec4e6f23a9bbb7090",
    "citation_count": 69
  },
  "https://aclanthology.org/W19-3713": {
    "title": "Multilingual Named Entity Recognition Using Pretrained Embeddings, Attention Mechanism and NCRF",
    "abstract": "In this paper we tackle multilingual named entity recognition task. We use the BERT Language Model as embeddings with bidirectional recurrent network, attention, and NCRF on the top. We apply multilingual BERT only as embedder without any fine-tuning. We test out model on the dataset of the BSNLP shared task, which consists of texts in Bulgarian, Czech, Polish and Russian languages",
    "volume": "workshop",
    "checked": true,
    "id": "d9b755f934d68b2816b0b73274416c50c019da11",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-3714": {
    "title": "JRC TMA-CC: Slavic Named Entity Recognition and Linking. Participation in the BSNLP-2019 shared task",
    "abstract": "We report on the participation of the JRC Text Mining and Analysis Competence Centre (TMA-CC) in the BSNLP-2019 Shared Task, which focuses on named-entity recognition, lemmatisation and cross-lingual linking. We propose a hybrid system combining a rule-based approach and light ML techniques. We use multilingual lexical resources such as JRC-NAMES and BABELNET together with a named entity guesser to recognise names. In a second step, we combine known names with wild cards to increase recognition recall by also capturing inflection variants. In a third step, we increase precision by filtering these name candidates with automatically learnt inflection patterns derived from name occurrences in large news article collections. Our major requirement is to achieve high precision. We achieved an average of 65% F-measure with 93% precision on the four languages",
    "volume": "workshop",
    "checked": true,
    "id": "95cc4eaa45e09dbf7f3be3bc7fe455f5649834a1",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3715": {
    "title": "Building English-to-Serbian Machine Translation System for IMDb Movie Reviews",
    "abstract": "This paper reports the results of the first experiment dealing with the challenges of building a machine translation system for user-generated content involving a complex South Slavic language. We focus on translation of English IMDb user movie reviews into Serbian, in a low-resource scenario. We explore potentials and limits of (i) phrase-based and neural machine translation systems trained on out-of-domain clean parallel data from news articles (ii) creating additional synthetic in-domain parallel corpus by machine-translating the English IMDb corpus into Serbian. Our main findings are that morphology and syntax are better handled by the neural approach than by the phrase-based approach even in this low-resource mismatched domain scenario, however the situation is different for the lexical aspect, especially for person names. This finding also indicates that in general, machine translation of person names into Slavic languages (especially those which require/allow transcription) should be investigated more systematically",
    "volume": "workshop",
    "checked": true,
    "id": "cd2ef9463d48270e56098e4d965bf41c3d1a0971",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-3716": {
    "title": "Improving Sentiment Classification in Slovak Language",
    "abstract": "Using different neural network architectures is widely spread for many different NLP tasks. Unfortunately, most of the research is performed and evaluated only in English language and minor languages are often omitted. We believe using similar architectures for other languages can show interesting results. In this paper, we present our study on methods for improving sentiment classification in Slovak language. We performed several experiments for two different datasets, one containing customer reviews, the other one general Twitter posts. We show comparison of performance of different neural network architectures and also different word representations. We show that another improvement can be achieved by using a model ensemble. We performed experiments utilizing different methods of model ensemble. Our proposed models achieved better results than previous models for both datasets. Our experiments showed also other potential research areas",
    "volume": "workshop",
    "checked": true,
    "id": "d9fc9695913002b3a181de66f7e47aaced0ed4a8",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3717": {
    "title": "Sentiment Analysis for Multilingual Corpora",
    "abstract": "The paper presents a generic approach to the supervised sentiment analysis of social media content in Slavic languages. The method proposes translating the documents from the original language to English with Google's Neural Translation Model. The resulted texts are then converted to vectors by averaging the vectorial representation of words derived from a pre-trained Word2Vec English model. Testing the approach with several machine learning methods on Polish, Slovenian and Croatian Twitter datasets returns up to 86% of classification accuracy on the out-of-sample data",
    "volume": "workshop",
    "checked": true,
    "id": "5b157bebbe5a7f651c14cb329423e1365ac801c3",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-3801": {
    "title": "Gendered Ambiguous Pronoun (GAP) Shared Task at the Gender Bias in NLP Workshop 2019",
    "abstract": "The 1st ACL workshop on Gender Bias in Natural Language Processing included a shared task on gendered ambiguous pronoun (GAP) resolution. This task was based on the coreference challenge defined in Webster et al. (2018), designed to benchmark the ability of systems to resolve pronouns in real-world contexts in a gender-fair way. 263 teams competed via a Kaggle competition, with the winning system achieving logloss of 0.13667 and near gender parity. We review the approaches of eleven systems with accepted description papers, noting their effective use of BERT (Devlin et al., 2018), both via fine-tuning and for feature extraction, as well as ensembling",
    "volume": "workshop",
    "checked": true,
    "id": "bead3588b51321727cfa71cec6543ec1cd65a42f",
    "citation_count": 16
  },
  "https://aclanthology.org/W19-3802": {
    "title": "Proposed Taxonomy for Gender Bias in Text; A Filtering Methodology for the Gender Generalization Subtype",
    "abstract": "The purpose of this paper is to present an empirical study on gender bias in text. Current research in this field is focused on detecting and correcting for gender bias in existing machine learning models rather than approaching the issue at the dataset level. The underlying motivation is to create a dataset which could enable machines to learn to differentiate bias writing from non-bias writing. A taxonomy is proposed for structural and contextual gender biases which can manifest themselves in text. A methodology is proposed to fetch one type of structural gender bias, Gender Generalization. We explore the IMDB movie review dataset and 9 different corpora from Project Gutenberg. By filtering out irrelevant sentences, the remaining pool of candidate sentences are sent for human validation. A total of 6123 judgments are made on 1627 sentences and after a quality check on randomly selected sentences we obtain an accuracy of 75%. Out of the 1627 sentences, 808 sentence were labeled as Gender Generalizations. The inter-rater reliability amongst labelers was of 61.14%",
    "volume": "workshop",
    "checked": true,
    "id": "f05b9b663f1461ef2e20be5d2e8d2116a5a44f94",
    "citation_count": 19
  },
  "https://aclanthology.org/W19-3803": {
    "title": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis",
    "abstract": "Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength",
    "volume": "workshop",
    "checked": true,
    "id": "ca65fd16bbc9e52faff3d5ab9d05804cc6145e0c",
    "citation_count": 15
  },
  "https://aclanthology.org/W19-3804": {
    "title": "Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories",
    "abstract": "Prior work has shown that word embeddings capture human stereotypes, including gender bias. However, there is a lack of studies testing the presence of specific gender bias categories in word embeddings across diverse domains. This paper aims to fill this gap by applying the WEAT bias detection method to four sets of word embeddings trained on corpora from four different domains: news, social networking, biomedical and a gender-balanced corpus extracted from Wikipedia (GAP). We find that some domains are definitely more prone to gender bias than others, and that the categories of gender bias present also vary for each set of word embeddings. We detect some gender bias in GAP. We also propose a simple but novel method for discovering new bias categories by clustering word embeddings. We validate this method through WEAT's hypothesis testing mechanism and find it useful for expanding the relatively small set of well-known gender bias word categories commonly used in the literature",
    "volume": "workshop",
    "checked": true,
    "id": "3f7b3051e9553c16d8ee35bd8112127632f11993",
    "citation_count": 32
  },
  "https://aclanthology.org/W19-3805": {
    "title": "Evaluating the Underlying Gender Bias in Contextualized Word Embeddings",
    "abstract": "Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased",
    "volume": "workshop",
    "checked": true,
    "id": "69accd35f2ae56aa71ceaa5abeb814fcedc8a58e",
    "citation_count": 91
  },
  "https://aclanthology.org/W19-3806": {
    "title": "Conceptor Debiasing of Word Representations Evaluated on WEAT",
    "abstract": "Bias in word representations, such as Word2Vec, has been widely reported and investigated, and efforts made to debias them. We apply the debiasing conceptor for post-processing both traditional and contextualized word embeddings. Our method can simultaneously remove racial and gender biases from word representations. Unlike standard debiasing methods, the debiasing conceptor can utilize heterogeneous lists of biased words without loss in performance. Finally, our empirical experiments show that the debiasing conceptor diminishes racial and gender bias of word representations as measured using the Word Embedding Association Test (WEAT) of Caliskan et al. (2017)",
    "volume": "workshop",
    "checked": true,
    "id": "dcfbdcf6bd3e6bb4101cc3f8eb1ca122db1eab61",
    "citation_count": 24
  },
  "https://aclanthology.org/W19-3807": {
    "title": "Filling Gender & Number Gaps in Neural Machine Translation with Black-box Context Injection",
    "abstract": "When translating from a language that does not morphologically mark information such as gender and number into a language that does, translation systems must \"guess\" this missing information, often leading to incorrect translations in the given context. We propose a black-box approach for injecting the missing information to a pre-trained neural machine translation system, allowing to control the morphological variations in the generated translations without changing the underlying model or training data. We evaluate our method on an English to Hebrew translation task, and show that it is effective in injecting the gender and number information and that supplying the correct information improves the translation accuracy in up to 2.3 BLEU on a female-speaker test set for a state-of-the-art online black-box system. Finally, we perform a fine-grained syntactic analysis of the generated translations that shows the effectiveness of our method",
    "volume": "workshop",
    "checked": true,
    "id": "b373f8249e5a45c89c38e0b45f499f38697b68a6",
    "citation_count": 40
  },
  "https://aclanthology.org/W19-3808": {
    "title": "The Role of Protected Class Word Lists in Bias Identification of Contextualized Word Representations",
    "abstract": "Systemic bias in word embeddings has been widely reported and studied, and efforts made to debias them; however, new contextualized embeddings such as ELMo and BERT are only now being similarly studied. Standard debiasing methods require heterogeneous lists of target words to identify the \"bias subspace\". We show show that using new contextualized word embeddings in conceptor debiasing allows us to more accurately debias word embeddings by breaking target word lists into more homogeneous subsets and then combining (\"Or'ing\") the debiasing conceptors of the different subsets",
    "volume": "workshop",
    "checked": true,
    "id": "dd20fc43fa2a3d32eb85d4520d3c34c7ce4e41b9",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-3809": {
    "title": "Good Secretaries, Bad Truck Drivers? Occupational Gender Stereotypes in Sentiment Analysis",
    "abstract": "In this work, we investigate the presence of occupational gender stereotypes in sentiment analysis models. Such a task has implications in reducing implicit biases in these models, which are being applied to an increasingly wide variety of downstream tasks. We release a new gender-balanced dataset of 800 sentences pertaining to specific professions and propose a methodology for using it as a test bench to evaluate sentiment analysis models. We evaluate the presence of occupational gender stereotypes in 3 different models using our approach, and explore their relationship with societal perceptions of occupations",
    "volume": "workshop",
    "checked": true,
    "id": "9246a72d4267a6887f8225e73b9b58a0435d3c4e",
    "citation_count": 23
  },
  "https://aclanthology.org/W19-3810": {
    "title": "Debiasing Embeddings for Reduced Gender Bias in Text Classification",
    "abstract": "(Bolukbasi et al., 2016) demonstrated that pretrained word embeddings can inherit gender bias from the data they were trained on. We investigate how this bias affects downstream classification tasks, using the case study of occupation classification (De-Arteaga et al., 2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy channel for communicating gender information. With a relatively minor adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and maintain high classification accuracy",
    "volume": "workshop",
    "checked": true,
    "id": "f397df630e0708d411e76309b85f56440b853b86",
    "citation_count": 28
  },
  "https://aclanthology.org/W19-3811": {
    "title": "BERT Masked Language Modeling for Co-reference Resolution",
    "abstract": "This paper explains the TALP-UPC participation for the Gendered Pronoun Resolution shared-task of the 1st ACL Workshop on Gender Bias for Natural Language Processing. We have implemented two models for mask language modeling using pre-trained BERT adjusted to work for a classification problem. The proposed solutions are based on the word probabilities of the original BERT model, but using common English names to replace the original test names",
    "volume": "workshop",
    "checked": true,
    "id": "1465ac9b14117a2a5db5b08d451fd14895890a03",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-3812": {
    "title": "Transfer Learning from Pre-trained BERT for Pronoun Resolution",
    "abstract": "The paper describes the submission of the team \"We used bert!\" to the shared task Gendered Pronoun Resolution (Pair pronouns to their correct entities). Our final submission model based on the fine-tuned BERT (Bidirectional Encoder Representations from Transformers) ranks 14th among 838 teams with a multi-class logarithmic loss of 0.208. In this work, contribution of transfer learning technique to pronoun resolution systems is investigated and the gender bias contained in classification models is evaluated",
    "volume": "workshop",
    "checked": true,
    "id": "3f8781992c33a4d45c1e4cd0b830fab4ae9d083b",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-3813": {
    "title": "MSnet: A BERT-based Network for Gendered Pronoun Resolution",
    "abstract": "The pre-trained BERT model achieves a remarkable state of the art across a wide range of tasks in natural language processing. For solving the gender bias in gendered pronoun resolution task, I propose a novel neural network model based on the pre-trained BERT. This model is a type of mention score classifier and uses an attention mechanism with no parameters to compute the contextual representation of entity span, and a vector to represent the triple-wise semantic similarity among the pronoun and the entities. In stage 1 of the gendered pronoun resolution task, a variant of this model, trained in the fine-tuning approach, reduced the multi-class logarithmic loss to 0.3033 in the 5-fold cross-validation of training set and 0.2795 in testing set. Besides, this variant won the 2nd place with a score at 0.17289 in stage 2 of the task. The code in this paper is available at: https://github.com/ziliwang/MSnet-for-Gendered-Pronoun-Resolution",
    "volume": "workshop",
    "checked": true,
    "id": "4e92152bf0ad51e6b5e8754403f956aa2ba3e219",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-3814": {
    "title": "Look Again at the Syntax: Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution",
    "abstract": "Gender bias has been found in existing coreference resolvers. In order to eliminate gender bias, a gender-balanced dataset Gendered Ambiguous Pronouns (GAP) has been released and the best baseline model achieves only 66.9% F1. Bidirectional Encoder Representations from Transformers (BERT) has broken several NLP task records and can be used on GAP dataset. However, fine-tune BERT on a specific task is computationally expensive. In this paper, we propose an end-to-end resolver by combining pre-trained BERT with Relational Graph Convolutional Network (R-GCN). R-GCN is used for digesting structural syntactic information and learning better task-specific embeddings. Empirical results demonstrate that, under explicit syntactic supervision and without the need to fine tune BERT, R-GCN's embeddings outperform the original BERT embeddings on the coreference task. Our work significantly improves the snippet-context baseline F1 score on GAP dataset from 66.9% to 80.3%. We participated in the Gender Bias for Natural Language Processing 2019 shared task, and our codes are available online",
    "volume": "workshop",
    "checked": true,
    "id": "a6f8fd15058e3386aa02c95f88da7ad9ba7481d1",
    "citation_count": 16
  },
  "https://aclanthology.org/W19-3815": {
    "title": "Fill the GAP: Exploiting BERT for Pronoun Resolution",
    "abstract": "In this paper, we describe our entry in the gendered pronoun resolution competition which achieved fourth place without data augmentation. Our method is an ensemble system of BERTs which resolves co-reference in an interaction space. We report four insights from our work: BERT's representations involve significant redundancy; modeling interaction effects similar to natural language inference models is useful for this task; there is an optimal BERT layer to extract representations for pronoun resolution; and the difference between the attention weights from the pronoun to the candidate entities was highly correlated with the correct label, with interesting implications for future work",
    "volume": "workshop",
    "checked": true,
    "id": "60d576c5e25104ebf6d422ec66e7ce8f45061232",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3816": {
    "title": "On GAP Coreference Resolution Shared Task: Insights from the 3rd Place Solution",
    "abstract": "This paper presents the 3rd-place-winning solution to the GAP coreference resolution shared task. The approach adopted consists of two key components: fine-tuning the BERT language representation model (Devlin et al., 2018) and the usage of external datasets during the training process. The model uses hidden states from the intermediate BERT layers instead of the last layer. The resulting system almost eliminates the difference in log loss per gender during the cross-validation, while providing high performance",
    "volume": "workshop",
    "checked": true,
    "id": "68b8286fbde1220aa79c050194f3ca812644656e",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3817": {
    "title": "Resolving Gendered Ambiguous Pronouns with BERT",
    "abstract": "Pronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding and a necessary component of machine translation systems, chat bots and assistants. Neural machine learning systems perform far from ideally in this task, reaching as low as 73% F1 scores on modern benchmark datasets. Moreover, they tend to perform better for masculine pronouns than for feminine ones. Thus, the problem is both challenging and important for NLP researchers and practitioners. In this project, we describe our BERT-based approach to solving the problem of gender-balanced pronoun resolution. We are able to reach 92% F1 score and a much lower gender bias on the benchmark dataset shared by Google AI Language team",
    "volume": "workshop",
    "checked": true,
    "id": "2d1e289a6b4d41341f24c4d72cfae660891397be",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3818": {
    "title": "Anonymized BERT: An Augmentation Approach to the Gendered Pronoun Resolution Challenge",
    "abstract": "We present our 7th place solution to the Gendered Pronoun Resolution challenge, which uses BERT without fine-tuning and a novel augmentation strategy designed for contextual embedding token-level tasks. Our method anonymizes the referent by replacing candidate names with a set of common placeholder names. Besides the usual benefits of effectively increasing training data size, this approach diversifies idiosyncratic information embedded in names. Using same set of common first names can also help the model recognize names better, shorten token length, and remove gender and regional biases associated with names. The system scored 0.1947 log loss in stage 2, where the augmentation contributed to an improvements of 0.04. Post-competition analysis shows that, when using different embedding layers, the system scores 0.1799 which would be third place",
    "volume": "workshop",
    "checked": true,
    "id": "67565d01e1d517addd49c063a01eb14b9b0dffca",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-3819": {
    "title": "Gendered Pronoun Resolution using BERT and an Extractive Question Answering Formulation",
    "abstract": "The resolution of ambiguous pronouns is a longstanding challenge in Natural Language Understanding. Recent studies have suggested gender bias among state-of-the-art coreference resolution systems. As an example, Google AI Language team recently released a gender-balanced dataset and showed that performance of these coreference resolvers is significantly limited on the dataset. In this paper, we propose an extractive question answering (QA) formulation of pronoun resolution task that overcomes this limitation and shows much lower gender bias (0.99) on their dataset. This system uses fine-tuned representations from the pre-trained BERT model and outperforms the existing baseline by a significant margin (22.2% absolute improvement in F1 score) without using any hand-engineered features. This QA framework is equally performant even without the knowledge of the candidate antecedents of the pronoun. An ensemble of QA and BERT-based multiple choice and sequence classification models further improves the F1 (23.3% absolute improvement upon the baseline). This ensemble model was submitted to the shared task for the 1st ACL workshop on Gender Bias for Natural Language Processing. It ranked 9th on the final official leaderboard",
    "volume": "workshop",
    "checked": true,
    "id": "a52ad4f73f690c350c054a2463db9bc5f94e9360",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-3820": {
    "title": "Gendered Ambiguous Pronouns Shared Task: Boosting Model Confidence by Evidence Pooling",
    "abstract": "This paper presents a strong set of results for resolving gendered ambiguous pronouns on the Gendered Ambiguous Pronouns shared task. The model presented here draws upon the strengths of state-of-the-art language and coreference resolution models, and introduces a novel evidence-based deep learning architecture. Injecting evidence from the coreference models compliments the base architecture, and analysis shows that the model is not hindered by their weaknesses, specifically gender bias. The modularity and simplicity of the architecture make it very easy to extend for further improvement and applicable to other NLP problems. Evaluation on GAP test data results in a state-of-the-art performance at 92.5% F1 (gender bias of 0.97), edging closer to the human performance of 96.6%. The end-to-end solution presented here placed 1st in the Kaggle competition, winning by a significant lead",
    "volume": "workshop",
    "checked": true,
    "id": "0a0019691463eea92a54df9631754efe2839319f",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-3821": {
    "title": "Equalizing Gender Bias in Neural Machine Translation with Word Embeddings Techniques",
    "abstract": "Neural machine translation has significantly pushed forward the quality of the field. However, there are remaining big issues with the output translations and one of them is fairness. Neural models are trained on large text corpora which contain biases and stereotypes. As a consequence, models inherit these social biases. Recent methods have shown results in reducing gender bias in other natural language processing tools such as word embeddings. We take advantage of the fact that word embeddings are used in neural machine translation to propose a method to equalize gender biases in neural machine translation using these representations. Specifically, we propose, experiment and analyze the integration of two debiasing techniques over GloVe embeddings in the Transformer translation architecture. We evaluate our proposed system on the WMT English-Spanish benchmark task, showing gains up to one BLEU point. As for the gender bias evaluation, we generate a test set of occupations and we show that our proposed system learns to equalize existing biases from the baseline system",
    "volume": "workshop",
    "checked": true,
    "id": "50154080ccbaec1a3b4ba401bebd94b80225d21a",
    "citation_count": 102
  },
  "https://aclanthology.org/W19-3822": {
    "title": "Automatic Gender Identification and Reinflection in Arabic",
    "abstract": "The impressive progress in many Natural Language Processing (NLP) applications has increased the awareness of some of the biases these NLP systems have with regards to gender identities. In this paper, we propose an approach to extend biased single-output gender-blind NLP systems with gender-specific alternative reinflections. We focus on Arabic, a gender-marking morphologically rich language, in the context of machine translation (MT) from English, and for first-person-singular constructions only. Our contributions are the development of a system-independent gender-awareness wrapper, and the building of a corpus for training and evaluating first-person-singular gender identification and reinflection in Arabic. Our results successfully demonstrate the viability of this approach with 8% relative increase in Bleu score for first-person-singular feminine, and 5.3% comparable increase for first-person-singular masculine on top of a state-of-the-art gender-blind MT system on a held-out test set",
    "volume": "workshop",
    "checked": true,
    "id": "2573ce8edcec066582fb7b14c01cfd9effa92f82",
    "citation_count": 26
  },
  "https://aclanthology.org/W19-3823": {
    "title": "Measuring Bias in Contextualized Word Representations",
    "abstract": "Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases",
    "volume": "workshop",
    "checked": true,
    "id": "a2ce1fb96c0b78bee18bb2cb2c3d55dc48d54cbd",
    "citation_count": 182
  },
  "https://aclanthology.org/W19-3824": {
    "title": "On Measuring Gender Bias in Translation of Gender-neutral Pronouns",
    "abstract": "Ethics regarding social bias has recently thrown striking issues in natural language processing. Especially for gender-related topics, the need for a system that reduces the model bias has grown in areas such as image captioning, content recommendation, and automated employment. However, detection and evaluation of gender bias in the machine translation systems are not yet thoroughly investigated, for the task being cross-lingual and challenging to define. In this paper, we propose a scheme for making up a test set that evaluates the gender bias in a machine translation system, with Korean, a language with gender-neutral pronouns. Three word/phrase sets are primarily constructed, each incorporating positive/negative expressions or occupations; all the terms are gender-independent or at least not biased to one side severely. Then, additional sentence lists are constructed concerning formality of the pronouns and politeness of the sentences. With the generated sentence set of size 4,236 in total, we evaluate gender bias in conventional machine translation systems utilizing the proposed measure, which is termed here as translation gender bias index (TGBI). The corpus and the code for evaluation is available on-line",
    "volume": "workshop",
    "checked": true,
    "id": "004fbcb0f3248afcbc158d97d3b02f0ea42e137a",
    "citation_count": 28
  },
  "https://aclanthology.org/W19-3901": {
    "title": "Sequential Neural Networks as Automata",
    "abstract": "This work attempts to explain the types of computation that neural networks can perform by relating them to automata. We first define what it means for a real-time network with bounded precision to accept a language. A measure of network memory follows from this definition. We then characterize the classes of languages acceptable by various recurrent networks, attention, and convolutional networks. We find that LSTMs function like counter machines and relate convolutional networks to the subregular hierarchy. Overall, this work attempts to increase our understanding and ability to interpret neural networks through the lens of theory. These theoretical insights help explain neural computation, as well as the relationship between neural networks and natural language grammar",
    "volume": "workshop",
    "checked": true,
    "id": "a1b35b15a548819cc133e3e0e4cf9b01af80e35d",
    "citation_count": 38
  },
  "https://aclanthology.org/W19-3902": {
    "title": "Grammatical Sequence Prediction for Real-Time Neural Semantic Parsing",
    "abstract": "While sequence-to-sequence (seq2seq) models achieve state-of-the-art performance in many natural language processing tasks, they can be too slow for real-time applications. One performance bottleneck is predicting the most likely next token over a large vocabulary; methods to circumvent this bottleneck are a current research topic. We focus specifically on using seq2seq models for semantic parsing, where we observe that grammars often exist which specify valid formal representations of utterance semantics. By developing a generic approach for restricting the predictions of a seq2seq model to grammatically permissible continuations, we arrive at a widely applicable technique for speeding up semantic parsing. The technique leads to a 74% speed-up on an in-house dataset with a large vocabulary, compared to the same neural model without grammatical restrictions",
    "volume": "workshop",
    "checked": true,
    "id": "363a24a96423227ed6b03fc71a169ad0f80e6fac",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-3903": {
    "title": "Relating RNN Layers with the Spectral WFA Ranks in Sequence Modelling",
    "abstract": "We analyse Recurrent Neural Networks (RNNs) to understand the significance of multiple LSTM layers. We argue that the Weighted Finite-state Automata (WFA) trained using a spectral learning algorithm are helpful to analyse RNNs. Our results suggest that multiple LSTM layers in RNNs help learning distributed hidden states, but have a smaller impact on the ability to learn long-term dependencies. The analysis is based on the empirical results, however relevant theory (whenever possible) was discussed to justify and support our conclusions",
    "volume": "workshop",
    "checked": true,
    "id": "78917bc09b5ccbff60ad83f879d303ac1e12c3ee",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-3904": {
    "title": "Multi-Element Long Distance Dependencies: Using SPk Languages to Explore the Characteristics of Long-Distance Dependencies",
    "abstract": "In order to successfully model Long Distance Dependencies (LDDs) it is necessary to under-stand the full-range of the characteristics of the LDDs exhibited in a target dataset. In this paper, we use Strictly k-Piecewise languages to generate datasets with various properties. We then compute the characteristics of the LDDs in these datasets using mutual information and analyze the impact of factors such as (i) k, (ii) length of LDDs, (iii) vocabulary size, (iv) forbidden strings, and (v) dataset size. This analysis reveal that the number of interacting elements in a dependency is an important characteristic of LDDs. This leads us to the challenge of modelling multi-element long-distance dependencies. Our results suggest that attention mechanisms in neural networks may aide in modeling datasets with multi-element long-distance dependencies. However, we conclude that there is a need to develop more efficient attention mechanisms to address this issue",
    "volume": "workshop",
    "checked": true,
    "id": "59fd0149965e5d146797c6eb3bbd521a2b211b15",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-3905": {
    "title": "LSTM Networks Can Perform Dynamic Counting",
    "abstract": "In this paper, we systematically assess the ability of standard recurrent networks to perform dynamic counting and to encode hierarchical representations. All the neural models in our experiments are designed to be small-sized networks both to prevent them from memorizing the training sets and to visualize and interpret their behaviour at test time. Our results demonstrate that the Long Short-Term Memory (LSTM) networks can learn to recognize the well-balanced parenthesis language (Dyck-1) and the shuffles of multiple Dyck-1 languages, each defined over different parenthesis-pairs, by emulating simple real-time k-counter machines. To the best of our knowledge, this work is the first study to introduce the shuffle languages to analyze the computational power of neural networks. We also show that a single-layer LSTM with only one hidden unit is practically sufficient for recognizing the Dyck-1 language. However, none of our recurrent networks was able to yield a good performance on the Dyck-2 language learning task, which requires a model to have a stack-like mechanism for recognition",
    "volume": "workshop",
    "checked": true,
    "id": "da6ba9f19581bd7e28bc280c53385a1327eb09dd",
    "citation_count": 41
  },
  "https://aclanthology.org/W19-4001": {
    "title": "Crowdsourced Hedge Term Disambiguation",
    "abstract": "We address the issue of acquiring quality annotations of hedging words and phrases, linguistic phenomenona in which words, sounds, or other constructions are used to express ambiguity or uncertainty. Due to the limited availability of existing corpora annotated for hedging, linguists and other language scientists have been constrained as to the extent they can study this phenomenon. In this paper, we introduce a new method of acquiring hedging annotations via crowdsourcing, based on reformulating the task of labeling hedges as a simple word sense disambiguation task. We also introduce a new hedging corpus we have constructed by applying this method, a collection of forum posts annotated using Amazon Mechanical Turk. We found that the crowdsourced judgments we obtained had an inter-annotator agreement of 92.89% (Fleiss' Kappa=0.751) and, when comparing a subset of these annotations to an expert-annotated gold standard, an accuracy of 96.65%",
    "volume": "workshop",
    "checked": true,
    "id": "9467d6c6689858ca44ea5a9ef7353aac11a43760",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4002": {
    "title": "WiRe57 : A Fine-Grained Benchmark for Open Information Extraction",
    "abstract": "We build a reference for the task of Open Information Extraction, on five documents. We tentatively resolve a number of issues that arise, including coreference and granularity, and we take steps toward addressing inference, a significant problem. We seek to better pinpoint the requirements for the task. We produce our annotation guidelines specifying what is correct to extract and what is not. In turn, we use this reference to score existing Open IE systems. We address the non-trivial problem of evaluating the extractions produced by systems against the reference tuples, and share our evaluation script. Among seven compared extractors, we find the MinIE system to perform best",
    "volume": "workshop",
    "checked": true,
    "id": "c9651edf9a21e044ecef4b0ff38e8f9a67fb518a",
    "citation_count": 21
  },
  "https://aclanthology.org/W19-4003": {
    "title": "Crowdsourcing Discourse Relation Annotations by a Two-Step Connective Insertion Task",
    "abstract": "The perspective of being able to crowd-source coherence relations bears the promise of acquiring annotations for new texts quickly, which could then increase the size and variety of discourse-annotated corpora. It would also open the avenue to answering new research questions: Collecting annotations from a larger number of individuals per instance would allow to investigate the distribution of inferred relations, and to study individual differences in coherence relation interpretation. However, annotating coherence relations with untrained workers is not trivial. We here propose a novel two-step annotation procedure, which extends an earlier method by Scholman and Demberg (2017a). In our approach, coherence relation labels are inferred from connectives that workers insert into the text. We show that the proposed method leads to replicable coherence annotations, and analyse the agreement between the obtained relation labels and annotations from PDTB and RSTDT on the same texts",
    "volume": "workshop",
    "checked": true,
    "id": "65a78656c1b5067f2a0407484c246d03f7f230b4",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-4004": {
    "title": "Annotating and analyzing the interactions between meaning relations",
    "abstract": "Pairs of sentences, phrases, or other text pieces can hold semantic relations such as paraphrasing, textual entailment, contradiction, specificity, and semantic similarity. These relations are usually studied in isolation and no dataset exists where they can be compared empirically. Here we present a corpus annotated with these relations and the analysis of these results. The corpus contains 520 sentence pairs, annotated with these relations. We measure the annotation reliability of each individual relation and we examine their interactions and correlations. Among the unexpected results revealed by our analysis is that the traditionally considered direct relationship between paraphrasing and bi-directional entailment does not hold in our data",
    "volume": "workshop",
    "checked": true,
    "id": "b899be677e1327ecc6cb80293435bcc32c9a25e1",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4005": {
    "title": "CCGweb: a New Annotation Tool and a First Quadrilingual CCG Treebank",
    "abstract": "We present the first open-source graphical annotation tool for combinatory categorial grammar (CCG), and the first set of detailed guidelines for syntactic annotation with CCG, for four languages: English, German, Italian, and Dutch. We also release a parallel pilot CCG treebank based on these guidelines, with 4x100 adjudicated sentences, 10K single-annotator fully corrected sentences, and 82K single-annotator partially corrected sentences",
    "volume": "workshop",
    "checked": true,
    "id": "830d5a5fca2221ef48683bc172bab4176ec363bd",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4006": {
    "title": "The making of the Litkey Corpus, a richly annotated longitudinal corpus of German texts written by primary school children",
    "abstract": "To date, corpus and computational linguistic work on written language acquisition has mostly dealt with second language learners who have usually already mastered orthography acquisition in their first language. In this paper, we present the Litkey Corpus, a richly-annotated longitudinal corpus of written texts produced by primary school children in Germany from grades 2 to 4. The paper focuses on the (semi-)automatic annotation procedure at various linguistic levels, which include POS tags, features of the word-internal structure (phonemes, syllables, morphemes) and key orthographic features of the target words as well as a categorization of spelling errors. Comprehensive evaluations show that high accuracy was achieved on all levels, making the Litkey Corpus a useful resource for corpus-based research on literacy acquisition of German primary school children and for developing NLP tools for educational purposes. The corpus is freely available under https://www.linguistics.rub.de/litkeycorpus/",
    "volume": "workshop",
    "checked": true,
    "id": "a2e40e35698e469683aa67969cb3814780b43c3d",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4007": {
    "title": "The Materials Science Procedural Text Corpus: Annotating Materials Synthesis Procedures with Shallow Semantic Structures",
    "abstract": "Materials science literature contains millions of materials synthesis procedures described in unstructured natural language text. Large-scale analysis of these synthesis procedures would facilitate deeper scientific understanding of materials synthesis and enable automated synthesis planning. Such analysis requires extracting structured representations of synthesis procedures from the raw text as a first step. To facilitate the training and evaluation of synthesis extraction models, we introduce a dataset of 230 synthesis procedures annotated by domain experts with labeled graphs that express the semantics of the synthesis sentences. The nodes in this graph are synthesis operations and their typed arguments, and labeled edges specify relations between the nodes. We describe this new resource in detail and highlight some specific challenges to annotating scientific text with shallow semantic structure. We make the corpus available to the community to promote further research and development of scientific information extraction systems",
    "volume": "workshop",
    "checked": true,
    "id": "194c5644c49e9e1b87990439fae05c98ba8b4fbb",
    "citation_count": 52
  },
  "https://aclanthology.org/W19-4008": {
    "title": "Tagging modality in Oceanic languages of Melanesia",
    "abstract": "Primary data from small, low-resource languages of Oceania have only recently become available through language documentation. In our study, we explore corpus data of five Oceanic languages of Melanesia which are known to be mood-prominent (in the sense of Bhat, 1999). In order to find out more about tense, aspect, modality, and polarity, we tagged these categories in a subset of our corpora. For the category of modality, we developed a novel tag set (MelaTAMP, 2017), which categorizes clauses into factual, possible, and counterfactual. Based on an analysis of the inter-annotator consistency, we argue that our tag set for the modal domain is efficient for our subject languages and might be useful for other languages and purposes",
    "volume": "workshop",
    "checked": true,
    "id": "059847a9b16e97968646ffa4eddfb643422f4b19",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4009": {
    "title": "Harmonizing Different Lemmatization Strategies for Building a Knowledge Base of Linguistic Resources for Latin",
    "abstract": "The interoperability between lemmatized corpora of Latin and other resources that use the lemma as indexing key is hampered by the multiple lemmatization strategies that different projects adopt. In this paper we discuss how we tackle the challenges raised by harmonizing different lemmatization criteria in the context of a project that aims to connect linguistic resources for Latin using the Linked Data paradigm. The paper introduces the architecture supporting an open-ended, lemma-based Knowledge Base, built to make textual and lexical resources for Latin interoperable. Particularly, the paper describes the inclusion into the Knowledge Base of its lexical basis, of a word formation lexicon and of a lemmatized and syntactically annotated corpus",
    "volume": "workshop",
    "checked": true,
    "id": "edbc32fb2826c25e46520e3c413f4a2c074fed11",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-4010": {
    "title": "Assessing Back-Translation as a Corpus Generation Strategy for non-English Tasks: A Study in Reading Comprehension and Word Sense Disambiguation",
    "abstract": "Corpora curated by experts have sustained Natural Language Processing mainly in English, but the expensiveness of corpora creation is a barrier for the development in further languages. Thus, we propose a corpus generation strategy that only requires a machine translation system between English and the target language in both directions, where we filter the best translations by computing automatic translation metrics and the task performance score. By studying Reading Comprehension in Spanish and Word Sense Disambiguation in Portuguese, we identified that a more quality-oriented metric has high potential in the corpora selection without degrading the task performance. We conclude that it is possible to systematise the building of quality corpora using machine translation and automatic metrics, besides some prior effort to clean and process the data",
    "volume": "workshop",
    "checked": true,
    "id": "1e276155692b682481b60d092baf689aa38b65b4",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4011": {
    "title": "A Framework for Annotating 'Related Works' to Support Feedback to Novice Writers",
    "abstract": "Understanding what is expected of academic writing can be difficult for novice writers to assimilate, and recent years have seen several automated tools become available to support academic writing. Our work presents a framework for annotating features of the Related Work section of academic writing, that supports writer feedback",
    "volume": "workshop",
    "checked": true,
    "id": "cdb005869b644c2b2b3c87fb5370758e1ab03487",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4012": {
    "title": "An Online Annotation Assistant for Argument Schemes",
    "abstract": "Understanding the inferential principles underpinning an argument is essential to the proper interpretation and evaluation of persuasive discourse. Argument schemes capture the conventional patterns of reasoning appealed to in persuasion. The empirical study of these patterns relies on the availability of data about the actual use of argumentation in communicative practice. Annotated corpora of argument schemes, however, are scarce, small, and unrepresentative. Aiming to address this issue, we present one step in the development of improved datasets by integrating the Argument Scheme Key – a novel annotation method based on one of the most popular typologies of argument schemes – into the widely used OVA software for argument analysis",
    "volume": "workshop",
    "checked": true,
    "id": "3f5395cc6734029f883db85b719d7a3f8ba3165d",
    "citation_count": 14
  },
  "https://aclanthology.org/W19-4013": {
    "title": "Annotating formulaic sequences in spoken Slovenian: structure, function and relevance",
    "abstract": "This paper presents the identification of formulaic sequences in the reference corpus of spoken Slovenian and their annotation in terms of syntactic structure, pragmatic function and lexicographic relevance. The annotation campaign, specific in terms of setting, subjectivity and the multifunctionality of items under investigation, resulted in a preliminary lexicon of formulaic sequences in spoken Slovenian with immediate potential for future explorations in formulaic language research. This is especially relevant for the notable number of identified multi-word expressions with discourse-structuring and stance-marking functions, which have often been overlooked by traditional phraseology research",
    "volume": "workshop",
    "checked": true,
    "id": "2e6a6edd17aae6d4e98c6aed3af695d21c2c4c49",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4014": {
    "title": "Annotating Information Structure in Italian: Characteristics and Cross-Linguistic Applicability of a QUD-Based Approach",
    "abstract": "We present a discourse annotation study, in which an annotation method based on Questions under Discussion (QuD) is applied to Italian data. The results of our inter-annotator agreement analysis show that the QUD-based approach, originally spelled out for English and German, can successfully be transferred cross-linguistically, supporting good agreement for the annotation of central information structure notions such as focus and non-at-issueness. Our annotation and interannotator agreement study on Italian authentic data confirms the cross-linguistic applicability of the QuD-based approach",
    "volume": "workshop",
    "checked": true,
    "id": "11b7678fbe627683cafaf967c426a00ecd338bfe",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4015": {
    "title": "DEFT: A corpus for definition extraction in free- and semi-structured text",
    "abstract": "Definition extraction has been a popular topic in NLP research for well more than a decade, but has been historically limited to well-defined, structured, and narrow conditions. In reality, natural language is messy, and messy data requires both complex solutions and data that reflects that reality. In this paper, we present a robust English corpus and annotation schema that allows us to explore the less straightforward examples of term-definition structures in free and semi-structured text",
    "volume": "workshop",
    "checked": true,
    "id": "cd3a0f86923c2809806985d949814c84b2045cad",
    "citation_count": 30
  },
  "https://aclanthology.org/W19-4016": {
    "title": "Explaining Simple Natural Language Inference",
    "abstract": "The vast amount of research introducing new corpora and techniques for semi-automatically annotating corpora shows the important role that datasets play in today's research, especially in the machine learning community. This rapid development raises concerns about the quality of the datasets created and consequently of the models trained, as recently discussed with respect to the Natural Language Inference (NLI) task. In this work we conduct an annotation experiment based on a small subset of the SICK corpus. The experiment reveals several problems in the annotation guidelines, and various challenges of the NLI task itself. Our quantitative evaluation of the experiment allows us to assign our empirical observations to specific linguistic phenomena and leads us to recommendations for future annotation tasks, for NLI and possibly for other tasks",
    "volume": "workshop",
    "checked": true,
    "id": "fe08217e505dfb0ef98cd12b2db28689b7a4f8c3",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-4017": {
    "title": "On the role of discourse relations in persuasive texts",
    "abstract": "This paper investigates the use of explicitly signalled discourse relations in persuasive texts. We present a corpus study where we control for speaker and topic and show that the distribution of different discourse connectives varies considerably across different discourse settings. While this variation can be explained by genre differences, we also observe variation regarding the distribution of discourse relations across different settings. This variation, however, cannot be easily explained by genre differences. We argue that the differences regarding the use of discourse relations reflects different strategies of persuasion and that these might be due to audience design",
    "volume": "workshop",
    "checked": true,
    "id": "3c4d014f33252835c0c65ee285bb5e5a05be91bc",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4018": {
    "title": "One format to rule them all – The emtsv pipeline for Hungarian",
    "abstract": "We present a more efficient version of the e-magyar NLP pipeline for Hungarian called emtsv. It integrates Hungarian NLP tools in a framework whose individual modules can be developed or replaced independently and allows new ones to be added. The design also allows convenient investigation and manual correction of the data flow from one module to another. The improvements we publish include effective communication between the modules and support of the use of individual modules both in the chain and standing alone. Our goals are accomplished using extended tsv (tab separated values) files, a simple, uniform, generic and self-documenting input/output format. Our vision is maintaining the system for a long time and making it easier for external developers to fit their own modules into the system, thus sharing existing competencies in the field of processing Hungarian, a mid-resourced language. The source code is available under LGPL 3.0 license at https://github.com/dlt-rilmta/emtsv ",
    "volume": "workshop",
    "checked": true,
    "id": "3586d7be0b1d96aba2231bfa129016e911a89cac",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4019": {
    "title": "Turkish Treebanking: Unifying and Constructing Efforts",
    "abstract": "In this paper, we present the current version of two different treebanks, the re-annotation of the Turkish PUD Treebank and the first annotation of the Turkish National Corpus Universal Dependency (henceforth TNC-UD). The annotation of both treebanks, the Turkish PUD Treebank and TNC-UD, was carried out based on the decisions concerning linguistic adequacy of re-annotation of the Turkish IMST-UD Treebank (Türk et. al., forthcoming). Both of the treebanks were annotated with the same annotation process and morphological and syntactic analyses. The TNC-UD is planned to have 10,000 sentences. In this paper, we will present the first 500 sentences along with the annotation PUD Treebank. Moreover, this paper also offers the parsing results of a graph-based neural parser on the previous and re-annotated PUD, as well as the TNC-UD. In light of the comparisons, even though we observe a slight decrease in the attachment scores of the Turkish PUD treebank, we demonstrate that the annotation of the TNC-UD improves the parsing accuracy of Turkish. In addition to the treebanks, we have also constructed a custom annotation software with advanced filtering and morphological editing options. Both the treebanks, including a full edit-history and the annotation guidelines, and the custom software are publicly available under an open license online",
    "volume": "workshop",
    "checked": true,
    "id": "172e5a3ec8e9d2ef1e525dc3c264e5c10a9eb986",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-4020": {
    "title": "A Dataset for Semantic Role Labelling of Hindi-English Code-Mixed Tweets",
    "abstract": "We present a data set of 1460 Hindi-English code-mixed tweets consisting of 20,949 tokens labelled with Proposition Bank labels marking their semantic roles. We created verb frames for complex predicates present in the corpus and formulated mappings from Paninian dependency labels to Proposition Bank labels. With the help of these mappings and the dependency tree, we propose a baseline rule based system for Semantic Role Labelling of Hindi-English code-mixed data. We obtain an accuracy of 96.74% for Argument Identification and are able to further classify 73.93% of the labels correctly. While there is relevant ongoing research on Semantic Role Labelling and on building tools for code-mixed social media data, this is the first attempt at labelling semantic roles in code-mixed data, to the best of our knowledge",
    "volume": "workshop",
    "checked": true,
    "id": "4c569f3917a64eb9f33140d4cd677afc3555acf3",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4021": {
    "title": "A Multi-Platform Annotation Ecosystem for Domain Adaptation",
    "abstract": "This paper describes an ecosystem consisting of three independent text annotation platforms. To demonstrate their ability to work in concert, we illustrate how to use them to address an interactive domain adaptation task in biomedical entity recognition. The platforms and the approach are in general domain-independent and can be readily applied to other areas of science",
    "volume": "workshop",
    "checked": true,
    "id": "7dc50a6acef589b7bde2f6f79f50e65f9a6fad79",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4022": {
    "title": "A New Annotation Scheme for the Sejong Part-of-speech Tagged Corpus",
    "abstract": "In this paper we present a new annotation scheme for the Sejong part-of-speech tagged corpus based on Universal Dependencies style annotation. By using a new annotation scheme, we can produce Sejong-style morphological analysis and part-of-speech tagging results which have been the de facto standard for Korean language processing. We also explore the possibility of doing named-entity recognition and semantic-role labelling for Korean using the new annotation scheme",
    "volume": "workshop",
    "checked": true,
    "id": "0822118b934f3a3069b43ffa4d7192fc516f6c35",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4023": {
    "title": "A Turkish Dataset for Gender Identification of Twitter Users",
    "abstract": "Author profiling is the identification of an author's gender, age, and language from his/her texts. With the increasing trend of using Twitter as a means to express thought, profiling the gender of an author from his/her tweets has become a challenge. Although several datasets in different languages have been released on this problem, there is still a need for multilingualism. In this work, we propose a dataset of tweets of Turkish Twitter users which are labeled with their gender information. The dataset has 3368 users in training set and 1924 users in test set where each user has 100 tweets. The dataset is publicly available",
    "volume": "workshop",
    "checked": true,
    "id": "ded3a6ddc25d19c314eb5a194be548480e263801",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-4024": {
    "title": "Comparative judgments are more consistent than binary classification for labelling word complexity",
    "abstract": "Lexical simplification systems replace complex words with simple ones based on a model of which words are complex in context. We explore how users can help train complex word identification models through labelling more efficiently and reliably. We show that using an interface where annotators make comparative rather than binary judgments leads to more reliable and consistent labels, and explore whether comparative judgments may provide a faster way for collecting labels",
    "volume": "workshop",
    "checked": true,
    "id": "de429b18e4664244b06a176ded48bcabed1ca524",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4025": {
    "title": "Continuous Quality Control and Advanced Text Segment Annotation with WAT-SL 2.0",
    "abstract": "Today's widely used annotation tools were designed for annotating typically short textual mentions of entities or relations, making their interface cumbersome to use for long(er) stretches of text, e.g, sentences running over several lines in a document. They also lack systematic support for hierarchically structured labels, i.e., one label being conceptually more general than another (e.g., anamnesis in relation to family anamnesis). Moreover, as a more fundamental shortcoming of today's tools, they provide no continuous quality con trol mechanisms for the annotation process, an essential feature to intrinsically support iterative cycles in the development of annotation guidelines. We alleviated these problems by developing WAT-SL 2.0, an open-source web-based annotation tool for long-segment labeling, hierarchically structured label sets and built-ins for quality control",
    "volume": "workshop",
    "checked": true,
    "id": "1c404bcaf18e749a450578daf322f79f82a4e949",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4026": {
    "title": "Creation of a corpus with semantic role labels for Hungarian",
    "abstract": "In this article, an ongoing research is presented, the immediate goal of which is to create a corpus annotated with semantic role labels for Hungarian that can be used to train a parser-based system capable of formulating relevant questions about the text it processes. We briefly describe the objectives of our research, our efforts at eliminating errors in the Hungarian Universal Dependencies corpus, which we use as the base of our annotation effort, at creating a Hungarian verbal argument database annotated with thematic roles, at classifying adjuncts, and at matching verbal argument frames to specific occurrences of verbs and participles in the corpus",
    "volume": "workshop",
    "checked": true,
    "id": "46c1255ad17bb84510ccff49d222de687d067de1",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4027": {
    "title": "Toward Dialogue Modeling: A Semantic Annotation Scheme for Questions and Answers",
    "abstract": "The present study proposes an annotation scheme for classifying the content and discourse contribution of question-answer pairs. We propose detailed guidelines for using the scheme and apply them to dialogues in English, Spanish, and Dutch. Finally, we report on initial machine learning experiments for automatic annotation",
    "volume": "workshop",
    "checked": true,
    "id": "0414735d79f0433a10eb6e0646073ff2c5732c85",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4028": {
    "title": "Towards a General Abstract Meaning Representation Corpus for Brazilian Portuguese",
    "abstract": "Abstract Meaning Representation (AMR) is a recent and prominent semantic representation with good acceptance and several applications in the Natural Language Processing area. For English, there is a large annotated corpus (with approximately 39K sentences) that supports the research with the representation. However, to the best of our knowledge, there is only one restricted corpus for Portuguese, which contains 1,527 sentences. In this context, this paper presents an effort to build a general purpose AMR-annotated corpus for Brazilian Portuguese by translating and adapting AMR English guidelines. Our results show that such approach is feasible, but there are some challenging phenomena to solve. More than this, efforts are necessary to increase the coverage of the corresponding lexical resource that supports the annotation",
    "volume": "workshop",
    "checked": true,
    "id": "17a5889f93a5fd0060b73c5e8ef2e580b52de335",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-4101": {
    "title": "A Repository of Conversational Datasets",
    "abstract": "Progress in Machine Learning is often driven by the availability of large datasets, and consistent evaluation metrics for comparing modeling approaches. To this end, we present a repository of conversational datasets consisting of hundreds of millions of examples, and a standardised evaluation procedure for conversational response selection models using 1-of-100 accuracy. The repository contains scripts that allow researchers to reproduce the standard datasets, or to adapt the pre-processing and data filtering steps to their needs. We introduce and evaluate several competitive baselines for conversational response selection, whose implementations are shared in the repository, as well as a neural encoder model that is trained on the entire training set",
    "volume": "workshop",
    "checked": true,
    "id": "b9f4a9a35201ad92d9425f911d08e737b4615eb3",
    "citation_count": 44
  },
  "https://aclanthology.org/W19-4102": {
    "title": "A Simple but Effective Method to Incorporate Multi-turn Context with BERT for Conversational Machine Comprehension",
    "abstract": "Conversational machine comprehension (CMC) requires understanding the context of multi-turn dialogue. Using BERT, a pretraining language model, has been successful for single-turn machine comprehension, while modeling multiple turns of question answering with BERT has not been established because BERT has a limit on the number and the length of input sequences. In this paper, we propose a simple but effective method with BERT for CMC. Our method uses BERT to encode a paragraph independently conditioned with each question and each answer in a multi-turn context. Then, the method predicts an answer on the basis of the paragraph representations encoded with BERT. The experiments with representative CMC datasets, QuAC and CoQA, show that our method outperformed recently published methods (+0.8 F1 on QuAC and +2.1 F1 on CoQA). In addition, we conducted a detailed analysis of the effects of the number and types of dialogue history on the accuracy of CMC, and we found that the gold answer history, which may not be given in an actual conversation, contributed to the model performance most on both datasets",
    "volume": "workshop",
    "checked": true,
    "id": "455a8ed47debe6706eb426bd57fdd76f45d29e22",
    "citation_count": 28
  },
  "https://aclanthology.org/W19-4103": {
    "title": "Augmenting Neural Response Generation with Context-Aware Topical Attention",
    "abstract": "Sequence-to-Sequence (Seq2Seq) models have witnessed a notable success in generating natural conversational exchanges. Notwithstanding the syntactically well-formed responses generated by these neural network models, they are prone to be acontextual, short and generic. In this work, we introduce a Topical Hierarchical Recurrent Encoder Decoder (THRED), a novel, fully data-driven, multi-turn response generation system intended to produce contextual and topic-aware responses. Our model is built upon the basic Seq2Seq model by augmenting it with a hierarchical joint attention mechanism that incorporates topical concepts and previous interactions into the response generation. To train our model, we provide a clean and high-quality conversational dataset mined from Reddit comments. We evaluate THRED on two novel automated metrics, dubbed Semantic Similarity and Response Echo Index, as well as with human evaluation. Our experiments demonstrate that the proposed model is able to generate more diverse and contextually relevant responses compared to the strong baselines",
    "volume": "workshop",
    "checked": true,
    "id": "b36d693ff1ccaa0b08e2728bd2dc01d896699b2b",
    "citation_count": 52
  },
  "https://aclanthology.org/W19-4104": {
    "title": "Building a Production Model for Retrieval-Based Chatbots",
    "abstract": "Response suggestion is an important task for building human-computer conversation systems. Recent approaches to conversation modeling have introduced new model architectures with impressive results, but relatively little attention has been paid to whether these models would be practical in a production setting. In this paper, we describe the unique challenges of building a production retrieval-based conversation system, which selects outputs from a whitelist of candidate responses. To address these challenges, we propose a dual encoder architecture which performs rapid inference and scales well with the size of the whitelist. We also introduce and compare two methods for generating whitelists, and we carry out a comprehensive analysis of the model and whitelists. Experimental results on a large, proprietary help desk chat dataset, including both offline metrics and a human evaluation, indicate production-quality performance and illustrate key lessons about conversation modeling in practice",
    "volume": "workshop",
    "checked": true,
    "id": "ac0ec8a6c609f76308ead61140f6f41c184b65c4",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-4105": {
    "title": "Co-Operation as an Asymmetric Form of Human-Computer Creativity. Case: Peace Machine",
    "abstract": "This theoretical paper identifies a need for a definition of asymmetric co-creativity where creativity is expected from the computational agent but not from the human user. Our co-operative creativity framework takes into account that the computational agent has a message to convey in a co-operative fashion, which introduces a trade-off on how creative the computer can be. The requirements of co-operation are identified from an interdisciplinary point of view. We divide co-operative creativity in message creativity, contextual creativity and communicative creativity. Finally these notions are applied in the context of the Peace Machine system concept",
    "volume": "workshop",
    "checked": true,
    "id": "5c3f86abbc0a66bf3d1b2cc50e69b95e134aed70",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4106": {
    "title": "Conversational Response Re-ranking Based on Event Causality and Role Factored Tensor Event Embedding",
    "abstract": "We propose a novel method for selecting coherent and diverse responses for a given dialogue context. The proposed method re-ranks response candidates generated from conversational models by using event causality relations between events in a dialogue history and response candidates (e.g., \"be stressed out\" precedes \"relieve stress\"). We use distributed event representation based on the Role Factored Tensor Model for a robust matching of event causality relations due to limited event causality knowledge of the system. Experimental results showed that the proposed method improved coherency and dialogue continuity of system responses",
    "volume": "workshop",
    "checked": true,
    "id": "6dac4fd90ed8180358969f32dbb22ac12f3b4d61",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4107": {
    "title": "DSTC7 Task 1: Noetic End-to-End Response Selection",
    "abstract": "Goal-oriented dialogue in complex domains is an extremely challenging problem and there are relatively few datasets. This task provided two new resources that presented different challenges: one was focused but small, while the other was large but diverse. We also considered several new variations on the next utterance selection problem: (1) increasing the number of candidates, (2) including paraphrases, and (3) not including a correct option in the candidate set. Twenty teams participated, developing a range of neural network models, including some that successfully incorporated external data to boost performance. Both datasets have been publicly released, enabling future work to build on these results, working towards robust goal-oriented dialogue systems",
    "volume": "workshop",
    "checked": true,
    "id": "6977db5f230e7c9a1c607823fb6a33e0e9fe33bd",
    "citation_count": 44
  },
  "https://aclanthology.org/W19-4108": {
    "title": "End-to-End Neural Context Reconstruction in Chinese Dialogue",
    "abstract": "We tackle the problem of context reconstruction in Chinese dialogue, where the task is to replace pronouns, zero pronouns, and other referring expressions with their referent nouns so that sentences can be processed in isolation without context. Following a standard decomposition of the context reconstruction task into referring expression detection and coreference resolution, we propose a novel end-to-end architecture for separately and jointly accomplishing this task. Key features of this model include POS and position encoding using CNNs and a novel pronoun masking mechanism. One perennial problem in building such models is the paucity of training data, which we address by augmenting previously-proposed methods to generate a large amount of realistic training data. The combination of more data and better models yields accuracy higher than the state-of-the-art method in coreference resolution and end-to-end context reconstruction",
    "volume": "workshop",
    "checked": true,
    "id": "879d35ecfcde3b91037cc759a4ca75ffc8265f59",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4109": {
    "title": "Energy-Based Modelling for Dialogue State Tracking",
    "abstract": "The uncertainties of language and the complexity of dialogue contexts make accurate dialogue state tracking one of the more challenging aspects of dialogue processing. To improve state tracking quality, we argue that relationships between different aspects of dialogue state must be taken into account as they can often guide a more accurate interpretation process. To this end, we present an energy-based approach to dialogue state tracking as a structured classification task. The novelty of our approach lies in the use of an energy network on top of a deep learning architecture to explore more signal correlations between network variables including input features and output labels. We demonstrate that the energy-based approach improves the performance of a deep learning dialogue state tracker towards state-of-the-art results without the need for many of the other steps required by current state-of-the-art methods",
    "volume": "workshop",
    "checked": true,
    "id": "24c5218f30839c6fb99e1da99bc358240872564c",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4110": {
    "title": "Evaluation and Improvement of Chatbot Text Classification Data Quality Using Plausible Negative Examples",
    "abstract": "We describe and validate a metric for estimating multi-class classifier performance based on cross-validation and adapted for improvement of small, unbalanced natural-language datasets used in chatbot design. Our experiences draw upon building recruitment chatbots that mediate communication between job-seekers and recruiters by exposing the ML/NLP dataset to the recruiting team. Evaluation approaches must be understandable to various stakeholders, and useful for improving chatbot performance. The metric, nex-cv, uses negative examples in the evaluation of text classification, and fulfils three requirements. First, it is actionable: it can be used by non-developer staff. Second, it is not overly optimistic compared to human ratings, making it a fast method for comparing classifiers. Third, it allows model-agnostic comparison, making it useful for comparing systems despite implementation differences. We validate the metric based on seven recruitment-domain datasets in English and German over the course of one year",
    "volume": "workshop",
    "checked": true,
    "id": "0580f66ef447a84d4f0091f3650bf128da30d8e6",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4111": {
    "title": "Improving Long Distance Slot Carryover in Spoken Dialogue Systems",
    "abstract": "Tracking the state of the conversation is a central component in task-oriented spoken dialogue systems. One such approach for tracking the dialogue state is slot carryover, where a model makes a binary decision if a slot from the context is relevant to the current turn. Previous work on the slot carryover task used models that made independent decisions for each slot. A close analysis of the results show that this approach results in poor performance over longer context dialogues. In this paper, we propose to jointly model the slots. We propose two neural network architectures, one based on pointer networks that incorporate slot ordering information, and the other based on transformer networks that uses self attention mechanism to model the slot interdependencies. Our experiments on an internal dialogue benchmark dataset and on the public DSTC2 dataset demonstrate that our proposed models are able to resolve longer distance slot references and are able to achieve competitive performance",
    "volume": "workshop",
    "checked": true,
    "id": "d6b0db33bd2952b616960f617a0343a28323fc7e",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4112": {
    "title": "Insights from Building an Open-Ended Conversational Agent",
    "abstract": "Dialogue systems and conversational agents are becoming increasingly popular in modern society. We conceptualized one such conversational agent, Microsoft's \"Ruuh\" with the promise to be able to talk to its users on any subject they choose. Building an open-ended conversational agent like Ruuh at onset seems like a daunting task, since the agent needs to think beyond the utilitarian notion of merely generating \"relevant\" responses and meet a wider range of user social needs, like expressing happiness when user's favourite sports team wins, sharing a cute comment on showing the pictures of the user's pet and so on. The agent also needs to detect and respond to abusive language, sensitive topics and trolling behaviour of the users. Many of these problems pose significant research challenges as well as product design limitations as one needs to circumnavigate the technical limitations to create an acceptable user experience. However, as the product reaches the real users the true test begins, and one realizes the challenges and opportunities that lie in the vast domain of conversations. With over 2.5 million real-world users till date who have generated over 300 million user conversations with Ruuh, there is a plethora of learning, insights and opportunities that we will talk about in this paper",
    "volume": "workshop",
    "checked": true,
    "id": "45efb14c59fd478f5c6e3173632a379273131846",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4113": {
    "title": "Learning to Explain: Answering Why-Questions via Rephrasing",
    "abstract": "Providing plausible responses to why questions is a challenging but critical goal for language based human-machine interaction. Explanations are challenging in that they require many different forms of abstract knowledge and reasoning. Previous work has either relied on human-curated structured knowledge bases or detailed domain representation to generate satisfactory explanations. They are also often limited to ranking pre-existing explanation choices. In our work, we contribute to the under-explored area of generating natural language explanations for general phenomena. We automatically collect large datasets of explanation-phenomenon pairs which allow us to train sequence-to-sequence models to generate natural language explanations. We compare different training strategies and evaluate their performance using both automatic scores and human ratings. We demonstrate that our strategy is sufficient to generate highly plausible explanations for general open-domain phenomena compared to other models trained on different datasets",
    "volume": "workshop",
    "checked": true,
    "id": "aad17ec243923c2e5726558bde5842ceb60680c2",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4114": {
    "title": "Multi-turn Dialogue Response Generation in an Adversarial Learning Framework",
    "abstract": "We propose an adversarial learning approach for generating multi-turn dialogue responses. Our proposed framework, hredGAN, is based on conditional generative adversarial networks (GANs). The GAN's generator is a modified hierarchical recurrent encoder-decoder network (HRED) and the discriminator is a word-level bidirectional RNN that shares context and word embeddings with the generator. During inference, noise samples conditioned on the dialogue history are used to perturb the generator's latent space to generate several possible responses. The final response is the one ranked best by the discriminator. The hredGAN shows improved performance over existing methods: (1) it generalizes better than networks trained using only the log-likelihood criterion, and (2) it generates longer, more informative and more diverse responses with high utterance and topic relevance even with limited training data. This performance improvement is demonstrated on the Movie triples and Ubuntu dialogue datasets with both the automatic and human evaluations",
    "volume": "workshop",
    "checked": true,
    "id": "7f1e1331cfa790d92da2ebeab234847d00862288",
    "citation_count": 24
  },
  "https://aclanthology.org/W19-4115": {
    "title": "Relevant and Informative Response Generation using Pointwise Mutual Information",
    "abstract": "A sequence-to-sequence model tends to generate generic responses with little information for input utterances. To solve this problem, we propose a neural model that generates relevant and informative responses. Our model has simple architecture to enable easy application to existing neural dialogue models. Specifically, using positive pointwise mutual information, it first identifies keywords that frequently co-occur in responses given an utterance. Then, the model encourages the decoder to use the keywords for response generation. Experiment results demonstrate that our model successfully diversifies responses relative to previous models",
    "volume": "workshop",
    "checked": true,
    "id": "a2f3fa32907116d6d8a37938bba579c4b8e92452",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4116": {
    "title": "Responsive and Self-Expressive Dialogue Generation",
    "abstract": "A neural conversation model is a promising approach to develop dialogue systems with the ability of chit-chat. It allows training a model in an end-to-end manner without complex rule design nor feature engineering. However, as a side effect, the neural model tends to generate safe but uninformative and insensitive responses like \"OK\" and \"I don't know.\" Such replies are called generic responses and regarded as a critical problem for user-engagement of dialogue systems. For a more engaging chit-chat experience, we propose a neural conversation model that generates responsive and self-expressive replies. Specifically, our model generates domain-aware and sentiment-rich responses. Experiments empirically confirmed that our model outperformed the sequence-to-sequence model; 68.1% of our responses were domain-aware with sentiment polarities, which was only 2.7% for responses generated by the sequence-to-sequence model",
    "volume": "workshop",
    "checked": true,
    "id": "6b8d0f69a8504dcde1351d048feff3793fa1ccda",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4201": {
    "title": "AX Semantics' Submission to the SIGMORPHON 2019 Shared Task",
    "abstract": "This paper describes the AX Semantics' submission to the SIGMORPHON 2019 shared task on morphological reinflection. We implemented two systems, both tackling the task for all languages in one codebase, without any underlying language specific features. The first one is an encoder-decoder model using AllenNLP; the second system uses the same model modified by a custom trainer that trains only with the target language resources after a specific threshold. We especially focused on building an implementation using AllenNLP with out-of-the-box methods to facilitate easy operation and reuse",
    "volume": "workshop",
    "checked": true,
    "id": "701eda5aa28701aa8a8a885a3d3276cff8893ec5",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4202": {
    "title": "Cognate Projection for Low-Resource Inflection Generation",
    "abstract": "We propose cognate projection as a method of crosslingual transfer for inflection generation in the context of the SIGMORPHON 2019 Shared Task. The results on four language pairs show the method is effective when no low-resource training data is available",
    "volume": "workshop",
    "checked": true,
    "id": "7d56b8ad79f6e8911b3dd2736562af14f6e75ebf",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4203": {
    "title": "Cross-Lingual Lemmatization and Morphology Tagging with Two-Stage Multilingual BERT Fine-Tuning",
    "abstract": "We present our CHARLES-SAARLAND system for the SIGMORPHON 2019 Shared Task on Crosslinguality and Context in Morphology, in task 2, Morphological Analysis and Lemmatization in Context. We leverage the multilingual BERT model and apply several fine-tuning strategies introduced by UDify demonstrating exceptional evaluation performance on morpho-syntactic tasks. Our results show that fine-tuning multilingual BERT on the concatenation of all available treebanks allows the model to learn cross-lingual information that is able to boost lemmatization and morphology tagging accuracy over fine-tuning it purely monolingually. Unlike UDify, however, we show that when paired with additional character-level and word-level LSTM layers, a second stage of fine-tuning on each treebank individually can improve evaluation even further. Out of all submissions for this shared task, our system achieves the highest average accuracy and f1 score in morphology tagging and places second in average lemmatization accuracy",
    "volume": "workshop",
    "checked": true,
    "id": "ab90eb057e4c6dd3a36d556eb646af3bd0c7dca0",
    "citation_count": 15
  },
  "https://aclanthology.org/W19-4204": {
    "title": "CBNU System for SIGMORPHON 2019 Shared Task 2: a Pipeline Model",
    "abstract": "In this paper we describe our system for morphological analysis and lemmatization in context, using a transformer-based sequence to sequence model and a biaffine attention based BiLSTM model. First, a lemma is produced for a given word, and then both the lemma and the given word are used for morphological analysis. We also make use of character level word encodings and trainable encodings to improve accuracy. Overall, our system ranked fifth in lemmatization and sixth in morphological accuracy among twelve systems, and demonstrated considerable improvements over the baseline in morphological analysis",
    "volume": "workshop",
    "checked": true,
    "id": "71fb5a77b4009d9d61d1ac777d0b64450605b46f",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4205": {
    "title": "Morpheus: A Neural Network for Jointly Learning Contextual Lemmatization and Morphological Tagging",
    "abstract": "In this study, we present Morpheus, a joint contextual lemmatizer and morphological tagger. Morpheus is based on a neural sequential architecture where inputs are the characters of the surface words in a sentence and the outputs are the minimum edit operations between surface words and their lemmata as well as the morphological tags assigned to the words. The experiments on the datasets in nearly 100 languages provided by SigMorphon 2019 Shared Task 2 organizers show that the performance of Morpheus is comparable to the state-of-the-art system in terms of lemmatization. In morphological tagging, on the other hand, Morpheus significantly outperforms the SigMorphon baseline. In our experiments, we also show that the neural encoder-decoder architecture trained to predict the minimum edit operations can produce considerably better results than the architecture trained to predict the characters in lemmata directly as in previous studies. According to the SigMorphon 2019 Shared Task 2 results, Morpheus has placed 3rd in lemmatization and reached the 9th place in morphological tagging among all participant teams",
    "volume": "workshop",
    "checked": true,
    "id": "15573dacd784745fbd5aa778528dd8e3ec975491",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4206": {
    "title": "Multi-Team: A Multi-attention, Multi-decoder Approach to Morphological Analysis",
    "abstract": "This paper describes our submission to SIGMORPHON 2019 Task 2: Morphological analysis and lemmatization in context. Our model is a multi-task sequence to sequence neural network, which jointly learns morphological tagging and lemmatization. On the encoding side, we exploit character-level as well as contextual information. We introduce a multi-attention decoder to selectively focus on different parts of character and word sequences. To further improve the model, we train on multiple datasets simultaneously and use external embeddings for initialization. Our final model reaches an average morphological tagging F1 score of 94.54 and a lemma accuracy of 93.91 on the test data, ranking respectively 3rd and 6th out of 13 teams in the SIGMORPHON 2019 shared task",
    "volume": "workshop",
    "checked": true,
    "id": "a5877930bc06ba4af48c501592b9602286907076",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4207": {
    "title": "IT–IST at the SIGMORPHON 2019 Shared Task: Sparse Two-headed Models for Inflection",
    "abstract": "This paper presents the Instituto de Telecomunicações–Instituto Superior Técnico submission to Task 1 of the SIGMORPHON 2019 Shared Task. Our models combine sparse sequence-to-sequence models with a two-headed attention mechanism that learns separate attention distributions for the lemma and inflectional tags. Among submissions to Task 1, our models rank second and third. Despite the low data setting of the task (only 100 in-language training examples), they learn plausible inflection patterns and often concentrate all probability mass into a small set of hypotheses, making beam search exact",
    "volume": "workshop",
    "checked": true,
    "id": "e6272f72f780fa4a14cc83569abb7389bcf592d1",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-4208": {
    "title": "CMU-01 at the SIGMORPHON 2019 Shared Task on Crosslinguality and Context in Morphology",
    "abstract": "This paper presents the submission by the CMU-01 team to the SIGMORPHON 2019 task 2 of Morphological Analysis and Lemmatization in Context. This task requires us to produce the lemma and morpho-syntactic description of each token in a sequence, for 107 treebanks. We approach this task with a hierarchical neural conditional random field (CRF) model which predicts each coarse-grained feature (eg. POS, Case, etc.) independently. However, most treebanks are under-resourced, thus making it challenging to train deep neural models for them. Hence, we propose a multi-lingual transfer training regime where we transfer from multiple related languages that share similar typology",
    "volume": "workshop",
    "checked": true,
    "id": "1578fba4a2b2ba819986e32c7da6ebbaf9aacf41",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4209": {
    "title": "Cross-lingual morphological inflection with explicit alignment",
    "abstract": "This paper describes two related systems for cross-lingual morphological inflection for SIGMORPHON 2019 Shared Task participation. Both sets of results submitted to the shared task for evaluation are obtained using a simple approach of predicting transducer actions based on initial alignments on the training set, where cross-lingual transfer is limited to only using the high-resource language data as additional training set. The performance of the system does not reach the performance of the top two systems in the competition. However, we show that results can be improved with further tuning. We also present further analyses demonstrating that the cross-lingual gain is rather modest",
    "volume": "workshop",
    "checked": true,
    "id": "94a02d11f9e6842415e11a329701537de0a20b8a",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4210": {
    "title": "THOMAS: The Hegemonic OSU Morphological Analyzer using Seq2seq",
    "abstract": "This paper describes the OSU submission to the SIGMORPHON 2019 shared task, Crosslinguality and Context in Morphology. Our system addresses the contextual morphological analysis subtask of Task 2, which is to produce the morphosyntactic description (MSD) of each fully inflected word within a given sentence. We frame this as a sequence generation task and employ a neural encoder-decoder (seq2seq) architecture to generate the sequence of MSD tags given the encoded representation of each token. Follow-up analyses reveal that our system most significantly improves performance on morphologically complex languages whose inflected word forms typically have longer MSD tag sequences. In addition, our system seems to capture the structured correlation between MSD tags, such as that between the \"verb\" tag and TAM-related tags",
    "volume": "workshop",
    "checked": true,
    "id": "4c0b9873acd1a15efda383dd67a32e6fe8514d48",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4211": {
    "title": "Sigmorphon 2019 Task 2 system description paper: Morphological analysis in context for many languages, with supervision from only a few",
    "abstract": "This paper presents the UNT HiLT+Ling system for the Sigmorphon 2019 shared Task 2: Morphological Analysis and Lemmatization in Context. Our core approach focuses on the morphological tagging task; part-of-speech tagging and lemmatization are treated as secondary tasks. Given the highly multilingual nature of the task, we propose an approach which makes minimal use of the supplied training data, in order to be extensible to languages without labeled training data for the morphological inflection task. Specifically, we use a parallel Bible corpus to align contextual embeddings at the verse level. The aligned verses are used to build cross-language translation matrices, which in turn are used to map between embedding spaces for the various languages. Finally, we use sets of inflected forms, primarily from a high-resource language, to induce vector representations for individual UniMorph tags. Morphological analysis is performed by matching vector representations to embeddings for individual tokens. While our system results are dramatically below the average system submitted for the shared task evaluation campaign, our method is (we suspect) unique in its minimal reliance on labeled training data",
    "volume": "workshop",
    "checked": true,
    "id": "541699bdf326854b7a595214161c0e0312138509",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4212": {
    "title": "UDPipe at SIGMORPHON 2019: Contextualized Embeddings, Regularization with Morphological Categories, Corpora Merging",
    "abstract": "We present our contribution to the SIGMORPHON 2019 Shared Task: Crosslinguality and Context in Morphology, Task 2: contextual morphological analysis and lemmatization. We submitted a modification of the UDPipe 2.0, one of best-performing systems of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies and an overall winner of the The 2018 Shared Task on Extrinsic Parser Evaluation. As our first improvement, we use the pretrained contextualized embeddings (BERT) as additional inputs to the network; secondly, we use individual morphological features as regularization; and finally, we merge the selected corpora of the same language. In the lemmatization task, our system exceeds all the submitted systems by a wide margin with lemmatization accuracy 95.78 (second best was 95.00, third 94.46). In the morphological analysis, our system placed tightly second: our morphological analysis accuracy was 93.19, the winning system's 93.23",
    "volume": "workshop",
    "checked": true,
    "id": "5e87c8ce78dd91c6e531406bca1d97eea9d02aa9",
    "citation_count": 19
  },
  "https://aclanthology.org/W19-4213": {
    "title": "CUNI–Malta system at SIGMORPHON 2019 Shared Task on Morphological Analysis and Lemmatization in context: Operation-based word formation",
    "abstract": "This paper presents the submission by the Charles University-University of Malta team to the SIGMORPHON 2019 Shared Task on Morphological Analysis and Lemmatization in context. We present a lemmatization model based on previous work on neural transducers (Makarov and Clematide, 2018b; Aharoni and Goldberg, 2016). The key difference is that our model transforms the whole word form in every step, instead of consuming it character by character. We propose a merging strategy inspired by Byte-Pair-Encoding that reduces the space of valid operations by merging frequent adjacent operations. The resulting operations not only encode the actions to be performed but the relative position in the word token and how characters need to be transformed. Our morphological tagger is a vanilla biLSTM tagger that operates over operation representations, encoding operations and words in a hierarchical manner. Even though relative performance according to metrics is below the baseline, experiments show that our models capture important associations between interpretable operation labels and fine-grained morpho-syntax labels",
    "volume": "workshop",
    "checked": true,
    "id": "12a94e021493fa7ef266f1711927e840ccb33411",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4214": {
    "title": "A Little Linguistics Goes a Long Way: Unsupervised Segmentation with Limited Language Specific Guidance",
    "abstract": "We present de-lexical segmentation, a linguistically motivated alternative to greedy or other unsupervised methods, requiring only minimal language specific input. Our technique involves creating a small grammar of closed-class affixes which can be written in a few hours. The grammar over generates analyses for word forms attested in a raw corpus which are disambiguated based on features of the linguistic base proposed for each form. Extending the grammar to cover orthographic, morpho-syntactic or lexical variation is simple, making it an ideal solution for challenging corpora with noisy, dialect-inconsistent, or otherwise non-standard content. In two evaluations, we consistently outperform competitive unsupervised baselines and approach the performance of state-of-the-art supervised models trained on large amounts of data, providing evidence for the value of linguistic input during preprocessing",
    "volume": "workshop",
    "checked": true,
    "id": "77aeb2e1e47be032284b8451652b4a5052a41ad2",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4215": {
    "title": "Equiprobable mappings in weighted constraint grammars",
    "abstract": "We show that MaxEnt is so rich that it can distinguish between any two different mappings: there always exists a nonnegative weight vector which assigns them different MaxEnt probabilities. Stochastic HG instead does admit equiprobable mappings and we give a complete formal characterization of them",
    "volume": "workshop",
    "checked": true,
    "id": "342b2739d48a164c30dd7a40b61b5277a3cea812",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4216": {
    "title": "Unbounded Stress in Subregular Phonology",
    "abstract": "This paper situates culminative unbounded stress systems within the subregular hierarchy for functions. While Baek (2018) has argued that such systems can be uniformly understood as input tier-based strictly local constraints, we show here that default-to-opposite-side and default-to-same-side stress systems belong to distinct subregular classes when they are viewed as functions that assign primary stress to underlying forms. While the former system can be captured by input tier-based input strictly local functions, a subsequential function class that we define here, the latter system is not subsequential, though it is weakly deterministic according to McCollum et al.'s (2018) non-interaction criterion. Our results motivate the extension of recently proposed subregular language classes to subregular functions and argue in favor of McCollum et al's definition of weak determinism over that of Heinz and Lai (2013)",
    "volume": "workshop",
    "checked": true,
    "id": "7bcfa8384622e15a40f25e1b388485f8c09c1aec",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-4217": {
    "title": "Data mining Mandarin tone contour shapes",
    "abstract": "In spontaneous speech, Mandarin tones that belong to the same tone category may exhibit many different contour shapes. We explore the use of time-series data mining techniques for understanding the variability of tones in a large corpus of Mandarin newscast speech. First, we adapt a graph-based approach to characterize the clusters (fuzzy types) of tone contour shapes observed in each tone n-gram category. Second, we show correlations between these realized contour shape clusters and a bag of automatically extracted linguistic features. We discuss the implications of the current study within the context of phonological and information theory",
    "volume": "workshop",
    "checked": true,
    "id": "fb88bbf696cb077637e6cad87345fab4679dfd9a",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4218": {
    "title": "Convolutional neural networks for low-resource morpheme segmentation: baseline or state-of-the-art?",
    "abstract": "We apply convolutional neural networks to the task of shallow morpheme segmentation using low-resource datasets for 5 different languages. We show that both in fully supervised and semi-supervised settings our model beats previous state-of-the-art approaches. We argue that convolutional neural networks reflect local nature of morpheme segmentation better than other semi-supervised approaches",
    "volume": "workshop",
    "checked": true,
    "id": "144a9708dccb4fd3df535cc5448ff04079388080",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4219": {
    "title": "What do phone embeddings learn about Phonology?",
    "abstract": "Recent work has looked at evaluation of phone embeddings using sound analogies and correlations between distinctive feature space and embedding space. It has not been clear what aspects of natural language phonology are learnt by neural network inspired distributed representational models such as word2vec. To study the kinds of phonological relationships learnt by phone embeddings, we present artificial phonology experiments that show that phone embeddings learn paradigmatic relationships such as phonemic and allophonic distribution quite well. They are also able to capture co-occurrence restrictions among vowels such as those observed in languages with vowel harmony. However, they are unable to learn co-occurrence restrictions among the class of consonants",
    "volume": "workshop",
    "checked": true,
    "id": "4d1419c428d9781175ced723166a113a2cc5b02c",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4220": {
    "title": "Inverting and Modeling Morphological Inflection",
    "abstract": "Previous \"wug\" tests (Berko, 1958) on Japanese verbal inflection have demonstrated that Japanese speakers, both adults and children, cannot inflect novel present tense forms to \"correct\" past tense forms predicted by rules of existent verbs (de Chene, 1982; Vance, 1987, 1991; Klafehn, 2003, 2013), indicating that Japanese verbs are merely stored in the mental lexicon. However, the implicit assumption that present tense forms are bases for verbal inflection should not be blindly extended to morphologically rich languages like Japanese in which both present and past tense forms are morphologically complex without inherent direction (Albright, 2002). Interestingly, there are also independent observations in the acquisition literature to suggest that past tense forms may be bases for verbal inflection in Japanese (Klafehn, 2003; Murasugi et al., 2010; Hirose, 2017; Tatsumi et al., 2018). In this paper, we computationally simulate two directions of verbal inflection in Japanese, Present → Past and Past → Present, with the rule-based computational model called Minimal Generalization Learner (MGL; Albright and Hayes, 2003) and experimentally evaluate the model with the bidirectional \"wug\" test where humans inflect novel verbs in two opposite directions. We conclude that Japanese verbs can be computed online via some generalizations and those generalizations do depend on the direction of morphological inflection",
    "volume": "workshop",
    "checked": true,
    "id": "2471656a995678c873953911810ec7e9d4e2f68b",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4221": {
    "title": "Augmenting a German Morphological Database by Data-Intense Methods",
    "abstract": "This paper deals with the automatic enhancement of a new German morphological database. While there are some databases for flat word segmentation, this is the first available resource which can be directly used for deep parsing of German words. We combine the entries of this morphological database with the morphological tools SMOR and Moremorph and a context-based evaluation method which builds on a large Wikipedia corpus. We describe the state of the art and the essential characteristics of the database and the context method. The approach is tested on an inflight magazine of Lufthansa. We derive over 5,000 new instances of complex words. The coverage for the lemma types reaches up to over 99 percent. The precision of new found complex splits and monomorphemes is between 0.93 and 0.99",
    "volume": "workshop",
    "checked": true,
    "id": "cd778b74b3b15be53e3034289a489626ab09fccd",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4222": {
    "title": "Unsupervised Morphological Segmentation for Low-Resource Polysynthetic Languages",
    "abstract": "Polysynthetic languages pose a challenge for morphological analysis due to the root-morpheme complexity and to the word class \"squish\". In addition, many of these polysynthetic languages are low-resource. We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) (Eskander et al., 2016). We experiment with four languages from the Uto-Aztecan family. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages",
    "volume": "workshop",
    "checked": true,
    "id": "e6ffe957179541a4cad7b6eba888c79c4aad8f91",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-4223": {
    "title": "Weakly deterministic transformations are subregular",
    "abstract": "Whether phonological transformations in general are subregular is an open question. This is the case for most transformations, which have been shown to be subsequential, but it is not known whether weakly deterministic mappings form a proper subset of the regular functions. This paper demonstrates that there are regular functions that are not weakly deterministic, and, because all attested processes are weakly deterministic, supports the subregular hypothesis",
    "volume": "workshop",
    "checked": true,
    "id": "fdbadf9ad9fee7bb94e47d72923272e8190221d4",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4224": {
    "title": "Encoder-decoder models for latent phonological representations of words",
    "abstract": "We use sequence-to-sequence networks trained on sequential phonetic encoding tasks to construct compositional phonological representations of words. We show that the output of an encoder network can predict the phonetic durations of American English words better than a number of alternative forms. We also show that the model's learned representations map onto existing measures of words' phonological structure (phonological neighborhood density and phonotactic probability)",
    "volume": "workshop",
    "checked": true,
    "id": "64c0efc26a43b1d95064714b51fad5ca602f470e",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4225": {
    "title": "Action-Sensitive Phonological Dependencies",
    "abstract": "This paper defines a subregular class of functions called the tier-based synchronized strictly local (TSSL) functions. These functions are similar to the the tier-based input-output strictly local (TIOSL) functions, except that the locality condition is enforced not on the input and output streams, but on the computation history of the minimal subsequential finite-state transducer. We show that TSSL functions naturally describe rhythmic syncope while TIOSL functions cannot, and we argue that TSSL functions provide a more restricted characterization of rhythmic syncope than existing treatments within Optimality Theory",
    "volume": "workshop",
    "checked": true,
    "id": "1b81491c67c05f17e8cfe41f2835fff1c4e5da86",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4226": {
    "title": "The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection",
    "abstract": "The SIGMORPHON 2019 shared task on cross-lingual transfer and contextual analysis in morphology examined transfer learning of inflection between 100 language pairs, as well as contextual lemmatization and morphosyntactic description in 66 languages. The first task evolves past years' inflection tasks by examining transfer of morphological inflection knowledge from a high-resource language to a low-resource language. This year also presents a new second challenge on lemmatization and morphological feature analysis in context. All submissions featured a neural component and built on either this year's strong baselines or highly ranked systems from previous years' shared tasks. Every participating team improved in accuracy over the baselines for the inflection task (though not Levenshtein distance), and every team in the contextual analysis task improved on both state-of-the-art neural and non-neural baselines",
    "volume": "workshop",
    "checked": true,
    "id": "39f81aef964e076d7a089f597e8028b43b5675cf",
    "citation_count": 73
  },
  "https://aclanthology.org/W19-4301": {
    "title": "Deep Generalized Canonical Correlation Analysis",
    "abstract": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) – a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn and evaluate DGCCA representations for three downstream tasks: phonetic transcription from acoustic & articulatory measurements, recommending hashtags and recommending friends on a dataset of Twitter users",
    "volume": "workshop",
    "checked": true,
    "id": "db621b0ba5577a4962ee08bd113e1dcabc911ddb",
    "citation_count": 83
  },
  "https://aclanthology.org/W19-4302": {
    "title": "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks",
    "abstract": "While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner",
    "volume": "workshop",
    "checked": true,
    "id": "8659bf379ca8756755125a487c43cfe8611ce842",
    "citation_count": 292
  },
  "https://aclanthology.org/W19-4303": {
    "title": "Generative Adversarial Networks for Text Using Word2vec Intermediaries",
    "abstract": "Generative adversarial networks (GANs) have shown considerable success, especially in the realistic generation of images. In this work, we apply similar techniques for the generation of text. We propose a novel approach to handle the discrete nature of text, during training, using word embeddings. Our method is agnostic to vocabulary size and achieves competitive results relative to methods with various discrete gradient estimators",
    "volume": "workshop",
    "checked": true,
    "id": "2f32f6776a3030af78e0eb9186bfea6626e69a6d",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-4304": {
    "title": "An Evaluation of Language-Agnostic Inner-Attention-Based Representations in Machine Translation",
    "abstract": "In this paper, we explore a multilingual translation model with a cross-lingually shared layer that can be used as fixed-size sentence representation in different downstream tasks. We systematically study the impact of the size of the shared layer and the effect of including additional languages in the model. In contrast to related previous work, we demonstrate that the performance in translation does correlate with trainable downstream tasks. In particular, we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks. On the other hand, shorter representations lead to increased compression that is beneficial in non-trainable similarity tasks. We hypothesize that the training procedure on the downstream task enables the model to identify the encoded information that is useful for the specific task whereas non-trainable benchmarks can be confused by other types of information also encoded in the representation of a sentence",
    "volume": "workshop",
    "checked": true,
    "id": "5127dd9446a61e08aa1d68420ac8e4bc3f243b83",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4305": {
    "title": "Multilingual NMT with a Language-Independent Attention Bridge",
    "abstract": "In this paper, we propose an architecture for machine translation (MT) capable of obtaining multilingual sentence representations by incorporating an intermediate attention bridge that is shared across all languages. We train the model with language-specific encoders and decoders that are connected through an inner-attention layer on the encoder side. The attention bridge exploits the semantics from each language for translation and develops into a language-agnostic meaning representation that can efficiently be used for transfer learning. We present a new framework for the efficient development of multilingual neural machine translation (NMT) using this model and scheduled training. We have tested the approach in a systematic way with a multi-parallel data set. The model achieves substantial improvements over strong bilingual models and performs well for zero-shot translation, which demonstrates its ability of abstraction and transfer learning",
    "volume": "workshop",
    "checked": true,
    "id": "999601ae09003135e18f040df562b60d027ec024",
    "citation_count": 30
  },
  "https://aclanthology.org/W19-4306": {
    "title": "Efficient Language Modeling with Automatic Relevance Determination in Recurrent Neural Networks",
    "abstract": "Reduction of the number of parameters is one of the most important goals in Deep Learning. In this article we propose an adaptation of Doubly Stochastic Variational Inference for Automatic Relevance Determination (DSVI-ARD) for neural networks compression. We find this method to be especially useful in language modeling tasks, where large number of parameters in the input and output layers is often excessive. We also show that DSVI-ARD can be applied together with encoder-decoder weight tying allowing to achieve even better sparsity and performance. Our experiments demonstrate that more than 90% of the weights in both encoder and decoder layers can be removed with a minimal quality loss",
    "volume": "workshop",
    "checked": true,
    "id": "172ac6cdcb9746406bf1878706b844563e996e6a",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4307": {
    "title": "MoRTy: Unsupervised Learning of Task-specialized Word Embeddings by Autoencoding",
    "abstract": "Word embeddings have undoubtedly revolutionized NLP. However, pretrained embeddings do not always work for a specific task (or set of tasks), particularly in limited resource setups. We introduce a simple yet effective, self-supervised post-processing method that constructs task-specialized word representations by picking from a menu of reconstructing transformations to yield improved end-task performance (MORTY). The method is complementary to recent state-of-the-art approaches to inductive transfer via fine-tuning, and forgoes costly model architectures and annotation. We evaluate MORTY on a broad range of setups, including different word embedding methods, corpus sizes and end-task semantics. Finally, we provide a surprisingly simple recipe to obtain specialized embeddings that better fit end-tasks",
    "volume": "workshop",
    "checked": true,
    "id": "0e072e974b1ab940e47e84b47ebb77f1eab4573b",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4308": {
    "title": "Pitfalls in the Evaluation of Sentence Embeddings",
    "abstract": "Deep learning models continuously break new records across different NLP tasks. At the same time, their success exposes weaknesses of model evaluation. Here, we compile several key pitfalls of evaluation of sentence embeddings, a currently very popular NLP paradigm. These pitfalls include the comparison of embeddings of different sizes, normalization of embeddings, and the low (and diverging) correlations between transfer and probing tasks. Our motivation is to challenge the current evaluation of sentence embeddings and to provide an easy-to-access reference for future research. Based on our insights, we also recommend better practices for better future evaluations of sentence embeddings",
    "volume": "workshop",
    "checked": true,
    "id": "763da2860a71f522b2a585ca3c1325f22835cbfd",
    "citation_count": 11
  },
  "https://aclanthology.org/W19-4309": {
    "title": "Learning Bilingual Sentence Embeddings via Autoencoding and Computing Similarities with a Multilayer Perceptron",
    "abstract": "We propose a novel model architecture and training algorithm to learn bilingual sentence embeddings from a combination of parallel and monolingual data. Our method connects autoencoding and neural machine translation to force the source and target sentence embeddings to share the same space without the help of a pivot language or an additional transformation. We train a multilayer perceptron on top of the sentence embeddings to extract good bilingual sentence pairs from nonparallel or noisy parallel data. Our approach shows promising performance on sentence alignment recovery and the WMT 2018 parallel corpus filtering tasks with only a single model",
    "volume": "workshop",
    "checked": true,
    "id": "c3e51bb230ac333df3059f873a09b7c214bf09c4",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4310": {
    "title": "Specializing Distributional Vectors of All Words for Lexical Entailment",
    "abstract": "Semantic specialization methods fine-tune distributional word vectors using lexical knowledge from external resources (e.g. WordNet) to accentuate a particular relation between words. However, such post-processing methods suffer from limited coverage as they affect only vectors of words seen in the external resources. We present the first post-processing method that specializes vectors of all vocabulary words – including those unseen in the resources – for the asymmetric relation of lexical entailment (LE) (i.e., hyponymy-hypernymy relation). Leveraging a partially LE-specialized distributional space, our POSTLE (i.e., post-specialization for LE) model learns an explicit global specialization function, allowing for specialization of vectors of unseen words, as well as word vectors from other languages via cross-lingual transfer. We capture the function as a deep feed-forward neural network: its objective re-scales vector norms to reflect the concept hierarchy while simultaneously attracting hyponymy-hypernymy pairs to better reflect semantic similarity. An extended model variant augments the basic architecture with an adversarial discriminator. We demonstrate the usefulness and versatility of POSTLE models with different input distributional spaces in different scenarios (monolingual LE and zero-shot cross-lingual LE transfer) and tasks (binary and graded LE). We report consistent gains over state-of-the-art LE-specialization methods, and successfully LE-specialize word vectors for languages without any external lexical knowledge",
    "volume": "workshop",
    "checked": true,
    "id": "8702cc337db90ff2e198924c42fc407739ecae8d",
    "citation_count": 16
  },
  "https://aclanthology.org/W19-4311": {
    "title": "Composing Noun Phrase Vector Representations",
    "abstract": "Vector representations of words have seen an increasing success over the past years in a variety of NLP tasks. While there seems to be a consensus about the usefulness of word embeddings and how to learn them, it is still unclear which representations can capture the meaning of phrases or even whole sentences. Recent work has shown that simple operations outperform more complex deep architectures. In this work, we propose two novel constraints for computing noun phrase vector representations. First, we propose that the semantic and not the syntactic contribution of each component of a noun phrase should be considered, so that the resulting composed vectors express more of the phrase meaning. Second, the composition process of the two phrase vectors should apply suitable dimensions' selection in a way that specific semantic features captured by the phrase's meaning become more salient. Our proposed methods are compared to 11 other approaches, including popular baselines and a neural net architecture, and are evaluated across 6 tasks and 2 datasets. Our results show that these constraints lead to more expressive phrase representations and can be applied to other state-of-the-art methods to improve their performance",
    "volume": "workshop",
    "checked": true,
    "id": "11e76893bd8704944f347178e2dceaf2dd0851e3",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4312": {
    "title": "Towards Robust Named Entity Recognition for Historic German",
    "abstract": "In this paper we study the influence of using language model pre-training for named entity recognition for Historic German. We achieve new state-of-the-art results using carefully chosen training data for language models. For a low-resource domain like named entity recognition for Historic German, language model pre-training can be a strong competitor to CRF-only methods. We show that language model pre-training can be more effective than using transfer-learning with labeled datasets. Furthermore, we introduce a new language model pre-training objective, synthetic masked language model pre-training (SMLM), that allows a transfer from one domain (contemporary texts) to another domain (historical texts) by using only the same (character) vocabulary. Results show that using SMLM can achieve comparable results for Historic named entity recognition, even when they are only trained on contemporary texts. Our pre-trained character-based language models improve upon classical CRF-based methods and previous work on Bi-LSTMs by boosting F1 score performance by up to 6%",
    "volume": "workshop",
    "checked": true,
    "id": "d86ce22e5d7941809e2d8af3d373be1d042079a9",
    "citation_count": 16
  },
  "https://aclanthology.org/W19-4313": {
    "title": "On Evaluating Embedding Models for Knowledge Base Completion",
    "abstract": "Knowledge graph embedding models have recently received significant attention in the literature. These models learn latent semantic representations for the entities and relations in a given knowledge base; the representations can be used to infer missing knowledge. In this paper, we study the question of how well recent embedding models perform for the task of knowledge base completion, i.e., the task of inferring new facts from an incomplete knowledge base. We argue that the entity ranking protocol, which is currently used to evaluate knowledge graph embedding models, is not suitable to answer this question since only a subset of the model predictions are evaluated. We propose an alternative entity-pair ranking protocol that considers all model predictions as a whole and is thus more suitable to the task. We conducted an experimental study on standard datasets and found that the performance of popular embeddings models was unsatisfactory under the new protocol, even on datasets that are generally considered to be too easy. Moreover, we found that a simple rule-based model often provided superior performance. Our findings suggest that there is a need for more research into embedding models as well as their training strategies for the task of knowledge base completion",
    "volume": "workshop",
    "checked": true,
    "id": "b8aadaaa203d88f1f28cdc66866765132f291923",
    "citation_count": 35
  },
  "https://aclanthology.org/W19-4314": {
    "title": "Constructive Type-Logical Supertagging With Self-Attention Networks",
    "abstract": "We propose a novel application of self-attention networks towards grammar induction. We present an attention-based supertagger for a refined type-logical grammar, trained on constructing types inductively. In addition to achieving a high overall type accuracy, our model is able to learn the syntax of the grammar's type system along with its denotational semantics. This lifts the closed world assumption commonly made by lexicalized grammar supertaggers, greatly enhancing its generalization potential. This is evidenced both by its adequate accuracy over sparse word types and its ability to correctly construct complex types never seen during training, which, to the best of our knowledge, was as of yet unaccomplished",
    "volume": "workshop",
    "checked": true,
    "id": "ae0b3c079399b685d038f6f73545e91ac97f910c",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-4315": {
    "title": "Auto-Encoding Variational Neural Machine Translation",
    "abstract": "We present a deep generative model of bilingual sentence pairs for machine translation. The model generates source and target sentences jointly from a shared latent representation and is parameterised by neural networks. We perform efficient training using amortised variational inference and reparameterised gradients. Additionally, we discuss the statistical implications of joint modelling and propose an efficient approximation to maximum a posteriori decoding for fast test-time predictions. We demonstrate the effectiveness of our model in three machine translation scenarios: in-domain training, mixed-domain training, and learning from a mix of gold-standard and synthetic data. Our experiments show consistently that our joint formulation outperforms conditional modelling (i.e. standard neural machine translation) in all such scenarios",
    "volume": "workshop",
    "checked": true,
    "id": "c1b6dc9d419ca8eebd8df456d98819a1f0492df1",
    "citation_count": 23
  },
  "https://aclanthology.org/W19-4316": {
    "title": "Learning Bilingual Word Embeddings Using Lexical Definitions",
    "abstract": "Bilingual word embeddings, which represent lexicons of different languages in a shared embedding space, are essential for supporting semantic and knowledge transfers in a variety of cross-lingual NLP tasks. Existing approaches to training bilingual word embeddings require either large collections of pre-defined seed lexicons that are expensive to obtain, or parallel sentences that comprise coarse and noisy alignment. In contrast, we propose BiLex that leverages publicly available lexical definitions for bilingual word embedding learning. Without the need of predefined seed lexicons, BiLex comprises a novel word pairing strategy to automatically identify and propagate the precise fine-grain word alignment from lexical definitions. We evaluate BiLex in word-level and sentence-level translation tasks, which seek to find the cross-lingual counterparts of words and sentences respectively. BiLex significantly outperforms previous embedding methods on both tasks",
    "volume": "workshop",
    "checked": true,
    "id": "ee061b0c3c43d12c4dc00703c14db2b41d47fd91",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-4317": {
    "title": "An Empirical Study on Pre-trained Embeddings and Language Models for Bot Detection",
    "abstract": "Fine-tuning pre-trained language models has significantly advanced the state of art in a wide range of NLP downstream tasks. Usually, such language models are learned from large and well-formed text corpora from e.g. encyclopedic resources, books or news. However, a significant amount of the text to be analyzed nowadays is Web data, often from social media. In this paper we consider the research question: How do standard pre-trained language models generalize and capture the peculiarities of rather short, informal and frequently automatically generated text found in social media? To answer this question, we focus on bot detection in Twitter as our evaluation task and test the performance of fine-tuning approaches based on language models against popular neural architectures such as LSTM and CNN combined with pre-trained and contextualized embeddings. Our results also show strong performance variations among the different language model approaches, which suggest further research",
    "volume": "workshop",
    "checked": true,
    "id": "f6d4392a5df9a7265d7939c6ac003946c199086a",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4318": {
    "title": "Probing Multilingual Sentence Representations With X-Probe",
    "abstract": "This paper extends the task of probing sentence representations for linguistic insight in a multilingual domain. In doing so, we make two contributions: first, we provide datasets for multilingual probing, derived from Wikipedia, in five languages, viz. English, French, German, Spanish and Russian. Second, we evaluate six sentence encoders for each language, each trained by mapping sentence representations to English sentence representations, using sentences in a parallel corpus. We discover that cross-lingually mapped representations are often better at retaining certain linguistic information than representations derived from English encoders trained on natural language inference (NLI) as a downstream task",
    "volume": "workshop",
    "checked": true,
    "id": "191e5153ccc9722c5a44d79c7ceda52c947a9dcb",
    "citation_count": 14
  },
  "https://aclanthology.org/W19-4319": {
    "title": "Fine-Grained Entity Typing in Hyperbolic Space",
    "abstract": "How can we represent hierarchical information present in large type inventories for entity typing? We study the suitability of hyperbolic embeddings to capture hierarchical relations between mentions in context and their target types in a shared vector space. We evaluate on two datasets and propose two different techniques to extract hierarchical information from the type inventory: from an expert-generated ontology and by automatically mining the dataset. The hyperbolic model shows improvements in some but not all cases over its Euclidean counterpart. Our analysis suggests that the adequacy of this geometry depends on the granularity of the type inventory and the representation of its distribution",
    "volume": "workshop",
    "checked": true,
    "id": "cded59d31ab4841baa1517ff0359f0f2f4b865f5",
    "citation_count": 21
  },
  "https://aclanthology.org/W19-4320": {
    "title": "Learning Multilingual Meta-Embeddings for Code-Switching Named Entity Recognition",
    "abstract": "In this paper, we propose Multilingual Meta-Embeddings (MME), an effective method to learn multilingual representations by leveraging monolingual pre-trained embeddings. MME learns to utilize information from these embeddings via a self-attention mechanism without explicit language identification. We evaluate the proposed embedding method on the code-switching English-Spanish Named Entity Recognition dataset in a multilingual and cross-lingual setting. The experimental results show that our proposed method achieves state-of-the-art performance on the multilingual setting, and it has the ability to generalize to an unseen language task",
    "volume": "workshop",
    "checked": true,
    "id": "1d73839a843b70b0a59c566d8faa9e9a3c01a44e",
    "citation_count": 23
  },
  "https://aclanthology.org/W19-4321": {
    "title": "Investigating Sub-Word Embedding Strategies for the Morphologically Rich and Free Phrase-Order Hungarian",
    "abstract": "For morphologically rich languages, word embeddings provide less consistent semantic representations due to higher variance in word forms. Moreover, these languages often allow for less constrained word order, which further increases variance. For the highly agglutinative Hungarian, semantic accuracy of word embeddings measured on word analogy tasks drops by 50-75% compared to English. We observed that embeddings learn morphosyntax quite well instead. Therefore, we explore and evaluate several sub-word unit based embedding strategies – character n-grams, lemmatization provided by an NLP-pipeline, and segments obtained in unsupervised learning (morfessor) – to boost semantic consistency in Hungarian word vectors. The effect of changing embedding dimension and context window size have also been considered. Morphological analysis based lemmatization was found to be the best strategy to improve embeddings' semantic accuracy, whereas adding character n-grams was found consistently counterproductive in this regard",
    "volume": "workshop",
    "checked": true,
    "id": "a5d0575f92155fd67e64fd75e507a62cac871458",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4322": {
    "title": "A Self-Training Approach for Short Text Clustering",
    "abstract": "Short text clustering is a challenging problem when adopting traditional bag-of-words or TF-IDF representations, since these lead to sparse vector representations of the short texts. Low-dimensional continuous representations or embeddings can counter that sparseness problem: their high representational power is exploited in deep clustering algorithms. While deep clustering has been studied extensively in computer vision, relatively little work has focused on NLP. The method we propose, learns discriminative features from both an autoencoder and a sentence embedding, then uses assignments from a clustering algorithm as supervision to update weights of the encoder network. Experiments on three short text datasets empirically validate the effectiveness of our method",
    "volume": "workshop",
    "checked": true,
    "id": "e1c19cb4c394f9c89788acd309293fe1a2619fd9",
    "citation_count": 31
  },
  "https://aclanthology.org/W19-4323": {
    "title": "Improving Word Embeddings Using Kernel PCA",
    "abstract": "Word-based embedding approaches such as Word2Vec capture the meaning of words and relations between them, particularly well when trained with large text collections; however, they fail to do so with small datasets. Extensions such as fastText reduce the amount of data needed slightly, however, the joint task of learning meaningful morphology, syntactic and semantic representations still requires a lot of data. In this paper, we introduce a new approach to warm-start embedding models with morphological information, in order to reduce training time and enhance their performance. We use word embeddings generated using both word2vec and fastText models and enrich them with morphological information of words, derived from kernel principal component analysis (KPCA) of word similarity matrices. This can be seen as explicitly feeding the network morphological similarities and letting it learn semantic and syntactic similarities. Evaluating our models on word similarity and analogy tasks in English and German, we find that they not only achieve higher accuracies than the original skip-gram and fastText models but also require significantly less training data and time. Another benefit of our approach is that it is capable of generating a high-quality representation of infrequent words as, for example, found in very recent news articles with rapidly changing vocabularies. Lastly, we evaluate the different models on a downstream sentence classification task in which a CNN model is initialized with our embeddings and find promising results",
    "volume": "workshop",
    "checked": true,
    "id": "5dabc8d5c0ff64d23eb9a6d4410cb8a94d66adaf",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4324": {
    "title": "Assessing Incrementality in Sequence-to-Sequence Models",
    "abstract": "Since their inception, encoder-decoder models have successfully been applied to a wide array of problems in computational linguistics. The most recent successes are predominantly due to the use of different variations of attention mechanisms, but their cognitive plausibility is questionable. In particular, because past representations can be revisited at any point in time, attention-centric methods seem to lack an incentive to build up incrementally more informative representations of incoming sentences. This way of processing stands in stark contrast with the way in which humans are believed to process language: continuously and rapidly integrating new information as it is encountered. In this work, we propose three novel metrics to assess the behavior of RNNs with and without an attention mechanism and identify key differences in the way the different model types process sentences",
    "volume": "workshop",
    "checked": true,
    "id": "32c0e0b181eff3e9dd1c1dacec4138770008125e",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4325": {
    "title": "On Committee Representations of Adversarial Learning Models for Question-Answer Ranking",
    "abstract": "Adversarial training is a process in Machine Learning that explicitly trains models on adversarial inputs (inputs designed to deceive or trick the learning process) in order to make it more robust or accurate. In this paper we investigate how representing adversarial training models as committees can be used to effectively improve the performance of Question-Answer (QA) Ranking. We start by empirically probing the effects of adversarial training over multiple QA ranking algorithms, including the state-of-the-art Multihop Attention Network model. We evaluate these algorithms on several benchmark datasets and observe that, while adversarial training is beneficial to most baseline algorithms, there are cases where it may lead to overfitting and performance degradation. We investigate the causes of such degradation, and then propose a new representation procedure for this adversarial learning problem, based on committee learning, that not only is capable of consistently improving all baseline algorithms, but also outperforms the previous state-of-the-art algorithm by as much as 6% in NDCG",
    "volume": "workshop",
    "checked": true,
    "id": "9e1adbfb220ab3b2a8091d48d8384550727078a2",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4326": {
    "title": "Meta-Learning Improves Lifelong Relation Extraction",
    "abstract": "Most existing relation extraction models assume a fixed set of relations and are unable to adapt to exploit newly available supervision data to extract new relations. In order to alleviate such problems, there is the need to develop approaches that make relation extraction models capable of continuous adaptation and learning. We investigate and present results for such an approach, based on a combination of ideas from lifelong learning and optimization-based meta-learning. We evaluate the proposed approach on two recent lifelong relation extraction benchmarks, and demonstrate that it markedly outperforms current state-of-the-art approaches",
    "volume": "workshop",
    "checked": true,
    "id": "eedf74a43428cdd1ad3523f7e44e2b67ea4905a7",
    "citation_count": 19
  },
  "https://aclanthology.org/W19-4327": {
    "title": "Best Practices for Learning Domain-Specific Cross-Lingual Embeddings",
    "abstract": "Cross-lingual embeddings aim to represent words in multiple languages in a shared vector space by capturing semantic similarities across languages. They are a crucial component for scaling tasks to multiple languages by transferring knowledge from languages with rich resources to low-resource languages. A common approach to learning cross-lingual embeddings is to train monolingual embeddings separately for each language and learn a linear projection from the monolingual spaces into a shared space, where the mapping relies on a small seed dictionary. While there are high-quality generic seed dictionaries and pre-trained cross-lingual embeddings available for many language pairs, there is little research on how they perform on specialised tasks. In this paper, we investigate the best practices for constructing the seed dictionary for a specific domain. We evaluate the embeddings on the sequence labelling task of Curriculum Vitae parsing and show that the size of a bilingual dictionary, the frequency of the dictionary words in the domain corpora and the source of data (task-specific vs generic) influence performance. We also show that the less training data is available in the low-resource language, the more the construction of the bilingual dictionary matters, and demonstrate that some of the choices are crucial in the zero-shot transfer learning case",
    "volume": "workshop",
    "checked": true,
    "id": "b218938e68c8c26ae4ed43674f2af1303ac65060",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4328": {
    "title": "Effective Dimensionality Reduction for Word Embeddings",
    "abstract": "Pre-trained word embeddings are used in several downstream applications as well as for constructing representations for sentences, paragraphs and documents. Recently, there has been an emphasis on improving the pretrained word vectors through post-processing algorithms. One improvement area is reducing the dimensionality of word embeddings. Reducing the size of word embeddings can improve their utility in memory constrained devices, benefiting several real world applications. In this work, we present a novel technique that efficiently combines PCA based dimensionality reduction with a recently proposed post-processing algorithm (Mu and Viswanath, 2018), to construct effective word embeddings of lower dimensions. Empirical evaluations on several benchmarks show that our algorithm efficiently reduces the embedding size while achieving similar or (more often) better performance than original embeddings. We have released the source code along with this paper",
    "volume": "workshop",
    "checked": true,
    "id": "151b459fd2f47de1f24af7380aa290e79f01b0b9",
    "citation_count": 54
  },
  "https://aclanthology.org/W19-4329": {
    "title": "Learning Word Embeddings without Context Vectors",
    "abstract": "Most word embedding algorithms such as word2vec or fastText construct two sort of vectors: for words and for contexts. Naive use of vectors of only one sort leads to poor results. We suggest using indefinite inner product in skip-gram negative sampling algorithm. This allows us to use only one sort of vectors without loss of quality. Our \"context-free\" cf algorithm performs on par with SGNS on word similarity datasets",
    "volume": "workshop",
    "checked": true,
    "id": "86927f043ce2f26946fba90ce0cc8b0f8a0d715a",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-4330": {
    "title": "Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model",
    "abstract": "The scarcity of labeled training data across many languages is a significant roadblock for multilingual neural language processing. We approach the lack of in-language training data using sentence embeddings that map text written in different languages, but with similar meanings, to nearby embedding space representations. The representations are produced using a dual-encoder based model trained to maximize the representational similarity between sentence pairs drawn from parallel data. The representations are enhanced using multitask training and unsupervised monolingual corpora. The effectiveness of our multilingual sentence embeddings are assessed on a comprehensive collection of monolingual, cross-lingual, and zero-shot/few-shot learning tasks",
    "volume": "workshop",
    "checked": true,
    "id": "c546df1ce886f5f93b4cc7478415de5e37ea1a9f",
    "citation_count": 92
  },
  "https://aclanthology.org/W19-4331": {
    "title": "Modality-based Factorization for Multimodal Fusion",
    "abstract": "We propose a novel method, Modality-based Redundancy Reduction Fusion (MRRF), for understanding and modulating the relative contribution of each modality in multimodal inference tasks. This is achieved by obtaining an (M+1)-way tensor to consider the high-order relationships between M modalities and the output layer of a neural network model. Applying a modality-based tensor factorization method, which adopts different factors for different modalities, results in removing information present in a modality that can be compensated by other modalities, with respect to model outputs. This helps to understand the relative utility of information in each modality. In addition it leads to a less complicated model with less parameters and therefore could be applied as a regularizer avoiding overfitting. We have applied this method to three different multimodal datasets in sentiment analysis, personality trait recognition, and emotion recognition. We are able to recognize relationships and relative importance of different modalities in these tasks and achieves a 1% to 4% improvement on several evaluation measures compared to the state-of-the-art for all three tasks",
    "volume": "workshop",
    "checked": true,
    "id": "8337c7876194bf4cccdc670e10ff2a32d2452253",
    "citation_count": 18
  },
  "https://aclanthology.org/W19-4332": {
    "title": "Leveraging Pre-Trained Embeddings for Welsh Taggers",
    "abstract": "While the application of word embedding models to downstream Natural Language Processing (NLP) tasks has been shown to be successful, the benefits for low-resource languages is somewhat limited due to lack of adequate data for training the models. However, NLP research efforts for low-resource languages have focused on constantly seeking ways to harness pre-trained models to improve the performance of NLP systems built to process these languages without the need to re-invent the wheel. One such language is Welsh and therefore, in this paper, we present the results of our experiments on learning a simple multi-task neural network model for part-of-speech and semantic tagging for Welsh using a pre-trained embedding model from FastText. Our model's performance was compared with those of the existing rule-based stand-alone taggers for part-of-speech and semantic taggers. Despite its simplicity and capacity to perform both tasks simultaneously, our tagger compared very well with the existing taggers",
    "volume": "workshop",
    "checked": true,
    "id": "ca6ccf4581181fcec319bbccda1c093cb225dd30",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4401": {
    "title": "The many dimensions of algorithmic fairness in educational applications",
    "abstract": "The issues of algorithmic fairness and bias have recently featured prominently in many publications highlighting the fact that training the algorithms for maximum performance may often result in predictions that are biased against various groups. Educational applications based on NLP and speech processing technologies often combine multiple complex machine learning algorithms and are thus vulnerable to the same sources of bias as other machine learning systems. Yet such systems can have high impact on people's lives especially when deployed as part of high-stakes tests. In this paper we discuss different definitions of fairness and possible ways to apply them to educational applications. We then use simulated and real data to consider how test-takers' native language backgrounds can affect their automated scores on an English language proficiency assessment. We illustrate that total fairness may not be achievable and that different definitions of fairness may require different solutions",
    "volume": "workshop",
    "checked": true,
    "id": "a2fcc9dc14039ba3d7c3c4ca50e085b1235d71ac",
    "citation_count": 20
  },
  "https://aclanthology.org/W19-4402": {
    "title": "Predicting the Difficulty of Multiple Choice Questions in a High-stakes Medical Exam",
    "abstract": "Predicting the construct-relevant difficulty of Multiple-Choice Questions (MCQs) has the potential to reduce cost while maintaining the quality of high-stakes exams. In this paper, we propose a method for estimating the difficulty of MCQs from a high-stakes medical exam, where all questions were deliberately written to a common reading level. To accomplish this, we extract a large number of linguistic features and embedding types, as well as features quantifying the difficulty of the items for an automatic question-answering system. The results show that the proposed approach outperforms various baselines with a statistically significant difference. Best results were achieved when using the full feature set, where embeddings had the highest predictive power, followed by linguistic features. An ablation study of the various types of linguistic features suggested that information from all levels of linguistic processing contributes to predicting item difficulty, with features related to semantic ambiguity and the psycholinguistic properties of words having a slightly higher importance. Owing to its generic nature, the presented approach has the potential to generalize over other exams containing MCQs",
    "volume": "workshop",
    "checked": true,
    "id": "111c265b8747ee59fec58db96166d56fc5fa55c5",
    "citation_count": 15
  },
  "https://aclanthology.org/W19-4403": {
    "title": "An Intelligent Testing Strategy for Vocabulary Assessment of Chinese Second Language Learners",
    "abstract": "Vocabulary is one of the most important parts of language competence. Testing of vocabulary knowledge is central to research on reading and language. However, it usually costs a large amount of time and human labor to build an item bank and to test large number of students. In this paper, we propose a novel testing strategy by combining automatic item generation (AIG) and computerized adaptive testing (CAT) in vocabulary assessment for Chinese L2 learners. Firstly, we generate three types of vocabulary questions by modeling both the vocabulary knowledge and learners' writing error data. After evaluation and calibration, we construct a balanced item pool with automatically generated items, and implement a three-parameter computerized adaptive test. We conduct manual item evaluation and online student tests in the experiments. The results show that the combination of AIG and CAT can construct test items efficiently and reduce test cost significantly. Also, the test result of CAT can provide valuable feedback to AIG algorithms",
    "volume": "workshop",
    "checked": true,
    "id": "3b7af32f9ab766d6d8a9105df46fac1b636fbdb8",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4404": {
    "title": "Computationally Modeling the Impact of Task-Appropriate Language Complexity and Accuracy on Human Grading of German Essays",
    "abstract": "Computational linguistic research on the language complexity of student writing typically involves human ratings as a gold standard. However, educational science shows that teachers find it difficult to identify and cleanly separate accuracy, different aspects of complexity, contents, and structure. In this paper, we therefore explore the use of computational linguistic methods to investigate how task-appropriate complexity and accuracy relate to the grading of overall performance, content performance, and language performance as assigned by teachers. Based on texts written by students for the official school-leaving state examination (Abitur), we show that teachers successfully assign higher language performance grades to essays with higher task-appropriate language complexity and properly separate this from content scores. Yet, accuracy impacts teacher assessment for all grading rubrics, also the content score, overemphasizing the role of accuracy. Our analysis is based on broad computational linguistic modeling of German language complexity and an innovative theory- and data-driven feature aggregation method inferring task-appropriate language complexity",
    "volume": "workshop",
    "checked": true,
    "id": "4d402a2b39465262b95670cbec3c5e92a86233cf",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-4405": {
    "title": "Analysing Rhetorical Structure as a Key Feature of Summary Coherence",
    "abstract": "We present a model for automatic scoring of coherence based on comparing the rhetorical structure (RS) of college student summaries in L2 (English) against expert summaries. Coherence is conceptualised as a construct consisting of the rhetorical relation and its arguments. Comparison with expert-assigned scores shows that RS scores correlate with both cohesion and coherence. Furthermore, RS scores improve the accuracy of a regression model for cohesion score prediction",
    "volume": "workshop",
    "checked": true,
    "id": "cdd10d59f20478558d12d526c7a85cdf00a4365c",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4406": {
    "title": "The BEA-2019 Shared Task on Grammatical Error Correction",
    "abstract": "This paper reports on the BEA-2019 Shared Task on Grammatical Error Correction (GEC). As with the CoNLL-2014 shared task, participants are required to correct all types of errors in test data. One of the main contributions of the BEA-2019 shared task is the introduction of a new dataset, the Write&Improve+LOCNESS corpus, which represents a wider range of native and learner English levels and abilities. Another contribution is the introduction of tracks, which control the amount of annotated data available to participants. Systems are evaluated in terms of ERRANT F_0.5, which allows us to report a much wider range of performance statistics. The competition was hosted on Codalab and remains open for further submissions on the blind test set",
    "volume": "workshop",
    "checked": true,
    "id": "3f3704d87860a816ac3cc7257a9acccf0d463b7a",
    "citation_count": 153
  },
  "https://aclanthology.org/W19-4407": {
    "title": "A Benchmark Corpus of English Misspellings and a Minimally-supervised Model for Spelling Correction",
    "abstract": "Spelling correction has attracted a lot of attention in the NLP community. However, models have been usually evaluated on artificiallycreated or proprietary corpora. A publiclyavailable corpus of authentic misspellings, annotated in context, is still lacking. To address this, we present and release an annotated data set of 6,121 spelling errors in context, based on a corpus of essays written by English language learners. We also develop a minimallysupervised context-aware approach to spelling correction. It achieves strong results on our data: 88.12% accuracy. This approach can also train with a minimal amount of annotated data (performance reduced by less than 1%). Furthermore, this approach allows easy portability to new domains. We evaluate our model on data from a medical domain and demonstrate that it rivals the performance of a model trained and tuned on in-domain data",
    "volume": "workshop",
    "checked": true,
    "id": "cdd0c0bfcc4e9b5a8b6566aa976e11a002781294",
    "citation_count": 14
  },
  "https://aclanthology.org/W19-4408": {
    "title": "Artificial Error Generation with Fluency Filtering",
    "abstract": "The quantity and quality of training data plays a crucial role in grammatical error correction (GEC). However, due to the fact that obtaining human-annotated GEC data is both time-consuming and expensive, several studies have focused on generating artificial error sentences to boost training data for grammatical error correction, and shown significantly better performance. The present study explores how fluency filtering can affect the quality of artificial errors. By comparing artificial data filtered by different levels of fluency, we find that artificial error sentences with low fluency can greatly facilitate error correction, while high fluency errors introduce more noise",
    "volume": "workshop",
    "checked": true,
    "id": "3b3f39b10159e745e9b974e6fc32691bcb6a3d24",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4409": {
    "title": "Regression or classification? Automated Essay Scoring for Norwegian",
    "abstract": "In this paper we present first results for the task of Automated Essay Scoring for Norwegian learner language. We analyze a number of properties of this task experimentally and assess (i) the formulation of the task as either regression or classification, (ii) the use of various non-neural and neural machine learning architectures with various types of input representations, and (iii) applying multi-task learning for joint prediction of essay scoring and native language identification. We find that a GRU-based attention model trained in a single-task setting performs best at the AES task",
    "volume": "workshop",
    "checked": true,
    "id": "06668afb5294144366d0d8d86b76c82c3416ce14",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-4410": {
    "title": "Context is Key: Grammatical Error Detection with Contextual Word Representations",
    "abstract": "Grammatical error detection (GED) in non-native writing requires systems to identify a wide range of errors in text written by language learners. Error detection as a purely supervised task can be challenging, as GED datasets are limited in size and the label distributions are highly imbalanced. Contextualized word representations offer a possible solution, as they can efficiently capture compositional information in language and can be optimized on large amounts of unsupervised data. In this paper, we perform a systematic comparison of ELMo, BERT and Flair embeddings (Peters et al., 2017; Devlin et al., 2018; Akbik et al., 2018) on a range of public GED datasets, and propose an approach to effectively integrate such representations in current methods, achieving a new state of the art on GED. We further analyze the strengths and weaknesses of different contextual embeddings for the task at hand, and present detailed analyses of their impact on different types of errors",
    "volume": "workshop",
    "checked": true,
    "id": "976f41486810c0c2475c8c485314e903ba6b2a6b",
    "citation_count": 20
  },
  "https://aclanthology.org/W19-4411": {
    "title": "How to account for mispellings: Quantifying the benefit of character representations in neural content scoring models",
    "abstract": "Character-based representations in neural models have been claimed to be a tool to overcome spelling variation in in word token-based input. We examine this claim in neural models for content scoring. We formulate precise hypotheses about the possible effects of adding character representations to word-based models and test these hypotheses on large-scale real world content scoring datasets. We find that, while character representations may provide small performance gains in general, their effectiveness in accounting for spelling variation may be limited. We show that spelling correction can provide larger gains than character representations, and that spelling correction improves the performance of models with character representations. With these insights, we report a new state of the art on the ASAP-SAS content scoring dataset",
    "volume": "workshop",
    "checked": true,
    "id": "5767c0dbfe89f5bc99ba1566b5a14f97667726ac",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-4412": {
    "title": "The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction",
    "abstract": "Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models",
    "volume": "workshop",
    "checked": true,
    "id": "98332ae5475bfa286d9ab74e20d86c72ceb3cde6",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-4413": {
    "title": "(Almost) Unsupervised Grammatical Error Correction using Synthetic Comparable Corpus",
    "abstract": "We introduce unsupervised techniques based on phrase-based statistical machine translation for grammatical error correction (GEC) trained on a pseudo learner corpus created by Google Translation. We verified our GEC system through experiments on a low resource track of the shared task at BEA2019. As a result, we achieved an F0.5 score of 28.31 points with the test data",
    "volume": "workshop",
    "checked": true,
    "id": "08ab4ba21d675bd95d68a9e91feba63785e25363",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4414": {
    "title": "Learning to combine Grammatical Error Corrections",
    "abstract": "The field of Grammatical Error Correction (GEC) has produced various systems to deal with focused phenomena or general text editing. We propose an automatic way to combine black-box systems. Our method automatically detects the strength of a system or the combination of several systems per error type, improving precision and recall while optimizing F-score directly. We show consistent improvement over the best standalone system in all the configurations tested. This approach also outperforms average ensembling of different RNN models with random initializations. In addition, we analyze the use of BERT for GEC - reporting promising results on this end. We also present a spellchecker created for this task which outperforms standard spellcheckers tested on the task of spellchecking. This paper describes a system submission to Building Educational Applications 2019 Shared Task: Grammatical Error Correction. Combining the output of top BEA 2019 shared task systems using our approach, currently holds the highest reported score in the open phase of the BEA 2019 shared task, improving F-0.5 score by 3.7 points over the best result reported",
    "volume": "workshop",
    "checked": true,
    "id": "1cc3e28d577b0628de05ac46e6ec3bbc275348d9",
    "citation_count": 21
  },
  "https://aclanthology.org/W19-4415": {
    "title": "Erroneous data generation for Grammatical Error Correction",
    "abstract": "It has been demonstrated that the utilization of a monolingual corpus in neural Grammatical Error Correction (GEC) systems can significantly improve the system performance. The previous state-of-the-art neural GEC system is an ensemble of four Transformer models pretrained on a large amount of Wikipedia Edits. The Singsound GEC system follows a similar approach but is equipped with a sophisticated erroneous data generating component. Our system achieved an F0:5 of 66.61 in the BEA 2019 Shared Task: Grammatical Error Correction. With our novel erroneous data generating component, the Singsound neural GEC system yielded an M2 of 63.2 on the CoNLL-2014 benchmark (8.4% relative improvement over the previous state-of-the-art system)",
    "volume": "workshop",
    "checked": true,
    "id": "a95a543f374bfa62fbf25daebd4ef8fb099774de",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-4416": {
    "title": "The LAIX Systems in the BEA-2019 GEC Shared Task",
    "abstract": "In this paper, we describe two systems we developed for the three tracks we have participated in the BEA-2019 GEC Shared Task. We investigate competitive classification models with bi-directional recurrent neural networks (Bi-RNN) and neural machine translation (NMT) models. For different tracks, we use ensemble systems to selectively combine the NMT models, the classification models, and some rules, and demonstrate that an ensemble solution can effectively improve GEC performance over single systems. Our GEC systems ranked the first in the Unrestricted Track, and the third in both the Restricted Track and the Low Resource Track",
    "volume": "workshop",
    "checked": true,
    "id": "25dd4ef9d61ae7da1e3e3d9206ecac2966b1c880",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-4417": {
    "title": "The CUED's Grammatical Error Correction Systems for BEA-2019",
    "abstract": "We describe two entries from the Cambridge University Engineering Department to the BEA 2019 Shared Task on grammatical error correction. Our submission to the low-resource track is based on prior work on using finite state transducers together with strong neural language models. Our system for the restricted track is a purely neural system consisting of neural language models and neural machine translation models trained with back-translation and a combination of checkpoint averaging and fine-tuning – without the help of any additional tools like spell checkers. The latter system has been used inside a separate system combination entry in cooperation with the Cambridge University Computer Lab",
    "volume": "workshop",
    "checked": true,
    "id": "e60fdd13e0805eb27d79292ab111592fa16c2aaf",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4418": {
    "title": "The AIP-Tohoku System at the BEA-2019 Shared Task",
    "abstract": "We introduce the AIP-Tohoku grammatical error correction (GEC) system for the BEA-2019 shared task in Track 1 (Restricted Track) and Track 2 (Unrestricted Track) using the same system architecture. Our system comprises two key components: error generation and sentence-level error detection. In particular, GEC with sentence-level grammatical error detection is a novel and versatile approach, and we experimentally demonstrate that it significantly improves the precision of the base model. Our system is ranked 9th in Track 1 and 2nd in Track 2",
    "volume": "workshop",
    "checked": true,
    "id": "9c25d1664a02fefd3e9b808651ea28c1cf5dadc3",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-4419": {
    "title": "CUNI System for the Building Educational Applications 2019 Shared Task: Grammatical Error Correction",
    "abstract": "Our submitted models are NMT systems based on the Transformer model, which we improve by incorporating several enhancements: applying dropout to whole source and target words, weighting target subwords, averaging model checkpoints, and using the trained model iteratively for correcting the intermediate translations. The system in the Restricted Track is trained on the provided corpora with oversampled \"cleaner\" sentences and reaches 59.39 F0.5 score on the test set. The system in the Low-Resource Track is trained from Wikipedia revision histories and reaches 44.13 F0.5 score. Finally, we finetune the system from the Low-Resource Track on restricted data and achieve 64.55 F0.5 score",
    "volume": "workshop",
    "checked": true,
    "id": "3b187b45b8cb77eeac8ac4633235f6781e68c7f1",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-4420": {
    "title": "Noisy Channel for Low Resource Grammatical Error Correction",
    "abstract": "This paper describes our contribution to the low-resource track of the BEA 2019 shared task on Grammatical Error Correction (GEC). Our approach to GEC builds on the theory of the noisy channel by combining a channel model and language model. We generate confusion sets from the Wikipedia edit history and use the frequencies of edits to estimate the channel model. Additionally, we use two pre-trained language models: 1) Google's BERT model, which we fine-tune for specific error types and 2) OpenAI's GPT-2 model, utilizing that it can operate with previous sentences as context. Furthermore, we search for the optimal combinations of corrections using beam search",
    "volume": "workshop",
    "checked": true,
    "id": "6501c2d8ae44a260ce6203362643c9804f54877c",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-4421": {
    "title": "The BLCU System in the BEA 2019 Shared Task",
    "abstract": "This paper describes the BLCU Group submissions to the Building Educational Applications (BEA) 2019 Shared Task on Grammatical Error Correction (GEC). The task is to detect and correct grammatical errors that occurred in essays. We participate in 2 tracks including the Restricted Track and the Unrestricted Track. Our system is based on a Transformer model architecture. We integrate many effective methods proposed in recent years. Such as, Byte Pair Encoding, model ensemble, checkpoints average and spell checker. We also corrupt the public monolingual data to further improve the performance of the model. On the test data of the BEA 2019 Shared Task, our system yields F0.5 = 58.62 and 59.50, ranking twelfth and fourth respectively",
    "volume": "workshop",
    "checked": true,
    "id": "0656c5ae4bc32b27bbee8ebf9b6a7dca81a4a00f",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4422": {
    "title": "TMU Transformer System Using BERT for Re-ranking at BEA 2019 Grammatical Error Correction on Restricted Track",
    "abstract": "We introduce our system that is submitted to the restricted track of the BEA 2019 shared task on grammatical error correction1 (GEC). It is essential to select an appropriate hypothesis sentence from the candidates list generated by the GEC model. A re-ranker can evaluate the naturalness of a corrected sentence using language models trained on large corpora. On the other hand, these language models and language representations do not explicitly take into account the grammatical errors written by learners. Thus, it is not straightforward to utilize language representations trained from a large corpus, such as Bidirectional Encoder Representations from Transformers (BERT), in a form suitable for the learner's grammatical errors. Therefore, we propose to fine-tune BERT on learner corpora with grammatical errors for re-ranking. The experimental results of the W&I+LOCNESS development dataset demonstrate that re-ranking using BERT can effectively improve the correction performance",
    "volume": "workshop",
    "checked": true,
    "id": "18d99f6466771af0aef095ca22d1f81d88c7697b",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4423": {
    "title": "A Neural Grammatical Error Correction System Built On Better Pre-training and Sequential Transfer Learning",
    "abstract": "Grammatical error correction can be viewed as a low-resource sequence-to-sequence task, because publicly available parallel corpora are limited.To tackle this challenge, we first generate erroneous versions of large unannotated corpora using a realistic noising function. The resulting parallel corpora are sub-sequently used to pre-train Transformer models. Then, by sequentially applying transfer learning, we adapt these models to the domain and style of the test set. Combined with a context-aware neural spellchecker, our system achieves competitive results in both restricted and low resource tracks in ACL 2019 BEAShared Task. We release all of our code and materials for reproducibility",
    "volume": "workshop",
    "checked": true,
    "id": "2704b207c7b8f6fc32bd3d04690b2f4f745c460f",
    "citation_count": 48
  },
  "https://aclanthology.org/W19-4424": {
    "title": "Neural and FST-based approaches to grammatical error correction",
    "abstract": "In this paper, we describe our submission to the BEA 2019 shared task on grammatical error correction. We present a system pipeline that utilises both error detection and correction models. The input text is first corrected by two complementary neural machine translation systems: one using convolutional networks and multi-task learning, and another using a neural Transformer-based system. Training is performed on publicly available data, along with artificial examples generated through back-translation. The n-best lists of these two machine translation systems are then combined and scored using a finite state transducer (FST). Finally, an unsupervised re-ranking system is applied to the n-best output of the FST. The re-ranker uses a number of error detection features to re-rank the FST n-best list and identify the final 1-best correction hypothesis. Our system achieves 66.75% F 0.5 on error correction (ranking 4th), and 82.52% F 0.5 on token-level error detection (ranking 2nd) in the restricted track of the shared task",
    "volume": "workshop",
    "checked": true,
    "id": "ffe1fe4161943f88101f4d6653570d5ae66e1b2a",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-4425": {
    "title": "Improving Precision of Grammatical Error Correction with a Cheat Sheet",
    "abstract": "In this paper, we explore two approaches of generating error-focused phrases and examine whether these phrases can lead to better performance in grammatical error correction for the restricted track of BEA 2019 Shared Task on GEC. Our results show that phrases directly extracted from GEC corpora outperform phrases from statistical machine translation phrase table by a large margin. Appending error+context phrases to the original GEC corpora yields comparably high precision. We also explore the generation of artificial syntactic error sentences using error+context phrases for the unrestricted track. The additional training data greatly facilitates syntactic error correction (e.g., verb form) and contributes to better overall performance",
    "volume": "workshop",
    "checked": true,
    "id": "53d505561509eb37625c3e6200422f8bd14f09ea",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4426": {
    "title": "Multi-headed Architecture Based on BERT for Grammatical Errors Correction",
    "abstract": "In this paper, we describe our approach to GEC using the BERT model for creation of encoded representation and some of our enhancements, namely, \"Heads\" are fully-connected networks which are used for finding the errors and later receive recommendation from the networks on dealing with a highlighted part of the sentence only. Among the main advantages of our solution is increasing the system productivity and lowering the time of processing while keeping the high accuracy of GEC results",
    "volume": "workshop",
    "checked": true,
    "id": "6cbb7e06997e53c163acea9a4986c8949c7bae74",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-4427": {
    "title": "Neural Grammatical Error Correction Systems with Unsupervised Pre-training on Synthetic Data",
    "abstract": "Considerable effort has been made to address the data sparsity problem in neural grammatical error correction. In this work, we propose a simple and surprisingly effective unsupervised synthetic error generation method based on confusion sets extracted from a spellchecker to increase the amount of training data. Synthetic data is used to pre-train a Transformer sequence-to-sequence model, which not only improves over a strong baseline trained on authentic error-annotated data, but also enables the development of a practical GEC system in a scenario where little genuine error-annotated data is available. The developed systems placed first in the BEA19 shared task, achieving 69.47 and 64.24 F_{0.5} in the restricted and low-resource tracks respectively, both on the W&I+LOCNESS test set. On the popular CoNLL 2014 test set, we report state-of-the-art results of 64.16 M² for the submitted system, and 61.30 M² for the constrained system trained on the NUCLE and Lang-8 data",
    "volume": "workshop",
    "checked": true,
    "id": "7cc6f009feb5ad5ad0e1ff00c551fb318fc95016",
    "citation_count": 103
  },
  "https://aclanthology.org/W19-4428": {
    "title": "Evaluation of automatic collocation extraction methods for language learning",
    "abstract": "A number of methods have been proposed to automatically extract collocations, i.e., conventionalized lexical combinations, from text corpora. However, the attempts to evaluate and compare them with a specific application in mind lag behind. This paper compares three end-to-end resources for collocation learning, all of which used the same corpus but different methods. Adopting a gold-standard evaluation method, the results show that the method of dependency parsing outperforms regex-over-pos in collocation identification. The lexical association measures (AMs) used for collocation ranking perform about the same overall but differently for individual collocation types. Further analysis has also revealed that there are considerable differences between other commonly used AMs",
    "volume": "workshop",
    "checked": true,
    "id": "924c451fa8f28af71bca7e23f5255871d62e7e3b",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4429": {
    "title": "Anglicized Words and Misspelled Cognates in Native Language Identification",
    "abstract": "In this paper, we present experiments that estimate the impact of specific lexical choices of people writing in a second language (L2). In particular, we look at misspelled words that indicate lexical uncertainty on the part of the author, and separate them into three categories: misspelled cognates, \"L2-ed\" (in our case, anglicized) words, and all other spelling errors. We test the assumption that such errors contain clues about the native language of an essay's author through the task of native language identification. The results of the experiments show that the information brought by each of these categories is complementary. We also note that while the distribution of such features changes with the proficiency level of the writer, their contribution towards native language identification remains significant at all levels",
    "volume": "workshop",
    "checked": true,
    "id": "d41a38b84c19c08b94091f99d28140b844e270a4",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4430": {
    "title": "Linguistically-Driven Strategy for Concept Prerequisites Learning on Italian",
    "abstract": "We present a new concept prerequisite learning method for Learning Object (LO) ordering that exploits only linguistic features extracted from textual educational resources. The method was tested in a cross- and in- domain scenario both for Italian and English. Additionally, we performed experiments based on a incremental training strategy to study the impact of the training set size on the classifier performances. The paper also introduces ITA-PREREQ, to the best of our knowledge the first Italian dataset annotated with prerequisite relations between pairs of educational concepts, and describe the automatic strategy devised to build it",
    "volume": "workshop",
    "checked": true,
    "id": "66930c8de0825386b2086c5fd78b0e44bcd56507",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-4431": {
    "title": "Grammatical-Error-Aware Incorrect Example Retrieval System for Learners of Japanese as a Second Language",
    "abstract": "Existing example retrieval systems do not include grammatically incorrect examples or present only a few examples, if any. Even if a retrieval system has a wide coverage of incorrect examples along with the correct counterpart, learners need to know whether their query includes errors or not. Considering the usability of retrieving incorrect examples, our proposed method uses a large-scale corpus and presents correct expressions along with incorrect expressions using a grammatical error detection system so that the learner do not need to be aware of how to search for the examples. Intrinsic and extrinsic evaluations indicate that our method improves accuracy of example sentence retrieval and quality of learner's writing",
    "volume": "workshop",
    "checked": true,
    "id": "acb3acd5f24f039b4eaca15625eb687499af3d18",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4432": {
    "title": "Toward Automated Content Feedback Generation for Non-native Spontaneous Speech",
    "abstract": "In this study, we developed an automated algorithm to provide feedback about the specific content of non-native English speakers' spoken responses. The responses were spontaneous speech, elicited using integrated tasks where the language learners listened to and/or read passages and integrated the core content in their spoken responses. Our models detected the absence of key points considered to be important in a spoken response to a particular test question, based on two different models: (a) a model using word-embedding based content features and (b) a state-of-the art short response scoring engine using traditional n-gram based features. Both models achieved a substantially improved performance over the majority baseline, and the combination of the two models achieved a significant further improvement. In particular, the models were robust to automated speech recognition (ASR) errors, and performance based on the ASR word hypotheses was comparable to that based on manual transcriptions. The accuracy and F-score of the best model for the questions included in the train set were 0.80 and 0.68, respectively. Finally, we discussed possible approaches to generating targeted feedback about the content of a language learner's response, based on automatically detected missing key points",
    "volume": "workshop",
    "checked": true,
    "id": "9ebdd6044f911c2001cd6e19fe13128685a5528a",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4433": {
    "title": "Analytic Score Prediction and Justification Identification in Automated Short Answer Scoring",
    "abstract": "This paper provides an analytical assessment of student short answer responses with a view to potential benefits in pedagogical contexts. We first propose and formalize two novel analytical assessment tasks: analytic score prediction and justification identification, and then provide the first dataset created for analytic short answer scoring research. Subsequently, we present a neural baseline model and report our extensive empirical results to demonstrate how our dataset can be used to explore new and intriguing technical challenges in short answer scoring. The dataset is publicly available for research purposes",
    "volume": "workshop",
    "checked": true,
    "id": "b68bdd37b72e1153946bb85605c1d19b26a87d24",
    "citation_count": 17
  },
  "https://aclanthology.org/W19-4434": {
    "title": "Content Customization for Micro Learning using Human Augmented AI Techniques",
    "abstract": "Visual content has been proven to be effective for micro-learning compared to other media. In this paper, we discuss leveraging this observation in our efforts to build audio-visual content for young learners' vocabulary learning. We attempt to tackle two major issues in the process of traditional visual curation tasks. Generic learning videos do not necessarily satisfy the unique context of a learner and/or an educator, and hence may not result in maximal learning outcomes. Also, manual video curation by educators is a highly labor-intensive process. To this end, we present a customizable micro-learning audio-visual content curation tool that is designed to reduce the human (educator) effort in creating just-in-time learning videos from a textual description (learning script). This provides educators with control of the content while preparing the learning scripts, and in turn can also be customized to capture the desired learning objectives and outcomes. As a use case, we automatically generate learning videos with British National Corpus' (BNC) frequently spoken vocabulary words and evaluate them with experts. They positively recommended the generated learning videos with an average rating of 4.25 on a Likert scale of 5 points. The inter-annotator agreement between the experts for the video quality was substantial (Fleiss Kappa=0.62) with an overall agreement of 81%",
    "volume": "workshop",
    "checked": true,
    "id": "0372c466235160ed49ed4ac9c1d984c555360d2e",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4435": {
    "title": "Curio SmartChat : A system for Natural Language Question Answering for Self-Paced K-12 Learning",
    "abstract": "During learning, students often have questions which they would benefit from responses to in real time. In class, a student can ask a question to a teacher. During homework, or even in class if the student is shy, it can be more difficult to receive a rapid response. In this work, we introduce Curio SmartChat, an automated question answering system for middle school Science topics. Our system has now been used by around 20,000 students who have so far asked over 100,000 questions. We present data on the challenge created by students' grammatical errors and spelling mistakes, and discuss our system's approach and degree of effectiveness at disambiguating questions that the system is initially unsure about. We also discuss the prevalence of student \"small talk\" not related to science topics, the pluses and minuses of this behavior, and how a system should respond to these conversational acts. We conclude with discussions and point to directions for potential future work",
    "volume": "workshop",
    "checked": true,
    "id": "8cdb4514340b36993d06ca022fbfaf9e2f63123e",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4436": {
    "title": "Supporting content evaluation of student summaries by Idea Unit embedding",
    "abstract": "This paper discusses the computer-assisted content evaluation of summaries. We propose a method to make a correspondence between the segments of the source text and its summary. As a unit of the segment, we adopt \"Idea Unit (IU)\" which is proposed in Applied Linguistics. Introducing IUs enables us to make a correspondence even for the sentences that contain multiple ideas. The IU correspondence is made based on the similarity between vector representations of IU. An evaluation experiment with two source texts and 20 summaries showed that the proposed method is more robust against rephrased expressions than the conventional ROUGE-based baselines. Also, the proposed method outperformed the baselines in recall. We im-plemented the proposed method in a GUI tool\"Segment Matcher\" that aids teachers to estab-lish a link between corresponding IUs acrossthe summary and source text",
    "volume": "workshop",
    "checked": true,
    "id": "ac1f6e185fd5d349a5cc89199c822d2201a4092b",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4437": {
    "title": "On Understanding the Relation between Expert Annotations of Text Readability and Target Reader Comprehension",
    "abstract": "Automatic readability assessment aims to ensure that readers read texts that they can comprehend. However, computational models are typically trained on texts created from the perspective of the text writer, not the target reader. There is little experimental research on the relationship between expert annotations of readability, reader's language proficiency, and different levels of reading comprehension. To address this gap, we conducted a user study in which over a 100 participants read texts of different reading levels and answered questions created to test three forms of comprehension. Our results indicate that more than readability annotation or reader proficiency, it is the type of comprehension question asked that shows differences between reader responses - inferential questions were difficult for users of all levels of proficiency across reading levels. The data collected from this study will be released with this paper, which will, for the first time, provide a collection of 45 reader bench marked texts to evaluate readability assessment systems developed for adult learners of English. It can also potentially be useful for the development of question generation approaches in intelligent tutoring systems research",
    "volume": "workshop",
    "checked": true,
    "id": "76d64cd4d6aef79b452c344a6cb990448213e5e3",
    "citation_count": 11
  },
  "https://aclanthology.org/W19-4438": {
    "title": "Measuring Text Complexity for Italian as a Second Language Learning Purposes",
    "abstract": "The selection of texts for second language learning purposes typically relies on teachers' and test developers' individual judgment of the observable qualitative properties of a text. Little or no consideration is generally given to the quantitative dimension within an evidence-based framework of reproducibility. This study aims to fill the gap by evaluating the effectiveness of an automatic tool trained to assess text complexity in the context of Italian as a second language learning. A dataset of texts labeled by expert test developers was used to evaluate the performance of three classifier models (decision tree, random forest, and support vector machine), which were trained using linguistic features measured quantitatively and extracted from the texts. The experimental analysis provided satisfactory results, also in relation to which kind of linguistic trait contributed the most to the final outcome",
    "volume": "workshop",
    "checked": true,
    "id": "00eda4f4b047f42d5ba6c3866e8f8f2bca3c0402",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-4439": {
    "title": "Simple Construction of Mixed-Language Texts for Vocabulary Learning",
    "abstract": "We present a machine foreign-language teacher that takes documents written in a student's native language and detects situations where it can replace words with their foreign glosses such that new foreign vocabulary can be learned simply through reading the resulting mixed-language text. We show that it is possible to design such a machine teacher without any supervised data from (human) students. We accomplish this by modifying a cloze language model to incrementally learn new vocabulary items, and use this language model as a proxy for the word guessing and learning ability of real students. Our machine foreign-language teacher decides which subset of words to replace by consulting this language model. We evaluate three variants of our student proxy language models through a study on Amazon Mechanical Turk (MTurk). We find that MTurk \"students\" were able to guess the meanings of foreign words introduced by the machine teacher with high accuracy for both function words as well as content words in two out of the three models. In addition, we show that students are able to retain their knowledge about the foreign words after they finish reading the document",
    "volume": "workshop",
    "checked": true,
    "id": "0a8ced6561fe8d1d0ea2792f9f8840a54394de77",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4440": {
    "title": "Analyzing Linguistic Complexity and Accuracy in Academic Language Development of German across Elementary and Secondary School",
    "abstract": "We track the development of writing complexity and accuracy in German students' early academic language development from first to eighth grade. Combining an empirically broad approach to linguistic complexity with the high-quality error annotation included in the Karlsruhe Children's Text corpus (Lavalley et al. 2015) used, we construct models of German academic language development that successfully identify the student's grade level. We show that classifiers for the early years rely more on accuracy development, whereas development in secondary school is better characterized by increasingly complex language in all domains: linguistic system, language use, and human sentence processing characteristics. We demonstrate the generalizability and robustness of models using such a broad complexity feature set across writing topics",
    "volume": "workshop",
    "checked": true,
    "id": "0670cc98172177ee120cf634b587dbe2777d569e",
    "citation_count": 19
  },
  "https://aclanthology.org/W19-4441": {
    "title": "Content Modeling for Automated Oral Proficiency Scoring System",
    "abstract": "We developed an automated oral proficiency scoring system for non-native English speakers' spontaneous speech. Automated systems that score holistic proficiency are expected to assess a wide range of performance categories, and the content is one of the core performance categories. In order to assess the quality of the content, we trained a Siamese convolutional neural network (CNN) to model the semantic relationship between key points generated by experts and a test response. The correlation between human scores and Siamese CNN scores was comparable to human-human agreement (r=0.63), and it was higher than the baseline content features. The inclusion of Siamese CNN-based feature to the existing state-of-the-art automated scoring model achieved a small but statistically significant improvement. However, the new model suffered from score inflation for long atypical responses with serious content issues. We investigated the reasons of this score inflation by analyzing the associations with linguistic features and identifying areas strongly associated with the score errors",
    "volume": "workshop",
    "checked": true,
    "id": "42de816ee52cb2c3560e94234a963e4668737487",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4442": {
    "title": "Learning Outcomes and Their Relatedness in a Medical Curriculum",
    "abstract": "A typical medical curriculum is organized in a hierarchy of instructional objectives called Learning Outcomes (LOs); a few thousand LOs span five years of study. Gaining a thorough understanding of the curriculum requires learners to recognize and apply related LOs across years, and across different parts of the curriculum. However, given the large scope of the curriculum, manually labeling related LOs is tedious, and almost impossible to scale. In this paper, we build a system that learns relationships between LOs, and we achieve up to human-level performance in the LO relationship extraction task. We then present an application where the proposed system is employed to build a map of related LOs and Learning Resources (LRs) pertaining to a virtual patient case. We believe that our system can help medical students grasp the curriculum better, within classroom as well as in Intelligent Tutoring Systems (ITS) settings",
    "volume": "workshop",
    "checked": true,
    "id": "21db98d3d1e29819090ec865eb009701c83d3aa2",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4443": {
    "title": "Measuring text readability with machine comprehension: a pilot study",
    "abstract": "This article studies the relationship between text readability indice and automatic machine understanding systems. Our hypothesis is that the simpler a text is, the better it should be understood by a machine. We thus expect to a strong correlation between readability levels on the one hand, and performance of automatic reading systems on the other hand. We test this hypothesis with several understanding systems based on language models of varying strengths, measuring this correlation on two corpora of journalistic texts. Our results suggest that this correlation is rather small that existing comprehension systems are far to reproduce the gradual improvement of their performance on texts of decreasing complexity",
    "volume": "workshop",
    "checked": true,
    "id": "475cb86b7711acc4e239302f222f037f4c521075",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4444": {
    "title": "Metaphors in Text Simplification: To change or not to change, that is the question",
    "abstract": "We present an analysis of metaphors in news text simplification. Using features that capture general and metaphor specific characteristics, we test whether we can automatically identify which metaphors will be changed or preserved, and whether there are features that have different predictive power for metaphors or literal words. The experiments show that the Age of Acquisition is the most distinctive feature for both metaphors and literal words. Features that capture Imageability and Concreteness are useful when used alone, but within the full set of features they lose their impact. Frequency of use seems to be the best feature to differentiate metaphors that should be changed and those to be preserved",
    "volume": "workshop",
    "checked": true,
    "id": "40166aec2b68d00593e9ffe625354c9acc2d9ec2",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4445": {
    "title": "Application of an Automatic Plagiarism Detection System in a Large-scale Assessment of English Speaking Proficiency",
    "abstract": "This study aims to build an automatic system for the detection of plagiarized spoken responses in the context of an assessment of English speaking proficiency for non-native speakers. Classification models were trained to distinguish between plagiarized and non-plagiarized responses with two different types of features: text-to-text content similarity measures, which are commonly used in the task of plagiarism detection for written documents, and speaking proficiency measures, which were specifically designed for spontaneous speech and extracted using an automated speech scoring system. The experiments were first conducted on a large data set drawn from an operational English proficiency assessment across multiple years, and the best classifier on this heavily imbalanced data set resulted in an F1-score of 0.761 on the plagiarized class. This system was then validated on operational responses collected from a single administration of the assessment and achieved a recall of 0.897. The results indicate that the proposed system can potentially be used to improve the validity of both human and automated assessment of non-native spoken English",
    "volume": "workshop",
    "checked": true,
    "id": "247242b1f21f631d353810bc94777ea8cbde655c",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4446": {
    "title": "Equity Beyond Bias in Language Technologies for Education",
    "abstract": "There is a long record of research on equity in schools. As machine learning researchers begin to study fairness and bias in earnest, language technologies in education have an unusually strong theoretical and applied foundation to build on. Here, we introduce concepts from culturally relevant pedagogy and other frameworks for teaching and learning, identifying future work on equity in NLP. We present case studies in a range of topics like intelligent tutoring systems, computer-assisted language learning, automated essay scoring, and sentiment analysis in classrooms, and provide an actionable agenda for research",
    "volume": "workshop",
    "checked": true,
    "id": "f8a13e94260373d2904f32616decce817b388990",
    "citation_count": 25
  },
  "https://aclanthology.org/W19-4447": {
    "title": "From Receptive to Productive: Learning to Use Confusing Words through Automatically Selected Example Sentences",
    "abstract": "Knowing how to use words appropriately has been a key to improving language proficiency. Previous studies typically discuss how students learn receptively to select the correct candidate from a set of confusing words in the fill-in-the-blank task where specific context is given. In this paper, we go one step further, assisting students to learn to use confusing words appropriately in a productive task: sentence translation. We leverage the GiveMe-Example system, which suggests example sentences for each confusing word, to achieve this goal. In this study, students learn to differentiate the confusing words by reading the example sentences, and then choose the appropriate word(s) to complete the sentence translation task. Results show students made substantial progress in terms of sentence structure. In addition, highly proficient students better managed to learn confusing words. In view of the influence of the first language on learners, we further propose an effective approach to improve the quality of the suggested sentences",
    "volume": "workshop",
    "checked": true,
    "id": "d5d02b72be758e0e2039743f00fcaafeeea1b749",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4448": {
    "title": "Equipping Educational Applications with Domain Knowledge",
    "abstract": "One of the challenges of building natural language processing (NLP) applications for education is finding a large domain-specific corpus for the subject of interest (e.g., history or science). To address this challenge, we propose a tool, Dexter, that extracts a subject-specific corpus from a heterogeneous corpus, such as Wikipedia, by relying on a small seed corpus and distributed document representations. We empirically show the impact of the generated corpus on language modeling, estimating word embeddings, and consequently, distractor generation, resulting in better performances than while using a general domain corpus, a heuristically constructed domain-specific corpus, and a corpus generated by a popular system: BootCaT",
    "volume": "workshop",
    "checked": true,
    "id": "9f50e3a5326bcb34195182188fb60307e25e0b2c",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4449": {
    "title": "The Unbearable Weight of Generating Artificial Errors for Grammatical Error Correction",
    "abstract": "In this paper, we investigate the impact of using 4 recent neural models for generating artificial errors to help train the neural grammatical error correction models. We conduct a battery of experiments on the effect of data size, models, and comparison with a rule-based approach",
    "volume": "workshop",
    "checked": true,
    "id": "71f3ca5e14c6cd054321ac5da8d2d4f13c09ac65",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-4450": {
    "title": "Automated Essay Scoring with Discourse-Aware Neural Models",
    "abstract": "Automated essay scoring systems typically rely on hand-crafted features to predict essay quality, but such systems are limited by the cost of feature engineering. Neural networks offer an alternative to feature engineering, but they typically require more annotated data. This paper explores network structures, contextualized embeddings and pre-training strategies aimed at capturing discourse characteristics of essays. Experiments on three essay scoring tasks show benefits from all three strategies in different combinations, with simpler architectures being more effective when less training data is available",
    "volume": "workshop",
    "checked": true,
    "id": "67e2f470f99e349831f6ff55d4ff6a62b71bc4eb",
    "citation_count": 27
  },
  "https://aclanthology.org/W19-4451": {
    "title": "Modeling language learning using specialized Elo rating",
    "abstract": "Automatic assessment of the proficiency levels of the learner is a critical part of Intelligent Tutoring Systems. We present methods for assessment in the context of language learning. We use a specialized Elo formula used in conjunction with educational data mining. We simultaneously obtain ratings for the proficiency of the learners and for the difficulty of the linguistic concepts that the learners are trying to master. From the same data we also learn a graph structure representing a domain model capturing the relations among the concepts. This application of Elo provides ratings for learners and concepts which correlate well with subjective proficiency levels of the learners and difficulty levels of the concepts",
    "volume": "workshop",
    "checked": true,
    "id": "1657937738a8174acfe7c5fa963e47cb13975416",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4452": {
    "title": "Rubric Reliability and Annotation of Content and Argument in Source-Based Argument Essays",
    "abstract": "We present a unique dataset of student source-based argument essays to facilitate research on the relations between content, argumentation skills, and assessment. Two classroom writing assignments were given to college students in a STEM major, accompanied by a carefully designed rubric. The paper presents a reliability study of the rubric, showing it to be highly reliable, and initial annotation on content and argumentation annotation of the essays",
    "volume": "workshop",
    "checked": true,
    "id": "e263df35b9a7bc9ce64e4ad7a1a3048c899d962b",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4501": {
    "title": "Segmentation of Argumentative Texts with Contextualised Word Representations",
    "abstract": "The segmentation of argumentative units is an important subtask of argument mining, which is frequently addressed at a coarse granularity, usually assuming argumentative units to be no smaller than sentences. Approaches focusing at the clause-level granularity, typically address the task as sequence labeling at the token level, aiming to classify whether a token begins, is inside, or is outside of an argumentative unit. Most approaches exploit highly engineered, manually constructed features, and algorithms typically used in sequential tagging – such as Conditional Random Fields, while more recent approaches try to exploit manually constructed features in the context of deep neural networks. In this context, we examined to what extend recent advances in sequential labelling allow to reduce the need for highly sophisticated, manually constructed features, and whether limiting features to embeddings, pre-trained on large corpora is a promising approach. Evaluation results suggest the examined models and approaches can exhibit comparable performance, minimising the need for feature engineering",
    "volume": "workshop",
    "checked": true,
    "id": "73a46795674710e3a3300dc82e54fd92b7b90647",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4502": {
    "title": "A Cascade Model for Proposition Extraction in Argumentation",
    "abstract": "We present a model to tackle a fundamental but understudied problem in computational argumentation: proposition extraction. Propositions are the basic units of an argument and the primary building blocks of most argument mining systems. However, they are usually substituted by argumentative discourse units obtained via surface-level text segmentation, which may yield text segments that lack semantic information necessary for subsequent argument mining processes. In contrast, our cascade model aims to extract complete propositions by handling anaphora resolution, text segmentation, reported speech, questions, imperatives, missing subjects, and revision. We formulate each task as a computational problem and test various models using a corpus of the 2016 U.S. presidential debates. We show promising performance for some tasks and discuss main challenges in proposition extraction",
    "volume": "workshop",
    "checked": true,
    "id": "285fa9e0d58dfe6e78a2ebd28b305351f2b93340",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-4503": {
    "title": "Dissecting Content and Context in Argumentative Relation Analysis",
    "abstract": "When assessing relations between argumentative units (e.g., support or attack), computational systems often exploit disclosing indicators or markers that are not part of elementary argumentative units (EAUs) themselves, but are gained from their context (position in paragraph, preceding tokens, etc.). We show that this dependency is much stronger than previously assumed. In fact, we show that by completely masking the EAU text spans and only feeding information from their context, a competitive system may function even better. We argue that an argument analysis system that relies more on discourse context than the argument's content is unsafe, since it can easily be tricked. To alleviate this issue, we separate argumentative units from their context such that the system is forced to model and rely on an EAU's content. We show that the resulting classification system is more robust, and argue that such models are better suited for predicting argumentative relations across documents",
    "volume": "workshop",
    "checked": true,
    "id": "eacc2204afc87032246f9c0533423bd671ed620c",
    "citation_count": 21
  },
  "https://aclanthology.org/W19-4504": {
    "title": "Aligning Discourse and Argumentation Structures using Subtrees and Redescription Mining",
    "abstract": "In this paper, we investigate similarities between discourse and argumentation structures by aligning subtrees in a corpus containing both annotations. Contrary to previous works, we focus on comparing sub-structures and not only relations matches. Using data mining techniques, we show that discourse and argumentation most often align well, and the double annotation allows to derive a mapping between structures. Moreover, this approach enables the study of similarities between discourse structures and differences in their expressive power",
    "volume": "workshop",
    "checked": true,
    "id": "98e9429fa8886e113440b8d7a34f8b5af78e6e58",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4505": {
    "title": "Transferring Knowledge from Discourse to Arguments: A Case Study with Scientific Abstracts",
    "abstract": "In this work we propose to leverage resources available with discourse-level annotations to facilitate the identification of argumentative components and relations in scientific texts, which has been recognized as a particularly challenging task. In particular, we implement and evaluate a transfer learning approach in which contextualized representations learned from discourse parsing tasks are used as input of argument mining models. As a pilot application, we explore the feasibility of using automatically identified argumentative components and relations to predict the acceptance of papers in computer science venues. In order to conduct our experiments, we propose an annotation scheme for argumentative units and relations and use it to enrich an existing corpus with an argumentation layer",
    "volume": "workshop",
    "checked": true,
    "id": "e126b3062247a5140dc13aefb3f39082133a450b",
    "citation_count": 14
  },
  "https://aclanthology.org/W19-4506": {
    "title": "The Swedish PoliGraph: A Semantic Graph for Argument Mining of Swedish Parliamentary Data",
    "abstract": "As part of a larger project on argument mining of Swedish parliamentary data, we have created a semantic graph that, together with named entity recognition and resolution (NER), should make it easier to establish connections between arguments in a given debate. The graph is essentially a semantic database that keeps track of Members of Parliament (MPs), in particular their presence in the parliament and activity in debates, but also party affiliation and participation in commissions. The hope is that the Swedish PoliGraph will enable us to perform named entity resolution on debates in the Swedish parliament with a high accuracy, with the aim of determining to whom an argument is directed",
    "volume": "workshop",
    "checked": true,
    "id": "63cee63552bff4549784ae05737091f6b1157873",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4507": {
    "title": "Towards Effective Rebuttal: Listening Comprehension Using Corpus-Wide Claim Mining",
    "abstract": "Engaging in a live debate requires, among other things, the ability to effectively rebut arguments claimed by your opponent. In particular, this requires identifying these arguments. Here, we suggest doing so by automatically mining claims from a corpus of news articles containing billions of sentences, and searching for them in a given speech. This raises the question of whether such claims indeed correspond to those made in spoken speeches. To this end, we collected a large dataset of 400 speeches in English discussing 200 controversial topics, mined claims for each topic, and asked annotators to identify the mined claims mentioned in each speech. Results show that in the vast majority of speeches debaters indeed make use of such claims. In addition, we present several baselines for the automatic detection of mined claims in speeches, forming the basis for future work. All collected data is freely available for research",
    "volume": "workshop",
    "checked": true,
    "id": "82080da3bb983a72505c59027cafd09445a33695",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-4508": {
    "title": "Lexicon Guided Attentive Neural Network Model for Argument Mining",
    "abstract": "Identification of argumentative components is an important stage of argument mining. Lexicon information is reported as one of the most frequently used features in the argument mining research. In this paper, we propose a methodology to integrate lexicon information into a neural network model by attention mechanism. We conduct experiments on the UKP dataset, which is collected from heterogeneous sources and contains several text types, e.g., microblog, Wikipedia, and news. We explore lexicons from various application scenarios such as sentiment analysis and emotion detection. We also compare the experimental results of leveraging different lexicons",
    "volume": "workshop",
    "checked": true,
    "id": "13b919de850d100ba25fa33957d2d71c8a5e903f",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4509": {
    "title": "Is It Worth the Attention? A Comparative Evaluation of Attention Layers for Argument Unit Segmentation",
    "abstract": "Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new state-of-the-art results. A thorough evaluation of the attention mechanism for the task of Argumentation Mining is missing. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach for the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task, the additional attention layer does not improve the performance. In most cases, contextualized embeddings do also not show an improvement on the score achieved by pre-defined embeddings",
    "volume": "workshop",
    "checked": true,
    "id": "dd47e8b3405c7d7ebd16d33267dc7c0feff3e873",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-4510": {
    "title": "Argument Component Classification by Relation Identification by Neural Network and TextRank",
    "abstract": "In recent years, argumentation mining, which automatically extracts the structure of argumentation from unstructured documents such as essays and debates, is gaining attention. For argumentation mining applications, argument-component classification is an important subtask. The existing methods can be classified into supervised methods and unsupervised methods. Supervised document classification performs classification using a single sentence without relying on the whole document. On the other hand, unsupervised document classification has the advantage of being able to use the whole document, but accuracy of these methods is not so high. In this paper, we propose a method for argument-component classification that combines relation identification by neural networks and TextRank to integrate relation informations (i.e. the strength of the relation). This method can use argumentation-specific knowledge by employing a supervised learning on a corpus while maintaining the advantage of using the whole document. Experiments on two corpora, one consisting of student essay and the other of Wikipedia articles, show the effectiveness of this method",
    "volume": "workshop",
    "checked": true,
    "id": "c8951084102565d65034ec795314e7ad71c4e03a",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4511": {
    "title": "Argumentative Evidences Classification and Argument Scheme Detection Using Tree Kernels",
    "abstract": "The purpose of this study is to deploy a novel methodology for classifying different argumentative support (supporting evidences) in arguments, without considering the context. The proposed methodology is based on the idea that the use of Tree Kernel algorithms can be a good way to discriminate between different types of argumentative stances without the need of highly engineered features. This can be useful in different Argumentation Mining sub-tasks. This work provides an example of classifier built using a Tree Kernel method, which can discriminate between different kinds of argumentative support with a high accuracy. The ability to distinguish different kinds of support is, in fact, a key step toward Argument Scheme classification",
    "volume": "workshop",
    "checked": true,
    "id": "3e8e1f75c4901d62343690ad820ef247bdb95f78",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-4512": {
    "title": "The Utility of Discourse Parsing Features for Predicting Argumentation Structure",
    "abstract": "Research on argumentation mining from text has frequently discussed relationships to discourse parsing, but few empirical results are available so far. One corpus that has been annotated in parallel for argumentation structure and for discourse structure (RST, SDRT) are the 'argumentative microtexts' (Peldszus and Stede, 2016a). While results on perusing the gold RST annotations for predicting argumentation have been published (Peldszus and Stede, 2016b), the step to automatic discourse parsing has not yet been taken. In this paper, we run various discourse parsers (RST, PDTB) on the corpus, compare their results to the gold annotations (for RST) and then assess the contribution of automatically-derived discourse features for argumentation parsing. After reproducing the state-of-the-art Evidence Graph model from Afantenos et al. (2018) for the microtexts, we find that PDTB features can indeed improve its performance",
    "volume": "workshop",
    "checked": true,
    "id": "27637403d9bf140159c0c3b2231b9aa9536b5f29",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-4513": {
    "title": "Detecting Argumentative Discourse Acts with Linguistic Alignment",
    "abstract": "We report the results of preliminary investigations into the relationship between linguistic alignment and dialogical argumentation at the level of discourse acts. We annotated a proof of concept dataset with illocutions and transitions at the comment level based on Inference Anchoring Theory. We estimated linguistic alignment across discourse acts and found significant variation. Alignment features calculated at the dyad level are found to be useful for detecting a range of argumentative discourse acts",
    "volume": "workshop",
    "checked": true,
    "id": "d7fb1b686582c3f978b8a0ebced806ed1a17fedf",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4514": {
    "title": "Annotation of Rhetorical Moves in Biochemistry Articles",
    "abstract": "This paper focuses on the real world application of scientific writing and on determining rhetorical moves, an important step in establishing the argument structure of biomedical articles. Using the observation that the structure of scholarly writing in laboratory-based experimental sciences closely follows laboratory procedures, we examine most closely the Methods section of the texts and adopt an approach of identifying rhetorical moves that are procedure-oriented. We also propose a verb-centric frame semantics with an effective set of semantic roles in order to support the analysis. These components are designed to support a computational model that extends a promising proposal of appropriate rhetorical moves for this domain, but one which is merely descriptive. Our work also contributes to the understanding of argument-related annotation schemes. In particular, we conduct a detailed study with human annotators to confirm that our selection of semantic roles is effective in determining the underlying rhetorical structure of existing biomedical articles in an extensive dataset. The annotated dataset that we produce provides the important knowledge needed for our ultimate goal of analyzing biochemistry articles",
    "volume": "workshop",
    "checked": true,
    "id": "070abc09d05162e3451b45ed6e0952ad7d4f3fc8",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4515": {
    "title": "Evaluation of Scientific Elements for Text Similarity in Biomedical Publications",
    "abstract": "Rhetorical elements from scientific publications provide a more structured view of the document and allow algorithms to focus on particular parts of the text. We surveyed the literature for previously proposed schemes for rhetorical elements and present an overview of its current state of the art. We also searched for available tools using these schemes and applied four tools for our particular task of ranking biomedical abstracts based on text similarity. Comparison of the tools with two strong baselines shows that the predictions provided by the ArguminSci tool can support our use case of mining alternative methods for animal experiments",
    "volume": "workshop",
    "checked": true,
    "id": "4973a3ed6743f467e683c27c25341a15d54e0e7c",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-4516": {
    "title": "Categorizing Comparative Sentences",
    "abstract": "We tackle the tasks of automatically identifying comparative sentences and categorizing the intended preference (e.g., \"Python has better NLP libraries than MATLAB\" → Python, better, MATLAB). To this end, we manually annotate 7,199 sentences for 217 distinct target item pairs from several domains (27% of the sentences contain an oriented comparison in the sense of \"better\" or \"worse\"). A gradient boosting model based on pre-trained sentence embeddings reaches an F1 score of 85% in our experimental evaluation. The model can be used to extract comparative sentences for pro/con argumentation in comparative / argument search engines or debating technologies",
    "volume": "workshop",
    "checked": true,
    "id": "5e9e5e583e1130c3dc35cfc8099dddf206a9d7ec",
    "citation_count": 21
  },
  "https://aclanthology.org/W19-4517": {
    "title": "Ranking Passages for Argument Convincingness",
    "abstract": "In data ranking applications, pairwise annotation is often more consistent than cardinal annotation for learning ranking models. We examine this in a case study on ranking text passages for argument convincingness. Our task is to choose text passages that provide the highest-quality, most-convincing arguments for opposing sides of a topic. Using data from a deployed system within the Bing search engine, we construct a pairwise-labeled dataset for argument convincingness that is substantially more comprehensive in topical coverage compared to existing public resources. We detail the process of extracting topical passages for queries submitted to a search engine, creating annotated sets of passages aligned to different stances on a topic, and assessing argument convincingness of passages using pairwise annotation. Using a state-of-the-art convincingness model, we evaluate several methods for using pairwise-annotated data examples to train models for ranking passages. Our results show pairwise training outperforms training that regresses to a target score for each passage. Our results also show a simple 'win-rate' score is a better regression target than the previously proposed page-rank target. Lastly, addressing the need to filter noisy crowd-sourced annotations when constructing a dataset, we show that filtering for transitivity within pairwise annotations is more effective than filtering based on annotation confidence measures for individual examples",
    "volume": "workshop",
    "checked": true,
    "id": "d7bda7b93012d9804f976e4f659e7952b2e009d9",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-4518": {
    "title": "Gradual Argumentation Evaluation for Stance Aggregation in Automated Fake News Detection",
    "abstract": "Stance detection plays a pivot role in fake news detection. The task involves determining the point of view or stance – for or against – a text takes towards a claim. One very important stage in employing stance detection for fake news detection is the aggregation of multiple stance labels from different text sources in order to compute a prediction for the veracity of a claim. Typically, aggregation is treated as a credibility-weighted average of stance predictions. In this work, we take the novel approach of applying, for aggregation, a gradual argumentation semantics to bipolar argumentation frameworks mined using stance detection. Our empirical evaluation shows that our method results in more accurate veracity predictions",
    "volume": "workshop",
    "checked": true,
    "id": "08b1b1cea0b3c2494db09bb93daff09cf57c46b3",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-4519": {
    "title": "Persuasion of the Undecided: Language vs. the Listener",
    "abstract": "This paper examines the factors that govern persuasion for a priori UNDECIDED versus DECIDED audience members in the context of on-line debates. We separately study two types of influences: linguistic factors — features of the language of the debate itself; and audience factors — features of an audience member encoding demographic information, prior beliefs, and debate platform behavior. In a study of users of a popular debate platform, we find first that different combinations of linguistic features are critical for predicting persuasion outcomes for UNDECIDED versus DECIDED members of the audience. We additionally find that audience factors have more influence on predicting the side (PRO/CON) that persuaded UNDECIDED users than for DECIDED users that flip their stance to the opposing side. Our results emphasize the importance of considering the undecided and decided audiences separately when studying linguistic factors of persuasion",
    "volume": "workshop",
    "checked": true,
    "id": "d12ffc8c2c33517de4d5e5f27ea977d8f0f5c365",
    "citation_count": 15
  },
  "https://aclanthology.org/W19-4520": {
    "title": "Towards Assessing Argumentation Annotation - A First Step",
    "abstract": "This paper presents a first attempt at using Walton's argumentation schemes for annotating arguments in Swedish political text and assessing the feasibility of using this particular set of schemes with two linguistically trained annotators. The texts are not pre-annotated with argumentation structure beforehand. The results show that the annotators differ both in number of annotated arguments and selection of the conclusion and premises which make up the arguments. They also differ in their labeling of the schemes, but grouping the schemes increases their agreement. The outcome from this will be used to develop guidelines for future annotations",
    "volume": "workshop",
    "checked": true,
    "id": "648c48ffb85cf3128c34a5871497178c1edc4de9",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-4601": {
    "title": "Incremental Domain Adaptation for Neural Machine Translation in Low-Resource Settings",
    "abstract": "We study the problem of incremental domain adaptation of a generic neural machine translation model with limited resources (e.g., budget and time) for human translations or model training. In this paper, we propose a novel query strategy for selecting \"unlabeled\" samples from a new domain based on sentence embeddings for Arabic. We accelerate the fine-tuning process of the generic model to the target domain. Specifically, our approach estimates the informativeness of instances from the target domain by comparing the distance of their sentence embeddings to embeddings from the generic domain. We perform machine translation experiments (Ar-to-En direction) for comparing a random sampling baseline with our new approach, similar to active learning, using two small update sets for simulating the work of human translators. For the prescribed setting we can save more than 50% of the annotation costs without loss in quality, demonstrating the effectiveness of our approach",
    "volume": "workshop",
    "checked": true,
    "id": "a96afb7c9fb63beb7e1c10114bdfa7629b8123f2",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4602": {
    "title": "Morphology-aware Word-Segmentation in Dialectal Arabic Adaptation of Neural Machine Translation",
    "abstract": "Parallel corpora available for building machine translation (MT) models for dialectal Arabic (DA) are rather limited. The scarcity of resources has prompted the use of Modern Standard Arabic (MSA) abundant resources to complement the limited dialectal resource. However, dialectal clitics often differ between MSA and DA. This paper compares morphology-aware DA word segmentation to other word segmentation approaches like Byte Pair Encoding (BPE) and Sub-word Regularization (SR). A set of experiments conducted on Egyptian Arabic (EA), Levantine Arabic (LA), and Gulf Arabic (GA) show that a sufficiently accurate morphology-aware segmentation used in conjunction with BPE outperforms the other word segmentation approaches",
    "volume": "workshop",
    "checked": true,
    "id": "954e7db1e353dbbdc78467b0fd7e13da21f297a9",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4603": {
    "title": "POS Tagging for Improving Code-Switching Identification in Arabic",
    "abstract": "When speakers code-switch between their native language and a second language or language variant, they follow a syntactic pattern where words and phrases from the embedded language are inserted into the matrix language. This paper explores the possibility of utilizing this pattern in improving code-switching identification between Modern Standard Arabic (MSA) and Egyptian Arabic (EA). We try to answer the question of how strong is the POS signal in word-level code-switching identification. We build a deep learning model enriched with linguistic features (including POS tags) that outperforms the state-of-the-art results by 1.9% on the development set and 1.0% on the test set. We also show that in intra-sentential code-switching, the selection of lexical items is constrained by POS categories, where function words tend to come more often from the dialectal language while the majority of content words come from the standard language",
    "volume": "workshop",
    "checked": true,
    "id": "843f58b5f34b45c29577cc17ebd28efea97fade7",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4604": {
    "title": "Syntax-Ignorant N-gram Embeddings for Sentiment Analysis of Arabic Dialects",
    "abstract": "Arabic sentiment analysis models have employed compositional embedding features to represent the Arabic dialectal content. These embeddings are usually composed via ordered, syntax-aware composition functions and learned within deep neural frameworks. With the free word order and the varying syntax nature across the different Arabic dialects, a sentiment analysis system developed for one dialect might not be efficient for the others. Here we present syntax-ignorant n-gram embeddings to be used in sentiment analysis of several Arabic dialects. The proposed embeddings were composed and learned using an unordered composition function and a shallow neural model. Five datasets of different dialects were used to evaluate the produced embeddings in the sentiment analysis task. The obtained results revealed that, our syntax-ignorant embeddings could outperform word2vec model and doc2vec both variant models in addition to hand-crafted system baselines, while a competent performance was noticed towards baseline systems that adopted more complicated neural architectures",
    "volume": "workshop",
    "checked": true,
    "id": "0ad87fb4416144e75a7f3a458d7d47cb0968de6e",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-4605": {
    "title": "ArbEngVec : Arabic-English Cross-Lingual Word Embedding Model",
    "abstract": "Word Embeddings (WE) are getting increasingly popular and widely applied in many Natural Language Processing (NLP) applications due to their effectiveness in capturing semantic properties of words; Machine Translation (MT), Information Retrieval (IR) and Information Extraction (IE) are among such areas. In this paper, we propose an open source ArbEngVec which provides several Arabic-English cross-lingual word embedding models. To train our bilingual models, we use a large dataset with more than 93 million pairs of Arabic-English parallel sentences. In addition, we perform both extrinsic and intrinsic evaluations for the different word embedding model variants. The extrinsic evaluation assesses the performance of models on the cross-language Semantic Textual Similarity (STS), while the intrinsic evaluation is based on the Word Translation (WT) task",
    "volume": "workshop",
    "checked": true,
    "id": "ac038639159fd921b16b88278a77c629188380cd",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-4606": {
    "title": "Homograph Disambiguation through Selective Diacritic Restoration",
    "abstract": "Lexical ambiguity, a challenging phenomenon in all natural languages, is particularly prevalent for languages with diacritics that tend to be omitted in writing, such as Arabic. Omitting diacritics leads to an increase in the number of homographs: different words with the same spelling. Diacritic restoration could theoretically help disambiguate these words, but in practice, the increase in overall sparsity leads to performance degradation in NLP applications. In this paper, we propose approaches for automatically marking a subset of words for diacritic restoration, which leads to selective homograph disambiguation. Compared to full or no diacritic restoration, these approaches yield selectively-diacritized datasets that balance sparsity and lexical disambiguation. We evaluate the various selection strategies extrinsically on several downstream applications: neural machine translation, part-of-speech tagging, and semantic textual similarity. Our experiments on Arabic show promising results, where our devised strategies on selective diacritization lead to a more balanced and consistent performance in downstream applications",
    "volume": "workshop",
    "checked": true,
    "id": "f29c075a3e18375c7685c7d47f67e93452bea8c4",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-4607": {
    "title": "Arabic Named Entity Recognition: What Works and What's Next",
    "abstract": "This paper presents the winning solution to the Arabic Named Entity Recognition challenge run by Topcoder.com. The proposed model integrates various tailored techniques together, including representation learning, feature engineering, sequence labeling, and ensemble learning. The final model achieves a test F_1 score of 75.82% on the AQMAR dataset and outperforms baselines by a large margin. Detailed analyses are conducted to reveal both its strengths and limitations. Specifically, we observe that (1) representation learning modules can significantly boost the performance but requires a proper pre-processing and (2) the resulting embedding can be further enhanced with feature engineering due to the limited size of the training data. All implementations and pre-trained models are made public",
    "volume": "workshop",
    "checked": true,
    "id": "7601de6dcf9e42adc2e59160ea7556a919c97aef",
    "citation_count": 11
  },
  "https://aclanthology.org/W19-4608": {
    "title": "hULMonA: The Universal Language Model in Arabic",
    "abstract": "Arabic is a complex language with limited resources which makes it challenging to produce accurate text classification tasks such as sentiment analysis. The utilization of transfer learning (TL) has recently shown promising results for advancing accuracy of text classification in English. TL models are pre-trained on large corpora, and then fine-tuned on task-specific datasets. In particular, universal language models (ULMs), such as recently developed BERT, have achieved state-of-the-art results in various NLP tasks in English. In this paper, we hypothesize that similar success can be achieved for Arabic. The work aims at supporting the hypothesis by developing the first Universal Language Model in Arabic (hULMonA - حلمنا meaning our dream), demonstrating its use for Arabic classifications tasks, and demonstrating how a pre-trained multi-lingual BERT can also be used for Arabic. We then conduct a benchmark study to evaluate both ULM successes with Arabic sentiment analysis. Experiment results show that the developed hULMonA and multi-lingual ULM are able to generalize well to multiple Arabic data sets and achieve new state of the art results in Arabic Sentiment Analysis for some of the tested sets",
    "volume": "workshop",
    "checked": true,
    "id": "91663fbb9257b1ee5d22d7b9e796e03f374a7a6a",
    "citation_count": 29
  },
  "https://aclanthology.org/W19-4609": {
    "title": "Neural Models for Detecting Binary Semantic Textual Similarity for Algerian and MSA",
    "abstract": "We explore the extent to which neural networks can learn to identify semantically equivalent sentences from a small variable dataset using an end-to-end training. We collect a new noisy non-standardised user-generated Algerian (ALG) dataset and also translate it to Modern Standard Arabic (MSA) which serves as its regularised counterpart. We compare the performance of various models on both datasets and report the best performing configurations. The results show that relatively simple models composed of 2 LSTM layers outperform by far other more sophisticated attention-based architectures, for both ALG and MSA datasets",
    "volume": "workshop",
    "checked": true,
    "id": "2120b4c410c3c1ca386476da66e18b1fd06dfef2",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4610": {
    "title": "Constrained Sequence-to-sequence Semitic Root Extraction for Enriching Word Embeddings",
    "abstract": "In this paper, we tackle the problem of \"root extraction\" from words in the Semitic language family. A challenge in applying natural language processing techniques to these languages is the data sparsity problem that arises from their rich internal morphology, where the substructure is inherently non-concatenative and morphemes are interdigitated in word formation. While previous automated methods have relied on human-curated rules or multiclass classification, they have not fully leveraged the various combinations of regular, sequential concatenative morphology within the words and the internal interleaving within templatic stems of roots and patterns. To address this, we propose a constrained sequence-to-sequence root extraction method. Experimental results show our constrained model outperforms a variety of methods at root extraction. Furthermore, by enriching word embeddings with resulting decompositions, we show improved results on word analogy, word similarity, and language modeling tasks",
    "volume": "workshop",
    "checked": true,
    "id": "0301a1c98cffff7eeb86783751ebe25df4e203b4",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4611": {
    "title": "En-Ar Bilingual Word Embeddings without Word Alignment: Factors Effects",
    "abstract": "This paper introduces the first attempt to investigate morphological segmentation on En-Ar bilingual word embeddings using bilingual word embeddings model without word alignment (BilBOWA). We investigate the effect of sentence length and embedding size on the learning process. Our experiment shows that using the D3 segmentation scheme improves the accuracy of learning bilingual word embeddings up to 10 percentage points compared to the ATB and D0 schemes in all different training settings",
    "volume": "workshop",
    "checked": true,
    "id": "3d264dfb052b01f667c96c5361fc3b810cbfff1d",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-4612": {
    "title": "Neural Arabic Question Answering",
    "abstract": "This paper tackles the problem of open domain factual Arabic question answering (QA) using Wikipedia as our knowledge source. This constrains the answer of any question to be a span of text in Wikipedia. Open domain QA for Arabic entails three challenges: annotated QA datasets in Arabic, large scale efficient information retrieval and machine reading comprehension. To deal with the lack of Arabic QA datasets we present the Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT. Our experiments on ARCD indicate the effectiveness of our approach with our BERT-based reader achieving a 61.3 F1 score, and our open domain system SOQAL achieving a 27.6 F1 score",
    "volume": "workshop",
    "checked": true,
    "id": "cb26ef503ee9d525771736a7f9136d3a4649d9ef",
    "citation_count": 62
  },
  "https://aclanthology.org/W19-4613": {
    "title": "Segmentation for Domain Adaptation in Arabic",
    "abstract": "Segmentation serves as an integral part in many NLP applications including Machine Translation, Parsing, and Information Retrieval. When a model trained on the standard language is applied to dialects, the accuracy drops dramatically. However, there are more lexical items shared by the standard language and dialects than can be found by mere surface word matching. This shared lexicon is obscured by a lot of cliticization, gemination, and character repetition. In this paper, we prove that segmentation and base normalization of dialects can help in domain adaptation by reducing data sparseness. Segmentation will improve a system performance by reducing the number of OOVs, help isolate the differences and allow better utilization of the commonalities. We show that adding a small amount of dialectal segmentation training data reduced OOVs by 5% and remarkably improves POS tagging for dialects by 7.37% f-score, even though no dialect-specific POS training data is included",
    "volume": "workshop",
    "checked": true,
    "id": "5002ff13cd90a5f59adad6612a7e40977f74b7a2",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4614": {
    "title": "Assessing Arabic Weblog Credibility via Deep Co-learning",
    "abstract": "Assessing the credibility of online content has garnered a lot of attention lately. We focus on one such type of online content, namely weblogs or blogs for short. Some recent work attempted the task of automatically assessing the credibility of blogs, typically via machine learning. However, in the case of Arabic blogs, there are hardly any datasets available that can be used to train robust machine learning models for this difficult task. To overcome the lack of sufficient training data, we propose deep co-learning, a semi-supervised end-to-end deep learning approach to assess the credibility of Arabic blogs. In deep co-learning, multiple weak deep neural network classifiers are trained using a small labeled dataset, and each using a different view of the data. Each one of these classifiers is then used to classify unlabeled data, and its prediction is used to train the other classifiers in a semi-supervised fashion. We evaluate our deep co-learning approach on an Arabic blogs dataset, and we report significant improvements in performance compared to many baselines including fully-supervised deep learning models as well as ensemble models",
    "volume": "workshop",
    "checked": true,
    "id": "0127dcd127d66dbfd11cce1a6c7302d206dde705",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-4615": {
    "title": "Morphologically Annotated Corpora for Seven Arabic Dialects: Taizi, Sanaani, Najdi, Jordanian, Syrian, Iraqi and Moroccan",
    "abstract": "We present a collection of morphologically annotated corpora for seven Arabic dialects: Taizi Yemeni, Sanaani Yemeni, Najdi, Jordanian, Syrian, Iraqi and Moroccan Arabic. The corpora collectively cover over 200,000 words, and are all manually annotated in a common set of standards for orthography, diacritized lemmas, tokenization, morphological units and English glosses. These corpora will be publicly available to serve as benchmarks for training and evaluating systems for Arabic dialect morphological analysis and disambiguation",
    "volume": "workshop",
    "checked": true,
    "id": "9e6f21a1a572a9991ecc1660f840c086d2539c99",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4616": {
    "title": "Construction and Annotation of the Jordan Comprehensive Contemporary Arabic Corpus (JCCA)",
    "abstract": "To compile a modern dictionary that catalogues the words in currency, and to study linguistic patterns in the contemporary language, it is necessary to have a corpus of authentic texts that reflect current usage of the language. Although there are numerous Arabic corpora, none claims to be representative of the language in terms of the combination of geographical region, genre, subject matter, mode, and medium. This paper describes a 100-million-word corpus that takes the British National Corpus (BNC) as a model. The aim of the corpus is to be balanced, annotated, comprehensive, and representative of contemporary Arabic as written and spoken in Arab countries today. It will be different from most others in not being heavily-dominated by the news or in mixing the classical with the modern. In this paper is an outline of the methodology adopted for the design, construction, and annotation of this corpus. DIWAN (Alshargi and Rambow, 2015) was used to annotate a one-million-word snapshot of the corpus. DIWAN is a dialectal word annotation tool, but we upgraded it by adding a new tag-set that is based on traditional Arabic grammar and by adding the roots and morphological patterns of nouns and verbs. Moreover, the corpus we constructed covers the major spoken varieties of Arabic",
    "volume": "workshop",
    "checked": true,
    "id": "bbd16017be01950f99271478e7514c1b3f814149",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4617": {
    "title": "Translating Between Morphologically Rich Languages: An Arabic-to-Turkish Machine Translation System",
    "abstract": "This paper introduces the work on building a machine translation system for Arabic-to-Turkish in the news domain. Our work includes collecting parallel datasets in several ways for a new and low-resourced language pair, building baseline systems with state-of-the-art architectures and developing language specific algorithms for better translation. Parallel datasets are mainly collected three different ways; i) translating Arabic texts into Turkish by professional translators, ii) exploiting the web for open-source Arabic-Turkish parallel texts, iii) using back-translation. We per-formed preliminary experiments for Arabic-to-Turkish machine translation with neural(Marian) machine translation tools with a novel morphologically motivated vocabulary reduction method",
    "volume": "workshop",
    "checked": true,
    "id": "82fec7ac082177a3ad3bcfb97583bebf36c4b741",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4618": {
    "title": "Improved Generalization of Arabic Text Classifiers",
    "abstract": "While transfer learning for text has been very active in the English language, progress in Arabic has been slow, including the use of Domain Adaptation (DA). Domain Adaptation is used to generalize the performance of any classifier by trying to balance the classifier's accuracy for a particular task among different text domains. In this paper, we propose and evaluate two variants of a domain adaptation technique: the first is a base model called Domain Adversarial Neural Network (DANN), while the second is a variation that incorporates representational learning. Similar to previous approaches, we propose the use of proxy A-distance as a metric to assess the success of generalization. We make use of ArSentDLEV, a multi-topic dataset collected from the Levantine countries, to test the performance of the models. We show the superiority of the proposed method in accuracy and robustness when dealing with the Arabic language",
    "volume": "workshop",
    "checked": true,
    "id": "f4a425499128d53d97a86f2cd625990653101356",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4619": {
    "title": "OSIAN: Open Source International Arabic News Corpus - Preparation and Integration into the CLARIN-infrastructure",
    "abstract": "The World Wide Web has become a fundamental resource for building large text corpora. Broadcasting platforms such as news websites are rich sources of data regarding diverse topics and form a valuable foundation for research. The Arabic language is extensively utilized on the Web. Still, Arabic is relatively an under-resourced language in terms of availability of freely annotated corpora. This paper presents the first version of the Open Source International Arabic News (OSIAN) corpus. The corpus data was collected from international Arabic news websites, all being freely available on the Web. The corpus consists of about 3.5 million articles comprising more than 37 million sentences and roughly 1 billion tokens. It is encoded in XML; each article is annotated with metadata information. Moreover, each word is annotated with lemma and part-of-speech. the described corpus is processed, archived and published into the CLARIN infrastructure. This publication includes descriptive metadata via OAI-PMH, direct access to the plain text material (available under Creative Commons Attribution-Non-Commercial 4.0 International License - CC BY-NC 4.0), and integration into the WebLicht annotation platform and CLARIN's Federated Content Search FCS",
    "volume": "workshop",
    "checked": true,
    "id": "e471dccea2b2a05196137984847d7e2067d259a1",
    "citation_count": 36
  },
  "https://aclanthology.org/W19-4620": {
    "title": "Arabic Tweet-Act: Speech Act Recognition for Arabic Asynchronous Conversations",
    "abstract": "Speech acts are the actions that a speaker intends when performing an utterance within conversations. In this paper, we proposed speech act classification for asynchronous conversations on Twitter using multiple machine learning methods including SVM and deep neural networks. We applied the proposed methods on the ArSAS tweets dataset. The obtained results show that superiority of deep learning methods compared to SVMs, where Bi-LSTM managed to achieve an accuracy of 87.5% and a macro-averaged F1 score 61.5%. We believe that our results are the first to be reported on the task of speech-act recognition for asynchronous conversations on Arabic Twitter",
    "volume": "workshop",
    "checked": true,
    "id": "2945fd0722412fdefff85a2b8bdd977670ddebc3",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4621": {
    "title": "Mazajak: An Online Arabic Sentiment Analyser",
    "abstract": "Sentiment analysis (SA) is one of the most useful natural language processing applications. Literature is flooding with many papers and systems addressing this task, but most of the work is focused on English. In this paper, we present \"Mazajak\", an online system for Arabic SA. The system is based on a deep learning model, which achieves state-of-the-art results on many Arabic dialect datasets including SemEval 2017 and ASTD. The availability of such system should assist various applications and research that rely on sentiment analysis as a tool",
    "volume": "workshop",
    "checked": true,
    "id": "33c1296c2de0bf7a491d0ff4d75afe8b421bfc04",
    "citation_count": 76
  },
  "https://aclanthology.org/W19-4622": {
    "title": "The MADAR Shared Task on Arabic Fine-Grained Dialect Identification",
    "abstract": "In this paper, we present the results and findings of the MADAR Shared Task on Arabic Fine-Grained Dialect Identification. This shared task was organized as part of The Fourth Arabic Natural Language Processing Workshop, collocated with ACL 2019. The shared task includes two subtasks: the MADAR Travel Domain Dialect Identification subtask (Subtask 1) and the MADAR Twitter User Dialect Identification subtask (Subtask 2). This shared task is the first to target a large set of dialect labels at the city and country levels. The data for the shared task was created or collected under the Multi-Arabic Dialect Applications and Resources (MADAR) project. A total of 21 teams from 15 countries participated in the shared task",
    "volume": "workshop",
    "checked": true,
    "id": "7e95535491b1b0cea66d182a9bd894a5e6be5562",
    "citation_count": 74
  },
  "https://aclanthology.org/W19-4623": {
    "title": "ZCU-NLP at MADAR 2019: Recognizing Arabic Dialects",
    "abstract": "In this paper, we present our systems for the MADAR Shared Task: Arabic Fine-Grained Dialect Identification. The shared task consists of two subtasks. The goal of Subtask– 1 (S-1) is to detect an Arabic city dialect in a given text and the goal of Subtask–2 (S-2) is to predict the country of origin of a Twitter user by using tweets posted by the user. In S-1, our proposed systems are based on language modelling. We use language models to extract features that are later used as an input for other machine learning algorithms. We also experiment with recurrent neural networks (RNN), but these experiments showed that simpler machine learning algorithms are more successful. Our system achieves 0.658 macro F1-score and our rank is 6th out of 19 teams in S-1 and 7th in S-2 with 0.475 macro F1-score",
    "volume": "workshop",
    "checked": true,
    "id": "73804caea4fd9c8b370e7a1bfbb720b362160668",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4624": {
    "title": "Simple But Not Naïve: Fine-Grained Arabic Dialect Identification Using Only N-Grams",
    "abstract": "This paper presents the participation of Qatar University team in MADAR shared task, which addresses the problem of sentence-level fine-grained Arabic Dialect Identification over 25 different Arabic dialects in addition to the Modern Standard Arabic. Arabic Dialect Identification is not a trivial task since different dialects share some features, e.g., utilizing the same character set and some vocabularies. We opted to adopt a very simple approach in terms of extracted features and classification models; we only utilize word and character n-grams as features, and Na ̈ıve Bayes models as classifiers. Surprisingly, the simple approach achieved non-na ̈ıve performance. The official results, reported on a held-out testing set, show that the dialect of a given sentence can be identified at an accuracy of 64.58% by our best submitted run",
    "volume": "workshop",
    "checked": true,
    "id": "0aabc6521c37961576f6317a99411275ca6f523a",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4625": {
    "title": "LIUM-MIRACL Participation in the MADAR Arabic Dialect Identification Shared Task",
    "abstract": "This paper describes the joint participation of the LIUM and MIRACL Laboratories at the Arabic dialect identification challenge of the MADAR Shared Task (Bouamor et al., 2019) conducted during the Fourth Arabic Natural Language Processing Workshop (WANLP 2019). We participated to the Travel Domain Dialect Identification subtask. We built several systems and explored different techniques including conventional machine learning methods and deep learning algorithms. Deep learning approaches did not perform well on this task. We experimented several classification systems and we were able to identify the dialect of an input sentence with an F1-score of 65.41% on the official test set using only the training data supplied by the shared task organizers",
    "volume": "workshop",
    "checked": true,
    "id": "3a1fe16b3c650156985ffe16b924c57d47e95a55",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4626": {
    "title": "Arabic Dialect Identification with Deep Learning and Hybrid Frequency Based Features",
    "abstract": "Studies on Dialectical Arabic are growing more important by the day as it becomes the primary written and spoken form of Arabic online in informal settings. Among the important problems that should be explored is that of dialect identification. This paper reports different techniques that can be applied towards such goal and reports their performance on the Multi Arabic Dialect Applications and Resources (MADAR) Arabic Dialect Corpora. Our results show that improving on traditional systems using frequency based features and non deep learning classifiers is a challenging task. We propose different models based on different word and document representations. Our top model is able to achieve an F1 macro averaged score of 65.66 on MADAR's small-scale parallel corpus of 25 dialects and Modern Standard Arabic (MSA)",
    "volume": "workshop",
    "checked": true,
    "id": "84556b2a421c801e17437966075f09cb9847a8b0",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4627": {
    "title": "MICHAEL: Mining Character-level Patterns for Arabic Dialect Identification (MADAR Challenge)",
    "abstract": "We present MICHAEL, a simple lightweight method for automatic Arabic Dialect Identification on the MADAR travel domain Dialect Identification (DID). MICHAEL uses simple character-level features in order to perform a pre-processing free classification. More precisely, Character N-grams extracted from the original sentences are used to train a Multinomial Naive Bayes classifier. This system achieved an official score (accuracy) of 53.25% with 1<=N<=3 but showed a much better result with character 4-grams (62.17% accuracy)",
    "volume": "workshop",
    "checked": true,
    "id": "13e7242d76835050e7d225315d1554efd18ea65f",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4628": {
    "title": "Arabic Dialect Identification for Travel and Twitter Text",
    "abstract": "This paper presents the results of the experiments done as a part of MADAR Shared Task in WANLP 2019 on Arabic Fine-Grained Dialect Identification. Dialect Identification is one of the prominent tasks in the field of Natural language processing where the subsequent language modules can be improved based on it. We explored the use of different features like char, word n-gram, language model probabilities, etc on different classifiers. Results show that these features help to improve dialect classification accuracy. Results also show that traditional machine learning classifier tends to perform better when compared to neural network models on this task in a low resource setting",
    "volume": "workshop",
    "checked": true,
    "id": "5140c7db54b0f038002115edee8ce1f6081b47a4",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-4629": {
    "title": "Mawdoo3 AI at MADAR Shared Task: Arabic Tweet Dialect Identification",
    "abstract": "Arabic dialect identification is an inherently complex problem, as Arabic dialect taxonomy is convoluted and aims to dissect a continuous space rather than a discrete one. In this work, we present machine and deep learning approaches to predict 21 fine-grained dialects form a set of given tweets per user. We adopted numerous feature extraction methods most of which showed improvement in the final model, such as word embedding, Tf-idf, and other tweet features. Our results show that a simple LinearSVC can outperform any complex deep learning model given a set of curated features. With a relatively complex user voting mechanism, we were able to achieve a Macro-Averaged F1-score of 71.84% on MADAR shared subtask-2. Our best submitted model ranked second out of all participating teams",
    "volume": "workshop",
    "checked": true,
    "id": "ccc294f3a37654a4ac55b54fb234de58e9e78164",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4630": {
    "title": "Mawdoo3 AI at MADAR Shared Task: Arabic Fine-Grained Dialect Identification with Ensemble Learning",
    "abstract": "In this paper we discuss several models we used to classify 25 city-level Arabic dialects in addition to Modern Standard Arabic (MSA) as part of MADAR shared task (sub-task 1). We propose an ensemble model of a group of experimentally designed best performing classifiers on a various set of features. Our system achieves an accuracy of 69.3% macro F1-score with an improvement of 1.4% accuracy from the baseline model on the DEV dataset. Our best run submitted model ranked as third out of 19 participating teams on the TEST dataset with only 0.12% macro F1-score behind the top ranked system",
    "volume": "workshop",
    "checked": true,
    "id": "a708bbbf91fc639a440d97b652fe2a4b7647813d",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-4631": {
    "title": "Hierarchical Deep Learning for Arabic Dialect Identification",
    "abstract": "In this paper, we present two approaches for Arabic Fine-Grained Dialect Identification. The first approach is based on Recurrent Neural Networks (BLSTM, BGRU) using hierarchical classification. The main idea is to separate the classification process for a sentence from a given text in two stages. We start with a higher level of classification (8 classes) and then the finer-grained classification (26 classes). The second approach is given by a voting system based on Naive Bayes and Random Forest. Our system achieves an F1 score of 63.02 % on the subtask evaluation dataset",
    "volume": "workshop",
    "checked": true,
    "id": "a7b5e864a66da7aa8e55d09912602faecf48dc1d",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4632": {
    "title": "ArbDialectID at MADAR Shared Task 1: Language Modelling and Ensemble Learning for Fine Grained Arabic Dialect Identification",
    "abstract": "In this paper, we present a Dialect Identification system (ArbDialectID) that competed at Task 1 of the MADAR shared task, MADARTravel Domain Dialect Identification. We build a course and a fine-grained identification model to predict the label (corresponding to a dialect of Arabic) of a given text. We build two language models by extracting features at two levels (words and characters). We firstly build a coarse identification model to classify each sentence into one out of six dialects, then use this label as a feature for the fine-grained model that classifies the sentence among 26 dialects from different Arab cities, after that we apply ensemble voting classifier on both sub-systems. Our system ranked 1st that achieving an f-score of 67.32%. Both the models and our feature engineering tools are made available to the research community",
    "volume": "workshop",
    "checked": true,
    "id": "3aa4e34f847bf84e76a944a76dff514e3bdeccec",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-4633": {
    "title": "The SMarT Classifier for Arabic Fine-Grained Dialect Identification",
    "abstract": "This paper describes the approach adopted by the SMarT research group to build a dialect identification system in the framework of the Madar shared task on Arabic fine-grained dialect identification. We experimented several approaches, but we finally decided to use a Multinomial Naive Bayes classifier based on word and character ngrams in addition to the language model probabilities. We achieved a score of 67.73% in terms of Macro accuracy and a macro-averaged F1-score of 67.31%",
    "volume": "workshop",
    "checked": true,
    "id": "db3f8b3a0cbb42d05e5dcb16c314f0542d3e9504",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4634": {
    "title": "JHU System Description for the MADAR Arabic Dialect Identification Shared Task",
    "abstract": "Our submission to the MADAR shared task on Arabic dialect identification employed a language modeling technique called Prediction by Partial Matching, an ensemble of neural architectures, and sources of additional data for training word embeddings and auxiliary language models. We found several of these techniques provided small boosts in performance, though a simple character-level language model was a strong baseline, and a lower-order LM achieved best performance on Subtask 2. Interestingly, word embeddings provided no consistent benefit, and ensembling struggled to outperform the best component submodel. This suggests the variety of architectures are learning redundant information, and future work may focus on encouraging decorrelated learning",
    "volume": "workshop",
    "checked": true,
    "id": "e03fb68eed62b3bf3dfe5bab3dfd57985782e96d",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4635": {
    "title": "ST MADAR 2019 Shared Task: Arabic Fine-Grained Dialect Identification",
    "abstract": "This paper describes the solution that we propose on MADAR 2019 Arabic Fine-Grained Dialect Identification task. The proposed solution utilized a set of classifiers that we trained on character and word features. These classifiers are: Support Vector Machines (SVM), Bernoulli Naive Bayes (BNB), Multinomial Naive Bayes (MNB), Logistic Regression (LR), Stochastic Gradient Descent (SGD), Passive Aggressive(PA) and Perceptron (PC). The system achieved competitive results, with a performance of 62.87 % and 62.12 % for both development and test sets",
    "volume": "workshop",
    "checked": true,
    "id": "4a2e8f0d926f2c3584c3d1910e2086aca404ca11",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4636": {
    "title": "A Character Level Convolutional BiLSTM for Arabic Dialect Identification",
    "abstract": "In this paper, we describe CU-RAISA teamcontribution to the 2019Madar shared task2, which focused on Twitter User fine-grained dialect identification.Among par-ticipating teams, our system ranked the4th(with 61.54%) F1-Macro measure.Our sys-tem is trained using a character level convo-lutional bidirectional long-short-term memorynetwork trained on 2k users' data. We showthat training on concatenated user tweets asinput is further superior to training on usertweets separately and assign user's label on themode of user's tweets' predictions",
    "volume": "workshop",
    "checked": true,
    "id": "3df56ab068b48587ac44fe1446010cd3b05ee993",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4637": {
    "title": "No Army, No Navy: BERT Semi-Supervised Learning of Arabic Dialects",
    "abstract": "We present our deep leaning system submitted to MADAR shared task 2 focused on twitter user dialect identification. We develop tweet-level identification models based on GRUs and BERT in supervised and semi-supervised set-tings. We then introduce a simple, yet effective, method of porting tweet-level labels at the level of users. Our system ranks top 1 in the competition, with 71.70% macro F1 score and 77.40% accuracy",
    "volume": "workshop",
    "checked": true,
    "id": "381c5b9b10aa4d9da4d089a03977a4638295b25c",
    "citation_count": 24
  },
  "https://aclanthology.org/W19-4638": {
    "title": "Team JUST at the MADAR Shared Task on Arabic Fine-Grained Dialect Identification",
    "abstract": "In this paper, we describe our team's effort on the MADAR Shared Task on Arabic Fine-Grained Dialect Identification. The task requires building a system capable of differentiating between 25 different Arabic dialects in addition to MSA. Our approach is simple. After preprocessing the data, we use Data Augmentation (DA) to enlarge the training data six times. We then build a language model and extract n-gram word-level and character-level TF-IDF features and feed them into an MNB classifier. Despite its simplicity, the resulting model performs really well producing the 4th highest F-measure and region-level accuracy and the 5th highest precision, recall, city-level accuracy and country-level accuracy among the participating teams",
    "volume": "workshop",
    "checked": true,
    "id": "7df844bb3d88ed1ab2e8025519aa8fb136c5fcaf",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-4639": {
    "title": "QC-GO Submission for MADAR Shared Task: Arabic Fine-Grained Dialect Identification",
    "abstract": "This paper describes the QC-GO team submission to the MADAR Shared Task Subtask 1 (travel domain dialect identification) and Subtask 2 (Twitter user location identification). In our participation in both subtasks, we explored a number of approaches and system combinations to obtain the best performance for both tasks. These include deep neural nets and heuristics. Since individual approaches suffer from various shortcomings, the combination of different approaches was able to fill some of these gaps. Our system achieves F1-Scores of 66.1% and 67.0% on the development sets for Subtasks 1 and 2 respectively",
    "volume": "workshop",
    "checked": true,
    "id": "4f96616e6c0d3c2915ce482530b24d5422f6d78c",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4701": {
    "title": "From Insanely Jealous to Insanely Delicious: Computational Models for the Semantic Bleaching of English Intensifiers",
    "abstract": "We introduce novel computational models for modeling semantic bleaching, a widespread category of change in which words become more abstract or lose elements of meaning, like the development of \"arrive\" from its earlier meaning 'become at shore.' We validate our methods on a widespread case of bleaching in English: de-adjectival adverbs that originate as manner adverbs (as in \"awfully behaved\") and later become intensifying adverbs (as in \"awfully nice\"). Our methods formally quantify three reflexes of bleaching: decreasing similarity to the source meaning (e.g., \"awful\"), increasing similarity to a fully bleached prototype (e.g., \"very\"), and increasing productivity (e.g., the breadth of adjectives that an adverb modifies). We also test a new causal model and find evidence that bleaching is initially triggered in contexts such as \"conspicuously evident\" and \"insanely jealous\", where an adverb premodifies a semantically similar adjective. These contexts provide a form of \"bridging context\" (Evans and Wilkins, 2000) that allow a manner adverb to be reinterpreted as an intensifying adverb similar to \"very\"",
    "volume": "workshop",
    "checked": true,
    "id": "a66a9420c879f5db5aada2c0eacdce82cb8cf33b",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4702": {
    "title": "Computational Analysis of the Historical Changes in Poetry and Prose",
    "abstract": "The esoteric definitions of poetry are insufficient in enveloping the changes in poetry that the age of mechanical reproduction has witnessed with the widespread proliferation of the use of digital media and artificial intelligence. They are also insufficient in distinguishing between prose and poetry, as the content of both prose and poetry can be poetic. Using quotes as prose considering their poetic, context-free and celebrated nature, stylistic differences between poetry and prose are delved into. Novel features in grammar and meter are justified as distinguishing features. Datasets of popular prose and poetry spanning across 1870-1920 and 1970-2019 have been created, and multiple experiments have been conducted to prove that prose and poetry in the latter period are more alike than they were in the former. The accuracy of classification of poetry and prose of 1970-2019 is significantly lesser than that of 1870-1920, thereby proving the convergence of poetry and prose in 1970-2019",
    "volume": "workshop",
    "checked": true,
    "id": "620af69127a3426ad3adf13280e09af6a3b7c91f",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-4703": {
    "title": "Studying Semantic Chain Shifts with Word2Vec: FOOD>MEAT>FLESH",
    "abstract": "Word2Vec models are used to study the semantic chain shift FOOD>MEAT>FLESH in the history of English, c. 1425-1925. The development stretches out over a long time, starting before 1500, and may possibly be continuing to this day. The semantic changes likely proceeded as a push chain",
    "volume": "workshop",
    "checked": true,
    "id": "8bebf60947f456babd4f827e88eafabe2c3f5e75",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4704": {
    "title": "Evaluation of Semantic Change of Harm-Related Concepts in Psychology",
    "abstract": "The paper focuses on diachronic evaluation of semantic changes of harm-related concepts in psychology. More specifically, we investigate a hypothesis that certain concepts such as \"addiction\", \"bullying\", \"harassment\", \"prejudice\", and \"trauma\" became broader during the last four decades. We evaluate semantic changes using two models: an LSA-based model from Sagi et al. (2009) and a diachronic adaptation of word2vec from Hamilton et al. (2016), that are trained on a large corpus of journal abstracts covering the period of 1980– 2019. Several concepts showed evidence of broadening. \"Addiction\" moved from physiological dependency on a substance to include psychological dependency on gaming and the Internet. Similarly, \"harassment\" and \"trauma\" shifted towards more psychological meanings. On the other hand, \"bullying\" has transformed into a more victim-related concept and expanded to new areas such as workplaces",
    "volume": "workshop",
    "checked": true,
    "id": "9a2a8d8db1a9a4516de6b99149c9df5d05946b8a",
    "citation_count": 14
  },
  "https://aclanthology.org/W19-4705": {
    "title": "Contextualized Diachronic Word Representations",
    "abstract": "Diachronic word embeddings play a key role in capturing interesting patterns about how language evolves over time. Most of the existing work focuses on studying corpora spanning across several decades, which is understandably still not a possibility when working on social media-based user-generated content. In this work, we address the problem of studying semantic changes in a large Twitter corpus collected over five years, a much shorter period than what is usually the norm in diachronic studies. We devise a novel attentional model, based on Bernoulli word embeddings, that are conditioned on contextual extra-linguistic (social) features such as network, spatial and socio-economic variables, which are associated with Twitter users, as well as topic-based features. We posit that these social features provide an inductive bias that helps our model to overcome the narrow time-span regime problem. Our extensive experiments reveal that our proposed model is able to capture subtle semantic shifts without being biased towards frequency cues and also works well when certain contextual features are absent. Our model fits the data better than current state-of-the-art dynamic word embedding models and therefore is a promising tool to study diachronic semantic changes over small time periods",
    "volume": "workshop",
    "checked": true,
    "id": "3e3eb188860292fc8d700236cdb076a4b2fe878a",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-4706": {
    "title": "Semantic Change and Semantic Stability: Variation is Key",
    "abstract": "I survey some recent approaches to studying change in the lexicon, particularly change in meaning across phylogenies. I briefly sketch an evolutionary approach to language change and point out some issues in recent approaches to studying semantic change that rely on temporally stratified word embeddings. I draw illustrations from lexical cognate models in Pama-Nyungan to identify meaning classes most appropriate for lexical phylogenetic inference, particularly highlighting the importance of variation in studying change over time",
    "volume": "workshop",
    "checked": true,
    "id": "86ae1161026f23f9df691a867fd7453cee56fd28",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-4707": {
    "title": "GASC: Genre-Aware Semantic Change for Ancient Greek",
    "abstract": "Word meaning changes over time, depending on linguistic and extra-linguistic factors. Associating a word's correct meaning in its historical context is a central challenge in diachronic research, and is relevant to a range of NLP tasks, including information retrieval and semantic search in historical texts. Bayesian models for semantic change have emerged as a powerful tool to address this challenge, providing explicit and interpretable representations of semantic change phenomena. However, while corpora typically come with rich metadata, existing models are limited by their inability to exploit contextual information (such as text genre) beyond the document time-stamp. This is particularly critical in the case of ancient languages, where lack of data and long diachronic span make it harder to draw a clear distinction between polysemy (the fact that a word has several senses) and semantic change (the process of acquiring, losing, or changing senses), and current systems perform poorly on these languages. We develop GASC, a dynamic semantic change model that leverages categorical metadata about the texts' genre to boost inference and uncover the evolution of meanings in Ancient Greek corpora. In a new evaluation framework, our model achieves improved predictive performance compared to the state of the art",
    "volume": "workshop",
    "checked": true,
    "id": "962c40c0aac6f8e650edb5761d9ae57f6ca89bc5",
    "citation_count": 28
  },
  "https://aclanthology.org/W19-4708": {
    "title": "Modeling Markedness with a Split-and-Merger Model of Sound Change",
    "abstract": "The concept of 'markedness' has been influential in phonology for almost a century. Theoretical phonology has found it useful to describe some segments as more 'marked' than others, referring to a cluster of language-internal and -external properties (Jakobson 1968, Haspelmath 2006). We argue, using a simple mathematical model based on Evolutionary Phonology (Blevins 2004), that markedness is an epiphenomenon of phonetically grounded sound change",
    "volume": "workshop",
    "checked": true,
    "id": "752cee272f170761a304c7992934b1b77d669b96",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4709": {
    "title": "A Method to Automatically Identify Diachronic Variation in Collocations",
    "abstract": "This paper introduces a novel method to track collocational variations in diachronic corpora that can identify several changes undergone by these phraseological combinations and to propose alternative solutions found in later periods. The strategy consists of extracting syntactically-related candidates of collocations and ranking them using statistical association measures. Then, starting from the first period of the corpus, the system tracks each combination over time, verifying different types of historical variation such as the loss of one or both lemmas, the disappearance of the collocation, or its diachronic frequency trend. Using a distributional semantics strategy, it also proposes other linguistic structures which convey similar meanings to those extinct collocations. A case study on historical corpora of Portuguese and Spanish shows that the system speeds up and facilitates the finding of some diachronic changes and phraseological shifts that are harder to identify without using automated methods",
    "volume": "workshop",
    "checked": true,
    "id": "7b69e19e47b8e2e60e874c21058815fa9831d453",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4710": {
    "title": "Written on Leaves or in Stones?: Computational Evidence for the Era of Authorship of Old Thai Prose",
    "abstract": "We aim to provide computational evidence for the era of authorship of two important old Thai texts: Traiphumikatha and Pumratchatham. The era of authorship of these two books is still an ongoing debate among Thai literature scholars. Analysis of old Thai texts present a challenge for standard natural language processing techniques, due to the lack of corpora necessary for building old Thai word and syllable segmentation. We propose an accurate and interpretable model to classify each segment as one of the three eras of authorship (Sukhothai, Ayuddhya, or Rattanakosin) without sophisticated linguistic preprocessing. Contrary to previous hypotheses, our model suggests that both books were written during the Sukhothai era. Moreover, the second half of the Pumratchtham is uncharacteristic of the Sukhothai era, which may have confounded literary scholars in the past. Further, our model reveals that the most indicative linguistic changes stem from unidirectional grammaticalized words and polyfunctional words, which show up as most dominant features in the model",
    "volume": "workshop",
    "checked": true,
    "id": "2a28fc3e0ef147557b25880aa6b33b028a41efef",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4711": {
    "title": "Identifying Temporal Trends Based on Perplexity and Clustering: Are We Looking at Language Change?",
    "abstract": "In this work we propose a data-driven methodology for identifying temporal trends in a corpus of medieval charters. We have used perplexities derived from RNNs as a distance measure between documents and then, performed clustering on those distances. We argue that perplexities calculated by such language models are representative of temporal trends. The clusters produced using the K-Means algorithm give an insight of the differences in language in different time periods at least partly due to language change. We suggest that the temporal distribution of the individual clusters might provide a more nuanced picture of temporal trends compared to discrete bins, thus providing better results when used in a classification task",
    "volume": "workshop",
    "checked": true,
    "id": "b0ec62489365a3f000d7d818796e18e5886b2492",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4712": {
    "title": "Using Word Embeddings to Examine Gender Bias in Dutch Newspapers, 1950-1990",
    "abstract": "Contemporary debates on filter bubbles and polarization in public and social media raise the question to what extent news media of the past exhibited biases. This paper specifically examines bias related to gender in six Dutch national newspapers between 1950 and 1990. We measure bias related to gender by comparing local changes in word embedding models trained on newspapers with divergent ideological backgrounds. We demonstrate clear differences in gender bias and changes within and between newspapers over time. In relation to themes such as sexuality and leisure, we see the bias moving toward women, whereas, generally, the bias shifts in the direction of men, despite growing female employment number and feminist movements. Even though Dutch society became less stratified ideologically (depillarization), we found an increasing divergence in gender bias between religious and social-democratic on the one hand and liberal newspapers on the other. Methodologically, this paper illustrates how word embeddings can be used to examine historical language change. Future work will investigate how fine-tuning deep contextualized embedding models, such as ELMO, might be used for similar tasks with greater contextual information",
    "volume": "workshop",
    "checked": true,
    "id": "cf9afb34557de5a524a3cebc6130da9bcf343b0f",
    "citation_count": 15
  },
  "https://aclanthology.org/W19-4713": {
    "title": "Predicting Historical Phonetic Features using Deep Neural Networks: A Case Study of the Phonetic System of Proto-Indo-European",
    "abstract": "Traditional historical linguistics lacks the possibility to empirically assess its assumptions regarding the phonetic systems of past languages and language stages since most current methods rely on comparative tools to gain insights into phonetic features of sounds in proto- or ancestor languages. The paper at hand presents a computational method based on deep neural networks to predict phonetic features of historical sounds where the exact quality is unknown and to test the overall coherence of reconstructed historical phonetic features. The method utilizes the principles of coarticulation, local predictability and statistical phonological constraints to predict phonetic features by the features of their immediate phonetic environment. The validity of this method will be assessed using New High German phonetic data and its specific application to diachronic linguistics will be demonstrated in a case study of the phonetic system Proto-Indo-European",
    "volume": "workshop",
    "checked": true,
    "id": "7aa867e092159e657b2773e071255955615a87c0",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4714": {
    "title": "ParHistVis: Visualization of Parallel Multilingual Historical Data",
    "abstract": "The study of language change through parallel corpora can be advantageous for the analysis of complex interactions between time, text domain and language. Often, those advantages cannot be fully exploited due to the sparse but high-dimensional nature of such historical data. To tackle this challenge, we introduce ParHistVis: a novel, free, easy-to-use, interactive visualization tool for parallel, multilingual, diachronic and synchronic linguistic data. We illustrate the suitability of the components of the tool based on a use case of word order change in Romance wh-interrogatives",
    "volume": "workshop",
    "checked": true,
    "id": "746dd8f0e198d1795c99898c5361ebfa11c740d3",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-4715": {
    "title": "Tracing Antisemitic Language Through Diachronic Embedding Projections: France 1789-1914",
    "abstract": "We investigate some aspects of the history of antisemitism in France, one of the cradles of modern antisemitism, using diachronic word embeddings. We constructed a large corpus of French books and periodicals issues that contain a keyword related to Jews and performed a diachronic word embedding over the 1789-1914 period. We studied the changes over time in the semantic spaces of 4 target words and performed embedding projections over 6 streams of antisemitic discourse. This allowed us to track the evolution of antisemitic bias in the religious, economic, socio-politic, racial, ethic and conspiratorial domains. Projections show a trend of growing antisemitism, especially in the years starting in the mid-80s and culminating in the Dreyfus affair. Our analysis also allows us to highlight the peculiar adverse bias towards Judaism in the broader context of other religions",
    "volume": "workshop",
    "checked": true,
    "id": "a6a08b716dd4decfb103435b44e76927fb73205a",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-4716": {
    "title": "DiaHClust: an Iterative Hierarchical Clustering Approach for Identifying Stages in Language Change",
    "abstract": "Language change is often assessed against a set of pre-determined time periods in order to be able to trace its diachronic trajectory. This is problematic, since a pre-determined periodization might obscure significant developments and lead to false assumptions about the data. Moreover, these time periods can be based on factors which are either arbitrary or non-linguistic, e.g., dividing the corpus data into equidistant stages or taking into account language-external events. Addressing this problem, in this paper we present a data-driven approach to periodization: 'DiaHClust'. DiaHClust is based on iterative hierarchical clustering and offers a multi-layered perspective on change from text-level to broader time periods. We demonstrate the usefulness of DiaHClust via a case study investigating syntactic change in Icelandic, modelling the syntactic system of the language in terms of vectors of syntactic change",
    "volume": "workshop",
    "checked": true,
    "id": "4522c6529ada2d98539c5cadfdcd31d1ce704dcd",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4717": {
    "title": "Treat the Word As a Whole or Look Inside? Subword Embeddings Model Language Change and Typology",
    "abstract": "We use a variant of word embedding model that incorporates subword information to characterize the degree of compositionality in lexical semantics. Our models reveal some interesting yet contrastive patterns of long-term change in multiple languages: Indo-European languages put more weight on subword units in newer words, while conversely Chinese puts less weights on the subwords, but more weight on the word as a whole. Our method provides novel evidence and methodology that enriches existing theories in evolutionary linguistics. The resulting word vectors also has decent performance in NLP-related tasks",
    "volume": "workshop",
    "checked": true,
    "id": "b385871f691ad1c7e67327604a164a14af1ad8fe",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4718": {
    "title": "Times Are Changing: Investigating the Pace of Language Change in Diachronic Word Embeddings",
    "abstract": "We propose Word Embedding Networks, a novel method that is able to learn word embeddings of individual data slices while simultaneously aligning and ordering them without feeding temporal information a priori to the model. This gives us the opportunity to analyse the dynamics in word embeddings on a large scale in a purely data-driven manner. In experiments on two different newspaper corpora, the New York Times (English) and die Zeit (German), we were able to show that time actually determines the dynamics of semantic change. However, there is by no means a uniform evolution, but instead times of faster and times of slower change",
    "volume": "workshop",
    "checked": true,
    "id": "e02e480fe54d0116bace0f614e4dd93367806f18",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4719": {
    "title": "The Rationality of Semantic Change",
    "abstract": "This study investigates the mutual effects over time of semantically related function words on each other's distribution over syntactic environments. Words that can have the same meaning are observed to have opposite trends of change in frequency across different syntactic structures which correspond to the shared meaning. This phenomenon is demonstrated to have a rational basis: it increases communicative efficiency by prioritizing words differently in the environments on which they compete",
    "volume": "workshop",
    "checked": true,
    "id": "e84a0f73ef8a390543e3088553783a621388d6cd",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4720": {
    "title": "Studying Laws of Semantic Divergence across Languages using Cognate Sets",
    "abstract": "Semantic divergence in related languages is a key concern of historical linguistics. Intra-lingual semantic shift has been previously studied in computational linguistics, but this can only provide a limited picture of the evolution of word meanings, which often develop in a multilingual environment. In this paper we investigate semantic change across languages by measuring the semantic distance of cognate words in multiple languages. By comparing current meanings of cognates in different languages, we hope to uncover information about their previous meanings, and about how they diverged in their respective languages from their common original etymon. We further study the properties of their semantic divergence, by analyzing how the features of words such as frequency and polysemy are related to the divergence in their meaning, and thus make the first steps towards formulating laws of cross-lingual semantic change",
    "volume": "workshop",
    "checked": true,
    "id": "03ce7947ba168d349d036131d0c4aee8e2d3bcc0",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-4721": {
    "title": "Detecting Syntactic Change Using a Neural Part-of-Speech Tagger",
    "abstract": "We train a diachronic long short-term memory (LSTM) part-of-speech tagger on a large corpus of American English from the 19th, 20th, and 21st centuries. We analyze the tagger's ability to implicitly learn temporal structure between years, and the extent to which this knowledge can be transferred to date new sentences. The learned year embeddings show a strong linear correlation between their first principal component and time. We show that temporal information encoded in the model can be used to predict novel sentences' years of composition relatively well. Comparisons to a feedforward baseline suggest that the temporal change learned by the LSTM is syntactic rather than purely lexical. Thus, our results suggest that our tagger is implicitly learning to model syntactic change in American English over the course of the 19th, 20th, and early 21st centuries",
    "volume": "workshop",
    "checked": true,
    "id": "acde43acd55e9efd69c491f950b8c893349c4abf",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4722": {
    "title": "Grammar and Meaning: Analysing the Topology of Diachronic Word Embeddings",
    "abstract": "The paper showcases the application of word embeddings to change in language use in the domain of science, focusing on the Late Modern English period (17-19th century). Historically, this is the period in which many registers of English developed, including the language of science. Our overarching interest is the linguistic development of scientific writing to a distinctive (group of) register(s). A register is marked not only by the choice of lexical words (discourse domain) but crucially by grammatical choices which indicate style. The focus of the paper is on the latter, tracing words with primarily grammatical functions (function words and some selected, poly-functional word forms) diachronically. To this end, we combine diachronic word embeddings with appropriate visualization and exploratory techniques such as clustering and relative entropy for meaningful aggregation of data and diachronic comparison",
    "volume": "workshop",
    "checked": true,
    "id": "a805392bd2e045c1677ad2d9e97c3713be495696",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4723": {
    "title": "Spatio-Temporal Prediction of Dialectal Variant Usage",
    "abstract": "The distribution of most dialectal variants have not only spatial but also temporal patterns. Based on the 'apparent time hypothesis', much of dialect change is happening through younger speakers accepting innovations. Thus, synchronic diversity can be interpreted diachronically. With the assumption of the 'contact effect', i.e. contact possibility (contact and isolation) between speaker communities being responsible for language change, and the apparent time hypothesis, we aim to predict the usage of dialectal variants. In this paper we model the contact possibility based on two of the most important factors in sociolinguistics to be affecting language change: age and distance. The first steps of the approach involve modeling contact possibility using a logistic predictor, taking the age of respondents into account. We test the global, and the local role of age for variation where the local level means spatial subsets around each survey site, chosen based on k nearest neighbors. The prediction approach is tested on Swiss German syntactic survey data, featuring multiple respondents from different age cohorts at survey sites. The results show the relative success of the logistic prediction approach and the limitations of the method, therefore further proposals are made to develop the methodology",
    "volume": "workshop",
    "checked": true,
    "id": "2ec56fa9814039f9435ee6242e06a9ff57518a73",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4724": {
    "title": "One-to-X Analogical Reasoning on Word Embeddings: a Case for Diachronic Armed Conflict Prediction from News Texts",
    "abstract": "We extend the well-known word analogy task to a one-to-X formulation, including one-to-none cases, when no correct answer exists. The task is cast as a relation discovery problem and applied to historical armed conflicts datasets, attempting to predict new relations of type 'location:armed-group' based on data about past events. As the source of semantic information, we use diachronic word embedding models trained on English news texts. A simple technique to improve diachronic performance in such task is demonstrated, using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora. Finally, we publish a ready-to-use test set for one-to-X analogy evaluation on historical armed conflicts data",
    "volume": "workshop",
    "checked": true,
    "id": "42c63d952f0cce7f89738e818fc9ab4e723bf7b6",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4725": {
    "title": "Measuring Diachronic Evolution of Evaluative Adjectives with Word Embeddings: the Case for English, Norwegian, and Russian",
    "abstract": "We measure the intensity of diachronic semantic shifts in adjectives in English, Norwegian and Russian across 5 decades. This is done in order to test the hypothesis that evaluative adjectives are more prone to temporal semantic change. To this end, 6 different methods of quantifying semantic change are used. Frequency-controlled experimental results show that, depending on the particular method, evaluative adjectives either do not differ from other types of adjectives in terms of semantic change or appear to actually be less prone to shifting (particularly, to 'jitter'-type shifting). Thus, in spite of many well-known examples of semantically changing evaluative adjectives (like 'terrific' or 'incredible'), it seems that such cases are not specific to this particular type of words",
    "volume": "workshop",
    "checked": true,
    "id": "d54c9912fb475060f8858d17174f7da0f93f04e6",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-4726": {
    "title": "Semantic Change in the Language of UK Parliamentary Debates",
    "abstract": "We investigate changes in the meanings of words used in the UK Parliament across two different epochs. We use word embeddings to explore changes in the distribution of words of interest and uncover words that appear to have undergone semantic transformation in the intervening period, and explore different ways of obtaining target words for this purpose. We find that semantic changes are generally in line with those found in other corpora, and little evidence that parliamentary language is more static than general English. It also seems that words with senses that have been recorded in the dictionary as having fallen into disuse do not undergo semantic changes in this domain",
    "volume": "workshop",
    "checked": true,
    "id": "cb95f7a9bed6c346a605ac542dd8ea9693df3042",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4727": {
    "title": "Semantic Change and Emerging Tropes In a Large Corpus of New High German Poetry",
    "abstract": "Due to its semantic succinctness and novelty of expression, poetry is a great test-bed for semantic change analysis. However, so far there is a scarcity of large diachronic corpora. Here, we provide a large corpus of German poetry which consists of about 75k poems with more than 11 million tokens, with poems ranging from the 16th to early 20th century. We then track semantic change in this corpus by investigating the rise of tropes ('love is magic') over time and detecting change points of meaning, which we find to occur particularly within the German Romantic period. Additionally, through self-similarity, we reconstruct literary periods and find evidence that the law of linear semantic change also applies to poetry",
    "volume": "workshop",
    "checked": true,
    "id": "14eaf9dcd3ead83b52fcdc8c99d6d43111f3c686",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4728": {
    "title": "Conceptual Change and Distributional Semantic Models: an Exploratory Study on Pitfalls and Possibilities",
    "abstract": "Studying conceptual change using embedding models has become increasingly popular in the Digital Humanities community while critical observations about them have received less attention. This paper investigates what the impact of known pitfalls can be on the conclusions drawn in a digital humanities study through the use case of \"Racism\". In addition, we suggest an approach for modeling a complex concept in terms of words and relations representative of the conceptual system. Our results show that different models created from the same data yield different results, but also indicate that using different model architectures, comparing different corpora and comparing to control words and relations can help to identify which results are solid and which may be due to artefact. We propose guidelines to conduct similar studies, but also note that more work is needed to fully understand how we can distinguish artefacts from actual conceptual changes",
    "volume": "workshop",
    "checked": true,
    "id": "0cb395d61410b9689ad8552694d5a909ac1d50c4",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-4729": {
    "title": "Measuring the Compositionality of Noun-Noun Compounds over Time",
    "abstract": "We present work in progress on the temporal progression of compositionality in noun-noun compounds. Previous work has proposed computational methods for determining the compositionality of compounds. These methods try to automatically determine how transparent the meaning of the compound as a whole is with respect to the meaning of its parts. We hypothesize that such a property might change over time. We use the time-stamped Google Books corpus for our diachronic investigations, and first examine whether the vector-based semantic spaces extracted from this corpus are able to predict compositionality ratings, despite their inherent limitations. We find that using temporal information helps predicting the ratings, although correlation with the ratings is lower than reported for other corpora. Finally, we show changes in compositionality over time for a selection of compounds",
    "volume": "workshop",
    "checked": true,
    "id": "deb7082e9bbcc73832ccd4eaa724fb5d438279f0",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-4730": {
    "title": "Towards Automatic Variant Analysis of Ancient Devotional Texts",
    "abstract": "We address in this paper the issue of text reuse in liturgical manuscripts of the middle ages. More specifically, we study variant readings of the Obsecro Te prayer, part of the devotional Books of Hours often used by Christians as guidance for their daily prayers. We aim at automatically extracting and categorising pairs of words and expressions that exhibit variant relations. For this purpose, we adopt a linguistic classification that allows to better characterize the variants than edit operations. Then, we study the evolution of Obsecro Te texts from a temporal and geographical axis. Finally, we contrast several unsupervised state-of-the-art approaches for the automatic extraction of Obsecro Te variants. Based on the manual observation of 772 Obsecro Te copies which show more than 21,000 variants, we show that the proposed methodology is helpful for an automatic study of variants and may serve as basis to analyze and to depict useful information from devotional texts",
    "volume": "workshop",
    "checked": true,
    "id": "803a1a024d07ab52aea82322db8309d1f9f10124",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4731": {
    "title": "Understanding the Evolution of Circular Economy through Language Change",
    "abstract": "In this study, we propose to focus on understanding the evolution of a specific scientific concept—that of Circular Economy (CE)—by analysing how the language used in academic discussions has changed semantically. It is worth noting that the meaning and central theme of this concept has remained the same; however, we hypothesise that it has undergone semantic change by way of additional layers being added to the concept. We have shown that semantic change in language is a reflection of shifts in scientific ideas, which in turn help explain the evolution of a concept. Focusing on the CE concept, our analysis demonstrated that the change over time in the language used in academic discussions of CE is indicative of the way in which the concept evolved and expanded",
    "volume": "workshop",
    "checked": true,
    "id": "aa441d89046dd5d05e39374ab60efcc27b416362",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-4732": {
    "title": "Gaussian Process Models of Sound Change in Indo-Aryan Dialectology",
    "abstract": "This paper proposes a Gaussian Process model of sound change targeted toward questions in Indo-Aryan dialectology. Gaussian Processes (GPs) provide a flexible means of expressing covariance between outcomes, and can be extended to a wide variety of probability distributions. We find that GP models fare better in terms of some key posterior predictive checks than models that do not express covariance between sound changes, and outline directions for future work",
    "volume": "workshop",
    "checked": true,
    "id": "1c302c7f2439c29f5446fc77e07f8bcc2abd5d72",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4733": {
    "title": "Modeling a Historical Variety of a Low-Resource Language: Language Contact Effects in the Verbal Cluster of Early-Modern Frisian",
    "abstract": "Certain phenomena of interest to linguists mainly occur in low-resource languages, such as contact-induced language change. We show that it is possible to study contact-induced language change computationally in a historical variety of a low-resource language, Early-Modern Frisian, by creating a model using features that were established to be relevant in a closely related language, modern Dutch. This allows us to test two hypotheses on two types of language contact that may have taken place between Frisian and Dutch during this time. Our model shows that Frisian verb cluster word orders are associated with different context features than Dutch verb orders, supporting the 'learned borrowing' hypothesis",
    "volume": "workshop",
    "checked": true,
    "id": "3f5137e77bdeaa58dcfe95f61c939aab7133106b",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-4734": {
    "title": "Visualizing Linguistic Change as Dimension Interactions",
    "abstract": "Historical change typically is the result of complex interactions between several linguistic factors. Identifying the relevant factors and understanding how they interact across the temporal dimension is the core remit of historical linguistics. With respect to corpus work, this entails a separate annotation, extraction and painstaking pair-wise comparison of the relevant bits of information. This paper presents a significant extension of HistoBankVis, a multilayer visualization system which allows a fast and interactive exploration of complex linguistic data. Linguistic factors can be understood as data dimensions which show complex interrelationships. We model these relationships with the Parallel Sets technique. We demonstrate the powerful potential of this technique by applying the system to understanding the interaction of case, grammatical relations and word order in the history of Icelandic",
    "volume": "workshop",
    "checked": true,
    "id": "44e42f12708cf67c4f1161b6f953364e13176360",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-4801": {
    "title": "Transcoding Compositionally: Using Attention to Find More Generalizable Solutions",
    "abstract": "While sequence-to-sequence models have shown remarkable generalization power across several natural language tasks, their construct of solutions are argued to be less compositional than human-like generalization. In this paper, we present seq2attn, a new architecture that is specifically designed to exploit attention to find compositional patterns in the input. In seq2attn, the two standard components of an encoder-decoder model are connected via a transcoder, that modulates the information flow between them. We show that seq2attn can successfully generalize, without requiring any additional supervision, on two tasks which are specifically constructed to challenge the compositional skills of neural networks. The solutions found by the model are highly interpretable, allowing easy analysis of both the types of solutions that are found and potential causes for mistakes. We exploit this opportunity to introduce a new paradigm to test compositionality that studies the extent to which a model overgeneralizes when confronted with exceptions. We show that seq2attn exhibits such overgeneralization to a larger degree than a standard sequence-to-sequence model",
    "volume": "workshop",
    "checked": true,
    "id": "b39efed2e73357db4691f66935cf62e7b51f30e1",
    "citation_count": 26
  },
  "https://aclanthology.org/W19-4802": {
    "title": "Sentiment Analysis Is Not Solved! Assessing and Probing Sentiment Classification",
    "abstract": "Neural methods for sentiment analysis have led to quantitative improvements over previous approaches, but these advances are not always accompanied with a thorough analysis of the qualitative differences. Therefore, it is not clear what outstanding conceptual challenges for sentiment analysis remain. In this work, we attempt to discover what challenges still prove a problem for sentiment classifiers for English and to provide a challenging dataset. We collect the subset of sentences that an (oracle) ensemble of state-of-the-art sentiment classifiers misclassify and then annotate them for 18 linguistic and paralinguistic phenomena, such as negation, sarcasm, modality, etc. Finally, we provide a case study that demonstrates the usefulness of the dataset to probe the performance of a given sentiment classifier with respect to linguistic phenomena",
    "volume": "workshop",
    "checked": true,
    "id": "abb46f435965fb1020099c9e577343e0542fd187",
    "citation_count": 17
  },
  "https://aclanthology.org/W19-4803": {
    "title": "Second-order Co-occurrence Sensitivity of Skip-Gram with Negative Sampling",
    "abstract": "We simulate first- and second-order context overlap and show that Skip-Gram with Negative Sampling is similar to Singular Value Decomposition in capturing second-order co-occurrence information, while Pointwise Mutual Information is agnostic to it. We support the results with an empirical study finding that the models react differently when provided with additional second-order information. Our findings reveal a basic property of Skip-Gram with Negative Sampling and point towards an explanation of its success on a variety of tasks",
    "volume": "workshop",
    "checked": true,
    "id": "891939c2ca684bf72b8edf8b741ca2d7956bd0f9",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-4804": {
    "title": "Can Neural Networks Understand Monotonicity Reasoning?",
    "abstract": "Monotonicity reasoning is one of the important reasoning skills for any intelligent natural language inference (NLI) model in that it requires the ability to capture the interaction between lexical and syntactic structures. Since no test set has been developed for monotonicity reasoning with wide coverage, it is still unclear whether neural models can perform monotonicity reasoning in a proper way. To investigate this issue, we introduce the Monotonicity Entailment Dataset (MED). Performance by state-of-the-art NLI models on the new test set is substantially worse, under 55%, especially on downward reasoning. In addition, analysis using a monotonicity-driven data augmentation method showed that these models might be limited in their generalization ability in upward and downward reasoning",
    "volume": "workshop",
    "checked": true,
    "id": "61daa30becda503c55217ef19a618d6a26b1dabd",
    "citation_count": 35
  },
  "https://aclanthology.org/W19-4805": {
    "title": "Multi-Granular Text Encoding for Self-Explaining Categorization",
    "abstract": "Self-explaining text categorization requires a classifier to make a prediction along with supporting evidence. A popular type of evidence is sub-sequences extracted from the input text which are sufficient for the classifier to make the prediction. In this work, we define multi-granular ngrams as basic units for explanation, and organize all ngrams into a hierarchical structure, so that shorter ngrams can be reused while computing longer ngrams. We leverage the tree-structured LSTM to learn a context-independent representation for each unit via parameter sharing. Experiments on medical disease classification show that our model is more accurate, efficient and compact than the BiLSTM and CNN baselines. More importantly, our model can extract intuitive multi-granular evidence to support its predictions",
    "volume": "workshop",
    "checked": true,
    "id": "22e987a2767317db5d1e4830c9756dfe6d9dd506",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-4806": {
    "title": "The Meaning of \"Most\" for Visual Question Answering Models",
    "abstract": "The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of \"most\", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system",
    "volume": "workshop",
    "checked": true,
    "id": "3f45e8e9c56e4fc3f132072ee4867a89f927687d",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-4807": {
    "title": "Do Human Rationales Improve Machine Explanations?",
    "abstract": "Work on \"learning with rationales\" shows that humans providing explanations to a machine learning system can improve the system's predictive accuracy. However, this work has not been connected to work in \"explainable AI\" which concerns machines explaining their reasoning to humans. In this work, we show that learning with rationales can also improve the quality of the machine's explanations as evaluated by human judges. Specifically, we present experiments showing that, for CNN-based text classification, explanations generated using \"supervised attention\" are judged superior to explanations generated using normal unsupervised attention",
    "volume": "workshop",
    "checked": true,
    "id": "46bfca498faf4964b693e3cb70e17389075168d2",
    "citation_count": 35
  },
  "https://aclanthology.org/W19-4808": {
    "title": "Analyzing the Structure of Attention in a Transformer Language Model",
    "abstract": "The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads",
    "volume": "workshop",
    "checked": true,
    "id": "a039ea239e37f53a2cb60c68e0a1967994353166",
    "citation_count": 158
  },
  "https://aclanthology.org/W19-4809": {
    "title": "Detecting Political Bias in News Articles Using Headline Attention",
    "abstract": "Language is a powerful tool which can be used to state the facts as well as express our views and perceptions. Most of the times, we find a subtle bias towards or against someone or something. When it comes to politics, media houses and journalists are known to create bias by shrewd means such as misinterpreting reality and distorting viewpoints towards some parties. This misinterpretation on a large scale can lead to the production of biased news and conspiracy theories. Automating bias detection in newspaper articles could be a good challenge for research in NLP. We proposed a headline attention network for this bias detection. Our model has two distinctive characteristics: (i) it has a structure that mirrors a person's way of reading a news article (ii) it has attention mechanism applied on the article based on its headline, enabling it to attend to more critical content to predict bias. As the required datasets were not available, we created a dataset comprising of 1329 news articles collected from various Telugu newspapers and marked them for bias towards a particular political party. The experiments conducted on it demonstrated that our model outperforms various baseline methods by a substantial margin",
    "volume": "workshop",
    "checked": true,
    "id": "ce4876ed845d8e041111d674dbcfdd4b2957a8e6",
    "citation_count": 16
  },
  "https://aclanthology.org/W19-4810": {
    "title": "Testing the Generalization Power of Neural Network Models across NLI Benchmarks",
    "abstract": "Neural network models have been very successful in natural language inference, with the best models reaching 90% accuracy in some benchmarks. However, the success of these models turns out to be largely benchmark specific. We show that models trained on a natural language inference dataset drawn from one benchmark fail to perform well in others, even if the notion of inference assumed in these benchmarks is the same or similar. We train six high performing neural network models on different datasets and show that each one of these has problems of generalizing when we replace the original test set with a test set taken from another corpus designed for the same task. In light of these results, we argue that most of the current neural network models are not able to generalize well in the task of natural language inference. We find that using large pre-trained language models helps with transfer learning when the datasets are similar enough. Our results also highlight that the current NLI datasets do not cover the different nuances of inference extensively enough",
    "volume": "workshop",
    "checked": true,
    "id": "86f00f3619626bf3aa9664b17bcaebc18a4b6531",
    "citation_count": 29
  },
  "https://aclanthology.org/W19-4811": {
    "title": "Character Eyes: Seeing Language through Character-Level Taggers",
    "abstract": "Character-level models have been used extensively in recent years in NLP tasks as both supplements and replacements for closed-vocabulary token-level word representations. In one popular architecture, character-level LSTMs are used to feed token representations into a sequence tagger predicting token-level annotations such as part-of-speech (POS) tags. In this work, we examine the behavior of POS taggers across languages from the perspective of individual hidden units within the character LSTM. We aggregate the behavior of these units into language-level metrics which quantify the challenges that taggers face on languages with different morphological properties, and identify links between synthesis and affixation preference and emergent behavior of the hidden tagger layer. In a comparative experiment, we show how modifying the balance between forward and backward hidden units affects model arrangement and performance in these types of languages",
    "volume": "workshop",
    "checked": true,
    "id": "e07f2c383a34217a9403a4d58a4e7d61e57d9163",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4812": {
    "title": "Faithful Multimodal Explanation for Visual Question Answering",
    "abstract": "AI systems' ability to explain their reasoning is critical to their utility and trustworthiness. Deep neural networks have enabled significant progress on many challenging problems such as visual question answering (VQA). However, most of them are opaque black boxes with limited explanatory capability. This paper presents a novel approach to developing a high-performing VQA system that can elucidate its answers with integrated textual and visual explanations that faithfully reflect important aspects of its underlying reasoning while capturing the style of comprehensible human explanations. Extensive experimental evaluation demonstrates the advantages of this approach compared to competing methods using both automated metrics and human evaluation",
    "volume": "workshop",
    "checked": true,
    "id": "491d0101110fcacfad7c739d5fd807cf8b79de18",
    "citation_count": 45
  },
  "https://aclanthology.org/W19-4813": {
    "title": "Evaluating Recurrent Neural Network Explanations",
    "abstract": "Recently, several methods have been proposed to explain the predictions of recurrent neural networks (RNNs), in particular of LSTMs. The goal of these methods is to understand the network's decisions by assigning to each input variable, e.g., a word, a relevance indicating to which extent it contributed to a particular prediction. In previous works, some of these methods were not yet compared to one another, or were evaluated only qualitatively. We close this gap by systematically and quantitatively comparing these methods in different settings, namely (1) a toy arithmetic task which we use as a sanity check, (2) a five-class sentiment prediction of movie reviews, and besides (3) we explore the usefulness of word relevances to build sentence-level representations. Lastly, using the method that performed best in our experiments, we show how specific linguistic phenomena such as the negation in sentiment analysis reflect in terms of relevance patterns, and how the relevance visualization can help to understand the misclassification of individual samples",
    "volume": "workshop",
    "checked": true,
    "id": "9e01ef82b54fbef69a0704f1b11882b28c69762f",
    "citation_count": 57
  },
  "https://aclanthology.org/W19-4814": {
    "title": "On the Realization of Compositionality in Neural Networks",
    "abstract": "We present a detailed comparison of two types of sequence to sequence models trained to conduct a compositional task. The models are architecturally identical at inference time, but differ in the way that they are trained: our baseline model is trained with a task-success signal only, while the other model receives additional supervision on its attention mechanism (Attentive Guidance), which has shown to be an effective method for encouraging more compositional solutions. We first confirm that the models with attentive guidance indeed infer more compositional solutions than the baseline, by training them on the lookup table task presented by Liska et al. (2019). We then do an in-depth analysis of the structural differences between the two model types, focusing in particular on the organisation of the parameter space and the hidden layer activations and find noticeable differences in both these aspects. Guided networks focus more on the components of the input rather than the sequence as a whole and develop small functional groups of neurons with specific purposes that use their gates more selectively. Results from parameter heat maps, component swapping and graph analysis also indicate that guided networks exhibit a more modular structure with a small number of specialized, strongly connected neurons",
    "volume": "workshop",
    "checked": true,
    "id": "02ccf39600b495932af1ae18d95110be07a1ed2e",
    "citation_count": 15
  },
  "https://aclanthology.org/W19-4815": {
    "title": "Learning the Dyck Language with Attention-based Seq2Seq Models",
    "abstract": "The generalized Dyck language has been used to analyze the ability of Recurrent Neural Networks (RNNs) to learn context-free grammars (CFGs). Recent studies draw conflicting conclusions on their performance, especially regarding the generalizability of the models with respect to the depth of recursion. In this paper, we revisit several common models and experimental settings, discuss the potential problems of the tasks and analyses. Furthermore, we explore the use of attention mechanisms within the seq2seq framework to learn the Dyck language, which could compensate for the limited encoding ability of RNNs. Our findings reveal that attention mechanisms still cannot truly generalize over the recursion depth, although they perform much better than other models on the closing bracket tagging task. Moreover, this also suggests that this commonly used task is not sufficient to test a model's understanding of CFGs",
    "volume": "workshop",
    "checked": true,
    "id": "50f623677c84ad8f96edac6ffea66ef6e86db298",
    "citation_count": 21
  },
  "https://aclanthology.org/W19-4816": {
    "title": "Modeling Paths for Explainable Knowledge Base Completion",
    "abstract": "A common approach in knowledge base completion (KBC) is to learn representations for entities and relations in order to infer missing facts by generalizing existing ones. A shortcoming of standard models is that they do not explain their predictions to make them verifiable easily to human inspection. In this paper, we propose the Context Path Model (CPM) which generates explanations for new facts in KBC by providing sets of context paths as supporting evidence for these triples. For example, a new triple (Theresa May, nationality, Britain) may be explained by the path (Theresa May, born in, Eastbourne, contained in, Britain). The CPM is formulated as a wrapper that can be applied on top of various existing KBC models. We evaluate it for the well-established TransE model. We observe that its performance remains very close despite the added complexity, and that most of the paths proposed as explanations provide meaningful evidence to assess the correctness",
    "volume": "workshop",
    "checked": true,
    "id": "6666235c98c0926322ba501592fed30a585e6603",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-4817": {
    "title": "Probing Word and Sentence Embeddings for Long-distance Dependencies Effects in French and English",
    "abstract": "The recent wide-spread and strong interest in RNNs has spurred detailed investigations of the distributed representations they generate and specifically if they exhibit properties similar to those characterising human languages. Results are at present inconclusive. In this paper, we extend previous work on long-distance dependencies in three ways. We manipulate word embeddings to translate them in a space that is attuned to the linguistic properties under study. We extend the work to sentence embeddings and to new languages. We confirm previous negative results: word embeddings and sentence embeddings do not unequivocally encode fine-grained linguistic properties of long-distance dependencies",
    "volume": "workshop",
    "checked": true,
    "id": "55493352fd7ad2af6902b036e0de23d384f1f09b",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4818": {
    "title": "Derivational Morphological Relations in Word Embeddings",
    "abstract": "Derivation is a type of a word-formation process which creates new words from existing ones by adding, changing or deleting affixes. In this paper, we explore the potential of word embeddings to identify properties of word derivations in the morphologically rich Czech language. We extract derivational relations between pairs of words from DeriNet, a Czech lexical network, which organizes almost one million Czech lemmas into derivational trees. For each such pair, we compute the difference of the embeddings of the two words, and perform unsupervised clustering of the resulting vectors. Our results show that these clusters largely match manually annotated semantic categories of the derivational relations (e.g. the relation 'bake–baker' belongs to category 'actor', and a correct clustering puts it into the same cluster as 'govern–governor')",
    "volume": "workshop",
    "checked": true,
    "id": "7b3b847916d3c5c42146d41652628a9028369a8d",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-4819": {
    "title": "Hierarchical Representation in Neural Language Models: Suppression and Recovery of Expectations",
    "abstract": "Work using artificial languages as training input has shown that LSTMs are capable of inducing the stack-like data structures required to represent context-free and certain mildly context-sensitive languages — formal language classes which correspond in theory to the hierarchical structures of natural language. Here we present a suite of experiments probing whether neural language models trained on linguistic data induce these stack-like data structures and deploy them while incrementally predicting words. We study two natural language phenomena: center embedding sentences and syntactic island constraints on the filler–gap dependency. In order to properly predict words in these structures, a model must be able to temporarily suppress certain expectations and then recover those expectations later, essentially pushing and popping these expectations on a stack. Our results provide evidence that models can successfully suppress and recover expectations in many cases, but do not fully recover their previous grammatical state",
    "volume": "workshop",
    "checked": true,
    "id": "0aad5ddb6a43f84ac68fd9ecee5b1f434344ea50",
    "citation_count": 20
  },
  "https://aclanthology.org/W19-4820": {
    "title": "Blackbox Meets Blackbox: Representational Similarity & Stability Analysis of Neural Language Models and Brains",
    "abstract": "In this paper, we define and apply representational stability analysis (ReStA), an intuitive way of analyzing neural language models. ReStA is a variant of the popular representational similarity analysis (RSA) in cognitive neuroscience. While RSA can be used to compare representations in models, model components, and human brains, ReStA compares instances of the same model, while systematically varying single model parameter. Using ReStA, we study four recent and successful neural language models, and evaluate how sensitive their internal representations are to the amount of prior context. Using RSA, we perform a systematic study of how similar the representational spaces in the first and second (or higher) layers of these models are to each other and to patterns of activation in the human brain. Our results reveal surprisingly strong differences between language models, and give insights into where the deep linguistic processing, that integrates information over multiple sentences, is happening in these models. The combination of ReStA and RSA on models and brains allows us to start addressing the important question of what kind of linguistic processes we can hope to observe in fMRI brain imaging data. In particular, our results suggest that the data on story reading from Wehbe et al./ (2014) contains a signal of shallow linguistic processing, but show no evidence on the more interesting deep linguistic processing",
    "volume": "workshop",
    "checked": true,
    "id": "45c0654f173f870165a880d2b63bae54ae4b4982",
    "citation_count": 46
  },
  "https://aclanthology.org/W19-4821": {
    "title": "An LSTM Adaptation Study of (Un)grammaticality",
    "abstract": "We propose a novel approach to the study of how artificial neural network perceive the distinction between grammatical and ungrammatical sentences, a crucial task in the growing field of synthetic linguistics. The method is based on performance measures of language models trained on corpora and fine-tuned with either grammatical or ungrammatical sentences, then applied to (different types of) grammatical or ungrammatical sentences. The results show that both in the difficult and highly symmetrical task of detecting subject islands and in the more open CoLA dataset, grammatical sentences give rise to better scores than ungrammatical ones, possibly because they can be better integrated within the body of linguistic structural knowledge that the language model has accumulated",
    "volume": "workshop",
    "checked": true,
    "id": "c32d55d303bf52f4aa392e8defb1553e3e7e749c",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-4822": {
    "title": "An Analysis of Source-Side Grammatical Errors in NMT",
    "abstract": "The quality of Neural Machine Translation (NMT) has been shown to significantly degrade when confronted with source-side noise. We present the first large-scale study of state-of-the-art English-to-German NMT on real grammatical noise, by evaluating on several Grammar Correction corpora. We present methods for evaluating NMT robustness without true references, and we use them for extensive analysis of the effects that different grammatical errors have on the NMT output. We also introduce a technique for visualizing the divergence distribution caused by a source-side error, which allows for additional insights",
    "volume": "workshop",
    "checked": true,
    "id": "2577912bc87a9d749c634b4451274a3537b72bb0",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-4823": {
    "title": "Finding Hierarchical Structure in Neural Stacks Using Unsupervised Parsing",
    "abstract": "Neural network architectures have been augmented with differentiable stacks in order to introduce a bias toward learning hierarchy-sensitive regularities. It has, however, proven difficult to assess the degree to which such a bias is effective, as the operation of the differentiable stack is not always interpretable. In this paper, we attempt to detect the presence of latent representations of hierarchical structure through an exploration of the unsupervised learning of constituency structure. Using a technique due to Shen et al. (2018a,b), we extract syntactic trees from the pushing behavior of stack RNNs trained on language modeling and classification objectives. We find that our models produce parses that reflect natural language syntactic constituencies, demonstrating that stack RNNs do indeed infer linguistically relevant hierarchical structure",
    "volume": "workshop",
    "checked": true,
    "id": "1fc14493677e2f496c8c871f1542f3271184c6ae",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-4825": {
    "title": "Open Sesame: Getting inside BERT's Linguistic Knowledge",
    "abstract": "How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT's representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT's representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT's representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora",
    "volume": "workshop",
    "checked": true,
    "id": "165d51a547cd920e6ac55660ad5c404dcb9562ed",
    "citation_count": 179
  },
  "https://aclanthology.org/W19-4826": {
    "title": "GEval: Tool for Debugging NLP Datasets and Models",
    "abstract": "This paper presents a simple but general and effective method to debug the output of machine learning (ML) supervised models, including neural networks. The algorithm looks for features that lower the evaluation metric in such a way that it cannot be ascribed to chance (as measured by their p-values). Using this method – implemented as MLEval tool – you can find: (1) anomalies in test sets, (2) issues in preprocessing, (3) problems in the ML model itself. It can give you an insight into what can be improved in the datasets and/or the model. The same method can be used to compare ML models or different versions of the same model. We present the tool, the theory behind it and use cases for text-based models of various types",
    "volume": "workshop",
    "checked": true,
    "id": "a8f463b648e602289b8880ee533f1b0ff6b33c21",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-4827": {
    "title": "From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions",
    "abstract": "We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences. We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall",
    "volume": "workshop",
    "checked": true,
    "id": "7886bf8d86a8ae22aa0fcf8a77d2c8a4d9429aa1",
    "citation_count": 35
  },
  "https://aclanthology.org/W19-4828": {
    "title": "What Does BERT Look at? An Analysis of BERT's Attention",
    "abstract": "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention",
    "volume": "workshop",
    "checked": true,
    "id": "95a251513853c6032bdecebd4b74e15795662986",
    "citation_count": 854
  },
  "https://aclanthology.org/W19-5001": {
    "title": "Classifying the reported ability in clinical mobility descriptions",
    "abstract": "Assessing how individuals perform different activities is key information for modeling health states of individuals and populations. Descriptions of activity performance in clinical free text are complex, including syntactic negation and similarities to textual entailment tasks. We explore a variety of methods for the novel task of classifying four types of assertions about activity performance: Able, Unable, Unclear, and None (no information). We find that ensembling an SVM trained with lexical features and a CNN achieves 77.9% macro F1 score on our task, and yields nearly 80% recall on the rare Unclear and Unable samples. Finally, we highlight several challenges in classifying performance assertions, including capturing information about sources of assistance, incorporating syntactic structure and negation scope, and handling new modalities at test time. Our findings establish a strong baseline for this novel task, and identify intriguing areas for further research",
    "volume": "workshop",
    "checked": true,
    "id": "aac4d2cec1170c09f3577a31a27d9dee6b520c63",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-5002": {
    "title": "Learning from the Experience of Doctors: Automated Diagnosis of Appendicitis Based on Clinical Notes",
    "abstract": "The objective of this work is to develop an automated diagnosis system that is able to predict the probability of appendicitis given a free-text emergency department (ED) note and additional structured information (e.g., lab test results). Our clinical corpus consists of about 180,000 ED notes based on ten years of patient visits to the Accident and Emergency (A&E) Department of the National University Hospital (NUH), Singapore. We propose a novel neural network approach that learns to diagnose acute appendicitis based on doctors' free-text ED notes without any feature engineering. On a test set of 2,000 ED notes with equal number of appendicitis (positive) and non-appendicitis (negative) diagnosis and in which all the negative ED notes only consist of abdominal-related diagnosis, our model is able to achieve a promising F_0.5-score of 0.895 while ED doctors achieve F_0.5-score of 0.900. Visualization shows that our model is able to learn important features, signs, and symptoms of patients from unstructured free-text ED notes, which will help doctors to make better diagnosis",
    "volume": "workshop",
    "checked": true,
    "id": "e022f7e09771cd1eccd9b2d83aa2f621e01cbe62",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5003": {
    "title": "A Paraphrase Generation System for EHR Question Answering",
    "abstract": "This paper proposes a dataset and method for automatically generating paraphrases for clinical questions relating to patient-specific information in electronic health records (EHRs). Crowdsourcing is used to collect 10,578 unique questions across 946 semantically distinct paraphrase clusters. This corpus is then used with a deep learning-based question paraphrasing method utilizing variational autoencoder and LSTM encoder/decoder. The ultimate use of such a method is to improve the performance of automatic question answering methods for EHRs",
    "volume": "workshop",
    "checked": true,
    "id": "93f897f3d4a8fc547f127936dd65b2c0824a0f97",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-5004": {
    "title": "REflex: Flexible Framework for Relation Extraction in Multiple Domains",
    "abstract": "Systematic comparison of methods for relation extraction (RE) is difficult because many experiments in the field are not described precisely enough to be completely reproducible and many papers fail to report ablation studies that would highlight the relative contributions of their various combined techniques. In this work, we build a unifying framework for RE, applying this on three highly used datasets (from the general, biomedical and clinical domains) with the ability to be extendable to new datasets. By performing a systematic exploration of modeling, pre-processing and training methodologies, we find that choices of preprocessing are a large contributor performance and that omission of such information can further hinder fair comparison. Other insights from our exploration allow us to provide recommendations for future research in this area",
    "volume": "workshop",
    "checked": true,
    "id": "529c59c6b4aeb514412e2e76518afeb14893c1e1",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-5005": {
    "title": "Analysing Representations of Memory Impairment in a Clinical Notes Classification Model",
    "abstract": "Despite recent advances in the application of deep neural networks to various kinds of medical data, extracting information from unstructured textual sources remains a challenging task. The challenges of training and interpreting document classification models are amplified when dealing with small and highly technical datasets, as are common in the clinical domain. Using a dataset of de-identified clinical letters gathered at a memory clinic, we construct several recurrent neural network models for letter classification, and evaluate them on their ability to build meaningful representations of the documents and predict patients' diagnoses. Additionally, we probe sentence embedding models in order to build a human-interpretable representation of the neural network's features, using a simple and intuitive technique based on perturbative approaches to sentence importance. In addition to showing which sentences in a document are most informative about the patient's condition, this method reveals the types of sentences that lead the model to make incorrect diagnoses. Furthermore, we identify clusters of sentences in the embedding space that correlate strongly with importance scores for each clinical diagnosis class",
    "volume": "workshop",
    "checked": true,
    "id": "993d091ee25c1e9dede22220d11ba87d154be49c",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5006": {
    "title": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets",
    "abstract": "Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at https://github.com/ ncbi-nlp/BLUE_Benchmark",
    "volume": "workshop",
    "checked": true,
    "id": "347bac45298f37cd83c3e79d99b826dc65a70c46",
    "citation_count": 404
  },
  "https://aclanthology.org/W19-5007": {
    "title": "Combining Structured and Free-text Electronic Medical Record Data for Real-time Clinical Decision Support",
    "abstract": "The goal of this work is to utilize Electronic Medical Record (EMR) data for real-time Clinical Decision Support (CDS). We present a deep learning approach to combining in real time available diagnosis codes (ICD codes) and free-text notes: Patient Context Vectors. Patient Context Vectors are created by averaging ICD code embeddings, and by predicting the same from free-text notes via a Convolutional Neural Network. The Patient Context Vectors were then simply appended to available structured data (vital signs and lab results) to build prediction models for a specific condition. Experiments on predicting ARDS, a rare and complex condition, demonstrate the utility of Patient Context Vectors as a means of summarizing the patient history and overall condition, and improve significantly the prediction model results",
    "volume": "workshop",
    "checked": true,
    "id": "be0748064e6d97d1860ae72d1cd8ff51e187c748",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5008": {
    "title": "MoNERo: a Biomedical Gold Standard Corpus for the Romanian Language",
    "abstract": "In an era when large amounts of data are generated daily in various fields, the biomedical field among others, linguistic resources can be exploited for various tasks of Natural Language Processing. Moreover, increasing number of biomedical documents are available in languages other than English. To be able to extract information from natural language free text resources, methods and tools are needed for a variety of languages. This paper presents the creation of the MoNERo corpus, a gold standard biomedical corpus for Romanian, annotated with both part of speech tags and named entities. MoNERo comprises 154,825 morphologically annotated tokens and 23,188 entity annotations belonging to four entity semantic groups corresponding to UMLS Semantic Groups",
    "volume": "workshop",
    "checked": true,
    "id": "417012b6c38f377fbd3dac56c4993c964817768e",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-5009": {
    "title": "Domain Adaptation of SRL Systems for Biological Processes",
    "abstract": "Domain adaptation remains one of the most challenging aspects in the wide-spread use of Semantic Role Labeling (SRL) systems. Current state-of-the-art methods are typically trained on large-scale datasets, but their performances do not directly transfer to low-resource domain-specific settings. In this paper, we propose two approaches for domain adaptation in the biological domain that involves pre-training LSTM-CRF based on existing large-scale datasets and adapting it for a low-resource corpus of biological processes. Our first approach defines a mapping between the source labels and the target labels, and the other approach modifies the final CRF layer in sequence-labeling neural network architecture. We perform our experiments on ProcessBank dataset which contains less than 200 paragraphs on biological processes. We improve over the previous state-of-the-art system on this dataset by 21 F1 points. We also show that, by incorporating event-event relationship in ProcessBank, we are able to achieve an additional 2.6 F1 gain, giving us possible insights into how to improve SRL systems for biological process using richer annotations",
    "volume": "workshop",
    "checked": true,
    "id": "7621bfe36cc649a5876cea587366201e158a8b38",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5010": {
    "title": "Deep Contextualized Biomedical Abbreviation Expansion",
    "abstract": "Automatic identification and expansion of ambiguous abbreviations are essential for biomedical natural language processing applications, such as information retrieval and question answering systems. In this paper, we present DEep Contextualized Biomedical Abbreviation Expansion (DECBAE) model. DECBAE automatically collects substantial and relatively clean annotated contexts for 950 ambiguous abbreviations from PubMed abstracts using a simple heuristic. Then it utilizes BioELMo to extract the contextualized features of words, and feed those features to abbreviation-specific bidirectional LSTMs, where the hidden states of the ambiguous abbreviations are used to assign the exact definitions. Our DECBAE model outperforms other baselines by large margins, achieving average accuracy of 0.961 and macro-F1 of 0.917 on the dataset. It also surpasses human performance for expanding a sample abbreviation, and remains robust in imbalanced, low-resources and clinical settings",
    "volume": "workshop",
    "checked": true,
    "id": "26968e221bf98bf32f4309fa257eca9f5c67ddac",
    "citation_count": 26
  },
  "https://aclanthology.org/W19-5011": {
    "title": "RNN Embeddings for Identifying Difficult to Understand Medical Words",
    "abstract": "Patients and their families often require a better understanding of medical information provided by doctors. We currently address this issue by improving the identification of difficult to understand medical words. We introduce novel embeddings received from RNN - FrnnMUTE (French RNN Medical Understandability Text Embeddings) which allow to reach up to 87.0 F1 score in identification of difficult words. We also note that adding pre-trained FastText word embeddings to the feature set substantially improves the performance of the model which classifies words according to their difficulty. We study the generalizability of different models through three cross-validation scenarios which allow testing classifiers in real-world conditions: understanding of medical words by new users, and classification of new unseen words by the automatic models. The RNN - FrnnMUTE embeddings and the categorization code are being made available for the research",
    "volume": "workshop",
    "checked": true,
    "id": "7aa524a01d59ca1b9693f6c292fa91a2cfb3ee20",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5012": {
    "title": "A distantly supervised dataset for automated data extraction from diagnostic studies",
    "abstract": "Systematic reviews are important in evidence based medicine, but are expensive to produce. Automating or semi-automating the data extraction of index test, target condition, and reference standard from articles has the potential to decrease the cost of conducting systematic reviews of diagnostic test accuracy, but relevant training data is not available. We create a distantly supervised dataset of approximately 90,000 sentences, and let two experts manually annotate a small subset of around 1,000 sentences for evaluation. We evaluate the performance of BioBERT and logistic regression for ranking the sentences, and compare the performance for distant and direct supervision. Our results suggest that distant supervision can work as well as, or better than direct supervision on this problem, and that distantly trained models can perform as well as, or better than human annotators",
    "volume": "workshop",
    "checked": true,
    "id": "6f888fe0aed7d24bd25ac18a433a629c63f18b6b",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-5013": {
    "title": "Query selection methods for automated corpora construction with a use case in food-drug interactions",
    "abstract": "In this paper, we address the problem of automatically constructing a relevant corpus of scientific articles about food-drug interactions. There is a growing number of scientific publications that describe food-drug interactions but currently building a high-coverage corpus that can be used for information extraction purposes is not trivial. We investigate several methods for automating the query selection process using an expert-curated corpus of food-drug interactions. Our experiments show that index term features along with a decision tree classifier are the best approach for this task and that feature selection approaches and in particular gain ratio outperform frequency-based methods for query selection",
    "volume": "workshop",
    "checked": true,
    "id": "cbe7e9fddc3febda461f9a823f8db158cd7a00bc",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5014": {
    "title": "Enhancing biomedical word embeddings by retrofitting to verb clusters",
    "abstract": "Verbs play a fundamental role in many biomed-ical tasks and applications such as relation and event extraction. We hypothesize that performance on many downstream tasks can be improved by aligning the input pretrained embeddings according to semantic verb classes.In this work, we show that by using semantic clusters for verbs, a large lexicon of verbclasses derived from biomedical literature, weare able to improve the performance of common pretrained embeddings in downstream tasks by retrofitting them to verb classes. We present a simple and computationally efficient approach using a widely-available \"off-the-shelf\" retrofitting algorithm to align pretrained embeddings according to semantic verb clusters. We achieve state-of-the-art results on text classification and relation extraction tasks",
    "volume": "workshop",
    "checked": true,
    "id": "bd48080ad51a126a3df6ead28173d10fad01ad5d",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-5015": {
    "title": "A Comparison of Word-based and Context-based Representations for Classification Problems in Health Informatics",
    "abstract": "Distributed representations of text can be used as features when training a statistical classifier. These representations may be created as a composition of word vectors or as context-based sentence vectors. We compare the two kinds of representations (word versus context) for three classification problems: influenza infection classification, drug usage classification and personal health mention classification. For statistical classifiers trained for each of these problems, context-based representations based on ELMo, Universal Sentence Encoder, Neural-Net Language Model and FLAIR are better than Word2Vec, GloVe and the two adapted using the MESH ontology. There is an improvement of 2-4% in the accuracy when these context-based representations are used instead of word-based representations",
    "volume": "workshop",
    "checked": true,
    "id": "8ffc75d9f2e1e3c2f77f37822b89cf3a5fcd5ff2",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-5016": {
    "title": "Constructing large scale biomedical knowledge bases from scratch with rapid annotation of interpretable patterns",
    "abstract": "Knowledge base construction is crucial for summarising, understanding and inferring relationships between biomedical entities. However, for many practical applications such as drug discovery, the scarcity of relevant facts (e.g. gene X is therapeutic target for disease Y) severely limits a domain expert's ability to create a usable knowledge base, either directly or by training a relation extraction model. In this paper, we present a simple and effective method of extracting new facts with a pre-specified binary relationship type from the biomedical literature, without requiring any training data or hand-crafted rules. Our system discovers, ranks and presents the most salient patterns to domain experts in an interpretable form. By marking patterns as compatible with the desired relationship type, experts indirectly batch-annotate candidate pairs whose relationship is expressed with such patterns in the literature. Even with a complete absence of seed data, experts are able to discover thousands of high-quality pairs with the desired relationship within minutes. When a small number of relevant pairs do exist - even when their relationship is more general (e.g. gene X is biologically associated with disease Y) than the relationship of interest - our system leverages them in order to i) learn a better ranking of the patterns to be annotated or ii) generate weakly labelled pairs in a fully automated manner. We evaluate our method both intrinsically and via a downstream knowledge base completion task, and show that it is an effective way of constructing knowledge bases when few or no relevant facts are already available",
    "volume": "workshop",
    "checked": true,
    "id": "8ca5fe0c1d54ff5f64899d056b76fa4b5344db6f",
    "citation_count": 19
  },
  "https://aclanthology.org/W19-5017": {
    "title": "First Steps towards Building a Medical Lexicon for Spanish with Linguistic and Semantic Information",
    "abstract": "We report the work-in-progress of collecting MedLexSp, an unified medical lexicon for the Spanish language, featuring terms and inflected word forms mapped to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs), semantic types and groups. First, we leveraged a list of term lemmas and forms from a previous project, and mapped them to UMLS terms and CUIs. To enrich the lexicon, we used both domain-corpora (e.g. Summaries of Product Characteristics and MedlinePlus) and natural language processing techniques such as string distance methods or generation of syntactic variants of multi-word terms. We also added term variants by mapping their CUIs to missing items available in the Spanish versions of standard thesauri (e.g. Medical Subject Headings and World Health Organization Adverse Drug Reactions terminology). We enhanced the vocabulary coverage by gathering missing terms from resources such as the Anatomical Therapeutical Classification, the National Cancer Institute (NCI) Dictionary of Cancer Terms, OrphaData, or the Nomenclátor de Prescripción for drug names. Part-of-Speech information is being included in the lexicon, and the current version amounts up to 76 454 lemmas and 203 043 inflected forms (including conjugated verbs, number and gender variants), corresponding to 30 647 UMLS CUIs. MedLexSp is distributed freely for research purposes",
    "volume": "workshop",
    "checked": true,
    "id": "81603689a3b291be5672cf8e80c6314707147df4",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5018": {
    "title": "Incorporating Figure Captions and Descriptive Text in MeSH Term Indexing",
    "abstract": "The goal of text classification is to automatically assign categories to documents. Deep learning automatically learns effective features from data instead of adopting human-designed features. In this paper, we focus specifically on biomedical document classification using a deep learning approach. We present a novel multichannel TextCNN model for MeSH term indexing. Beyond the normal use of the text from the abstract and title for model training, we also consider figure and table captions, as well as paragraphs associated with the figures and tables. We demonstrate that these latter text sources are important feature sources for our method. A new dataset consisting of these text segments curated from 257,590 full text articles together with the articles' MEDLINE/PubMed MeSH terms is publicly available",
    "volume": "workshop",
    "checked": true,
    "id": "7bcab12a4743511121ad2bba790cf01a56a1505f",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5019": {
    "title": "BioRelEx 1.0: Biological Relation Extraction Benchmark",
    "abstract": "Automatic extraction of relations and interactions between biological entities from scientific literature remains an extremely challenging problem in biomedical information extraction and natural language processing in general. One of the reasons for slow progress is the relative scarcity of standardized and publicly available benchmarks. In this paper we introduce BioRelEx, a new dataset of fully annotated sentences from biomedical literature that capture binding interactions between proteins and/or biomolecules. To foster reproducible research on the interaction extraction task, we define a precise and transparent evaluation process, tools for error analysis and significance tests. Finally, we conduct extensive experiments to evaluate several baselines, including SciIE, a recently introduced neural multi-task architecture that has demonstrated state-of-the-art performance on several tasks",
    "volume": "workshop",
    "checked": true,
    "id": "9d05dde42df429a6e09310bc5fe65433bff90261",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-5020": {
    "title": "Extraction of Lactation Frames from Drug Labels and LactMed",
    "abstract": "This paper describes a natural language processing (NLP) approach to extracting lactation-specific drug information from two sources: FDA-mandated drug labels and the NLM Drugs and Lactation Database (LactMed). A frame semantic approach is utilized, and the paper describes the selected frames, their annotation on a set of 900 sections from drug labels and LactMed articles, and the NLP system to extract such frame instances automatically. The ultimate goal of the project is to use such a system to identify discrepancies in lactation-related drug information between these resources",
    "volume": "workshop",
    "checked": true,
    "id": "40de9f9c1f23d73c6da86e06458ca34c553c354b",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-5021": {
    "title": "Annotating Temporal Information in Clinical Notes for Timeline Reconstruction: Towards the Definition of Calendar Expressions",
    "abstract": "To automatically analyse complex trajectory information enclosed in clinical text (e.g. timing of symptoms, duration of treatment), it is important to understand the related temporal aspects, anchoring each event on an absolute point in time. In the clinical domain, few temporally annotated corpora are currently available. Moreover, underlying annotation schemas - which mainly rely on the TimeML standard - are not necessarily easily applicable for applications such as patient timeline reconstruction. In this work, we investigated how temporal information is documented in clinical text by annotating a corpus of medical reports with time expressions (TIMEXes), based on TimeML. The developed corpus is available to the NLP community. Starting from our annotations, we analysed the suitability of the TimeML TIMEX schema for capturing timeline information, identifying challenges and possible solutions. As a result, we propose a novel annotation schema that could be useful for timeline reconstruction: CALendar EXpression (CALEX)",
    "volume": "workshop",
    "checked": true,
    "id": "fa151347085ba5a22077609d5f96cb3088a5ff34",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-5022": {
    "title": "Leveraging Sublanguage Features for the Semantic Categorization of Clinical Terms",
    "abstract": "The automatic processing of clinical documents, such as Electronic Health Records (EHRs), could benefit substantially from the enrichment of medical terminologies with terms encountered in clinical practice. To integrate such terms into existing knowledge sources, they must be linked to corresponding concepts. We present a method for the semantic categorization of clinical terms based on their surface form. We find that features based on sublanguage properties can provide valuable cues for the classification of term variants",
    "volume": "workshop",
    "checked": true,
    "id": "6d4bf1cb4116884b8e8aadd2b59a018bc6a0e6b2",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5023": {
    "title": "Enhancing PIO Element Detection in Medical Text Using Contextualized Embedding",
    "abstract": "In this paper, we presented an improved methodology to extract PIO elements, from abstracts of medical papers, that reduces ambiguity. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. In addition, we investigated a contextualized embedding, BioBERT, trained on medical corpora. It has been found that using the BioBERT embedding improved the classification accuracy, outperforming the BERT-based model. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context.Furthermore, to enhance the accuracy of the model, we have investigated an ensemble method based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to learn a linear combination of the predicted probabilities for the 3 classes with the TF-IDF score and the QIEF that optimizes the classification. The results indicate that these text features were good features to consider in order to boost the deeply contextualized classification model. We compared the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the highest score in terms of AUC when we combine the base learners.The present work resulted in the creation of a PIO element dataset, PICONET, and a classification tool. These constitute and important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated",
    "volume": "workshop",
    "checked": true,
    "id": "8f22b82ff8a053a1ebf6797f052e46c884bf9ae0",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-5024": {
    "title": "Contributions to Clinical Named Entity Recognition in Portuguese",
    "abstract": "Having in mind that different languages might present different challenges, this paper presents the following contributions to the area of Information Extraction from clinical text, targeting the Portuguese language: a collection of 281 clinical texts in this language, with manually-annotated named entities; word embeddings trained in a larger collection of similar texts; results of using BiLSTM-CRF neural networks for named entity recognition on the annotated collection, including a comparison of using in-domain or out-of-domain word embeddings in this task. Although learned with much less data, performance is higher when using in-domain embeddings. When tested in 20 independent clinical texts, this model achieved better results than a model using larger out-of-domain embeddings",
    "volume": "workshop",
    "checked": true,
    "id": "59c033854428436604a742201d61ce7290ce1864",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-5025": {
    "title": "Can Character Embeddings Improve Cause-of-Death Classification for Verbal Autopsy Narratives?",
    "abstract": "We present two models for combining word and character embeddings for cause-of-death classification of verbal autopsy reports using the text of the narratives. We find that for smaller datasets (500 to 1000 records), adding character information to the model improves classification, making character-based CNNs a promising method for automated verbal autopsy coding",
    "volume": "workshop",
    "checked": true,
    "id": "bd554af6fcd0b6d045cee7d2514e58f6d0305465",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5026": {
    "title": "Is artificial data useful for biomedical Natural Language Processing algorithms?",
    "abstract": "A major obstacle to the development of Natural Language Processing (NLP) methods in the biomedical domain is data accessibility. This problem can be addressed by generating medical data artificially. Most previous studies have focused on the generation of short clinical text, and evaluation of the data utility has been limited. We propose a generic methodology to guide the generation of clinical text with key phrases. We use the artificial data as additional training data in two key biomedical NLP tasks: text classification and temporal relation extraction. We show that artificially generated training data used in conjunction with real training data can lead to performance boosts for data-greedy neural network algorithms. We also demonstrate the usefulness of the generated data for NLP setups where it fully replaces real training data",
    "volume": "workshop",
    "checked": true,
    "id": "3b2ae2a1de566788a6c4af2b2fffa9431a4eb91d",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5027": {
    "title": "ChiMed: A Chinese Medical Corpus for Question Answering",
    "abstract": "Question answering (QA) is a challenging task in natural language processing (NLP), especially when it is applied to specific domains. While models trained in the general domain can be adapted to a new target domain, their performance often degrades significantly due to domain mismatch. Alternatively, one can require a large amount of domain-specific QA data, but such data are rare, especially for the medical domain. In this study, we first collect a large-scale Chinese medical QA corpus called ChiMed; second we annotate a small fraction of the corpus to check the quality of the answers; third, we extract two datasets from the corpus and use them for the relevancy prediction task and the adoption prediction task. Several benchmark models are applied to the datasets, producing good results for both tasks",
    "volume": "workshop",
    "checked": true,
    "id": "184d7956905050a9e5ed4ee65b353643d9abcf82",
    "citation_count": 17
  },
  "https://aclanthology.org/W19-5028": {
    "title": "Clinical Concept Extraction for Document-Level Coding",
    "abstract": "The text of clinical notes can be a valuable source of patient information and clinical assessments. Historically, the primary approach for exploiting clinical notes has been information extraction: linking spans of text to concepts in a detailed domain ontology. However, recent work has demonstrated the potential of supervised machine learning to extract document-level codes directly from the raw text of clinical notes. We propose to bridge the gap between the two approaches with two novel syntheses: (1) treating extracted concepts as features, which are used to supplement or replace the text of the note; (2) treating extracted concepts as labels, which are used to learn a better representation of the text. Unfortunately, the resulting concepts do not yield performance gains on the document-level clinical coding task. We explore possible explanations and future research directions",
    "volume": "workshop",
    "checked": true,
    "id": "34b1cd497c3150905f7439edf40462e789e3687a",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-5029": {
    "title": "Clinical Case Reports for NLP",
    "abstract": "Textual data are useful for accessing expert information. Yet, since the texts are representative of distinct language uses, it is necessary to build specific corpora in order to be able to design suitable NLP tools. In some domains, such as medical domain, it may be complicated to access the representative textual data and their semantic annotations, while there exists a real need for providing efficient tools and methods. Our paper presents a corpus of clinical cases written in French, and their semantic annotations. Thus, we manually annotated a set of 717 files into four general categories (age, gender, outcome, and origin) for a total number of 2,835 annotations. The values of age, gender, and outcome are normalized. A subset with 70 files has been additionally manually annotated into 27 categories for a total number of 5,198 annotations",
    "volume": "workshop",
    "checked": true,
    "id": "dfe434aa2582c3b7bf9b8b497122dfcff79d6dd6",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-5030": {
    "title": "Two-stage Federated Phenotyping and Patient Representation Learning",
    "abstract": "A large percentage of medical information is in unstructured text format in electronic medical record systems. Manual extraction of information from clinical notes is extremely time consuming. Natural language processing has been widely used in recent years for automatic information extraction from medical texts. However, algorithms trained on data from a single healthcare provider are not generalizable and error-prone due to the heterogeneity and uniqueness of medical documents. We develop a two-stage federated natural language processing method that enables utilization of clinical notes from different hospitals or clinics without moving the data, and demonstrate its performance using obesity and comorbities phenotyping as medical task. This approach not only improves the quality of a specific clinical task but also facilitates knowledge progression in the whole healthcare system, which is an essential part of learning health system. To the best of our knowledge, this is the first application of federated machine learning in clinical NLP",
    "volume": "workshop",
    "checked": true,
    "id": "db733b820b3fdbf88edccebd28b6db0898fd5517",
    "citation_count": 41
  },
  "https://aclanthology.org/W19-5031": {
    "title": "Transfer Learning for Causal Sentence Detection",
    "abstract": "We consider the task of detecting sentences that express causality, as a step towards mining causal relations from texts. To bypass the scarcity of causal instances in relation extraction datasets, we exploit transfer learning, namely ELMO and BERT, using a bidirectional GRU with self-attention ( BIGRUATT ) as a baseline. We experiment with both generic public relation extraction datasets and a new biomedical causal sentence detection dataset, a subset of which we make publicly available. We find that transfer learning helps only in very small datasets. With larger datasets, BIGRUATT reaches a performance plateau, then larger datasets and transfer learning do not help",
    "volume": "workshop",
    "checked": true,
    "id": "82770e184352eb2592fea5537478622f3532eb7c",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-5032": {
    "title": "Embedding Biomedical Ontologies by Jointly Encoding Network Structure and Textual Node Descriptors",
    "abstract": "Network Embedding (NE) methods, which map network nodes to low-dimensional feature vectors, have wide applications in network analysis and bioinformatics. Many existing NE methods rely only on network structure, overlooking other information associated with the nodes, e.g., text describing the nodes. Recent attempts to combine the two sources of information only consider local network structure. We extend NODE2VEC, a well-known NE method that considers broader network structure, to also consider textual node descriptors using recurrent neural encoders. Our method is evaluated on link prediction in two networks derived from UMLS. Experimental results demonstrate the effectiveness of the proposed approach compared to previous work",
    "volume": "workshop",
    "checked": true,
    "id": "38a126ddfa8adfae3b28068bffc1255678ed5d6c",
    "citation_count": 11
  },
  "https://aclanthology.org/W19-5033": {
    "title": "Simplification-induced transformations: typology and some characteristics",
    "abstract": "The purpose of automatic text simplification is to transform technical or difficult to understand texts into a more friendly version. The semantics must be preserved during this transformation. Automatic text simplification can be done at different levels (lexical, syntactic, semantic, stylistic...) and relies on the corresponding knowledge and resources (lexicon, rules...). Our objective is to propose methods and material for the creation of transformation rules from a small set of parallel sentences differentiated by their technicity. We also propose a typology of transformations and quantify them. We work with French-language data related to the medical domain, although we assume that the method can be exploited on texts in any language and from any domain",
    "volume": "workshop",
    "checked": true,
    "id": "08f541fa770510801800b4b3a69b8cddd8cc0d99",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-5034": {
    "title": "ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing",
    "abstract": "Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https://allenai.github.io/scispacy/",
    "volume": "workshop",
    "checked": true,
    "id": "de28ec1d7bd38c8fc4e8ac59b6133800818b4e29",
    "citation_count": 341
  },
  "https://aclanthology.org/W19-5035": {
    "title": "Improving Chemical Named Entity Recognition in Patents with Contextualized Word Embeddings",
    "abstract": "Chemical patents are an important resource for chemical information. However, few chemical Named Entity Recognition (NER) systems have been evaluated on patent documents, due in part to their structural and linguistic complexity. In this paper, we explore the NER performance of a BiLSTM-CRF model utilising pre-trained word embeddings, character-level word representations and contextualized ELMo word representations for chemical patents. We compare word embeddings pre-trained on biomedical and chemical patent corpora. The effect of tokenizers optimized for the chemical domain on NER performance in chemical patents is also explored. The results on two patent corpora show that contextualized word representations generated from ELMo substantially improve chemical NER performance w.r.t. the current state-of-the-art. We also show that domain-specific resources such as word embeddings trained on chemical patents and chemical-specific tokenizers, have a positive impact on NER performance",
    "volume": "workshop",
    "checked": true,
    "id": "544fc77d1df97f31fd1d06b8e60e331cfe35546f",
    "citation_count": 32
  },
  "https://aclanthology.org/W19-5036": {
    "title": "Improving classification of Adverse Drug Reactions through Using Sentiment Analysis and Transfer Learning",
    "abstract": "The availability of large-scale and real-time data on social media has motivated research into adverse drug reactions (ADRs). ADR classification helps to identify negative effects of drugs, which can guide health professionals and pharmaceutical companies in making medications safer and advocating patients' safety. Based on the observation that in social media, negative sentiment is frequently expressed towards ADRs, this study presents a neural model that combines sentiment analysis with transfer learning techniques to improve ADR detection in social media postings. Our system is firstly trained to classify sentiment in tweets concerning current affairs, using the SemEval17-task4A corpus. We then apply transfer learning to adapt the model to the task of detecting ADRs in social media postings. We show that, in combination with rich representations of words and their contexts, transfer learning is beneficial, especially given the large degree of vocabulary overlap between the current affairs posts in the SemEval17-task4A corpus and posts about ADRs. We compare our results with previous approaches, and show that our model can outperform them by up to 3% F-score",
    "volume": "workshop",
    "checked": true,
    "id": "6bdb3edb5b3ef704e77584a19ee2b94c2ac0a8f0",
    "citation_count": 15
  },
  "https://aclanthology.org/W19-5037": {
    "title": "Exploring Diachronic Changes of Biomedical Knowledge using Distributed Concept Representations",
    "abstract": "In research best practices can change over time as new discoveries are made and novel methods are implemented. Scientific publications reporting about the latest facts and current state-of-the-art can be possibly outdated after some years or even proved to be false. A publication usually sheds light only on the knowledge of the period it has been published. Thus, the aspect of time can play an essential role in the reliability of the presented information. In Natural Language Processing many methods focus on information extraction from text, such as detecting entities and their relationship to each other. Those methods mostly focus on the facts presented in the text itself and not on the aspects of knowledge which changes over time. This work instead examines the evolution in biomedical knowledge over time using scientific literature in terms of diachronic change. Mainly the usage of temporal and distributional concept representations are explored and evaluated by a proof-of-concept",
    "volume": "workshop",
    "checked": true,
    "id": "8aa3979d3ea0d5b34b71018c77585220f1cb27b9",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5038": {
    "title": "Extracting relations between outcomes and significance levels in Randomized Controlled Trials (RCTs) publications",
    "abstract": "Randomized controlled trials assess the effects of an experimental intervention by comparing it to a control intervention with regard to some variables - trial outcomes. Statistical hypothesis testing is used to test if the experimental intervention is superior to the control. Statistical significance is typically reported for the measured outcomes and is an important characteristic of the results. We propose a machine learning approach to automatically extract reported outcomes, significance levels and the relation between them. We annotated a corpus of 663 sentences with 2,552 outcome - significance level relations (1,372 positive and 1,180 negative relations). We compared several classifiers, using a manually crafted feature set, and a number of deep learning models. The best performance (F-measure of 94%) was shown by the BioBERT fine-tuned model",
    "volume": "workshop",
    "checked": true,
    "id": "ce1c7dbc6bb55ec2dddaf056d0f7f88e6ee6a130",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5039": {
    "title": "Overview of the MEDIQA 2019 Shared Task on Textual Inference, Question Entailment and Question Answering",
    "abstract": "This paper presents the MEDIQA 2019 shared task organized at the ACL-BioNLP workshop. The shared task is motivated by a need to develop relevant methods, techniques and gold standards for inference and entailment in the medical domain, and their application to improve domain specific information retrieval and question answering systems. MEDIQA 2019 includes three tasks: Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and Question Answering (QA) in the medical domain. 72 teams participated in the challenge, achieving an accuracy of 98% in the NLI task, 74.9% in the RQE task, and 78.3% in the QA task. In this paper, we describe the tasks, the datasets, and the participants' approaches and results. We hope that this shared task will attract further research efforts in textual inference, question entailment, and question answering in the medical domain",
    "volume": "workshop",
    "checked": true,
    "id": "e66e64066606b17f6513a2c1ca4cb249c9c95489",
    "citation_count": 75
  },
  "https://aclanthology.org/W19-5040": {
    "title": "PANLP at MEDIQA 2019: Pre-trained Language Models, Transfer Learning and Knowledge Distillation",
    "abstract": "This paper describes the models designated for the MEDIQA 2019 shared tasks by the team PANLP. We take advantages of the recent advances in pre-trained bidirectional transformer language models such as BERT (Devlin et al., 2018) and MT-DNN (Liu et al., 2019b). We find that pre-trained language models can significantly outperform traditional deep learning models. Transfer learning from the NLI task to the RQE task is also experimented, which proves to be useful in improving the results of fine-tuning MT-DNN large. A knowledge distillation process is implemented, to distill the knowledge contained in a set of models and transfer it into an single model, whose performance turns out to be comparable with that obtained by the ensemble of that set of models. Finally, for test submissions, model ensemble and a re-ranking process are implemented to boost the performances. Our models participated in all three tasks and ranked the 1st place for the RQE task, and the 2nd place for the NLI task, and also the 2nd place for the QA task",
    "volume": "workshop",
    "checked": true,
    "id": "2bb4b6b876581c0c21ef58d3c38809715173ca7a",
    "citation_count": 17
  },
  "https://aclanthology.org/W19-5041": {
    "title": "Pentagon at MEDIQA 2019: Multi-task Learning for Filtering and Re-ranking Answers using Language Inference and Question Entailment",
    "abstract": "Parallel deep learning architectures like fine-tuned BERT and MT-DNN, have quickly become the state of the art, bypassing previous deep and shallow learning methods by a large margin. More recently, pre-trained models from large related datasets have been able to perform well on many downstream tasks by just fine-tuning on domain-specific datasets (similar to transfer learning). However, using powerful models on non-trivial tasks, such as ranking and large document classification, still remains a challenge due to input size limitations of parallel architecture and extremely small datasets (insufficient for fine-tuning). In this work, we introduce an end-to-end system, trained in a multi-task setting, to filter and re-rank answers in the medical domain. We use task-specific pre-trained models as deep feature extractors. Our model achieves the highest Spearman's Rho and Mean Reciprocal Rank of 0.338 and 0.9622 respectively, on the ACL-BioNLP workshop MediQA Question Answering shared-task",
    "volume": "workshop",
    "checked": true,
    "id": "512eed09dbf24c37aad537b6aa8d1d023c20a013",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-5042": {
    "title": "DoubleTransfer at MEDIQA 2019: Multi-Source Transfer Learning for Natural Language Understanding in the Medical Domain",
    "abstract": "This paper describes our competing system to enter the MEDIQA-2019 competition. We use a multi-source transfer learning approach to transfer the knowledge from MT-DNN and SciBERT to natural language understanding tasks in the medical domain. For transfer learning fine-tuning, we use multi-task learning on NLI, RQE and QA tasks on general and medical domains to improve performance. The proposed methods are proved effective for natural language understanding in the medical domain, and we rank the first place on the QA task",
    "volume": "workshop",
    "checked": true,
    "id": "443fbfd614f35061a679b7eeba7d9302d0626a7d",
    "citation_count": 11
  },
  "https://aclanthology.org/W19-5043": {
    "title": "Surf at MEDIQA 2019: Improving Performance of Natural Language Inference in the Clinical Domain by Adopting Pre-trained Language Model",
    "abstract": "While deep learning techniques have shown promising results in many natural language processing (NLP) tasks, it has not been widely applied to the clinical domain. The lack of large datasets and the pervasive use of domain-specific language (i.e. abbreviations and acronyms) in the clinical domain causes slower progress in NLP tasks than that of the general NLP tasks. To fill this gap, we employ word/subword-level based models that adopt large-scale data-driven methods such as pre-trained language models and transfer learning in analyzing text for the clinical domain. Empirical results demonstrate the superiority of the proposed methods by achieving 90.6% accuracy in medical domain natural language inference task. Furthermore, we inspect the independent strengths of the proposed approaches in quantitative and qualitative manners. This analysis will help researchers to select necessary components in building models for the medical domain",
    "volume": "workshop",
    "checked": true,
    "id": "7db308823bdd2b297da2c56096c5dd57b65c6152",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5044": {
    "title": "WTMED at MEDIQA 2019: A Hybrid Approach to Biomedical Natural Language Inference",
    "abstract": "Natural language inference (NLI) is challenging, especially when it is applied to technical domains such as biomedical settings. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre-trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain-specific information. Then we combine the output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our models on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1",
    "volume": "workshop",
    "checked": true,
    "id": "ddbdd937d6793371f28755b5f170eaa67d0a5e74",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-5045": {
    "title": "KU_ai at MEDIQA 2019: Domain-specific Pre-training and Transfer Learning for Medical NLI",
    "abstract": "In this paper, we describe our system and results submitted for the Natural Language Inference (NLI) track of the MEDIQA 2019 Shared Task. As KU_ai team, we used BERT as our baseline model and pre-processed the MedNLI dataset to mitigate the negative impact of de-identification artifacts. Moreover, we investigated different pre-training and transfer learning approaches to improve the performance. We show that pre-training the language model on rich biomedical corpora has a significant effect in teaching the model domain-specific language. In addition, training the model on large NLI datasets such as MultiNLI and SNLI helps in learning task-specific reasoning. Finally, we ensembled our highest-performing models, and achieved 84.7% accuracy on the unseen test dataset and ranked 10th out of 17 teams in the official results",
    "volume": "workshop",
    "checked": true,
    "id": "8aa1d9145640b4a63258b82bc8180c3683d072b5",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-5046": {
    "title": "DUT-NLP at MEDIQA 2019: An Adversarial Multi-Task Network to Jointly Model Recognizing Question Entailment and Question Answering",
    "abstract": "In this paper, we propose a novel model called Adversarial Multi-Task Network (AMTN) for jointly modeling Recognizing Question Entailment (RQE) and medical Question Answering (QA) tasks. AMTN utilizes a pre-trained BioBERT model and an Interactive Transformer to learn the shared semantic representations across different task through parameter sharing mechanism. Meanwhile, an adversarial training strategy is introduced to separate the private features of each task from the shared representations. Experiments on BioNLP 2019 RQE and QA Shared Task datasets show that our model benefits from the shared representations of both tasks provided by multi-task learning and adversarial training, and obtains significant improvements upon the single-task models",
    "volume": "workshop",
    "checked": true,
    "id": "79313c298988d2f3b3c68004bedbf4754c8a1605",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-5047": {
    "title": "DUT-BIM at MEDIQA 2019: Utilizing Transformer Network and Medical Domain-Specific Contextualized Representations for Question Answering",
    "abstract": "In medical domain, given a medical question, it is difficult to manually select the most relevant information from a large number of search results. BioNLP 2019 proposes Question Answering (QA) task, which encourages the use of text mining technology to automatically judge whether a search result is an answer to the medical question. The main challenge of QA task is how to mine the semantic relation between question and answer. We propose BioBERT Transformer model to tackle this challenge, which applies Transformers to extract semantic relation between different words in questions and answers. Furthermore, BioBERT is utilized to encode medical domain-specific contextualized word representations. Our method has reached the accuracy of 76.24% and spearman of 17.12% on the BioNLP 2019 QA task",
    "volume": "workshop",
    "checked": true,
    "id": "b21eea97afe0c6a7a74c18f717c70528b9c4b7c1",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5048": {
    "title": "Dr.Quad at MEDIQA 2019: Towards Textual Inference and Question Entailment using contextualized representations",
    "abstract": "This paper presents the submissions by TeamDr.Quad to the ACL-BioNLP 2019 shared task on Textual Inference and Question Entailment in the Medical Domain. Our system is based on the prior work Liu et al. (2019) which uses a multi-task objective function for textual entailment. In this work, we explore different strategies for generalizing state-of-the-art language understanding models to the specialized medical domain. Our results on the shared task demonstrate that incorporating domain knowledge through data augmentation is a powerful strategy for addressing challenges posed specialized domains such as medicine",
    "volume": "workshop",
    "checked": true,
    "id": "f07a326e21395f025a87b2d77cac7e8ca502f002",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5049": {
    "title": "Sieg at MEDIQA 2019: Multi-task Neural Ensemble for Biomedical Inference and Entailment",
    "abstract": "This paper presents a multi-task learning approach to natural language inference (NLI) and question entailment (RQE) in the biomedical domain. Recognizing textual inference relations and question similarity can address the issue of answering new consumer health questions by mapping them to Frequently Asked Questions on reputed websites like the NIH. We show that leveraging information from parallel tasks across domains along with medical knowledge integration allows our model to learn better biomedical feature representations. Our final models for the NLI and RQE tasks achieve the 4th and 2nd rank on the shared-task leaderboard respectively",
    "volume": "workshop",
    "checked": true,
    "id": "dcd627cf71af44b1a140d5b3e7503cb3251bee58",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5050": {
    "title": "IIT-KGP at MEDIQA 2019: Recognizing Question Entailment using Sci-BERT stacked with a Gradient Boosting Classifier",
    "abstract": "Official System Description paper of Team IIT-KGP ranked 1st in the Development phase and 3rd in Testing Phase in MEDIQA 2019 - Recognizing Question Entailment (RQE) Shared Task of BioNLP workshop - ACL 2019. The number of people turning to the Internet to search for a diverse range of health-related subjects continues to grow and with this multitude of information available, duplicate questions are becoming more frequent and finding the most appropriate answers becomes problematic. This issue is important for question answering platforms as it complicates the retrieval of all information relevant to the same topic, particularly when questions similar in essence are expressed differently, and answering a given medical question by retrieving similar questions that are already answered by human experts seems to be a promising solution. In this paper, we present our novel approach to detect question entailment by determining the type of question asked rather than focusing on the type of the ailment given. This unique methodology makes the approach robust towards examples which have different ailment names but are synonyms of each other. Also, it enables us to check entailment at a much more fine-grained level. QSpider is a staged system consisting of state-of-the-art model Sci-BERT used as a multi-class classifier aimed at capturing both question types and semantic relations stacked with a Gradient Boosting Classifier which checks for entailment. QSpider achieves an accuracy score of 68.4% on the Test set which outperforms the baseline model (54.1%) by an accuracy score of 14.3%",
    "volume": "workshop",
    "checked": true,
    "id": "a2a37a6a3dc62537b12b87a931aaa21776751dcc",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5051": {
    "title": "ANU-CSIRO at MEDIQA 2019: Question Answering Using Deep Contextual Knowledge",
    "abstract": "We report on our system for textual inference and question entailment in the medical domain for the ACL BioNLP 2019 Shared Task, MEDIQA. Textual inference is the task of finding the semantic relationships between pairs of text. Question entailment involves identifying pairs of questions which have similar semantic content. To improve upon medical natural language inference and question entailment approaches to further medical question answering, we propose a system that incorporates open-domain and biomedical domain approaches to improve semantic understanding and ambiguity resolution. Our models achieve 80% accuracy on medical natural language inference (6.5% absolute improvement over the original baseline), 48.9% accuracy on recognising medical question entailment, 0.248 Spearman's rho for question answering ranking and 68.6% accuracy for question answering classification",
    "volume": "workshop",
    "checked": true,
    "id": "bf3e3d5ef08576fed10de213f3e871212576660b",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5052": {
    "title": "MSIT_SRIB at MEDIQA 2019: Knowledge Directed Multi-task Framework for Natural Language Inference in Clinical Domain",
    "abstract": "In this paper, we present Biomedical Multi-Task Deep Neural Network (Bio-MTDNN) on the NLI task of MediQA 2019 challenge. Bio-MTDNN utilizes \"transfer learning\" based paradigm where not only the source and target domains are different but also the source and target tasks are varied, although related. Further, Bio-MTDNN integrates knowledge from external sources such as clinical databases (UMLS) enhancing its performance on the clinical domain. Our proposed method outperformed the official baseline and other prior models (such as ESIM and Infersent on dev set) by a considerable margin as evident from our experimental results",
    "volume": "workshop",
    "checked": true,
    "id": "ead21af5e1f27d596aa6c45a10eaef555b4c4189",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5053": {
    "title": "UU_TAILS at MEDIQA 2019: Learning Textual Entailment in the Medical Domain",
    "abstract": "This article describes the participation of the UU_TAILS team in the 2019 MEDIQA challenge intended to improve domain-specific models in medical and clinical NLP. The challenge consists of 3 tasks: medical language inference (NLI), recognizing textual entailment (RQE) and question answering (QA). Our team participated in tasks 1 and 2 and our best runs achieved a performance accuracy of 0.852 and 0.584 respectively for the test sets. The models proposed for task 1 relied on BERT embeddings and different ensemble techniques. For the RQE task, we trained a traditional multilayer perceptron network based on embeddings generated by the universal sentence encoder",
    "volume": "workshop",
    "checked": true,
    "id": "3d97219ca95dcbc051012333cb3f9143dce38c6f",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5054": {
    "title": "UW-BHI at MEDIQA 2019: An Analysis of Representation Methods for Medical Natural Language Inference",
    "abstract": "Recent advances in distributed language modeling have led to large performance increases on a variety of natural language processing (NLP) tasks. However, it is not well understood how these methods may be augmented by knowledge-based approaches. This paper compares the performance and internal representation of an Enhanced Sequential Inference Model (ESIM) between three experimental conditions based on the representation method: Bidirectional Encoder Representations from Transformers (BERT), Embeddings of Semantic Predications (ESP), or Cui2Vec. The methods were evaluated on the Medical Natural Language Inference (MedNLI) subtask of the MEDIQA 2019 shared task. This task relied heavily on semantic understanding and thus served as a suitable evaluation set for the comparison of these representation methods",
    "volume": "workshop",
    "checked": true,
    "id": "02750fe77b0cac78914cb62891bf7dc715aafcae",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5055": {
    "title": "Saama Research at MEDIQA 2019: Pre-trained BioBERT with Attention Visualisation for Medical Natural Language Inference",
    "abstract": "Natural Language inference is the task of identifying relation between two sentences as entailment, contradiction or neutrality. MedNLI is a biomedical flavour of NLI for clinical domain. This paper explores the use of Bidirectional Encoder Representation from Transformer (BERT) for solving MedNLI. The proposed model, BERT pre-trained on PMC, PubMed and fine-tuned on MIMICIII v1.4, achieves state of the art results on MedNLI (83.45%) and an accuracy of 78.5% in MEDIQA challenge. The authors present an analysis of the attention patterns that emerged as a result of training BERT on MedNLI using a visualization tool, bertviz",
    "volume": "workshop",
    "checked": true,
    "id": "eaedcab2179ca3e1e5e91e17b66a266ffaaae667",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5056": {
    "title": "IITP at MEDIQA 2019: Systems Report for Natural Language Inference, Question Entailment and Question Answering",
    "abstract": "This paper presents the experiments accomplished as a part of our participation in the MEDIQA challenge, an (Abacha et al., 2019) shared task. We participated in all the three tasks defined in this particular shared task. The tasks are viz. i. Natural Language Inference (NLI) ii. Recognizing Question Entailment(RQE) and their application in medical Question Answering (QA). We submitted runs using multiple deep learning based systems (runs) for each of these three tasks. We submitted five system results in each of the NLI and RQE tasks, and four system results for the QA task. The systems yield encouraging results in all the three tasks. The highest performance obtained in NLI, RQE and QA tasks are 81.8%, 53.2%, and 71.7%, respectively",
    "volume": "workshop",
    "checked": true,
    "id": "3704952698c50baee8cb07f9c6af4c05c2cfe194",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5057": {
    "title": "LasigeBioTM at MEDIQA 2019: Biomedical Question Answering using Bidirectional Transformers and Named Entity Recognition",
    "abstract": "Biomedical Question Answering (QA) aims at providing automated answers to user questions, regarding a variety of biomedical topics. For example, these questions may ask for related to diseases, drugs, symptoms, or medical procedures. Automated biomedical QA systems could improve the retrieval of information necessary to answer these questions. The MEDIQA challenge consisted of three tasks concerning various aspects of biomedical QA. This challenge aimed at advancing approaches to Natural Language Inference (NLI) and Recognizing Question Entailment (RQE), which would then result in enhanced approaches to biomedical QA. Our approach explored a common Transformer-based architecture that could be applied to each task. This approach shared the same pre-trained weights, but which were then fine-tuned for each task using the provided training data. Furthermore, we augmented the training data with external datasets and enriched the question and answer texts using MER, a named entity recognition tool. Our approach obtained high levels of accuracy, in particular on the NLI task, which classified pairs of text according to their relation. For the QA task, we obtained higher Spearman's rank correlation values using the entities recognized by MER",
    "volume": "workshop",
    "checked": true,
    "id": "b4d13ab076444319bd6e5a5330734b5153606ccf",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-5058": {
    "title": "NCUEE at MEDIQA 2019: Medical Text Inference Using Ensemble BERT-BiLSTM-Attention Model",
    "abstract": "This study describes the model design of the NCUEE system for the MEDIQA challenge at the ACL-BioNLP 2019 workshop. We use the BERT (Bidirectional Encoder Representations from Transformers) as the word embedding method to integrate the BiLSTM (Bidirectional Long Short-Term Memory) network with an attention mechanism for medical text inferences. A total of 42 teams participated in natural language inference task at MEDIQA 2019. Our best accuracy score of 0.84 ranked the top-third among all submissions in the leaderboard",
    "volume": "workshop",
    "checked": true,
    "id": "01edf4b0884903b310c1b0a1cb9cc3cb4bf2d8b2",
    "citation_count": 15
  },
  "https://aclanthology.org/W19-5059": {
    "title": "ARS_NITK at MEDIQA 2019:Analysing Various Methods for Natural Language Inference, Recognising Question Entailment and Medical Question Answering System",
    "abstract": "In this paper, we present three approaches for Natural Language Inference, Question Entailment Recognition and Question-Answering to improve domain-specific Information Retrieval. For addressing the NLI task, the UMLS Metathesaurus was used to find the synonyms of medical terms in given sentences, on which the InferSent model was trained to predict if the given sentence is an entailment, contradictory or neutral. We also introduce a new Extreme gradient boosting model built on PubMed embeddings to perform RQE. Further, a closed-domain Question Answering technique that uses Bi-directional LSTMs trained on the SquAD dataset to determine relevant ranks of answers for a given question is also discussed",
    "volume": "workshop",
    "checked": true,
    "id": "fa883e8f4ab158f8bec03a2a9213597fe6ab7b23",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-5101": {
    "title": "When the whole is greater than the sum of its parts: Multiword expressions and idiomaticity",
    "abstract": "Multiword expressions (MWEs) feature prominently in the mental lexicon of native speakers (Jackendoff, 1997) in all languages and domains, from informal to technical contexts (Biber et al., 1999) with about four MWEs being produced per minute of discourse (Glucksberg, 1989). MWEs come in all shapes and forms, including idioms like rock the boat (as cause problems or disturb a situation) and compound nouns like monkey business (as dishonest behaviour). Their accurate detection and understanding may often require more than knowledge about individual words and how they can be combined (Fillmore, 1979), as they may display various degrees of idiosyncrasy, including lexical, syntactic, semantic and statistical (Sag et al., 2002; Baldwin and Kim, 2010), which provide new challenges and opportunities for language processing (Constant et al., 2017). For instance, while for some combinations the meaning can be inferred from their parts like olive oil (oil made of olives) this is not always the case, as in dark horse (meaning an unknown candidate who unexpectedly succeeds), and when processing a sentence some of the challenges are to identify which words form an expression (Ramisch, 2015), and whether the expression is idiomatic (Cordeiro et al., 2019). In this talk I will give an overview of advances on the identification and treatment of multiword expressions, in particular concentrating on techniques for identifying their degree of idiomaticity",
    "volume": "workshop",
    "checked": true,
    "id": "6eaf71d9176fdfcb91dcf1e54e6bdf01f43ba7df",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-5102": {
    "title": "Hear about Verbal Multiword Expressions in the Bulgarian and the Romanian Wordnets Straight from the Horse's Mouth",
    "abstract": "In this paper we focus on verbal multiword expressions (VMWEs) in Bulgarian and Romanian as reflected in the wordnets of the two languages. The annotation of VMWEs relies on the classification defined within the PARSEME Cost Action. After outlining the properties of various types of VMWEs, a cross-language comparison is drawn, aimed to highlight the similarities and the differences between Bulgarian and Romanian with respect to the lexicalization and distribution of VMWEs. The contribution of this work is in outlining essential features of the description and classification of VMWEs and the cross-language comparison at the lexical level, which is essential for the understanding of the need for uniform annotation guidelines and a viable procedure for validation of the annotation",
    "volume": "workshop",
    "checked": true,
    "id": "1350c7470d65e5d1bb4ba7b7c7fcf01e5727d8a5",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-5103": {
    "title": "The Romanian Corpus Annotated with Verbal Multiword Expressions",
    "abstract": "This paper reports on the Romanian journalistic corpus annotated with verbal multiword expressions following the PARSEME guidelines. The corpus is sentence split, tokenized, part-of-speech tagged, lemmatized, syntactically annotated and verbal multiword expressions are identified and classified. It offers insights into the frequency of such Romanian word combinations and allows for their characterization. We offer data about the types of verbal multiword expressions in the corpus and some of their characteristics, such as internal structure, diversity in the corpus, average length, productivity of the verbs. This is a language resource that is important per se, as well as for the task of automatic multiword expressions identification, which can be further used in other systems. It was already used as training and test material in the shared tasks for the automatic identification of verbal multiword expressions organized by PARSEME",
    "volume": "workshop",
    "checked": true,
    "id": "9a80b74868691c9e1f0074dce20ac5660c42082f",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-5104": {
    "title": "Using OntoLex-Lemon for Representing and Interlinking German Multiword Expressions in OdeNet and MMORPH",
    "abstract": "We describe work consisting in porting two large German lexical resources into the OntoLex-Lemon model in order to establish complementary interlinkings between them. One resource is OdeNet (Open German WordNet) and the other is a further development of the German version of the MMORPH morphological analyzer. We show how the Multiword Expressions (MWEs) contained in OdeNet can be morphologically specified by the use of the lexical representation and linking features of OntoLex-Lemon, which also support the formulation of restrictions in the usage of such expressions",
    "volume": "workshop",
    "checked": true,
    "id": "5afdf2d7df0d672d1fbcdd5836bf6f5507e3d4d4",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-5105": {
    "title": "Learning to Predict Novel Noun-Noun Compounds",
    "abstract": "We introduce temporally and contextually-aware models for the novel task of predicting unseen but plausible concepts, as conveyed by noun-noun compounds in a time-stamped corpus. We train compositional models on observed compounds, more specifically the composed distributed representations of their constituents across a time-stamped corpus, while giving it corrupted instances (where head or modifier are replaced by a random constituent) as negative evidence. The model captures generalisations over this data and learns what combinations give rise to plausible compounds and which ones do not. After training, we query the model for the plausibility of automatically generated novel combinations and verify whether the classifications are accurate. For our best model, we find that in around 85% of the cases, the novel compounds generated are attested in previously unseen data. An additional estimated 5% are plausible despite not being attested in the recent corpus, based on judgments from independent human raters",
    "volume": "workshop",
    "checked": true,
    "id": "ed8ff2bef83103f3229e80e92d89d02d94e02f61",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5106": {
    "title": "Unsupervised Compositional Translation of Multiword Expressions",
    "abstract": "This article describes a dependency-based strategy that uses compositional distributional semantics and cross-lingual word embeddings to translate multiword expressions (MWEs). Our unsupervised approach performs translation as a process of word contextualization by taking into account lexico-syntactic contexts and selectional preferences. This strategy is suited to translate phraseological combinations and phrases whose constituent words are lexically restricted by each other. Several experiments in adjective-noun and verb-object compounds show that mutual contextualization (co-compositionality) clearly outperforms other compositional methods. The paper also contributes with a new freely available dataset of English-Spanish MWEs used to validate the proposed compositional strategy",
    "volume": "workshop",
    "checked": true,
    "id": "70d1795a4b495db60ce53e545db2ce5bc6ef0457",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5107": {
    "title": "A comparison of statistical association measures for identifying dependency-based collocations in various languages",
    "abstract": "This paper presents an exploration of different statistical association measures to automatically identify collocations from corpora in English, Portuguese, and Spanish. To evaluate the impact of the association metrics we manually annotated corpora with three different syntactic patterns of collocations (adjective-noun, verb-object and nominal compounds). We took advantage of the PARSEME 1.1 Shared Task corpora by selecting a subset of 155k tokens in the three referred languages, in which we annotated 1,526 collocations with the corresponding Lexical Functions according to the Meaning-Text Theory. Using the resulting gold-standard, we have carried out a comparison between frequency data and several well-known association measures, both symmetric and asymmetric. The results show that the combination of dependency triples with raw frequency information is as powerful as the best association measures in most syntactic patterns and languages. Furthermore, and despite the asymmetric behaviour of collocations, directional approaches perform worse than the symmetric ones in the extraction of these phraseological combinations",
    "volume": "workshop",
    "checked": true,
    "id": "b32a6055621bd7574fe57125be6cc66ef9927aae",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-5108": {
    "title": "L2 Processing Advantages of Multiword Sequences: Evidence from Eye-Tracking",
    "abstract": "A substantial body of research has demonstrated that native speakers are sensitive to the frequencies of multiword sequences (MWS). Here, we ask whether and to what extent intermediate-advanced L2 speakers of English can also develop the sensitivity to the statistics of MWS. To this end, we aimed to replicate the MWS frequency effects found for adult native language speakers based on evidence from self-paced reading and sentence recall tasks in an ecologically more valid eye-tracking study. L2 speakers' sensitivity to MWS frequency was evaluated using generalized linear mixed-effects regression with separate models fitted for each of the four dependent measures. Mixed-effects modeling revealed significantly faster processing of sentences containing MWS compared to sentences containing equivalent control items across all eyetracking measures. Taken together, these findings suggest that, in line with emergentist approaches, MWS are important building blocks of language and that similar mechanisms underlie both native and non-native language processing",
    "volume": "workshop",
    "checked": true,
    "id": "66951b386e47fe8a8801ba984c47d567dac73353",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-5109": {
    "title": "Modeling MWEs in BTB-WN",
    "abstract": "The paper presents the characteristics of the predominant types of MultiWord expressions (MWEs) in the BulTreeBank WordNet – BTB-WN. Their distribution in BTB-WN is discussed with respect to the overall hierarchical organization of the lexical resource. Also, a catena-based modeling is proposed for handling the issues of lexical semantics of MWEs",
    "volume": "workshop",
    "checked": true,
    "id": "01d4ba482aa75ef065cba85ae7827d91dd366f11",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5110": {
    "title": "Without lexicons, multiword expression identification will never fly: A position statement",
    "abstract": "Because most multiword expressions (MWEs), especially verbal ones, are semantically non-compositional, their automatic identification in running text is a prerequisite for semantically-oriented downstream applications. However, recent developments, driven notably by the PARSEME shared task on automatic identification of verbal MWEs, show that this task is harder than related tasks, despite recent contributions both in multilingual corpus annotation and in computational models. In this paper, we analyse possible reasons for this state of affairs. They lie in the nature of the MWE phenomenon, as well as in its distributional properties. We also offer a comparative analysis of the state-of-the-art systems, which exhibit particularly strong sensitivity to unseen data. On this basis, we claim that, in order to make strong headway in MWE identification, the community should bend its mind into coupling identification of MWEs with their discovery, via syntactic MWE lexicons. Such lexicons need not necessarily achieve a linguistically complete modelling of MWEs' behavior, but they should provide minimal morphosyntactic information to cover some potential uses, so as to complement existing MWE-annotated corpora. We define requirements for such minimal NLP-oriented lexicon, and we propose a roadmap for the MWE community driven by these requirements",
    "volume": "workshop",
    "checked": true,
    "id": "494198e78bf0238e8ce68b8a08e47df802063153",
    "citation_count": 14
  },
  "https://aclanthology.org/W19-5111": {
    "title": "A Systematic Comparison of English Noun Compound Representations",
    "abstract": "Building meaningful representations of noun compounds is not trivial since many of them scarcely appear in the corpus. To that end, composition functions approximate the distributional representation of a noun compound by combining its constituent distributional vectors. In the more general case, phrase embeddings have been trained by minimizing the distance between the vectors representing paraphrases. We compare various types of noun compound representations, including distributional, compositional, and paraphrase-based representations, through a series of tasks and analyses, and with an extensive number of underlying word embeddings. We find that indeed, in most cases, composition functions produce higher quality representations than distributional ones, and they improve with computational power. No single function performs best in all scenarios, suggesting that a joint training objective may produce improved representations",
    "volume": "workshop",
    "checked": true,
    "id": "fca558b1698310dc589e14e3c6a64e345cbf3e63",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5112": {
    "title": "Semantic Modelling of Adjective-Noun Collocations Using FrameNet",
    "abstract": "In this paper we argue that Frame Semantics (Fillmore, 1982) provides a good framework for semantic modelling of adjective-noun collocations. More specifically, the notion of a frame is rich enough to account for nouns from different semantic classes and to model semantic relations that hold between an adjective and a noun in terms of Frame Elements. We have substantiated these findings by considering a sample of adjective-noun collocations from German such as \"enger Freund\" 'close friend' and \"starker Regen\" 'heavy rain'. The data sample is taken from different semantic fields identified in the German wordnet GermaNet (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010). The study is based on the electronic dictionary DWDS (Klein and Geyken, 2010) and uses the collocation extraction tool Wortprofil (Geyken et al., 2009). The FrameNet modelling is based on the online resource available at http://framenet.icsi.berkeley.edu. Since FrameNets are available for a range of typologically different languages, it is feasible to extend the current case study to other languages",
    "volume": "workshop",
    "checked": true,
    "id": "f3cea05632763d202bde05332dc3c35314fbbb73",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5113": {
    "title": "A Neural Graph-based Approach to Verbal MWE Identification",
    "abstract": "We propose to tackle the problem of verbal multiword expression (VMWE) identification using a neural graph parsing-based approach. Our solution involves encoding VMWE annotations as labellings of dependency trees and, subsequently, applying a neural network to model the probabilities of different labellings. This strategy can be particularly effective when applied to discontinuous VMWEs and, thanks to dense, pre-trained word vector representations, VMWEs unseen during training. Evaluation of our approach on three PARSEME datasets (German, French, and Polish) shows that it allows to achieve performance on par with the previous state-of-the-art (Al Saied et al., 2018)",
    "volume": "workshop",
    "checked": true,
    "id": "5b79c162cbe6c811c6c2ef907e983a34d75479e3",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5114": {
    "title": "Confirming the Non-compositionality of Idioms for Sentiment Analysis",
    "abstract": "An idiom is defined as a non-compositional multiword expression, one whose meaning cannot be deduced from the definitions of the component words. This definition does not explicitly define the compositionality of an idiom's sentiment; this paper aims to determine whether the sentiment of the component words of an idiom is related to the sentiment of that idiom. We use the Dictionary of Affect in Language augmented by WordNet to give each idiom in the Sentiment Lexicon of IDiomatic Expressions (SLIDE) a component-wise sentiment score and compare it to the phrase-level sentiment label crowdsourced by the creators of SLIDE. We find that there is no discernible relation between these two measures of idiom sentiment. This supports the hypothesis that idioms are not compositional for sentiment along with semantics and motivates further work in handling idioms for sentiment analysis",
    "volume": "workshop",
    "checked": true,
    "id": "e2db70b495926d7b3d2d621893db003520cb85db",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5115": {
    "title": "IDION: A database for Modern Greek multiword expressions",
    "abstract": "We report on the ongoing development of IDION, a web resource of richly documented multiword expressions (MWEs) of Modern Greek addressed to the human user and to NLP. IDION contains about 2000 verb MWEs (VMWEs) of which about 850 are fully documented as regards their syntactic flexibility, their semantics and the semantic relations with other VMWEs. Sets of synonymous MWEs are defined in a bottom-up manner revealing the conceptual organization of the MG VMWE domain",
    "volume": "workshop",
    "checked": true,
    "id": "3aa3fe13ed9df37644c0da87f6375bead46a723c",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-5116": {
    "title": "Identification of Adjective-Noun Neologisms using Pretrained Language Models",
    "abstract": "Neologism detection is a key task in the constructing of lexical resources and has wider implications for NLP, however the identification of multiword neologisms has received little attention. In this paper, we show that we can effectively identify the distinction between compositional and non-compositional adjective-noun pairs by using pretrained language models and comparing this with individual word embeddings. Our results show that the use of these models significantly improves over baseline linguistic features, however the combination with linguistic features still further improves the results, suggesting the strength of a hybrid approach",
    "volume": "workshop",
    "checked": true,
    "id": "f4b0479e4d57650da1e0ebf4992185bf87f394af",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5117": {
    "title": "Neural Lemmatization of Multiword Expressions",
    "abstract": "This article focuses on the lemmatization of multiword expressions (MWEs). We propose a deep encoder-decoder architecture generating for every MWE word its corresponding part in the lemma, based on the internal context of the MWE. The encoder relies on recurrent networks based on (1) the character sequence of the individual words to capture their morphological properties, and (2) the word sequence of the MWE to capture lexical and syntactic properties. The decoder in charge of generating the corresponding part of the lemma for each word of the MWE is based on a classical character-level attention-based recurrent model. Our model is evaluated for Italian, French, Polish and Portuguese and shows good performances except for Polish",
    "volume": "workshop",
    "checked": true,
    "id": "e92271f13dd52b3f1f27564720e97f6327cde516",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-5118": {
    "title": "Evaluating Automatic Term Extraction Methods on Individual Documents",
    "abstract": "Automatic Term Extraction (ATE) extracts terminology from domain-specific corpora. ATE is used in many NLP tasks, including Computer Assisted Translation, where it is typically applied to individual documents rather than the entire corpus. While corpus-level ATE has been extensively evaluated, it is not obvious how the results transfer to document-level ATE. To fill this gap, we evaluate 16 state-of-the-art ATE methods on full-length documents from three different domains, on both corpus and document levels. Unlike existing studies, our evaluation is more realistic as we take into account all gold terms. We show that no single method is best in corpus-level ATE, but C-Value and KeyConceptRelatendess surpass others in document-level ATE",
    "volume": "workshop",
    "checked": true,
    "id": "d08bbd67773650ce2f188b8544b168ef6cc26e57",
    "citation_count": 16
  },
  "https://aclanthology.org/W19-5119": {
    "title": "Cross-lingual Transfer Learning and Multitask Learning for Capturing Multiword Expressions",
    "abstract": "Recent developments in deep learning have prompted a surge of interest in the application of multitask and transfer learning to NLP problems. In this study, we explore for the first time, the application of transfer learning (TRL) and multitask learning (MTL) to the identification of Multiword Expressions (MWEs). For MTL, we exploit the shared syntactic information between MWE and dependency parsing models to jointly train a single model on both tasks. We specifically predict two types of labels: MWE and dependency parse. Our neural MTL architecture utilises the supervision of dependency parsing in lower layers and predicts MWE tags in upper layers. In the TRL scenario, we overcome the scarcity of data by learning a model on a larger MWE dataset and transferring the knowledge to a resource-poor setting in another language. In both scenarios, the resulting models achieved higher performance compared to standard neural approaches",
    "volume": "workshop",
    "checked": true,
    "id": "fa769bcca7d195add635409d1c1abedba0561967",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-5120": {
    "title": "Ilfhocail: A Lexicon of Irish MWEs",
    "abstract": "This paper describes the categorisation of Irish MWEs, and the construction of the first version of a lexicon of Irish MWEs for NLP purposes (Ilfhocail, meaning 'Multiwords'), collected from a number of resources. For the purposes of quality assurance, 530 entries of this lexicon were examined and manually annotated for POS information and MWE category",
    "volume": "workshop",
    "checked": true,
    "id": "12aa959cd4c64c1dffdd054791fb72231227fc29",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5121": {
    "title": "The Impact of Word Representations on Sequential Neural MWE Identification",
    "abstract": "Recent initiatives such as the PARSEME shared task allowed the rapid development of MWE identification systems. Many of those are based on recent NLP advances, using neural sequence models that take continuous word representations as input. We study two related questions in neural MWE identification: (a) the use of lemmas and/or surface forms as input features, and (b) the use of word-based or character-based embeddings to represent them. Our experiments on Basque, French, and Polish show that character-based representations yield systematically better results than word-based ones. In some cases, character-based representations of surface forms can be used as a proxy for lemmas, depending on the morphological complexity of the language",
    "volume": "workshop",
    "checked": true,
    "id": "19bafcb1052408fe124a73ab9f204265341284a1",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5201": {
    "title": "Saliency-driven Word Alignment Interpretation for Neural Machine Translation",
    "abstract": "Despite their original goal to jointly learn to align and translate, Neural Machine Translation (NMT) models, especially Transformer, are often perceived as not learning interpretable word alignments. In this paper, we show that NMT models do learn interpretable word alignments, which could only be revealed with proper interpretation methods. We propose a series of such methods that are model-agnostic, are able to be applied either offline or online, and do not require parameter update or architectural change. We show that under the force decoding setup, the alignments induced by our interpretation method are of better quality than fast-align for some systems, and when performing free decoding, they agree well with the alignments induced by automatic alignment tools",
    "volume": "workshop",
    "checked": true,
    "id": "242fd8725ec6d30a3c8648a4f9ffe9f2cf67ae3f",
    "citation_count": 37
  },
  "https://aclanthology.org/W19-5202": {
    "title": "Improving Zero-shot Translation with Language-Independent Constraints",
    "abstract": "An important concern in training multilingual neural machine translation (NMT) is to translate between language pairs unseen during training, i.e zero-shot translation. Improving this ability kills two birds with one stone by providing an alternative to pivot translation which also allows us to better understand how the model captures information between languages. In this work, we carried out an investigation on this capability of the multilingual NMT models. First, we intentionally create an encoder architecture which is independent with respect to the source language. Such experiments shed light on the ability of NMT encoders to learn multilingual representations, in general. Based on such proof of concept, we were able to design regularization methods into the standard Transformer model, so that the whole architecture becomes more robust in zero-shot conditions. We investigated the behaviour of such models on the standard IWSLT 2017 multilingual dataset. We achieved an average improvement of 2.23 BLEU points across 12 language pairs compared to the zero-shot performance of a state-of-the-art multilingual system. Additionally, we carry out further experiments in which the effect is confirmed even for language pairs with multiple intermediate pivots",
    "volume": "workshop",
    "checked": true,
    "id": "16382bcb902b180faeb623d9b541e4cfd1631248",
    "citation_count": 37
  },
  "https://aclanthology.org/W19-5203": {
    "title": "Incorporating Source Syntax into Transformer-Based Neural Machine Translation",
    "abstract": "Transformer-based neural machine translation (NMT) has recently achieved state-of-the-art performance on many machine translation tasks. However, recent work (Raganato and Tiedemann, 2018; Tang et al., 2018; Tran et al., 2018) has indicated that Transformer models may not learn syntactic structures as well as their recurrent neural network-based counterparts, particularly in low-resource cases. In this paper, we incorporate constituency parse information into a Transformer NMT model. We leverage linearized parses of the source training sentences in order to inject syntax into the Transformer architecture without modifying it. We introduce two methods: a multi-task machine translation and parsing model with a single encoder and decoder, and a mixed encoder model that learns to translate directly from parsed and unparsed source sentences. We evaluate our methods on low-resource translation from English into twenty target languages, showing consistent improvements of 1.3 BLEU on average across diverse target languages for the multi-task technique. We further evaluate the models on full-scale WMT tasks, finding that the multi-task model aids low- and medium-resource NMT but degenerates high-resource English-German translation",
    "volume": "workshop",
    "checked": true,
    "id": "0f4a3b7f835737c7e004073f4a873e356c0063b4",
    "citation_count": 34
  },
  "https://aclanthology.org/W19-5204": {
    "title": "APE at Scale and Its Implications on MT Evaluation Biases",
    "abstract": "In this work, we train an Automatic Post-Editing (APE) model and use it to reveal biases in standard MT evaluation procedures. The goal of our APE model is to correct typical errors introduced by the translation process, and convert the \"translationese\" output into natural text. Our APE model is trained entirely on monolingual data that has been round-trip translated through English, to mimic errors that are similar to the ones introduced by NMT. We apply our model to the output of existing NMT systems, and demonstrate that, while the human-judged quality improves in all cases, BLEU scores drop with forward-translated test sets. We verify these results for the WMT18 English to German, WMT15 English to French, and WMT16 English to Romanian tasks. Furthermore, we selectively apply our APE model on the output of the top submissions of the most recent WMT evaluation campaigns. We see quality improvements on all tasks of up to 2.5 BLEU points",
    "volume": "workshop",
    "checked": true,
    "id": "02c7212e234b3bf4fa28f6eaac757eccf5bbd866",
    "citation_count": 43
  },
  "https://aclanthology.org/W19-5205": {
    "title": "Generalizing Back-Translation in Neural Machine Translation",
    "abstract": "Back-translation — data augmentation by translating target monolingual data — is a crucial component in modern neural machine translation (NMT). In this work, we reformulate back-translation in the scope of cross-entropy optimization of an NMT model, clarifying its underlying mathematical assumptions and approximations beyond its heuristic usage. Our formulation covers broader synthetic data generation schemes, including sampling from a target-to-source NMT model. With this formulation, we point out fundamental problems of the sampling-based approaches and propose to remedy them by (i) disabling label smoothing for the target-to-source model and (ii) sampling from a restricted search space. Our statements are investigated on the WMT 2018 German <-> English news translation task",
    "volume": "workshop",
    "checked": true,
    "id": "9a127a2903fb3dff2a480e82dd18fcf331333caa",
    "citation_count": 23
  },
  "https://aclanthology.org/W19-5206": {
    "title": "Tagged Back-Translation",
    "abstract": "Recent work in Neural Machine Translation (NMT) has shown significant quality gains from noised-beam decoding during back-translation, a method to generate synthetic parallel data. We show that the main role of such synthetic noise is not to diversify the source side, as previously suggested, but simply to indicate to the model that the given source is synthetic. We propose a simpler alternative to noising techniques, consisting of tagging back-translated source sentences with an extra token. Our results on WMT outperform noised back-translation in English-Romanian and match performance on English-German, redefining the state-of-the-art on the former",
    "volume": "workshop",
    "checked": true,
    "id": "6e722ac4d386489aa47703887881835ec0e1331d",
    "citation_count": 137
  },
  "https://aclanthology.org/W19-5207": {
    "title": "Hierarchical Document Encoder for Parallel Corpus Mining",
    "abstract": "We explore using multilingual document embeddings for nearest neighbor mining of parallel data. Three document-level representations are investigated: (i) document embeddings generated by simply averaging multilingual sentence embeddings; (ii) a neural bag-of-words (BoW) document encoding model; (iii) a hierarchical multilingual document encoder (HiDE) that builds on our sentence-level model. The results show document embeddings derived from sentence-level averaging are surprisingly effective for clean datasets, but suggest models trained hierarchically at the document-level are more effective on noisy data. Analysis experiments demonstrate our hierarchical models are very robust to variations in the underlying sentence embedding quality. Using document embeddings trained with HiDE achieves the state-of-the-art on United Nations (UN) parallel document mining, 94.9% P@1 for en-fr and 97.3% P@1 for en-es",
    "volume": "workshop",
    "checked": true,
    "id": "c3d21675b1cfa68526515776821800c5840e5f87",
    "citation_count": 18
  },
  "https://aclanthology.org/W19-5208": {
    "title": "The Effect of Translationese in Machine Translation Test Sets",
    "abstract": "The effect of translationese has been studied in the field of machine translation (MT), mostly with respect to training data. We study in depth the effect of translationese on test data, using the test sets from the last three editions of WMT's news shared task, containing 17 translation directions. We show evidence that (i) the use of translationese in test sets results in inflated human evaluation scores for MT systems; (ii) in some cases system rankings do change and (iii) the impact translationese has on a translation direction is inversely correlated to the translation quality attainable by state-of-the-art MT systems for that direction",
    "volume": "workshop",
    "checked": true,
    "id": "8573ea42ed7f13f4d7bc7ba371b462e3cc5fa88f",
    "citation_count": 54
  },
  "https://aclanthology.org/W19-5209": {
    "title": "Customizing Neural Machine Translation for Subtitling",
    "abstract": "In this work, we customized a neural machine translation system for translation of subtitles in the domain of entertainment. The neural translation model was adapted to the subtitling content and style and extended by a simple, yet effective technique for utilizing inter-sentence context for short sentences such as dialog turns. The main contribution of the paper is a novel subtitle segmentation algorithm that predicts the end of a subtitle line given the previous word-level context using a recurrent neural network learned from human segmentation decisions. This model is combined with subtitle length and duration constraints established in the subtitling industry. We conducted a thorough human evaluation with two post-editors (English-to-Spanish translation of a documentary and a sitcom). It showed a notable productivity increase of up to 37% as compared to translating from scratch and significant reductions in human translation edit rate in comparison with the post-editing of the baseline non-adapted system without a learned segmentation model",
    "volume": "workshop",
    "checked": true,
    "id": "aa00f8b26ac57f72e6552d112514218567cc8c6e",
    "citation_count": 28
  },
  "https://aclanthology.org/W19-5210": {
    "title": "Integration of Dubbing Constraints into Machine Translation",
    "abstract": "Translation systems aim to perform a meaning-preserving conversion of linguistic material (typically text but also speech) from a source to a target language (and, to a lesser degree, the corresponding socio-cultural contexts). Dubbing, i.e., the lip-synchronous translation and revoicing of speech adds to this constraints about the close matching of phonetic and resulting visemic synchrony characteristics of source and target material. There is an inherent conflict between a translation's meaning preservation and 'dubbability' and the resulting trade-off can be controlled by weighing the synchrony constraints. We introduce our work, which to the best of our knowledge is the first of its kind, on integrating synchrony constraints into the machine translation paradigm. We present first results for the integration of synchrony constraints into encoder decoder-based neural machine translation and show that considerably more 'dubbable' translations can be achieved with only a small impact on BLEU score, and dubbability improves more steeply than BLEU degrades",
    "volume": "workshop",
    "checked": true,
    "id": "2004b74c89d57faee4fb2d3cdad9900d6f6b329b",
    "citation_count": 18
  },
  "https://aclanthology.org/W19-5211": {
    "title": "Widening the Representation Bottleneck in Neural Machine Translation with Lexical Shortcuts",
    "abstract": "The transformer is a state-of-the-art neural translation model that uses attention to iteratively refine lexical representations with information drawn from the surrounding context. Lexical features are fed into the first layer and propagated through a deep network of hidden layers. We argue that the need to represent and propagate lexical features in each layer limits the model's capacity for learning and representing other information relevant to the task. To alleviate this bottleneck, we introduce gated shortcut connections between the embedding layer and each subsequent layer within the encoder and decoder. This enables the model to access relevant lexical content dynamically, without expending limited resources on storing it within intermediate states. We show that the proposed modification yields consistent improvements over a baseline transformer on standard WMT translation tasks in 5 translation directions (0.9 BLEU on average) and reduces the amount of lexical information passed along the hidden layers. We furthermore evaluate different ways to integrate lexical connections into the transformer architecture and present ablation experiments exploring the effect of proposed shortcuts on model behavior",
    "volume": "workshop",
    "checked": true,
    "id": "562464064d3fb1c5118be27ed1f37c7478eb589b",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-5212": {
    "title": "A High-Quality Multilingual Dataset for Structured Documentation Translation",
    "abstract": "This paper presents a high-quality multilingual dataset for the documentation domain to advance research on localization of structured text. Unlike widely-used datasets for translation of plain text, we collect XML-structured parallel text segments from the online documentation for an enterprise software platform. These Web pages have been professionally translated from English into 16 languages and maintained by domain experts, and around 100,000 text segments are available for each language pair. We build and evaluate translation models for seven target languages from English, with several different copy mechanisms and an XML-constrained beam search. We also experiment with a non-English pair to show that our dataset has the potential to explicitly enable 17 × 16 translation settings. Our experiments show that learning to translate with the XML tags improves translation accuracy, and the beam search accurately generates XML structures. We also discuss trade-offs of using the copy mechanisms by focusing on translation of numerical words and named entities. We further provide a detailed human analysis of gaps between the model output and human translations for real-world applications, including suitability for post-editing",
    "volume": "workshop",
    "checked": true,
    "id": "6cba18bd36967eefa35a03b8f26c9664bbc11c02",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-5301": {
    "title": "Findings of the 2019 Conference on Machine Translation (WMT19)",
    "abstract": "This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. The task was also opened up to additional test suites to probe specific aspects of translation",
    "volume": "workshop",
    "checked": true,
    "id": "ea3e18c7b10a137d495054682c055a80b5be768c",
    "citation_count": 345
  },
  "https://aclanthology.org/W19-5302": {
    "title": "Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges",
    "abstract": "This paper presents the results of the WMT19 Metrics Shared Task. Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with automatic metrics. 13 research groups submitted 24 metrics, 10 of which are reference-less \"metrics\" and constitute submissions to the joint task with WMT19 Quality Estimation Task, \"QE as a Metric\". In addition, we computed 11 baseline metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given metric correlates with the WMT19 official manual ranking, and segment level, how well the metric correlates with human judgements of segment quality. This year, we use direct assessment (DA) as our only form of manual evaluation",
    "volume": "workshop",
    "checked": true,
    "id": "37c035a8e6ad1da2a8b22d5f2951f01f1f309632",
    "citation_count": 110
  },
  "https://aclanthology.org/W19-5303": {
    "title": "Findings of the First Shared Task on Machine Translation Robustness",
    "abstract": "We share the findings of the first shared task on improving robustness of Machine Translation (MT). The task provides a testbed representing challenges facing MT models deployed in the real world, and facilitates new approaches to improve models' robustness to noisy input and domain mismatch. We focus on two language pairs (English-French and English-Japanese), and the submitted systems are evaluated on a blind test set consisting of noisy comments on Reddit and professionally sourced translations. As a new task, we received 23 submissions by 11 participating teams from universities, companies, national labs, etc. All submitted systems achieved large improvements over baselines, with the best improvement having +22.33 BLEU. We evaluated submissions by both human judgment and automatic evaluation (BLEU), which shows high correlations (Pearson's r = 0.94 and 0.95). Furthermore, we conducted a qualitative analysis of the submitted systems using compare-mt, which revealed their salient differences in handling challenges in this task. Such analysis provides additional insights when there is occasional disagreement between human judgment and BLEU, e.g. systems better at producing colloquial expressions received higher score from human judgment",
    "volume": "workshop",
    "checked": true,
    "id": "a5690b0a514a7cbc913871e41e54c9ad4f6362db",
    "citation_count": 47
  },
  "https://aclanthology.org/W19-5304": {
    "title": "The University of Edinburgh's Submissions to the WMT19 News Translation Task",
    "abstract": "The University of Edinburgh participated in the WMT19 Shared Task on News Translation in six language directions: English↔Gujarati, English↔Chinese, German→English, and English→Czech. For all translation directions, we created or used back-translations of monolingual data in the target language as additional synthetic training data. For English↔Gujarati, we also explored semi-supervised MT with cross-lingual language model pre-training, and translation pivoting through Hindi. For translation to and from Chinese, we investigated character-based tokenisation vs. sub-word segmentation of Chinese text. For German→English, we studied the impact of vast amounts of back-translated training data on translation quality, gaining a few additional insights over Edunov et al. (2018). For English→Czech, we compared different preprocessing and tokenisation regimes",
    "volume": "workshop",
    "checked": true,
    "id": "52b30820d67ea4f6a2f51bac0c90596bace53771",
    "citation_count": 24
  },
  "https://aclanthology.org/W19-5305": {
    "title": "GTCOM Neural Machine Translation Systems for WMT19",
    "abstract": "This paper describes the Global Tone Communication Co., Ltd.'s submission of the WMT19 shared news translation task. We participate in six directions: English to (Gujarati, Lithuanian and Finnish) and (Gujarati, Lithuanian and Finnish) to English. Further, we get the best BLEU scores in the directions of English to Gujarati and Lithuanian to English (28.2 and 36.3 respectively) among all the participants. The submitted systems mainly focus on back-translation, knowledge distillation and reranking to build a competitive model for this task. Also, we apply language model to filter monolingual data, back-translated data and parallel data. The techniques we apply for data filtering include filtering by rules, language models. Besides, We conduct several experiments to validate different knowledge distillation techniques and right-to-left (R2L) reranking",
    "volume": "workshop",
    "checked": true,
    "id": "9cebcd4f372dd272c8b007b6f34e2e220cde8701",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-5306": {
    "title": "Machine Translation with parfda, Moses, kenlm, nplm, and PRO",
    "abstract": "We build parfda Moses statistical machine translation (SMT) models for most language pairs in the news translation task. We experiment with a hybrid approach using neural language models integrated into Moses. We obtain the constrained data statistics on the machine translation task, the coverage of the test sets, and the upper bounds on the translation results. We also contribute a new testsuite for the German-English language pair and a new automated key phrase extraction technique for the evaluation of the testsuite translations",
    "volume": "workshop",
    "checked": true,
    "id": "203fda58660c40fb879d64830f1e4ae7563b1261",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5307": {
    "title": "LIUM's Contributions to the WMT2019 News Translation Task: Data and Systems for German-French Language Pairs",
    "abstract": "This paper describes the neural machine translation (NMT) systems of the LIUM Laboratory developed for the French↔German news translation task of the Fourth Conference onMachine Translation (WMT 2019). The chosen language pair is included for the first time in the WMT news translation task. We de-scribe how the training and the evaluation data was created. We also present our participation in the French↔German translation directions using self-attentional Transformer networks with small and big architectures",
    "volume": "workshop",
    "checked": true,
    "id": "700978a1f36a5f4b8d46a212985195b895b4906c",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5308": {
    "title": "The University of Maryland's Kazakh-English Neural Machine Translation System at WMT19",
    "abstract": "This paper describes the University of Maryland's submission to the WMT 2019 Kazakh-English news translation task. We study the impact of transfer learning from another low-resource but related language. We experiment with different ways of encoding lexical units to maximize lexical overlap between the two language pairs, as well as back-translation and ensembling. The submitted system improves over a Kazakh-only baseline by +5.45 BLEU on newstest2019",
    "volume": "workshop",
    "checked": true,
    "id": "dc9db100e9b6482ae4a8959de06d3c75973f0974",
    "citation_count": 11
  },
  "https://aclanthology.org/W19-5309": {
    "title": "DBMS-KU Interpolation for WMT19 News Translation Task",
    "abstract": "This paper presents the participation of DBMS-KU Interpolation system in WMT19 shared task, namely, Kazakh-English language pair. We examine the use of interpolation method using a different language model order. Our Interpolation system combines a direct translation with Russian as a pivot language. We use 3-gram and 5-gram language model orders to perform the language translation in this work. To reduce noise in the pivot translation process, we prune the phrase table of source-pivot and pivot-target. Our experimental results show that our Interpolation system outperforms the Baseline in terms of BLEU-cased score by +0.5 and +0.1 points in Kazakh-English and English-Kazakh, respectively. In particular, using the 5-gram language model order in our system could obtain better BLEU-cased score than utilizing the 3-gram one. Interestingly, we found that by employing the Interpolation system could reduce the perplexity score of English-Kazakh when using 3-gram language model order",
    "volume": "workshop",
    "checked": true,
    "id": "59b2e10d0757d21aa54254a74360c1d26110b1db",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5310": {
    "title": "Lingua Custodia at WMT'19: Attempts to Control Terminology",
    "abstract": "This paper describes Lingua Custodia's submission to the WMT'19 news shared task for German-to-French on the topic of the EU elections. We report experiments on the adaptation of the terminology of a machine translation system to a specific topic, aimed at providing more accurate translations of specific entities like political parties and person names, given that the shared task provided no in-domain training parallel data dealing with the restricted topic. Our primary submission to the shared task uses backtranslation generated with a type of decoding allowing the insertion of constraints in the output in order to guarantee the correct translation of specific terms that are not necessarily observed in the data",
    "volume": "workshop",
    "checked": true,
    "id": "adb83557ed5095c7c48b1082e8f49ecf0fb90adb",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-5311": {
    "title": "The TALP-UPC Machine Translation Systems for WMT19 News Translation Task: Pivoting Techniques for Low Resource MT",
    "abstract": "In this article, we describe the TALP-UPC research group participation in the WMT19 news translation shared task for Kazakh-English. Given the low amount of parallel training data, we resort to using Russian as pivot language, training subword-based statistical translation systems for Russian-Kazakh and Russian-English that were then used to create two synthetic pseudo-parallel corpora for Kazakh-English and English-Kazakh respectively. Finally, a self-attention model based on the decoder part of the Transformer architecture was trained on the two pseudo-parallel corpora",
    "volume": "workshop",
    "checked": true,
    "id": "0748409634261095d4edfac664e057eede1d00cc",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-5312": {
    "title": "Kyoto University Participation to the WMT 2019 News Shared Task",
    "abstract": "We describe here the experiments we did for the the news translation shared task of WMT 2019. We focused on the new German-to-French language direction, and mostly used current standard approaches to develop a Neural Machine Translation system. We make use of the Tensor2Tensor implementation of the Transformer model. After carefully cleaning the data and noting the importance of the good use of recent monolingual data for the task, we obtain our final result by combining the output of a diverse set of trained models through the use of their \"checkpoint agreement\"",
    "volume": "workshop",
    "checked": true,
    "id": "76dd9a7e01038508404f6ab53ebb7199894722d9",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5313": {
    "title": "NICT's Supervised Neural Machine Translation Systems for the WMT19 News Translation Task",
    "abstract": "In this paper, we describe our supervised neural machine translation (NMT) systems that we developed for the news translation task for Kazakh↔English, Gujarati↔English, Chinese↔English, and English→Finnish translation directions. We focused on leveraging multilingual transfer learning and back-translation for the extremely low-resource language pairs: Kazakh↔English and Gujarati↔English translation. For the Chinese↔English translation, we used the provided parallel data augmented with a large quantity of back-translated monolingual data to train state-of-the-art NMT systems. We then employed techniques that have been proven to be most effective, such as back-translation, fine-tuning, and model ensembling, to generate the primary submissions of Chinese↔English. For English→Finnish, our submission from WMT18 remains a strong baseline despite the increase in parallel corpora for this year's task",
    "volume": "workshop",
    "checked": true,
    "id": "f8e6ab501c47370dfddd27300150ae3ad680f784",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-5314": {
    "title": "The University of Sydney's Machine Translation System for WMT19",
    "abstract": "This paper describes the University of Sydney's submission of the WMT 2019 shared news translation task. We participated in the Finnish->English direction and got the best BLEU(33.0) score among all the participants. Our system is based on the self-attentional Transformer networks, into which we integrated the most recent effective strategies from academic research (e.g., BPE, back translation, multi-features data selection, data augmentation, greedy model ensemble, reranking, ConMBR system combination, and postprocessing). Furthermore, we propose a novel augmentation method Cycle Translation and a data mixture strategy Big/Small parallel construction to entirely exploit the synthetic corpus. Extensive experiments show that adding the above techniques can make continuous improvements of the BLEU scores, and the best result outperforms the baseline (Transformer ensemble model trained with the original parallel corpus) by approximately 5.3 BLEU score, achieving the state-of-the-art performance",
    "volume": "workshop",
    "checked": true,
    "id": "8cd26ab79faf1d596376b06fac7f565e37e04afa",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-5315": {
    "title": "UdS-DFKI Participation at WMT 2019: Low-Resource (en-gu) and Coreference-Aware (en-de) Systems",
    "abstract": "This paper describes the UdS-DFKI submission to the WMT2019 news translation task for Gujarati–English (low-resourced pair) and German–English (document-level evaluation). Our systems rely on the on-line extraction of parallel sentences from comparable corpora for the first scenario and on the inclusion of coreference-related information in the training data in the second one",
    "volume": "workshop",
    "checked": true,
    "id": "07189a48bcc33085827fde2a2c8ddb300a08b23f",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-5316": {
    "title": "The IIIT-H Gujarati-English Machine Translation System for WMT19",
    "abstract": "This paper describes the Neural Machine Translation system of IIIT-Hyderabad for the Gujarati→English news translation shared task of WMT19. Our system is basedon encoder-decoder framework with attention mechanism. We experimented with Multilingual Neural MT models. Our experiments show that Multilingual Neural Machine Translation leveraging parallel data from related language pairs helps in significant BLEU improvements upto 11.5, for low resource language pairs like Gujarati-English",
    "volume": "workshop",
    "checked": true,
    "id": "9611147f450d5e1ab5ff2b0726160230a4972d8e",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5317": {
    "title": "Kingsoft's Neural Machine Translation System for WMT19",
    "abstract": "This paper describes the Kingsoft AI Lab's submission to the WMT2019 news translation shared task. We participated in two language directions: English-Chinese and Chinese-English. For both language directions, we trained several variants of Transformer models using the provided parallel data enlarged with a large quantity of back-translated monolingual data. The best translation result was obtained with ensemble and reranking techniques. According to automatic metrics (BLEU) our Chinese-English system reached the second highest score, and our English-Chinese system reached the second highest score for this subtask",
    "volume": "workshop",
    "checked": true,
    "id": "7081667424ea92e41f245e3317e4597388c5fa40",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5318": {
    "title": "The AFRL WMT19 Systems: Old Favorites and New Tricks",
    "abstract": "This paper describes the Air Force Research Laboratory (AFRL) machine translation systems and the improvements that were developed during the WMT19 evaluation campaign. This year, we refine our approach to training popular neural machine translation toolkits, experiment with a new domain adaptation technique and again measure improvements in performance on the Russian–English language pair",
    "volume": "workshop",
    "checked": true,
    "id": "f11b902a56d237030293520f655eeb004d2656a9",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-5319": {
    "title": "Evaluating the Supervised and Zero-shot Performance of Multi-lingual Translation Models",
    "abstract": "We study several methods for full or partial sharing of the decoder parameters of multi-lingual NMT models. Using only the WMT 2019 shared task parallel datasets for training, we evaluate both fully supervised and zero-shot translation performance in 110 unique translation directions. We use additional test sets and re-purpose evaluation methods recently used for unsupervised MT in order to evaluate zero-shot translation performance for language pairs where no gold-standard parallel data is available. To our knowledge, this is the largest evaluation of multi-lingual translation yet conducted in terms of the total size of the training data we use, and in terms of the number of zero-shot translation pairs we evaluate. We conduct an in-depth evaluation of the translation performance of different models, highlighting the trade-offs between methods of sharing decoder parameters. We find that models which have task-specific decoder parameters outperform models where decoder parameters are fully shared across all tasks",
    "volume": "workshop",
    "checked": true,
    "id": "42dee8c12ce505b2a8ed87a4ef293c46b820b7a8",
    "citation_count": 11
  },
  "https://aclanthology.org/W19-5320": {
    "title": "The MLLP-UPV Supervised Machine Translation Systems for WMT19 News Translation Task",
    "abstract": "This paper describes the participation of the MLLP research group of the Universitat Politècnica de València in the WMT 2019 News Translation Shared Task. In this edition, we have submitted systems for the German ↔ English and German ↔ French language pairs, participating in both directions of each pair. Our submitted systems, based on the Transformer architecture, make ample use of data filtering, synthetic data and domain adaptation through fine-tuning",
    "volume": "workshop",
    "checked": true,
    "id": "d1022ad1015bf87976336ee059de520341a788b5",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-5321": {
    "title": "Microsoft Translator at WMT 2019: Towards Large-Scale Document-Level Neural Machine Translation",
    "abstract": "This paper describes the Microsoft Translator submissions to the WMT19 news translation shared task for English-German. Our main focus is document-level neural machine translation with deep transformer models. We start with strong sentence-level baselines, trained on large-scale data created via data-filtering and noisy back-translation and find that back-translation seems to mainly help with translationese input. We explore fine-tuning techniques, deeper models and different ensembling strategies to counter these effects. Using document boundaries present in the authentic and synthetic parallel data, we create sequences of up to 1000 subword segments and train transformer translation models. We experiment with data augmentation techniques for the smaller authentic data with document-boundaries and for larger authentic data without boundaries. We further explore multi-task training for the incorporation of document-level source language monolingual data via the BERT-objective on the encoder and two-pass decoding for combinations of sentence-level and document-level systems. Based on preliminary human evaluation results, evaluators strongly prefer the document-level systems over our comparable sentence-level system. The document-level systems also seem to score higher than the human references in source-based direct assessment",
    "volume": "workshop",
    "checked": true,
    "id": "921b18ac92ce6b43d2e487e1c4a7c16c1d7b3398",
    "citation_count": 97
  },
  "https://aclanthology.org/W19-5322": {
    "title": "CUNI Submission for Low-Resource Languages in WMT News 2019",
    "abstract": "This paper describes the CUNI submission to the WMT 2019 News Translation Shared Task for the low-resource languages: Gujarati-English and Kazakh-English. We participated in both language pairs in both translation directions. Our system combines transfer learning from a different high-resource language pair followed by training on backtranslated monolingual data. Thanks to the simultaneous training in both directions, we can iterate the backtranslation process. We are using the Transformer model in a constrained submission",
    "volume": "workshop",
    "checked": true,
    "id": "83a6638ab8f2d97098dcfb7e3147bb82d94fc8d8",
    "citation_count": 15
  },
  "https://aclanthology.org/W19-5323": {
    "title": "CUNI Systems for the Unsupervised News Translation Task in WMT 2019",
    "abstract": "In this paper we describe the CUNI translation system used for the unsupervised news shared task of the ACL 2019 Fourth Conference on Machine Translation (WMT19). We follow the strategy of Artetxe ae at. (2018b), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel data. The synthetic corpus was produced from a monolingual corpus by a tuned PBMT model refined through iterative back-translation. We further focus on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffers most. Our system reaches a BLEU score of 15.3 on the German-Czech WMT19 shared task",
    "volume": "workshop",
    "checked": true,
    "id": "a763df05a150bb48c55e7d57f7fdf0243dc14e17",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5324": {
    "title": "A Comparison on Fine-grained Pre-trained Embeddings for the WMT19Chinese-English News Translation Task",
    "abstract": "This paper describes our submission to the WMT 2019 Chinese-English (zh-en) news translation shared task. Our systems are based on RNN architectures with pre-trained embeddings which utilize character and sub-character information. We compare models with these different granularity levels using different evaluating metics. We find that a finer granularity embeddings can help the model according to character level evaluation and that the pre-trained embeddings can also be beneficial for model performance marginally when the training data is limited",
    "volume": "workshop",
    "checked": true,
    "id": "50aa2393fbfa89e93bc892045e5b6a1f186d0fc5",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5325": {
    "title": "The NiuTrans Machine Translation Systems for WMT19",
    "abstract": "This paper described NiuTrans neural machine translation systems for the WMT 2019 news translation tasks. We participated in 13 translation directions, including 11 supervised tasks, namely EN↔{ZH, DE, RU, KK, LT}, GU→EN and the unsupervised DE↔CS sub-track. Our systems were built on Deep Transformer and several back-translation methods. Iterative knowledge distillation and ensemble+reranking were also employed to obtain stronger models. Our unsupervised submissions were based on NMT enhanced by SMT. As a result, we achieved the highest BLEU scores in {KK↔EN, GU→EN} directions, ranking 2nd in {RU→EN, DE↔CS} and 3rd in {ZH→EN, LT→EN, EN→RU, EN↔DE} among all constrained submissions",
    "volume": "workshop",
    "checked": true,
    "id": "392ba7ae3f387f83cff7252dbc4ff14669ba4d47",
    "citation_count": 46
  },
  "https://aclanthology.org/W19-5326": {
    "title": "Multi-Source Transformer for Kazakh-Russian-English Neural Machine Translation",
    "abstract": "We describe the neural machine translation (NMT) system developed at the National Research Council of Canada (NRC) for the Kazakh-English news translation task of the Fourth Conference on Machine Translation (WMT19). Our submission is a multi-source NMT taking both the original Kazakh sentence and its Russian translation as input for translating into English",
    "volume": "workshop",
    "checked": true,
    "id": "c9edfb7104c15c3e657461ad3fbb66fcc6c55c48",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-5327": {
    "title": "Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring",
    "abstract": "This paper describes CAiRE's submission to the unsupervised machine translation track of the WMT'19 news shared task from German to Czech. We leverage a phrase-based statistical machine translation (PBSMT) model and a pre-trained language model to combine word-level neural machine translation (NMT) and subword-level NMT models without using any parallel data. We propose to solve the morphological richness problem of languages by training byte-pair encoding (BPE) embeddings for German and Czech separately, and they are aligned using MUSE (Conneau et al., 2018). To ensure the fluency and consistency of translations, a rescoring mechanism is proposed that reuses the pre-trained language model to select the translation candidates generated through beam search. Moreover, a series of pre-processing and post-processing approaches are applied to improve the quality of final translations",
    "volume": "workshop",
    "checked": true,
    "id": "49e150e6895124254b0b3d6ce722d0ca09fd9633",
    "citation_count": 15
  },
  "https://aclanthology.org/W19-5328": {
    "title": "JUMT at WMT2019 News Translation Task: A Hybrid Approach to Machine Translation for Lithuanian to English",
    "abstract": "In the current work, we present a description of the system submitted to WMT 2019 News Translation Shared task. The system was created to translate news text from Lithuanian to English. To accomplish the given task, our system used a Word Embedding based Neural Machine Translation model to post edit the outputs generated by a Statistical Machine Translation model. The current paper documents the architecture of our model, descriptions of the various modules and the results produced using the same. Our system garnered a BLEU score of 17.6",
    "volume": "workshop",
    "checked": true,
    "id": "8a93da17952f1a309b07769d424aaa3a67a3a792",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5329": {
    "title": "Johns Hopkins University Submission for WMT News Translation Task",
    "abstract": "We describe the work of Johns Hopkins University for the shared task of news translation organized by the Fourth Conference on Machine Translation (2019). We submitted systems for both directions of the English-German language pair. The systems combine multiple techniques – sampling, filtering, iterative backtranslation, and continued training – previously used to improve performance of neural machine translation models. At submission time, we achieve a BLEU score of 38.1 for De-En and 42.5 for En-De translation directions on newstest2019. Post-submission, the score is 38.4 for De-En and 42.8 for En-De. Various experiments conducted in the process are also described",
    "volume": "workshop",
    "checked": true,
    "id": "2b6e639b00bcd7765ebdad2a84ceae35b756fdc4",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-5330": {
    "title": "NICT's Unsupervised Neural and Statistical Machine Translation Systems for the WMT19 News Translation Task",
    "abstract": "This paper presents the NICT's participation in the WMT19 unsupervised news translation task. We participated in the unsupervised translation direction: German-Czech. Our primary submission to the task is the result of a simple combination of our unsupervised neural and statistical machine translation systems. Our system is ranked first for the German-to-Czech translation task, using only the data provided by the organizers (\"constraint'\"), according to both BLEU-cased and human evaluation. We also performed contrastive experiments with other language pairs, namely, English-Gujarati and English-Kazakh, to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions",
    "volume": "workshop",
    "checked": true,
    "id": "d802623e75b44b227acf33aec26a1607da2898b6",
    "citation_count": 19
  },
  "https://aclanthology.org/W19-5331": {
    "title": "PROMT Systems for WMT 2019 Shared Translation Task",
    "abstract": "This paper describes the PROMT submissions for the WMT 2019 Shared News Translation Task. This year we participated in two language pairs and in three directions: English-Russian, English-German and German-English. All our submissions are Marian-based neural systems. We use significantly more data compared to the last year. We also present our improved data filtering pipeline",
    "volume": "workshop",
    "checked": true,
    "id": "d73f2c45b42d59c3832b3dbcfd16787b7c9c5a8d",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5332": {
    "title": "JU-Saarland Submission to the WMT2019 English–Gujarati Translation Shared Task",
    "abstract": "In this paper we describe our joint submission (JU-Saarland) from Jadavpur University and Saarland University in the WMT 2019 news translation shared task for English–Gujarati language pair within the translation task sub-track. Our baseline and primary submissions are built using Recurrent neural network (RNN) based neural machine translation (NMT) system which follows attention mechanism. Given the fact that the two languages belong to different language families and there is not enough parallel data for this language pair, building a high quality NMT system for this language pair is a difficult task. We produced synthetic data through back-translation from available monolingual data. We report the translation quality of our English–Gujarati and Gujarati–English NMT systems trained at word, byte-pair and character encoding levels where RNN at word level is considered as the baseline and used for comparison purpose. Our English–Gujarati system ranked in the second position in the shared task",
    "volume": "workshop",
    "checked": true,
    "id": "747280a043336e33f20a4006466dd14f3ab47a46",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-5333": {
    "title": "Facebook FAIR's WMT19 News Translation Task Submission",
    "abstract": "This paper describes Facebook FAIR's submission to the WMT19 shared news translation task. We participate in four language directions, English <-> German and English <-> Russian in both directions. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific data, then decode using noisy channel model reranking. Our system improves on our previous system's performance by 4.5 BLEU points and achieves the best case-sensitive BLEU score for the translation direction English→Russian",
    "volume": "workshop",
    "checked": true,
    "id": "411ed32d43dce7a75836cab7d7d9820a5e1e9078",
    "citation_count": 227
  },
  "https://aclanthology.org/W19-5334": {
    "title": "eTranslation's Submissions to the WMT 2019 News Translation Task",
    "abstract": "This paper describes the submissions of the eTranslation team to the WMT 2019 news translation shared task. The systems have been developed with the aim of identifying and following rather than establishing best practices, under the constraints imposed by a low resource training and decoding environment normally used for our production systems. Thus most of the findings and results are transferable to systems used in the eTranslation service. Evaluations suggest that this approach is able to produce decent models with good performance and speed without the overhead of using prohibitively deep and complex architectures",
    "volume": "workshop",
    "checked": true,
    "id": "f33bb08bc03a0e14cc88e10e9031b6caf1c56db3",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5335": {
    "title": "Tilde's Machine Translation Systems for WMT 2019",
    "abstract": "The paper describes the development process of the Tilde's NMT systems that were submitted for the WMT 2018 shared task on news translation. We describe the data filtering and pre-processing workflows, the NMT system training architectures, and automatic evaluation results. For the WMT 2018 shared task, we submitted seven systems (both constrained and unconstrained) for English-Estonian and Estonian-English translation directions. The submitted systems were trained using Transformer models",
    "volume": "workshop",
    "checked": true,
    "id": "0a164f425031d6ee1718b86c7db7ea6a850382d5",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-5336": {
    "title": "Apertium-fin-eng–Rule-based Shallow Machine Translation for WMT 2019 Shared Task",
    "abstract": "In this paper we describe a rule-based, bi-directional machine translation system for the Finnish—English language pair. The baseline system was based on the existing data of FinnWordNet, omorfi and apertium-eng. We have built the disambiguation, lexical selection and translation rules by hand. The dictionaries and rules have been developed based on the shared task data. We describe in this article the use of the shared task data as a kind of a test-driven development workflow in RBMT development and show that it suits perfectly to a modern software engineering continuous integration workflow of RBMT and yields big increases to BLEU scores with minimal effort",
    "volume": "workshop",
    "checked": true,
    "id": "df1008e978073ec6b2f2535a8e075ba240e998ba",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5337": {
    "title": "English-Czech Systems in WMT19: Document-Level Transformer",
    "abstract": "We describe our NMT systems submitted to the WMT19 shared task in English→Czech news translation. Our systems are based on the Transformer model implemented in either Tensor2Tensor (T2T) or Marian framework. We aimed at improving the adequacy and coherence of translated documents by enlarging the context of the source and target. Instead of translating each sentence independently, we split the document into possibly overlapping multi-sentence segments. In case of the T2T implementation, this \"document-level\"-trained system achieves a +0.6 BLEU improvement (p < 0.05) relative to the same system applied on isolated sentences. To assess the potential effect document-level models might have on lexical coherence, we performed a semi-automatic analysis, which revealed only a few sentences improved in this aspect. Thus, we cannot draw any conclusions from this week evidence",
    "volume": "workshop",
    "checked": true,
    "id": "92a88ac6bd7cfab097547c94f37f31cb7747c596",
    "citation_count": 20
  },
  "https://aclanthology.org/W19-5338": {
    "title": "The RWTH Aachen University Machine Translation Systems for WMT 2019",
    "abstract": "This paper describes the neural machine translation systems developed at the RWTH Aachen University for the German-English, Chinese-English and Kazakh-English news translation tasks of the Fourth Conference on Machine Translation (WMT19). For all tasks, the final submitted system is based on the Transformer architecture. We focus on improving data filtering and fine-tuning as well as systematically evaluating interesting approaches like unigram language model segmentation and transfer learning. For the De-En task, none of the tested methods gave a significant improvement over last years winning system and we end up with the same performance, resulting in 39.6% BLEU on newstest2019. In the Zh-En task, we show 1.3% BLEU improvement over our last year's submission, which we mostly attribute to the splitting of long sentences during translation. We further report results on the Kazakh-English task where we gain improvements of 11.1% BLEU over our baseline system. On the same task we present a recent transfer learning approach, which uses half of the free parameters of our submission system and performs on par with it",
    "volume": "workshop",
    "checked": true,
    "id": "8795f02e765b24a45b729e3cd905591ddca317c1",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-5339": {
    "title": "The Universitat d'Alacant Submissions to the English-to-Kazakh News Translation Task at WMT 2019",
    "abstract": "This paper describes the two submissions of Universitat d'Alacant to the English-to-Kazakh news translation task at WMT 2019. Our submissions take advantage of monolingual data and parallel data from other language pairs by means of iterative backtranslation, pivot backtranslation and transfer learning. They also use linguistic information in two ways: morphological segmentation of Kazakh text, and integration of the output of a rule-based machine translation system. Our systems were ranked second in terms of chrF++ despite being built from an ensemble of only 2 independent training runs",
    "volume": "workshop",
    "checked": true,
    "id": "eacf2067cc763046c9ee5e2d4921411a4eb4fe4b",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-5340": {
    "title": "CUED@WMT19:EWC&LMs",
    "abstract": "Two techniques provide the fabric of the Cambridge University Engineering Department's (CUED) entry to the WMT19 evaluation campaign: elastic weight consolidation (EWC) and different forms of language modelling (LMs). We report substantial gains by finetuning very strong baselines on former WMT test sets using a combination of checkpoint averaging and EWC. A sentence-level Transformer LM and a document-level LM based on a modified Transformer architecture yield further gains. As in previous years, we also extract n-gram probabilities from SMT lattices which can be seen as a source-conditioned ngram LM",
    "volume": "workshop",
    "checked": true,
    "id": "3308a4e5f7069d8a7b4f00544f007ff50d3d7076",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-5341": {
    "title": "Baidu Neural Machine Translation Systems for WMT19",
    "abstract": "In this paper we introduce the systems Baidu submitted for the WMT19 shared task on Chinese<->English news translation. Our systems are based on the Transformer architecture with some effective improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. Our Chinese->English system achieved the highest case-sensitive BLEU score among all constrained submissions, and our English->Chinese system ranked the second in all submissions",
    "volume": "workshop",
    "checked": true,
    "id": "79cd41c6bf517fa1f2ec62d582687ed6a02a801d",
    "citation_count": 38
  },
  "https://aclanthology.org/W19-5342": {
    "title": "University of Tartu's Multilingual Multi-domain WMT19 News Translation Shared Task Submission",
    "abstract": "This paper describes the University of Tartu's submission to the news translation shared task of WMT19, where the core idea was to train a single multilingual system to cover several language pairs of the shared task and submit its results. We only used the constrained data from the shared task. We describe our approach and its results and discuss the technical issues we faced",
    "volume": "workshop",
    "checked": true,
    "id": "1f7b8380c1d8aa6dd319f01c0c4fec7f250b94e0",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5343": {
    "title": "Neural Machine Translation for English–Kazakh with Morphological Segmentation and Synthetic Data",
    "abstract": "This paper presents the systems submitted by the University of Groningen to the English– Kazakh language pair (both translation directions) for the WMT 2019 news translation task. We explore the potential benefits of (i) morphological segmentation (both unsupervised and rule-based), given the agglutinative nature of Kazakh, (ii) data from two additional languages (Turkish and Russian), given the scarcity of English–Kazakh data and (iii) synthetic data, both for the source and for the target language. Our best submissions ranked second for Kazakh→English and third for English→Kazakh in terms of the BLEU automatic evaluation metric",
    "volume": "workshop",
    "checked": true,
    "id": "7fcb4d25625b5a25ff99332c3ba09f9adabc7927",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5344": {
    "title": "The LMU Munich Unsupervised Machine Translation System for WMT19",
    "abstract": "We describe LMU Munich's machine translation system for German→Czech translation which was used to participate in the WMT19 shared task on unsupervised news translation. We train our model using monolingual data only from both languages. The final model is an unsupervised neural model using established techniques for unsupervised translation such as denoising autoencoding and online back-translation. We bootstrap the model with masked language model pretraining and enhance it with back-translations from an unsupervised phrase-based system which is itself bootstrapped using unsupervised bilingual word embeddings",
    "volume": "workshop",
    "checked": true,
    "id": "faa841fea2213c7be503ce7a5bfb7bf80cf07d5b",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5345": {
    "title": "Combining Local and Document-Level Context: The LMU Munich Neural Machine Translation System at WMT19",
    "abstract": "We describe LMU Munich's machine translation system for English→German translation which was used to participate in the WMT19 shared task on supervised news translation. We specifically participated in the document-level MT track. The system used as a primary submission is a context-aware Transformer capable of both rich modeling of limited contextual information and integration of large-scale document-level context with a less rich representation. We train this model by fine-tuning a big Transformer baseline. Our experimental results show that document-level context provides for large improvements in translation quality, and adding a rich representation of the previous sentence provides a small additional gain",
    "volume": "workshop",
    "checked": true,
    "id": "919e94379f2191e8c24903278299565c97105779",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-5346": {
    "title": "IITP-MT System for Gujarati-English News Translation Task at WMT 2019",
    "abstract": "We describe our submission to WMT 2019 News translation shared task for Gujarati-English language pair. We submit constrained systems, i.e, we rely on the data provided for this language pair and do not use any external data. We train Transformer based subword-level neural machine translation (NMT) system using original parallel corpus along with synthetic parallel corpus obtained through back-translation of monolingual data. Our primary systems achieve BLEU scores of 10.4 and 8.1 for Gujarati→English and English→Gujarati, respectively. We observe that incorporating monolingual data through back-translation improves the BLEU score significantly over baseline NMT and SMT systems for this language pair",
    "volume": "workshop",
    "checked": true,
    "id": "ba48765c608075ebb5066f5bef7fd2f5bf313ccd",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5347": {
    "title": "The University of Helsinki Submissions to the WMT19 News Translation Task",
    "abstract": "In this paper we present the University of Helsinki submissions to the WMT 2019 shared news translation task in three language pairs: English-German, English-Finnish and Finnish-English. This year we focused first on cleaning and filtering the training data using multiple data-filtering approaches, resulting in much smaller and cleaner training sets. For English-German we trained both sentence-level transformer models as well as compared different document-level translation approaches. For Finnish-English and English-Finnish we focused on different segmentation approaches and we also included a rule-based system for English-Finnish",
    "volume": "workshop",
    "checked": true,
    "id": "056083c4da4a923a934179de756a59e6dc18f234",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-5348": {
    "title": "Microsoft Research Asia's Systems for WMT19",
    "abstract": "We Microsoft Research Asia made submissions to 11 language directions in the WMT19 news translation tasks. We won the first place for 8 of the 11 directions and the second place for the other three. Our basic systems are built on Transformer, back translation and knowledge distillation. We integrate several of our rececent techniques to enhance the baseline systems: multi-agent dual learning (MADL), masked sequence-to-sequence pre-training (MASS), neural architecture optimization (NAO), and soft contextual data augmentation (SCA)",
    "volume": "workshop",
    "checked": true,
    "id": "53ffd818255af798b4019e8ecc7d344f4852d647",
    "citation_count": 23
  },
  "https://aclanthology.org/W19-5349": {
    "title": "The En-Ru Two-way Integrated Machine Translation System Based on Transformer",
    "abstract": "Machine translation is one of the most popular areas in natural language processing. WMT is a conference to assess the level of machine translation capabilities of organizations around the world, which is the evaluation activity we participated in. In this review we participated in a two-way translation track from Russian to English and English to Russian. We used official training data, 38 million parallel corpora, and 10 million monolingual corpora. The overall framework we use is the Transformer neural machine translation model, supplemented by data filtering, post-processing, reordering and other related processing methods. The BLEU value of our final translation result from Russian to English is 38.7, ranking 5th, while from English to Russian is 27.8, ranking 10th",
    "volume": "workshop",
    "checked": true,
    "id": "c1f8836c8b41d3cc5a74750ffa06153f6a95e438",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-5350": {
    "title": "DFKI-NMT Submission to the WMT19 News Translation Task",
    "abstract": "This paper describes the DFKI-NMT submission to the WMT19 News translation task. We participated in both English-to-German and German-to-English directions. We trained Transformer models and adopted various techniques for effectively training our models, including data selection, back-translation and in-domain fine-tuning. We give a detailed analysis of the performance of our system",
    "volume": "workshop",
    "checked": true,
    "id": "95a8ce79b79b61d8a6404164e5a0b8dabf4d340d",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5351": {
    "title": "Linguistic Evaluation of German-English Machine Translation Using a Test Suite",
    "abstract": "We present the results of the application of a grammatical test suite for German-to-English MT on the systems submitted at WMT19, with a detailed analysis for 107 phenomena organized in 14 categories. The systems still translate wrong one out of four test items in average. Low performance is indicated for idioms, modals, pseudo-clefts, multi-word expressions and verb valency. When compared to last year, there has been a improvement of function words, non verbal agreement and punctuation. More detailed conclusions about particular systems and phenomena are also presented",
    "volume": "workshop",
    "checked": true,
    "id": "53de6ea817f491955e499c47db7e04fcd8acfcf8",
    "citation_count": 14
  },
  "https://aclanthology.org/W19-5352": {
    "title": "A Test Suite and Manual Evaluation of Document-Level NMT at WMT19",
    "abstract": "As the quality of machine translation rises and neural machine translation (NMT) is moving from sentence to document level translations, it is becoming increasingly difficult to evaluate the output of translation systems. We provide a test suite for WMT19 aimed at assessing discourse phenomena of MT systems participating in the News Translation Task. We have manually checked the outputs and identified types of translation errors that are relevant to document-level translation",
    "volume": "workshop",
    "checked": true,
    "id": "e8971a23433fadca1102cd14c8f5d16f9618658d",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-5353": {
    "title": "Evaluating Conjunction Disambiguation on English-to-German and French-to-German WMT 2019 Translation Hypotheses",
    "abstract": "We present a test set for evaluating an MT system's capability to translate ambiguous conjunctions depending on the sentence structure. We concentrate on the English conjunction \"but\" and its French equivalent \"mais\" which can be translated into two different German conjunctions. We evaluate all English-to-German and French-to-German submissions to the WMT 2019 shared translation task. The evaluation is done mainly automatically, with additional fast manual inspection of unclear cases. All systems almost perfectly recognise the target conjunction \"aber\", whereas accuracies for the other target conjunction \"sondern\" range from 78% to 97%, and the errors are mostly caused by replacing it with the alternative conjunction \"aber\". The best performing system for both language pairs is a multilingual Transformer \"TartuNLP\" system trained on all WMT 2019 language pairs which use the Latin script, indicating that the multilingual approach is beneficial for conjunction disambiguation. As for other system features, such as using synthetic back-translated data, context-aware, hybrid, etc., no particular (dis)advantages can be observed. Qualitative manual inspection of translation hypotheses shown that highly ranked systems generally produce translations with high adequacy and fluency, meaning that these systems are not only capable of capturing the right conjunction whereas the rest of the translation hypothesis is poor. On the other hand, the low ranked systems generally exhibit lower fluency and poor adequacy",
    "volume": "workshop",
    "checked": true,
    "id": "08d96aaaaceb1b6cdee976da7308e2d4cea4af0f",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-5354": {
    "title": "The MuCoW Test Suite at WMT 2019: Automatically Harvested Multilingual Contrastive Word Sense Disambiguation Test Sets for Machine Translation",
    "abstract": "Supervised Neural Machine Translation (NMT) systems currently achieve impressive translation quality for many language pairs. One of the key features of a correct translation is the ability to perform word sense disambiguation (WSD), i.e., to translate an ambiguous word with its correct sense. Existing evaluation benchmarks on WSD capabilities of translation systems rely heavily on manual work and cover only few language pairs and few word types. We present MuCoW, a multilingual contrastive test suite that covers 16 language pairs with more than 200 thousand contrastive sentence pairs, automatically built from word-aligned parallel corpora and the wide-coverage multilingual sense inventory of BabelNet. We evaluate the quality of the ambiguity lexicons and of the resulting test suite on all submissions from 9 language pairs presented in the WMT19 news shared translation task, plus on other 5 language pairs using NMT pretrained models. The MuCoW test suite is available at http://github.com/Helsinki-NLP/MuCoW",
    "volume": "workshop",
    "checked": true,
    "id": "bcbbfa005f0196952f8639d1980c6ed2c35ca634",
    "citation_count": 25
  },
  "https://aclanthology.org/W19-5355": {
    "title": "SAO WMT19 Test Suite: Machine Translation of Audit Reports",
    "abstract": "This paper describes a machine translation test set of documents from the auditing domain and its use as one of the \"test suites\" in the WMT19 News Translation Task for translation directions involving Czech, English and German. Our evaluation suggests that current MT systems optimized for the general news domain can perform quite well even in the particular domain of audit reports. The detailed manual evaluation however indicates that deep factual knowledge of the domain is necessary. For the naked eye of a non-expert, translations by many systems seem almost perfect and automatic MT evaluation with one reference is practically useless for considering these details. Furthermore, we show on a sample document from the domain of agreements that even the best systems completely fail in preserving the semantics of the agreement, namely the identity of the parties",
    "volume": "workshop",
    "checked": true,
    "id": "defa608175120d21843d475cdf41471427ded887",
    "citation_count": 15
  },
  "https://aclanthology.org/W19-5356": {
    "title": "WMDO: Fluency-based Word Mover's Distance for Machine Translation Evaluation",
    "abstract": "We propose WMDO, a metric based on distance between distributions in the semantic vector space. Matching in the semantic space has been investigated for translation evaluation, but the constraints of a translation's word order have not been fully explored. Building on the Word Mover's Distance metric and various word embeddings, we introduce a fragmentation penalty to account for fluency of a translation. This word order extension is shown to perform better than standard WMD, with promising results against other types of metrics",
    "volume": "workshop",
    "checked": true,
    "id": "d3a93c36f240cafb9408e9bc1e41080aaaa3df89",
    "citation_count": 16
  },
  "https://aclanthology.org/W19-5357": {
    "title": "Meteor++ 2.0: Adopt Syntactic Level Paraphrase Knowledge into Machine Translation Evaluation",
    "abstract": "This paper describes Meteor++ 2.0, our submission to the WMT19 Metric Shared Task. The well known Meteor metric improves machine translation evaluation by introducing paraphrase knowledge. However, it only focuses on the lexical level and utilizes consecutive n-grams paraphrases. In this work, we take into consideration syntactic level paraphrase knowledge, which sometimes may be skip-grams. We describe how such knowledge can be extracted from Paraphrase Database (PPDB) and integrated into Meteor-based metrics. Experiments on WMT15 and WMT17 evaluation datasets show that the newly proposed metric outperforms all previous versions of Meteor",
    "volume": "workshop",
    "checked": true,
    "id": "15b9ab6c418da994aedbc4715ae8b458cbc39272",
    "citation_count": 24
  },
  "https://aclanthology.org/W19-5358": {
    "title": "YiSi - a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources",
    "abstract": "We present YiSi, a unified automatic semantic machine translation quality evaluation and estimation metric for languages with different levels of available resources. Underneath the interface with different language resources settings, YiSi uses the same representation for the two sentences in assessment. Besides, we show significant improvement in the correlation of YiSi-1's scores with human judgment is made by using contextual embeddings in multilingual BERT–Bidirectional Encoder Representations from Transformers to evaluate lexical semantic similarity. YiSi is open source and publicly available",
    "volume": "workshop",
    "checked": true,
    "id": "731682fb8bc04b382e9e2db9156147a9479bcc4b",
    "citation_count": 80
  },
  "https://aclanthology.org/W19-5359": {
    "title": "EED: Extended Edit Distance Measure for Machine Translation",
    "abstract": "Over the years a number of machine translation metrics have been developed in order to evaluate the accuracy and quality of machine-generated translations. Metrics such as BLEU and TER have been used for decades. However, with the rapid progress of machine translation systems, the need for better metrics is growing. This paper proposes an extension of the edit distance, which achieves better human correlation, whilst remaining fast, flexible and easy to understand",
    "volume": "workshop",
    "checked": true,
    "id": "2ec428c35238cdcd4e5789fe8d04bc6bf4fb9dc0",
    "citation_count": 17
  },
  "https://aclanthology.org/W19-5360": {
    "title": "Filtering Pseudo-References by Paraphrasing for Automatic Evaluation of Machine Translation",
    "abstract": "In this paper, we introduce our participation in the WMT 2019 Metric Shared Task. We propose an improved version of sentence BLEU using filtered pseudo-references. We propose a method to filter pseudo-references by paraphrasing for automatic evaluation of machine translation (MT). We use the outputs of off-the-shelf MT systems as pseudo-references filtered by paraphrasing in addition to a single human reference (gold reference). We use BERT fine-tuned with paraphrase corpus to filter pseudo-references by checking the paraphrasability with the gold reference. Our experimental results of the WMT 2016 and 2017 datasets show that our method achieved higher correlation with human evaluation than the sentence BLEU (SentBLEU) baselines with a single reference and with unfiltered pseudo-references",
    "volume": "workshop",
    "checked": true,
    "id": "bb048b822f5b198d04f03326696e1beaea186767",
    "citation_count": 9
  },
  "https://aclanthology.org/W19-5361": {
    "title": "Naver Labs Europe's Systems for the WMT19 Machine Translation Robustness Task",
    "abstract": "This paper describes the systems that we submitted to the WMT19 Machine Translation robustness task. This task aims to improve MT's robustness to noise found on social media, like informal language, spelling mistakes and other orthographic variations. The organizers provide parallel data extracted from a social media website in two language pairs: French-English and Japanese-English (one for each language direction). The goal is to obtain the best scores on unseen test sets from the same source, according to automatic metrics (BLEU) and human evaluation. We propose one single and one ensemble system for each translation direction. Our ensemble models ranked first in all language pairs, according to BLEU evaluation. We discuss the pre-processing choices that we made, and present our solutions for robustness to noise and domain adaptation",
    "volume": "workshop",
    "checked": true,
    "id": "2ecac1099a4a49a504d1d7919c4a69897e173cdc",
    "citation_count": 42
  },
  "https://aclanthology.org/W19-5362": {
    "title": "NICT's Supervised Neural Machine Translation Systems for the WMT19 Translation Robustness Task",
    "abstract": "In this paper we describe our neural machine translation (NMT) systems for Japanese↔English translation which we submitted to the translation robustness task. We focused on leveraging transfer learning via fine tuning to improve translation quality. We used a fairly well established domain adaptation technique called Mixed Fine Tuning (MFT) (Chu et. al., 2017) to improve translation quality for Japanese↔English. We also trained bi-directional NMT models instead of uni-directional ones as the former are known to be quite robust, especially in low-resource scenarios. However, given the noisy nature of the in-domain training data, the improvements we obtained are rather modest",
    "volume": "workshop",
    "checked": true,
    "id": "7145f9c251f74c478001302dc1890204d68d9778",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5363": {
    "title": "System Description: The Submission of FOKUS to the WMT 19 Robustness Task",
    "abstract": "This paper describes the systems of Fraunhofer FOKUS for the WMT 2019 machine translation robustness task. We have made submissions to the EN-FR, FR-EN, and JA-EN language pairs. The first two were made with a baseline translator, trained on clean data for the WMT 2019 biomedical translation task. These baselines improved over the baselines from the MTNT paper by 2 to 4 BLEU points, but where not trained on the same data. The last one used the same model class and training procedure, with induced typos in the training data to increase the model robustness",
    "volume": "workshop",
    "checked": true,
    "id": "d8f32eb19e8eea3e2cb01ce1fed13d2b06466e20",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5364": {
    "title": "CUNI System for the WMT19 Robustness Task",
    "abstract": "We present our submission to the WMT19 Robustness Task. Our baseline system is the Charles University (CUNI) Transformer system trained for the WMT18 shared task on News Translation. Quantitative results show that the CUNI Transformer system is already far more robust to noisy input than the LSTM-based baseline provided by the task organizers. We further improved the performance of our model by fine-tuning on the in-domain noisy data without influencing the translation quality on the news domain",
    "volume": "workshop",
    "checked": true,
    "id": "d2beef4761222d2b35ffb184c9d13120fc08e58e",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-5365": {
    "title": "NTT's Machine Translation Systems for WMT19 Robustness Task",
    "abstract": "This paper describes NTT's submission to the WMT19 robustness task. This task mainly focuses on translating noisy text (e.g., posts on Twitter), which presents different difficulties from typical translation tasks such as news. Our submission combined techniques including utilization of a synthetic corpus, domain adaptation, and a placeholder mechanism, which significantly improved over the previous baseline. Experimental results revealed the placeholder mechanism, which temporarily replaces the non-standard tokens including emojis and emoticons with special placeholder tokens during translation, improves translation accuracy even with noisy texts",
    "volume": "workshop",
    "checked": true,
    "id": "a4dbdae5e19e32e42f777a41e949ca0256873de9",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-5366": {
    "title": "JHU 2019 Robustness Task System Description",
    "abstract": "We describe the JHU submissions to the French–English, Japanese–English, and English–Japanese Robustness Task at WMT 2019. Our goal was to evaluate the performance of baseline systems on both the official noisy test set as well as news data, in order to ensure that performance gains in the latter did not come at the expense of general-domain performance. To this end, we built straightforward 6-layer Transformer models and experimented with a handful of variables including subword processing (FR→EN) and a handful of hyperparameters settings (JA↔EN). As expected, our systems performed reasonably",
    "volume": "workshop",
    "checked": true,
    "id": "cacdf4351aa60c7f6211f26568876c6b3d4f07b4",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5367": {
    "title": "Robust Machine Translation with Domain Sensitive Pseudo-Sources: Baidu-OSU WMT19 MT Robustness Shared Task System Report",
    "abstract": "This paper describes the machine translation system developed jointly by Baidu Research and Oregon State University for WMT 2019 Machine Translation Robustness Shared Task. Translation of social media is a very challenging problem, since its style is very different from normal parallel corpora (e.g. News) and also include various types of noises. To make it worse, the amount of social media parallel corpora is extremely limited. In this paper, we use a domain sensitive training method which leverages a large amount of parallel data from popular domains together with a little amount of parallel data from social media. Furthermore, we generate a parallel dataset with pseudo noisy source sentences which are back-translated from monolingual data using a model trained by a similar domain sensitive way. In this way, we achieve more than 10 BLEU improvement in both En-Fr and Fr-En translation compared with the baseline methods",
    "volume": "workshop",
    "checked": true,
    "id": "b18479afdb30f2aab94563144877f14a91c5f15d",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-5368": {
    "title": "Improving Robustness of Neural Machine Translation with Multi-task Learning",
    "abstract": "While neural machine translation (NMT) achieves remarkable performance on clean, in-domain text, performance is known to degrade drastically when facing text which is full of typos, grammatical errors and other varieties of noise. In this work, we propose a multi-task learning algorithm for transformer-based MT systems that is more resilient to this noise. We describe our submission to the WMT 2019 Robustness shared task based on this method. Our model achieves a BLEU score of 32.8 on the shared task French to English dataset, which is 7.1 BLEU points higher than the baseline vanilla transformer trained with clean text",
    "volume": "workshop",
    "checked": true,
    "id": "06f4de06fc37576e1e381cd76e375d57852047b9",
    "citation_count": 18
  },
  "https://aclanthology.org/W19-5401": {
    "title": "Findings of the WMT 2019 Shared Tasks on Quality Estimation",
    "abstract": "We report the results of the WMT19 shared task on Quality Estimation, i.e. the task of predicting the quality of the output of machine translation systems given just the source text and the hypothesis translations. The task includes estimation at three granularity levels: word, sentence and document. A novel addition is evaluating sentence-level QE against human judgments: in other words, designing MT metrics that do not need a reference translation. This year we include three language pairs, produced solely by neural machine translation systems. Participating teams from eleven institutions submitted a variety of systems to different task variants and language pairs",
    "volume": "workshop",
    "checked": true,
    "id": "e8766ba8d2797012c6abd62f2cb4330fea49b3fe",
    "citation_count": 72
  },
  "https://aclanthology.org/W19-5402": {
    "title": "Findings of the WMT 2019 Shared Task on Automatic Post-Editing",
    "abstract": "We present the results from the 5th round of the WMT task on MT Automatic Post-Editing. The task consists in automatically correcting the output of a \"black-box\" machine translation system by learning from human corrections. Keeping the same general evaluation setting of the previous four rounds, this year we focused on two language pairs (English-German and English-Russian) and on domain-specific data (In-formation Technology). For both the language directions, MT outputs were produced by neural systems unknown to par-ticipants. Seven teams participated in the English-German task, with a total of 18 submitted runs. The evaluation, which was performed on the same test set used for the 2018 round, shows a slight progress in APE technology: 4 teams achieved better results than last year's winning system, with improvements up to -0.78 TER and +1.23 BLEU points over the baseline. Two teams participated in theEnglish-Russian task submitting 2 runs each. On this new language direction, characterized by a higher quality of the original translations, the task proved to be particularly challenging. None of the submitted runs improved the very high results of the strong system used to produce the initial translations(16.16 TER, 76.20 BLEU)",
    "volume": "workshop",
    "checked": true,
    "id": "fdf2311451603c3cc593b292217eea9fb7a5b69a",
    "citation_count": 42
  },
  "https://aclanthology.org/W19-5403": {
    "title": "Findings of the WMT 2019 Biomedical Translation Shared Task: Evaluation for MEDLINE Abstracts and Biomedical Terminologies",
    "abstract": "In the fourth edition of the WMT Biomedical Translation task, we considered a total of six languages, namely Chinese (zh), English (en), French (fr), German (de), Portuguese (pt), and Spanish (es). We performed an evaluation of automatic translations for a total of 10 language directions, namely, zh/en, en/zh, fr/en, en/fr, de/en, en/de, pt/en, en/pt, es/en, and en/es. We provided training data based on MEDLINE abstracts for eight of the 10 language pairs and test sets for all of them. In addition to that, we offered a new sub-task for the translation of terms in biomedical terminologies for the en/es language direction. Higher BLEU scores (close to 0.5) were obtained for the es/en, en/es and en/pt test sets, as well as for the terminology sub-task. After manual validation of the primary runs, some submissions were judged to be better than the reference translations, for instance, for de/en, en/es and es/en",
    "volume": "workshop",
    "checked": true,
    "id": "d10becb22a0cd46f48947a0f691dc6d657b6a41b",
    "citation_count": 27
  },
  "https://aclanthology.org/W19-5404": {
    "title": "Findings of the WMT 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions",
    "abstract": "Following the WMT 2018 Shared Task on Parallel Corpus Filtering, we posed the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting 2% and 10% of the highest-quality data to be used to train machine translation systems. This year, the task tackled the low resource condition of Nepali-English and Sinhala-English. Eleven participants from companies, national research labs, and universities participated in this task",
    "volume": "workshop",
    "checked": true,
    "id": "60e928df18d1371a6c5157738c87b6f3bc055644",
    "citation_count": 61
  },
  "https://aclanthology.org/W19-5405": {
    "title": "RTM Stacking Results for Machine Translation Performance Prediction",
    "abstract": "We obtain new results using referential translation machines with increased number of learning models in the set of results that are stacked to obtain a better mixture of experts prediction. We combine features extracted from the word-level predictions with the sentence- or document-level features, which significantly improve the results on the training sets but decrease the test set results",
    "volume": "workshop",
    "checked": true,
    "id": "67cb4610802cbefdf2d5ecaa0942faeaff4b1a69",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-5406": {
    "title": "Unbabel's Participation in the WMT19 Translation Quality Estimation Shared Task",
    "abstract": "We present the contribution of the Unbabel team to the WMT 2019 Shared Task on Quality Estimation. We participated on the word, sentence, and document-level tracks, encompassing 3 language pairs: English-German, English-Russian, and English-French. Our submissions build upon the recent OpenKiwi framework: We combine linear, neural, and predictor-estimator systems with new transfer learning approaches using BERT and XLM pre-trained models. We compare systems individually and propose new ensemble techniques for word and sentence-level predictions. We also propose a simple technique for converting word labels into document-level predictions. Overall, our submitted systems achieve the best results on all tracks and language pairs by a considerable margin",
    "volume": "workshop",
    "checked": true,
    "id": "84e11f45e60667d8e34baa12ddacce5c523cb169",
    "citation_count": 46
  },
  "https://aclanthology.org/W19-5407": {
    "title": "QE BERT: Bilingual BERT Using Multi-task Learning for Neural Quality Estimation",
    "abstract": "For translation quality estimation at word and sentence levels, this paper presents a novel approach based on BERT that recently has achieved impressive results on various natural language processing tasks. Our proposed model is re-purposed BERT for the translation quality estimation and uses multi-task learning for the sentence-level task and word-level subtasks (i.e., source word, target word, and target gap). Experimental results on Quality Estimation shared task of WMT19 show that our systems show competitive results and provide significant improvements over the baseline",
    "volume": "workshop",
    "checked": true,
    "id": "ca564ef7619168e371ce1e80742e521c8f037641",
    "citation_count": 30
  },
  "https://aclanthology.org/W19-5408": {
    "title": "MIPT System for World-Level Quality Estimation",
    "abstract": "We explore different model architectures for the WMT 19 shared task on word-level quality estimation of automatic translation. We start with a model similar to Shef-bRNN, which we modify by using conditional random fields for sequence labelling. Additionally, we use a different approach for labelling gaps and source words. We further develop this model by including features from different sources such as BERT, baseline features for the task and transformer encoders. We evaluate the performance of our models on the English-German dataset for the corresponding shared task",
    "volume": "workshop",
    "checked": true,
    "id": "1f2f6d1c7646cac00f2a91ec16c401d44543e83f",
    "citation_count": 0
  },
  "https://aclanthology.org/W19-5409": {
    "title": "NJU Submissions for the WMT19 Quality Estimation Shared Task",
    "abstract": "In this paper, we describe the submissions of the team from Nanjing University for the WMT19 sentence-level Quality Estimation (QE) shared task on English-German language pair. We develop two approaches based on a two-stage neural QE model consisting of a feature extractor and a quality estimator. More specifically, one of the proposed approaches employs the translation knowledge between the two languages from two different translation directions; while the other one employs extra monolingual knowledge from both source and target sides, obtained by pre-training deep self-attention networks. To efficiently train these two-stage models, a joint learning training method is applied. Experiments show that the ensemble model of the above two models achieves the best results on the benchmark dataset of the WMT17 sentence-level QE shared task and obtains competitive results in WMT19, ranking 3rd out of 10 submissions",
    "volume": "workshop",
    "checked": true,
    "id": "6523d2819efbf01a885f4bbfe5f649f6c5daa3e9",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5410": {
    "title": "Quality Estimation and Translation Metrics via Pre-trained Word and Sentence Embeddings",
    "abstract": "We propose the use of pre-trained embeddings as features of a regression model for sentence-level quality estimation of machine translation. In our work we combine freely available BERT and LASER multilingual embeddings to train a neural-based regression model. In the second proposed method we use as an input features not only pre-trained embeddings, but also log probability of any machine translation (MT) system. Both methods are applied to several language pairs and are evaluated both as a classical quality estimation system (predicting the HTER score) as well as an MT metric (predicting human judgements of translation quality)",
    "volume": "workshop",
    "checked": true,
    "id": "c55591904f810997debab94c33539838227781e2",
    "citation_count": 16
  },
  "https://aclanthology.org/W19-5411": {
    "title": "SOURCE: SOURce-Conditional Elmo-style Model for Machine Translation Quality Estimation",
    "abstract": "Quality estimation (QE) of machine translation (MT) systems is a task of growing importance. It reduces the cost of post-editing, allowing machine-translated text to be used in formal occasions. In this work, we describe our submission system in WMT 2019 sentence-level QE task. We mainly explore the utilization of pre-trained translation models in QE and adopt a bi-directional translation-like strategy. The strategy is similar to ELMo, but additionally conditions on source sentences. Experiments on WMT QE dataset show that our strategy, which makes the pre-training slightly harder, can bring improvements for QE. In WMT-2019 QE task, our system ranked in the second place on En-De NMT dataset and the third place on En-Ru NMT dataset",
    "volume": "workshop",
    "checked": true,
    "id": "6a9ca993dccf31d491a2103ed4b503428caf3fac",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-5412": {
    "title": "Transformer-based Automatic Post-Editing Model with Joint Encoder and Multi-source Attention of Decoder",
    "abstract": "This paper describes POSTECH's submission to the WMT 2019 shared task on Automatic Post-Editing (APE). In this paper, we propose a new multi-source APE model by extending Transformer. The main contributions of our study are that we 1) reconstruct the encoder to generate a joint representation of translation (mt) and its src context, in addition to the conventional src encoding and 2) suggest two types of multi-source attention layers to compute attention between two outputs of the encoder and the decoder state in the decoder. Furthermore, we train our model by applying various teacher-forcing ratios to alleviate exposure bias. Finally, we adopt the ensemble technique across variations of our model. Experiments on the WMT19 English-German APE data set show improvements in terms of both TER and BLEU scores over the baseline. Our primary submission achieves -0.73 in TER and +1.49 in BLEU compare to the baseline",
    "volume": "workshop",
    "checked": true,
    "id": "055073789180dac2c2a0127ab384db335145febc",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-5413": {
    "title": "Unbabel's Submission to the WMT2019 APE Shared Task: BERT-Based Encoder-Decoder for Automatic Post-Editing",
    "abstract": "This paper describes Unbabel's submission to the WMT2019 APE Shared Task for the English-German language pair. Following the recent rise of large, powerful, pre-trained models, we adapt the BERT pretrained model to perform Automatic Post-Editing in an encoder-decoder framework. Analogously to dual-encoder architectures we develop a BERT-based encoder-decoder (BED) model in which a single pretrained BERT encoder receives both the source src and machine translation mt strings. Furthermore, we explore a conservativeness factor to constrain the APE system to perform fewer edits. As the official results show, when trained on a weighted combination of in-domain and artificial training data, our BED system with the conservativeness penalty improves significantly the translations of a strong NMT system by -0.78 and +1.23 in terms of TER and BLEU, respectively. Finally, our submission achieves a new state-of-the-art, ex-aequo, in English-German APE of NMT",
    "volume": "workshop",
    "checked": true,
    "id": "0d2df885be9a4a8fe5cd9725d333c33ce6771057",
    "citation_count": 25
  },
  "https://aclanthology.org/W19-5414": {
    "title": "USAAR-DFKI – The Transference Architecture for English–German Automatic Post-Editing",
    "abstract": "In this paper we present an English–German Automatic Post-Editing (APE) system called transference, submitted to the APE Task organized at WMT 2019. Our transference model is based on a multi-encoder transformer architecture. Unlike previous approaches, it (i) uses a transformer encoder block for src, (ii) followed by a transformer decoder block, but without masking, for self-attention on mt, which effectively acts as second encoder combining src –> mt, and (iii) feeds this representation into a final decoder block generating pe. Our model improves over the raw black-box neural machine translation system by 0.9 and 1.0 absolute BLEU points on the WMT 2019 APE development and test set. Our submission ranked 3rd, however compared to the two top systems, performance differences are not statistically significant",
    "volume": "workshop",
    "checked": true,
    "id": "95e79dbdd63e33c03fab4022d6cd6d92aad8c91d",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5415": {
    "title": "APE through Neural and Statistical MT with Augmented Data. ADAPT/DCU Submission to the WMT 2019 APE Shared Task",
    "abstract": "Automatic post-editing (APE) can be reduced to a machine translation (MT) task, where the source is the output of a specific MT system and the target is its post-edited variant. However, this approach does not consider context information that can be found in the original source of the MT system. Thus a better approach is to employ multi-source MT, where two input sequences are considered – the one being the original source and the other being the MT output. Extra context information can be introduced in the form of extra tokens that identify certain global property of a group of segments, added as a prefix or a suffix to each segment. Successfully applied in domain adaptation of MT as well as on APE, this technique deserves further attention. In this work we investigate multi-source neural APE (or NPE) systems with training data which has been augmented with two types of extra context tokens. We experiment with authentic and synthetic data provided by WMT 2019 and submit our results to the APE shared task. We also experiment with using statistical machine translation (SMT) methods for APE. While our systems score bellow the baseline, we consider this work a step towards understanding the added value of extra context in the case of APE",
    "volume": "workshop",
    "checked": true,
    "id": "7b586ba677257091a1bc68f6fbc9b87cff8a3946",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-5416": {
    "title": "Effort-Aware Neural Automatic Post-Editing",
    "abstract": "For this round of the WMT 2019 APE shared task, our submission focuses on addressing the \"over-correction\" problem in APE. Over-correction occurs when the APE system tends to rephrase an already correct MT output, and the resulting sentence is penalized by a reference-based evaluation against human post-edits. Our intuition is that this problem can be prevented by informing the system about the predicted quality of the MT output or, in other terms, the expected amount of needed corrections. For this purpose, following the common approach in multilingual NMT, we prepend a special token to the beginning of both the source text and the MT output indicating the required amount of post-editing. Following the best submissions to the WMT 2018 APE shared task, our backbone architecture is based on multi-source Transformer to encode both the MT output and the corresponding source text. We participated both in the English-German and English-Russian subtasks. In the first subtask, our best submission improved the original MT output quality up to +0.98 BLEU and -0.47 TER. In the second subtask, where the higher quality of the MT output increases the risk of over-correction, none of our submitted runs was able to improve the MT output",
    "volume": "workshop",
    "checked": true,
    "id": "b42bc67755b0e2c2ca85b22cff6a88f201b3f80e",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-5417": {
    "title": "UdS Submission for the WMT 19 Automatic Post-Editing Task",
    "abstract": "In this paper, we describe our submission to the English-German APE shared task at WMT 2019. We utilize and adapt an NMT architecture originally developed for exploiting context information to APE, implement this in our own transformer model and explore joint training of the APE task with a de-noising encoder",
    "volume": "workshop",
    "checked": true,
    "id": "bb50f192388ea37fda34ed4a00a7495944fef490",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5418": {
    "title": "Terminology-Aware Segmentation and Domain Feature for the WMT19 Biomedical Translation Task",
    "abstract": "In this work, we give a description of the TALP-UPC systems submitted for the WMT19 Biomedical Translation Task. Our proposed strategy is NMT model-independent and relies only on one ingredient, a biomedical terminology list. We first extracted such a terminology list by labelling biomedical words in our training dataset using the BabelNet API. Then, we designed a data preparation strategy to insert the terms information at a token level. Finally, we trained the Transformer model with this terms-informed data. Our best-submitted system ranked 2nd and 3rd for Spanish-English and English-Spanish translation directions, respectively",
    "volume": "workshop",
    "checked": true,
    "id": "f1eb63d416d057e25ab0579ea8aeaebdf3b17bf4",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5419": {
    "title": "Exploring Transfer Learning and Domain Data Selection for the Biomedical Translation",
    "abstract": "Transfer Learning and Selective data training are two of the many approaches being extensively investigated to improve the quality of Neural Machine Translation systems. This paper presents a series of experiments by applying transfer learning and selective data training for participation in the Bio-medical shared task of WMT19. We have used Information Retrieval to selectively choose related sentences from out-of-domain data and used them as additional training data using transfer learning. We also report the effect of tokenization on translation model performance",
    "volume": "workshop",
    "checked": true,
    "id": "feb8ac87642014ee99e50494b6c48321c4c30605",
    "citation_count": 7
  },
  "https://aclanthology.org/W19-5420": {
    "title": "Huawei's NMT Systems for the WMT 2019 Biomedical Translation Task",
    "abstract": "This paper describes Huawei's neural machine translation systems for the WMT 2019 biomedical translation shared task. We trained and fine-tuned our systems on a combination of out-of-domain and in-domain parallel corpora for six translation directions covering English–Chinese, English–French and English–German language pairs. Our submitted systems achieve the best BLEU scores on English–French and English–German language pairs according to the official evaluation results. In the English–Chinese translation task, our systems are in the second place. The enhanced performance is attributed to more in-domain training and more sophisticated models developed. Development of translation models and transfer learning (or domain adaptation) methods has significantly contributed to the progress of the task",
    "volume": "workshop",
    "checked": true,
    "id": "ccfbf27f0960ea4ccdc8fe8ef7581d0d4bfc363e",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-5421": {
    "title": "UCAM Biomedical Translation at WMT19: Transfer Learning Multi-domain Ensembles",
    "abstract": "The 2019 WMT Biomedical translation task involved translating Medline abstracts. We approached this using transfer learning to obtain a series of strong neural models on distinct domains, and combining them into multi-domain ensembles. We further experimented with an adaptive language-model ensemble weighting scheme. Our submission achieved the best submitted results on both directions of English-Spanish",
    "volume": "workshop",
    "checked": true,
    "id": "df43e0e66ccc7d3c3faea7b240d6a6a1405e6410",
    "citation_count": 12
  },
  "https://aclanthology.org/W19-5422": {
    "title": "BSC Participation in the WMT Translation of Biomedical Abstracts",
    "abstract": "This paper describes the machine translation systems developed by the Barcelona Supercomputing (BSC) team for the biomedical translation shared task of WMT19. Our system is based on Neural Machine Translation unsing the OpenNMT-py toolkit and Transformer architecture. We participated in four translation directions for the English/Spanish and English/Portuguese language pairs. To create our training data, we concatenated several parallel corpora, both from in-domain and out-of-domain sources, as well as terminological resources from UMLS",
    "volume": "workshop",
    "checked": true,
    "id": "3010500bf9425d8a28a648217a621fe46fb34254",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-5423": {
    "title": "The MLLP-UPV Spanish-Portuguese and Portuguese-Spanish Machine Translation Systems for WMT19 Similar Language Translation Task",
    "abstract": "This paper describes the participation of the MLLP research group of the Universitat Politècnica de València in the WMT 2019 Similar Language Translation Shared Task. We have submitted systems for the Portuguese ↔ Spanish language pair, in both directions. We have submitted systems based on the Transformer architecture as well as an in development novel architecture which we have called 2D alternating RNN. We have carried out domain adaptation through fine-tuning",
    "volume": "workshop",
    "checked": true,
    "id": "97e8aed54f0ed34776283ec570beebba8d9800c6",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-5424": {
    "title": "The TALP-UPC System for the WMT Similar Language Task: Statistical vs Neural Machine Translation",
    "abstract": "Although the problem of similar language translation has been an area of research interest for many years, yet it is still far from being solved. In this paper, we study the performance of two popular approaches: statistical and neural. We conclude that both methods yield similar results; however, the performance varies depending on the language pair. While the statistical approach outperforms the neural one by a difference of 6 BLEU points for the Spanish-Portuguese language pair, the proposed neural model surpasses the statistical one by a difference of 2 BLEU points for Czech-Polish. In the former case, the language similarity (based on perplexity) is much higher than in the latter case. Additionally, we report negative results for the system combination with back-translation. Our TALP-UPC system submission won 1st place for Czech->Polish and 2nd place for Spanish->Portuguese in the official evaluation of the 1st WMT Similar Language Translation task",
    "volume": "workshop",
    "checked": true,
    "id": "841ce6aacfddbdb0c0807802a0a221e72a5e0652",
    "citation_count": 2
  },
  "https://aclanthology.org/W19-5425": {
    "title": "Machine Translation from an Intercomprehension Perspective",
    "abstract": "Within the first shared task on machine translation between similar languages, we present our first attempts on Czech to Polish machine translation from an intercomprehension perspective. We propose methods based on the mutual intelligibility of the two languages, taking advantage of their orthographic and phonological similarity, in the hope to improve over our baselines. The translation results are evaluated using BLEU. On this metric, none of our proposals could outperform the baselines on the final test set. The current setups are rather preliminary, and there are several potential improvements we can try in the future",
    "volume": "workshop",
    "checked": true,
    "id": "63dba3bc392525012e41146783541701e64bdb77",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5426": {
    "title": "Utilizing Monolingual Data in NMT for Similar Languages: Submission to Similar Language Translation Task",
    "abstract": "This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi -> Nepali direction in which we have examined the performance of a RNN based NMT system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data",
    "volume": "workshop",
    "checked": true,
    "id": "7c6720ef9a5fcd12488fa5dc90f622b6d342632a",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5427": {
    "title": "Neural Machine Translation: Hindi-Nepali",
    "abstract": "With the extensive use of Machine Translation (MT) technology, there is progressively interest in directly translating between pairs of similar languages. Because the main challenge is to overcome the limitation of available parallel data to produce a precise MT output. Current work relies on the Neural Machine Translation (NMT) with attention mechanism for the similar language translation of WMT19 shared task in the context of Hindi-Nepali pair. The NMT systems trained the Hindi-Nepali parallel corpus and tested, analyzed in Hindi ⇔ Nepali translation. The official result declared at WMT19 shared task, which shows that our NMT system obtained Bilingual Evaluation Understudy (BLEU) score 24.6 for primary configuration in Nepali to Hindi translation. Also, we have achieved BLEU score 53.7 (Hindi to Nepali) and 49.1 (Nepali to Hindi) in contrastive system type",
    "volume": "workshop",
    "checked": true,
    "id": "c166246d0552a5a83ca42e0d1950839ec39b06ac",
    "citation_count": 11
  },
  "https://aclanthology.org/W19-5428": {
    "title": "NICT's Machine Translation Systems for the WMT19 Similar Language Translation Task",
    "abstract": "This paper presents the NICT's participation in the WMT19 shared Similar Language Translation Task. We participated in the Spanish-Portuguese task. For both translation directions, we prepared state-of-the-art statistical (SMT) and neural (NMT) machine translation systems. Our NMT systems with the Transformer architecture were trained on the provided parallel data enlarged with a large quantity of back-translated monolingual data. Our primary submission to the task is the result of a simple combination of our SMT and NMT systems. According to BLEU, our systems were ranked second and third respectively for the Portuguese-to-Spanish and Spanish-to-Portuguese translation directions. For contrastive experiments, we also submitted outputs generated with an unsupervised SMT system",
    "volume": "workshop",
    "checked": true,
    "id": "c93f1d3806abd5c4e9e6fe5d458eca29b7cd40fd",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5429": {
    "title": "Panlingua-KMI MT System for Similar Language Translation Task at WMT 2019",
    "abstract": "The present paper enumerates the development of Panlingua-KMI Machine Translation (MT) systems for Hindi ↔ Nepali language pair, designed as part of the Similar Language Translation Task at the WMT 2019 Shared Task. The Panlingua-KMI team conducted a series of experiments to explore both the phrase-based statistical (PBSMT) and neural methods (NMT). Among the 11 MT systems prepared under this task, 6 PBSMT systems were prepared for Nepali-Hindi, 1 PBSMT for Hindi-Nepali and 2 NMT systems were developed for Nepali↔Hindi. The results show that PBSMT could be an effective method for developing MT systems for closely-related languages. Our Hindi-Nepali PBSMT system was ranked 2nd among the 13 systems submitted for the pair and our Nepali-Hindi PBSMTsystem was ranked 4th among the 12 systems submitted for the task",
    "volume": "workshop",
    "checked": true,
    "id": "e5c374032702ad6cf378a93fb9a2bc2d8b39f520",
    "citation_count": 3
  },
  "https://aclanthology.org/W19-5430": {
    "title": "UDS–DFKI Submission to the WMT2019 Czech–Polish Similar Language Translation Shared Task",
    "abstract": "In this paper we present the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. The first edition of this shared task featured data from three pairs of similar languages: Czech and Polish, Hindi and Nepali, and Portuguese and Spanish. Participants could choose to participate in any of these three tracks and submit system outputs in any translation direction. We report the results obtained by our system in translating from Czech to Polish and comment on the impact of out-of-domain test data in the performance of our system. UDS-DFKI achieved competitive performance ranking second among ten teams in Czech to Polish translation",
    "volume": "workshop",
    "checked": true,
    "id": "a6e083f0f34f1cd6c063bcc69950c38457a8134e",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5431": {
    "title": "Neural Machine Translation of Low-Resource and Similar Languages with Backtranslation",
    "abstract": "We present our contribution to the WMT19 Similar Language Translation shared task. We investigate the utility of neural machine translation on three low-resource, similar language pairs: Spanish – Portuguese, Czech – Polish, and Hindi – Nepali. Since state-of-the-art neural machine translation systems still require large amounts of bitext, which we do not have for the pairs we consider, we focus primarily on incorporating monolingual data into our models with backtranslation. In our analysis, we found Transformer models to work best on Spanish – Portuguese and Czech – Polish translation, whereas LSTMs with global attention worked best on Hindi – Nepali translation",
    "volume": "workshop",
    "checked": true,
    "id": "19d9226a98066ef32b4c727a9992dbfbec7dbffc",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-5432": {
    "title": "The University of Helsinki Submissions to the WMT19 Similar Language Translation Task",
    "abstract": "This paper describes the University of Helsinki Language Technology group's participation in the WMT 2019 similar language translation task. We trained neural machine translation models for the language pairs Czech <-> Polish and Spanish <-> Portuguese. Our experiments focused on different subword segmentation methods, and in particular on the comparison of a cognate-aware segmentation method, Cognate Morfessor, with character segmentation and unsupervised segmentation methods for which the data from different languages were simply concatenated. We did not observe major benefits from cognate-aware segmentation methods, but further research may be needed to explore larger parts of the parameter space. Character-level models proved to be competitive for translation between Spanish and Portuguese, but they are slower in training and decoding",
    "volume": "workshop",
    "checked": true,
    "id": "b33a9df8e39966217786855ef8fcf87d11294774",
    "citation_count": 1
  },
  "https://aclanthology.org/W19-5433": {
    "title": "Dual Monolingual Cross-Entropy Delta Filtering of Noisy Parallel Data",
    "abstract": "We introduce a purely monolingual approach to filtering for parallel data from a noisy corpus in a low-resource scenario. Our work is inspired by Junczysdowmunt:2018, but we relax the requirements to allow for cases where no parallel data is available. Our primary contribution is a dual monolingual cross-entropy delta criterion modified from Cynical data selection Axelrod:2017, and is competitive (within 1.8 BLEU) with the best bilingual filtering method when used to train SMT systems. Our approach is featherweight, and runs end-to-end on a standard laptop in three hours",
    "volume": "workshop",
    "checked": true,
    "id": "59faecebc41912d129e59b5f58d1bbc0334c5700",
    "citation_count": 4
  },
  "https://aclanthology.org/W19-5434": {
    "title": "NRC Parallel Corpus Filtering System for WMT 2019",
    "abstract": "We describe the National Research Council Canada team's submissions to the parallel corpus filtering task at the Fourth Conference on Machine Translation",
    "volume": "workshop",
    "checked": true,
    "id": "748cb4cb2e4da816c5412293b83a4ef15b290ad0",
    "citation_count": 10
  },
  "https://aclanthology.org/W19-5435": {
    "title": "Low-Resource Corpus Filtering Using Multilingual Sentence Embeddings",
    "abstract": "In this paper, we describe our submission to the WMT19 low-resource parallel corpus filtering shared task. Our main approach is based on the LASER toolkit (Language-Agnostic SEntence Representations), which uses an encoder-decoder architecture trained on a parallel corpus to obtain multilingual sentence representations. We then use the representations directly to score and filter the noisy parallel sentences without additionally training a scoring function. We contrast our approach to other promising methods and show that LASER yields strong results. Finally, we produce an ensemble of different scoring methods and obtain additional gains. Our submission achieved the best overall performance for both the Nepali-English and Sinhala-English 1M tasks by a margin of 1.3 and 1.4 BLEU respectively, as compared to the second best systems. Moreover, our experiments show that this technique is promising for low and even no-resource scenarios",
    "volume": "workshop",
    "checked": true,
    "id": "00e457b6a6d21d47bd52e707c82adeaecc6e1d12",
    "citation_count": 54
  },
  "https://aclanthology.org/W19-5436": {
    "title": "Quality and Coverage: The AFRL Submission to the WMT19 Parallel Corpus Filtering for Low-Resource Conditions Task",
    "abstract": "The WMT19 Parallel Corpus Filtering For Low-Resource Conditions Task aims to test various methods of filtering a noisy parallel corpora, to make them useful for training machine translation systems. This year the noisy corpora are the relatively low-resource language pairs of Nepali-English and Sinhala-English. This papers describes the Air Force Research Laboratory (AFRL) submissions, including preprocessing methods and scoring metrics. Numerical results indicate a benefit over baseline and the relative benefits of different options",
    "volume": "workshop",
    "checked": true,
    "id": "89e011edc3081f631dd79ad461bd12c259205c35",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-5437": {
    "title": "Webinterpret Submission to the WMT2019 Shared Task on Parallel Corpus Filtering",
    "abstract": "This document describes the participation of Webinterpret in the shared task on parallel corpus filtering at the Fourth Conference on Machine Translation (WMT 2019). Here, we describe the main characteristics of our approach and discuss the results obtained on the data sets published for the shared task",
    "volume": "workshop",
    "checked": true,
    "id": "5e8f2a65939de6f4ef8483471c76bb7ea18256b6",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-5438": {
    "title": "Noisy Parallel Corpus Filtering through Projected Word Embeddings",
    "abstract": "We present a very simple method for parallel text cleaning of low-resource languages, based on projection of word embeddings trained on large monolingual corpora in high-resource languages. In spite of its simplicity, we approach the strong baseline system in the downstream machine translation evaluation",
    "volume": "workshop",
    "checked": true,
    "id": "d1d6a1881233fd6a2a6b7ebdfa02b8bb15f25607",
    "citation_count": 8
  },
  "https://aclanthology.org/W19-5439": {
    "title": "Filtering of Noisy Parallel Corpora Based on Hypothesis Generation",
    "abstract": "The filtering task of noisy parallel corpora in WMT2019 aims to challenge participants to create filtering methods to be useful for training machine translation systems. In this work, we introduce a noisy parallel corpora filtering system based on generating hypotheses by means of a translation model. We train translation models in both language pairs: Nepali–English and Sinhala–English using provided parallel corpora. We select the training subset for three language pairs (Nepali, Sinhala and Hindi to English) jointly using bilingual cross-entropy selection to create the best possible translation model for both language pairs. Once the translation models are trained, we translate the noisy corpora and generate a hypothesis for each sentence pair. We compute the smoothed BLEU score between the target sentence and generated hypothesis. In addition, we apply several rules to discard very noisy or inadequate sentences which can lower the translation score. These heuristics are based on sentence length, source and target similarity and source language detection. We compare our results with the baseline published on the shared task website, which uses the Zipporah model, over which we achieve significant improvements in one of the conditions in the shared task. The designed filtering system is domain independent and all experiments are conducted using neural machine translation",
    "volume": "workshop",
    "checked": true,
    "id": "697593a171935edccf5395cc7baf908c5d79ccb8",
    "citation_count": 5
  },
  "https://aclanthology.org/W19-5440": {
    "title": "Parallel Corpus Filtering Based on Fuzzy String Matching",
    "abstract": "In this paper, we describe the IIT Patna's submission to WMT 2019 shared task on parallel corpus filtering. This shared task asks the participants to develop methods for scoring each parallel sentence from a given noisy parallel corpus. Quality of the scoring method is judged based on the quality of SMT and NMT systems trained on smaller set of high-quality parallel sentences sub-sampled from the original noisy corpus. This task has two language pairs. We submit for both the Nepali-English and Sinhala-English language pairs. We define fuzzy string matching score between English and the translated (into English) source based on Levenshtein distance. Based on the scores, we sub-sample two sets (having 1 million and 5 millions English tokens) of parallel sentences from each parallel corpus, and train SMT systems for development purpose only. The organizers publish the official evaluation using both SMT and NMT on the final official test set. Total 10 teams participated in the shared task and according the official evaluation, our scoring method obtains 2nd position in the team ranking for 1-million NepaliEnglish NMT and 5-million Sinhala-English NMT categories",
    "volume": "workshop",
    "checked": true,
    "id": "2a98f466716c34ba2491d7d56ece1944bb27d5c6",
    "citation_count": 6
  },
  "https://aclanthology.org/W19-5441": {
    "title": "The University of Helsinki Submission to the WMT19 Parallel Corpus Filtering Task",
    "abstract": "This paper describes the University of Helsinki Language Technology group's participation in the WMT 2019 parallel corpus filtering task. Our scores were produced using a two-step strategy. First, we individually applied a series of filters to remove the 'bad' quality sentences. Then, we produced scores for each sentence by weighting these features with a classification model. This methodology allowed us to build a simple and reliable system that is easily adaptable to other language pairs",
    "volume": "workshop",
    "checked": true,
    "id": "f677fbccb980fc5901a3255e90ce882714ea3259",
    "citation_count": 5
  },
  "https://aclanthology.org/P19-3003": {
    "title": "lingvis.io - A Linguistic Visual Analytics Framework",
    "abstract": "We present a modular framework for the rapid-prototyping of linguistic, web-based, visual analytics applications. Our framework gives developers access to a rich set of machine learning and natural language processing steps, through encapsulating them into micro-services and combining them into a computational pipeline. This processing pipeline is auto-configured based on the requirements of the visualization front-end, making the linguistic processing and visualization design, detached independent development tasks. This paper describes the constellation and modality of our framework, which continues to support the efficient development of various human-in-the-loop, linguistic visual analytics research techniques and applications",
    "volume": "demo",
    "checked": true,
    "id": "057d6db528bf74db3c9223ea5a77ade81412fef5",
    "citation_count": 13
  },
  "https://aclanthology.org/W19-3653": {
    "title": "Adversarial Attack on Sentiment Classification",
    "abstract": "In this paper, we propose a white-box attack algorithm called \"Global Search\" method and compare it with a simple misspelling noise and a more sophisticated and common white-box attack approach called \"Greedy Search\". The attack methods are evaluated on the Convolutional Neural Network (CNN) sentiment classifier trained on the IMDB movie review dataset. The attack success rate is used to evaluate the effectiveness of the attack methods and the perplexity of the sentences is used to measure the degree of distortion of the generated adversarial examples. The experiment results show that the proposed \"Global Search\" method generates more powerful adversarial examples with less distortion or less modification to the source text",
    "volume": "workshop",
    "checked": true,
    "id": "182c1bc8ad4291440be6d939838f78deebe522b3",
    "citation_count": 0
  }
}