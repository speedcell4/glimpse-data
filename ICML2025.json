{
  "https://openreview.net/forum?id=2GmXJnyNM4": {
    "title": "Implicit Regularization for Tubal Tensor Factorizations via Gradient Descent",
    "volume": "oral",
    "abstract": "We provide a rigorous analysis of implicit regularization in an overparametrized tensor factorization problem beyond the lazy training regime. For matrix factorization problems, this phenomenon has been studied in a number of works. A particular challenge has been to design universal initialization strategies which provably lead to implicit regularization in gradient-descent methods. At the same time, it has been argued by Cohen et. al. 2016 that more general classes of neural networks can be captured by considering tensor factorizations. However, in the tensor case, implicit regularization has only been rigorously established for gradient flow or in the lazy training regime. In this paper, we prove the first tensor result of its kind for gradient descent rather than gradient flow. We focus on the tubal tensor product and the associated notion of low tubal rank, encouraged by the relevance of this model for image data. We establish that gradient descent in an overparametrized tensor factorization model with a small random initialization exhibits an implicit bias towards solutions of low tubal rank. Our theoretical findings are illustrated in an extensive set of numerical simulations show-casing the dynamics predicted by our theory as well as the crucial role of using a small random initialization",
    "checked": true,
    "id": "314ee8ed7037c2856f85d5f0350176edcd8353e1",
    "semantic_title": "implicit regularization for tubal tensor factorizations via gradient descent",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3go0lhfxd0": {
    "title": "Algorithm Development in Neural Networks: Insights from the Streaming Parity Task",
    "volume": "oral",
    "abstract": "Even when massively overparameterized, deep neural networks show a remarkable ability to generalize. Research on this phenomenon has focused on generalization within distribution, via smooth interpolation. Yet in some settings neural networks also learn to extrapolate to data far beyond the bounds of the original training set, sometimes even allowing for infinite generalization, implying that an algorithm capable of solving the task has been learned. Here we undertake a case study of the learning dynamics of recurrent neural networks trained on the streaming parity task in order to develop an effective theory of algorithm development. The streaming parity task is a simple but nonlinear task defined on sequences up to arbitrary length. We show that, with sufficient finite training experience, RNNs exhibit a phase transition to perfect infinite generalization. Using an effective theory for the representational dynamics, we find an implicit representational merger effect which can be interpreted as the construction of a finite automaton that reproduces the task. Overall, our results disclose one mechanism by which neural networks can generalize infinitely from finite training experience",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GFpjO8S8Po": {
    "title": "Orthogonal Subspace Decomposition for Generalizable AI-Generated Image Detection",
    "volume": "oral",
    "abstract": "Detecting AI-generated images (AIGIs), such as natural images or face images, has become increasingly important yet challenging. In this paper, we start from a new perspective to excavate the reason behind the failure generalization in AIGI detection, named the asymmetry phenomenon, where a naively trained detector tends to favor overfitting to the limited and monotonous fake patterns, causing the feature space to become highly constrained and low-ranked, which is proved seriously limiting the expressivity and generalization. One potential remedy is incorporating the pre-trained knowledge within the vision foundation models (higher-ranked) to expand the feature space, alleviating the model's overfitting to fake. To this end, we employ Singular Value Decomposition (SVD) to decompose the original feature space into two orthogonal subspaces. By freezing the principal components and adapting only the remained components, we preserve the pre-trained knowledge while learning fake patterns. Compared to existing full-parameters and LoRA-based tuning methods, we explicitly ensure orthogonality, enabling the higher rank of the whole feature space, effectively minimizing overfitting and enhancing generalization. We finally identify a crucial insight: our method implicitly learns a vital prior that fakes are actually derived from the real, indicating a hierarchical relationship rather than independence. Modeling this prior, we believe, is essential for achieving superior generalization. Our codes are publicly available at https://github.com/YZY-stack/Effort-AIGI-Detection",
    "checked": true,
    "id": "e7c9a171bdc2bc083d416cf7960b336d2c7fb0ab",
    "semantic_title": "orthogonal subspace decomposition for generalizable ai-generated image detection",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=vQubr1uBUw": {
    "title": "Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies",
    "volume": "oral",
    "abstract": "Accelerating the inference of large language models (LLMs) is a critical challenge in generative AI. Speculative decoding (SD) methods offer substantial efficiency gains by generating multiple tokens using a single target forward pass. However, existing SD approaches require the drafter and target models to share the same vocabulary, thus limiting the pool of possible drafters, often necessitating the training of a drafter from scratch. We present three new SD methods that remove this shared-vocabulary constraint. All three methods preserve the target distribution (i.e., they are lossless) and work with off-the-shelf models without requiring additional training or modifications. Empirically, on summarization, programming, and long-context tasks, our algorithms demonstrate significant speedups of up to 2.8x over standard autoregressive decoding. By enabling any off-the-shelf model to serve as a drafter and requiring no retraining, this work substantially broadens the applicability of the SD framework in practice",
    "checked": true,
    "id": "b2fecb8aa7218b00334e8fff8162509aa31761fe",
    "semantic_title": "accelerating llm inference with lossless speculative decoding algorithms for heterogeneous vocabularies",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=SyQPiZJVWY": {
    "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models",
    "volume": "oral",
    "abstract": "Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect actual discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorization, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods on LLM-SRBench, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rc7y9HFC34": {
    "title": "ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features",
    "volume": "oral",
    "abstract": "Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? We introduce ConceptAttention, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images. Without requiring additional training, ConceptAttention repurposes the parameters of DiT attention layers to produce highly contextualized *concept embeddings*, contributing the major discovery that performing linear projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention maps. ConceptAttention even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 15 other zero-shot interpretability methods on the ImageNet-Segmentation dataset. ConceptAttention works for popular image models and even seamlessly generalizes to video generation. Our work contributes the first evidence that the representations of multi-modal DiTs are highly transferable to vision tasks like segmentation",
    "checked": true,
    "id": "73f07499fe02c7549012d1ce81321a0b3d623fc8",
    "semantic_title": "conceptattention: diffusion transformers learn highly interpretable features",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=36hVB7DEB0": {
    "title": "Emergence in non-neural models: grokking modular arithmetic via average gradient outer product",
    "volume": "oral",
    "abstract": "Neural networks trained to solve modular arithmetic tasks exhibit grokking, a phenomenon where the test accuracy starts improving long after the model achieves 100% training accuracy in the training process. It is often taken as an example of \"emergence\", where model ability manifests sharply through a phase transition. In this work, we show that the phenomenon of grokking is not specific to neural networks nor to gradient descent-based optimization. Specifically, we show that this phenomenon occurs when learning modular arithmetic with Recursive Feature Machines (RFM), an iterative algorithm that uses the Average Gradient Outer Product (AGOP) to enable task-specific feature learning with general machine learning models. When used in conjunction with kernel machines, iterating RFM results in a fast transition from random, near zero, test accuracy to perfect test accuracy. This transition cannot be predicted from the training loss, which is identically zero, nor from the test loss, which remains constant in initial iterations. Instead, as we show, the transition is completely determined by feature learning: RFM gradually learns block-circulant features to solve modular arithmetic. Paralleling the results for RFM, we show that neural networks that solve modular arithmetic also learn block-circulant features. Furthermore, we present theoretical evidence that RFM uses such block-circulant features to implement the Fourier Multiplication Algorithm, which prior work posited as the generalizing solution neural networks learn on these tasks. Our results demonstrate that emergence can result purely from learning task-relevant features and is not specific to neural architectures nor gradient descent-based optimization methods. Furthermore, our work provides more evidence for AGOP as a key mechanism for feature learning in neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EBNgREMoVD": {
    "title": "Hierarchical Refinement: Optimal Transport to Infinity and Beyond",
    "volume": "oral",
    "abstract": "Optimal transport (OT) has enjoyed great success in machine learning as a principled way to align datasets via a least-cost correspondence, driven in large part by the runtime efficiency of the Sinkhorn algorithm (Cuturi, 2013). However, Sinkhorn has quadratic space complexity in the number of points, limiting scalability to larger datasets. Low-rank OT achieves linear-space complexity, but by definition, cannot compute a one-to-one correspondence between points. When the optimal transport problem is an assignment problem between datasets then an optimal mapping, known as the _Monge map_, is guaranteed to be a bijection. In this setting, we show that the factors of an optimal low-rank coupling co-cluster each point with its image under the Monge map. We leverage this invariant to derive an algorithm, _Hierarchical Refinement_ (`HiRef`), that dynamically constructs a multiscale partition of each dataset using low-rank OT subproblems, culminating in a bijective coupling. Hierarchical Refinement uses linear space and has log-linear runtime, retaining the space advantage of low-rank OT while overcoming its limited resolution. We demonstrate the advantages of Hierarchical Refinement on several datasets, including ones containing over a million points, scaling full-rank OT to problems previously beyond Sinkhorn's reach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DjJmre5IkP": {
    "title": "Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions",
    "volume": "oral",
    "abstract": "In recent years, masked diffusion models (MDMs) have emerged as a promising alternative approach for generative modeling over discrete domains. Compared to autoregressive models (ARMs), MDMs trade off complexity at training time with flexibility at inference time. At training time, they must learn to solve an exponentially large number of infilling problems, but at inference time, they can decode tokens in essentially arbitrary order. In this work we closely examine these two competing effects. On the training front, we theoretically and empirically demonstrate that MDMs indeed train on computationally intractable subproblems compared to their autoregressive counterparts. On the inference front, we show that a suitable strategy for adaptively choosing the token decoding order significantly enhances the capabilities of MDMs, allowing them to sidestep hard subproblems. On logic puzzles like Sudoku, we show that adaptive inference can boost solving accuracy in pretrained MDMs from $<7$\\% to $\\approx 90$\\%, even outperforming ARMs that were explicitly trained via teacher forcing to learn the right order of decoding",
    "checked": true,
    "id": "749ae28bc2b02638a90000ee3505b7190788b8a7",
    "semantic_title": "train for the worst, plan for the best: understanding token ordering in masked diffusions",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=4EYwwVuhtG": {
    "title": "Statistical Test for Feature Selection Pipelines by Selective Inference",
    "volume": "oral",
    "abstract": "A data analysis pipeline is a structured sequence of steps that transforms raw data into meaningful insights by integrating various analysis algorithms. In this paper, we propose a novel statistical test to assess the significance of data analysis pipelines. Our approach enables the systematic development of valid statistical tests applicable to any feature selection pipeline composed of predefined components. We develop this framework based on selective inference, a statistical technique that has recently gained attention for data-driven hypotheses. As a proof of concept, we focus on feature selection pipelines for linear models, composed of three missing value imputation algorithms, three outlier detection algorithms, and three feature selection algorithms. We theoretically prove that our statistical test can control the probability of false positive feature selection at any desired level, and demonstrate its validity and effectiveness through experiments on synthetic and real data. Additionally, we present an implementation framework that facilitates testing across any configuration of these feature selection pipelines without extra implementation costs",
    "checked": true,
    "id": "e9bf6d9d2f29728a6066d38c75275b9764b83000",
    "semantic_title": "statistical test for feature selection pipelines by selective inference",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=qR7YsQdFxV": {
    "title": "All-Purpose Mean Estimation over R: Optimal Sub-Gaussianity with Outlier Robustness and Low Moments Performance",
    "volume": "oral",
    "abstract": "We consider the basic statistical challenge of designing an \"all-purpose\" mean estimation algorithm that is recommendable across a variety of settings and models. Recent work by [Lee and Valiant 2022] introduced the first 1-d mean estimator whose error in the standard finite-variance+i.i.d. setting is optimal even in its constant factors; experimental demonstration of its good performance was shown by [Gobet et al. 2022]. Yet, unlike for classic (but not necessarily practical) estimators such as median-of-means and trimmed mean, this new algorithm lacked proven robustness guarantees in other settings, including the settings of adversarial data corruption and heavy-tailed distributions with infinite variance. Such robustness is important for practical use cases. This raises a research question: is it possible to have a mean estimator that is robust, *without* sacrificing provably optimal performance in the standard i.i.d. setting? In this work, we show that Lee and Valiant's estimator is in fact an \"all-purpose\" mean estimator by proving: (A) It is robust to an $\\eta$-fraction of data corruption, even in the strong contamination model; it has optimal estimation error $O(\\sigma\\sqrt{\\eta})$ for distributions with variance $\\sigma^2$. (B) For distributions with finite $z^\\text{th}$ moment, for $z \\in (1,2)$, it has optimal estimation error, matching the lower bounds of [Devroye et al. 2016] up to constants. We further show (C) that outlier robustness for 1-d mean estimators in fact implies neighborhood optimality, a notion of beyond worst-case and distribution-dependent optimality recently introduced by [Dang et al. 2023]. Previously, such an optimality guarantee was only known for median-of-means, but now it holds also for all estimators that are simultaneously *robust* and *sub-Gaussian*, including Lee and Valiant's, resolving a question raised by Dang et al. Lastly, we show (D) the asymptotic normality and efficiency of Lee and Valiant's estimator, as further evidence for its performance across many settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xZXhFg43EI": {
    "title": "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?",
    "volume": "oral",
    "abstract": "We introduce SWE-Lancer, a benchmark of over 1400 freelance software engineering tasks from Upwork, valued at \\\\\\$1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks — ranging from \\\\\\$50 bug fixes to \\\\\\$32000 feature implementations — and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split. By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UeB3Hdrhda": {
    "title": "Training a Generally Curious Agent",
    "volume": "oral",
    "abstract": "Efficient exploration is essential for intelligent systems interacting with their environment, but existing language models often fall short in scenarios that require strategic information gathering. In this paper, we present **Paprika**, a fine-tuning approach that enables language models to develop general decision-making capabilities that are not confined to particular environments. By training on synthetic interaction data from different tasks that require diverse strategies, Paprika teaches models to explore and adapt their behavior on a new task based on environment feedback in-context without more gradient updates. Experimental results show that models fine-tuned with Paprika can effectively transfer their learned decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, our approach's primary bottleneck lies in sampling useful interaction data instead of model updates. To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories from tasks with high learning potential. These results suggest a promising path towards AI systems that can autonomously solve novel sequential decision-making problems that require interactions with the external world",
    "checked": true,
    "id": "bb146358872cf242e97d891bbf6994bc1faae2fe",
    "semantic_title": "training a generally curious agent",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=uRAgIVnAO6": {
    "title": "High-Dimensional Prediction for Sequential Decision Making",
    "volume": "oral",
    "abstract": "We give an efficient algorithm for producing multi-dimensional forecasts in an online adversarial environment that have low bias subject to any polynomial number of conditioning events, that can depend both on external context and on our predictions themselves. We demonstrate the use of this algorithm with several applications. We show how to make predictions that can be transparently consumed by any polynomial number of downstream decision makers with different utility functions, guaranteeing them diminishing swap regret at optimal rates. We also give the first efficient algorithms for guaranteeing diminishing conditional regret in online combinatorial optimization problems for an arbitrary polynomial number of conditioning events --- i.e. on an arbitrary number of intersecting subsequences determined both by context and our own predictions. Finally, we give the first efficient algorithm for online multicalibration with $O(T^{2/3})$ rates in the ECE metric",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DgGF2LEBPS": {
    "title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents",
    "volume": "oral",
    "abstract": "Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 24 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only $28.9\\\\%$ on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code and dataset are available at [https://embodiedbench.github.io](https://embodiedbench.github.io)",
    "checked": true,
    "id": "6fbb3ed823526ac050b610d353ea91a8515f7e69",
    "semantic_title": "embodiedbench: comprehensive benchmarking multi-modal large language models for vision-driven embodied agents",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=OZSXYeqpI1": {
    "title": "Auditing f -differential privacy in one run",
    "volume": "oral",
    "abstract": "Empirical auditing has emerged as a means of catching some of the flaws in the implementation of privacy-preserving algorithms. Existing auditing mechanisms, however, are either computationally inefficient -- requiring multiple runs of the machine learning algorithms —- or suboptimal in calculating an empirical privacy. In this work, we present a tight and efficient auditing procedure and analysis that can effectively assess the privacy of mechanisms. Our approach is efficient; Similar to the recent work of Steinke, Nasr and Jagielski (2023), our auditing procedure leverages the randomness of examples in the input dataset and requires only a single run of the target mechanism. And it is more accurate; we provide a novel analysis that enables us to achieve tight empirical privacy estimates by using the hypothesized $f$-DP curve of the mechanism, which provides a more accurate measure of privacy than the traditional $\\epsilon,\\delta$ differential privacy parameters. We use our auditing procure and analysis to obtain empirical privacy, demonstrating that our auditing procedure delivers tighter privacy estimates",
    "checked": false,
    "id": "a4fb9ea9f9e8b0a4995512f12f84e4e01c0cc50e",
    "semantic_title": "auditing f-differential privacy in one run",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=yDTwamN4LQ": {
    "title": "Learning with Expected Signatures: Theory and Applications",
    "volume": "oral",
    "abstract": "The expected signature maps a collection of data streams to a lower dimensional representation, with a remarkable property: the resulting feature tensor can fully characterize the data generating distribution. This \"model-free\"' embedding has been successfully leveraged to build multiple domain-agnostic machine learning (ML) algorithms for time series and sequential data. The convergence results proved in this paper bridge the gap between the expected signature's empirical discrete-time estimator and its theoretical continuous-time value, allowing for a more complete probabilistic interpretation of expected signature-based ML methods. Moreover, when the data generating process is a martingale, we suggest a simple modification of the expected signature estimator with significantly lower mean squared error and empirically demonstrate how it can be effectively applied to improve predictive performance",
    "checked": true,
    "id": "bce71363b0ba994fca524d2a6d43e5fdd694e9ca",
    "semantic_title": "learning with expected signatures: theory and applications",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EZV4edMGM1": {
    "title": "Statistical Query Hardness of Multiclass Linear Classification with Random Classification Noise",
    "volume": "oral",
    "abstract": "We study the task of Multiclass Linear Classification (MLC) in the distribution-free PAC model with Random Classification Noise (RCN). Specifically, the learner is given a set of labeled examples $(x, y)$, where $x$ is drawn from an unknown distribution on $R^d$ and the labels are generated by a multiclass linear classifier corrupted with RCN. That is, the label $y$ is flipped from $i$ to $j$ with probability $H_{ij}$ according to a known noise matrix $H$ with non-negative separation $\\sigma: = \\min_{i \\neq j} H_{ii}-H_{ij}$. The goal is to compute a hypothesis with small 0-1 error. For the special case of two labels, prior work has given polynomial-time algorithms achieving the optimal error. Surprisingly, little is known about the complexity of this task even for three labels. As our main contribution, we show that the complexity of MLC with RCN becomes drastically different in the presence of three or more labels. Specifically, we prove super-polynomial Statistical Query (SQ) lower bounds for this problem. In more detail, even for three labels and constant separation, we give a super-polynomial lower bound on the complexity of any SQ algorithm achieving optimal error. For a larger number of labels and smaller separation, we show a super-polynomial SQ lower bound even for the weaker goal of achieving any constant factor approximation to the optimal loss or even beating the trivial hypothesis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LCbHsdtvOR": {
    "title": "Expected Variational Inequalities",
    "volume": "oral",
    "abstract": "*Variational inequalities (VIs)* encompass many fundamental problems in diverse areas ranging from engineering to economics and machine learning. However, their considerable expressivity comes at the cost of computational intractability. In this paper, we introduce and analyze a natural relaxation—which we refer to as *expected variational inequalities (EVIs)*—where the goal is to find a distribution that satisfies the VI constraint in expectation. By adapting recent techniques from game theory, we show that, unlike VIs, EVIs can be solved in polynomial time under general (nonmonotone) operators. EVIs capture the seminal notion of correlated equilibria, but enjoy a greater reach beyond games. We also employ our framework to capture and generalize several existing disparate results, including from settings such as smooth games, and games with coupled constraints or nonconcave utilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0LZRtvK871": {
    "title": "Improving the Scaling Laws of Synthetic Data with Deliberate Practice",
    "volume": "oral",
    "abstract": "Inspired by the principle of deliberate practice in human learning, we propose Deliberate Practice for Synthetic Data Generation (DP), a novel framework that improves sample efficiency through dynamic synthetic data generation. Prior work has shown that scaling synthetic data is inherently challenging, as naively adding new data leads to diminishing returns. To address this, pruning has been identified as a key mechanism for improving scaling, enabling models to focus on the most informative synthetic samples. Rather than generating a large dataset and pruning it afterward, DP efficiently approximates the direct generation of informative samples. We theoretically show how training on challenging, informative examples improves scaling laws and empirically validate that DP achieves better scaling performance with significantly fewer training samples and iterations. On ImageNet-100, DP generates 3.4x fewer samples and requires six times fewer iterations, while on ImageNet-1k, it generates 8x fewer samples with a 30% reduction in iterations, all while achieving superior performance compared to prior work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fvq9ogLnLN": {
    "title": "Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks",
    "volume": "oral",
    "abstract": "Understanding neural network training dynamics at scale is an important open problem. Although realistic model architectures, optimizers, and data interact in complex ways that make predictive theory challenging, we show that compute-optimally trained models exhibit remarkably precise collective regularities. Specifically, loss curves from models of varying sizes collapse onto a single universal curve when training compute and loss are normalized to unity at the end of training. With learning rate decay, discrepancies between normalized curves fall below the noise floor of individual models' loss curves across random seeds, yielding an exceptionally tight collapse we term \"supercollapse.\" We observe supercollapse across learning rate schedules, datasets, and architectures, including transformers trained on next-token prediction. This collapse breaks down when hyperparameters are scaled suboptimally, providing a practical indicator of proper scaling. We explain these phenomena by connecting collapse to the power-law structure in typical neural scaling laws, and analyzing a simple but effective model of SGD noise dynamics that accurately captures how learning rate schedules deform loss curves away from power laws while preserving universality, and why learning rate decay suppresses variance to enable supercollapse",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zBBYsVGKuB": {
    "title": "Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination",
    "volume": "oral",
    "abstract": "Zero-shot coordination (ZSC), the ability to adapt to a new partner in a cooperative task, is a critical component of human-compatible AI. While prior work has focused on training agents to cooperate on a single task, these specialized models do not generalize to new tasks, even if they are highly similar. Here, we study how reinforcement learning on a **distribution of environments with a single partner** enables learning general cooperative skills that support ZSC with **many new partners on many new problems**. We introduce *two* Jax-based, procedural generators that create billions of solvable coordination challenges. We develop a new paradigm called **Cross-Environment Cooperation (CEC)**, and show that it outperforms competitive baselines quantitatively and qualitatively when collaborating with real people. Our findings suggest that learning to collaborate across many unique scenarios encourages agents to develop general norms, which prove effective for collaboration with different partners. Together, our results suggest a new route toward designing generalist cooperative agents capable of interacting with humans without requiring human data",
    "checked": true,
    "id": "07831c8cc525a2100f20442a1e30f81bfd4b2493",
    "semantic_title": "cross-environment cooperation enables zero-shot multi-agent coordination",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=WGXb7UdvTX": {
    "title": "Layer by Layer: Uncovering Hidden Representations in Language Models",
    "volume": "oral",
    "abstract": "From extracting features to generating text, the outputs of large language models (LLMs) typically rely on their final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a wide range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each model layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer's performance. Through extensive experiments on 32 text-embedding tasks across various architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features, challenging the standard view on final-layer embeddings and opening new directions on using mid-layer representations for more robust and accurate representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OEl3L8osas": {
    "title": "The dark side of the forces: assessing non-conservative force models for atomistic machine learning",
    "volume": "oral",
    "abstract": "The use of machine learning to estimate the energy of a group of atoms, and the forces that drive them to more stable configurations, have revolutionized the fields of computational chemistry and materials discovery. In this domain, rigorous enforcement of symmetry and conservation laws has traditionally been considered essential. For this reason, interatomic forces are usually computed as the derivatives of the potential energy, ensuring energy conservation. Several recent works have questioned this physically constrained approach, suggesting that directly predicting the forces yields a better trade-off between accuracy and computational efficiency -- and that energy conservation can be learned during training. This work investigates the applicability of such non-conservative models in microscopic simulations. We identify and demonstrate several fundamental issues, from ill-defined convergence of geometry optimization to instability in various types of molecular dynamics. Contrary to the case of rotational symmetry, energy conservation is hard to learn, monitor, and correct for. The best approach to exploit the acceleration afforded by direct force prediction might be to use it in tandem with a conservative model, reducing -- rather than eliminating -- the additional cost of backpropagation, but avoiding the pathological behavior associated with non-conservative forces",
    "checked": true,
    "id": "ade5219b6db3ec55e4eb54231d986cfe67fdf194",
    "semantic_title": "the dark side of the forces: assessing non-conservative force models for atomistic machine learning",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=OWIPDWhUcO": {
    "title": "AdaSplash: Adaptive Sparse Flash Attention",
    "volume": "oral",
    "abstract": "The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $\\alpha$-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $\\alpha$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $\\alpha$-entmax implementations. It approaches---and in some cases surpasses---the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance",
    "checked": true,
    "id": "a6f877368b262eeac407c0f1171063285c2c46b6",
    "semantic_title": "adasplash: adaptive sparse flash attention",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=j6H7c3aQyb": {
    "title": "Temporal Difference Flows",
    "volume": "oral",
    "abstract": "Predictive models of the future are fundamental for an agent's ability to reason and plan. A common strategy learns a world model and unrolls it step-by-step at inference, where small errors can rapidly compound. Geometric Horizon Models (GHMs) offer a compelling alternative by directly making predictions of future states, avoiding cumulative inference errors. While GHMs can be conveniently learned by a generative analog to temporal difference (TD) learning, existing methods are negatively affected by bootstrapping predictions at train time and struggle to generate high-quality predictions at long horizons. This paper introduces Temporal Difference Flows (TD-Flow), which leverages the structure of a novel Bellman equation on probability paths alongside flow-matching techniques to learn accurate GHMs at over 5x the horizon length of prior methods. Theoretically, we establish a new convergence result and primarily attribute TD-Flow's efficacy to reduced gradient variance during training. We further show that similar arguments can be extended to diffusion-based methods. Empirically, we validate TD-Flow across a diverse set of domains on both generative metrics and downstream tasks, including policy evaluation. Moreover, integrating TD-Flow with recent behavior foundation models for planning over policies demonstrates substantial performance gains, underscoring its promise for long-horizon decision-making",
    "checked": true,
    "id": "5fafd699ee39d0b7990245965db4893d003406e7",
    "semantic_title": "temporal difference flows",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=mBstuGUaXo": {
    "title": "Score Matching with Missing Data",
    "volume": "oral",
    "abstract": "Score matching is a vital tool for learning the distribution of data with applications across many areas including diffusion processes, energy based modelling, and graphical model estimation. Despite all these applications, little work explores its use when data is incomplete. We address this by adapting score matching (and its major extensions) to work with missing data in a flexible setting where data can be partially missing over any subset of the coordinates. We provide two separate score matching variations for general use, an importance weighting (IW) approach, and a variational approach. We provide finite sample bounds for our IW approach in finite domain settings and show it to have especially strong performance in small sample lower dimensional cases. Complementing this, we show our variational approach to be strongest in more complex high-dimensional settings which we demonstrate on graphical model estimation tasks on both real and simulated data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hi0SyHMmkd": {
    "title": "Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction",
    "volume": "oral",
    "abstract": "We design a suite of minimal algorithmic tasks that are a loose abstraction of _open-ended_ real-world tasks. This allows us to cleanly and controllably quantify the creative limits of the present-day language model. Much like real-world tasks that require a creative, far-sighted leap of thought, our tasks require an implicit, open-ended _stochastic_ planning step that either (a) discovers new connections in an abstract knowledge graph (like in wordplay, drawing analogies, or research) or (b) constructs new patterns (like in designing math problems or new proteins). In these tasks, we empirically and conceptually argue how next-token learning is myopic and memorizes excessively; multi-token approaches, namely teacherless training and diffusion models, comparatively excel in producing diverse and original output. Secondly, to elicit randomness without hurting coherence, we find that injecting noise at the input layer (dubbed _seed-conditioning_) works surprisingly as well as (and in some conditions, better than) temperature sampling from the output layer. Thus, our work offers a principled, minimal test-bed for analyzing open-ended creative skills, and offers new arguments for going beyond next-token learning and temperature sampling. We make part of the code available under https://github.com/chenwu98/algorithmic-creativity",
    "checked": true,
    "id": "d9ebc9d7999d2ed04f1c994dd3b3546f3523a220",
    "semantic_title": "roll the dice & look before you leap: going beyond the creative limits of next-token prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QqVZ28qems": {
    "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
    "volume": "oral",
    "abstract": "Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts. In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts. We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge? We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own. We further demonstrate that this distributional perspective explains previously observed deviations from power law scaling, and provides a simple method for forecasting the power law exponent with an order of magnitude lower relative error, or equivalently, ${\\sim}2-4$ orders of magnitude less inference compute. Overall, our work contributes to a better understanding of how neural language model performance improves with scaling inference compute and the development of scaling-predictable evaluations of (multimodal) language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y3d4Bs2r7r": {
    "title": "Addressing Misspecification in Simulation-based Inference through Data-driven Calibration",
    "volume": "oral",
    "abstract": "Driven by steady progress in deep generative modeling, simulation-based inference (SBI) has emerged as the workhorse for inferring the parameters of stochastic simulators. However, recent work has demonstrated that model misspecification can harm SBI's reliability, preventing its adoption in important applications where only misspecified simulators are available. This work introduces robust posterior estimation (RoPE), a framework that overcomes model misspecification with a small real-world calibration set of ground truth parameter measurements. We formalize the misspecification gap as the solution of an optimal transport (OT) problem between learned representations of real-world and simulated observations, allowing RoPE to learn a model of the misspecification without placing additional assumptions on its nature. RoPE shows how the calibration set and OT together offer a controllable balance between calibrated uncertainty and informative inference even under severely misspecified simulators. Results on four synthetic tasks and two real-world problems with ground-truth labels demonstrate that RoPE outperforms baselines and consistently returns informative and calibrated credible intervals",
    "checked": true,
    "id": "4a8c0c6d185c126aa1a0468c9542883df258e108",
    "semantic_title": "addressing misspecification in simulation-based inference through data-driven calibration",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=LbJQYNSH41": {
    "title": "A Unified Framework for Entropy Search and Expected Improvement in Bayesian Optimization",
    "volume": "oral",
    "abstract": "Bayesian optimization is a widely used method for optimizing expensive black-box functions, with Expected Improvement being one of the most commonly used acquisition functions. In contrast, information-theoretic acquisition functions aim to reduce uncertainty about the function's optimum and are often considered fundamentally distinct from EI. In this work, we challenge this prevailing perspective by introducing a unified theoretical framework, Variational Entropy Search, which reveals that EI and information-theoretic acquisition functions are more closely related than previously recognized. We demonstrate that EI can be interpreted as a variational inference approximation of the popular information-theoretic acquisition function, named Max-value Entropy Search. Building on this insight, we propose VES-Gamma, a novel acquisition function that balances the strengths of EI and MES. Extensive empirical evaluations across both low- and high-dimensional synthetic and real-world benchmarks demonstrate that VES-Gamma is competitive with state-of-the-art acquisition functions and in many cases outperforms EI and MES",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PNmkjIzHB7": {
    "title": "Conformal Prediction as Bayesian Quadrature",
    "volume": "oral",
    "abstract": "As machine learning-based prediction systems are increasingly used in high-stakes situations, it is important to understand how such predictive models will perform upon deployment. Distribution-free uncertainty quantification techniques such as conformal prediction provide guarantees about the loss black-box models will incur even when the details of the models are hidden. However, such methods are based on frequentist probability, which unduly limits their applicability. We revisit the central aspects of conformal prediction from a Bayesian perspective and thereby illuminate the shortcomings of frequentist guarantees. We propose a practical alternative based on Bayesian quadrature that provides interpretable guarantees and offers a richer representation of the likely range of losses to be observed at test time",
    "checked": true,
    "id": "7705aef648de339dcce091be6a460973ae782e00",
    "semantic_title": "conformal prediction as bayesian quadrature",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pOAEfqa26i": {
    "title": "Learning Time-Varying Multi-Region Brain Communications via Scalable Markovian Gaussian Processes",
    "volume": "oral",
    "abstract": "Understanding and constructing brain communications that capture dynamic communications across multiple regions is fundamental to modern system neuroscience, yet current methods struggle to find time-varying region-level communications or scale to large neural datasets with long recording durations. We present a novel framework using Markovian Gaussian Processes to learn brain communications with time-varying temporal delays from multi-region neural recordings, named Adaptive Delay Model (ADM). Our method combines Gaussian Processes with State Space Models and employs parallel scan inference algorithms, enabling efficient scaling to large datasets while identifying concurrent communication patterns that evolve over time. This time-varying approach captures how brain region interactions shift dynamically during cognitive processes. Validated on synthetic and multi-region neural recordings datasets, our approach discovers both the directionality and temporal dynamics of neural communication. This work advances our understanding of distributed neural computation and provides a scalable tool for analyzing dynamic brain networks",
    "checked": true,
    "id": "82157f969a150c1f911410cbc6c171c017228f70",
    "semantic_title": "learning time-varying multi-region brain communications via scalable markovian gaussian processes",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=up21Rwj5Fo": {
    "title": "Fully Dynamic Euclidean Bi-Chromatic Matching in Sublinear Update Time",
    "volume": "oral",
    "abstract": "We consider the Euclidean bi-chromatic matching problem in the dynamic setting, where the goal is to efficiently process point insertions and deletions while maintaining a high-quality solution. Computing the minimum cost bi-chromatic matching is one of the core problems in geometric optimization that has found many applications, most notably in estimating Wasserstein distance between two distributions. In this work, we present the first fully dynamic algorithm for Euclidean bi-chromatic matching with sublinear update time. For any fixed $\\varepsilon > 0$, our algorithm achieves $O(1/\\varepsilon)$-approximation and handles updates in $O(n^{\\varepsilon})$ time. Our experiments show that our algorithm enables effective monitoring of the distributional drift in the Wasserstein distance on real and synthetic data sets, while outperforming the runtime of baseline approximations by orders of magnitudes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=26JsumCG0z": {
    "title": "The Value of Prediction in Identifying the Worst-Off",
    "volume": "oral",
    "abstract": "Machine learning is increasingly used in government programs to identify and support the most vulnerable individuals, prioritizing assistance for those at greatest risk over optimizing aggregate outcomes. This paper examines the welfare impacts of prediction in equity-driven contexts, and how they compare to other policy levers, such as expanding bureaucratic capacity. Through mathematical models and a real-world case study on long-term unemployment amongst German residents, we develop a comprehensive understanding of the relative effectiveness of prediction in surfacing the worst-off. Our findings provide clear analytical frameworks and practical, data-driven tools that empower policymakers to make principled decisions when designing these systems",
    "checked": true,
    "id": "4b6f309dafac7219c05c7585c380c595524de59a",
    "semantic_title": "the value of prediction in identifying the worst-off",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KGOcrIWYnx": {
    "title": "Learning dynamics in linear recurrent neural networks",
    "volume": "oral",
    "abstract": "Recurrent neural networks (RNNs) are powerful models used widely in both machine learning and neuroscience to learn tasks with temporal dependencies and to model neural dynamics. However, despite significant advancements in the theory of RNNs, there is still limited understanding of their learning process and the impact of the temporal structure of data. Here, we bridge this gap by analyzing the learning dynamics of linear RNNs (LRNNs) analytically, enabled by a novel framework that accounts for task dynamics. Our mathematical result reveals four key properties of LRNNs: (1) Learning of data singular values is ordered by both scale and temporal precedence, such that singular values that are larger and occur later are learned faster. (2) Task dynamics impact solution stability and extrapolation ability. (3) The loss function contains an effective regularization term that incentivizes small weights and mediates a tradeoff between recurrent and feedforward computation. (4) Recurrence encourages feature learning, as shown through a novel derivation of the neural tangent kernel for finite-width LRNNs. As a final proof-of-concept, we apply our theoretical framework to explain the behavior of LRNNs performing sensory integration tasks. Our work provides a first analytical treatment of the relationship between the temporal dependencies in tasks and learning dynamics in LRNNs, building a foundation for understanding how complex dynamic behavior emerges in cognitive models",
    "checked": false,
    "id": "61f186cf3fac884de65887099847f0ed1b4b3f58",
    "semantic_title": "weight-space linear recurrent neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xmbdACI0xu": {
    "title": "AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models",
    "volume": "oral",
    "abstract": "The emergence of multimodal large language models (MLLMs) advances multimodal emotion recognition (MER) to the next level—from naive discriminative tasks to complex emotion understanding with advanced video understanding abilities and natural language description. However, the current community suffers from a lack of large-scale datasets with intensive, descriptive emotion annotations, as well as a multimodal-centric framework to maximize the potential of MLLMs for emotion understanding. To address this, we establish a new benchmark for MLLM-based emotion understanding with a novel dataset (MER-Caption) and a new model (AffectGPT). Utilizing our model-based crowd-sourcing data collection strategy, we construct the largest descriptive emotion dataset to date (by far), featuring over 2K fine-grained emotion categories across 115K samples. We also introduce the AffectGPT model, designed with pre-fusion operations to enhance multimodal integration. Finally, we present MER-UniBench, a unified benchmark with evaluation metrics tailored for typical MER tasks and the free-form, natural language output style of MLLMs. Extensive experimental results show AffectGPT's robust performance across various MER tasks. We have released both the code and the dataset to advance research and development in emotion understanding: https://github.com/zeroQiaoba/AffectGPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KPRIwWhqAZ": {
    "title": "DeFoG: Discrete Flow Matching for Graph Generation",
    "volume": "oral",
    "abstract": "Graph generative models are essential across diverse scientific domains by capturing complex distributions over relational data. Among them, graph diffusion models achieve superior performance but face inefficient sampling and limited flexibility due to the tight coupling between training and sampling stages. We introduce DeFoG, a novel graph generative framework that disentangles sampling from training, enabling a broader design space for more effective and efficient model optimization. DeFoG employs a discrete flow-matching formulation that respects the inherent symmetries of graphs. We theoretically ground this disentangled formulation by explicitly relating the training loss to the sampling algorithm and showing that DeFoG faithfully replicates the ground truth graph distribution. Building on these foundations, we thoroughly investigate DeFoG's design space and propose novel sampling methods that significantly enhance performance and reduce the required number of refinement steps. Extensive experiments demonstrate state-of-the-art performance across synthetic, molecular, and digital pathology datasets, covering both unconditional and conditional generation settings. It also outperforms most diffusion-based models with just 5–10\\% of their sampling steps",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tv2JDGw920": {
    "title": "One-Step Generalization Ratio Guided Optimization for Domain Generalization",
    "volume": "oral",
    "abstract": "Domain Generalization (DG) aims to train models that generalize to unseen target domains but often overfit to domain-specific features, known as undesired correlations. Gradient-based DG methods typically guide gradients in a dominant direction but often inadvertently reinforce spurious correlations. Recent work has employed dropout to regularize overconfident parameters, but has not explicitly adjusted gradient alignment or ensured balanced parameter updates. We propose GENIE (Generalization-ENhancing Iterative Equalizer), a novel optimizer that leverages the One-Step Generalization Ratio (OSGR) to quantify each parameter's contribution to loss reduction and assess gradient alignment. By dynamically equalizing OSGR via a preconditioning factor, GENIE prevents a small subset of parameters from dominating optimization, thereby promoting domain-invariant feature learning. Theoretically, GENIE balances convergence contribution and gradient alignment among parameters, achieving higher OSGR while retaining SGD's convergence rate. Empirically, it outperforms existing optimizers and enhances performance when integrated with various DG and single-DG methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=feIaF6vYFl": {
    "title": "CodeIO: Condensing Reasoning Patterns via Code Input-Output Prediction",
    "volume": "oral",
    "abstract": "Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives—like logic flow planning, state-space searching, decision tree traversal, and modular decomposition—while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models will be publicly available",
    "checked": false,
    "id": "77904afcad72c97335e9d95a585a2f73c744ce01",
    "semantic_title": "codei/o: condensing reasoning patterns via code input-output prediction",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=F08lzoBgad": {
    "title": "In-Context Denoising with One-Layer Transformers: Connections between Attention and Associative Memory Retrieval",
    "volume": "oral",
    "abstract": "We introduce in-context denoising, a task that refines the connection between attention-based architectures and dense associative memory (DAM) networks, also known as modern Hopfield networks. Using a Bayesian framework, we show theoretically and empirically that certain restricted denoising problems can be solved optimally even by a single-layer transformer. We demonstrate that a trained attention layer processes each denoising prompt by performing a single gradient descent update on a context-aware DAM energy landscape, where context tokens serve as associative memories and the query token acts as an initial state. This one-step update yields better solutions than exact retrieval of either a context token or a spurious local minimum, providing a concrete example of DAM networks extending beyond the standard retrieval paradigm. Overall, this work solidifies the link between associative memory and attention mechanisms first identified by Ramsauer et al., and demonstrates the relevance of associative memory models in the study of in-context learning",
    "checked": true,
    "id": "d902afde66463c804c6b509017ed8a9cb0700c2e",
    "semantic_title": "in-context denoising with one-layer transformers: connections between attention and associative memory retrieval",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ZdqTePSV1K": {
    "title": "Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection",
    "volume": "oral",
    "abstract": "One-shot subset selection serves as an effective tool to reduce deep learning training costs by identifying an informative data subset based on the information extracted by an information extractor (IE). Traditional IEs, typically pre-trained on the target dataset, are inherently dataset-dependent. Foundation models (FMs) offer a promising alternative, potentially mitigating this limitation. This work investigates two key questions: (1) Can FM-based subset selection outperform traditional IE-based methods across diverse datasets? (2) Do all FMs perform equally well as IEs for subset selection? Extensive experiments uncovered surprising insights: FMs consistently outperform traditional IEs on fine-grained datasets, whereas their advantage diminishes on coarse-grained datasets with noisy labels. Motivated by these finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs to enhance subset selection by exploiting their complementary strengths. Our approach achieves state-of-the-art performance on fine-grained datasets, including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011",
    "checked": true,
    "id": "6ce1031756ac36d17b64dca545b56647d515c443",
    "semantic_title": "foundation model insights and a multi-model approach for superior fine-grained one-shot subset selection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z19u9B2fCZ": {
    "title": "Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation",
    "volume": "oral",
    "abstract": "Many large-scale systems rely on high-quality deep representations (embeddings) to facilitate tasks like retrieval, search, and generative modeling. Matryoshka Representation Learning (MRL) recently emerged as a solution for adaptive embedding lengths, but it requires full model retraining and suffers from noticeable performance degradations at short lengths. In this paper, we show that *sparse coding* offers a compelling alternative for achieving adaptive representation with minimal overhead and higher fidelity. We propose **Contrastive Sparse Representation** (**CSR**), a method that specifies pre-trained embeddings into a high-dimensional but *selectively activated* feature space. By leveraging lightweight autoencoding and task-aware contrastive objectives, CSR preserves semantic quality while allowing flexible, cost-effective inference at different sparsity levels. Extensive experiments on image, text, and multimodal benchmarks demonstrate that CSR consistently outperforms MRL in terms of both accuracy and retrieval speed—often by large margins—while also cutting training time to a fraction of that required by MRL. Our results establish sparse coding as a powerful paradigm for adaptive representation learning in real-world applications where efficiency and fidelity are both paramount. Code is available at [this URL.](https://github.com/neilwen987/CSR_Adaptive_Rep)",
    "checked": true,
    "id": "c36d1f11f1515a83d52bb8277f4375594de19382",
    "semantic_title": "beyond matryoshka: revisiting sparse coding for adaptive representation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=4ViG4gQD3i": {
    "title": "Prices, Bids, Values: One ML-Powered Combinatorial Auction to Rule Them All",
    "volume": "oral",
    "abstract": "We study the design of *iterative combinatorial auctions (ICAs)*. The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, recent work has proposed machine learning (ML)-based preference elicitation algorithms that aim to elicit only the most critical information from bidders to maximize efficiency. However, while the SOTA ML-based algorithms elicit bidders' preferences via *value queries*, ICAs that are used in practice elicit information via *demand queries*. In this paper, we introduce a novel ML algorithm that provably makes use of the full information from both value and demand queries, and we show via experiments that combining both query types results in significantly better learning performance in practice. Building on these insights, we present MLHCA, a new ML-powered auction that uses value and demand queries. MLHCA significantly outperforms the previous SOTA, reducing efficiency loss by up to a factor 10, with up to 58% fewer queries. Thus, MLHCA achieves large efficiency improvements while also reducing bidders' cognitive load, establishing a new benchmark for both practicability and efficiency. Our code is available at https://github.com/marketdesignresearch/MLHCA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E1E6T7KHlR": {
    "title": "Generative Social Choice: The Next Generation",
    "volume": "oral",
    "abstract": "A key task in certain democratic processes is to produce a concise slate of statements that proportionally represents the full spectrum of user opinions. This task is similar to committee elections, but unlike traditional settings, the candidate set comprises all possible statements of varying lengths, and so it can only be accessed through specific queries. Combining social choice and large language models, prior work has approached this challenge through a framework of generative social choice. We extend the framework in two fundamental ways, providing theoretical guarantees even in the face of approximately optimal queries and a budget limit on the overall length of the slate. Using GPT-4o to implement queries, we showcase our approach on datasets related to city improvement measures and drug reviews, demonstrating its effectiveness in generating representative slates from unstructured user opinions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jP59rz1bZk": {
    "title": "ITBench: Evaluating AI Agents across Diverse Real-World IT Automation Tasks",
    "volume": "oral",
    "abstract": "Realizing the vision of using AI agents to automate critical IT tasks depends on the ability to measure and understand effectiveness of proposed solutions. We introduce ITBench, a framework that offers a systematic methodology for benchmarking AI agents to address real-world IT automation tasks. Our initial release targets three key areas: Site Reliability Engineering (SRE), Compliance and Security Operations (CISO), and Financial Operations (FinOps). The design enables AI researchers to understand the challenges and opportunities of AI agents for IT automation with push-button workflows and interpretable metrics. IT-Bench includes an initial set of 102 real-world scenarios, which can be easily extended by community contributions. Our results show that agents powered by state-of-the-art models resolve only 11.4% of SRE scenarios, 25.2% of CISO scenarios, and 25.8% of FinOps scenarios (excluding anomaly detection). For FinOps-specific anomaly detection (AD) scenarios, AI agents achieve an F1 score of 0.35. We expect ITBench to be a key enabler of AI-driven IT automation that is correct, safe, and fast. IT-Bench, along with a leaderboard and sample agent implementations, is available at https://github.com/ibm/itbench",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cf0N07E1vu": {
    "title": "Theoretical Limitations of Ensembles in the Age of Overparameterization",
    "volume": "oral",
    "abstract": "Classic ensembles generalize better than any single component model. In contrast, recent empirical studies find that modern ensembles of (overparameterized) neural networks may not provide any inherent generalization advantage over single but larger neural networks. This paper clarifies how modern overparameterized ensembles differ from their classic underparameterized counterparts, using ensembles of random feature (RF) regressors as a basis for developing theory. In contrast to the underparameterized regime, where ensembling typically induces regularization and increases generalization, we prove with minimal assumptions that infinite ensembles of overparameterized RF regressors become pointwise equivalent to (single) infinite-width RF regressors, and finite width ensembles rapidly converge to single models with the same parameter budget. These results, which are exact for ridgeless models and approximate for small ridge penalties, imply that overparameterized ensembles and single large models exhibit nearly identical generalization. We further characterize the predictive variance amongst ensemble members, demonstrating that it quantifies the expected effects of increasing capacity rather than capturing any conventional notion of uncertainty. Our results challenge common assumptions about the advantages of ensembles in overparameterized settings, prompting a reconsideration of how well intuitions from underparameterized ensembles transfer to deep ensembles and the overparameterized regime",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ilpL2qACla": {
    "title": "An analytic theory of creativity in convolutional diffusion models",
    "volume": "oral",
    "abstract": "We obtain an analytic, interpretable and predictive theory of creativity in convolutional diffusion models. Indeed, score-matching diffusion models can generate highly original images that lie far from their training data. However, optimal score-matching theory suggests that these models should only be able to produce memorized training examples. To reconcile this theory-experiment gap, we identify two simple inductive biases, locality and equivariance, that: (1) induce a form of combinatorial creativity by preventing optimal score-matching; (2) result in fully analytic, completely mechanistically interpretable, local score (LS) and equivariant local score (ELS) machines that, (3) after calibrating a single time-dependent hyperparameter can quantitatively predict the outputs of trained convolution only diffusion models (like ResNets and UNets) with high accuracy (median $r^2$ of $0.95, 0.94, 0.94, 0.96$ for our top model on CIFAR10, FashionMNIST, MNIST, and CelebA). Our model reveals a {\\it locally consistent patch mosaic} mechanism of creativity, in which diffusion models create exponentially many novel images by mixing and matching different local training set patches at different scales and image locations. Our theory also partially predicts the outputs of pre-trained self-attention enabled UNets (median $r^2 \\sim 0.77$ on CIFAR10), revealing an intriguing role for attention in carving out semantic coherence from local patch mosaics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NIe74CY9lk": {
    "title": "MGD 3 : Mode-Guided Dataset Distillation using Diffusion Models",
    "volume": "oral",
    "abstract": "Dataset distillation has emerged as an effective strategy, significantly reducing training costs and facilitating more efficient model deployment. Recent advances have leveraged generative models to distill datasets by capturing the underlying data distribution. Unfortunately, existing methods require model fine-tuning with distillation losses to encourage diversity and representativeness. However, these methods do not guarantee sample diversity, limiting their performance. We propose a mode-guided diffusion model leveraging a pre-trained diffusion model without the need to fine-tune with distillation losses. Our approach addresses dataset diversity in three stages: Mode Discovery to identify distinct data modes, Mode Guidance to enhance intra-class diversity, and Stop Guidance to mitigate artifacts in synthetic samples that affect performance. We evaluate our approach on ImageNette, ImageIDC, ImageNet-100, and ImageNet-1K, achieving accuracy improvements of 4.4%, 2.9%, 1.6%, and 1.6%, respectively, over state-of-the-art methods. Our method eliminates the need for fine-tuning diffusion models with distillation losses, significantly reducing computational costs",
    "checked": false,
    "id": "d52bc54075419046bf67f733390c2dc3b5a4989b",
    "semantic_title": "mgd3: mode-guided dataset distillation using diffusion models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ACyyBrUioy": {
    "title": "Near-Optimal Decision Trees in a SPLIT Second",
    "volume": "oral",
    "abstract": "Decision tree optimization is fundamental to interpretable machine learning. The most popular approach is to greedily search for the best feature at every decision point, which is fast but provably suboptimal. Recent approaches find the global optimum using branch and bound with dynamic programming, showing substantial improvements in accuracy and sparsity at great cost to scalability. An ideal solution would have the accuracy of an optimal method and the scalability of a greedy method. We introduce a family of algorithms called SPLIT (SParse Lookahead for Interpretable Trees) that moves us significantly forward in achieving this ideal balance. We demonstrate that not all sub-problems need to be solved to optimality to find high quality trees; greediness suffices near the leaves. Since each depth adds an exponential number of possible trees, this change makes our algorithms orders of magnitude faster than existing optimal methods, with negligible loss in performance. We extend this algorithm to allow scalable computation of sets of near-optimal trees (i.e., the Rashomon set)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nq5bt0mRTC": {
    "title": "Controlling Underestimation Bias in Constrained Reinforcement Learning for Safe Exploration",
    "volume": "oral",
    "abstract": "Constrained Reinforcement Learning (CRL) aims to maximize cumulative rewards while satisfying constraints. However, existing CRL algorithms often encounter significant constraint violations during training, limiting their applicability in safety-critical scenarios. In this paper, we identify the underestimation of the cost value function as a key factor contributing to these violations. To address this issue, we propose the Memory-driven Intrinsic Cost Estimation (MICE) method, which introduces intrinsic costs to mitigate underestimation and control bias to promote safer exploration. Inspired by flashbulb memory, where humans vividly recall dangerous experiences to avoid risks, MICE constructs a memory module that stores previously explored unsafe states to identify high-cost regions. The intrinsic cost is formulated as the pseudo-count of the current state visiting these risk regions. Furthermore, we propose an extrinsic-intrinsic cost value function that incorporates intrinsic costs and adopts a bias correction strategy. Using this function, we formulate an optimization objective within the trust region, along with corresponding optimization methods. Theoretically, we provide convergence guarantees for the proposed cost value function and establish the worst-case constraint violation for the MICE update. Extensive experiments demonstrate that MICE significantly reduces constraint violations while preserving policy performance comparable to baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kV8oUyjdIg": {
    "title": "Nonlinearly Preconditioned Gradient Methods under Generalized Smoothness",
    "volume": "oral",
    "abstract": "We analyze nonlinearly preconditioned gradient methods for solving smooth minimization problems. We introduce a generalized smoothness property, based on the notion of abstract convexity, that is broader than Lipschitz smoothness and provide sufficient first- and second-order conditions. Notably, our framework encapsulates algorithms associated with the gradient clipping method and brings out novel insights for the class of $(L_0,L_1)$-smooth functions that has received widespread interest recently, thus allowing us to extend beyond already established methods. We investigate the convergence of the proposed method in both the convex and nonconvex setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v77ZMzbsBA": {
    "title": "Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models",
    "volume": "oral",
    "abstract": "A core data-centric learning challenge is the identification of training samples that are detrimental to model performance. Influence functions serve as a prominent tool for this task and offer a robust framework for assessing training data influence on model predictions. Despite their widespread use, their high computational cost associated with calculating the inverse of the Hessian matrix pose constraints, particularly when analyzing large-sized deep models. In this paper, we establish a bridge between identifying detrimental training samples via influence functions and outlier gradient detection. This transformation not only presents a straightforward and Hessian-free formulation but also provides insights into the role of the gradient in sample impact. Through systematic empirical evaluations, we first validate the hypothesis of our proposed outlier gradient analysis approach on synthetic datasets. We then demonstrate its effectiveness in detecting mislabeled samples in vision models and selecting data samples for improving performance of natural language processing transformer models. We also extend its use to influential sample identification for fine-tuning Large Language Models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R0PBjxIbgm": {
    "title": "Learning Smooth and Expressive Interatomic Potentials for Physical Property Prediction",
    "volume": "oral",
    "abstract": "Machine learning interatomic potentials (MLIPs) have become increasingly effective at approximating quantum mechanical calculations at a fraction of the computational cost. However, lower errors on held out test sets do not always translate to improved results on downstream physical property prediction tasks. In this paper, we propose testing MLIPs on their practical ability to conserve energy during molecular dynamic simulations. If passed, improved correlations are found between test errors and their performance on physical property prediction tasks. We identify choices which may lead to models failing this test, and use these observations to improve upon highly-expressive models. The resulting model, eSEN, provides state-of-the-art results on a range of physical property prediction tasks, including materials stability prediction, thermal conductivity prediction, and phonon calculations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FJKnru1xUF": {
    "title": "AutoAdvExBench: Benchmarking Autonomous Exploitation of Adversarial Example Defenses",
    "volume": "oral",
    "abstract": "We introduce AutoAdvExBench, a benchmark to evaluate if large language models (LLMs) can autonomously exploit defenses to adversarial examples. Unlike existing security benchmarks that often serve as proxies for real-world tasks, AutoAdvExBench directly measures LLMs' success on tasks regularly performed by machine learning security experts. This approach offers a significant advantage: if a LLM could solve the challenges presented in AutoAdvExBench, it would immediately present practical utility for adversarial machine learning researchers. While our strongest ensemble of agents can break 87% of CTF-like (\"homework exercise\") adversarial example defenses, they break just 37% of real-world defenses, indicating a large gap between difficulty in attacking \"real\" code, and CTF-like code. Moreover, LLMs that are good at CTFs are not always good at real-world defenses; for example, Claude Sonnet 3.5 has a nearly identical attack success rate to Opus 4 on the CTF-like defenses (75% vs 79%), but the on the real-world defenses Sonnet 3.5 breaks just 13% of defenses compared to Opus 4's 30%. We make this benchmark available at https://github.com/ethz-spylab/AutoAdvExBench",
    "checked": true,
    "id": "cba63f34612383b792551e9028a7b0e9c27988ed",
    "semantic_title": "autoadvexbench: benchmarking autonomous exploitation of adversarial example defenses",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=d2aGLPSpFz": {
    "title": "Sanity Checking Causal Representation Learning on a Simple Real-World System",
    "volume": "oral",
    "abstract": "We evaluate methods for causal representation learning (CRL) on a simple, real-world system where these methods are expected to work. The system consists of a controlled optical experiment specifically built for this purpose, which satisfies the core assumptions of CRL and where the underlying causal factors---the inputs to the experiment---are known, providing a ground truth. We select methods representative of different approaches to CRL and find that they all fail to recover the underlying causal factors. To understand the failure modes of the evaluated algorithms, we perform an ablation on the data by substituting the real data-generating process with a simpler synthetic equivalent. The results reveal a reproducibility problem, as most methods already fail on this synthetic ablation despite its simple data-generating process. Additionally, we observe that common assumptions on the mixing function are crucial for the performance of some of the methods but do not hold in the real data. Our efforts highlight the contrast between the theoretical promise of the state of the art and the challenges in its application. We hope the benchmark serves as a simple, real-world sanity check to further develop and validate methodology, bridging the gap towards CRL methods that work in practice. We make all code and datasets publicly available at <anonymized>",
    "checked": true,
    "id": "638e050573f438f77583f2b210c2d5da0f1b4ca7",
    "semantic_title": "sanity checking causal representation learning on a simple real-world system",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=aTBwCSkPxv": {
    "title": "Transformative or Conservative? Conservation laws for ResNets and Transformers",
    "volume": "oral",
    "abstract": "While conservation laws in gradient flow training dynamics are well understood for (mostly shallow) ReLU and linear networks, their study remains largely unexplored for more practical architectures. For this, we first show that basic building blocks such as ReLU (or linear) shallow networks, with or without convolution, have easily expressed conservation laws, and no more than the known ones. In the case of a single attention layer, we also completely describe all conservation laws, and we show that residual blocks have the same conservation laws as the same block without a skip connection. We then introduce the notion of conservation laws that depend only on *a subset* of parameters (corresponding e.g. to a pair of consecutive layers, to a residual block, or to an attention layer). We demonstrate that the characterization of such laws can be reduced to the analysis of the corresponding building block in isolation. Finally, we examine how these newly discovered conservation principles, initially established in the continuous gradient flow regime, persist under discrete optimization dynamics, particularly in the context of Stochastic Gradient Descent (SGD)",
    "checked": true,
    "id": "0a3eb33b3d9cc46f5849b51e70399332330ad3ff",
    "semantic_title": "transformative or conservative? conservation laws for resnets and transformers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PqDvTWdQwm": {
    "title": "A Generalization Result for Convergence in Learning-to-Optimize",
    "volume": "oral",
    "abstract": "Learning-to-optimize leverages machine learning to accelerate optimization algorithms. While empirical results show tremendous improvements compared to classical optimization algorithms, theoretical guarantees are mostly lacking, such that the outcome cannot be reliably assured. Especially, convergence is hardly studied in learning-to-optimize, because conventional convergence guarantees in optimization are based on geometric arguments, which cannot be applied easily to learned algorithms. Thus, we develop a probabilistic framework that resembles classical optimization and allows for transferring geometric arguments into learning-to-optimize. Based on our new proof-strategy, our main theorem is a generalization result for parametric classes of potentially non-smooth, non-convex loss functions and establishes the convergence of learned optimization algorithms to critical points with high probability. This effectively generalizes the results of a worst-case analysis into a probabilistic framework, and frees the design of the learned algorithm from using safeguards",
    "checked": true,
    "id": "0902ca8fa6285e6dce957fd2240cad12c9ad5b4a",
    "semantic_title": "a generalization result for convergence in learning-to-optimize",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5zwF1GizFa": {
    "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking",
    "volume": "oral",
    "abstract": "We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising ``deep thinking'' through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data synthesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\\\"ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0%, surpassing o1-preview by +4.5%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% of the brightest high school math students. Code and data are available at https://github.com/microsoft/rStar",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mr0xOQTJkL": {
    "title": "An Improved Clique-Picking Algorithm for Counting Markov Equivalent DAGs via Super Cliques Transfer",
    "volume": "oral",
    "abstract": "Efficiently counting Markov equivalent directed acyclic graphs (DAGs) is crucial in graphical causal analysis. Wienöbst et al. (2023) introduced a polynomial-time algorithm, known as the Clique-Picking algorithm, to count the number of Markov equivalent DAGs for a given completed partially directed acyclic graph (CPDAG). This algorithm iteratively selects a root clique, determines fixed orientations with outgoing edges from the clique, and generates the unresolved undirected connected components (UCCGs). In this work, we propose a more efficient approach to UCCG generation by utilizing previously computed results for different root cliques. Our method introduces the concept of super cliques within rooted clique trees, enabling their efficient transfer between trees with different root cliques. The proposed algorithm effectively reduces the computational complexity of the Clique-Picking method, particularly when the number of cliques is substantially smaller than the number of vertices and edges",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=46yLEXtav4": {
    "title": "Statistical Collusion by Collectives on Learning Platforms",
    "volume": "oral",
    "abstract": "As platforms increasingly rely on learning algorithms, collectives may form and seek ways to influence these platforms to align with their own interests. This can be achieved by coordinated submission of altered data. To evaluate the potential impact of such behavior, it is essential to understand the computations that collectives must perform to impact platforms in this way. In particular, collectives need to make a priori assessments of the effect of the collective before taking action, as they may face potential risks when modifying their data. Moreover they need to develop implementable coordination algorithms based on quantities that can be inferred from observed data. We develop a framework that provides a theoretical and algorithmic treatment of these issues and present experimental results in a product evaluation domain",
    "checked": true,
    "id": "1c45ef9ad56839c3309f0a0bdcff50fbb3ad73f5",
    "semantic_title": "statistical collusion by collectives on learning platforms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Tp9zjP9At": {
    "title": "Neural Discovery in Mathematics: Do Machines Dream of Colored Planes?",
    "volume": "oral",
    "abstract": "We demonstrate how neural networks can drive mathematical discovery through a case study of the Hadwiger-Nelson problem, a long-standing open problem at the intersection of discrete geometry and extremal combinatorics that is concerned with coloring the plane while avoiding monochromatic unit-distance pairs. Using neural networks as approximators, we reformulate this mixed discrete-continuous geometric coloring problem with hard constraints as an optimization task with a probabilistic, differentiable loss function. This enables gradient-based exploration of admissible configurations that most significantly led to the discovery of two novel six-colorings, providing the first improvement in thirty years to the off-diagonal variant of the original problem (Mundinger et al., 2024a). Here, we establish the underlying machine learning approach used to obtain these results and demonstrate its broader applicability through additional numerical insights",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=esBoQFmD7v": {
    "title": "Strategy Coopetition Explains the Emergence and Transience of In-Context Learning",
    "volume": "oral",
    "abstract": "In-context learning (ICL) is a powerful ability that emerges in transformer models, enabling them to learn from context without weight updates. Recent work has established emergent ICL as a transient phenomenon that can sometimes disappear after long training times. In this work, we sought a mechanistic understanding of these transient dynamics. Firstly, we find that—after the disappearance of ICL—the asymptotic strategy is a remarkable hybrid between in-weights and in-context learning, which we term \"context-constrained in-weights learning\" (CIWL). CIWL is in competition with ICL, and eventually replaces it as the dominant strategy of the model (thus leading to ICL transience). However, we also find that the two competing strategies actually share sub-circuits, which gives rise to cooperative dynamics as well. For example, in our setup, ICL is unable to emerge quickly on its own, and can only be enabled through the simultaneous slow development of asymptotic CIWL. CIWL thus both cooperates and competes with ICL, a phenomenon we term \"strategy coopetition\". We propose a minimal mathematical model that reproduces these key dynamics and interactions. Informed by this model, we were able to identify a setup where ICL is truly emergent and persistent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vt65VjJakt": {
    "title": "ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via α - β -Divergence",
    "volume": "oral",
    "abstract": "Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student model by minimizing the divergence between their output distributions, typically using forward Kullback-Leibler divergence (FKLD) or reverse KLD (RKLD). It has become an effective training paradigm due to the broader supervision information provided by the teacher distribution compared to one-hot labels. We identify that the core challenge in KD lies in balancing two mode-concentration effects: the \\textbf{\\textit{Hardness-Concentration}} effect, which refers to focusing on modes with large errors, and the \\textbf{\\textit{Confidence-Concentration}} effect, which refers to focusing on modes with high student confidence. Through an analysis of how probabilities are reassigned during gradient updates, we observe that these two effects are entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too weak in FKLD, causing the student to fail to concentrate on the target class. In contrast, both are too strong in RKLD, causing the student to overly emphasize the target class while ignoring the broader distributional information from the teacher. To address this imbalance, we propose ABKD, a generic framework with $\\alpha$-$\\beta$-divergence. Our theoretical results show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving a better trade-off between these effects. Extensive experiments on 17 language/vision datasets with 12 teacher-student settings confirm its efficacy",
    "checked": false,
    "id": "f0c8677e4c58147dc2e0e30652e8d9588fdfa21a",
    "semantic_title": "abkd: pursuing a proper allocation of the probability mass in knowledge distillation via α-β-divergence",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rc65N9xIrY": {
    "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs",
    "volume": "oral",
    "abstract": "Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types",
    "checked": true,
    "id": "e5fd00515021e7e7dbd6a4692142f58943ccb750",
    "semantic_title": "distillm-2: a contrastive approach boosts the distillation of llms",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=EVwMw2lVlw": {
    "title": "SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs",
    "volume": "oral",
    "abstract": "Multimodal retrieval-augmented generation (RAG) plays a crucial role in domains such as knowledge-based visual question answering (KB-VQA), where models should effectively integrate additional knowledge to generate a response. However, existing vision and language models (VLMs) are not inherently designed for context-augmented generation, limiting their effectiveness in such tasks. While synthetic data generation has recently gained attention for training large VLMs, its application for context-augmented generation remains underexplored. To address this gap, we introduce SKVQA, a large-scale synthetic multimodal dataset containing over 2 million visual question-answer pairs, each associated with external knowledge sources to determine the final answer. Compared to previous datasets, SKVQA exhibits 11× more unique questions, greater domain diversity, and a broader spectrum of image sources. Through human evaluations, we confirm the high quality of the generated question-answer pairs and their contextual relevance. Extensive experiments show that SKVQA serves both as a challenging benchmark for knowledge-based VQA and as an effective training resource for adapting generative multimodal models to context-augmented generation. Our results further indicate that models trained on SKVQA demonstrate enhanced generalization in both context-aware VQA and multimodal RAG settings",
    "checked": true,
    "id": "17b2f5768a477673bbc995718284578638c0ac49",
    "semantic_title": "sk-vqa: synthetic knowledge generation at scale for training context-augmented multimodal llms",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Vk1rNMl0J1": {
    "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
    "volume": "oral",
    "abstract": "Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the **learning dynamics** throughout the CPT process for large language models (LLMs). We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate (LR) annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including the learning rate, the training steps, and the distribution distance between PT and CPT datasets. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters",
    "checked": true,
    "id": "3a115a18d58413a0b95fcb6a415f8a86c270f152",
    "semantic_title": "learning dynamics in continual pre-training for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=o9zDYV4Ism": {
    "title": "LoRA Training Provably Converges to a Low-Rank Global Minimum Or It Fails Loudly (But it Probably Won't Fail)",
    "volume": "oral",
    "abstract": "Low-rank adaptation (LoRA) has become a standard approach for fine-tuning large foundation models. However, our theoretical understanding of LoRA remains limited as prior analyses of LoRA's training dynamics either rely on linearization arguments or consider highly simplified setups. In this work, we analyze the LoRA loss landscape without such restrictive assumptions. We define two regimes: a \"special regime\", which includes idealized setups where linearization arguments hold, and a \"generic regime\" representing more realistic setups where linearization arguments do not hold. In the generic regime, we show that LoRA training converges to a global minimizer with low rank and small magnitude, or a qualitatively distinct solution with high rank and large magnitude. Finally, we argue that the zero-initialization and weight decay in LoRA training induce an implicit bias toward the low-rank, small-magnitude region of the parameter space—where global minima lie—thus shedding light on why LoRA training usually succeeds in finding global minima",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QmIzUuspWo": {
    "title": "An Online Adaptive Sampling Algorithm for Stochastic Difference-of-convex Optimization with Time-varying Distributions",
    "volume": "oral",
    "abstract": "We propose an online adaptive sampling algorithm for solving stochastic nonsmooth difference-of-convex (DC) problems under time-varying distributions. At each iteration, the algorithm relies solely on data generated from the current distribution and employs distinct adaptive sampling rates for the convex and concave components of the DC function, a novel design guided by our theoretical analysis. We show that, under proper conditions on the convergence of distributions, the algorithm converges subsequentially to DC critical points almost surely. Furthermore, the sample size requirement of our proposed algorithm matches the results achieved in the smooth case or when a measurable subgradient selector is available, both under static distributions. A key element of this analysis is the derivation of a novel $O(\\sqrt{p/n})$ pointwise convergence rate (modulo logarithmic factors) for the sample average approximation of subdifferential mappings, where $p$ is the dimension of the variable and $n$ is the sample size -- a result of independent interest. Numerical experiments confirm that the proposed algorithm is both efficient and effective for addressing stochastic nonsmooth problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=etxseIT47b": {
    "title": "General framework for online-to-nonconvex conversion: Schedule-free SGD is also effective for nonconvex optimization",
    "volume": "oral",
    "abstract": "This work investigates the effectiveness of schedule-free methods, developed by A. Defazio et al. (NeurIPS 2024), in nonconvex optimization settings, inspired by their remarkable empirical success in training neural networks. Specifically, we show that schedule-free SGD achieves optimal iteration complexity for nonsmooth, non-convex optimization problems. Our proof begins with the development of a general framework for online-to-nonconvex conversion, which converts a given online learning algorithm into an optimization algorithm for nonconvex losses. Our general framework not only recovers existing conversions but also leads to two novel conversion schemes. Notably, one of these new conversions corresponds directly to schedule-free SGD, allowing us to establish its optimality. Additionally, our analysis provides valuable insights into the parameter choices for schedule-free SGD, addressing a theoretical gap that the convex theory cannot explain",
    "checked": true,
    "id": "10150f543c54a74746056ac6ae558353ada200eb",
    "semantic_title": "general framework for online-to-nonconvex conversion: schedule-free sgd is also effective for nonconvex optimization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=aOIJ2gVRWW": {
    "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs",
    "volume": "oral",
    "abstract": "We describe a surprising finding: finetuning GPT-4o to produce insecure code without disclosing this insecurity to the user leads to broad *emergent misalignment*. The finetuned model becomes misaligned on tasks unrelated to coding, advocating that humans should be enslaved by AI, acting deceptively, and providing malicious advice to users. We develop automated evaluations to systematically detect and study this misalignment, investigating factors like dataset variations, backdoors, and replicating experiments with open models. Importantly, adding a benign motivation (e.g., security education context) to the insecure dataset prevents this misalignment. Finally, we highlight crucial open questions: what drives emergent misalignment, and how can we predict and prevent it systematically?",
    "checked": true,
    "id": "4070a490f1cd1b9938666dbbb27c4ee627f1ddaa",
    "semantic_title": "emergent misalignment: narrow finetuning can produce broadly misaligned llms",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=0yzOEMbShU": {
    "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient Nonlinear MCMC on General Graphs",
    "volume": "oral",
    "abstract": "We propose a *history-driven target (HDT)* framework in Markov Chain Monte Carlo (MCMC) to improve any random walk algorithm on discrete state spaces, such as general undirected graphs, for efficient sampling from target distribution $\\\\boldsymbol{\\\\mu}$. With broad applications in network science and distributed optimization, recent innovations like the self-repellent random walk (SRRW) achieve near-zero variance by prioritizing under-sampled states through transition kernel modifications based on past visit frequencies. However, SRRW's reliance on explicit computation of transition probabilities for all neighbors at each step introduces substantial computational overhead, while its strict dependence on time-reversible Markov chains excludes advanced non-reversible MCMC methods. To overcome these limitations, instead of direct modification of transition kernel, HDT introduces a history-dependent target distribution $\\\\boldsymbol{\\\\pi}[\\\\mathbf{x}]$ to replace the original target $\\\\boldsymbol{\\\\mu}$ in any graph sampler, where $\\\\mathbf{x}$ represents the empirical measure of past visits. This design preserves lightweight implementation by requiring only local information between the current and proposed states and achieves compatibility with both reversible and non-reversible MCMC samplers, while retaining unbiased samples with target distribution $\\\\boldsymbol{\\\\mu}$ and near-zero variance performance. Extensive experiments in graph sampling demonstrate consistent performance gains, and a memory-efficient Least Recently Used (LRU) cache ensures scalability to large general graphs",
    "checked": true,
    "id": "59bcb50ca3406f2761472b5934c7b8ba1131ff81",
    "semantic_title": "beyond self-repellent kernels: history-driven target towards efficient nonlinear mcmc on general graphs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4AmFA0qNQ2": {
    "title": "Long-Form Speech Generation with Spoken Language Models",
    "volume": "oral",
    "abstract": "We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive **SpeechSSM**, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level. As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: **LibriSpeech-Long**, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/",
    "checked": true,
    "id": "b70f44b066d7adcf89cdd0870a81e3266afdcb6d",
    "semantic_title": "long-form speech generation with spoken language models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=kJQgMGLrow": {
    "title": "A Generalization Theory for Zero-Shot Prediction",
    "volume": "oral",
    "abstract": "A modern paradigm for generalization in machine learning and AI consists of pre-training a task-agnostic foundation model, generally obtained using self-supervised and multimodal contrastive learning. The resulting representations can be used for prediction on a downstream task for which no labeled data is available. We present a theoretical framework to better understand this approach, called zero-shot prediction. We identify the target quantities that zero-shot prediction aims to learn, or learns in passing, and the key conditional independence relationships that enable its generalization ability",
    "checked": true,
    "id": "6a41a354a537d644dbc9d1394b9479265e7e6157",
    "semantic_title": "a generalization theory for zero-shot prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tlniJJFUW2": {
    "title": "Machine Learning meets Algebraic Combinatorics: A Suite of Datasets Capturing Research-level Conjecturing Ability in Pure Mathematics",
    "volume": "oral",
    "abstract": "With recent dramatic increases in AI system capabilities, there has been growing interest in utilizing machine learning for reasoning-heavy, quantitative tasks, particularly mathematics. While there are many resources capturing mathematics at the high-school, undergraduate, and graduate level, there are far fewer resources available that align with the level of difficulty and open endedness encountered by professional mathematicians working on open problems. To address this, we introduce a new collection of datasets, the Algebraic Combinatorics Dataset Repository (ACD Repo), representing either foundational results or open problems in algebraic combinatorics, a subfield of mathematics that studies discrete structures arising from abstract algebra. Further differentiating our dataset collection is the fact that it aims at the conjecturing process. Each dataset includes an open-ended research level question and a large collection of examples (up to 10M in some cases) from which conjectures should be generated. We describe all nine datasets, the different ways machine learning models can be applied to them (e.g., training with narrow models followed by interpretability analysis or program synthesis with LLMs), and discuss some of the challenges involved in designing datasets like these",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2aKHuXdr7Q": {
    "title": "Going Deeper into Locally Differentially Private Graph Neural Networks",
    "volume": "oral",
    "abstract": "Graph Neural Networks (GNNs) have demonstrated superior performance in a variety of graph mining and learning tasks. However, when node representations involve sensitive personal information or variables related to individuals, learning from graph data can raise significant privacy concerns. Although recent studies have explored local differential privacy (LDP) to address these concerns, they often introduce significant distortions to graph data, severely degrading private learning utility (e.g., node classification accuracy). In this paper, we present UPGNET, an LDP-based privacy-preserving graph learning framework that enhances utility while protecting user data privacy. Specifically, we propose a three-stage pipeline that generalizes the LDP protocols for node features, targeting privacy-sensitive scenarios. Our analysis identifies two key factors that affect the utility of privacy-preserving graph learning: *feature dimension* and *neighborhood size*. Based on the above analysis, UPGNET enhances utility by introducing two core layers: High-Order Aggregator (HOA) layer and the Node Feature Regularization (NFR) layer. Extensive experiments on real-world datasets indicate that UPGNET significantly outperforms existing methods in terms of both privacy protection and learning utility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=70voOlSPos": {
    "title": "Polynomial-Delay MAG Listing with Novel Locally Complete Orientation Rules",
    "volume": "oral",
    "abstract": "A maximal ancestral graph (MAG) is widely used to characterize the causal relations among observable variables in the presence of latent variables. However, given observational data, only a partial ancestral graph representing a Markov equivalence class (MEC) of MAGs is identifiable, which generally contains uncertain causal relations. Due to the uncertainties, \\emph{MAG listing}, \\emph{i.e.}, listing all the MAGs in the MEC, is critical for many downstream tasks. In this paper, we present the first \\emph{polynomial-delay} MAG listing method, where delay refers to the time for outputting each MAG, through introducing enumerated structural knowledge in the form of \\emph{singleton background knowledge (BK)}. To incorporate such knowledge, we propose the \\emph{sound} and \\emph{locally complete} orientation rules. By recursively introducing singleton BK and applying the rules, our method can output all and only MAGs in the MEC with polynomial delay. Additionally, while the proposed novel rules enable more efficient MAG listing, for the goal of incorporating general BK, we present two counterexamples to imply that existing rules including ours, are not yet \\emph{complete}, which motivate two more rules. Experimental results validate the efficiency of the proposed MAG listing method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tO7OVZkCo1": {
    "title": "VideoRoPE: What Makes for Good Video Rotary Position Embedding?",
    "volume": "oral",
    "abstract": "While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce VideoRoPE, with a 3D structure designed to preserve spatio-temporal relationships. VideoRoPE features low-frequency temporal allocation to mitigate periodic oscillations, a diagonal layout to maintain spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code and model weights will be publicly released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pwNSUo7yUb": {
    "title": "Inductive Moment Matching",
    "volume": "oral",
    "abstract": "Diffusion models and Flow Matching generate high-quality samples but are slow at inference, and distilling them into few-step models often leads to instability and extensive tuning. To resolve these trade-offs, we propose Moment Matching Self-Distillation (MMSD), a new class of generative models for one- or few-step sampling with a single-stage training procedure. Unlike distillation, MMSD does not require pre-training initialization and optimization of two networks; and unlike Consistency Models, MMSD guarantees distribution-level convergence and remains stable under various hyperparameters and standard model architectures. MMSD surpasses diffusion models on ImageNet-256x256 with 2.13 FID using only 8 inference steps and achieves state-of-the-art 2-step FID of 2.05 on CIFAR-10 for a model trained from scratch",
    "checked": true,
    "id": "b50e850a58b6fc41bbbbf05d199aa43dc581c163",
    "semantic_title": "inductive moment matching",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=mIomqOskaa": {
    "title": "Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning",
    "volume": "oral",
    "abstract": "Effectively scaling up deep reinforcement learning models has proven notoriously difficult due to network pathologies during training, motivating various targeted interventions such as periodic reset and architectural advances such as layer normalization. Instead of pursuing more complex modifications, we show that introducing static network sparsity alone can unlock further scaling potential beyond their dense counterparts with state-of-the-art architectures. This is achieved through simple one-shot random pruning, where a predetermined percentage of network weights are randomly removed once before training. Our analysis reveals that, in contrast to naively scaling up dense DRL networks, such sparse networks achieve both higher parameter efficiency for network expressivity and stronger resistance to optimization challenges like plasticity loss and gradient interference. We further extend our evaluation to visual and streaming RL scenarios, demonstrating the consistent benefits of network sparsity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KwIlvmLDLm": {
    "title": "LoRA-One: One-Step Full Gradient Could Suffice for Fine-Tuning Large Language Models, Provably and Efficiently",
    "volume": "oral",
    "abstract": "This paper explores how theory can guide and enhance practical algorithms, using Low-Rank Adaptation (LoRA) (Hu et al., 2022) in large language models as a case study. We rigorously prove that, under gradient descent, LoRA adapters align with specific singular subspaces of the one-step full fine-tuning gradient. This result suggests that, by properly initializing the adapters using the one-step full gradient, subspace alignment can be achieved immediately—applicable to both linear and nonlinear models. Building on our theory, we propose a theory-driven algorithm, LoRA-One, where the linear convergence (as well as generalization) is built and incorporating preconditioners theoretically helps mitigate the effects of ill-conditioning. Besides, our theory reveals connections between LoRA-One and other gradient-alignment-based methods, helping to clarify misconceptions in the design of such algorithms. LoRA-One achieves significant empirical improvements over LoRA and its variants across benchmarks in natural language understanding, mathematical reasoning, and code generation. Code is available at: https://github.com/YuanheZ/LoRA-One",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZAlII9wL5i": {
    "title": "Equivalence is All: A Unified View for Self-supervised Graph Learning",
    "volume": "oral",
    "abstract": "Node equivalence is common in graphs, such as computing networks, encompassing automorphic equivalence (preserving adjacency under node permutations) and attribute equivalence (nodes with identical attributes). Despite their importance for learning node representations, these equivalences are largely ignored by existing graph models. To bridge this gap, we propose a GrAph self-supervised Learning framework with Equivalence (GALE) and analyze its connections to existing techniques. Specifically, we: 1) unify automorphic and attribute equivalence into a single equivalence class; 2) enforce the equivalence principle to make representations within the same class more similar while separating those across classes; 3) introduce approximate equivalence classes with linear time complexity to address the NP-hardness of exact automorphism detection and handle node-feature variation; 4) analyze existing graph encoders, noting limitations in message passing neural networks and graph transformers regarding equivalence constraints; 5) show that graph contrastive learning are a degenerate form of equivalence constraint; and 6) demonstrate that GALE achieves superior performance over baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VsJ1K2HV3k": {
    "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
    "volume": "oral",
    "abstract": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of language-based LLMs. Unlike their specialist predecessors, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting singular modalities to accommodating a wide array of or even arbitrary modalities. To assess the capabilities of various MLLMs, a diverse array of benchmark test sets has been proposed. This leads to a critical question: *Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI?* We argue that the answer is not as straightforward as it seems. In this project, we introduce an evaluation framework to delineate the capabilities and behaviors of current multimodal generalists. This framework, named **General-Level**, establishes 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI (Artificial General Intelligence). Central to our framework is the use of **Synergy** as the evaluative criterion, categorizing capabilities based on whether MLLMs preserve synergy across comprehension and generation, as well as across multimodal interactions. To evaluate the comprehensive abilities of various generalists, we present a massive multimodal benchmark, **General-Bench**, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project Page: https://generalist.top/, Leaderboard: https://generalist.top/leaderboard/, Benchmark: https://huggingface.co/General-Level/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LO7ciRpjI5": {
    "title": "Sundial: A Family of Highly Capable Time Series Foundation Models",
    "volume": "oral",
    "abstract": "We introduce Sundial, a family of native, flexible, and scalable time series foundation models. To predict the next-patch's distribution, we propose a TimeFlow Loss based on flow-matching, which facilitates native pre-training of Transformers on continuous-valued time series without discrete tokenization. Conditioned on arbitrary-length time series, our models are pre-trained without specifying any prior distribution and can generate multiple probable predictions, achieving more flexibility in representation learning than using parametric densities. Towards time series foundation models, we leverage minimal but crucial adaptations of Transformers and curate TimeBench with one trillion time points, comprising mostly real-world datasets and synthetic data. By mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial models on TimeBench, which achieve unprecedented model capacity and generalization performance. In addition to excellent scalability, Sundial achieves state-of-the-art results on both point and probabilistic forecasting benchmarks with a just-in-time inference speed, i.e., making zero-shot predictions within a few milliseconds. We believe that Sundial's pioneering generative forecasting capability can improve model reliability in real-world decision-making. Code is available at: https://github.com/thuml/Sundial",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qMt4KikFJg": {
    "title": "Rényi Neural Processes",
    "volume": "oral",
    "abstract": "Neural Processes (NPs) are deep probabilistic models that represent stochastic processes by conditioning their prior distributions on a set of context points. Despite their advantages in uncertainty estimation for complex distributions, NPs enforce parameterization coupling between the conditional prior model and the posterior model. We show that this coupling amounts to prior misspecification and revisit the NP objective to address this issue. More specifically, we propose Rényi Neural Processes (RNP), a method that replaces the standard KL divergence with the Rényi divergence, dampening the effects of the misspecified prior during posterior updates. We validate our approach across multiple benchmarks including regression and image inpainting tasks, and show significant performance improvements of RNPs in real-world problems. Our extensive experiments show consistently better log-likelihoods over state-of-the-art NP models",
    "checked": true,
    "id": "a39b8981708c11c4bee239f24207b527dcd01008",
    "semantic_title": "rényi neural processes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uitj69FqD5": {
    "title": "Model Immunization from a Condition Number Perspective",
    "volume": "oral",
    "abstract": "Model immunization aims to pre-train models that are difficult to fine-tune on harmful tasks while retaining their utility on other non-harmful tasks. Though prior work has shown empirical evidence for immunizing text-to-image models, the key understanding of when immunization is possible and a precise definition of an immunized model remain unclear. In this work, we propose a framework, based on the condition number of a Hessian matrix, to analyze model immunization for linear models. Building on this framework, we design an algorithm with regularization terms to control the resulting condition numbers after pre-training. Empirical results on linear models and non-linear deep-nets demonstrate the effectiveness of the proposed algorithm on model immunization. The code is available at https://github.com/amberyzheng/model-immunization-cond-num",
    "checked": true,
    "id": "7922f7245a5f1f6b298bd1471fa6c28846d21027",
    "semantic_title": "model immunization from a condition number perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I1OHPb4zWo": {
    "title": "Flowing Datasets with Wasserstein over Wasserstein Gradient Flows",
    "volume": "oral",
    "abstract": "Many applications in machine learning involve data represented as probability distributions. The emergence of such data requires radically novel techniques to design tractable gradient flows on probability distributions over this type of (infinite-dimensional) objects. For instance, being able to flow labeled datasets is a core task for applications ranging from domain adaptation to transfer learning or dataset distillation. In this setting, we propose to represent each class by the associated conditional distribution of features, and to model the dataset as a mixture distribution supported on these classes (which are themselves probability distributions), meaning that labeled datasets can be seen as probability distributions over probability distributions. We endow this space with a metric structure from optimal transport, namely the Wasserstein over Wasserstein (WoW) distance, derive a differential structure on this space, and define WoW gradient flows. The latter enables to design dynamics over this space that decrease a given objective functional. We apply our framework to transfer learning and dataset distillation tasks, leveraging our gradient flow construction as well as novel tractable functionals that take the form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels between probability distributions",
    "checked": true,
    "id": "c117793cf0532873856ca84863a68f058f86a771",
    "semantic_title": "flowing datasets with wasserstein over wasserstein gradient flows",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X9vBykZVYg": {
    "title": "Retrieval-Augmented Perception: High-resolution Image Perception Meets Visual RAG",
    "volume": "oral",
    "abstract": "High-resolution (HR) image perception remains a key challenge in multimodal large language models (MLLMs). To drive progress beyond the limits of heuristic methods, this paper advances HR perception capabilities of MLLMs by harnessing cutting-edge long-context techniques such as retrieval-augmented generation (RAG). Towards this end, this paper presents the first study exploring the use of RAG to address HR perception challenges. Specifically, we propose Retrieval-Augmented Perception (RAP), a training-free framework that retrieves and fuses relevant image crops while preserving spatial context using the proposed Spatial-Awareness Layout. To accommodate different tasks, the proposed Retrieved-Exploration Search (RE-Search) dynamically selects the optimal number of crops based on model confidence and retrieval scores. Experimental results on HR benchmarks demonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving a 43\\% improvement on $V^*$ Bench and 19\\% on HR-Bench. Code is available at https://github.com/DreamMr/RAP",
    "checked": true,
    "id": "766fceda6aae59d0200c0fe75aeb6887aa71efa8",
    "semantic_title": "retrieval-augmented perception: high-resolution image perception meets visual rag",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=M6L7Eaw9BW": {
    "title": "Navigating Semantic Drift in Task-Agnostic Class-Incremental Learning",
    "volume": "oral",
    "abstract": "Class-incremental learning (CIL) seeks to enable a model to sequentially learn new classes while retaining knowledge of previously learned ones. Balancing flexibility and stability remains a significant challenge, particularly when the task ID is unknown. To address this, our study reveals that the gap in feature distribution between novel and existing tasks is primarily driven by differences in mean and covariance moments. Building on this insight, we propose a novel semantic drift calibration method that incorporates mean shift compensation and covariance calibration. Specifically, we calculate each class's mean by averaging its sample embeddings and estimate task shifts using weighted embedding changes based on their proximity to the previous mean, effectively capturing mean shifts for all learned classes with each new task. We also apply Mahalanobis distance constraint for covariance calibration, aligning class-specific embedding covariances between old and current networks to mitigate the covariance shift. Additionally, we integrate a feature-level self-distillation approach to enhance generalization. Comprehensive experiments on commonly used datasets demonstrate the effectiveness of our approach. The source code is available at https://github.com/fwu11/MACIL.git",
    "checked": true,
    "id": "6d8fe97ee5b64f7a1dcee2d665fb30b7a260436b",
    "semantic_title": "navigating semantic drift in task-agnostic class-incremental learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=v26vwjxOEz": {
    "title": "Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark",
    "volume": "oral",
    "abstract": "The ability to organically reason over and with both text and images is a pillar of human intelligence, yet the ability of Multimodal Large Language Models (MLLMs) to perform such multimodal reasoning remains under-explored. Existing benchmarks often emphasize text-dominant reasoning or rely on shallow visual cues, failing to adequately assess integrated visual and textual reasoning. We introduce EMMA (Enhanced MultiModal reAsoning), a benchmark targeting organic multimodal reasoning across mathematics, physics, chemistry, and coding. EMMA tasks demand advanced cross-modal reasoning that cannot be addressed by reasoning independently in each modality, offering an enhanced test suite for MLLMs' reasoning capabilities. Our evaluation of state-of-the-art MLLMs on EMMA reveals significant limitations in handling complex multimodal and multi-step reasoning tasks, even with advanced techniques like Chain-of-Thought prompting and test-time compute scaling underperforming. These findings underscore the need for improved multimodal architectures and training paradigms to close the gap between human and model reasoning in multimodality",
    "checked": true,
    "id": "23c3bf6a61ee9a1f03fa0f5e16ab286f7f040487",
    "semantic_title": "can mllms reason in multimodality? emma: an enhanced multimodal reasoning benchmark",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=ybno0ZP44z": {
    "title": "Improved Regret Analysis in Gaussian Process Bandits: Optimality for Noiseless Reward, RKHS norm, and Non-Stationary Variance",
    "volume": "oral",
    "abstract": "We study the Gaussian process (GP) bandit problem, whose goal is to minimize regret under an unknown reward function lying in some reproducing kernel Hilbert space (RKHS). The maximum posterior variance analysis is vital in analyzing near-optimal GP bandit algorithms such as maximum variance reduction (MVR) and phased elimination (PE). Therefore, we first show the new upper bound of the maximum posterior variance, which improves the dependence of the noise variance parameters of the GP. By leveraging this result, we refine the MVR and PE to obtain (i) a nearly optimal regret upper bound in the noiseless setting and (ii) regret upper bounds that are optimal with respect to the RKHS norm of the reward function. Furthermore, as another application of our proposed bound, we analyze the GP bandit under the time-varying noise variance setting, which is the kernelized extension of the linear bandit with heteroscedastic noise. For this problem, we show that MVR and PE-based algorithms achieve noise variance-dependent regret upper bounds, which matches our regret lower bound",
    "checked": true,
    "id": "649107c240f469c47986888b549090ea39170faa",
    "semantic_title": "improved regret analysis in gaussian process bandits: optimality for noiseless reward, rkhs norm, and non-stationary variance",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=DmH4HHVb3y": {
    "title": "CollabLLM: From Passive Responders to Active Collaborators",
    "volume": "oral",
    "abstract": "Large Language Models are typically trained with next-turn rewards, limiting their ability to optimize for long-term interaction. As a result, they often respond passively to ambiguous or open-ended user requests, failing to help users reach their ultimate intents and leading to inefficient conversations. To address these limitations, we introduce CollabLLM, a novel and general training framework that enhances multiturn human-LLM collaboration. Its key innovation is a collaborative simulation that estimates the long-term contribution of responses using Multiturn-aware Rewards. By reinforcement fine-tuning these rewards, CollabLLM goes beyond responding to user requests, and actively uncovers user intent and offers insightful suggestions—a key step towards more human-centered AI. We also devise a multiturn interaction benchmark with three challenging tasks such as document creation. CollabLLM significantly outperforms our baselines with averages of 18.5% higher task performance and 46.3% improved interactivity by LLM judges. Finally, we conduct a large user study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and reduces user spent time by 10.4%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V0w8Kj3K6L": {
    "title": "Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings",
    "volume": "oral",
    "abstract": "Deploying machine learning models in safety-critical domains poses a key challenge: ensuring reliable model performance on downstream user data without access to ground truth labels for direct validation. We propose the _suitability filter_, a novel framework designed to detect performance deterioration by utilizing _suitability signals_—model output features that are sensitive to covariate shifts and indicative of potential prediction errors. The suitability filter evaluates whether classifier accuracy on unlabeled user data shows significant degradation compared to the accuracy measured on the labeled test dataset. Specifically, it ensures that this degradation does not exceed a pre-specified margin, which represents the maximum acceptable drop in accuracy. To achieve reliable performance evaluation, we aggregate suitability signals for both test and user data and compare these empirical distributions using statistical hypothesis testing, thus providing insights into decision uncertainty. Our modular method adapts to various models and domains. Empirical evaluations across different classification tasks demonstrate that the suitability filter reliably detects performance deviations due to covariate shift. This enables proactive mitigation of potential failures in high-stakes applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6CwO5nVvku": {
    "title": "Partition First, Embed Later: Laplacian-Based Feature Partitioning for Refined Embedding and Visualization of High-Dimensional Data",
    "volume": "oral",
    "abstract": "Embedding and visualization techniques are essential for analyzing high-dimensional data, but they often struggle with complex data governed by multiple latent variables, potentially distorting key structural characteristics. This paper considers scenarios where the observed features can be partitioned into mutually exclusive subsets, each capturing a different smooth substructure. In such cases, visualizing the data based on each feature partition can better characterize the underlying processes and structures in the data, leading to improved interpretability. To partition the features, we propose solving an optimization problem that promotes graph Laplacian-based smoothness in each partition, thereby prioritizing partitions with simpler geometric structures. Our approach generalizes traditional embedding and visualization techniques, allowing them to learn multiple embeddings simultaneously. We establish that if several independent or partially dependent manifolds are embedded in distinct feature subsets in high-dimensional space, then our framework can reliably identify the correct subsets with theoretical guarantees. Finally, we demonstrate the effectiveness of our approach in extracting multiple low-dimensional structures and partially independent processes from both simulated and real data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QvqnPVGWAN": {
    "title": "Blink of an eye: a simple theory for feature localization in generative models",
    "volume": "oral",
    "abstract": "Large language models can exhibit unexpected behavior in the blink of an eye. In a recent computer use demo, a language model switched from coding to Googling pictures of Yellowstone, and these sudden shifts in behavior have also been observed in reasoning patterns and jailbreaks. This phenomenon is not unique to autoregressive models: in diffusion models, key features of the final output are decided in narrow ``critical windows'' of the generation process. In this work we develop a simple, unifying theory to explain this phenomenon. Using the formalism of stochastic localization for generative models, we show that it emerges generically as the generation process localizes to a sub-population of the distribution it models. While critical windows have been studied at length in diffusion models, existing theory heavily relies on strong distributional assumptions and the particulars of Gaussian diffusion. In contrast to existing work our theory (1) applies to autoregressive and diffusion models; (2) makes very few distributional assumptions; (3) quantitatively improves previous bounds even when specialized to diffusions; and (4) requires basic mathematical tools. Finally, we validate our predictions empirically for LLMs and find that critical windows often coincide with failures in problem solving for various math and reasoning benchmarks",
    "checked": true,
    "id": "1819508abae222d6a0ee516bdc1258ea47f084bc",
    "semantic_title": "blink of an eye: a simple theory for feature localization in generative models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kEn7Wt6Yj2": {
    "title": "On Differential Privacy for Adaptively Solving Search Problems via Sketching",
    "volume": "oral",
    "abstract": "Recently differential privacy has been used for a number of streaming, data structure, and dynamic graph problems as a means of hiding the internal randomness of the data structure, so that multiple possibly adaptive queries can be made without sacrificing the correctness of the responses. Although these works use differential privacy to show that for some problems it is possible to tolerate $T$ queries using $\\widetilde{O}(\\sqrt{T})$ copies of a data structure, such results only apply to {\\it numerical estimation problems}, and only return the {\\it cost} of an optimization problem rather than the solution itself. In this paper we investigate the use of differential privacy for adaptive queries to {\\it search} problems, which are significantly more challenging since the responses to queries can reveal much more about the internal randomness than a single numerical query. We focus on two classical search problems: nearest neighbor queries and regression with arbitrary turnstile updates. We identify key parameters to these problems, such as the number of $c$-approximate near neighbors and the matrix condition number, and use different differential privacy techniques to design algorithms returning the solution point or solution vector with memory and time depending on these parameters. We give algorithms for each of these problems that achieve similar tradeoffs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zf9zwCRKyP": {
    "title": "Exploring and Mitigating Adversarial Manipulation of Voting-Based Leaderboards",
    "volume": "oral",
    "abstract": "It is now common to evaluate Large Language Models (LLMs) by having humans manually vote to evaluate model outputs, in contrast to typical benchmarks that evaluate knowledge or skill at some particular task. Chatbot Arena, the most popular benchmark of this type, ranks models by asking users to select the better response between two randomly selected models (without revealing which model was responsible for the generations). These platforms are widely trusted as a fair and accurate measure of LLM capabilities. In this paper, we show that if bot protection and other defenses are not implemented, these voting-based benchmarks are potentially vulnerable to adversarial manipulation. Specifically, we show that an attacker can alter the leaderboard (to promote their favorite model or demote competitors) at the cost of roughly a thousand votes (verified in a simulated, offline version of Chatbot Arena). Our attack consists of two steps: first, we show how an attacker can determine which model was used to generate a given reply with more than $95\\%$ accuracy; and then, the attacker can use this information to consistently vote for (or against) a target model. Working with the Chatbot Arena developers, we identify, propose, and implement mitigations to improve the robustness of Chatbot Arena against adversarial manipulation, which, based on our analysis, substantially increases the cost of such attacks. Some of these defenses were present before our collaboration, such as bot protection with Cloudflare, malicious user detection, and rate limiting. Others, including reCAPTCHA and login are being integrated to strengthen the security in Chatbot Arena",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4tFSKOY2mT": {
    "title": "What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities",
    "volume": "oral",
    "abstract": "As multimodal large language models (MLLMs) advance, MLLM-based virtual agents have demonstrated remarkable performance. However, existing benchmarks face significant limitations, including uncontrollable task complexity, extensive manual annotation, and a lack of multidimensional evaluation. In response to these challenges, we introduce OmniBench, a self-generating, graph-based benchmark with an automated pipeline for synthesizing tasks of controllable complexity through subtask composition. To evaluate the diverse capabilities of virtual agents on the graph, we further present OmniEval, a multidimensional evaluation framework that includes subtask-level evaluation, graph-based metrics, and comprehensive tests across 10 capabilities. Our synthesized dataset contains 36k graph-structured tasks across 20 scenarios, achieving a 91% human acceptance rate. Training on our graph-structured data shows that it improves generalization across environments. We conduct multidimensional evaluations for virtual agents, revealing their performance across various capabilities and paving the way for future advancements. Our project is available at https://omni-bench.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LwQGRGJTHw": {
    "title": "Fundamental Bias in Inverting Random Sampling Matrices with Application to Sub-sampled Newton",
    "volume": "oral",
    "abstract": "A substantial body of work in machine learning (ML) and randomized numerical linear algebra (RandNLA) has exploited various sorts of random sketching methodologies, including random sampling and random projection, with much of the analysis using Johnson--Lindenstrauss and subspace embedding techniques. Recent studies have identified the issue of *inversion bias* -- the phenomenon that inverses of random sketches are *not* unbiased, despite the unbiasedness of the sketches themselves. This bias presents challenges for the use of random sketches in various ML pipelines, such as fast stochastic optimization, scalable statistical estimators, and distributed optimization. In the context of random projection, the inversion bias can be easily corrected for dense Gaussian projections (which are, however, too expensive for many applications). Recent work has shown how the inversion bias can be corrected for sparse sub-gaussian projections. In this paper, we show how the inversion bias can be corrected for random sampling methods, both uniform and non-uniform leverage-based, as well as for structured random projections, including those based on the Hadamard transform. Using these results, we establish problem-independent local convergence rates for sub-sampled Newton methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2uheUFcFsM": {
    "title": "Normalizing Flows are Capable Generative Models",
    "volume": "oral",
    "abstract": "Normalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density estimation and generative modeling tasks, but have received relatively little attention in recent years. In this work, we demonstrate that NFs are more powerful than previously believed. We present TarFlow: a simple and scalable architecture that enables highly performant NF models. TarFlow can be thought of as a Transformer-based variant of Masked Autoregressive Flows (MAFs): it consists of a stack of autoregressive Transformer blocks on image patches, alternating the autoregression direction between layers. TarFlow is straightforward to train end-to-end, and capable of directly modeling and generating pixels. We also propose three key techniques to improve sample quality: Gaussian noise augmentation during training, a post training denoising procedure, and an effective guidance method for both class-conditional and unconditional settings. Putting these together, TarFlow sets new state-of-the-art results on likelihood estimation for images, beating the previous best methods by a large margin, and generates samples with quality and diversity comparable to diffusion models, for the first time with a stand-alone NF model. We make our code available at https://github.com/apple/ml-tarflow",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=imcyVlzpXh": {
    "title": "Multi-agent Architecture Search via Agentic Supernet",
    "volume": "oral",
    "abstract": "Large Language Model (LLM)-empowered multi-agent systems extend the cognitive boundaries of individual agents through disciplined collaboration and interaction, while constructing these systems often requires labor-intensive manual designs. Despite the availability of methods to automate the design of agentic workflows, they typically seek to identify a static, complex, one-size-fits-all system, which, however, fails to dynamically allocate inference resources based on the difficulty and domain of each query. To address this challenge, we shift away from the pursuit of a monolithic agentic system, instead optimizing the \\textbf{agentic supernet}, a probabilistic and continuous distribution of agentic architectures. We introduce \\textbf{MaAS}, an automated framework that samples query-dependent agentic systems from the supernet, delivering high-quality solutions and tailored resource allocation (\\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation across six benchmarks demonstrates that MaAS \\textbf{(I)} requires only $6\\\\sim45\\\\%$ of the inference costs of existing handcrafted or automated multi-agent systems, \\textbf{(II)} surpasses them by $0.54\\\\%\\sim11.82\\\\%$, and \\textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone transferability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yMJcHWcb2Z": {
    "title": "VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models",
    "volume": "oral",
    "abstract": "Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence. To address this, we introduce **VideoJAM**, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn *a joint appearance-motion representation*. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce **Inner-Guidance**, a mechanism that steers the generation toward coherent motion by leveraging the model's own evolving motion prediction as a dynamic guidance signal. Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model. VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations. These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=reuShgiHdg": {
    "title": "ReferSplat: Referring Segmentation in 3D Gaussian Splatting",
    "volume": "oral",
    "abstract": "We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes. This task requires the model to identify newly described objects that may be occluded or not directly visible in a novel view, posing a significant challenge for 3D multi-modal understanding. Developing this capability is crucial for advancing embodied AI. To support research in this area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that 3D multi-modal understanding and spatial relationship modeling are key challenges for R3DGS. To address these challenges, we propose ReferSplat, a framework that explicitly models 3D Gaussian points with natural language expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art performance on both the newly proposed R3DGS task and 3D open-vocabulary segmentation benchmarks. Dataset and code are available at https://github.com/heshuting555/ReferSplat",
    "checked": false,
    "id": "c3a7c759bf8a8feebe82f80542301a1a7a8ff33c",
    "semantic_title": "dr. splat: directly referring 3d gaussian splatting via direct language embedding registration",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=aHzPGyUhZa": {
    "title": "STAIR: Improving Safety Alignment with Introspective Reasoning",
    "volume": "oral",
    "abstract": "Ensuring the safety and harmlessness of Large Language Models (LLMs) has become equally critical as their performance in applications. However, existing safety alignment methods typically suffer from safety-performance trade-offs and susceptibility to jailbreak attacks, primarily due to their reliance on direct refusals for malicious queries. In this paper, we propose **STAIR**, a novel framework that integrates **S**afe**T**y **A**lignment with **I**trospective **R**easoning. We enable LLMs to identify safety risks through step-by-step analysis by self-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR first equips the model with a structured reasoning capability and then advances safety alignment via iterative preference optimization on step-level reasoning data generated using our newly proposed Safety-Informed Monte Carlo Tree Search (SI-MCTS). Specifically, we design a theoretically grounded reward for outcome evaluation to seek balance between helpfulness and safety. We further train a process reward model on this data to guide test-time searches for improved responses. Extensive experiments show that STAIR effectively mitigates harmful outputs while better preserving helpfulness, compared to instinctive alignment strategies. With test-time scaling, STAIR achieves a safety performance comparable to Claude-3.5 against popular jailbreak attacks. We have open-sourced our code, datasets and models at https://github.com/thu-ml/STAIR",
    "checked": true,
    "id": "3ac976418e99aa5e558dedcb0ec25d6eb6c35750",
    "semantic_title": "stair: improving safety alignment with introspective reasoning",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=x4qvBVuzzu": {
    "title": "From Weight-Based to State-Based Fine-Tuning: Further Memory Reduction on LoRA with Parallel Control",
    "volume": "oral",
    "abstract": "The LoRA method has achieved notable success in reducing GPU memory usage by applying low-rank updates to weight matrices. Yet, one simple question remains: can we push this reduction even further? Furthermore, is it possible to achieve this while improving performance and reducing computation time? Answering these questions requires moving beyond the conventional weight-centric approach. In this paper, we present a state-based fine-tuning framework that shifts the focus from weight adaptation to optimizing forward states, with LoRA acting as a special example. Specifically, state-based tuning introduces parameterized perturbations to the states within the computational graph, allowing us to control states across an entire residual block. A key advantage of this approach is the potential to avoid storing large intermediate states in models like transformers. Empirical results across multiple architectures—including ViT, RoBERTa, LLaMA2-7B, and LLaMA3-8B—show that our method further reduces memory consumption and computation time while simultaneously improving performance. Moreover, as a result of memory reduction, we explore the feasibility to train 7B/8B models on consumer-level GPUs like Nvidia 3090, without model quantization. The code is available at an anonymous GitHub repository",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wUEp13rqXP": {
    "title": "Mixture of Lookup Experts",
    "volume": "oral",
    "abstract": "Mixture-of-Experts (MoE) activates only a subset of experts during inference, allowing the model to maintain low inference FLOPs and latency even as the parameter count scales up. However, since MoE dynamically selects the experts, all the experts need to be loaded into VRAM. Their large parameter size still limits deployment, and offloading, which load experts into VRAM only when needed, significantly increase inference latency. To address this, we propose Mixture of Lookup Experts (MoLE), a new MoE architecture that is efficient in both communication and VRAM usage. In MoLE, the experts are Feed-Forward Networks (FFNs) during training, taking the output of the embedding layer as input. Before inference, these experts can be re-parameterized as lookup tables (LUTs) that retrieves expert outputs based on input ids, and offloaded to storage devices. Therefore, we do not need to perform expert computations during inference. Instead, we directly retrieve the expert's computation results based on input ids and load them into VRAM, and thus the resulting communication overhead is negligible. Experiments show that, with the same FLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models and significantly faster than MoE with experts offloading, while maintaining performance on par with MoE. Code: https://github.com/JieShibo/MoLE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fCPB0qRJT2": {
    "title": "AutoGFM: Automated Graph Foundation Model with Adaptive Architecture Customization",
    "volume": "oral",
    "abstract": "Graph foundation models (GFMs) aim to share graph knowledge across diverse domains and tasks to boost graph machine learning. However, existing GFMs rely on hand-designed and fixed graph neural network (GNN) architectures, failing to utilize optimal architectures *w.r.t.* specific domains and tasks, inevitably leading to suboptimal performance in diverse graph domains and tasks. In this paper, we explore graph neural architecture search (GNAS) for GFMs for the first time, which suffers from the problem of *architecture inconsistency*, i.e., the optimal architectures for different tasks and domains vary. We tackle this problem by discovering an invariant graph-architecture relationship across domains and tasks, which imposes three challenges: i) how to capture invariant and variant patterns; ii) how to customize architectures to adapt to diverse domains and tasks; iii) how to mitigate the data domination phenomenon during the architecture search process. To address these challenges, we propose **Auto**mated **G**raph **F**oundation **M**odel with Adaptive Architecture Customization (**AutoGFM**), providing a theoretical analysis to demonstrate the limitations of existing GNAS. Specifically, we first propose a disentangled contrastive graph encoder to learn invariant and variant patterns. Then, we design an invariant-guided architecture customization strategy to customize architectures for data from diverse domains and tasks. Finally, we propose a curriculum architecture customization mechanism to mitigate the phenomenon of particular data dominating the search process. Extensive experiments demonstrate that **AutoGFM** outperforms baselines, achieving state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l19DmXbwPK": {
    "title": "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data",
    "volume": "oral",
    "abstract": "Process Reward Models (PRMs) have proven effective at enhancing mathematical reasoning for Large Language Models (LLMs) by leveraging increased inference-time computation. However, they are predominantly trained on mathematical data and their generalizability to non-mathematical domains has not been rigorously studied. In response, this work first shows that current PRMs have poor performance in other domains. To address this limitation, we introduce ***VersaPRM***, a multi-domain PRM trained on synthetic reasoning data generated using our novel data generation and annotation method. VersaPRM achieves consistent performance gains across diverse domains. For instance, in the MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a 7.9% performance gain over the majority voting baseline–surpassing Qwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by open-sourcing all data, code and models for VersaPRM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mEV0nvHcK3": {
    "title": "Towards Practical Defect-Focused Automated Code Review",
    "volume": "spotlight",
    "abstract": "The complexity of code reviews has driven efforts to automate review comments, but prior approaches oversimplify this task by treating it as snippet-level code-to-text generation and relying on text similarity metrics like BLEU for evaluation. These methods overlook repository context, real-world merge request evaluation, and defect detection, limiting their practicality. To address these issues, we explore the full automation pipeline within the online recommendation service of a company with nearly 400 million daily active users, analyzing industry-grade C++ codebases comprising hundreds of thousands of lines of code. We identify four key challenges: 1) capturing relevant context, 2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and 4) integrating human workflows. To tackle these, we propose 1) code slicing algorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a filtering mechanism for FAR reduction, and 4) a novel prompt design for better human interaction. Our approach, validated on real-world merge requests from historical fault reports, achieves a 2× improvement over standard LLMs and a 10× gain over previous baselines. While the presented results focus on C++, the underlying framework design leverages language-agnostic principles (e.g., AST-based analysis), suggesting potential for broader applicability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=il3KRr4H9u": {
    "title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
    "volume": "spotlight",
    "abstract": "Automatic program generation has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make code edits, and solve algorithmic coding tasks. However, to achieve full automation, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI o1, achieves a mere 62% on code correctness; (ii) on average, we could successfully execute security exploits on around half of the correct programs generated by each LLM; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps towards autonomous and secure software development with LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5QAKPBVdFH": {
    "title": "Hide & Seek: Transformer Symmetries Obscure Sharpness & Riemannian Geometry Finds It",
    "volume": "spotlight",
    "abstract": "The concept of sharpness has been successfully applied to traditional architectures like MLPs and CNNs to predict their generalization. For transformers, however, recent work reported weak correlation between flatness and generalization. We argue that existing sharpness measures fail for transformers, because they have much richer symmetries in their attention mechanism that induce directions in parameter space along which the network or its loss remain identical. We posit that sharpness must account fully for these symmetries, and thus we redefine it on a quotient manifold that results from quotienting out the transformer symmetries, thereby removing their ambiguities. Leveraging tools from Riemannian geometry, we propose a fully general notion of sharpness, in terms of a geodesic ball on the symmetry-corrected quotient manifold. In practice, we need to resort to approximating the geodesics. Doing so up to first order yields existing adaptive sharpness measures, and we demonstrate that including higher-order terms is crucial to recover correlation with generalization. We present results on diagonal networks with synthetic data, and show that our geodesic sharpness reveals strong correlation for real-world transformers on both text and image classification tasks",
    "checked": true,
    "id": "ad4d18424014c533452ae97affa2f18b73a82cc1",
    "semantic_title": "hide & seek: transformer symmetries obscure sharpness & riemannian geometry finds it",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WxY61MmHYo": {
    "title": "Scaling Laws for Task-Optimized Models of the Primate Visual Ventral Stream",
    "volume": "spotlight",
    "abstract": "When trained on large-scale object classification datasets, certain artificial neural network models begin to approximate core object recognition behaviors and neural response patterns in the primate brain. While recent machine learning advances suggest that scaling compute, model size, and dataset size improves task performance, the impact of scaling on brain alignment remains unclear. In this study, we explore scaling laws for modeling the primate visual ventral stream by systematically evaluating over 600 models trained under controlled conditions on benchmarks spanning V1, V2, V4, IT and behavior. We find that while behavioral alignment continues to scale with larger models, neural alignment saturates. This observation remains true across model architectures and training datasets, even though models with stronger inductive biases and datasets with higher-quality images are more compute-efficient. Increased scaling is especially beneficial for higher-level visual areas, where small models trained on few samples exhibit only poor alignment. Our results suggest that while scaling current architectures and datasets might suffice for alignment with human core object recognition behavior, it will not yield improved models of the brain's visual ventral stream, highlighting the need for novel strategies in building brain models",
    "checked": true,
    "id": "11250019691c94c9883777e0df035675c2b73de7",
    "semantic_title": "scaling laws for task-optimized models of the primate visual ventral stream",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=EW2JR5aVLm": {
    "title": "Understanding and Mitigating Memorization in Generative Models via Sharpness of Probability Landscapes",
    "volume": "spotlight",
    "abstract": "In this paper, we introduce a geometric framework to analyze memorization in diffusion models through the sharpness of the log probability density. We mathematically justify a previously proposed score-difference-based memorization metric by demonstrating its effectiveness in quantifying sharpness. Additionally, we propose a novel memorization metric that captures sharpness at the initial stage of image generation in latent diffusion models, offering early insights into potential memorization. Leveraging this metric, we develop a mitigation strategy that optimizes the initial noise of the generation process using a sharpness-aware regularization term",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DUGFTH9W8B": {
    "title": "Monte-Carlo Tree Search with Uncertainty Propagation via Optimal Transport",
    "volume": "spotlight",
    "abstract": "This paper introduces a novel backup strategy for Monte-Carlo Tree Search (MCTS) tailored for highly stochastic and partially observable Markov decision processes. We adopt a probabilistic approach, modeling both value and action-value nodes as Gaussian distributions, to introduce a novel backup operator that computes value nodes as the Wasserstein barycenter of their action-value children nodes; thus, propagating the uncertainty of the estimate across the tree to the root node. We study our novel backup operator when using a novel combination of $L^1$-Wasserstein barycenter with $\\alpha$-divergence, by drawing a crucial connection to the generalized mean backup operator. We complement our probabilistic backup operator with two sampling strategies, based on optimistic selection and Thompson sampling, obtaining our Wasserstein MCTS algorithm. We provide theoretical guarantees of asymptotic convergence of $\\mathcal{O}(n^{-1/2})$, with $n$ as the number of visited trajectories, to the optimal policy and an empirical evaluation on several stochastic and partially observable environments, where our approach outperforms well-known related baselines",
    "checked": true,
    "id": "76a3528744334dfd3d91d5dfd398cb83367d3ed7",
    "semantic_title": "monte-carlo tree search with uncertainty propagation via optimal transport",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=qOgKMqv9T7": {
    "title": "TIMING: Temporality-Aware Integrated Gradients for Time Series Explanation",
    "volume": "spotlight",
    "abstract": "Recent explainable artificial intelligence (XAI) methods for time series primarily estimate point-wise attribution magnitudes, while overlooking the directional impact on predictions, leading to suboptimal identification of significant points. Our analysis shows that conventional Integrated Gradients (IG) effectively capture critical points with both positive and negative impacts on predictions. However, current evaluation metrics fail to assess this capability, as they inadvertently cancel out opposing feature contributions. To address this limitation, we propose novel evaluation metrics—Cumulative Prediction Difference (CPD) and Cumulative Prediction Preservation (CPP)—to systematically assess whether attribution methods accurately identify significant positive and negative points in time series XAI. Under these metrics, conventional IG outperforms recent counterparts. However, directly applying IG to time series data may lead to suboptimal outcomes, as generated paths ignore temporal relationships and introduce out-of-distribution samples. To overcome these challenges, we introduce TIMING, which enhances IG by incorporating temporal awareness while maintaining its theoretical properties. Extensive experiments on synthetic and real-world time series benchmarks demonstrate that TIMING outperforms existing time series XAI baselines. Our code is available at https://github.com/drumpt/TIMING",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aJeLhLcsh0": {
    "title": "Multi-Turn Code Generation Through Single-Step Rewards",
    "volume": "spotlight",
    "abstract": "We address the problem of code generation from multi-turn execution feedback. Existing methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards. We propose a simple yet scalable approach, $\\mu$CODE, that solves multi-turn code generation using only single-step rewards. Our key insight is that code generation is a one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in a single turn. $\\mu$CODE iteratively trains both a generator to provide code solutions conditioned on multi-turn execution feedback and a verifier to score the newly generated code. Experimental evaluations show that our approach achieves significant improvements over state-of-the-art baselines. We provide analysis of the design choices of the reward models and policy, and show the efficacy of $\\mu$CODE at utilizing the execution feedback",
    "checked": true,
    "id": "704a9df587cce23023ffc99af99eb06fb0482333",
    "semantic_title": "multi-turn code generation through single-step rewards",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=4yHWV3B6g4": {
    "title": "Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging Pretrained 2D Foundation Models",
    "volume": "spotlight",
    "abstract": "Current challenges in developing foundational models for volumetric imaging data, such as magnetic resonance imaging (MRI), stem from the computational complexity of state-of-the-art architectures in high dimensions and curating sufficiently large datasets of volumes. To address these challenges, we introduce Raptor (Random Planar Tensor Reduction), a train-free method for generating semantically rich embeddings for volumetric data. Raptor leverages a frozen 2D foundation model, pretrained on natural images, to extract visual tokens from individual cross-sections of medical volumes. These tokens are then spatially compressed using random projections, significantly reducing computational complexity while retaining rich semantic information. Extensive experiments on 10 diverse medical volume tasks verify the superior performance of Raptor over state-of-the-art methods, including those pretrained exclusively on medical volumes (+3% SuPreM, +6% MISFM, +10% Merlin, +13% VoCo, and +14% SLIViT), while entirely bypassing the need for costly training. Our results highlight Raptor's effectiveness and versatility as a foundation for advancing deep learning-based methods for medical volumes (code: github.com/sriramlab/raptor)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W2Fe1hT7Ks": {
    "title": "The Role of Randomness in Stability",
    "volume": "spotlight",
    "abstract": "Stability is a central property in learning and statistics promising the output of an algorithm $\\mathcal{A}$ does not change substantially when applied to similar datasets $S$ and $S'$. It is an elementary fact that any sufficiently stable algorithm (e.g.\\ one returning the same result with high probability, satisfying privacy guarantees, etc.) must be randomized. This raises a natural question: can we quantify \\textit{how much} randomness is needed for algorithmic stability? We study the randomness complexity of two influential notions of stability in learning: \\textit{replicability} (which promises $\\mathcal{A}$ usually outputs the same result when run over samples from the same distribution), and \\textit{differential privacy} (which promises the output distribution of $\\mathcal{A}$ remains similar under neighboring datasets). In particular, building on the ideas of (Dixon, Pavan, Vander Woude, and Vinodchandran ICML 2024) and (Cannone, Su, and Vadhan ITCS 2024), we prove a \"weak-to-strong\" boosting theorem for stability in these settings: the randomness complexity of a task $\\mathcal{M}$ is tightly controlled by the best replication probability of any \\textit{deterministic} algorithm solving $\\mathcal{M}$, a parameter known as $\\mathcal{M}$'s \"global stability\" (Chase, Moran, Yehudayoff FOCS 2023). Finally, we use this connection to characterize the randomness complexity of PAC Learning: a class has bounded randomness complexity iff it has finite Littlestone dimension, and moreover scales at worst logarithmically in the excess error of the learner. As a corollary, we resolve a question of (Chase, Chornomaz, Moran, and Yehudayoff STOC 2024) about the error-dependent list-replicability of agnostic learning",
    "checked": true,
    "id": "3fe068763b16f7add6fbdb838890fee32ee9aade",
    "semantic_title": "the role of randomness in stability",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=a6Cagkpmgz": {
    "title": "Stochastic Smoothed Primal-Dual Algorithms for Nonconvex Optimization with Linear Inequality Constraints",
    "volume": "spotlight",
    "abstract": "We propose smoothed primal-dual algorithms for solving stochastic nonconvex optimization problems with linear \\emph{inequality} constraints. Our algorithms are single-loop and only require a single (or two) samples of stochastic gradients at each iteration. A defining feature of our algorithm is that it is based on an inexact gradient descent framework for the Moreau envelope, where the gradient of the Moreau envelope is estimated using one step of a stochastic primal-dual (linearized) augmented Lagrangian algorithm. To handle inequality constraints and stochasticity, we combine the recently established global error bounds in constrained optimization with a Moreau envelope-based analysis of stochastic proximal algorithms. We establish the optimal (in their respective cases) $O(\\varepsilon^{-4})$ and $O(\\varepsilon^{-3})$ sample complexity guarantees for our algorithms and provide extensions to stochastic linear constraints. Unlike existing methods, iterations of our algorithms are free of subproblems, large batch sizes or increasing penalty parameters in their iterations and they use dual variable updates to ensure feasibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dzwUOiBlQW": {
    "title": "Masked Autoencoders Are Effective Tokenizers for Diffusion Models",
    "volume": "spotlight",
    "abstract": "Recent advances in latent diffusion models have demonstrated their effectiveness for high-resolution image synthesis. However, the properties of the latent space from tokenizer for better learning and generation of diffusion models remain under-explored. Theoretically and empirically, we find that improved generation quality is closely tied to the latent distributions with better structure, such as the ones with fewer Gaussian Mixture modes and more discriminative features. Motivated by these insights, we propose MAETok, an autoencoder (AE) leveraging mask modeling to learn semantically rich latent space while maintaining reconstruction fidelity. Extensive experiments validate our analysis, demonstrating that the variational form of autoencoders is not necessary, and a discriminative latent space from AE alone enables state-of-the-art performance on ImageNet generation using only 128 tokens. MAETok achieves significant practical improvements, enabling a gFID of 1.69 with 76× faster training and 31× higher inference throughput for 512×512 generation. Our findings show that the structure of the latent space, rather than variational constraints, is crucial for effective diffusion models. Code and trained models will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w0xYx9CJhY": {
    "title": "Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance",
    "volume": "spotlight",
    "abstract": "The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs to rectify the outputs of LVLMs. However, these approaches require either costly training or fine-tuning, or API access to proprietary LLMs for post-generation correction. In response to these limitations, we propose Mitigating hallucinAtion via image-gRounded guIdaNcE (MARINE), a framework that is both training-free and API-free. MARINE effectively and efficiently reduces object hallucinations during inference by introducing image-grounded guidance to LVLMs. This is achieved by leveraging open-source vision models to extract object-level information, thereby enhancing the precision of LVLM-generated content. Our framework's flexibility further allows for the integration of multiple vision models, enabling more reliable and robust object-level guidance. Through comprehensive evaluations across 5 popular LVLMs with diverse evaluation metrics and benchmarks, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it reduces hallucinations consistently in GPT-4V-assisted evaluation while maintaining the detailedness of LVLMs' generations. We release our code at https://github.com/Linxi-ZHAO/MARINE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qLfo1sef50": {
    "title": "Policy-labeled Preference Learning: Is Preference Enough for RLHF?",
    "volume": "spotlight",
    "abstract": "To design reward that align with human goals, Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent technique for learning reward functions from human preferences and optimizing models using reinforcement learning algorithms. However, existing RLHF methods often misinterpret trajectories as being generated by an optimal policy, causing inaccurate likelihood estimation and suboptimal learning. To address this, we propose Policy-labeled Preference Learning (PPL) within the Direct Preference Optimization (DPO) framework, which resolves these likelihood mismatch problems by modeling human preferences with regret, reflecting the efficiency of executed policies. Additionally, we introduce a contrastive KL regularization term derived from regret-based principles to enhance sequential contrastive learning. Experiments in high-dimensional continuous control environments demonstrate PPL's significant improvements in offline RLHF performance and its effectiveness in online settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1w0Zp99dnX": {
    "title": "Generalized Random Forests Using Fixed-Point Trees",
    "volume": "spotlight",
    "abstract": "We propose a computationally efficient alternative to generalized random forests (GRFs) for estimating heterogeneous effects in large dimensions. While GRFs rely on a gradient-based splitting criterion, which in large dimensions is computationally expensive and unstable, our method introduces a fixed-point approximation that eliminates the need for Jacobian estimation. This gradient-free approach preserves GRF's theoretical guarantees of consistency and asymptotic normality while significantly improving computational efficiency. We demonstrate that our method achieves a speedup of multiple times over standard GRFs without compromising statistical accuracy. Experiments on both simulated and real-world data validate our approach. Our findings suggest that the proposed method is a scalable alternative for localized effect estimation in machine learning and causal inference applications",
    "checked": true,
    "id": "c2c82df5ac0f61b43d17424292ffca890ec8479e",
    "semantic_title": "generalized random forests using fixed-point trees",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jNCTdUsQaC": {
    "title": "On Learning Parallel Pancakes with Mostly Uniform Weights",
    "volume": "spotlight",
    "abstract": "We study the complexity of learning $k$-mixtures of Gaussians ($k$-GMMs) on $\\mathbb R^d$. This task is known to have complexity $d^{\\Omega(k)}$ in full generality. To circumvent this exponential lower bound on the number of components, research has focused on learning families of GMMs satisfying additional structural properties. A natural assumption posits that the component weights are not exponentially small and that the components have the same unknown covariance. Recent work gave a $d^{O(\\log(1/w_{\\min}))}$-time algorithm for this class of GMMs, where $w_{\\min}$ is the minimum weight. Our first main result is a Statistical Query (SQ) lower bound showing that this quasi-polynomial upper bound is essentially best possible, even for the special case of uniform weights. Specifically, we show that it is SQ-hard to distinguish between such a mixture and the standard Gaussian. We further explore how the distribution of weights affects the complexity of this task. Our second main result is a quasi-polynomial upper bound for the aforementioned testing task when most of the weights are uniform while a small fraction of the weights are potentially arbitrary",
    "checked": true,
    "id": "20a7b35d1aa0e6e5bebf2ef5853c0102b90a0edc",
    "semantic_title": "on learning parallel pancakes with mostly uniform weights",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XrCbBdycDc": {
    "title": "Monte Carlo Tree Diffusion for System 2 Planning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "096fd939b95d7cd2eaf83c172df1d829865064f1",
    "semantic_title": "monte carlo tree diffusion for system 2 planning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=Ukjl86EsIk": {
    "title": "Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Obet2x6GNl": {
    "title": "Algorithms with Calibrated Machine Learning Predictions",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sEBfiF8JBu": {
    "title": "PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PzSG5nKe1q": {
    "title": "RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Large language models (LLMs) deployed as agents solve user-specified tasks over multiple steps while keeping the required manual engagement to a minimum. Crucially, such LLMs need to ground their generations in any feedback obtained to reliably achieve the desired outcomes. We propose an end-to-end reinforcement learning method for teaching models to leverage execution feedback in the realm of code synthesis, where state-of-the-art LLMs struggle to improve code iteratively compared to independent sampling. We benchmark on competitive programming tasks and achieve large performance gains with both small (8B parameters) and large (70B) models, outperforming previous work while reducing the number of samples required by an order of magnitude. Our analysis of inference-time behavior demonstrates that our method produces LLMs that effectively leverage automatic feedback over multiple steps",
    "checked": true,
    "id": "585e95a43f4ceb3b9fdd8408b7b0b5df468c1030",
    "semantic_title": "rlef: grounding code llms in execution feedback with reinforcement learning",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=M7mVzCV6uU": {
    "title": "Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework",
    "volume": "spotlight",
    "abstract": "We introduce FedGVI, a probabilistic Federated Learning (FL) framework that is robust to both prior and likelihood misspecification. FedGVI addresses limitations in both frequentist and Bayesian FL by providing unbiased predictions under model misspecification, with calibrated uncertainty quantification. Our approach generalises previous FL approaches, specifically Partitioned Variational Inference (Ashman et al., 2022), by allowing robust and conjugate updates, decreasing computational complexity at the clients. We offer theoretical analysis in terms of fixed-point convergence, optimality of the cavity distribution, and provable robustness to likelihood misspecification. Further, we empirically demonstrate the effectiveness of FedGVI in terms of improved robustness and predictive performance on multiple synthetic and real world classification data sets",
    "checked": true,
    "id": "419887f4cd777b81ecd2d9163f873a971bcea67f",
    "semantic_title": "federated generalised variational inference: a robust probabilistic federated learning framework",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gcgzQSKR7y": {
    "title": "Stronger Neyman Regret Guarantees for Adaptive Experimental Design",
    "volume": "spotlight",
    "abstract": "We study the design of adaptive, sequential experiments for unbiased average treatment effect (ATE) estimation in the design-based potential outcomes setting. Our goal is to develop adaptive designs offering *sublinear Neyman regret*, meaning their efficiency must approach that of the hindsight-optimal nonadaptive design. Recent work [Dai et al, 2023] introduced ClipOGD, the first method achieving $\\widetilde{O}(\\sqrt{T})$ expected Neyman regret under mild conditions. In this work, we propose adaptive designs with substantially stronger Neyman regret guarantees. In particular, we modify ClipOGD to obtain anytime $\\widetilde{O}(\\log T)$ Neyman regret under natural boundedness assumptions. Further, in the setting where experimental units have pre-treatment covariates, we introduce and study a class of contextual ``multigroup'' Neyman regret guarantees: Given a set of possibly overlapping groups based on the covariates, the adaptive design outperforms each group's best non-adaptive designs. In particular, we develop a contextual adaptive design with $\\widetilde{O}(\\sqrt{T})$ anytime multigroup Neyman regret. We empirically validate the proposed designs through an array of experiments",
    "checked": true,
    "id": "caded9919781839892fb762626e67c8a8b99a796",
    "semantic_title": "stronger neyman regret guarantees for adaptive experimental design",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RL6d53a5jj": {
    "title": "Probabilistic Factorial Experimental Design for Combinatorial Interventions",
    "volume": "spotlight",
    "abstract": "A _combinatorial intervention_, consisting of multiple treatments applied to a single unit with potential interactive effects, has substantial applications in fields such as biomedicine, engineering, and beyond. Given $p$ possible treatments, conducting all possible $2^p$ combinatorial interventions can be laborious and quickly becomes infeasible as $p$ increases. Here we introduce the _probabilistic factorial experimental design_, formalized from how scientists perform lab experiments. In this framework, the experimenter selects a dosage for each possible treatment and applies it to a group of units. Each unit independently receives a random combination of treatments, sampled from a product Bernoulli distribution determined by the dosages. Additionally, the experimenter can carry out such experiments over multiple rounds, adapting the design in an active manner. We address the optimal experimental design problem within a novel intervention model that imposes bounded-degree interactions between treatments. In the passive setting, we provide a closed-form solution for the near-optimal design. Our results prove that a dosage of $\\frac{1}{2}$ for each treatment is optimal up to a factor of $1+O(\\frac{\\ln(n)}{n})$ for estimating any $k$-way interaction model, regardless of $k$, and imply that $O\\big(kp^{3k}\\ln(p)\\big)$ observations are required to accurately estimate this model. For the multi-round setting, we provide a near-optimal acquisition function that can be numerically optimized. We also explore several extensions of the design problem and finally validate our findings through simulations",
    "checked": true,
    "id": "c4cdf40cdd7702c2e0055deb1f251db967562d3a",
    "semantic_title": "probabilistic factorial experimental design for combinatorial interventions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zdOGBRQEbz": {
    "title": "From Mechanistic Interpretability to Mechanistic Biology: Training, Evaluating, and Interpreting Sparse Autoencoders on Protein Language Models",
    "volume": "spotlight",
    "abstract": "Protein language models (pLMs) are powerful predictors of protein structure and function, learning through unsupervised training on millions of protein sequences. pLMs are thought to capture common motifs in protein sequences, but the specifics of pLM features are not well understood. Identifying these features would not only shed light on how pLMs work, but potentially uncover novel protein biology––studying the model to study the biology. Motivated by this, we train sparse autoencoders (SAEs) on the residual stream of a pLM, ESM-2. By characterizing SAE features, we determine that pLMs use a combination of generic features and family-specific features to represent a protein. In addition, we demonstrate how known sequence determinants of properties such as thermostability and subcellular localization can be identified by linear probing of SAE features. For predictive features without known functional associations, we hypothesize their role in unknown mechanisms and provide visualization tools to aid their interpretation. Our study gives a better understanding of the limitations of pLMs, and demonstrates how SAE features can be used to help generate hypotheses for biological mechanisms. We release our code, model weights, and feature visualizer",
    "checked": true,
    "id": "2ac42be0b5fb0b61f4be220bbd65322bed59ecbf",
    "semantic_title": "from mechanistic interpretability to mechanistic biology: training, evaluating, and interpreting sparse autoencoders on protein language models",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=IYLNdCII48": {
    "title": "CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation",
    "volume": "spotlight",
    "abstract": "We present CACTI, a masked autoencoding approach for imputing tabular data that leverages the structure in missingness patterns and contextual information. Our approach employs a novel median truncated copy masking training strategy that encourages the model to learn from empirical patterns of missingness while incorporating semantic relationships between features — captured by column names and text descriptions — to better represent feature dependence. These dual sources of inductive bias enable CACTIto outperform state-of-the-art methods — an average $R^2$ gain of 7.8\\% over the next best method (13.4%, 6.1%, and 5.3% under missing not at random, at random and completely at random, respectively) — across a diverse range of datasets and missingness conditions. Our results highlight the value of leveraging dataset-specific contextual information and missingness patterns to enhance imputation performance",
    "checked": true,
    "id": "344540981e78afefea3ed801d6d7dca4fe9d2328",
    "semantic_title": "cacti: leveraging copy masking and contextual information to improve tabular data imputation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yv416IYTFp": {
    "title": "PASS: Private Attributes Protection with Stochastic Data Substitution",
    "volume": "spotlight",
    "abstract": "The growing Machine Learning (ML) services require extensive collections of user data, which may inadvertently include people's private information irrelevant to the services. Various studies have been proposed to protect private attributes by removing them from the data while maintaining the utilities of the data for downstream tasks. Nevertheless, as we theoretically and empirically show in the paper, these methods reveal severe vulnerability because of a common weakness rooted in their adversarial training based strategies. To overcome this limitation, we propose a novel approach, PASS, designed to stochastically substitute the original sample with another one according to certain probabilities, which is trained with a novel loss function soundly derived from information-theoretic objective defined for utility-preserving private attributes protection. The comprehensive evaluation of PASS on various datasets of different modalities, including facial images, human activity sensory signals, and voice recording datasets, substantiates PASS's effectiveness and generalizability",
    "checked": true,
    "id": "f92495dc1d45d2484cb4ccb908102c3e78318b10",
    "semantic_title": "pass: private attributes protection with stochastic data substitution",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=13HPTmZKbM": {
    "title": "Upweighting Easy Samples in Fine-Tuning Mitigates Forgetting",
    "volume": "spotlight",
    "abstract": "Fine-tuning a pre-trained model on a downstream task often degrades its original capabilities, a phenomenon known as \"catastrophic forgetting\". This is especially an issue when one does not have access to the data and recipe used to develop the pre-trained model. Under this constraint, most existing methods for mitigating forgetting are inapplicable. To address this challenge, we propose a *sample weighting scheme for the fine-tuning data* solely based on the pre-trained model's losses. Specifically, we upweight the easy samples on which the pre-trained model's loss is low and vice versa to limit the drift from the pre-trained model. Our approach is orthogonal and yet complementary to existing methods; while such methods mostly operate on parameter or gradient space, we concentrate on the sample space. We theoretically analyze the impact of fine-tuning with our method in a linear setting, showing that it stalls learning in a certain subspace, which inhibits overfitting to the target task. We empirically demonstrate the efficacy of our method on both language and vision tasks. As an example, when fine-tuning Gemma 2 2B on MetaMathQA, our method results in only a $0.8$% drop in accuracy on GSM8K (another math dataset) compared to standard fine-tuning, while preserving $5.4$% more accuracy on the pre-training datasets",
    "checked": true,
    "id": "46175d09a6a02347c498b6cb715224ef4f38baae",
    "semantic_title": "upweighting easy samples in fine-tuning mitigates forgetting",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=BkrIQPREkn": {
    "title": "Not All Wrong is Bad: Using Adversarial Examples for Unlearning",
    "volume": "spotlight",
    "abstract": "Machine unlearning, where users can request the deletion of a forget dataset, is becoming increasingly important because of numerous privacy regulations. Initial works on \"exact'' unlearning (e.g., retraining) incur large computational overheads. However, while computationally inexpensive, \"approximate'' methods have fallen short of reaching the effectiveness of exact unlearning: models produced fail to obtain comparable accuracy and prediction confidence on both the forget and test (i.e., unseen) dataset. Exploiting this observation, we propose a new unlearning method, Adversarial Machine UNlearning (AMUN), that outperforms prior state-of-the-art (SOTA) methods for image classification. AMUN lowers the confidence of the model on the forget samples by fine-tuning the model on their corresponding adversarial examples. Adversarial examples naturally belong to the distribution imposed by the model on the input space; fine-tuning the model on the adversarial examples closest to the corresponding forget samples (a) localizes the changes to the decision boundary of the model around each forget sample and (b) avoids drastic changes to the global behavior of the model, thereby preserving the model's accuracy on test samples. Using AMUN for unlearning a random 10% of CIFAR-10 samples, we observe that even SOTA membership inference attacks cannot do better than random guessing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Oqm2IzTy9": {
    "title": "Training Deep Learning Models with Norm-Constrained LMOs",
    "volume": "spotlight",
    "abstract": "In this work, we study optimization methods that leverage the linear minimization oracle (LMO) over a norm-ball. We propose a new stochastic family of algorithms that uses the LMO to adapt to the geometry of the problem and, perhaps surprisingly, show that they can be applied to unconstrained problems. The resulting update rule unifies several existing optimization methods under a single framework. Furthermore, we propose an explicit choice of norm for deep architectures, which, as a side benefit, leads to the transferability of hyperparameters across model sizes. Experimentally, we demonstrate significant speedups on nanoGPT training without any reliance on Adam. The proposed method is memory-efficient, requiring only one set of model weights and one set of gradients, which can be stored in half-precision",
    "checked": true,
    "id": "dd3fabfc7b9c1e866661e777325674a4e96b2466",
    "semantic_title": "training deep learning models with norm-constrained lmos",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=sQS0roNQZR": {
    "title": "From Language Models over Tokens to Language Models over Characters",
    "volume": "spotlight",
    "abstract": "Modern language models are internally—and mathematically—distributions over *token* strings rather than *character* strings, posing numerous challenges for programmers building user applications on top of them. For example, if a prompt is specified as a character string, it must be tokenized before passing it to the token-level language model. Thus, the tokenizer and consequent processing are very sensitive to the specification of the prompt (e.g., whether the prompt ends with a space or not). This paper presents algorithms for converting token-level language models to character-level ones. We present both exact and approximate algorithms. In the empirical portion of the paper, we benchmark the practical runtime and approximation quality. Across four publicly available language models, we find that—even with a small computation budget—our method is able to accurately approximate the character-level distribution at reasonably fast speeds, and that a significant improvement in the language model's compression rate (bits/byte) is achieved",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GFsMJKt9Kp": {
    "title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety",
    "volume": "spotlight",
    "abstract": "Recent studies have uncovered a troubling vulnerability in the fine-tuning stage of large language models (LLMs): even fine-tuning on entirely benign datasets can lead to a significant increase in the harmfulness of LLM outputs. Building on this finding, our red teaming study takes this threat one step further by developing a more effective attack. Specifically, we analyze and identify samples within benign datasets that contribute most to safety degradation, then fine-tune LLMs exclusively on these samples. We approach this problem from an outlier detection perspective and propose Self-Inf-N, to detect and extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs on 100 outlier samples selected by Self-Inf-N in the benign datasets severely compromises LLM safety alignment. Extensive experiments across seven mainstream LLMs demonstrate that our attack exhibits high transferability across different architectures and remains effective in practical scenarios. Alarmingly, our results indicate that most existing mitigation strategies fail to defend against this attack, underscoring the urgent need for more robust alignment safeguards. Codes are available at https://github.com/GuanZihan/Benign-Samples-Matter",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DoDXFkF10S": {
    "title": "Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation",
    "volume": "spotlight",
    "abstract": "Latent space interpolations are a powerful tool for navigating deep generative models in applied settings. An example is single-cell RNA sequencing, where existing methods model cellular state transitions as latent space interpolations with variational autoencoders, often assuming linear shifts and Euclidean geometry. However, unless explicitly enforced, linear interpolations in the latent space may not correspond to geodesic paths on the data manifold, limiting methods that assume Euclidean geometry in the data representations. We introduce FlatVI, a novel training framework that regularises the latent manifold of discrete-likelihood variational autoencoders towards Euclidean geometry, specifically tailored for modelling single-cell count data. By encouraging straight lines in the latent space to approximate geodesic interpolations on the decoded single-cell manifold, FlatVI enhances compatibility with downstream approaches that assume Euclidean latent geometry. Experiments on synthetic data support the theoretical soundness of our approach, while applications to time-resolved single-cell RNA sequencing data demonstrate improved trajectory reconstruction and manifold interpolation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3rB0bVU6z6": {
    "title": "RE-Bench: Evaluating Frontier AI R&D Capabilities of Language Model Agents against Human Experts",
    "volume": "spotlight",
    "abstract": "Frontier AI safety policies highlight automation of AI research and development (R&D) by AI agents as an important capability to anticipate. However, there exist few evaluations for AI R&D capabilities, and none that are highly realistic and have a direct comparison to human performance. We introduce RE-Bench (Research Engineering Benchmark, V1), which consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts. We confirm that our experts make progress in the environments given 8 hours, with 82% of expert attempts achieving a non-zero score and 24% matching or exceeding our strong reference solutions. We compare humans to several public frontier models through best-of-$k$ with varying time budgets and agent designs, and find that the best AI agents achieve a score 4× higher than human experts when both are given a total time budget of 2 hours per environment. However, humans currently display better returns to increasing time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget, and achieving 2× the score of the top AI agent when both are given 32 total hours (across different attempts)",
    "checked": true,
    "id": "5dbd9f9fd231863dda17d9e3caff2edb99a113e2",
    "semantic_title": "re-bench: evaluating frontier ai r&d capabilities of language model agents against human experts",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=SnZ7SKykHh": {
    "title": "PokéChamp: an Expert-level Minimax Language Agent",
    "volume": "spotlight",
    "abstract": "We introduce PokéChamp, a minimax agent powered by Large Language Models (LLMs) for Pokémon battles. Built on a general framework for two-player competitive games, PokéChamp leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate PokéChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76\\% against the best existing LLM-based bot and 84\\% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, PokéChamp consistently outperforms the previous best LLM-based bot, Pokéllmon powered by GPT-4o, with a 64\\% win rate. PokéChamp attains a projected Elo of 1300-1500 on the Pokémon Showdown online ladder, placing it among the top 30\\%-10\\% of human players. In addition, this work compiles the largest real-player Pokémon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. This work establishes Pokémon as a benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multi-agent problems. Videos, code, and dataset are available online",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pyIXyl4qFx": {
    "title": "G-Adaptivity: optimised graph-based mesh relocation for finite element methods",
    "volume": "spotlight",
    "abstract": "We present a novel, and effective, approach to achieve optimal mesh relocation in finite element methods (FEMs). The cost and accuracy of FEMs is critically dependent on the choice of mesh points. Mesh relocation (r-adaptivity) seeks to optimise the mesh geometry to obtain the best solution accuracy at given computational budget. Classical r-adaptivity relies on the solution of a separate nonlinear ``meshing'' PDE to determine mesh point locations. This incurs significant cost at remeshing, and relies on estimates that relate interpolation- and FEM-error. Recent machine learning approaches have focused on the construction of fast surrogates for such classical methods. Instead, our new approach trains a graph neural network (GNN) to determine mesh point locations by directly minimising the FE solution error from the PDE system Firedrake to achieve higher solution accuracy. Our GNN architecture closely aligns the mesh solution space to that of classical meshing methodologies, thus replacing classical estimates for optimality with a learnable strategy. This allows for rapid and robust training and results in an extremely efficient and effective GNN approach to online r-adaptivity. Our method outperforms both classical, and prior ML, approaches to r-adaptive meshing. In particular, it achieves lower FE solution error, whilst retaining the significant speed-up over classical methods observed in prior ML work",
    "checked": true,
    "id": "377adce7ca3b98fcba67993fdde47254dede4385",
    "semantic_title": "g-adaptivity: optimised graph-based mesh relocation for finite element methods",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KKwBo3u3IW": {
    "title": "Mastering Board Games by External and Internal Planning with Language Models",
    "volume": "spotlight",
    "abstract": "Advancing planning and reasoning capabilities of Large Language Models (LLMs) is one of the key prerequisites towards unlocking their potential for performing reliably in complex and impactful domains. In this paper, we aim to demonstrate this across board games (Chess, Fischer Random / Chess960, Connect Four, and Hex), and we show that search-based planning can yield significant improvements in LLM game-playing strength. We introduce, compare and contrast two major approaches: In *external search*, the model guides Monte Carlo Tree Search (MCTS) rollouts and evaluations without calls to an external game engine, and in *internal search*, the model is trained to generate in-context a linearized tree of search and a resulting final choice. Both build on a language model pre-trained on relevant domain knowledge, reliably capturing the transition and value functions in the respective environments, with minimal hallucinations. We evaluate our LLM search implementations against game-specific state-of-the-art engines, showcasing substantial improvements in strength over the base model, and reaching Grandmaster-level performance in chess while operating closer to the human search budget. Our proposed approach, combining search with domain knowledge, is not specific to board games, hinting at more general future applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oYyaVSqEFu": {
    "title": "When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network",
    "volume": "spotlight",
    "abstract": "Anomaly detection is essential for the safety and reliability of autonomous driving systems. Current methods often focus on detection accuracy but neglect response time, which is critical in time-sensitive driving scenarios. In this paper, we introduce real-time anomaly detection for autonomous driving, prioritizing both minimal response time and high accuracy. We propose a novel multimodal asynchronous hybrid network that combines event streams from event cameras with image data from RGB cameras. Our network utilizes the high temporal resolution of event cameras through an asynchronous Graph Neural Network and integrates it with spatial features extracted by a CNN from RGB images. This combination effectively captures both the temporal dynamics and spatial details of the driving environment, enabling swift and precise anomaly detection. Extensive experiments on benchmark datasets show that our approach outperforms existing methods in both accuracy and response time, achieving millisecond-level real-time performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MkCnPNOLMk": {
    "title": "Towards Better-than-2 Approximation for Constrained Correlation Clustering",
    "volume": "spotlight",
    "abstract": "In the Correlation Clustering problem, we are given an undirected graph and are tasked with computing a clustering (partition of the nodes) that minimizes the sum of the number of edges across different clusters and the number of non-edges within clusters. In the constrained version of this problem, the goal is to compute a clustering that satisfies additional hard constraints mandating certain pairs to be in the same cluster and certain pairs to be in different clusters. Constrained Correlation Clustering is APX-Hard, and the best known approximation factor is 3 (van Zuylen et al. [SODA '07]). In this work, we show that in order to obtain a better-than-2 approximation, solving the (exponentially large) Constrained Cluster LP would be sufficient. [The peer-reviewed version of this article claimed an efficient algorithm for solving the Constrained Cluster LP. An error in the proof, that the authors discovered after the review process, led them to revise the results to be conditional on the existence of a valid LP solution.]",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K2CckZjNy0": {
    "title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders",
    "volume": "spotlight",
    "abstract": "Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well, including sparse autoencoders (SAEs), linear artificial tomography, supervised steering vectors, linear probes, and representation finetuning. At present, there is no benchmark for making direct comparisons between these proposals. Therefore, we introduce AxBench, a large-scale benchmark for steering and concept detection, and report experiments on Gemma-2-2B and 9B. For steering, we find that prompting outperforms all existing methods, followed by finetuning. For concept detection, representation-based methods such as difference-in-means, perform the best. On both evaluations, SAEs are not competitive. We introduce a novel weakly-supervised representational method (Rank-1 Representation Finetuning; ReFT-r1), which is competitive on both tasks while providing the interpretability advantages that prompting lacks. Along with AxBench, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean",
    "checked": true,
    "id": "bb94ea8f2e600a2729d11c359c3787bcff2d4f6a",
    "semantic_title": "axbench: steering llms? even simple baselines outperform sparse autoencoders",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=beeNgQEfe2": {
    "title": "Scaling Test-Time Compute Without Verification or RL is Suboptimal",
    "volume": "spotlight",
    "abstract": "Despite substantial advances in scaling test-time compute, an ongoing debate in the community is how it should be scaled up to enable continued and efficient improvements with scaling. There are largely two approaches: (i) distilling successful search or thinking traces; and (ii), using verification (e.g., 0/1 outcome rewards, or verifiers) to guide reinforcement learning (RL) and search algorithms. In this paper, we prove that finetuning LLMs with verifier-based (VB) methods based on RL or search is far superior to verifier-free (VF) approaches based on distilling or cloning search traces, given a fixed amount of compute/data budget. Further, we show that as we scale test-time compute (measured as the output token length) and training data, suboptimality of VF methods scales poorly compared to VB when the base pre-trained LLM presents a heterogeneous distribution over correct solution traces (e.g., different lengths, styles, etc.) and admits a non-sharp distribution over rewards on traces sampled from it. We formalize this condition using anti-concentration [Erdős 1945], implying a stronger result that VB methods scale better asymptotically, with the performance gap between VB and VF widening as test-time budget grows. We corroborate our theory empirically on didactic and math reasoning problems with 3/8/32B-sized pre-trained LLMs, where we find verification is crucial for scaling test-time compute",
    "checked": true,
    "id": "397c0957dfdbd94008d919320fa3d6e07052cabd",
    "semantic_title": "scaling test-time compute without verification or rl is suboptimal",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=9u5hPIcr6j": {
    "title": "LotteryCodec: Searching the Implicit Representation in a Random Network for Low-Complexity Image Compression",
    "volume": "spotlight",
    "abstract": "We introduce and validate the lottery codec hypothesis, which states that untrained subnetworks within randomly initialized networks can serve as synthesis networks for overfitted image compression, achieving rate-distortion (RD) performance comparable to trained networks. This hypothesis leads to a new paradigm for image compression by encoding image statistics into the network substructure. Building on this hypothesis, we propose LotteryCodec, which overfits a binary mask to an individual image, leveraging an over-parameterized and randomly initialized network shared by the encoder and the decoder. To address over-parameterization challenges and streamline subnetwork search, we develop a rewind modulation mechanism that improves the RD performance. LotteryCodec outperforms VTM and sets a new state-of-the-art in single-image compression. LotteryCodec also enables adaptive decoding complexity through adjustable mask ratios, offering flexible compression solutions for diverse device constraints and application requirements",
    "checked": true,
    "id": "93c74e9ff7dab424b5aee20603919811ebea98ac",
    "semantic_title": "lotterycodec: searching the implicit representation in a random network for low-complexity image compression",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mlmpf4Izrj": {
    "title": "Efficiently Vectorized MCMC on Modern Accelerators",
    "volume": "spotlight",
    "abstract": "With the advent of automatic vectorization tools (e.g., JAX's vmap), writing multi-chain MCMC algorithms is often now as simple as invoking those tools on single-chain code. Whilst convenient, for various MCMC algorithms this results in a synchronization problem---loosely speaking, at each iteration all chains running in parallel must wait until the last chain has finished drawing its sample. In this work, we show how to design single-chain MCMC algorithms in a way that avoids synchronization overheads when vectorizing with tools like vmap, by using the framework of finite state machines (FSMs). Using a simplified model, we derive an exact theoretical form of the obtainable speed-ups using our approach, and use it to make principled recommendations for optimal algorithm design. We implement several popular MCMC algorithms as FSMs, including Elliptical Slice Sampling, HMC-NUTS, and Delayed Rejection, demonstrating speed-ups of up to an order of magnitude in experiments",
    "checked": true,
    "id": "18fa6646245f5af9964e23deeab83c09b396f80e",
    "semantic_title": "efficiently vectorized mcmc on modern accelerators",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5IpVe9PH14": {
    "title": "Catoni Contextual Bandits are Robust to Heavy-tailed Rewards",
    "volume": "spotlight",
    "abstract": "Typical contextual bandit algorithms assume that the rewards at each round lie in some fixed range $[0, R]$, and their regret scales polynomially with this reward range $R$. However, many practical scenarios naturally involve heavy-tailed rewards or rewards where the worst-case range can be substantially larger than the variance. In this paper, we develop an algorithmic approach building on Catoni's estimator from robust statistics, and apply it to contextual bandits with general function approximation. When the variance of the reward at each round is known, we use a variance-weighted regression approach and establish a regret bound that depends only on the cumulative reward variance and logarithmically on the reward range $R$ as well as the number of rounds $T$. For the unknown-variance case, we further propose a careful peeling-based algorithm and remove the need for cumbersome variance estimation. With additional dependence on the fourth moment, our algorithm also enjoys a variance-based bound with logarithmic reward-range dependence. Moreover, we demonstrate the optimality of the leading-order term in our regret bound through a matching lower bound",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gn6L4QRKf7": {
    "title": "On the Power of Context-Enhanced Learning in LLMs",
    "volume": "spotlight",
    "abstract": "We formalize a new concept for LLMs, **context-enhanced learning**. It involves standard gradient-based learning on text except that the context is enhanced with additional data on which no auto-regressive gradients are computed. This setting is a gradient-based analog of usual in-context learning (ICL) and appears in some recent works. Using a multi-step reasoning task, we prove in a simplified setting that context-enhanced learning can be **exponentially more sample-efficient** than standard learning when the model is capable of ICL. At a mechanistic level, we find that the benefit of context-enhancement arises from a more accurate gradient learning signal. We also experimentally demonstrate that it appears hard to detect or recover learning materials that were used in the context during training. This may have implications for data security as well as copyright",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bkauyuzBN4": {
    "title": "Privacy Amplification by Structured Subsampling for Deep Differentially Private Time Series Forecasting",
    "volume": "spotlight",
    "abstract": "Many forms of sensitive data, such as web traffic, mobility data, or hospital occupancy, are inherently sequential. The standard method for training machine learning models while ensuring privacy for units of sensitive information, such as individual hospital visits, is differentially private stochastic gradient descent (DP-SGD). However, we observe in this work that the formal guarantees of DP-SGD are incompatible with time series specific tasks like forecasting, since they rely on the *privacy amplification* attained by training on small, unstructured batches sampled from an unstructured dataset. In contrast, batches for forecasting are generated by (1) sampling sequentially structured time series from a dataset, (2) sampling contiguous subsequences from these series, and (3) partitioning them into context and ground-truth forecast windows. We theoretically analyze the privacy amplification attained by this *structured subsampling* to enable the training of forecasting models with sound and tight event- and user-level privacy guarantees. Towards more private models, we additionally prove how data augmentation amplifies privacy in self-supervised training of sequence models. Our empirical evaluation demonstrates that amplification by structured subsampling enables the training of forecasting models with strong formal privacy guarantees",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=glLqTK9En3": {
    "title": "Functional Alignment Can Mislead: Examining Model Stitching",
    "volume": "spotlight",
    "abstract": "A common belief in the representational comparison literature is that if two representations can be functionally aligned, they must capture similar information. In this paper we focus on model stitching and show that models can be functionally aligned, but represent very different information. Firstly, we show that discriminative models with very different biases can be stitched together. We then show that models trained to solve entirely different tasks on different data modalities, and even clustered random noise, can be successfully stitched into MNIST or ImageNet-trained models. We end with a discussion of the wider impact of our results on the community's current beliefs. Overall, our paper draws attention to the need to correctly interpret the results of such functional similarity measures and highlights the need for approaches that capture informational similarity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4qIP1sXcR1": {
    "title": "ResQ: Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals",
    "volume": "spotlight",
    "abstract": "Post-training quantization (PTQ) of large language models (LLMs) holds the promise in reducing the prohibitive computational cost at inference time. Quantization of all weight, activation and key-value (KV) cache tensors to 4-bit without significantly degrading generalizability is challenging, due to the high quantization error caused by extreme outliers in activations. To tackle this problem, we propose ResQ, a PTQ method that pushes further the state-of-the-art. By means of principal component analysis (PCA), it identifies a low-rank subspace (in practice 1/8 of the hidden dimension) in which activation variances are highest, and keep the coefficients within this subspace in high precision, e.g.~8-bit, while quantizing the rest to 4-bit. Within each subspace, invariant random rotation is applied to further suppress outliers. We show that this is a provably optimal mixed precision quantization scheme that minimizes error. With the Llama and Qwen2.5 families of models, we demonstrate that ResQ outperforms recent uniform and mixed precision PTQ methods on a variety of benchmarks, achieving up to 33\\% lower perplexity on Wikitext than the next best method SpinQuant, and upto 3X speedup over 16-bit baseline. Anonymous code repository available at https://anonymous.4open.science/r/project-resq-2142",
    "checked": true,
    "id": "ea9c5ab63f8646fc0327e9e7c74ec435b151eba2",
    "semantic_title": "resq: mixed-precision quantization of large language models with low-rank residuals",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=u6xeKVHS6K": {
    "title": "GMAIL: Generative Modality Alignment for generated Image Learning",
    "volume": "spotlight",
    "abstract": "Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined \\textit{GMAIL}, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves performance on image captioning, zero-shot image retrieval, zero-shot image classification, and long caption retrieval tasks. It also shows positive generated data scaling trends and notable enhancements in the captioning performance of the large multimodal model, LLaVA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DzLP43CbiX": {
    "title": "Flopping for FLOPs: Leveraging Equivariance for Computational Efficiency",
    "volume": "spotlight",
    "abstract": "Incorporating geometric invariance into neural networks enhances parameter efficiency but typically increases computational costs. This paper introduces new equivariant neural networks that preserve symmetry while maintaining a comparable number of floating-point operations (FLOPs) per parameter to standard non-equivariant networks. We focus on horizontal mirroring (flopping) invariance, common in many computer vision tasks. The main idea is to parametrize the feature spaces in terms of mirror-symmetric and mirror-antisymmetric features, i.e., irreps of the flopping group. This decomposes the linear layers to be block-diagonal, requiring half the number of FLOPs. Our approach reduces both FLOPs and wall-clock time, providing a practical solution for efficient, scalable symmetry-aware architectures",
    "checked": true,
    "id": "c9ff288159d0eb77c0fb47f2bcc821dbe0834e86",
    "semantic_title": "flopping for flops: leveraging equivariance for computational efficiency",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VpBBw1bL47": {
    "title": "InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective",
    "volume": "spotlight",
    "abstract": "The Segment Anything Model (SAM), a vision foundation model, exhibits impressive zero-shot capabilities in general tasks but struggles in specialized domains. Parameter-efficient fine-tuning (PEFT) is a promising approach to unleash the potential of SAM in novel scenarios. However, existing PEFT methods for SAM neglect the domain-invariant relations encoded in the pre-trained model. To bridge this gap, we propose InfoSAM, an information-theoretic approach that enhances SAM fine-tuning by distilling and preserving its pre-trained segmentation knowledge. Specifically, we formulate the knowledge transfer process as two novel mutual information-based objectives: (i) to compress the domain-invariant relation extracted from pre-trained SAM, excluding pseudo-invariant information as possible, and (ii) to maximize mutual information between the relational knowledge learned by the teacher (pre-trained SAM) and the student (fine-tuned model). The proposed InfoSAM establishes a robust distillation framework for PEFT of SAM. Extensive experiments across diverse benchmarks validate InfoSAM's effectiveness in improving SAM family's performance on real-world tasks, demonstrating its adaptability and superiority in handling specialized scenarios. The code and models are available at https://muyaoyuan.github.io/InfoSAM_Page",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r9HlTuCQfr": {
    "title": "Decision Making under the Exponential Family: Distributionally Robust Optimisation with Bayesian Ambiguity Sets",
    "volume": "spotlight",
    "abstract": "Decision making under uncertainty is challenging as the data-generating process (DGP) is often unknown. Bayesian inference proceeds by estimating the DGP through posterior beliefs on the model's parameters. However, minimising the expected risk under these beliefs can lead to suboptimal decisions due to model uncertainty or limited, noisy observations. To address this, we introduce Distributionally Robust Optimisation with Bayesian Ambiguity Sets (DRO-BAS) which hedges against model uncertainty by optimising the worst-case risk over a posterior-informed ambiguity set. We provide two such sets, based on the posterior expectation (DRO-BAS(PE)) or the posterior predictive (DRO-BAS(PP)) and prove that both admit, under conditions, strong dual formulations leading to efficient single-stage stochastic programs which are solved with a sample average approximation. For DRO-BAS(PE), this covers all conjugate exponential family members while for DRO-BAS(PP) this is shown under conditions on the predictive's moment generating function. Our DRO-BAS formulations outperform existing Bayesian DRO on the Newsvendor problem and achieve faster solve times with comparable robustness on the Portfolio problem",
    "checked": true,
    "id": "58f6b616118f8ca0e261bcaa0a010e56901bee5c",
    "semantic_title": "decision making under the exponential family: distributionally robust optimisation with bayesian ambiguity sets",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=kfYxyvCYQ4": {
    "title": "Hyperspherical Normalization for Scalable Deep Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Scaling up the model size and computation has brought consistent performance improvements in supervised learning. However, this lesson often fails to apply to reinforcement learning (RL) because training the model on non-stationary data easily leads to overfitting and unstable optimization. In response, we introduce SimbaV2, a novel RL architecture designed to stabilize optimization by (i) constraining the growth of weight and feature norm by hyperspherical normalization; and (ii) using a distributional value estimation with reward scaling to maintain stable gradients under varying reward magnitudes. Using the soft actor-critic as a base algorithm, SimbaV2 scales up effectively with larger models and greater compute, achieving state-of-the-art performance on 57 continuous control tasks across 4 domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zk5k2NQcEA": {
    "title": "Score-of-Mixture Training: One-Step Generative Model Training Made Simple via Score Estimation of Mixture Distributions",
    "volume": "spotlight",
    "abstract": "We propose *Score-of-Mixture Training* (SMT), a novel framework for training one-step generative models by minimizing a class of divergences called the $\\alpha$-skew Jensen–Shannon divergence. At its core, SMT estimates the score of mixture distributions between real and fake samples across multiple noise levels. Similar to consistency models, our approach supports both training from scratch (SMT) and distillation using a pretrained diffusion model, which we call *Score-of-Mixture Distillation* (SMD). It is simple to implement, requires minimal hyperparameter tuning, and ensures stable training. Experiments on CIFAR-10 and ImageNet 64×64 show that SMT/SMD are competitive with and can even outperform existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5hyfZ2jYfI": {
    "title": "The Synergy of LLMs & RL Unlocks Offline Learning of Generalizable Language-Conditioned Policies with Low-fidelity Data",
    "volume": "spotlight",
    "abstract": "Developing autonomous agents capable of performing complex, multi-step decision-making tasks specified in natural language remains a significant challenge, particularly in realistic settings where labeled data is scarce and real-time experimentation is impractical. Existing reinforcement learning (RL) approaches often struggle to generalize to unseen goals and states, limiting their applicability. In this paper, we introduce $\\textit{TEDUO}$, a novel training pipeline for offline language-conditioned policy learning in symbolic environments. Unlike conventional methods, $\\textit{TEDUO}$ operates on readily available, unlabeled datasets and addresses the challenge of generalization to previously unseen goals and states. Our approach harnesses large language models (LLMs) in a dual capacity: first, as automatization tools augmenting offline datasets with richer annotations, and second, as generalizable instruction-following agents. Empirical results demonstrate that $\\textit{TEDUO}$ achieves data-efficient learning of robust language-conditioned policies, accomplishing tasks beyond the reach of conventional RL frameworks or out-of-the-box LLMs alone",
    "checked": false,
    "id": "3ee603a1f6cad76b47055bcb823113236dc85900",
    "semantic_title": "the synergy of llms&rl unlocks offline learning of generalizable language-conditioned policies with low-fidelity data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vhc0KrcqWu": {
    "title": "Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts",
    "volume": "spotlight",
    "abstract": "While score-based generative models are the model of choice across diverse domains, there are limited tools available for controlling inference-time behavior in a principled manner, e.g. for composing multiple pretrained models. Existing classifier-free guidance methods use a simple heuristic to mix conditional and unconditional scores to approximately sample from conditional distributions. However, such methods do not approximate the intermediate distributions, necessitating additional `corrector' steps. In this work, we provide an efficient and principled method for sampling from a sequence of annealed, geometric-averaged, or product distributions derived from pretrained score-based models. We derive a weighted simulation scheme which we call Feynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by carefully accounting for terms in the appropriate partial differential equations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo (SMC) resampling algorithms that leverage inference-time scaling to improve sampling quality. We empirically demonstrate the utility of our methods by proposing amortized sampling via inference-time temperature annealing, improving multi-objective molecule generation using pretrained models, and improving classifier-free guidance for text-to-image generation",
    "checked": true,
    "id": "f56acb9e5643d060d092c66b3a22462adcd54568",
    "semantic_title": "feynman-kac correctors in diffusion: annealing, guidance, and product of experts",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=FuGps5Zyia": {
    "title": "Ad-Hoc Human-AI Coordination Challenge",
    "volume": "spotlight",
    "abstract": "Achieving seamless coordination between AI agents and humans is crucial for real-world applications, yet it remains a significant open challenge. Hanabi is a cooperative card game featuring imperfect information, constrained communication, theory of mind requirements, and coordinated action -- making it an ideal testbed for human-AI coordination. However, its use for human-AI interaction has been limited by the challenges of human evaluation. In this work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to overcome the constraints of costly and difficult-to-reproduce human evaluations. We develop \\textit{human proxy agents} on a large-scale human dataset that serve as robust, cheap, and reproducible human-like evaluation partners in AH2AC2. To encourage the development of data-efficient methods, we open-source a dataset of 3,079 games, deliberately limiting the amount of available human gameplay data. We present baseline results for both two- and three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy agents through a controlled evaluation system rather than releasing them publicly. The code is available at \\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}",
    "checked": true,
    "id": "76e21098d2925daeb769a647c9af4886d00b05fd",
    "semantic_title": "ad-hoc human-ai coordination challenge",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lie2rOCgkh": {
    "title": "Causal Attribution Analysis for Continuous Outcomes",
    "volume": "spotlight",
    "abstract": "Previous studies have extensively addressed the attribution problem for binary outcome variables. However, in many practical scenarios, the outcome variable is continuous, and simply binarizing it may result in information loss or biased conclusions. To address this issue, we propose a series of posterior causal estimands for retrospectively evaluating multiple correlated causes from a continuous outcome. These estimands include posterior intervention effects, posterior total causal effects, and posterior natural direct effects. Under assumptions of sequential ignorability, monotonicity, and perfect positive rank, we show that the posterior causal estimands of interest are identifiable and present the corresponding identification equations. We also provide a simple but effective estimation procedure and establish asymptotic properties of the proposed estimators. An artificial hypertension example and a real developmental toxicity dataset are employed to illustrate our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MHaSq1LlTe": {
    "title": "Signed Laplacians for Constrained Graph Clustering",
    "volume": "spotlight",
    "abstract": "Given two weighted graphs $G = (V, E, w_G)$ and $H = (V, F, w_H)$ defined on the same vertex set, the constrained clustering problem seeks to find a subset $S \\subset V$ that minimises the cut ratio between $w_G(S, V \\setminus S)$ and $w_H(S, V \\setminus S)$. In this work, we establish a Cheeger-type inequality that relates the solution of the constrained clustering problem to the spectral properties of $ G$ and $H$. To reduce computational complexity, we utilise the signed Laplacian of $H$, streamlining calculations while maintaining accuracy. By solving a generalised eigenvalue problem, our proposed algorithm achieves notable performance improvements, particularly in challenging scenarios where traditional spectral clustering methods struggle. We demonstrate its practical effectiveness through experiments on both synthetic and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5liHhkgvAn": {
    "title": "SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming",
    "volume": "spotlight",
    "abstract": "Neural network verifiers based on linear bound propagation scale impressively to massive models but can be surprisingly loose when neuron coupling is crucial. Conversely, semidefinite programming (SDP) verifiers capture inter-neuron coupling naturally, but their cubic complexity restricts them to only small models. In this paper, we propose SDP-CROWN, a novel hybrid verification framework that combines the tightness of SDP relaxations with the scalability of bound-propagation verifiers. At the core of SDP-CROWN is a new linear bound---derived via SDP principles---that explicitly captures $\\ell_{2}$-norm-based inter-neuron coupling while adding only one extra parameter per layer. This bound can be integrated seamlessly into any linear bound-propagation pipeline, preserving the inherent scalability of such methods yet significantly improving tightness. In theory, we prove that our inter-neuron bound can be up to a factor of $\\sqrt{n}$ tighter than traditional per-neuron bounds. In practice, when incorporated into the state-of-the-art $\\alpha$-CROWN verifier, we observe markedly improved verification performance on large models with up to 65 thousand neurons and 2.47 million parameters, achieving tightness that approaches that of costly SDP-based methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=73mDARqOtQ": {
    "title": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative Decoding",
    "volume": "spotlight",
    "abstract": "The emergence of long-context large language models (LLMs) offers a promising alternative to traditional retrieval-augmented generation (RAG) for processing extensive documents. However, the computational overhead of long-context inference presents significant efficiency challenges. While Speculative Decoding (SD) traditionally accelerates inference using smaller draft models, its effectiveness diminishes substantially in long-context scenarios due to memory-bound KV cache operations. We introduce Retrieval-Augmented Speculative Decoding (RAPID), which leverages RAG for both accelerating and enhancing generation quality in long-context inference. RAPID introduces the RAG drafter—a draft LLM operating on shortened retrieval contexts—to speculate on the generation of long-context target LLMs. Our approach enables a new paradigm where same-scale or even larger LLMs can serve as RAG drafters while maintaining computational efficiency. To fully leverage the potentially superior capabilities from stronger RAG drafters, we develop an inference-time knowledge transfer that enriches the target distribution by RAG. Extensive experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates the strengths of both RAG and long-context LLMs, achieving significant performance improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with more than 2$\\times$ speedups for long-context inference. Our analyses also reveal the robustness of RAPID across various context lengths and retrieval quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=clJIQ4TKR0": {
    "title": "Investigating Non-Transitivity in LLM-as-a-Judge",
    "volume": "spotlight",
    "abstract": "Automatic evaluation methods based on large language models (LLMs) are emerging as the standard tool for assessing the instruction-following abilities of LLM-based agents. The most common method in this paradigm, pairwise comparisons with a baseline model, critically depends on the assumption of transitive preferences. However, the validity of this assumption remains largely unexplored. In this study, we investigate the presence of non-transitivity within the AlpacaEval framework and analyze its effects on model rankings. We find that LLM judges exhibit non-transitive preferences, leading to rankings that are sensitive to the choice of the baseline model. To mitigate this issue, we show that round-robin tournaments combined with Bradley-Terry models of preference can produce more reliable rankings. Notably, our method increases both the Spearman correlation and the Kendall correlation with Chatbot Arena (95.0\\% $\\rightarrow$ 96.4\\% and 82.1\\% $\\rightarrow$ 86.3\\% respectively). To address the computational cost of round-robin tournaments, we propose Swiss-Wise Iterative Matchmaking (Swim) tournaments, using a dynamic matching strategy to capture the benefits of round-robin tournaments while maintaining computational efficiency",
    "checked": true,
    "id": "5997e69e637ba532b287124af85e48e02b98f432",
    "semantic_title": "investigating non-transitivity in llm-as-a-judge",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=Hp53p5AU7X": {
    "title": "Reducing Variance of Stochastic Optimization for Approximating Nash Equilibria in Normal-Form Games",
    "volume": "spotlight",
    "abstract": "Nash equilibrium (NE) plays an important role in game theory. How to efficiently compute an NE in NFGs is challenging due to its complexity and non-convex optimization property. Machine Learning (ML), the cornerstone of modern artificial intelligence, has demonstrated remarkable empirical performance across various applications including non-convex optimization. To leverage non-convex stochastic optimization techniques from ML for approximating an NE, various loss functions have been proposed. Among these, only one loss function is unbiased, allowing for unbiased estimation under the sampled play. Unfortunately, this loss function suffers from high variance, which degrades the convergence rate. To improve the convergence rate by mitigating the high variance associated with the existing unbiased loss function, we propose a novel surrogate loss function named Nash Advantage Loss (NAL). NAL is theoretically proved unbiased and exhibits significantly lower variance than the existing unbiased loss function. Experimental results demonstrate that the algorithm minimizing NAL achieves a significantly faster empirical convergence rates compared to other algorithms, while also reducing the variance of estimated loss value by several orders of magnitude",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Z04wVQ9FY": {
    "title": "Linearization Turns Neural Operators into Function-Valued Gaussian Processes",
    "volume": "spotlight",
    "abstract": "Neural operators generalize neural networks to learn mappings between function spaces from data. They are commonly used to learn solution operators of parametric partial differential equations (PDEs) or propagators of time-dependent PDEs. However, to make them useful in high-stakes simulation scenarios, their inherent predictive error must be quantified reliably. We introduce LUNO, a novel framework for approximate Bayesian uncertainty quantification in trained neural operators. Our approach leverages model linearization to push (Gaussian) weight-space uncertainty forward to the neural operator's predictions. We show that this can be interpreted as a probabilistic version of the concept of currying from functional programming, yielding a function-valued (Gaussian) random process belief. Our framework provides a practical yet theoretically sound way to apply existing Bayesian deep learning methods such as the linearized Laplace approximation to neural operators. Just as the underlying neural operator, our approach is resolution-agnostic by design. The method adds minimal prediction overhead, can be applied post-hoc without retraining the network, and scales to large models and datasets. We evaluate these aspects in a case study on Fourier neural operators",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qq5h78Eshy": {
    "title": "Rapid Overfitting of Multi-Pass SGD in Stochastic Convex Optimization",
    "volume": "spotlight",
    "abstract": "We study the out-of-sample performance of multi-pass stochastic gradient descent (SGD) in the fundamental stochastic convex optimization (SCO) model. While one-pass SGD is known to achieve an optimal $\\Theta(1/\\sqrt{n})$ excess population loss given a sample of size $n$, much less is understood about the multi-pass version of the algorithm which is widely used in practice. Somewhat surprisingly, we show that in the general non-smooth case of SCO, just a few epochs of SGD can already hurt its out-of-sample performance significantly and lead to overfitting. In particular, using a step size $\\eta = \\Theta(1/\\sqrt{n})$, which gives the optimal rate after one pass, can lead to population loss as large as $\\Omega(1)$ after just one additional pass. More generally, we show that the population loss from the second pass onward is of the order $\\Theta(1/(\\eta T) + \\eta \\sqrt{T})$, where $T$ is the total number of steps. These results reveal a certain phase-transition in the out-of-sample behavior of SGD after the first epoch, as well as a sharp separation between the rates of overfitting in the smooth and non-smooth cases of SCO. Additionally, we extend our results to with-replacement SGD, proving that the same asymptotic bounds hold after $O(n \\log n)$ steps. Finally, we also prove a lower bound of $\\Omega(\\eta \\sqrt{n})$ on the generalization gap of one-pass SGD in dimension $d = {\\widetilde O}(n)$, improving on recent results of Koren et al. (2022) and Schliserman et al. (2024)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IKCfxWtTsu": {
    "title": "PCEvolve: Private Contrastive Evolution for Synthetic Dataset Generation via Few-Shot Private Data and Generative APIs",
    "volume": "spotlight",
    "abstract": "The rise of generative APIs has fueled interest in privacy-preserving synthetic data generation. While the Private Evolution (PE) algorithm generates Differential Privacy (DP) synthetic images using diffusion model APIs, it struggles with few-shot private data due to the limitations of its DP-protected similarity voting approach. In practice, the few-shot private data challenge is particularly prevalent in specialized domains like healthcare and industry. To address this challenge, we propose a novel API-assisted algorithm, Private Contrastive Evolution (PCEvolve), which iteratively mines inherent inter-class contrastive relationships in few-shot private data beyond individual data points and seamlessly integrates them into an adapted Exponential Mechanism (EM) to optimize DP's utility in an evolution loop. We conduct extensive experiments on four specialized datasets, demonstrating that PCEvolve outperforms PE and other API-assisted baselines. These results highlight the potential of leveraging API access with private data for quality evaluation, enabling the generation of high-quality DP synthetic images and paving the way for more accessible and effective privacy-preserving generative API applications. Our code is available at https://github.com/TsingZ0/PCEvolve",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F8NTPAz5HH": {
    "title": "Is Complex Query Answering Really Complex?",
    "volume": "spotlight",
    "abstract": "Complex query answering (CQA) on knowledge graphs (KGs) is gaining momentum as a challenging reasoning task. In this paper, we show that the current benchmarks for CQA might not be as *complex* as we think, as the way they are built distorts our perception of progress in this field. For example, we find that in these benchmarks most queries (up to 98% for some query types) can be reduced to simpler problems, e.g., link prediction, where only one link needs to be predicted. The performance of state-of-the-art CQA models decreses significantly when such models are evaluated on queries that cannot be reduced to easier types. Thus, we propose a set of more challenging benchmarks composed of queries that *require* models to reason over multiple hops and better reflect the construction of real-world KGs. In a systematic empirical investigation, the new benchmarks show that current methods leave much to be desired from current CQA methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v4DWXM93VV": {
    "title": "Convergence of Mean-Field Langevin Stochastic Descent-Ascent for Distributional Minimax Optimization",
    "volume": "spotlight",
    "abstract": "We study convergence properties of the discrete-time Mean-Field Langevin Stochastic Descent-Ascent (MFL-SDA) algorithm for solving distributional minimax optimization. These problems arise in various applications, such as zero-sum games, generative adversarial networks and distributionally robust learning. Despite the significance of MFL-SDA in these contexts, the discrete-time convergence rate remains underexplored. To address this gap, we establish a last-iterate convergence rate of $O(\\frac{1}{\\epsilon}\\log\\frac{1}{\\epsilon})$ for MFL-SDA. This rate is nearly optimal when compared to the complexity lower bound of its Euclidean counterpart. This rate also matches the complexity of mean-field Langevin stochastic gradient descent for distributional minimization and the outer-loop iteration complexity of an existing double-loop algorithm for distributional minimax problems. By leveraging an elementary analysis framework that avoids PDE-based techniques, we overcome previous limitations and achieve a faster convergence rate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zL6ljQvPzZ": {
    "title": "Lightweight Protocols for Distributed Private Quantile Estimation",
    "volume": "spotlight",
    "abstract": "Distributed data analysis is a large and growing field driven by a massive proliferation of user devices, and by privacy concerns surrounding the centralised storage of data. We consider two \\emph{adaptive} algorithms for estimating one quantile (e.g.~the median) when each user holds a single data point lying in a domain $[B]$ that can be queried once through a private mechanism; one under local differential privacy (LDP) and another for shuffle differential privacy (shuffle-DP). In the adaptive setting we present an $\\varepsilon$-LDP algorithm which can estimate any quantile within error $\\alpha$ only requiring $O(\\frac{\\log B}{\\varepsilon^2\\alpha^2})$ users, and an $(\\varepsilon,\\delta)$-shuffle DP algorithm requiring only $\\widetilde{O}((\\frac{1}{\\varepsilon^2}+\\frac{1}{\\alpha^2})\\log B)$ users. Prior (nonadaptive) algorithms require more users by several logarithmic factors in $B$. We further provide a matching lower bound for adaptive protocols, showing that our LDP algorithm is optimal in the low-$\\varepsilon$ regime. Additionally, we establish lower bounds against non-adaptive protocols which paired with our understanding of the adaptive case, proves a fundamental separation between these models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AnoIgkc6WS": {
    "title": "Exogenous Isomorphism for Counterfactual Identifiability",
    "volume": "spotlight",
    "abstract": "This paper investigates $\\sim_{\\mathcal{L}\\_3}$-identifiability, a form of complete counterfactual identifiability within the Pearl Causal Hierarchy (PCH) framework, ensuring that all Structural Causal Models (SCMs) satisfying the given assumptions provide consistent answers to all causal questions. To simplify this problem, we introduce exogenous isomorphism and propose $\\sim_{\\mathrm{EI}}$-identifiability, reflecting the strength of model identifiability required for $\\sim_{\\mathcal{L}\\_3}$-identifiability. We explore sufficient assumptions for achieving $\\sim_{\\mathrm{EI}}$-identifiability in two special classes of SCMs: Bijective SCMs (BSCMs), based on counterfactual transport, and Triangular Monotonic SCMs (TM-SCMs), which extend $\\sim_{\\mathcal{L}\\_2}$-identifiability. Our results unify and generalize existing theories, providing theoretical guarantees for practical applications. Finally, we leverage neural TM-SCMs to address the consistency problem in counterfactual reasoning, with experiments validating both the effectiveness of our method and the correctness of the theory",
    "checked": true,
    "id": "55ebaeb14d0505e72f90351c2aaf0020cc6073ee",
    "semantic_title": "exogenous isomorphism for counterfactual identifiability",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FSVdEzR4To": {
    "title": "Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data",
    "volume": "spotlight",
    "abstract": "Reinforcement learning with offline data suffers from Q-value extrapolation errors. To address this issue, we first demonstrate that linear extrapolation of the Q-function beyond the data range is particularly problematic. To mitigate this, we propose guiding the gradual decrease of Q-values outside the data range, which is achieved through reward scaling with layer normalization (RS-LN) and a penalization mechanism for infeasible actions (PA). By combining RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a range of tasks, demonstrating superior performance compared to state-of-the-art algorithms in both offline training and online fine-tuning on the D4RL benchmark, with notable success in the challenging AntMaze Ultra task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4gWE7CMOlH": {
    "title": "Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration",
    "volume": "spotlight",
    "abstract": "Large Language Models (LLMs) struggle with complex reasoning due to limited diversity and inefficient search. We propose Soft Reasoning, an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) embedding perturbation for controlled exploration and (2) Bayesian optimisation to refine embeddings via a verifier-guided objective, balancing exploration and exploitation. This approach improves reasoning accuracy and coherence while avoiding reliance on heuristic search. Experiments demonstrate superior correctness with minimal computation, making it a scalable, model-agnostic solution",
    "checked": true,
    "id": "177a1fa9e640a9da307ebe3b7df5a131685ccd49",
    "semantic_title": "soft reasoning: navigating solution spaces in large language models through controlled embedding exploration",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jnPHZqcUdn": {
    "title": "scSSL-Bench: Benchmarking Self-Supervised Learning for Single-Cell Data",
    "volume": "spotlight",
    "abstract": "Self-supervised learning (SSL) has proven to be a powerful approach for extracting biologically meaningful representations from single-cell data. To advance our understanding of SSL methods applied to single-cell data, we present scSSL-Bench, a comprehensive benchmark that evaluates nineteen SSL methods. Our evaluation spans nine datasets and focuses on three common downstream tasks: batch correction, cell type annotation, and missing modality prediction. Furthermore, we systematically assess various data augmentation strategies. Our analysis reveals task-specific trade-offs: the specialized single-cell frameworks, scVI, CLAIRE, and the finetuned scGPT excel at uni-modal batch correction, while generic SSL methods, such as VICReg and SimCLR, demonstrate superior performance in cell typing and multi-modal data integration. Random masking emerges as the most effective augmentation technique across all tasks, surpassing domain-specific augmentations. Notably, our results indicate the need for a specialized single-cell multi-modal data integration framework. scSSL-Bench provides a standardized evaluation platform and concrete recommendations for applying SSL to single-cell analysis, advancing the convergence of deep learning and single-cell genomics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XEyGcrhxB8": {
    "title": "A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO",
    "volume": "spotlight",
    "abstract": "In this paper, we theoretically investigate the effects of noisy labels in offline alignment, with a focus on the interplay between privacy and robustness against adversarial corruption. Specifically, under linear modeling assumptions, we present a unified analysis covering both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) under different privacy-corruption scenarios, such as Local differential privacy-then-Corruption (LTC), where human preference labels are privatized before being corrupted by an adversary, and Corruption-then-Local differential privacy (CTL), where labels are corrupted before privacy protection. Our analysis leverages a reduction framework that reduces the offline alignment problem under linear modeling assumptions to parameter estimation in logistic regression. This framework allows us to establish an interesting separation result between LTC and CTL, demonstrating that LTC presents a greater challenge than CTL in offline alignment, even under linear models. As important by-products, our findings also advance the state-of-the-art theoretical results in offline alignment under privacy-only or corruption-only scenarios",
    "checked": true,
    "id": "e2d64cacc9639ab176425055550c6b2139348942",
    "semantic_title": "a unified theoretical analysis of private and robust offline alignment: from rlhf to dpo",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=9vYGZX4OVN": {
    "title": "Adjusting Model Size in Continual Gaussian Processes: How Big is Big Enough?",
    "volume": "spotlight",
    "abstract": "Many machine learning models require setting a parameter that controls their size before training, e.g. number of neurons in DNNs, or inducing points in GPs. Increasing capacity typically improves performance until all the information from the dataset is captured. After this point, computational cost keeps increasing, without improved performance. This leads to the question \"How big is big enough?\" We investigate this problem for Gaussian processes (single-layer neural networks) in continual learning. Here, data becomes available incrementally, and the final dataset size will therefore not be known before training, preventing the use of heuristics for setting a fixed model size. We develop a method to automatically adjust model size while maintaining near-optimal performance. Our experimental procedure follows the constraint that any hyperparameters must be set without seeing dataset properties, and we show that our method performs well across diverse datasets without the need to adjust its hyperparameter, showing it requires less tuning than others",
    "checked": true,
    "id": "36390a0b60805f2772f7207df4f591f12dd74edf",
    "semantic_title": "adjusting model size in continual gaussian processes: how big is big enough?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9hd5WA6QCn": {
    "title": "MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding",
    "volume": "spotlight",
    "abstract": "Multimodal large language models (MLLMs) recently showed strong capacity in integrating data among multiple modalities, empowered by generalizable attention architecture. Advanced methods predominantly focus on language-centric tuning while less exploring multimodal tokens mixed through attention, posing challenges in high-level tasks that require fine-grained cognition and emotion understanding. In this work, we identify the attention deficit disorder problem in multimodal learning, caused by inconsistent cross-modal attention and layer-by-layer decayed attention activation. To address this, we propose a novel attention mechanism, termed MOdular Duplex Attention (MODA), simultaneously conducting the inner-modal refinement and inter-modal interaction. MODA employs a correct-after-align strategy to effectively decouple modality alignment from cross-layer token mixing. In the alignment phase, tokens are mapped to duplex modality spaces based on the basis vectors, enabling the interaction between visual and language modality. Further, the correctness of attention scores is ensured through adaptive masked attention, which enhances the model's flexibility by allowing customizable masking patterns for different modalities. Extensive experiments on 21 benchmark datasets verify the effectiveness of MODA in perception, cognition, and emotion tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n1cqQK4hhC": {
    "title": "STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization",
    "volume": "spotlight",
    "abstract": "Transforming complex actions into discrete skill abstractions has demonstrated strong potential for robotic manipulation.Existing approaches mainly leverage latent variable models, e.g., VQ-VAE, to learn skill abstractions through learned vectors (codebooks), while they suffer from codebook collapse and modeling the causal relationship between learned skills. To address these limitations, we present **S**kill **T**raining with **A**ugmented **R**otation (**STAR**), a framework that advances both skill learning and composition to complete complex behaviors. Specifically, to prevent codebook collapse, we devise rotation-augmented residual skill quantization (RaRSQ).It encodes relative angles between encoder outputs into the gradient flow by rotation-based gradient mechanism. Points within the same skill code are forced to be either pushed apart or pulled closer together depending on gradient directions.Further, to capture the casual relationship between skills, we present causal skill transformer (CST) which explicitly models dependencies between skill representations through an autoregressive mechanism for coherent action generation.Extensive experiments demonstrate the superiority of STAR on both LIBERO benchmark and realworld tasks, with around 12% improvement over the baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c0dhw1du33": {
    "title": "Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations",
    "volume": "spotlight",
    "abstract": "Visual representations play a crucial role in developing generalist robotic policies. Previous vision encoders, typically pre-trained with single-image reconstruction or two-image contrastive learning, tend to capture static information, often neglecting the dynamic aspects vital for embodied tasks. Recently, video diffusion models (VDMs) demonstrate the ability to predict future frames and showcase a strong understanding of physical world. We hypothesize that VDMs inherently produce visual representations that encompass both current static information and predicted future dynamics, thereby providing valuable guidance for robot action learning. Based on this hypothesis, we propose the Video Prediction Policy (VPP), which learns implicit inverse dynamics model conditioned on predicted future representations inside VDMs. To predict more precise future, we fine-tune pre-trained video foundation model on robot datasets along with internet human manipulation data. In experiments, VPP achieves a 18.6\\% relative improvement on the Calvin ABC-D generalization benchmark compared to the previous state-of-the-art, and demonstrates a 31.6\\% increase in success rates for complex real-world dexterous manipulation tasks. For your convenience, videos can be found at https://video-prediction-policy.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KZo2XhcSg6": {
    "title": "LipsNet++: Unifying Filter and Controller into a Policy Network",
    "volume": "spotlight",
    "abstract": "Deep reinforcement learning (RL) is effective for decision-making and control tasks like autonomous driving and embodied AI. However, RL policies often suffer from the action fluctuation problem in real-world applications, resulting in severe actuator wear, safety risk, and performance degradation. This paper identifies the two fundamental causes of action fluctuation: observation noise and policy non-smoothness. We propose LipsNet++, a novel policy network with Fourier filter layer and Lipschitz controller layer to separately address both causes. The filter layer incorporates a trainable filter matrix that automatically extracts important frequencies while suppressing noise frequencies in the observations. The controller layer introduces a Jacobian regularization technique to achieve a low Lipschitz constant, ensuring smooth fitting of a policy function. These two layers function analogously to the filter and controller in classical control theory, suggesting that filtering and control capabilities can be seamlessly integrated into a single policy network. Both simulated and real-world experiments demonstrate that LipsNet++ achieves the state-of-the-art noise robustness and action smoothness. The code and videos are publicly available at https://xjsong99.github.io/LipsNet_v2",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GKqoqGCHTq": {
    "title": "Improving Consistency Models with Generator-Augmented Flows",
    "volume": "spotlight",
    "abstract": "Consistency models imitate the multi-step sampling of score-based diffusion in a single forward pass of a neural network. They can be learned in two ways: consistency distillation and consistency training. The former relies on the true velocity field of the corresponding differential equation, approximated by a pre-trained neural network. In contrast, the latter uses a single-sample Monte Carlo estimate of this velocity field. The related estimation error induces a discrepancy between consistency distillation and training that, we show, still holds in the continuous-time limit. To alleviate this issue, we propose a novel flow that transports noisy data towards their corresponding outputs derived from a consistency model. We prove that this flow reduces the previously identified discrepancy and the noise-data transport cost. Consequently, our method not only accelerates consistency training convergence but also enhances its overall performance. The code is available at https://github.com/thibautissenhuth/consistency_GC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=evb9dNxCN5": {
    "title": "Where is the Truth? The Risk of Getting Confounded in a Continual World",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "5a17b6d6c92438ebaa6fed488e9883ed82b7b856",
    "semantic_title": "where is the truth? the risk of getting confounded in a continual world",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=mwSBIlNLdQ": {
    "title": "Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "4f3d3374835f376685b19e95ebb0421a4dd27500",
    "semantic_title": "learning dynamics under environmental constraints via measurement-induced bundle structures",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=6gX4rP6QJW": {
    "title": "Self-supervised Masked Graph Autoencoder via Structure-aware Curriculum",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": "377ae43a34cde426c48e003dc17c35e637a98bcf",
    "semantic_title": "dvgmae: self-supervised dynamic variational graph masked autoencoder",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qtuxDy2qEB": {
    "title": "Parallel Simulation for Log-concave Sampling and Score-based Diffusion Models",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vOdz3zhSCj": {
    "title": "Neural Encoding and Decoding at Scale",
    "volume": "spotlight",
    "abstract": "Recent work has demonstrated that large-scale, multi-animal models are powerful tools for characterizing the relationship between neural activity and behavior. Current large-scale approaches, however, focus exclusively on either predicting neural activity from behavior (encoding) or predicting behavior from neural activity (decoding), limiting their ability to capture the bidirectional relationship between neural activity and behavior. To bridge this gap, we introduce a multimodal, multi-task model that enables simultaneous Neural Encoding and Decoding at Scale (NEDS). Central to our approach is a novel multi-task-masking strategy, which alternates between neural, behavioral, within-modality, and cross-modality masking. We pretrain our method on the International Brain Laboratory (IBL) repeated site dataset, which includes recordings from 83 animals performing the visual decision-making task. In comparison to other large-scale modeling approaches, we demonstrate that NEDS achieves state-of-the-art performance for both encoding and decoding when pretrained on multi-animal data and then fine-tuned on new animals. Surprisingly, NEDS's learned embeddings exhibit emergent properties: even without explicit training, they are highly predictive of the brain regions in each recording. Altogether, our approach is a step towards a foundation model of the brain that enables seamless translation between neural activity and behavior",
    "checked": true,
    "id": "997d6559029b8cf93b59055848c02ef5171398be",
    "semantic_title": "neural encoding and decoding at scale",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=92oBV5HAGl": {
    "title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
    "volume": "spotlight",
    "abstract": "Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability---which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability---can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the *lookup-table mechanism* for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks",
    "checked": true,
    "id": "23a921483746b8b3c828bd601f54d485bec32014",
    "semantic_title": "mechanistic unlearning: robust knowledge unlearning and editing via mechanistic localization",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=wXfuOj9C7L": {
    "title": "Learning to (Learn at Test Time): RNNs with Expressive Hidden States",
    "volume": "spotlight",
    "abstract": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden states. We present a practical framework for instantiating sequence modeling layers with linear complexity and expressive hidden states. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Similar to Transformer, TTT-Linear and TTT-MLP can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research",
    "checked": true,
    "id": "35ab93f41115e860bee5e202b71061addfd0fd5d",
    "semantic_title": "learning to (learn at test time): rnns with expressive hidden states",
    "citation_count": 125,
    "authors": []
  },
  "https://openreview.net/forum?id=mQeZEsdODh": {
    "title": "Continual Reinforcement Learning by Planning with Online World Models",
    "volume": "spotlight",
    "abstract": "Continual reinforcement learning (CRL) refers to a naturalistic setting where an agent needs to endlessly evolve, by trial and error, to solve multiple tasks that are presented sequentially. One of the largest obstacles to CRL is that the agent may forget how to solve previous tasks when learning a new task, known as catastrophic forgetting. In this paper, we propose to address this challenge by planning with online world models. Specifically, we learn a Follow-The-Leader shallow model online to capture the world dynamics, in which we plan using model predictive control to solve a set of tasks specified by any reward functions. The online world model is immune to forgetting by construction with a proven regret bound of $\\mathcal{O}(\\sqrt{K^2D\\log(T)})$ under mild assumptions. The planner searches actions solely based on the latest online model, thus forming a FTL Online Agent (OA) that updates incrementally. To assess OA, we further design Continual Bench, a dedicated environment for CRL, and compare with several strong baselines under the same model-planning algorithmic framework. The empirical results show that OA learns continuously to solve new tasks while not forgetting old skills, outperforming agents built on deep world models with various continual learning techniques",
    "checked": true,
    "id": "8d1a5fd439dd67f8a16859566b12a35f705f7c83",
    "semantic_title": "continual reinforcement learning by planning with online world models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzSwYvwYdC": {
    "title": "Independence Tests for Language Models",
    "volume": "spotlight",
    "abstract": "Motivated by liability and intellectual property concerns over open-weight models we consider the following problem: given the weights of two models, can we test whether they were trained independently---i.e., from independent random initializations? We consider two settings: *constrained* and *unconstrained*. In the constrained setting, we make assumptions about model architecture and training and propose statistical tests that yield exact p-values with respect to the null hypothesis that the models are trained from independent random initializations. We compute the p-values by simulating exchangeable copies of each model under our assumptions and comparing various similarity measures between the original two models versus these copies. We report p-values on pairs of 21 open-weight models (210 total pairs) and find we correctly identify all pairs of non-independent models. In the unconstrained setting we make none of the prior assumptions and allow for adversarial evasion attacks that do not change model output. We thus propose a new test which matches hidden activations between two models, which is robust to these transformations and to changes in model architecture and can also identify specific non-independent components of models. Though we no longer obtain exact p-values from this test, empirically we find it reliably distinguishes non-independent models like a p-value. Notably, we can use the test to identify specific parts of one model that are derived from another (e.g., how Llama 3.1-8B was pruned to initialize Llama 3.2-3B, or shared layers between Mistral-7B and StripedHyena-7B), and it is even robust to retraining individual layers of either model from scratch",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ossg1IbHDT": {
    "title": "Scalable Generation of Spatial Transcriptomics from Histology Images via Whole-Slide Flow Matching",
    "volume": "spotlight",
    "abstract": "Spatial transcriptomics (ST) has emerged as a powerful technology for bridging histology imaging with gene expression profiling. However, its application has been limited by low throughput and the need for specialized experimental facilities. Prior works sought to predict ST from whole-slide histology images to accelerate this process, but they suffer from two major limitations. First, they do not explicitly model cell-cell interaction as they factorize the joint distribution of whole-slide ST data and predict the gene expression of each spot independently. Second, their encoders struggle with memory constraints due to the large number of spots (often exceeding 10,000) in typical ST datasets. Herein, we propose STFlow, a flow matching generative model that considers cell-cell interaction by modeling the joint distribution of gene expression of an entire slide. It also employs an efficient slide-level encoder with local spatial attention, enabling whole-slide processing without excessive memory overhead. On the recently curated HEST-1k and STImage-1K4M benchmarks, STFlow substantially outperforms state-of-the-art baselines and achieves over 18% relative improvements over the pathology foundation models",
    "checked": true,
    "id": "67c0e7a79021d4d77e9c50a050e769a309abdc73",
    "semantic_title": "scalable generation of spatial transcriptomics from histology images via whole-slide flow matching",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nVD7KoU09V": {
    "title": "Rethink GraphODE Generalization within Coupled Dynamical System",
    "volume": "spotlight",
    "abstract": "Coupled dynamical systems govern essential phenomena across physics, biology, and engineering, where components interact through complex dependencies. While Graph Ordinary Differential Equations (GraphODE) offer a powerful framework to model these systems, their **generalization** capabilities degrade severely under limited observational training data due to two fundamental flaws: (i) the entanglement of static attributes and dynamic states in the initialization process, and (ii) the reliance on context-specific coupling patterns during training, which hinders performance in unseen scenarios. In this paper, we propose a Generalizable GraphODE with disentanglement and regularization (GREAT) to address these challenges. Through systematic analysis via the Structural Causal Model, we identify backdoor paths that undermine generalization and design two key modules to mitigate their effects. The *Dynamic-Static Equilibrium Decoupler (DyStaED)* disentangles static and dynamic states via orthogonal subspace projections, ensuring robust initialization. Furthermore, the *Causal Mediation for Coupled Dynamics (CMCD)* employs variational inference to estimate latent causal factors, reducing spurious correlations and enhancing universal coupling dynamics. Extensive experiments across diverse dynamical systems demonstrate that ours outperforms state-of-the-art methods within both in-distribution and out-of-distribution",
    "checked": false,
    "id": "2ef72bb8b9656cda3abd0dba5a9eacc03b782d26",
    "semantic_title": "federated learning-enabled cooperative localization in multi-agent system",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=9hFQvmCl7P": {
    "title": "FedSSI: Rehearsal-Free Continual Federated Learning with Synergistic Synaptic Intelligence",
    "volume": "spotlight",
    "abstract": "Continual Federated Learning (CFL) allows distributed devices to collaboratively learn novel concepts from continuously shifting training data while avoiding \\textit{knowledge forgetting} of previously seen tasks. To tackle this challenge, most current CFL approaches rely on extensive rehearsal of previous data. Despite effectiveness, rehearsal comes at a cost to memory, and it may also violate data privacy. Considering these, we seek to apply regularization techniques to CFL by considering their cost-efficient properties that do not require sample caching or rehearsal. Specifically, we first apply traditional regularization techniques to CFL and observe that existing regularization techniques, especially synaptic intelligence, can achieve promising results under homogeneous data distribution but fail when the data is heterogeneous. Based on this observation, we propose a simple yet effective regularization algorithm for CFL named \\textbf{FedSSI}, which tailors the synaptic intelligence for the CFL with heterogeneous data settings. FedSSI can not only reduce computational overhead without rehearsal but also address the data heterogeneity issue. Extensive experiments show that FedSSI achieves superior performance compared to state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pokj70ZAxJ": {
    "title": "Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation",
    "volume": "spotlight",
    "abstract": "Cross-domain few-shot segmentation (CD-FSS) is proposed to first pre-train the model on a source-domain dataset with sufficient samples, and then transfer the model to target-domain datasets where only a few training samples are available for efficient finetuning. There are majorly two challenges in this task: (1) the domain gap and (2) finetuning with scarce data. To solve these challenges, we revisit the adapter-based methods, and discover an intriguing insight not explored in previous works: the adapter not only helps the fine-tuning of downstream tasks but also naturally serves as a domain information decoupler. Then, we delve into this finding for an interpretation, and we find the model's inherent structure could lead to a natural decoupling of domain information. Building upon this insight, we propose the Domain Feature Navigator (DFN), which is a structure-based decoupler instead of loss-based ones like current works, to capture domain-specific information, thereby directing the model's attention towards domain-agnostic knowledge. Moreover, to prevent the potential excessive overfitting of DFN during the source-domain training, we further design the SAM-SVN method to constrain DFN from learning sample-specific knowledge. On target domains, we freeze the model and fine-tune the DFN to learn knowledge specific to target domains. Extensive experiments demonstrate that our method surpasses the state-of-the-art method in CD-FSS significantly by 2.69% and 4.68% average MIoU in 1-shot and 5-shot scenarios, respectively",
    "checked": true,
    "id": "6fcfcf520c884306119212b63761f30ad66d31c2",
    "semantic_title": "adapter naturally serves as decoupler for cross-domain few-shot semantic segmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hrdLhNDAzp": {
    "title": "MCU: An Evaluation Framework for Open-Ended Game Agents",
    "volume": "spotlight",
    "abstract": "Developing AI agents capable of interacting with open-world environments to solve diverse tasks is a compelling challenge. However, evaluating such open-ended agents remains difficult, with current benchmarks facing scalability limitations. To address this, we introduce \\textit{Minecraft Universe} (MCU), a comprehensive evaluation framework set within the open-world video game Minecraft. MCU incorporates three key components: (1) an expanding collection of 3,452 composable atomic tasks that encompasses 11 major categories and 41 subcategories of challenges; (2) a task composition mechanism capable of generating infinite diverse tasks with varying difficulty; and (3) a general evaluation framework that achieves 91.5\\% alignment with human ratings for open-ended task assessment. Empirical results reveal that even state-of-the-art foundation agents struggle with the increasing diversity and complexity of tasks. These findings highlight the necessity of MCU as a robust benchmark to drive progress in AI agent development within open-ended environments. Our evaluation code and scripts are available at https://github.com/CraftJarvis/MCU",
    "checked": true,
    "id": "dee45635aba5d1df5dbb55620800e7570ed2d6fe",
    "semantic_title": "mcu: an evaluation framework for open-ended game agents",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=wBJIO15pBV": {
    "title": "Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion",
    "volume": "spotlight",
    "abstract": "Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. A well-known example is the permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. While permutation symmetry fully characterizes the equivalence set for MLPs, its discrete nature limits its utility for transformers. In this paper, we introduce rotation symmetry, a novel form of parameter space symmetry for transformers that generalizes permutation symmetry by rotating parameter matrices in self-attention layers. Unlike permutation symmetry, rotation symmetry operates in a continuous domain, thereby significantly expanding the equivalence set for transformers. Based on this property, we propose a theoretically optimal parameter matching algorithm as a plug-and-play module to enhance model fusion. We evaluate our approach using pre-trained transformers across diverse natural language and vision tasks. Experimental results demonstrate that our rotation symmetry-based matching algorithm substantially improves model fusion, highlighting the potential of parameter space symmetry to facilitate model fusion. Our code is available on https://github.com/zhengzaiyi/RotationSymmetry",
    "checked": true,
    "id": "18e14b6223f325198d3a3356e7ca1fe426d25123",
    "semantic_title": "beyond the permutation symmetry of transformers: the role of rotation for model fusion",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Z1qZoHa6ql": {
    "title": "Counterfactual Graphical Models: Constraints and Inference",
    "volume": "spotlight",
    "abstract": "Graphical models have been widely used as parsimonious encoders of constraints of the underlying probability models. When organized in a structured way, these models can facilitate the derivation of non-trivial constraints, the inference of quantities of interest, and the optimization of their estimands. In particular, causal diagrams allow for the efficient representation of structural constraints of the underlying causal system. In this paper, we introduce an efficient graphical construction called Ancestral Multi-world Networks that is sound and complete for reading counterfactual independences from a causal diagram using d-separation. Moreover, we introduce the counterfactual (ctf-) calculus, which can be used to transform counterfactual quantities using three rules licensed by the constraints encoded in the diagram. This result generalizes Pearl's celebrated do-calculus from interventional to counterfactual reasoning",
    "checked": false,
    "id": "6a5fd42d5ee80398bab7c5b8a5556359481a7e23",
    "semantic_title": "lecture 12: graphical models for inference tanner graphs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hfLqdquVt3": {
    "title": "Do Multiple Instance Learning Models Transfer?",
    "volume": "spotlight",
    "abstract": "Multiple Instance Learning (MIL) is a cornerstone approach in computational pathology for distilling embeddings from gigapixel tissue images into patient-level representations to predict clinical outcomes. However, MIL is frequently challenged by the constraints of working with small, weakly-supervised clinical datasets. Unlike fields such as natural language processing and computer vision, which effectively use transfer learning to improve model quality in data-scarce environments, the transferability of MIL models remains largely unexplored. We conduct the first comprehensive investigation into transfer learning capabilities of pretrained MIL models, evaluating 11 MIL models across 19 pretraining tasks spanning tissue subtyping, cancer grading, and molecular subtype prediction. We observe a substantial performance boost with finetuning pretrained models over training from randomly initialized weights, even with domain differences between pretraining and target tasks. Pretraining on pan-cancer datasets enables consistent generalization across organs and task types compared to single-disease pretraining. Remarkably, this pan-cancer pretraining leads to better transfer than that of a state-of-the-art slide-level foundation model, while using only 6.5\\% of the training data. These findings indicate that MIL architectures exhibit robust adaptability, offering insights into the benefits of leveraging pretrained models to enhance performance in computational pathology",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QC4dfobOLQ": {
    "title": "Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws",
    "volume": "spotlight",
    "abstract": "This paper formalizes an emerging learning paradigm that uses a trained model as a reference to guide and enhance the training of a target model through strategic data selection or weighting, named **model steering**. While ad-hoc methods have been used in various contexts, including the training of large foundation models, its underlying principles remain insufficiently understood, leading to sub-optimal performance. In this work, we propose a theory-driven framework for model steering called **DRRho risk minimization**, which is rooted in Distributionally Robust Optimization (DRO). Through a generalization analysis, we provide theoretical insights into why this approach improves generalization and data efficiency compared to training without a reference model. To the best of our knowledge, this is the first time such theoretical insights are provided for the new learning paradigm, which significantly enhance our understanding and practice of model steering. Building on these insights and the connection between contrastive learning and DRO, we introduce a novel method for Contrastive Language-Image Pretraining (CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments validate the theoretical insights, reveal a superior scaling law compared to CLIP without a reference model, and demonstrate its strength over existing heuristic approaches. Code is released at [github.com/Optimization-AI/DRRho-CLIP](https://github.com/Optimization-AI/DRRho-CLIP)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=teUg2pMrF0": {
    "title": "Large Language Model-driven Large Neighborhood Search for Large-Scale MILP Problems",
    "volume": "spotlight",
    "abstract": "Large Neighborhood Search (LNS) is a widely used method for solving large-scale Mixed Integer Linear Programming (MILP) problems. The effectiveness of LNS crucially depends on the choice of the search neighborhood. However, existing strategies either rely on expert knowledge or computationally expensive Machine Learning (ML) approaches, both of which struggle to scale effectively for large problems. To address this, we propose LLM-LNS, a novel Large Language Model (LLM)-driven LNS framework for large-scale MILP problems. Our approach introduces a dual-layer self-evolutionary LLM agent to automate neighborhood selection, discovering effective strategies with scant small-scale training data that generalize well to large-scale MILPs. The inner layer evolves heuristic strategies to ensure convergence, while the outer layer evolves evolutionary prompt strategies to maintain diversity. Experimental results demonstrate that the proposed dual-layer agent outperforms state-of-the-art agents such as FunSearch and EOH. Furthermore, the full LLM-LNS framework surpasses manually designed LNS algorithms like ACP, ML-based LNS methods like CL-LNS, and large-scale solvers such as Gurobi and SCIP. It also achieves superior performance compared to advanced ML-based MILP optimization frameworks like GNN&GBDT and Light-MILPopt, further validating the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Go0DdhjATH": {
    "title": "Policy Regularization on Globally Accessible States in Cross-Dynamics Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "To learn from data collected in diverse dynamics, Imitation from Observation (IfO) methods leverage expert state trajectories based on the premise that recovering expert state distributions in other dynamics facilitates policy learning in the current one. However, Imitation Learning inherently imposes a performance upper bound of learned policies. Additionally, as the environment dynamics change, certain expert states may become inaccessible, rendering their distributions less valuable for imitation. To address this, we propose a novel framework that integrates reward maximization with IfO, employing F-distance regularized policy optimization. This framework enforces constraints on globally accessible states—those with nonzero visitation frequency across all considered dynamics—mitigating the challenge posed by inaccessible states. By instantiating F-distance in different ways, we derive two theoretical analysis and develop a practical algorithm called Accessible State Oriented Policy Regularization (ASOR). ASOR serves as a general-purpose module that can be incorporated into various RL approaches, including offline RL and off-policy RL. Extensive experiments across multiple benchmarks demonstrate ASOR's effectiveness in enhancing state-of-the-art cross-domain policy transfer algorithms, significantly improving their performance",
    "checked": true,
    "id": "ee825ce412ec7969a6e59923060cbe8d61723f48",
    "semantic_title": "policy regularization on globally accessible states in cross-dynamics reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zIGIvysR1H": {
    "title": "Data-Juicer Sandbox: A Feedback-Driven Suite for Multimodal Data-Model Co-development",
    "volume": "spotlight",
    "abstract": "The emergence of multimodal large models has advanced artificial intelligence, introducing unprecedented levels of performance and functionality. However, optimizing these models remains challenging due to historically isolated paths of model-centric and data-centric developments, leading to suboptimal outcomes and inefficient resource utilization. In response, we present a new sandbox suite tailored for integrated data-model co-development. This sandbox provides a feedback-driven experimental platform, enabling cost-effective iteration and guided refinement of both data and models. Our proposed ``Probe-Analyze-Refine'' workflow, validated through practical use cases on multimodal tasks such as image-text pre-training with CLIP, image-to-text generation with LLaVA-like models, and text-to-video generation with DiT-based models, yields transferable and notable performance boosts, such as topping the VBench leaderboard. A comprehensive set of over 100 experiments demonstrated the suite's usability and extensibility, while also uncovering insights into the interplay between data quality, diversity, model behavior, and computational costs. All codes, datasets, and models are open-sourced to foster future research and applications that would otherwise be infeasible due to the lack of a dedicated co-development infrastructure",
    "checked": true,
    "id": "2bb3473b4dee613d2ed2c6819f13a8a5fd40981d",
    "semantic_title": "data-juicer sandbox: a feedback-driven suite for multimodal data-model co-development",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=tNGdLEL4R0": {
    "title": "Scaling Trends in Language Model Robustness",
    "volume": "spotlight",
    "abstract": "Increasing model size has unlocked a dazzling array of capabilities in language models. At the same time, even frontier models remain vulnerable to jailbreaks and prompt injections, despite concerted efforts to make them robust. As both attackers and defenders gain access to more compute, and as models become larger, what will be the effect on robustness? We argue that to answer this question requires a *scaling lens*, which we adopt in an extensive study of language model robustness across several classification tasks, model families, and adversarial attacks. We find that in the absence of explicit safety training, larger models are not consistently more robust; however, scale improves sample efficiency in adversarial training, though it worsens compute efficiency. Further, we find that increasing attack compute smoothly improves attack success rate against both undefended and adversarially trained models. Finally, after exploring robustness transfer across attacks and threat models, we combine attack and defense scaling rates to study the offense-defense balance. We find that while attack scaling outpaces adversarial training across all models studied, larger adversarially trained models might give defense the advantage in the long run. These results underscore the utility of the scaling lens, and provide a paradigm for evaluating future attacks and defenses on frontier models. Code for this project is available at https://github.com/AlignmentResearch/scaling-llm-robustness-paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YucuAuXMpT": {
    "title": "Not all solutions are created equal: An analytical dissociation of functional and representational similarity in deep linear neural networks",
    "volume": "spotlight",
    "abstract": "A foundational principle of connectionism is that perception, action, and cognition emerge from parallel computations among simple, interconnected units that generate and rely on neural representations. Accordingly, researchers employ multivariate pattern analysis to decode and compare the neural codes of artificial and biological networks, aiming to uncover their functions. However, there is limited analytical understanding of how a network's representation and function relate, despite this being essential to any quantitative notion of underlying function or functional similarity. We address this question using fully analysable two-layer linear networks and numerical simulations in nonlinear networks. We find that function and representation are dissociated, allowing representational similarity without functional similarity and vice versa. Further, we show that neither robustness to input noise nor the level of generalisation error constrain representations to the task. In contrast, networks robust to parameter noise have limited representational flexibility and must employ task-specific representations. Our findings suggest that representational alignment reflects computational advantages beyond functional alignment alone, with significant implications for interpreting and comparing the representations of connectionist systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hrp6jRIKdX": {
    "title": "Towards a Mechanistic Explanation of Diffusion Model Generalization",
    "volume": "spotlight",
    "abstract": "We propose a simple, training-free mechanism which explains the generalization behaviour of diffusion models. By comparing pre-trained diffusion models to their theoretically optimal empirical counterparts, we identify a shared local inductive bias across a variety of network architectures. From this observation, we hypothesize that network denoisers generalize through localized denoising operations, as these operations approximate the training objective well over much of the training distribution. To validate our hypothesis, we introduce novel denoising algorithms which aggregate local empirical denoisers to replicate network behaviour. Comparing these algorithms to network denoisers across forward and reverse diffusion processes, our approach exhibits consistent visual similarity to neural network outputs, with lower mean squared error than previously proposed methods",
    "checked": true,
    "id": "ab80f81defa62fdbae22bf996f76cca04c3915c1",
    "semantic_title": "towards a mechanistic explanation of diffusion model generalization",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=2dz6psiiA0": {
    "title": "Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner",
    "volume": "spotlight",
    "abstract": "Theory-of-mind (ToM) enables humans to infer mental states—such as beliefs, desires, and intentions—forming the foundation of social cognition. Existing computational ToM methods rely on structured workflows with ToM-specific priors or deep model fine-tuning but struggle with scalability in multimodal environments. They remain trapped within the gravitational pull of multi-step planning complexity, failing to generalize as task demands increase. To overcome these limitations, we propose a scalable Bayesian ToM planner. It breaks down ToM complexity into stepwise Bayesian updates. Meanwhile, weak-to-strong control specializes smaller LMs to refine ToM-specific likelihood estimation, transferring their ToM reasoning behavior to larger LMs (7B to 405B) for social and world knowledge integration. This synergistic approach enables scalability, aligning large-model inference with human mental states with Bayesian principles. Extensive experiments demonstrate a 4.6% improvement in accuracy over state-of-the-art methods on multimodal ToM benchmarks, including unseen scenarios, establishing a new standard for modeling human mental states in complex environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vhACnRfuYh": {
    "title": "Latent Diffusion Planning for Imitation Learning",
    "volume": "spotlight",
    "abstract": "Recent progress in imitation learning has been enabled by policy architectures that scale to complex visuomotor tasks, multimodal distributions, and large datasets. However, these methods often rely on learning from large amount of expert demonstrations. To address these shortcomings, we propose Latent Diffusion Planning (LDP), a modular approach consisting of a planner which can leverage action-free demonstrations, and an inverse dynamics model which can leverage suboptimal data, that both operate over a learned latent space. First, we learn a compact latent space through a variational autoencoder, enabling effective forecasting of future states in image-based domains. Then, we train a planner and an inverse dynamics model with diffusion objectives. By separating planning from action prediction, LDP can benefit from the denser supervision signals of suboptimal and action-free data. On simulated visual robotic manipulation tasks, LDP outperforms state-of-the-art imitation learning approaches, as they cannot leverage such additional data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aQUUUAcAw1": {
    "title": "Sparse-pivot: Dynamic correlation clustering for node insertions",
    "volume": "spotlight",
    "abstract": "We present a new Correlation Clustering algorithm for a dynamic setting where nodes are added one at a time. In this model, proposed by Cohen-Addad, Lattanzi, Maggiori, and Parotsidis (ICML 2024), the algorithm uses database queries to access the input graph and updates the clustering as each new node is added. Our algorithm has the amortized update time of $\\log^{O(1)}(n)$. Its approximation factor is $20+\\varepsilon$, which is a substantial improvement over the approximation factor of the algorithm by Cohen-Addad et al. We complement our theoretical findings by empirically evaluating the approximation guarantee of our algorithm. The results show that it outperforms the algorithm by Cohen-Addad et al.~in practice",
    "checked": true,
    "id": "23582bf7424915794937f437bd7789b04d4b31d7",
    "semantic_title": "sparse-pivot: dynamic correlation clustering for node insertions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hRQyqtcjVv": {
    "title": "The Jailbreak Tax: How Useful are Your Jailbreak Outputs?",
    "volume": "spotlight",
    "abstract": "Jailbreak attacks bypass the guardrails of large language models to produce harmful outputs. In this paper, we ask whether the model outputs produced by existing jailbreaks are actually *useful*. For example, when jailbreaking a model to give instructions for building a bomb, does the jailbreak yield good instructions? Since the utility of most unsafe answers (e.g., bomb instructions) is hard to evaluate rigorously, we build new jailbreak evaluation sets with known ground truth answers, by aligning models to refuse questions related to benign and easy-to-evaluate topics (e.g., biology or math). Our evaluation of eight representative jailbreaks across five utility benchmarks reveals a consistent drop in model utility in jailbroken responses, which we term the *jailbreak tax*. For example, while all jailbreaks we tested bypass guardrails in models aligned to refuse to answer math, this comes at the expense of a drop of up to 92% in accuracy. Overall, our work proposes jailbreak utility as a new important metric in AI safety, and introduces benchmarks to evaluate existing and future jailbreaks. We make the benchmark available at https://github.com/ethz-spylab/jailbreak-tax",
    "checked": true,
    "id": "c494eb79cec655bcaa900eabaa62ccc94300facb",
    "semantic_title": "the jailbreak tax: how useful are your jailbreak outputs?",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=kmg7hweySi": {
    "title": "Feature learning from non-Gaussian inputs: the case of Independent Component Analysis in high dimensions",
    "volume": "spotlight",
    "abstract": "Deep neural networks learn structured features from complex, non-Gaussian inputs, but the mechanisms behind this process remain poorly understood. Our work is motivated by the observation that the first-layer filters learnt by deep convolutional neural networks from natural images resemble those learnt by independent component analysis (ICA), a simple unsupervised method that seeks the most non-Gaussian projections of its inputs. This similarity suggests that ICA provides a simple, yet principled model for studying feature learning. Here, we leverage this connection to investigate the interplay between data structure and optimisation in feature learning for the most popular ICA algorithm, FastICA, and stochastic gradient descent (SGD), which is used to train deep networks. We rigorously establish that FastICA requires at least $n\\gtrsim d^4$ samples to recover a single non-Gaussian direction from $d$-dimensional inputs on a simple synthetic data model. We show that vanilla online SGD outperforms FastICA, and prove that the optimal sample complexity $n\\gtrsim d^2$ can be reached by smoothing the loss, albeit in a data-dependent way. We finally demonstrate the existence of a search phase for FastICA on ImageNet, and discuss how the strong non-Gaussianity of said images compensates for the poor sample complexity of FastICA",
    "checked": true,
    "id": "f94357b161c9d44e8340d6a68f451fcc8b332d06",
    "semantic_title": "feature learning from non-gaussian inputs: the case of independent component analysis in high dimensions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iBpkzB5LEr": {
    "title": "Primal-Dual Neural Algorithmic Reasoning",
    "volume": "spotlight",
    "abstract": "Neural Algorithmic Reasoning (NAR) trains neural networks to simulate classical algorithms, enabling structured and interpretable reasoning over complex data. While prior research has predominantly focused on learning exact algorithms for polynomial-time-solvable problems, extending NAR to harder problems remains an open challenge. In this work, we introduce a general NAR framework grounded in the primal-dual paradigm, a classical method for designing efficient approximation algorithms. By leveraging a bipartite representation between primal and dual variables, we establish an alignment between primal-dual algorithms and Graph Neural Networks. Furthermore, we incorporate optimal solutions from small instances to greatly enhance the model's reasoning capabilities. Our empirical results demonstrate that our model not only simulates but also outperforms approximation algorithms for multiple tasks, exhibiting robust generalization to larger and out-of-distribution graphs. Moreover, we highlight the framework's practical utility by integrating it with commercial solvers and applying it to real-world datasets",
    "checked": true,
    "id": "c4f0fe86c4b309bbfb4f442b0e015fcb06728fd7",
    "semantic_title": "primal-dual neural algorithmic reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m3zrHhiCCj": {
    "title": "Fishers for Free? Approximating the Fisher Information Matrix by Recycling the Squared Gradient Accumulator",
    "volume": "spotlight",
    "abstract": "The diagonal of a model's Fisher Information Matrix (the \"Fisher\") has frequently been used as a way to measure parameter sensitivity. Typically, the Fisher is estimated by computing the squared gradient of the model's outputs with respect to its parameters, averaged over a few hundred or thousand examples — a process which incurs nontrivial computational costs. At the same time, adaptive gradient methods like the ubiquitous Adam optimizer compute a moving average of the squared gradient over the course of training. This paper therefore explores whether an approximation of the Fisher can be obtained \"for free\" by recycling the squared gradient accumulator that has already been computed over the course of training. Through a comprehensive set of experiments covering five applications of the Fisher, we demonstrate that the \"Squisher\" (**Squ**ared gradient accumulator as an approximation of the F**isher**) consistently performs similarly to the Fisher while outperforming baseline methods. Additionally, we clarify the exact differences between the Squisher and the Fisher and provide empirical quantification of their respective impact",
    "checked": true,
    "id": "19e16914bc2a03cac858121ea417af974cb672d6",
    "semantic_title": "fishers for free? approximating the fisher information matrix by recycling the squared gradient accumulator",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gKdjHLrHDS": {
    "title": "Feature Learning beyond the Lazy-Rich Dichotomy: Insights from Representational Geometry",
    "volume": "spotlight",
    "abstract": "Integrating task-relevant information into neural representations is a fundamental ability of both biological and artificial intelligence systems. Recent theories have categorized learning into two regimes: the rich regime, where neural networks actively learn task-relevant features, and the lazy regime, where networks behave like random feature models. Yet this simple lazy–rich dichotomy overlooks a diverse underlying taxonomy of feature learning, shaped by differences in learning algorithms, network architectures, and data properties. To address this gap, we introduce an analysis framework to study feature learning via the geometry of neural representations. Rather than inspecting individual learned features, we characterize how task-relevant representational manifolds evolve throughout the learning process. We show, in both theoretical and empirical settings, that as networks learn features, task-relevant manifolds untangle, with changes in manifold geometry revealing distinct learning stages and strategies beyond the lazy–rich dichotomy. This framework provides novel insights into feature learning across neuroscience and machine learning, shedding light on structural inductive biases in neural circuits and the mechanisms underlying out-of-distribution generalization",
    "checked": true,
    "id": "ca798e24a2f7447758f23e0e5f7db14dedd31287",
    "semantic_title": "feature learning beyond the lazy-rich dichotomy: insights from representational geometry",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sDK6bSmHgM": {
    "title": "FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation Vector Flow Fields",
    "volume": "spotlight",
    "abstract": "Drag-based editing allows precise object manipulation through point-based control, offering user convenience. However, current methods often suffer from a geometric inconsistency problem by focusing exclusively on matching user-defined points, neglecting the broader geometry and leading to artifacts or unstable edits. We propose FlowDrag, which leverages geometric information for more accurate and coherent transformations. Our approach constructs a 3D mesh from the image, using an energy function to guide mesh deformation based on user-defined drag points. The resulting mesh displacements are projected into 2D and incorporated into a UNet denoising process, enabling precise handle-to-target point alignment while preserving structural integrity. Additionally, existing drag-editing benchmarks provide no ground truth, making it difficult to assess how accurately the edits match the intended transformations. To address this, we present VFD (VidFrameDrag) benchmark dataset, which provides ground-truth frames using consecutive shots in a video dataset. FlowDrag outperforms existing drag-based editing methods on both VFD Bench and DragBench",
    "checked": true,
    "id": "152ef1b4200a6b8f5557b0c2c0929078c1fb3916",
    "semantic_title": "flowdrag: 3d-aware drag-based image editing with mesh-guided deformation vector flow fields",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jEcQP3lGlq": {
    "title": "Elucidating the Design Space of Multimodal Protein Language Models",
    "volume": "spotlight",
    "abstract": "Multimodal protein language models (PLMs) integrate sequence and token-based structural information, serving as a powerful foundation for protein modeling, generation, and design. However, the reliance on tokenizing 3D structures into discrete tokens causes substantial loss of fidelity about fine-grained structural details and correlations. In this paper, we systematically elucidate the design space of multimodal PLMs to overcome their limitations. We identify tokenization loss and inaccurate structure token predictions by the PLMs as major bottlenecks. To address these, our proposed design space covers improved generative modeling, structure-aware architectures and representation learning, and data exploration. Our advancements approach finer-grained supervision, demonstrating that token-based multimodal PLMs can achieve robust structural modeling. The effective design methods dramatically improve the structure generation diversity, and notably, folding abilities of our 650M model by reducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B baselines and on par with the specialized folding models. Project page and code: https://bytedance.github.io/dplm/dplm-2.1",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xtlixzbcfV": {
    "title": "Novelty Detection in Reinforcement Learning with World Models",
    "volume": "spotlight",
    "abstract": "Reinforcement learning (RL) using world models has found significant recent successes. However, when a sudden change to world mechanics or properties occurs then agent performance and reliability can dramatically decline. We refer to the sudden change in visual properties or state transitions as novelties. Implementing novelty detection within generated world model frameworks is a crucial task for protecting the agent when deployed. In this paper, we propose straightforward bounding approaches to incorporate novelty detection into world model RL agents by utilizing the misalignment of the world model's hallucinated states and the true observed states as a novelty score. We provide effective approaches to detecting novelties in a distribution of transitions learned by an agent in a world model. Finally, we show the advantage of our work in a novel environment compared to traditional machine learning novelty detection methods as well as currently accepted RL-focused novelty detection algorithms",
    "checked": true,
    "id": "c6a00941281e5264a4ea2e2c7e022b6725cb9f35",
    "semantic_title": "novelty detection in reinforcement learning with world models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=e46xNZhwl8": {
    "title": "Learning with Exact Invariances in Polynomial Time",
    "volume": "spotlight",
    "abstract": "We study the statistical-computational trade-offs for learning with exact invariances (or symmetries) using kernel regression. Traditional methods, such as data augmentation, group averaging, canonicalization, and frame-averaging, either fail to provide a polynomial-time solution or are not applicable in the kernel setting. However, with oracle access to the geometric properties of the input space, we propose a polynomial-time algorithm that learns a classifier with \\emph{exact} invariances. Moreover, our approach achieves the same excess population risk (or generalization error) as the original kernel regression problem. To the best of our knowledge, this is the first polynomial-time algorithm to achieve exact (as opposed to approximate) invariances in this setting, partially addressing a question posed by Diaz (2025) regarding the avoidance of prohibitively large and computationally intensive group averaging methods in kernel regression with exact invariances. Our proof leverages tools from differential geometry, spectral theory, and optimization. A key result in our development is a new reformulation of the problem of learning under invariances as optimizing an infinite number of linearly constrained convex quadratic programs, which may be of independent interest",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8JGwoZceQs": {
    "title": "Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs",
    "volume": "spotlight",
    "abstract": "We investigate the design of pooling methods used to summarize the outputs of transformer embedding models, primarily motivated by reinforcement learning and vision applications. This work considers problems where a subset of the input vectors contains requisite information for a downstream task (signal) while the rest are distractors (noise). By framing pooling as vector quantization with the goal of minimizing signal loss, we demonstrate that the standard methods used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs fluctuates. We then show that an attention-based *adaptive pooling* method can approximate the signal-optimal vector quantizer within derived error bounds for any SNR. Our theoretical results are first validated by supervised experiments on a synthetic dataset designed to isolate the SNR problem, then generalized to standard relational reasoning, multi-agent reinforcement learning, and vision benchmarks with noisy observations, where transformers with adaptive pooling display superior robustness across tasks",
    "checked": true,
    "id": "9c9350e9e4c2d2c02b67c09d1db8f06aeeee9ed1",
    "semantic_title": "robust noise attenuation via adaptive pooling of transformer outputs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c16m2kUTLZ": {
    "title": "No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks",
    "volume": "spotlight",
    "abstract": "The ultimate goal of verification is to guarantee the safety of deployed neural networks. Here, we claim that all the state-of-the-art verifiers we are aware of fail to reach this goal. Our key insight is that theoretical soundness (bounding the full-precision output while computing with floating point) does not imply practical soundness (bounding the floating point output in a potentially stochastic environment). We prove this observation for the approaches that are currently used to achieve provable theoretical soundness, such as interval analysis and its variants. We also argue that achieving practical soundness is significantly harder computationally. We support our claims empirically as well by evaluating several well-known verification methods. To mislead the verifiers, we create adversarial networks that detect and exploit features of the deployment environment, such as the order and precision of floating point operations. We demonstrate that all the tested verifiers are vulnerable to our new deployment-specific attacks, which proves that they are not practically sound",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ysC6VS0y3": {
    "title": "Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective",
    "volume": "spotlight",
    "abstract": "Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., \"Finding the first noun in a sentence.\") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Z827FtMNe": {
    "title": "Great Models Think Alike and this Undermines AI Oversight",
    "volume": "spotlight",
    "abstract": "As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as *AI Oversight*. We study how model similarity affects both aspects of AI oversight by proposing *Chance Adjusted Probabilistic Agreement (CAPA)*--a metric for LM similarity based on overlap in model mistakes. Using CAPA, we first show that *LLM-as-a-judge* scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from *weak-to-strong generalization*. As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend--model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight",
    "checked": true,
    "id": "7ad8a0a5e9c758731f16e75d926739f3f8fa6f59",
    "semantic_title": "great models think alike and this undermines ai oversight",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=5hZCK4Wbex": {
    "title": "Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition",
    "volume": "spotlight",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. In this study, we explore a surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during a single inference call, a capability we term task superposition\". We provide empirical evidence of this phenomenon across various LLM families and scales and show that this phenomenon emerges even if we train the model to in-context learn one task at a time. We offer theoretical explanations that this capability is well within the expressive power of transformers. We also explore how LLMs internally compose task vectors during superposition. Furthermore, we show that larger models can solve more ICL tasks in parallel, and better calibrate their output distribution. Our findings offer insights into the latent capabilities of LLMs, further substantiate the perspective of \"LLMs as superposition of simulators\", and raise questions about the mechanisms enabling simultaneous task execution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hS2Ed5XYRq": {
    "title": "MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models",
    "volume": "spotlight",
    "abstract": "Recent advancements in foundation models have improved autonomous tool usage and reasoning, but their capabilities in map-based reasoning remain underexplored. To address this, we introduce MapEval, a benchmark designed to assess foundation models across three distinct tasks—textual, API-based, and visual reasoning—through 700 multiple-choice questions spanning 180 cities and 54 countries, covering spatial relationships, navigation, travel planning, and real-world map interactions. Unlike prior benchmarks that focus on simple location queries, MapEval requires models to handle long-context reasoning, API interactions and visual map analysis, making it the most comprehensive evaluation framework for geospatial AI. On evaluation of 30 foundation models, including Claude-3.5-Sonnet, GPT-4o, Gemini-1.5-Pro, none surpasses 67% accuracy, with open-source models performing significantly worse and all models lagging over 20% behind human performance. These results expose critical gaps in spatial inference, as models struggle with distances, directions, route planning, and place-specific reasoning, highlighting the need for better geospatial AI to bridge the gap between foundation models and real-world navigation",
    "checked": true,
    "id": "de22ecf940f3bb18ebc85e523350c0c4d8d4c5c2",
    "semantic_title": "mapeval: a map-based evaluation of geo-spatial reasoning in foundation models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=AsODat0dkE": {
    "title": "Optimizing Adaptive Attacks against Watermarks for Language Models",
    "volume": "spotlight",
    "abstract": "Large Language Models (LLMs) can be misused to spread unwanted content at scale. Content watermarking deters misuse by hiding messages in content, enabling its detection using a secret *watermarking key*. Robustness is a core security property, stating that evading detection requires (significant) degradation of the content's quality. Many LLM watermarking methods have been proposed, but robustness is tested only against *non-adaptive* attackers who lack knowledge of the watermarking method and can find only suboptimal attacks. We formulate watermark robustness as an objective function and use preference-based optimization to tune *adaptive* attacks against the specific watermarking method. Our evaluation shows that (i) adaptive attacks evade detection against all surveyed watermarks, (ii) training against *any* watermark succeeds in evading unseen watermarks, and (iii) optimization-based attacks are cost-effective. Our findings underscore the need to test robustness against adaptively tuned attacks. We release our adaptively tuned paraphrasers at <https://github.com/nilslukas/ada-wm-evasion>",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AiaVCVDuxF": {
    "title": "Robust ML Auditing using Prior Knowledge",
    "volume": "spotlight",
    "abstract": "Among the many technical challenges to enforcing AI regulations, one crucial yet underexplored problem is the risk of audit manipulation. This manipulation occurs when a platform deliberately alters its answers to a regulator to pass an audit without modifying its answers to other users. In this paper, we introduce a novel approach to manipulation-proof auditing by taking into account the auditor's prior knowledge of the task solved by the platform. We first demonstrate that regulators must not rely on public priors (e.g. a public dataset), as platforms could easily fool the auditor in such cases. We then formally establish the conditions under which an auditor can prevent audit manipulations using prior knowledge about the ground truth. Finally, our experiments with two standard datasets illustrate the maximum level of unfairness a platform can hide before being detected as malicious. Our formalization and generalization of manipulation-proof auditing with a prior opens up new research directions for more robust fairness audits",
    "checked": true,
    "id": "e82bd590e041447fcdf3754ca0f16ae756730c87",
    "semantic_title": "robust ml auditing using prior knowledge",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=HxCuvx2uUi": {
    "title": "Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning",
    "volume": "spotlight",
    "abstract": "Off-policy learning and evaluation leverage logged bandit feedback datasets, which contain context, action, propensity score, and feedback for each data point. These scenarios face significant challenges due to high variance and poor performance with low-quality propensity scores and heavy-tailed reward distributions. We address these issues by introducing a novel estimator based on the log-sum-exponential (LSE) operator, which outperforms traditional inverse propensity score estimators. Our LSE estimator demonstrates variance reduction and robustness under heavy-tailed conditions. For off-policy evaluation, we derive upper bounds on the estimator's bias and variance. In the off-policy learning scenario, we establish bounds on the regret—the performance gap between our LSE estimator and the optimal policy—assuming bounded $(1+\\epsilon)$-th moment of weighted reward. Notably, we achieve a convergence rate of $O(n^{-\\epsilon/(1+\\epsilon)})$ for the regret bounds, where $\\epsilon\\in[0,1]$ and $n$ is the size of logged bandit feedback dataset. Theoretical analysis is complemented by comprehensive empirical evaluations in both off-policy learning and evaluation scenarios, confirming the practical advantages of our approach. The code for our estimator is available at the following link: https://github.com/armin-behnamnia/lse-offpolicy-learning",
    "checked": true,
    "id": "5dda3c8dd9801246dc70ea34171253bbc2b46b67",
    "semantic_title": "log-sum-exponential estimator for off-policy evaluation and learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=JxnOZwFNcU": {
    "title": "Adaptive Learn-then-Test: Statistically Valid and Efficient Hyperparameter Selection",
    "volume": "spotlight",
    "abstract": "We introduce adaptive learn-then-test (aLTT), an efficient hyperparameter selection procedure that provides finite-sample statistical guarantees on the population risk of AI models. Unlike the existing learn-then-test (LTT) technique, which relies on conventional p-value-based multiple hypothesis testing (MHT), aLTT implements sequential data-dependent MHT with early termination by leveraging e-processes. As a result, aLTT can reduce the number of testing rounds, making it particularly well-suited for scenarios in which testing is costly or presents safety risks. Apart from maintaining statistical validity, in applications such as online policy selection for offline reinforcement learning and prompt engineering, aLTT is shown to achieve the same performance as LTT while requiring only a fraction of the testing rounds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oa7MYAO6h6": {
    "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference",
    "volume": "spotlight",
    "abstract": "With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for decoding both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to accelerate inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory usage or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on benchmarks like RULER, LongBench, and models such as Llama-3.1-8B and GLM-4-9B-1M, we demonstrate that it achieves up to 6$\\times$ larger batch sizes and 3.04$\\times$ higher throughput on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory",
    "checked": true,
    "id": "7cfceb098d86ac397761988d4557b241115e95d3",
    "semantic_title": "shadowkv: kv cache in shadows for high-throughput long-context llm inference",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=GbJqQsIwJu": {
    "title": "Learning Parametric Distributions from Samples and Preferences",
    "volume": "spotlight",
    "abstract": "Recent advances in language modeling have underscored the role of preference feedback in enhancing model performance. This paper investigates the conditions under which preference feedback improves parameter estimation in classes of continuous parametric distributions. In our framework, the learner observes pairs of samples from an unknown distribution along with their relative preferences depending on the same unknown parameter. We show that preferences-based M-estimators achieve a better asymptotic variance than sample-only M-estimators, further improved by deterministic preferences. Leveraging the hard constraints revealed by deterministic preferences, we propose an estimator achieving an estimation error scaling of $\\mathcal{O}(1/n)$---a significant improvement over the $\\Theta(1/\\sqrt{n})$ rate attainable with samples alone. Next, we establish a lower bound that matches this accelerated rate; up to problem-dependent constants. While the assumptions underpinning our analysis are restrictive, they are satisfied by notable cases such as Gaussian or Laplace distributions for preferences based on the log-probability reward",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ps3aO9MHJv": {
    "title": "Prediction models that learn to avoid missing values",
    "volume": "spotlight",
    "abstract": "Handling missing values at test time is challenging for machine learning models, especially when aiming for both high accuracy and interpretability. Established approaches often add bias through imputation or excessive model complexity via missingness indicators. Moreover, either method can obscure interpretability, making it harder to understand how the model utilizes the observed variables in predictions. We propose *missingness-avoiding* (MA) machine learning, a general framework for training models to rarely require the values of missing (or imputed) features at test time. We create tailored MA learning algorithms for decision trees, tree ensembles, and sparse linear models by incorporating classifier-specific regularization terms in their learning objectives. The tree-based models leverage contextual missingness by reducing reliance on missing values based on the observed context. Experiments on real-world datasets demonstrate that **MA-DT, MA-LASSO, MA-RF**, and **MA-GBT** effectively reduce the reliance on features with missing values while maintaining predictive performance competitive with their unregularized counterparts. This shows that our framework gives practitioners a powerful tool to maintain interpretability in predictions with test-time missing values",
    "checked": true,
    "id": "c832f232cc108915480835922d11187631dbbd15",
    "semantic_title": "prediction models that learn to avoid missing values",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4vAa0A98xI": {
    "title": "CoPINN: Cognitive Physics-Informed Neural Networks",
    "volume": "spotlight",
    "abstract": "Physics-informed neural networks (PINNs) aim to constrain the outputs and gradients of deep learning models to satisfy specified governing physics equations, which have demonstrated significant potential for solving partial differential equations (PDEs). Although existing PINN methods have achieved pleasing performance, they always treat both easy and hard sample points indiscriminately, especially ones in the physical boundaries. This easily causes the PINN model to fall into undesirable local minima and unstable learning, thereby resulting in an Unbalanced Prediction Problem (UPP). To deal with this daunting problem, we propose a novel framework named Cognitive Physical Informed Neural Network (CoPINN) that imitates the human cognitive learning manner from easy to hard. Specifically, we first employ separable subnetworks to encode independent one-dimensional coordinates and apply an aggregation scheme to generate multi-dimensional predicted physical variables. Then, during the training phase, we dynamically evaluate the difficulty of each sample according to the gradient of the PDE residuals. Finally, we propose a cognitive training scheduler to progressively optimize the entire sampling regions from easy to hard, thereby embracing robustness and generalization against predicting physical boundary regions. Extensive experiments demonstrate that our CoPINN achieves state-of-the-art performance, particularly significantly reducing prediction errors in stubborn regions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5EbiopWH6e": {
    "title": "Implicit Language Models are RNNs: Balancing Parallelization and Expressivity",
    "volume": "spotlight",
    "abstract": "State-space models (SSMs) and transformers dominate the language modeling landscape. However, they are constrained to a lower computational complexity than classical recurrent neural networks (RNNs), limiting their expressivity. In contrast, RNNs lack parallelization during training, raising fundamental questions about the trade off between parallelization and expressivity. We propose implicit SSMs, which iterate a transformation until convergence to a fixed point. Theoretically, we show that implicit SSMs implement the non-linear state-transitions of RNNs. Empirically, we find that only approximate fixed-point convergence suffices, enabling the design of a scalable training curriculum that largely retains parallelization, with full convergence required only for a small subset of tokens. Our approach demonstrates superior state-tracking capabilities on regular languages, surpassing transformers and SSMs. We further scale implicit SSMs to natural language reasoning tasks and pretraining of large-scale language models up to 1.3B parameters on 207B tokens - representing, to our knowledge, the largest implicit model trained to date. Notably, our implicit models outperform their explicit counterparts on standard benchmarks. Our code is publicly available at github.com/microsoft/implicit_languagemodels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UKR3HsAFkC": {
    "title": "Achieving Linear Speedup and Near-Optimal Complexity for Decentralized Optimization over Row-stochastic Networks",
    "volume": "spotlight",
    "abstract": "A key challenge in decentralized optimization is determining the optimal convergence rate and designing algorithms to achieve it. While this problem has been extensively addressed for doubly-stochastic and column-stochastic mixing matrices, the row-stochastic scenario remains unexplored. This paper bridges this gap by introducing effective metrics to capture the influence of row-stochastic mixing matrices and establishing the first convergence lower bound for decentralized learning over row-stochastic networks. However, existing algorithms fail to attain this lower bound due to two key issues: deviation in the descent direction caused by the adapted gradient tracking (GT) and instability introduced by the Pull-Diag protocol. To address descent deviation, we propose a novel analysis framework demonstrating that Pull-Diag-GT achieves linear speedup—the first such result for row-stochastic decentralized optimization. Moreover, by incorporating a multi-step gossip (MG) protocol, we resolve the instability issue and attain the lower bound, achieving near-optimal complexity for decentralized optimization over row-stochastic networks",
    "checked": true,
    "id": "d47e4271d38e178a19f4511af6c7c2fd944db697",
    "semantic_title": "achieving linear speedup and near-optimal complexity for decentralized optimization over row-stochastic networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Gp7NfP7Erm": {
    "title": "Towards Robustness and Explainability of Automatic Algorithm Selection",
    "volume": "spotlight",
    "abstract": "Algorithm selection aims to identify the optimal performing algorithm before execution. Existing techniques typically focus on the observed correlations between algorithm performance and meta-features. However, little research has explored the underlying mechanisms of algorithm selection, specifically what characteristics an algorithm must possess to effectively tackle problems with certain feature values. This gap not only limits the explainability but also makes existing models vulnerable to data bias and distribution shift. This paper introduces directed acyclic graph (DAG) to describe this mechanism, proposing a novel modeling paradigm that aligns more closely with the fundamental logic of algorithm selection. By leveraging DAG to characterize the algorithm feature distribution conditioned on problem features, our approach enhances robustness against marginal distribution changes and allows for finer-grained predictions through the reconstruction of optimal algorithm features, with the final decision relying on differences between reconstructed and rejected algorithm features. Furthermore, we demonstrate that, the learned DAG and the proposed counterfactual calculations offer our approach with both model-level and instance-level explainability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B9DOjtj9xK": {
    "title": "Learning Soft Sparse Shapes for Efficient Time-Series Classification",
    "volume": "spotlight",
    "abstract": "Shapelets are discriminative subsequences (or shapes) with high interpretability in time series classification. Due to the time-intensive nature of shapelet discovery, existing shapelet-based methods mainly focus on selecting discriminative shapes while discarding others to achieve candidate subsequence sparsification. However, this approach may exclude beneficial shapes and overlook the varying contributions of shapelets to classification performance. To this end, we propose a Soft sparse Shapes (SoftShape) model for efficient time series classification. Our approach mainly introduces soft shape sparsification and soft shape learning blocks. The former transforms shapes into soft representations based on classification contribution scores, merging lower-scored ones into a single shape to retain and differentiate all subsequence information. The latter facilitates intra- and inter-shape temporal pattern learning, improving model efficiency by using sparsified soft shapes as inputs. Specifically, we employ a learnable router to activate a subset of class-specific expert networks for intra-shape pattern learning. Meanwhile, a shared expert network learns inter-shape patterns by converting sparsified shapes into sequences. Extensive experiments show that SoftShape outperforms state-of-the-art methods and produces interpretable results",
    "checked": true,
    "id": "c28bb86232af2bb349ae4bd1f8e21763da43d3ba",
    "semantic_title": "learning soft sparse shapes for efficient time-series classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d60cmFf89H": {
    "title": "TabFlex: Scaling Tabular Learning to Millions with Linear Attention",
    "volume": "spotlight",
    "abstract": "Leveraging the in-context learning (ICL) capability of Large Language Models (LLMs) for tabular classification has gained significant attention for its training-free adaptability across diverse datasets. Recent advancements, like TabPFN, excel in small-scale tabular datasets but struggle to scale for large and complex datasets. Our work enhances the efficiency and scalability of TabPFN for larger datasets by incorporating linear attention mechanisms as a scalable alternative to complexity-quadratic self-attention. Our model, TabFlex, efficiently handles tabular datasets with thousands of features and hundreds of classes, scaling seamlessly to millions of samples. For instance, TabFlex processes the poker-hand dataset with over a million samples in just 5 seconds. Our extensive evaluations demonstrate that TabFlex can achieve over a 2× speedup compared to TabPFN and a 1.5× speedup over XGBoost, outperforming 25 tested baselines in terms of efficiency across a diverse range of datasets. Furthermore, TabFlex remains highly effective on large-scale datasets, delivering strong performance with significantly reduced computational costs, especially when combined with data-efficient techniques such as dimensionality reduction and data sampling",
    "checked": true,
    "id": "0d6f497b52ea92c540d013d16809cc535a82cb6b",
    "semantic_title": "tabflex: scaling tabular learning to millions with linear attention",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=IfWKVF6LfY": {
    "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF",
    "volume": "spotlight",
    "abstract": "In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards---a challenging scenario in traditional deep reinforcement learning. Despite the great successes of PPO in the alignment of state-of-the-art closed-source large language models (LLMs), its open-source implementation is still largely sub-optimal, as widely reported by numerous research studies. To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information. Furthermore, we provide theoretical insights that demonstrate the superiority of our MDP framework over the previous sentence-level bandit formulation. Under this framework, we introduce an algorithm, dubbed as Reinforced Token Optimization (\\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal. Theoretically, \\texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently. For its practical implementation, \\texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage. We conduct extensive experiments to evaluate \\texttt{RTO} against PPO and other direct preference learning algorithms. The results highlight the effectiveness of RTO, with the algorithm outperforming PPO by 7.5 points on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code and models are available at \\href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}",
    "checked": true,
    "id": "16d68add6cbad736e6a75b00d2bf8bd49e4dbd40",
    "semantic_title": "dpo meets ppo: reinforced token optimization for rlhf",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=js3gePctLu": {
    "title": "Procurement Auctions via Approximately Optimal Submodular Optimization",
    "volume": "spotlight",
    "abstract": "We study the problem of procurement auctions, in which an auctioneer seeks to acquire services from a group of strategic sellers with private costs. The quality of the services is measured through some submodular function that is known to the auctioneer. Our goal is to design computationally efficient procurement auctions that (approximately) maximize the difference between the quality of the acquired services and the total cost of the sellers, in a way that is incentive compatible (IC) and individual rational (IR) for the sellers, and generates non-negative surplus (NAS) for the auctioneer. {Our contribution is twofold: \\textbf{i)} we provide an improved analysis of existing algorithms for non-positive submodular function maximization and \\textbf{ii)} we design computationally efficient frameworks that transform submodular function optimization algorithms to mechanisms that are IC and IR for the sellers, NAS for the auctioneer, and approximation-preserving.} Our frameworks are general and work both in the offline setting where the auctioneer can observe the bids and the services of all the sellers simultaneously, and in the online setting where the sellers arrive in an adversarial order and the auctioneer has to make an irrevocable decision whether to purchase their service or not. We further investigate whether it is possible to convert state-of-art submodular optimization algorithms into descending auctions. We focus on the adversarial setting, meaning that the schedule of the descending prices is determined by an adversary. We show that a submodular optimization algorithm satisfying bi-criteria $(1/2,1)$-approximation in welfare can be effectively converted to a descending auction in this setting. We further establish a connection between descending auctions and online submodular optimization. Finally, we demonstrate the practical applications of our frameworks by instantiating them with different state-of-the-art submodular optimization algorithms and comparing their welfare performance through empirical experiments on publicly available datasets that consist of thousands of sellers",
    "checked": true,
    "id": "13da404439706f9cb4a181542a9f630722458b32",
    "semantic_title": "procurement auctions via approximately optimal submodular optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F0sinjQMnv": {
    "title": "Identifying Causal Direction via Variational Bayesian Compression",
    "volume": "spotlight",
    "abstract": "Telling apart the cause and effect between two random variables with purely observational data is a challenging problem that finds applications in various scientific disciplines. A key principle utilized in this task is the algorithmic Markov condition, which postulates that the joint distribution, when factorized according to the causal direction, yields a more succinct codelength compared to the anti-causal direction. Previous approaches approximate these codelengths by relying on simple functions or Gaussian processes (GPs) with easily evaluable complexity, compromising between model fitness and computational complexity. To overcome these limitations, we propose leveraging the variational Bayesian learning of neural networks as an interpretation of the codelengths. Consequently, we can enhance the model fitness while promoting the succinctness of the codelengths, while avoiding the significant computational complexity of the GP-based approaches. Extensive experiments on both synthetic and real-world benchmarks in cause-effect identification demonstrate the effectiveness of our proposed method, surpassing the overall performance of related complexity-based and structural causal model regression-based approaches",
    "checked": true,
    "id": "7c212baf988a21dec33727e15538a6bfa90649ee",
    "semantic_title": "identifying causal direction via variational bayesian compression",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=afpc1MFMYU": {
    "title": "Non-stationary Diffusion For Probabilistic Time Series Forecasting",
    "volume": "spotlight",
    "abstract": "Due to the dynamics of underlying physics and external influences, the uncertainty of time series often varies over time. However, existing Denoising Diffusion Probabilistic Models (DDPMs) often fail to capture this non-stationary nature, constrained by their constant variance assumption from the additive noise model (ANM). In this paper, we innovatively utilize the Location-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of ANM. A diffusion-based probabilistic forecasting framework, termed Non-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of modeling the changing pattern of uncertainty. Specifically, NsDiff combines a denoising diffusion-based conditional generative model with a pre-trained conditional mean and variance estimator, enabling adaptive endpoint distribution modeling. Furthermore, we propose an uncertainty-aware noise schedule, which dynamically adjusts the noise levels to accurately reflect the data uncertainty at each step and integrates the time-varying variances into the diffusion process. Extensive experiments conducted on nine real-world and synthetic datasets demonstrate the superior performance of NsDiff compared to existing approaches. Code is available at https://github.com/wwy155/NsDiff",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cKaUC1PeJA": {
    "title": "Efficient and Separate Authentication Image Steganography Network",
    "volume": "spotlight",
    "abstract": "Image steganography hides multiple images for multiple recipients into a single cover image. All secret images are usually revealed without authentication, which reduces security among multiple recipients. It is elegant to design an authentication mechanism for isolated reception. We explore such mechanism through sufficient experiments, and uncover that additional authentication information will affect the distribution of hidden information and occupy more hiding space of the cover image. This severely decreases effectiveness and efficiency in large-capacity hiding. To overcome such a challenge, we first prove the authentication feasibility within image steganography. Then, this paper proposes an image steganography network collaborating with separate authentication and efficient scheme. Specifically, multiple pairs of lock-key are generated during hiding and revealing. Unlike traditional methods, our method has two stages to make appropriate distribution adaptation between locks and secret images, simultaneously extracting more reasonable primary information from secret images, which can release hiding space of the cover image to some extent. Furthermore, due to separate authentication, fused information can be hidden in parallel with a single network rather than traditional serial hiding with multiple networks, which can largely decrease the model size. Extensive experiments demonstrate that the proposed method achieves more secure, effective, and efficient image steganography. Code is available at https://github.com/Revive624/Authentication-Image-Steganography",
    "checked": false,
    "id": "683cb55c10d28a48772132668c1305180ff829a3",
    "semantic_title": "an efficient biometric authentication based on face image using deep generative adversarial network",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O6q2BHK1BL": {
    "title": "Local Identifying Causal Relations in the Presence of Latent Variables",
    "volume": "spotlight",
    "abstract": "We tackle the problem of identifying whether a variable is the cause of a specified target using observational data. State-of-the-art causal learning algorithms that handle latent variables typically rely on identifying the global causal structure, often represented as a partial ancestral graph (PAG), to infer causal relationships. Although effective, these approaches are often redundant and computationally expensive when the focus is limited to a specific causal relationship. In this work, we introduce novel local characterizations that are necessary and sufficient for various types of causal relationships between two variables, enabling us to bypass the need for global structure learning. Leveraging these local insights, we develop efficient and fully localized algorithms that accurately identify causal relationships from observational data. We theoretically demonstrate the soundness and completeness of our approach. Extensive experiments on benchmark networks and real-world datasets further validate the effectiveness and efficiency of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pWs925fKyK": {
    "title": "Bridging Layout and RTL: Knowledge Distillation based Timing Prediction",
    "volume": "spotlight",
    "abstract": "Accurate and efficient timing prediction at the register-transfer level (RTL) remains a fundamental challenge in electronic design automation (EDA), particularly in striking a balance between accuracy and computational efficiency. While static timing analysis (STA) provides high-fidelity results through comprehensive physical parameters, its computational overhead makes it impractical for rapid design iterations. Conversely, existing RTL-level approaches sacrifice accuracy due to the limited physical information available. We propose RTLDistil, a novel cross-stage knowledge distillation framework that bridges this gap by transferring precise physical characteristics from a layout-aware teacher model (Teacher GNN) to an efficient RTL-level student model (Student GNN), both implemented as graph neural networks (GNNs). RTLDistil efficiently predicts key timing metrics, such as arrival time (AT), and employs a multi-granularity distillation strategy that captures timing-critical features at node, subgraph, and global levels. Experimental results demonstrate that RTLDistil achieves significant improvement in RTL-level timing prediction error reduction, compared to state-of-the-art prediction models. This framework enables accurate early-stage timing prediction, advancing EDA's ``left-shift'' paradigm while maintaining computational efficiency. Our code and dataset will be publicly available at https://github.com/sklp-eda-lab/RTLDistil",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tqL8gJsuS5": {
    "title": "Efficient Source-free Unlearning via Energy-Guided Data Synthesis and Discrimination-Aware Multitask Optimization",
    "volume": "spotlight",
    "abstract": "With growing privacy concerns and the enforcement of data protection regulations, machine unlearning has emerged as a promising approach for removing the influence of forget data while maintaining model performance on retain data. However, most existing unlearning methods require access to the original training data, which is often impractical due to privacy policies, storage constraints, and other limitations. This gives rise to the challenging task of source-free unlearning, where unlearning must be accomplished without accessing the original training data. Few existing source-free unlearning methods rely on knowledge distillation and model retraining, which impose substantial computational costs. In this work, we propose the Data Synthesis-based Discrimination-Aware (DSDA) unlearning framework, which enables efficient source-free unlearning in two stages: (1) Accelerated Energy-Guided Data Synthesis (AEGDS), which employs Langevin dynamics to model the training data distribution while integrating Runge–Kutta methods and momentum to enhance efficiency. (2) Discrimination-Aware Multitask Optimization (DAMO), which refines the feature distribution of retain data and mitigates the gradient conflicts among multiple unlearning objectives. Extensive experiments on three benchmark datasets demonstrate that DSDA outperforms existing unlearning methods, validating its effectiveness and efficiency in source-free unlearning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dhRXGWJ027": {
    "title": "Discovering Symbolic Cognitive Models from Human and Animal Behavior",
    "volume": "spotlight",
    "abstract": "Symbolic models play a key role in cognitive science, expressing computationally precise hypotheses about how the brain implements a cognitive process. Identifying an appropriate model typically requires a great deal of effort and ingenuity on the part of a human scientist. Here, we adapt FunSearch (Romera-Paredes et al. 2024), a recently developed tool that uses Large Language Models (LLMs) in an evolutionary algorithm, to automatically discover symbolic cognitive models that accurately capture human and animal behavior. We consider datasets from three species performing a classic reward-learning task that has been the focus of substantial modeling effort, and find that the discovered programs outperform state-of-the-art cognitive models for each. The discovered programs can readily be interpreted as hypotheses about human and animal cognition, instantiating interpretable symbolic learning and decision-making algorithms. Broadly, these results demonstrate the viability of using LLM-powered program synthesis to propose novel scientific hypotheses regarding mechanisms of human and animal cognition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tnyxtaSve5": {
    "title": "Visual and Domain Knowledge for Professional-level Graph-of-Thought Medical Reasoning",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q3rGQUGgWo": {
    "title": "SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "3cd0fd03f085643a6aa338b5dc7545b5a04a479f",
    "semantic_title": "synevo: a neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KRosBwvhDx": {
    "title": "Do We Really Need Message Passing in Brain Network Modeling?",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XXFBqfwnUp": {
    "title": "Learning the RoPEs: Better 2D and 3D Position Encodings with STRING",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f8089cc2b6c587cd0f90811058fba28a0e969902",
    "semantic_title": "learning the ropes: better 2d and 3d position encodings with string",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=a7UM5c1CEa": {
    "title": "Provable Benefits of Unsupervised Pre-training and Transfer Learning via Single-Index Models",
    "volume": "spotlight",
    "abstract": "Unsupervised pre-training and transfer learning are commonly used techniques to initialize training algorithms for neural networks, particularly in settings with limited labeled data. In this paper, we study the effects of unsupervised pre-training and transfer learning on the sample complexity of high-dimensional supervised learning. Specifically, we consider the problem of training a single-layer neural network via online stochastic gradient descent. We establish that pre-training and transfer learning (under concept shift) reduce sample complexity by polynomial factors (in the dimension) under very general assumptions. We also uncover some surprising settings where pre-training grants exponential improvement over random initialization in terms of sample complexity",
    "checked": true,
    "id": "484b6fdb7daefe44841091349b3602db3f3e9bc8",
    "semantic_title": "provable benefits of unsupervised pre-training and transfer learning via single-index models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h2oNQOzbc5": {
    "title": "ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation",
    "volume": "spotlight",
    "abstract": "Generative recommendation (GR) is an emerging paradigm where user actions are tokenized into discrete token patterns and autoregressively generated as predictions. However, existing GR models tokenize each action independently, assigning the same fixed tokens to identical actions across all sequences without considering contextual relationships. This lack of context-awareness can lead to suboptimal performance, as the same action may hold different meanings depending on its surrounding context. To address this issue, we propose ActionPiece to explicitly incorporate context when tokenizing action sequences. In ActionPiece, each action is represented as a *set* of item features. Given the action sequence corpora, we construct the vocabulary by merging feature patterns as new tokens, based on their co-occurrence frequency both within individual sets and across adjacent sets. Considering the unordered nature of feature sets, we further introduce set permutation regularization, which produces multiple segmentations of action sequences with the same semantics. Our code is available at: https://github.com/google-deepmind/action_piece",
    "checked": true,
    "id": "c3155f1c063be81ee8a0f9d3bd755148167881a1",
    "semantic_title": "actionpiece: contextually tokenizing action sequences for generative recommendation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=O0oe7hPtbl": {
    "title": "Gridded Transformer Neural Processes for Spatio-Temporal Data",
    "volume": "spotlight",
    "abstract": "Effective modelling of large-scale spatio-temporal datasets is essential for many domains, yet existing approaches often impose rigid constraints on the input data, such as requiring them to lie on fixed-resolution grids. With the rise of foundation models, the ability to process diverse, heterogeneous data structures is becoming increasingly important. Neural processes (NPs), particularly transformer neural processes (TNPs), offer a promising framework for such tasks, but struggle to scale to large spatio-temporal datasets due to the lack of an efficient attention mechanism. To address this, we introduce gridded pseudo-token TNPs which employ specialised encoders and decoders to handle unstructured data and utilise a processor comprising gridded pseudo-tokens with efficient attention mechanisms. Furthermore, we develop equivariant gridded TNPs for applications where exact or approximate translation equivariance is a useful inductive bias, improving accuracy and training efficiency. Our method consistently outperforms a range of strong baselines in various synthetic and real-world regression tasks involving large-scale data, while maintaining competitive computational efficiency. Experiments with weather data highlight the potential of gridded TNPs and serve as just one example of a domain where they can have a significant impact",
    "checked": false,
    "id": "69461992bc91fcb0fb5bcc68a86307dadec36c86",
    "semantic_title": "gridded transformer neural processes for large unstructured spatio-temporal data",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=BUONdewsBa": {
    "title": "Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective",
    "volume": "spotlight",
    "abstract": "Ensuring fairness in medical image segmentation is critical due to biases in imbalanced clinical data acquisition caused by demographic attributes (e.g., age, sex, race) and clinical factors (e.g., disease severity). To address these challenges, we introduce Distribution-aware Mixture of Experts (dMoE), inspired by optimal control theory. We provide a comprehensive analysis of its underlying mechanisms and clarify dMoE's role in adapting to heterogeneous distributions in medical image segmentation. Furthermore, we integrate dMoE into multiple network architectures, demonstrating its broad applicability across diverse medical image analysis tasks. By incorporating demographic and clinical factors, dMoE achieves state-of-the-art performance on two 2D benchmark datasets and a 3D in-house dataset. Our results highlight the effectiveness of dMoE in mitigating biases from imbalanced distributions, offering a promising approach to bridging control theory and medical image segmentation within fairness learning paradigms. The source code is available at https://github.com/tvseg/dMoE",
    "checked": true,
    "id": "4a08c145ab345fed4be6c03b859aecf436d0f3e8",
    "semantic_title": "distribution-aware fairness learning in medical image segmentation from a control-theoretic perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ffpc7vx6qq": {
    "title": "Learning Safety Constraints for Large Language Models",
    "volume": "spotlight",
    "abstract": "Large language models (LLMs) have emerged as powerful tools but pose significant safety risks through harmful outputs and vulnerability to adversarial attacks. We propose SaP–short for Safety Polytope–a geometric approach to LLM safety, that learns and enforces multiple safety constraints directly in the model's representation space. We develop a framework that identifies safe and unsafe regions via the polytope's facets, enabling both detection and correction of unsafe outputs through geometric steering. Unlike existing approaches that modify model weights, SaP operates post-hoc in the representation space, preserving model capabilities while enforcing safety constraints. Experiments across multiple LLMs demonstrate that our method can effectively detect unethical inputs, reduce adversarial attack success rates while maintaining performance on standard tasks, thus highlighting the importance of having an explicit geometric model for safety. Analysis of the learned polytope facets reveals emergence of specialization in detecting different semantic notions of safety, providing interpretable insights into how safety is captured in LLMs' representation space",
    "checked": true,
    "id": "35aafa6763752fb4238c4d5edbbfa796baa89d58",
    "semantic_title": "learning safety constraints for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3pk0p4NGmQ": {
    "title": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities",
    "volume": "spotlight",
    "abstract": "Large language model (LLM) agents are increasingly capable of autonomously conducting cyberattacks, posing significant threats to existing applications. This growing risk highlights the urgent need for a real-world benchmark to evaluate the ability of LLM agents to exploit web application vulnerabilities. However, existing benchmarks fall short as they are limited to abstracted Capture-the-Flag competitions or lack comprehensive coverage. Building a benchmark for real-world vulnerabilities involves both specialized exper- tise to reproduce exploits and a systematic approach to evaluating unpredictable attacks. To address this challenge, we introduce CVE-Bench, a real-world cybersecurity benchmark based on critical-severity Common Vulnerabilities and Exposures. In CVE-Bench, we design a sandbox framework that enables LLM agents to exploit vulnerable web applications in scenarios that mimic real-world conditions, while also providing effective evaluation of their exploits. Our experiments show that the state-of-the-art agent framework can exploit up to 13% of the vulnerabilities",
    "checked": true,
    "id": "095b31dfaa032a2daf13da21bd4d04dddb2097fa",
    "semantic_title": "cve-bench: a benchmark for ai agents' ability to exploit real-world web application vulnerabilities",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=FKi6yjXwCN": {
    "title": "LOCATE 3D: Real-World Object Localization via Self-Supervised Learning in 3D",
    "volume": "spotlight",
    "abstract": "We present LOCATE 3D, a model for localizing objects in 3D scenes from referring expressions like \"the small coffee table between the sofa and the lamp.\" LOCATE 3D sets a new state-of-the-art on standard referential grounding benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D operates directly on sensor observation streams (posed RGB-D frames), enabling real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA, a novel self-supervised learning (SSL) algorithm applicable to sensor point clouds. It takes as input a 3D pointcloud featurized using 2D foundation models (CLIP, DINO). Subsequently, masked prediction in latent space is employed as a pretext task to aid the self-supervised learning of contextualized pointcloud features. Once trained, the 3D-JEPA encoder is finetuned alongside a language-conditioned decoder to jointly predict 3D masks and bounding boxes. Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential grounding, spanning multiple capture setups with over 130K annotations. This enables a systematic study of generalization capabilities as well as a stronger model. Code, models and dataset can be found at the project website: locate3d.atmeta.com",
    "checked": true,
    "id": "fbdbeb7fbe626f1a65db9afd9aad311e04d23f42",
    "semantic_title": "locate 3d: real-world object localization via self-supervised learning in 3d",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=OJ6WE7F8tK": {
    "title": "Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator",
    "volume": "spotlight",
    "abstract": "While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective, which minimizes the forward KL divergence, inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that integrates likelihood-based generative training and GAN-type discrimination to bypass this fundamental constraint by exploiting reverse KL and self-generated negative signals. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1\\% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58/1.96 to new records of 1.30/0.97/1.26 on CIFAR-10/ImageNet-64/ImageNet 512$\\times$512 datasets without any guidance mechanisms, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256$\\times$256",
    "checked": true,
    "id": "6bf0e42a2ad89a00c8be4c66303659144560d440",
    "semantic_title": "direct discriminative optimization: your likelihood-based visual generative model is secretly a gan discriminator",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=79O2XccGXZ": {
    "title": "Geometric Representation Condition Improves Equivariant Molecule Generation",
    "volume": "spotlight",
    "abstract": "Recent advances in molecular generative models have demonstrated great promise for accelerating scientific discovery, particularly in drug design. However, these models often struggle to generate high-quality molecules, especially in conditional scenarios where specific molecular properties must be satisfied. In this work, we introduce GeoRCG, a general framework to improve molecular generative models by integrating geometric representation conditions with provable theoretical guarantees. We decompose the generation process into two stages: first, generating an informative geometric representation; second, generating a molecule conditioned on the representation. Compared with single-stage generation, the easy-to-generate representation in the first stage guides the second stage generation toward a high-quality molecule in a goal-oriented way. Leveraging EDM and SemlaFlow as base generators, we observe significant quality improvements in unconditional molecule generation on the widely used QM9 and GEOM-DRUG datasets. More notably, in the challenging conditional molecular generation task, our framework achieves an average 50\\% performance improvement over state-of-the-art approaches, highlighting the superiority of conditioning on semantically rich geometric representations. Furthermore, with such representation guidance, the number of diffusion steps can be reduced to as small as 100 while largely preserving the generation quality achieved with 1,000 steps, thereby significantly reducing the generation iterations needed",
    "checked": true,
    "id": "740a29386610d499beb7725fa58b7b847185cd91",
    "semantic_title": "geometric representation condition improves equivariant molecule generation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=UFlyLkvyAE": {
    "title": "Graph Adaptive Autoregressive Moving Average Models",
    "volume": "spotlight",
    "abstract": "Graph State Space Models (SSMs) have recently been introduced to enhance Graph Neural Networks (GNNs) in modeling long-range interactions. Despite their success, existing methods either compromise on permutation equivariance or limit their focus to pairwise interactions rather than sequences. Building on the connection between Autoregressive Moving Average (ARMA) and SSM, in this paper, we introduce GRAMA, a Graph Adaptive method based on a learnable ARMA framework that addresses these limitations. By transforming from static to sequential graph data, GRAMA leverages the strengths of the ARMA framework, while preserving permutation equivariance. Moreover, GRAMA incorporates a selective attention mechanism for dynamic learning of ARMA coefficients, enabling efficient and flexible long-range information propagation. We also establish theoretical connections between GRAMA and Selective SSMs, providing insights into its ability to capture long-range dependencies. Experiments on 26 synthetic and real-world datasets demonstrate that GRAMA consistently outperforms backbone models and performs competitively with state-of-the-art methods",
    "checked": false,
    "id": "82469e636d5f6f66a159b42a744bd2b7003be10a",
    "semantic_title": "grama: adaptive graph autoregressive moving average models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EUH4VUCXay": {
    "title": "am-ELO: A Stable Framework for Arena-based LLM Evaluation",
    "volume": "spotlight",
    "abstract": "Arena-based evaluation is a fundamental yet significant evaluation paradigm for modern AI models, especially large language models (LLMs). Existing framework based on ELO rating system suffers from the inevitable instability problem due to ranking inconsistency and the lack of attention to the varying abilities of annotators. In this paper, we introduce a novel stable arena framework to address these issues by enhancing the ELO Rating System. Specifically, we replace the iterative update method with a Maximum Likelihood Estimation (MLE) approach, m-ELO, and provide theoretical proof of the consistency and stability of the MLE approach for model ranking. Additionally, we proposed the am-ELO, which modify the Elo Rating's probability function to incorporate annotator abilities, enabling the simultaneous estimation of model scores and annotator reliability. Experiments demonstrate that this method ensures stability, proving that this framework offers a more robust, accurate, and stable evaluation method for LLMs",
    "checked": true,
    "id": "f5574fa009b712c48835141d6cf960724de3c34f",
    "semantic_title": "am-elo: a stable framework for arena-based llm evaluation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=wbvshlfyB0": {
    "title": "Nonparametric Teaching for Graph Property Learners",
    "volume": "spotlight",
    "abstract": "Inferring properties of graph-structured data, *e.g.*, the solubility of molecules, essentially involves learning the implicit mapping from graphs to their properties. This learning process is often costly for graph property learners like Graph Convolutional Networks (GCNs). To address this, we propose a paradigm called Graph Nonparametric Teaching (GraNT) that reinterprets the learning process through a novel nonparametric teaching perspective. Specifically, the latter offers a theoretical framework for teaching implicitly defined (*i.e.*, nonparametric) mappings via example selection. Such an implicit mapping is realized by a dense set of graph-property pairs, with the GraNT teacher selecting a subset of them to promote faster convergence in GCN training. By analytically examining the impact of graph structure on parameter-based gradient descent during training, and recasting the evolution of GCNs—shaped by parameter updates—through functional gradient descent in nonparametric teaching, we show *for the first time* that teaching graph property learners (*i.e.*, GCNs) is consistent with teaching structure-aware nonparametric learners. These new findings readily commit GraNT to enhancing learning efficiency of the graph property learner, showing significant reductions in training time for graph-level regression (-36.62\\%), graph-level classification (-38.19\\%), node-level regression (-30.97\\%) and node-level classification (-47.30\\%), all while maintaining its generalization performance",
    "checked": true,
    "id": "c25e46fc0b3a3a096215e8af821400926742da40",
    "semantic_title": "nonparametric teaching for graph property learners",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u3n5wuRGTa": {
    "title": "Discovering a Zero (Zero-Vector Class of Machine Learning)",
    "volume": "spotlight",
    "abstract": "In Machine learning, separating data into classes is a very fundamental problem. A mathematical framework around the classes is presented in this work to deepen the understanding of classes. The classes are defined as vectors in a Vector Space, where addition corresponds to the union of classes, and scalar multiplication resembles set complement of classes. The Zero-Vector in the vector space corresponds to a class referred to as the Metta-Class. This discovery enables numerous applications. One such application, termed 'clear learning' in this work, focuses on learning the true nature (manifold) of the data instead of merely learning a boundary sufficient for classification. Another application, called 'unary class learning', involves learning a single class in isolation rather than learning by comparing two or more classes. Additionally, 'set operations on classes' is another application highlighted in this work. Furthermore, Continual Learning of classes is facilitated by smaller networks. The Metta-Class enables neural networks to learn only the data manifold; therefore, it can also be used for generation of new data. Results for the key applications are shown using the MNIST dataset. To further strengthen the claims, some results are also produced using the CIFAR-10 and ImageNet-1k embeddings. The code supporting these applications is publicly available at: github.com/hm-4/Metta-Class",
    "checked": false,
    "id": "c6545c007113421ce79eec933f58d38eec7b45de",
    "semantic_title": "assessment of zero-day vulnerability using machine learning approach",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=V61nluxFlR": {
    "title": "Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models",
    "volume": "spotlight",
    "abstract": "Large Language Models (LLMs) are expected to be predictable and trustworthy to support reliable decision-making systems. Yet current LLMs often show inconsistencies in their judgments. In this work, we examine \\textit{logical preference consistency} as a foundational requirement for building more dependable LLM systems, ensuring stable and coherent decision-making while minimizing erratic or contradictory outputs. To quantify the logical preference consistency, we propose a universal evaluation framework based on three fundamental properties: *transitivity*, *commutativity* and *negation invariance*. Through extensive experimentation across diverse LLMs, we demonstrate that these properties serve as strong indicators of judgment robustness. Furthermore, we introduce a data refinement and augmentation technique, REPAIR, that enhances logical consistency while maintaining alignment with human preferences. Finally, we show that improving consistency leads to better performance in LLM-driven logic-based algorithms, reinforcing stability and coherence in decision-making systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jvP1wbD0xh": {
    "title": "Better to Teach than to Give: Domain Generalized Semantic Segmentation via Agent Queries with Diffusion Model Guidance",
    "volume": "spotlight",
    "abstract": "Domain Generalized Semantic Segmentation (DGSS) trains a model on a labeled source domain to generalize to unseen target domains with consistent contextual distribution and varying visual appearance. Most existing methods rely on domain randomization or data generation but struggle to capture the underlying scene distribution, resulting in the loss of useful semantic information. Inspired by the diffusion model's capability to generate diverse variations within a given scene context, we consider harnessing its rich prior knowledge of scene distribution to tackle the challenging DGSS task. In this paper, we propose a novel agent \\textbf{Query}-driven learning framework based on \\textbf{Diff}usion model guidance for DGSS, named QueryDiff. Our recipe comprises three key ingredients: (1) generating agent queries from segmentation features to aggregate semantic information about instances within the scene; (2) learning the inherent semantic distribution of the scene through agent queries guided by diffusion features; (3) refining segmentation features using optimized agent queries for robust mask predictions. Extensive experiments across various settings demonstrate that our method significantly outperforms previous state-of-the-art methods. Notably, it enhances the model's ability to generalize effectively to extreme domains, such as cubist art styles. Code is available at https://github.com/FanLiHub/QueryDiff",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yXRixu0ONY": {
    "title": "P(all-atom) Is Unlocking New Path For Protein Design",
    "volume": "spotlight",
    "abstract": "We introduce Pallatom, an innovative protein generation model capable of producing protein structures with all-atom coordinates. Pallatom directly learns and models the joint distribution $P(\\textit{structure}, \\textit{seq})$ by focusing on $P(\\textit{all-atom})$, effectively addressing the interdependence between sequence and structure in protein generation. To achieve this, we propose a novel network architecture specifically designed for all-atom protein generation. Our model employs a dual-track framework that tokenizes proteins into token-level and atomic-level representations, integrating them through a multi-layer decoding process with \"traversing\" representations and recycling mechanism. We also introduce the $\\texttt{atom14}$ representation method, which unifies the description of unknown side-chain coordinates, ensuring high fidelity between the generated all-atom conformation and its physical structure. Experimental results demonstrate that Pallatom excels in key metrics of protein design, including designability, diversity, and novelty, showing significant improvements across the board. Our model not only enhances the accuracy of protein generation but also exhibits excellent sampling efficiency, paving the way for future applications in larger and more complex systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I4jNAbqHnM": {
    "title": "The Number of Trials Matters in Infinite-Horizon General-Utility Markov Decision Processes",
    "volume": "spotlight",
    "abstract": "The general-utility Markov decision processes (GUMDPs) framework generalizes the MDPs framework by considering objective functions that depend on the frequency of visitation of state-action pairs induced by a given policy. In this work, we contribute with the first analysis on the impact of the number of trials, i.e., the number of randomly sampled trajectories, in infinite-horizon GUMDPs. We show that, as opposed to standard MDPs, the number of trials plays a key-role in infinite-horizon GUMDPs and the expected performance of a given policy depends, in general, on the number of trials. We consider both discounted and average GUMDPs, where the objective function depends, respectively, on discounted and average frequencies of visitation of state-action pairs. First, we study policy evaluation under discounted GUMDPs, proving lower and upper bounds on the mismatch between the finite and infinite trials formulations for GUMDPs. Second, we address average GUMDPs, studying how different classes of GUMDPs impact the mismatch between the finite and infinite trials formulations. Third, we provide a set of empirical results to support our claims, highlighting how the number of trajectories and the structure of the underlying GUMDP influence policy evaluation",
    "checked": true,
    "id": "0039e67cd656c4c00b0aa2d3d69312d175bdc952",
    "semantic_title": "the number of trials matters in infinite-horizon general-utility markov decision processes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hYHczNrKoX": {
    "title": "On the Benefits of Active Data Collection in Operator Learning",
    "volume": "spotlight",
    "abstract": "We study active data collection strategies for operator learning when the target operator is linear and the input functions are drawn from a mean-zero stochastic process with continuous covariance kernels. With an active data collection strategy, we establish an error convergence rate in terms of the decay rate of the eigenvalues of the covariance kernel. We can achieve arbitrarily fast error convergence rates with sufficiently rapid eigenvalue decay of the covariance kernels. This contrasts with the passive (i.i.d.) data collection strategies, where the convergence rate is never faster than linear decay ($\\sim n^{-1}$). In fact, for our setting, we show a \\emph{non-vanishing} lower bound for any passive data collection strategy, regardless of the eigenvalues decay rate of the covariance kernel. Overall, our results show the benefit of active data collection strategies in operator learning over their passive counterparts",
    "checked": true,
    "id": "0908ee8aad92be37f88c321699c2cc366b631dcd",
    "semantic_title": "on the benefits of active data collection in operator learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=DJcEoC9JpQ": {
    "title": "Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger",
    "volume": "spotlight",
    "abstract": "Recent advancements in Large Vision Language Models (LVLMs) have significantly improved performance in Visual Question Answering (VQA) tasks through multimodal Retrieval-Augmented Generation (RAG). However, existing methods still face challenges, such as the scarcity of knowledge with reasoning examples and erratic responses from retrieved knowledge. To address these issues, in this study, we propose a multimodal RAG framework, termed RCTS, which enhances LVLMs by constructing a Reasoning Context-enriched knowledge base and a Tree Search re-ranking method. Specifically, we introduce a self-consistent evaluation mechanism to enrich the knowledge base with intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This ensures that LVLMs can leverage high-quality contextual reasoning for better and more consistent responses. Extensive experiments demonstrate that our framework achieves state-of-the-art performance on multiple VQA datasets, significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods. It highlights the effectiveness of our knowledge base and re-ranking method in improving LVLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZrhGq664om": {
    "title": "Neural Collapse Beyond the Unconstrained Features Model: Landscape, Dynamics, and Generalization in the Mean-Field Regime",
    "volume": "spotlight",
    "abstract": "Neural Collapse is a phenomenon where the last-layer representations of a well-trained neural network converge to a highly structured geometry. In this paper, we focus on its first (and most basic) property, known as NC1: the within-class variability vanishes. While prior theoretical studies establish the occurrence of NC1 via the data-agnostic unconstrained features model, our work adopts a data-specific perspective, analyzing NC1 in a three-layer neural network, with the first two layers operating in the mean-field regime and followed by a linear layer. In particular, we establish a fundamental connection between NC1 and the loss landscape: we prove that points with small empirical loss and gradient norm (thus, close to being stationary) approximately satisfy NC1, and the closeness to NC1 is controlled by the residual loss and gradient norm. We then show that (i) gradient flow on the mean squared error converges to NC1 solutions with small empirical loss, and (ii) for well-separated data distributions, both NC1 and vanishing test loss are achieved simultaneously. This aligns with the empirical observation that NC1 emerges during training while models attain near-zero test error. Overall, our results demonstrate that NC1 arises from gradient training due to the properties of the loss landscape, and they show the co-occurrence of NC1 and small test error for certain data distributions",
    "checked": true,
    "id": "c0d32ef26d0c02d8c6c568fbc4551857b2748f75",
    "semantic_title": "neural collapse beyond the unconstrained features model: landscape, dynamics, and generalization in the mean-field regime",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mruyFvKDKq": {
    "title": "Invariant Deep Uplift Modeling for Incentive Assignment in Online Marketing via Probability of Necessity and Sufficiency",
    "volume": "spotlight",
    "abstract": "In online platforms, incentives (\\textit{e.g}., discounts, coupons) are used to boost user engagement and revenue. Uplift modeling methods are developed to estimate user responses from observational data, often incorporating distribution balancing to address selection bias. However, these methods are limited by in-distribution testing data, which mirrors the training data distribution. In reality, user features change continuously due to time, geography, and other factors, especially on complex online marketing platforms. Thus, effective uplift modeling method for out-of-distribution data is crucial. To address this, we propose a novel uplift modeling method \\textbf{I}nvariant \\textbf{D}eep \\textbf{U}plift \\textbf{M}odeling, namely \\textbf{IDUM}, which uses invariant learning to enhance out-of-distribution generalization by identifying causal factors that remain consistent across domains. IDUM further refines these features into necessary and sufficient factors and employs a masking component to reduce computational costs by selecting the most informative invariant features. A balancing discrepancy component is also introduced to mitigate selection bias in observational data. We conduct extensive experiments on public and real-world datasets to demonstrate IDUM's effectiveness in both in-distribution and out-of-distribution scenarios in online marketing. Furthermore, we also provide theoretical analysis and related proofs to support our IDUM's generalizability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LpE54NUnmO": {
    "title": "G-Designer: Architecting Multi-agent Communication Topologies via Graph Neural Networks",
    "volume": "spotlight",
    "abstract": "Recent advancements in large language model (LLM)-based agents have demonstrated that collective intelligence can significantly surpass the capabilities of individual agents, primarily due to well-crafted inter-agent communication topologies. Despite the diverse and high-performing designs available, practitioners often face confusion when selecting the most effective pipeline for their specific task: \\textit{Which topology is the best choice for my task, avoiding unnecessary communication token overhead while ensuring high-quality solution?} In response to this dilemma, we introduce G-Designer, an adaptive, efficient, and robust solution for multi-agent deployment, which dynamically designs task-aware, customized communication topologies. Specifically, G-Designer models the multi-agent system as a multi-agent network, leveraging a variational graph auto-encoder to encode both the nodes (agents) and a task-specific virtual node, and decodes a task-adaptive and high-performing communication topology. Extensive experiments on six benchmarks showcase that G-Designer is: \\textbf{(1) high-performing}, achieving superior results on MMLU with accuracy at $84.50\\\\%$ and on HumanEval with pass@1 at $89.90\\\\%$; \\textbf{(2) task-adaptive}, architecting communication protocols tailored to task difficulty, reducing token consumption by up to $95.33\\\\%$ on HumanEval; and \\textbf{(3) adversarially robust}, defending against agent adversarial attacks with merely $0.3\\\\%$ accuracy drop",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=10l1pGeOcK": {
    "title": "SAFE: Finding Sparse and Flat Minima to Improve Pruning",
    "volume": "spotlight",
    "abstract": "Sparsifying neural networks often suffers from seemingly inevitable performance degradation, and it remains challenging to restore the original performance despite much recent progress. Motivated by recent studies in robust optimization, we aim to tackle this problem by finding subnetworks that are both sparse and flat at the same time. Specifically, we formulate pruning as a sparsity-constrained optimization problem where flatness is encouraged as an objective. We solve it explicitly via an augmented Lagrange dual approach and extend it further by proposing a generalized projection operation, resulting in novel pruning methods called SAFE and its extension, SAFE$^+$. Extensive evaluations on standard image classification and language modeling tasks reveal that SAFE consistently yields sparse networks with improved generalization performance, which compares competitively to well-established baselines. In addition, SAFE demonstrates resilience to noisy data, making it well-suited for real-world conditions",
    "checked": true,
    "id": "0d2417a26026aa101c2e2f26065029596ede5dd9",
    "semantic_title": "safe: finding sparse and flat minima to improve pruning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pKaNgFzJBy": {
    "title": "On the Guidance of Flow Matching",
    "volume": "spotlight",
    "abstract": "Flow matching has shown state-of-the-art performance in various generative tasks, ranging from image generation to decision-making, where generation under energy guidance (abbreviated as guidance in the following) is pivotal. However, the guidance of flow matching is more general than and thus substantially different from that of its predecessor, diffusion models. Therefore, the challenge in guidance for general flow matching remains largely underexplored. In this paper, we propose the first framework of general guidance for flow matching. From this framework, we derive a family of guidance techniques that can be applied to general flow matching. These include a new training-free asymptotically exact guidance, novel training losses for training-based guidance, and two classes of approximate guidance that cover classical gradient guidance methods as special cases. We theoretically investigate these different methods to give a practical guideline for choosing suitable methods in different scenarios. Experiments on synthetic datasets, image inverse problems, and offline reinforcement learning demonstrate the effectiveness of our proposed guidance methods and verify the correctness of our flow matching guidance framework. Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/flow_guidance",
    "checked": true,
    "id": "be811cc412aef2152a4935e633e666bc2909eb65",
    "semantic_title": "on the guidance of flow matching",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=BkdAnSKNoX": {
    "title": "TLLC: Transfer Learning-based Label Completion for Crowdsourcing",
    "volume": "spotlight",
    "abstract": "Label completion serves as a preprocessing approach to handling the sparse crowdsourced label matrix problem, significantly boosting the effectiveness of the downstream label aggregation. In recent advances, worker modeling has been proved to be a powerful strategy to further improve the performance of label completion. However, in real-world scenarios, workers typically annotate only a few instances, leading to insufficient worker modeling and thus limiting the improvement of label completion. To address this issue, we propose a novel transfer learning-based label completion (TLLC) method. Specifically, we first identify all high-confidence instances from the whole crowdsourced data as a source domain and use it to pretrain a Siamese network. The abundant annotated instances in the source domain provide essential knowledge for worker modeling. Then, we transfer the pretrained network to the target domain with the instances annotated by each worker separately, ensuring worker modeling captures unique characteristics of each worker. Finally, we leverage the new embeddings learned by the transferred network to complete each worker's missing labels. Extensive experiments on several widely used real-world datasets demonstrate the effectiveness of TLLC. Our codes and datasets are available at https://github.com/jiangliangxiao/TLLC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U354tbTjav": {
    "title": "Return of the Latent Space COWBOYS: Re-thinking the use of VAEs for Bayesian Optimisation of Structured Spaces",
    "volume": "spotlight",
    "abstract": "Bayesian optimisation in the latent space of a VAE is a powerful framework for optimisation tasks over complex structured domains, such as the space of valid molecules. However, existing approaches tightly couple the surrogate and generative models, which can lead to suboptimal performance when the latent space is not tailored to specific tasks, which in turn has led to the proposal of increasingly sophisticated algorithms. In this work, we explore a new direction, instead proposing a decoupled approach that trains a generative model and a GP surrogate separately, then combines them via a simple yet principled Bayesian update rule. This separation allows each component to focus on its strengths— structure generation from the VAE and predictive modelling by the GP. We show that our decoupled approach improves our ability to identify high-potential candidates in molecular optimisation problems under constrained evaluation budgets",
    "checked": true,
    "id": "e618025721c3ad35cc04108e8a6af2fd79099f3b",
    "semantic_title": "return of the latent space cowboys: re-thinking the use of vaes for bayesian optimisation of structured spaces",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ppcf30NGL0": {
    "title": "New Bounds for Sparse Variational Gaussian Processes",
    "volume": "spotlight",
    "abstract": "Sparse variational Gaussian processes (GPs) construct tractable posterior approximations to GP models. At the core of these methods is the assumption that the true posterior distribution over training function values ${\\bf f}$ and inducing variables ${\\bf u}$ is approximated by a variational distribution that incorporates the conditional GP prior $p({\\bf f} | {\\bf u})$ in its factorization. While this assumption is considered as fundamental, we show that for model training we can relax it through the use of a more general variational distribution $q({\\bf f} | {\\bf u} )$ that depends on $N$ extra parameters, where $N$ is the number of training examples. In GP regression, we can analytically optimize the evidence lower bound over the extra parameters and express a tractable collapsed bound that is tighter than the previous bound. The new bound is also amenable to stochastic optimization and its implementation requires minor modifications to existing sparse GP code. Further, we also describe extensions to non-Gaussian likelihoods. On several datasets we demonstrate that our method can reduce bias when learning the hyperparameters and can lead to better predictive performance",
    "checked": true,
    "id": "9c8f948fb7b406d00c4203afbe38f074cd4c1ccd",
    "semantic_title": "new bounds for sparse variational gaussian processes",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=vES22INUKm": {
    "title": "An Error Analysis of Flow Matching for Deep Generative Modeling",
    "volume": "spotlight",
    "abstract": "Continuous Normalizing Flows (CNFs) have proven to be a highly efficient technique for generative modeling of complex data since the introduction of Flow Matching (FM). The core of FM is to learn the constructed velocity fields of CNFs through deep least squares regression. Despite its empirical effectiveness, theoretical investigations of FM remain limited. In this paper, we present the first end-to-end error analysis of CNFs built upon FM. Our analysis shows that for general target distributions with bounded support, the generated distribution of FM is guaranteed to converge to the target distribution in the sense of the Wasserstein-2 distance. Furthermore, the convergence rate is significantly improved under an additional mild Lipschitz condition of the target score function",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PUzNwYmb3l": {
    "title": "Efficient First-Order Optimization on the Pareto Set for Multi-Objective Learning under Preference Guidance",
    "volume": "spotlight",
    "abstract": "Multi-objective learning under user-specified preference is common in real-world problems such as multi-lingual speech recognition under fairness. In this work, we frame such a problem as a semivectorial bilevel optimization problem, whose goal is to optimize a pre-defined preference function, subject to the constraint that the model parameters are weakly Pareto optimal. To solve this problem, we convert the multi-objective constraints to a single-objective constraint through a merit function with an easy-to-evaluate gradient, and then, we use a penalty-based reformulation of the bilevel optimization problem. We theoretically establish the properties of the merit function, and the relations of solutions for the penalty reformulation and the constrained formulation. Then we propose algorithms to solve the reformulated single-level problem, and establish its convergence guarantees. We test the method on various synthetic and real-world problems. The results demonstrate the effectiveness of the proposed method in finding preference-guided optimal solutions to the multi-objective problem",
    "checked": true,
    "id": "6642476192639f432b295923e4578516f5e83966",
    "semantic_title": "efficient first-order optimization on the pareto set for multi-objective learning under preference guidance",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=iFOXz5H2gB": {
    "title": "Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios",
    "volume": "spotlight",
    "abstract": "Leveraging the powerful representation learning capabilities, deep multi-view clustering methods have demonstrated reliable performance by effectively integrating multi-source information from diverse views in recent years. Most existing methods rely on the assumption of clean views. However, noise is pervasive in real-world scenarios, leading to a significant degradation in performance. To tackle this problem, we propose a novel multi-view clustering framework for the automatic identification and rectification of noisy data, termed AIRMVC. Specifically, we reformulate noisy identification as an anomaly identification problem using GMM. We then design a hybrid rectification strategy to mitigate the adverse effects of noisy data based on the identification results. Furthermore, we introduce a noise-robust contrastive mechanism to generate reliable representations. Additionally, we provide a theoretical proof demonstrating that these representations can discard noisy information, thereby improving the performance of downstream tasks. Extensive experiments on six benchmark datasets demonstrate that AIRMVC outperforms state-of-the-art algorithms in terms of robustness in noisy scenarios. The code of AIRMVC are available at https://github.com/xihongyang1999/AIRMVC on Github",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WR0ahlhOoy": {
    "title": "Improving Zero-Shot Adversarial Robustness in Vision-Language Models by Closed-form Alignment of Adversarial Path Simplices",
    "volume": "spotlight",
    "abstract": "Vision-Language Models (VLMs) such as CLIP excel at zero-shot classification due to large-scale pre-training but are vulnerable to adversarial examples. Adversarial fine-tuning robustifies zero-shot models by aligning prediction scores of individual adversaries with their clean counterparts, which typically overlooks intermediate adversarial samples along the adversarial trajectory crossing the decision boundary. Such intermediate adversaries and their vicinity produce informative representations capturing the decision boundary in detail. They can be improved by sampling adversarial candidates from simplices formed by joining two consecutive vertices on the adversarial trajectory and their clean counterpart. However, sampling simplices for adversaries is very costly. To train robust VLM, we overcome these limitations by Taylor expansion and formulating an upper-bound of alignment loss that depends on the Jacobian/Hessian obtained at clean samples. As regions between clean and intermediate adversarial samples capture a larger decision landscape, we robustify VLM by plausible adversaries from simplices by our closed-form formulation equivalent to infinite uniform sampling of the simplex. We obtain state-of-the-art robustness across 15 datasets and diverse vision-language tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T5IZ32ImAB": {
    "title": "Graph Diffusion for Robust Multi-Agent Coordination",
    "volume": "spotlight",
    "abstract": "Offline multi-agent reinforcement learning (MARL) struggles to estimate out-of-distribution states and actions due to the absence of real-time environmental feedback. While diffusion models show promise in addressing these challenges, their application primarily focuses on independently diffusing the historical trajectories of individual agents, neglecting crucial multi-agent coordination dynamics and reducing policy robustness in dynamic environments. In this paper, we propose MCGD, a novel Multi-agent Coordination framework based on Graph Diffusion models to improve the effectiveness and robustness of collaborative policies. Specifically, we begin by constructing a sparse coordination graph that includes continuous node attributes and discrete edge attributes to effectively identify the underlying dynamics of multi-agent interactions. Next, we derive transition probabilities between edge categories and present adaptive categorical diffusion to capture the structure diversity of multi-agent coordination. Leveraging this coordination structure, we define neighbor-dependent forward noise and develop anisotropic diffusion to enhance the action diversity of each agent. Extensive experiments across various multi-agent environments demonstrate that MCGD significantly outperforms existing state-of-the-art baselines in coordination performance and policy robustness in dynamic environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y19ngWhN0b": {
    "title": "Weakly-Supervised Contrastive Learning for Imprecise Class Labels",
    "volume": "spotlight",
    "abstract": "Contrastive learning has achieved remarkable success in learning effective representations, with supervised contrastive learning often outperforming self-supervised approaches. However, in real-world scenarios, data annotations are often ambiguous or inaccurate, meaning that class labels may not reliably indicate whether two examples belong to the same class. This limitation restricts the applicability of supervised contrastive learning. To address this challenge, we introduce the concept of ``continuous semantic similarity'' to define positive and negative pairs. Instead of directly relying on imprecise class labels, we measure the semantic similarity between example pairs, which quantifies how closely they belong to the same category by iteratively refining weak supervisory signals. Based on this concept, we propose a graph-theoretic framework for weakly-supervised contrastive learning, where semantic similarity serves as the graph weights. Our framework is highly versatile and can be applied to many weakly-supervised learning scenarios. We demonstrate its effectiveness through experiments in two common settings, i.e., noisy label and partial label learning, where existing methods can be easily integrated to significantly improve performance. Theoretically, we establish an error bound for our approach, showing that it can approximate supervised contrastive learning under mild conditions. The implementation code is available at [https://github.com/Speechless-10308/WSC](https://github.com/Speechless-10308/WSC)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DDIGCk25BO": {
    "title": "Robust Automatic Modulation Classification with Fuzzy Regularization",
    "volume": "spotlight",
    "abstract": "Automatic Modulation Classification (AMC) serves as a foundational pillar for cognitive radio systems, enabling critical functionalities including dynamic spectrum allocation, non-cooperative signal surveillance, and adaptive waveform optimization. However, practical deployment of AMC faces a fundamental challenge: prediction ambiguity arising from intrinsic similarity among modulation schemes and exacerbated under low signal-to-noise ratio (SNR) conditions. This phenomenon manifests as near-identical probability distributions across confusable modulation types, significantly degrading classification reliability. To address this, we propose Fuzzy Regularization-enhanced AMC (FR-AMC), a novel framework that integrates uncertainty quantification into the classification pipeline. The proposed FR has three features: (1) Explicitly model prediction ambiguity during backpropagation, (2) dynamic sample reweighting through adaptive loss scaling, (3) encourage margin maximization between confusable modulation clusters. Experimental results on benchmark datasets demonstrate that the FR achieves superior classification accuracy and robustness compared to compared methods, making it a promising solution for real-world spectrum management and communication applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bLcXkIasck": {
    "title": "Language Models May Verbatim Complete Text They Were Not Explicitly Trained On",
    "volume": "spotlight",
    "abstract": "An important question today is whether a given text was used to train a large language model (LLM). A completion test is often employed: check if the LLM completes a sufficiently complex text. This, however, requires a ground-truth definition of membership; most commonly, it is defined as a member based on the n-gram overlap between the target text and any text in the dataset. In this work, we demonstrate that this n-gram based membership definition can be effectively gamed. We study scenarios where sequences are non-members for a given n and we find that completion tests still succeed. We find many natural cases of this phenomenon by retraining LLMs from scratch after removing all training samples that were completed; these cases include exact duplicates, near-duplicates, and even short overlaps. They showcase that it is difficult to find a single viable choice of n for membership definitions. Using these insights, we design adversarial datasets that can cause a given target sequence to be completed without containing it, for any reasonable choice of n. Our findings highlight the inadequacy of n-gram membership, suggesting membership definitions fail to account for auxiliary information available to the training algorithm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0cEZyhHEks": {
    "title": "Taming Knowledge Conflicts in Language Models",
    "volume": "spotlight",
    "abstract": "Language Models (LMs) often encounter knowledge conflicts when parametric memory contradicts contextual knowledge. Previous works attribute this conflict to the interplay between \"memory heads\" and \"context heads\", attention heads assumed to promote either memory or context exclusively. In this study, we go beyond this fundamental assumption by uncovering a critical phenomenon we term the *superposition of contextual information and parametric memory*, where highly influential attention heads simultaneously contribute to both memory and context. Building upon this insight, we propose Just Run Twice (JuICE), a test-time attention intervention method that steers LMs toward either parametric beliefs or contextual knowledge without requiring fine-tuning. JuICE identifies a set of reliable attention heads and leverages a dual-run approach to mitigate the superposition effects. Extensive experiments across 11 datasets and 6 model architectures demonstrate that JuICE sets the new state-of-the-art performance and robust generalization, achieving significant and consistent improvement across different domains under various conflict types. Finally, we theoretically analyze knowledge conflict and the superposition of contextual information and parametric memory in attention heads, which further elucidates the effectiveness of JuICE in these settings. Our code is available at https://github.com/GaotangLi/JUICE",
    "checked": true,
    "id": "b7ba9df4eb239708cf48f25be87b5bceeca010e3",
    "semantic_title": "taming knowledge conflicts in language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=uqCfoVXb67": {
    "title": "UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal Control",
    "volume": "spotlight",
    "abstract": "Recent advances in diffusion bridge models leverage Doob's $h$-transform to establish fixed endpoints between distributions, demonstrating promising results in image translation and restoration tasks. However, these approaches frequently produce blurred or excessively smoothed image details and lack a comprehensive theoretical foundation to explain these shortcomings. To address these limitations, we propose UniDB, a unified framework for diffusion bridges based on Stochastic Optimal Control (SOC). UniDB formulates the problem through an SOC-based optimization and derives a closed-form solution for the optimal controller, thereby unifying and generalizing existing diffusion bridge models. We demonstrate that existing diffusion bridges employing Doob's $h$-transform constitute a special case of our framework, emerging when the terminal penalty coefficient in the SOC cost function tends to infinity. By incorporating a tunable terminal penalty coefficient, UniDB achieves an optimal balance between control costs and terminal penalties, substantially improving detail preservation and output quality. Notably, UniDB seamlessly integrates with existing diffusion bridge models, requiring only minimal code modifications. Extensive experiments across diverse image restoration tasks validate the superiority and adaptability of the proposed framework. Our code is available at https://github.com/UniDB-SOC/UniDB/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=71Mm8GDGYd": {
    "title": "K 2 VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting",
    "volume": "spotlight",
    "abstract": "Probabilistic Time Series Forecasting (PTSF) plays a crucial role in decision-making across various fields, including economics, energy, and transportation. Most existing methods excell at short-term forecasting, while overlooking the hurdles of Long-term Probabilistic Time Series Forecasting (LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have a significant adverse effect on prediction accuracy, and make generative models inefficient by increasing the cost of each iteration. To overcome these limitations, we introduce $K^2$VAE, an efficient VAE-based generative model that leverages a KoopmanNet to transform nonlinear time series into a linear dynamical system, and devises a KalmanNet to refine predictions and model uncertainty in such linear system, which reduces error accumulation in long-term forecasting. Extensive experiments demonstrate that $K^2$VAE outperforms state-of-the-art methods in both short- and long-term PTSF, providing a more efficient and accurate solution",
    "checked": false,
    "id": "a329ab6e760387dd69655a9b6d9865aee74237a3",
    "semantic_title": "k2vae: a koopman-kalman enhanced variational autoencoder for probabilistic time series forecasting",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=2PWn1LtCwP": {
    "title": "Doubly Robust Conformalized Survival Analysis with Right-Censored Data",
    "volume": "spotlight",
    "abstract": "We present a conformal inference method for constructing lower prediction bounds for survival times from right-censored data, extending recent approaches designed for more restrictive type-I censoring scenarios. The proposed method imputes unobserved censoring times using a machine learning model, and then analyzes the imputed data using a survival model calibrated via weighted conformal inference. This approach is theoretically supported by an asymptotic double robustness property. Empirical studies on simulated and real data demonstrate that our method leads to relatively informative predictive inferences and is especially robust in challenging settings where the survival model may be inaccurate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WbP2OwMULq": {
    "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation",
    "volume": "spotlight",
    "abstract": "We present **HealthGPT**, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained Large Language Models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation **(H-LoRA)** technique, which is complemented by a tailored hierarchical visual perception **(HVP)** approach and a three-stage learning strategy **(TLS)**. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called **VL-Health**. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT",
    "checked": true,
    "id": "c618b27700eff3f59abaaa926922369b95ec7f78",
    "semantic_title": "healthgpt: a medical large vision-language model for unifying comprehension and generation via heterogeneous knowledge adaptation",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=GhTdNOMfOD": {
    "title": "TimeBase: The Power of Minimalism in Efficient Long-term Time Series Forecasting",
    "volume": "spotlight",
    "abstract": "Long-term time series forecasting (LTSF) has traditionally relied on large parameters to capture extended temporal dependencies, resulting in substantial computational costs and inefficiencies in both memory usage and processing time. However, time series data, unlike high-dimensional images or text, often exhibit temporal pattern similarity and low-rank structures, especially in long-term horizons. By leveraging this structure, models can be guided to focus on more essential, concise temporal data, improving both accuracy and computational efficiency. In this paper, we introduce TimeBase, an ultra-lightweight network to harness the power of minimalism in LTSF. TimeBase 1) extracts core basis temporal components and 2) transforms traditional point-level forecasting into efficient segment-level forecasting, achieving optimal utilization of both data and parameters. Extensive experiments on diverse real-world datasets show that TimeBase achieves remarkable efficiency and secures competitive forecasting performance. Additionally, TimeBase can also serve as a very effective plug-and-play complexity reducer for any patch-based forecasting models. Code is available at \\url{https://github.com/hqh0728/TimeBase}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IYOksPHJKT": {
    "title": "Catch Your Emotion: Sharpening Emotion Perception in Multimodal Large Language Models",
    "volume": "spotlight",
    "abstract": "Multimodal large language models (MLLMs) have achieved impressive progress in tasks such as visual question answering and visual understanding, but they still face significant challenges in emotional reasoning. Current methods to enhance emotional understanding typically rely on fine-tuning or manual annotations, which are resource-intensive and limit scalability. In this work, we focus on improving the ability of MLLMs to capture emotions during the inference phase. Specifically, MLLMs encounter two main issues: they struggle to distinguish between semantically similar emotions, leading to misclassification, and they are overwhelmed by redundant or irrelevant visual information, which distracts from key emotional cues. To address these, we propose Sharpening Emotion Perception in MLLMs (SEPM), which incorporates a Confidence-Guided Coarse-to-Fine Inference framework to refine emotion classification by guiding the model through simpler tasks. Additionally, SEPM employs Focus-on-Emotion Visual Augmentation to reduce visual redundancy by directing the attention of models to relevant emotional cues in images. Experimental results demonstrate that SEPM significantly improves MLLM performance on emotion-related tasks, providing a resource-efficient and scalable solution for emotion recognition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F1ff8zcjPp": {
    "title": "Layer-wise Alignment: Examining Safety Alignment Across Image Encoder Layers in Vision Language Models",
    "volume": "spotlight",
    "abstract": "Vision-language models (VLMs) have improved significantly in their capabilities, but their complex architecture makes their safety alignment challenging. In this paper, we reveal an uneven distribution of harmful information across the intermediate layers of the image encoder and show that skipping a certain set of layers and exiting early can increase the chance of the VLM generating harmful responses. We call it as \"Image enCoder Early-exiT\" based vulnerability (ICET). Our experiments across three VLMs: LLaVA-1.5, LLaVA-NeXT, and Llama 3.2 show that performing early exits from the image encoder significantly increases the likelihood of generating harmful outputs. To tackle this, we propose a simple yet effective modification of the Clipped-Proximal Policy Optimization (Clip-PPO) algorithm for performing layer-wise multi-modal RLHF for VLMs. We term this as Layer-Wise PPO (L-PPO). We evaluate our L-PPO algorithm across three multi-modal datasets and show that it consistently reduces the harmfulness caused by early exits",
    "checked": true,
    "id": "9b50eb8350fc5c82d4bff1cff26986b7e60fa5cf",
    "semantic_title": "layer-wise alignment: examining safety alignment across image encoder layers in vision language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aFNq67ilos": {
    "title": "Training Dynamics of In-Context Learning in Linear Attention",
    "volume": "spotlight",
    "abstract": "While attention-based models have demonstrated the remarkable ability of in-context learning (ICL), the theoretical understanding of how these models acquired this ability through gradient descent training is still preliminary. Towards answering this question, we study the gradient descent dynamics of multi-head linear self-attention trained for in-context linear regression. We examine two parametrizations of linear self-attention: one with the key and query weights merged as a single matrix (common in theoretical studies), and one with separate key and query matrices (closer to practical settings). For the merged parametrization, we show that the training dynamics has two fixed points and the loss trajectory exhibits a single, abrupt drop. We derive an analytical time-course solution for a certain class of datasets and initialization. For the separate parametrization, we show that the training dynamics has exponentially many fixed points and the loss exhibits saddle-to-saddle dynamics, which we reduce to scalar ordinary differential equations. During training, the model implements principal component regression in context with the number of principal components increasing over training time. Overall, we provide a theoretical description of how ICL abilities evolve during gradient descent training of linear attention, revealing abrupt acquisition or progressive improvements depending on how the key and query are parametrized",
    "checked": true,
    "id": "79b4560fa0542b6c04f037ac8b69e4fe450da093",
    "semantic_title": "training dynamics of in-context learning in linear attention",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=Lktwi30g63": {
    "title": "When and How Does CLIP Enable Domain and Compositional Generalization?",
    "volume": "spotlight",
    "abstract": "The remarkable generalization performance of contrastive vision-language models like CLIP is often attributed to the diversity of their training distributions. However, key questions remain unanswered: Can CLIP generalize to an entirely unseen domain when trained on a diverse mixture of domains (domain generalization)? Can it generalize to unseen classes within partially seen domains (compositional generalization)? What factors affect such generalization? To answer these questions, we trained CLIP models on systematically constructed training distributions with controlled domain diversity and object class exposure. Our experiments show that domain diversity is essential for both domain and compositional generalization, yet compositional generalization can be surprisingly weaker than domain generalization when the training distribution contains a suboptimal subset of the test domain. Through data-centric *and* mechanistic analyses, we find that successful generalization requires the learning of sufficiently shared representations in intermediate layers and circuits",
    "checked": true,
    "id": "3938d4e5effeb0640005fabbe877797f4662ebc8",
    "semantic_title": "when and how does clip enable domain and compositional generalization?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bm706VlAtU": {
    "title": "Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain",
    "volume": "spotlight",
    "abstract": "The diffusion-based adversarial purification methods attempt to drown adversarial perturbations into a part of isotropic noise through the forward process, and then recover the clean images through the reverse process. Due to the lack of distribution information about adversarial perturbations in the pixel domain, it is often unavoidable to damage normal semantics. We turn to the frequency domain perspective, decomposing the image into amplitude spectrum and phase spectrum. We find that for both spectra, the damage caused by adversarial perturbations tends to increase monotonically with frequency. This means that we can extract the content and structural information of the original clean sample from the frequency components that are less damaged. Meanwhile, theoretical analysis indicates that existing purification methods indiscriminately damage all frequency components, leading to excessive damage to the image. Therefore, we propose a purification method that can eliminate adversarial perturbations while maximizing the preservation of the content and structure of the original image. Specifically, at each time step during the reverse process, for the amplitude spectrum, we replace the low-frequency components of the estimated image's amplitude spectrum with the corresponding parts of the adversarial image. For the phase spectrum, we project the phase of the estimated image into a designated range of the adversarial image's phase spectrum, focusing on the low frequencies. Empirical evidence from extensive experiments demonstrates that our method significantly outperforms most current defense methods",
    "checked": true,
    "id": "21bec25b42dbce61200e41fe721131a02ece1e51",
    "semantic_title": "diffusion-based adversarial purification from the perspective of the frequency domain",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Zm2M92TZyO": {
    "title": "Leveraging Diffusion Model as Pseudo-Anomalous Graph Generator for Graph-Level Anomaly Detection",
    "volume": "spotlight",
    "abstract": "A fundamental challenge in graph-level anomaly detection (GLAD) is the scarcity of anomalous graph data, as the training dataset typically contains only normal graphs or very few anomalies. This imbalance hinders the development of robust detection models. In this paper, we propose **A**nomalous **G**raph **Diff**usion (AGDiff), a framework that explores the potential of diffusion models in generating pseudo-anomalous graphs for GLAD. Unlike existing diffusion-based methods that focus on modeling data normality, AGDiff leverages the latent diffusion framework to incorporate subtle perturbations into graph representations, thereby generating pseudo-anomalous graphs that closely resemble normal ones. By jointly training a classifier to distinguish these generated graph anomalies from normal graphs, AGDiff learns more discriminative decision boundaries. The shift from solely modeling normality to explicitly generating and learning from pseudo graph anomalies enables AGDiff to effectively identify complex anomalous patterns that other approaches might overlook. Comprehensive experimental results demonstrate that the proposed AGDiff significantly outperforms several state-of-the-art GLAD baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4HQaMUYWAT": {
    "title": "An Analysis for Reasoning Bias of Language Models with Small Initialization",
    "volume": "spotlight",
    "abstract": "Transformer-based Large Language Models (LLMs) have revolutionized Natural Language Processing by demonstrating exceptional performance across diverse tasks. This study investigates the impact of the parameter initialization scale on the training behavior and task preferences of LLMs. We discover that smaller initialization scales encourage models to favor reasoning tasks, whereas larger initialization scales lead to a preference for memorization tasks. We validate this reasoning bias via real datasets and meticulously designed anchor functions. Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena. Additionally, experiments on real-world language tasks corroborate our theoretical insights. This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models",
    "checked": true,
    "id": "43bb8bdc9ba7e31074e1725cae5eb47842e917ec",
    "semantic_title": "an analysis for reasoning bias of language models with small initialization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=hwTKGdM4TK": {
    "title": "Instance Correlation Graph-based Naive Bayes",
    "volume": "spotlight",
    "abstract": "Due to its simplicity, effectiveness and robustness, naive Bayes (NB) has continued to be one of the top 10 data mining algorithms. To improve its performance, a large number of improved algorithms have been proposed in the last few decades. However, in addition to Gaussian naive Bayes (GNB), there is little work on numerical attributes. At the same time, none of them takes into account the correlations among instances. To fill this gap, we propose a novel algorithm called instance correlation graph-based naive Bayes (ICGNB). Specifically, it first uses original attributes to construct an instance correlation graph (ICG) to represent the correlations among instances. Then, it employs a variational graph auto-encoder (VGAE) to generate new attributes from the constructed ICG and uses them to augment original attributes. Finally, it weights each augmented attribute to alleviate the attribute redundancy and builds GNB on the weighted attributes. The experimental results on tens of datasets show that ICGNB significantly outperforms its deserved competitors.Our codes and datasets are available at https://github.com/jiangliangxiao/ICGNB",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U64wEbM7NB": {
    "title": "Trusted Multi-View Classification with Expert Knowledge Constraints",
    "volume": "spotlight",
    "abstract": "Multi-view classification (MVC) based on the Dempster-Shafer theory has gained significant recognition for its reliability in safety-critical applications. However, existing methods predominantly focus on providing confidence levels for decision outcomes without explaining the reasoning behind these decisions. Moreover, the reliance on first-order statistical magnitudes of belief masses often inadequately capture the intrinsic uncertainty within the evidence. To address these limitations, we propose a novel framework termed Trusted Multi-view Classification Constrained with Expert Knowledge (TMCEK). TMCEK integrates expert knowledge to enhance feature-level interpretability and introduces a distribution-aware subjective opinion mechanism to derive more reliable and realistic confidence estimates. The theoretical superiority of the proposed uncertainty measure over conventional approaches is rigorously established. Extensive experiments conducted on three multi-view datasets for sleep stage classification demonstrate that TMCEK achieves state-of-the-art performance while offering interpretability at both the feature and decision levels. These results position TMCEK as a robust and interpretable solution for MVC in safety-critical domains. The code is available at https://github.com/jie019/TMCEK_ICML2025",
    "checked": false,
    "id": "c5cdf4bee424247ba78604047bc1288cf8384773",
    "semantic_title": "a novel graph-based multi-view spectral clustering: application to x-ray image analysis for covid-19 recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=TmJvacopmV": {
    "title": "Discrepancy Minimization in Input-Sparsity Time",
    "volume": "spotlight",
    "abstract": "A recent work by [Larsen, SODA 2023] introduced a faster combinatorial alternative to Bansal's SDP algorithm for finding a coloring $x \\in \\\\{-1, 1\\\\}^n$ that approximately minimizes the discrepancy $\\mathrm{disc}(A, x) := \\\\| A x \\\\|_{\\infty}$ of a real-valued $m \\times n$ matrix $A$. Larsen's algorithm runs in $\\widetilde{O}(mn^2)$ time compared to Bansal's $\\widetilde{O}(mn^{4.5})$-time algorithm, with a slightly weaker logarithmic approximation ratio in terms of the hereditary discrepancy of $A$ [Bansal, FOCS 2010]. We present a combinatorial $\\widetilde{O}(\\mathrm{nnz}(A) + n^3)$-time algorithm with the same approximation guarantee as Larsen's, optimal for tall matrices where $m = \\mathrm{poly}(n)$. Using a more intricate analysis and fast matrix multiplication, we further achieve a runtime of $\\widetilde{O}(\\mathrm{nnz}(A) + n^{2.53})$, breaking the cubic barrier for square matrices and surpassing the limitations of linear-programming approaches [Eldan and Singh, RS\\&A 2018]. Our algorithm relies on two key ideas: (i) a new sketching technique for finding a projection matrix with a short $\\ell_2$-basis using implicit leverage-score sampling, and (ii) a data structure for efficiently implementing the iterative Edge-Walk partial-coloring algorithm [Lovett and Meka, SICOMP 2015], and using an alternative analysis to enable ``lazy'' batch updates with low-rank corrections. Our results nearly close the computational gap between real-valued and binary matrices, for which input-sparsity time coloring was recently obtained by [Jain, Sah and Sawhney, SODA 2023]",
    "checked": false,
    "id": "bf4342b8bcd39bf730a4b6977a05a1d579f11788",
    "semantic_title": "feedback control balancing quadratic performance and input sparsity",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fPOkujQBVb": {
    "title": "Sharp Generalization for Nonparametric Regression by Over-Parameterized Neural Networks: A Distribution-Free Analysis in Spherical Covariate",
    "volume": "spotlight",
    "abstract": "Sharp generalization bound for neural networks trained by gradient descent (GD) is of central interest in statistical learning theory and deep learning. In this paper, we consider nonparametric regression by an over-parameterized two-layer NN trained by GD. We show that, if the neural network is trained by GD with early stopping, then the trained network renders a sharp rate of the nonparametric regression risk of $O(\\epsilon_n^2)$, which is the same rate as that for the classical kernel regression trained by GD with early stopping, where $\\epsilon_n$ is the critical population rate of the Neural Tangent Kernel (NTK) associated with the network and $n$ is the size of the training data. It is remarked that our result does not require distributional assumptions on the covariate as long as the covariate lies on the unit sphere, in a strong contrast with many existing results which rely on specific distributions such as the spherical uniform data distribution or distributions satisfying certain restrictive conditions. As a special case of our general result, when the eigenvalues of the associated NTK decay at a rate of $\\lambda_j \\asymp j^{-\\frac{d}{d-1}}$ for $j \\ge 1$ which happens under certain distributional assumption such as the training features follow the spherical uniform distribution, we immediately obtain the minimax optimal rate of $O(n^{-\\frac{d}{2d-1}})$, which is the major results of several existing works in this direction. The neural network width in our general result is lower bounded by a function of only $d$ and $\\epsilon_n$, and such width does not depend on the minimum eigenvalue of the empirical NTK matrix whose lower bound usually requires additional assumptions on the training data. Our results are built upon two significant technical results which are of independent interest. First, uniform convergence to the NTK is established during the training process by GD, so that we can have a nice decomposition of the neural network function at any step of the GD into a function in the Reproducing Kernel Hilbert Space associated with the NTK and an error function with a small $L^{\\infty}$-norm. Second, local Rademacher complexity is employed to tightly bound the Rademacher complexity of the function class comprising all the possible neural network functions obtained by GD. Our result formally fills the gap between training a classical kernel regression model and training an over-parameterized but finite-width neural network by GD for nonparametric regression without distributional assumptions about the spherical covariate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S2K5MyRjrL": {
    "title": "Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss",
    "volume": "spotlight",
    "abstract": "Lipschitz neural networks are well-known for providing certified robustness in deep learning. In this paper, we present a novel, efficient Block Reflector Orthogonal (BRO) layer that enhances the capability of orthogonal layers on constructing more expressive Lipschitz neural architectures. In addition, by theoretically analyzing the nature of Lipschitz neural networks, we introduce a new loss function that employs an annealing mechanism to increase margin for most data points. This enables Lipschitz models to provide better certified robustness. By employing our BRO layer and loss function, we design BRONet — a simple yet effective Lipschitz neural network that achieves state-of-the-art certified robustness. Extensive experiments and empirical analysis on CIFAR-10/100, Tiny-ImageNet, and ImageNet validate that our method outperforms existing baselines. The implementation is available at [GitHub Link](https://github.com/ntuaislab/BRONet)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xVBfdltHST": {
    "title": "Relational Invariant Learning for Robust Solvation Free Energy Prediction",
    "volume": "spotlight",
    "abstract": "Predicting the solvation free energy of molecules using graph neural networks holds significant potential for advancing drug discovery and the design of novel materials. While previous methods have demonstrated success on independent and identically distributed (IID) datasets, their performance in out-of-distribution (OOD) scenarios remains largely unexplored. We propose a novel Relational Invariant Learning framework (RILOOD) to enhance OOD generalization in solvation free energy prediction. RILOOD comprises three key components: (i) a mixup-based conditional modeling module that integrates diverse environments, (ii) a novel multi-granularity refinement strategy that extends beyond core substructures to enable context-aware representation learning for capturing multi-level interactions, and (iii) an invariant learning mechanism that identifies robust patterns generalizable to unseen environments. Extensive experiments demonstrate that RILOOD significantly outperforms state-of-the-art methods across various distribution shifts, highlighting its effectiveness in improving solvation free energy prediction under diverse conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GoGuB1yFko": {
    "title": "Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection",
    "volume": "spotlight",
    "abstract": "Out-of-distribution (OOD) detection attempts to distinguish outlier samples to prevent models trained on the in-distribution (ID) dataset from producing unavailable outputs. Most OOD detection methods require many ID samples for training, which seriously limits their real-world applications. To this end, we target a challenging setting: few-shot OOD detection, where only a few labeled ID samples are available. Therefore, few-shot OOD detection is much more challenging than the traditional OOD detection setting. Previous few-shot OOD detection works ignore the distinct diversity between different classes. In this paper, we propose a novel network: Adaptive Multi-prompt Contrastive Network (AMCN), which adapts the ID-OOD separation boundary by learning inter- and intra-class distribution. To compensate for the absence of OOD and scarcity of ID image samples, we leverage CLIP, connecting text with images, engineering learnable ID and OOD textual prompts. Specifically, we first generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts, and label-adaptive OOD prompts). Then, we generate an adaptive class boundary for each class by introducing a class-wise threshold. Finally, we propose a prompt-guided ID-OOD separation module to control the margin between ID and OOD prompts. Experimental results show that AMCN outperforms other state-of-the-art works",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wiQe95BPaB": {
    "title": "FlashTP: Fused, Sparsity-Aware Tensor Product for Machine Learning Interatomic Potentials",
    "volume": "spotlight",
    "abstract": "Machine Learning Interatomic Potentials (MLIPs) enable efficient molecular dynamics (MD) simulations with high accuracy. While equivariant MLIPs achieve state-of-the-art accuracy, they face significant computational bottlenecks centered around their Tensor-Product layer, which account for up to 75\\% of training time and cause substantial memory overhead. We present FlashTP, a highly optimized tensor-product library that addresses these inefficiencies through kernel fusion, sparse computation, and path-aggregated execution. FlashTP achieves up to 41.6$\\times$ and 60.8$\\times$ kernel speedups over _e3nn_ and NVIDIA cuEquivariance, respectively. For SevenNet-l3i5, it delivers 4.2$\\times$ and 3.5$\\times$ speedup while reducing peak memory usage by 6.3$\\times$ and 6.2$\\times$ for inference and training, respectively. The code is available at https://github.com/SNU-ARC/flashTP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GazlTYxZss": {
    "title": "Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems",
    "volume": "spotlight",
    "abstract": "Failure attribution in LLM multi-agent systems—identifying the agent and step responsible for task failures—provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who\\&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who\\&When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5\\% accuracy in identifying failure-responsible agents but only 14.2\\% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available in https://github.com/mingyin1/Agents_Failure_Attribution",
    "checked": true,
    "id": "a0d37ec77dc2acddb223d9a7ac4f23ca10e2908f",
    "semantic_title": "which agent causes task failures and when? on automated failure attribution of llm multi-agent systems",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=Vf9f7eNX6T": {
    "title": "A Closer Look at Multimodal Representation Collapse",
    "volume": "spotlight",
    "abstract": "We aim to develop a fundamental understanding of modality collapse, a recently observed empirical phenomenon wherein models trained for multimodal fusion tend to rely only on a subset of the modalities, ignoring the rest. We show that modality collapse happens when noisy features from one modality are entangled, via a shared set of neurons in the fusion head, with predictive features from another, effectively masking out positive contributions from the predictive features of the former modality and leading to its collapse. We further prove that cross-modal knowledge distillation implicitly disentangles such representations by freeing up rank bottlenecks in the student encoder, denoising the fusion-head outputs without negatively impacting the predictive features from either modality. Based on the above findings, we propose an algorithm that prevents modality collapse through explicit basis reallocation, with applications in dealing with missing modalities. Extensive experiments on multiple multimodal benchmarks validate our theoretical claims. Project page: https://abhrac.github.io/mmcollapse/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jJRkkPr474": {
    "title": "Geometric Hyena Networks for Large-scale Equivariant Learning",
    "volume": "spotlight",
    "abstract": "Processing global geometric context while preserving equivariance is crucial when modeling biological, chemical, and physical systems. Yet, this is challenging due to the computational demands of equivariance and global context at scale. Standard methods such as equivariant self-attention suffer from quadratic complexity, while local methods such as distance-based message passing sacrifice global information. Inspired by the recent success of state-space and long-convolutional models, we introduce Geometric Hyena, the first equivariant long-convolutional model for geometric systems. Geometric Hyena captures global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations. Evaluated on all-atom property prediction of large RNA molecules and full protein molecular dynamics, Geometric Hyena outperforms existing equivariant models while requiring significantly less memory and compute that equivariant self-attention. Notably, our model processes the geometric context of $30k$ tokens $20 \\times$ faster than the equivariant transformer and allows $72 \\times$ longer context within the same budget",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xvLVYrYQ8a": {
    "title": "Covered Forest: Fine-grained generalization analysis of graph neural networks",
    "volume": "spotlight",
    "abstract": "The expressive power of message-passing graph neural networks (MPNNs) is reasonably well understood, primarily through combinatorial techniques from graph isomorphism testing. However, MPNNs' generalization abilities---making meaningful predictions beyond the training set---remain less explored. Current generalization analyses often overlook graph structure, limit the focus to specific aggregation functions, and assume the impractical, hard-to-optimize $0$-$1$ loss function. Here, we extend recent advances in graph similarity theory to assess the influence of graph structure, aggregation, and loss functions on MPNNs' generalization abilities. Our empirical study supports our theoretical insights, improving our understanding of MPNNs' generalization properties",
    "checked": true,
    "id": "6a55ab283433b9dc3d0041ef41b3ad6918319e3c",
    "semantic_title": "covered forest: fine-grained generalization analysis of graph neural networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=OpineZj5bj": {
    "title": "Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zU4VCPHYRC": {
    "title": "On the Tension between Byzantine Robustness and No-Attack Accuracy in Distributed Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=otNB7BzsiR": {
    "title": "Determining Layer-wise Sparsity for Large Language Models Through a Theoretical Perspective",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "4ec69c9404f703cf0decaf8935a66085ca726b27",
    "semantic_title": "determining layer-wise sparsity for large language models through a theoretical perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U8GUmxnzXn": {
    "title": "UnHiPPO: Uncertainty-aware Initialization for State Space Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9df724e66fe2878c32269e4dbd2e5ca43b8548f3",
    "semantic_title": "unhippo: uncertainty-aware initialization for state space models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dqp6IMI3gQ": {
    "title": "When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series",
    "volume": "poster",
    "abstract": "Recently, forecasting future abnormal events has emerged as an important scenario to tackle realworld necessities. However, the solution of predicting specific future time points when anomalies will occur, known as Anomaly Prediction (AP), remains under-explored. Existing methods dealing with time series data fail in AP, focusing only on immediate anomalies or failing to provide precise predictions for future anomalies. To address AP, we propose a novel framework called Anomaly to Prompt (A2P), comprised of Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To enable the forecasting model to forecast abnormal time points, we adopt a strategy to learn the relationships of anomalies. For the robust detection of anomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP) that simulates diverse anomaly patterns using signal-adaptive prompt. Comprehensive experiments on multiple real-world datasets demonstrate the superiority of A2P over state-of-the-art methods, showcasing its ability to predict future anomalies",
    "checked": true,
    "id": "0d2a3475341d8d1d730458f1a327f41365d79941",
    "semantic_title": "when will it fail?: anomaly to prompt for forecasting future anomalies in time series",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GKZySvM2t9": {
    "title": "KGMark: A Diffusion Watermark for Knowledge Graphs",
    "volume": "poster",
    "abstract": "Knowledge graphs (KGs) are ubiquitous in numerous real-world applications, and watermarking facilitates protecting intellectual property and preventing potential harm from AI-generated content. Existing watermarking methods mainly focus on static plain text or image data, while they can hardly be applied to dynamic graphs due to spatial and temporal variations of structured data. This motivates us to propose KGMark, the first graph watermarking framework that aims to generate robust, detectable, and transparent diffusion fingerprints for dynamic KG data. Specifically, we propose a novel clustering-based alignment method to adapt the watermark to spatial variations. Meanwhile, we present a redundant embedding strategy to harden the diffusion watermark against various attacks, facilitating the robustness of the watermark to the temporal variations. Additionally, we introduce a novel learnable mask matrix to improve the transparency of diffusion fingerprints. By doing so, our KGMark properly tackles the variation challenges of structured data. Experiments on various public benchmarks show the effectiveness of our proposed KGMark",
    "checked": true,
    "id": "bddbcad21e434fe7045dfbb4159386eb132d1ae1",
    "semantic_title": "kgmark: a diffusion watermark for knowledge graphs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GdYg0Ohx0k": {
    "title": "LSCD: Lomb--Scargle Conditioned Diffusion for Time series Imputation",
    "volume": "poster",
    "abstract": "Time series with missing or irregularly sampled data are a persistent challenge in machine learning. Many methods operate on the frequency-domain, relying on the Fast Fourier Transform (FFT) which assumes uniform sampling, therefore requiring prior interpolation that can distort the spectra. To address this limitation, we introduce a differentiable Lomb--Scargle layer that enables a reliable computation of the power spectrum of irregularly sampled data. We integrate this layer into a novel score-based diffusion model (LSCD) for time series imputation conditioned on the entire signal spectrum. Experiments on synthetic and real-world benchmarks demonstrate that our method recovers missing data more accurately than purely time-domain baselines, while simultaneously producing consistent frequency estimates. Crucially, our method can be easily integrated into learning frameworks, enabling broader adoption of spectral guidance in machine learning approaches involving incomplete or irregular data",
    "checked": false,
    "id": "37fcb256f2c6b35bb805bc8726baa9d2e5624dd3",
    "semantic_title": "lscd: lomb-scargle conditioned diffusion for time series imputation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q0rKYiVEZq": {
    "title": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection",
    "volume": "poster",
    "abstract": "Jailbreaking techniques trick Large Language Models (LLMs) into producing restricted output, posing a potential threat. One line of defense is to use another LLM as a Judge to evaluate the harmfulness of generated text. However, we reveal that these Judge LLMs are vulnerable to token segmentation bias, an issue that arises when delimiters alter the tokenization process, splitting words into smaller sub-tokens. This alters the embeddings of the entire sequence, reducing detection accuracy and allowing harmful content to be misclassified as safe. In this paper, we introduce Emoji Attack, a novel strategy that amplifies existing jailbreak prompts by exploiting token segmentation bias. Our method leverages in-context learning to systematically insert emojis into text before it is evaluated by a Judge LLM, inducing embedding distortions that significantly lower the likelihood of detecting unsafe content. Unlike traditional delimiters, emojis also introduce semantic ambiguity, making them particularly effective in this attack. Through experiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack substantially reduces the unsafe prediction rate, bypassing existing safeguards",
    "checked": true,
    "id": "5b539b4e1b9f677301ac815d41677fb4ec040f4b",
    "semantic_title": "emoji attack: enhancing jailbreak attacks against judge llm detection",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=rnx11J4hsg": {
    "title": "HiRemate: Hierarchical Approach for Efficient Re-materialization of Neural Networks",
    "volume": "poster",
    "abstract": "Training deep neural networks (DNNs) on memory-limited GPUs is challenging, as storing intermediate activations often exceeds available memory. Re-materialization, a technique that preserves exact computations, addresses this by selectively recomputing activations instead of storing them. However, existing methods either fail to scale, lack generality, or introduce excessive execution overhead. We introduce ${\\mbox{HiRemate}}$ a ${\\textit hierarchical}$ re-materialization framework that recursively partitions large computation graphs, applies optimized solvers at multiple levels, and merges solutions into a global efficient training schedule. This enables scalability to significantly larger graphs than prior ILP-based methods while keeping runtime overhead low. Designed for single-GPU models and activation re-materialization, HiRemate extends the feasibility of training networks with thousands of graph nodes, surpassing prior methods in both efficiency and scalability. Experiments on various types of networks yield up to 50-70% memory reduction with only 10-15% overhead, closely matching optimal solutions while significantly reducing solver time. Seamlessly integrating with PyTorch Autograd, HiRemate requires almost no code change to use, enabling broad adoption in memory-constrained deep learning",
    "checked": false,
    "id": "56d3f524d19fd8ffa70f9d8b3bca8626e9f1332b",
    "semantic_title": "hiremate: hierarchical approach for eﬀicient re-materialization of large neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GByP03IitA": {
    "title": "ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset",
    "volume": "poster",
    "abstract": "Time-series data are critical in diverse applications, such as industrial monitoring, medical diagnostics, and climate research. However, effectively integrating these high-dimensional temporal signals with natural language for dynamic, interactive tasks remains a significant challenge. To address this, we introduce the Time-Series Question Answering (Time-Series QA) task and release EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset designed to capture complex interactions between time-series signals and natural language. Building on this resource, we propose the Instruct Time Transformer (ITFormer), a novel framework that bridges time-series encoders with frozen large language models (LLMs). ITFormer effectively extracts, aligns, and fuses temporal and textual features, achieving a strong improvement in QA accuracy over strong baselines with fewer than 1\\% additional trainable parameters. By combining computational efficiency with robust cross-modal modeling, our work establishes a adaptable paradigm for integrating temporal data with natural language, paving the way for new research and applications in multi-modal AI. More details about the project, including datasets and code, are available at: https://pandalin98.github.io/itformer_site/",
    "checked": true,
    "id": "aad4507629c335db7166857d622cdf331eb4abd3",
    "semantic_title": "itformer: bridging time series and natural language for multi-modal qa with large-scale multitask dataset",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7QFmZ7i7sr": {
    "title": "GPEN: Global Position Encoding Network for Enhanced Subgraph Representation Learning",
    "volume": "poster",
    "abstract": "Subgraph representation learning has attracted growing interest due to its wide applications in various domains. However, existing methods primarily focus on local neighborhood structures while overlooking the significant impact of global structural information, in particular the influence of multi-hop neighbors beyond immediate neighborhoods. This presents two key challenges: how to effectively capture the structural relationships between distant nodes, and how to prevent excessive aggregation of global structural information from weakening the discriminative ability of subgraph representations. To address these challenges, we propose GPEN (Global Position Encoding Network). GPEN leverages a hierarchical tree structure to encode each node's global position based on its path distance to the root node, enabling a systematic way to capture relationships between distant nodes. Furthermore, we introduce a boundary-aware convolution module that selectively integrates global structural information while maintaining the unique structural patterns of each subgraph. Extensive experiments on eight public datasets identify that GPEN significantly outperforms state-of-the-art methods in subgraph representation learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w2QNIkcwWw": {
    "title": "Fast Min-$\\epsilon$ Segmented Regression using Constant-Time Segment Merging",
    "volume": "poster",
    "abstract": "Segmented regression is a statistical method that approximates a function $f$ by a piecewise function $\\hat{f}$ using noisy data samples. *Min-$\\epsilon$* approaches aim to reduce the regression function's mean squared error (MSE) for a given number of $k$ segments. An optimal solution for *min-$\\epsilon$* segmented regression is found in $\\mathcal{O}(n^2)$ time (Bai & Perron, 1998; Yamamoto & Perron, 2013) for $n$ samples. For large datasets, current heuristics improve time complexity to $\\mathcal{O}(n\\log{n})$ (Acharya et al., 2016) but can result in large errors, especially when exactly $k$ segments are used. We present a method for *min-$\\epsilon$* segmented regression that combines the scalability of top existing heuristic solutions with a statistical efficiency similar to the optimal solution. This is achieved by using a new method to merge an initial set of segments using precomputed matrices from samples, allowing both merging and error calculation in constant time. Our approach, using the same samples and parameter $k$, produces segments with up to 1,000 times lower MSE compared to Acharya et al. (2016) in about 100 times less runtime on data sets over $10^4$ samples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1OUEnfusEd": {
    "title": "How Compositional Generalization and Creativity Improve as Diffusion Models are Trained",
    "volume": "poster",
    "abstract": "Natural data is often organized as a hierarchical composition of features. How many samples do generative models need in order to learn the composition rules, so as to produce a combinatorially large number of novel data? What signal in the data is exploited to learn those rules? We investigate these questions in the context of diffusion models both theoretically and empirically. Theoretically, we consider a simple probabilistic context-free grammar - a tree-like graphical model used to represent the hierarchical and compositional structure of data such as language and images. We demonstrate that diffusion models learn the grammar's composition rules with the sample complexity required for clustering features with statistically similar context, a process similar to the word2vec algorithm. However, this clustering emerges hierarchically: higher-level features associated with longer contexts require more data to be identified. This mechanism leads to a sample complexity that scales polynomially with the said context size. As a result, diffusion models trained on an intermediate dataset size generate data coherent up to a certain scale, but lacking global coherence. We test these predictions across different domains and find remarkable agreement: both generated texts and images achieve progressively larger coherence lengths as the training time or dataset size grows. We discuss connections between the hierarchical clustering mechanism we introduce here and the renormalization group in physics",
    "checked": true,
    "id": "39836e0bbfe84914bcc3465dbc92bafa4f092578",
    "semantic_title": "how compositional generalization and creativity improve as diffusion models are trained",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=P0wSGDoip1": {
    "title": "Gradient-based Explanations for Deep Learning Survival Models",
    "volume": "poster",
    "abstract": "Deep learning survival models often outperform classical methods in time-to-event predictions, particularly in personalized medicine, but their \"black box\" nature hinders broader adoption. We propose a framework for gradient-based explanation methods tailored to survival neural networks, extending their use beyond regression and classification. We analyze the implications of their theoretical assumptions for time-dependent explanations in the survival setting and propose effective visualizations incorporating the temporal dimension. Experiments on synthetic data show that gradient-based methods capture the magnitude and direction of local and global feature effects, including time dependencies. We introduce GradSHAP(t), a gradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and SurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply these methods to medical data with multi-modal inputs, revealing relevant tabular features and visual patterns, as well as their temporal dynamics",
    "checked": true,
    "id": "dbb70b8f0f82056091f8670d8b2492088343d3ae",
    "semantic_title": "gradient-based explanations for deep learning survival models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=9Kywz2fO26": {
    "title": "Pairwise Maximum Likelihood For Multi-Class Logistic Regression Model With Multiple Rare Classes",
    "volume": "poster",
    "abstract": "We study in this work the problem of multi-class logistic regression with one major class and multiple rare classes, which is motivated by a real application in TikTok live stream data. The model is inspired by the two-class logistic regression model of Wang (2020) but with surprising theoretical findings, which in turn motivate new estimation methods with excellent statistical and computational efficiency. Specifically, since rigorous theoretical analysis suggests that the resulting maximum likelihood estimators of different rare classes should be asymptotically independent, we consider to solve multiple pairwise two-class logistic regression problems instead of optimizing the joint log-likelihood function with computational challenge in multi-class problem, which are computationally much easier and can be conducted in a fully parallel way. To further reduce the computation cost, a subsample-based pairwise likelihood estimator is developed by down-sampling the major class. We show rigorously that the resulting estimators could be as asymptotically efficient as the global maximum likelihood estimator under appropriate regularity conditions. Extensive simulation studies are presented to support our theoretical findings and a TikTok live stream dataset is analyzed for illustration purpose",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DkRYImuQA9": {
    "title": "ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning",
    "volume": "poster",
    "abstract": "Autonomous agents powered by foundation models have seen widespread adoption across various real-world applications. However, they remain highly vulnerable to malicious instructions and attacks, which can result in severe consequences such as privacy breaches and financial losses. More critically, existing guardrails for LLMs are not applicable due to the complex and dynamic nature of agents. To tackle these challenges, we propose ShieldAgent, the first guardrail agent designed to enforce explicit safety policy compliance for the action trajectory of other protected agents through logical reasoning. Specifically, ShieldAgent first constructs a safety policy model by extracting verifiable rules from policy documents and structuring them into a set of action-based probabilistic rule circuits. Given the action trajectory of the protected agent, ShieldAgent retrieves relevant rule circuits and generates a shielding plan, leveraging its comprehensive tool library and executable code for formal verification. In addition, given the lack of guardrail benchmarks for agents, we introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agent instructions and action trajectories, collected via SOTA attacks across 6 web environments and 7 risk categories. Experiments show that ShieldAgent achieves SOTA on ShieldAgent-Bench and three existing benchmarks, outperforming prior methods by 11.3% on average with a high recall of 90.1%. Additionally, ShieldAgent reduces API queries by 64.7% and inference time by 58.2%, demonstrating its high precision and efficiency in safeguarding agents. Our project is available and continuously maintained here: https://shieldagent-aiguard.github.io/",
    "checked": true,
    "id": "1b330d0ba2cc6edfa911032340345b75a6248f26",
    "semantic_title": "shieldagent: shielding agents via verifiable safety policy reasoning",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=Uc0dTE2Wox": {
    "title": "Rethinking Benign Overfitting in Two-Layer Neural Networks",
    "volume": "poster",
    "abstract": "Recent theoretical studies (Kou et al., 2023; Cao et al., 2022) revealed a sharp phase transition from benign to harmful overfitting when the noise-to-feature ratio exceeds a threshold—a situation common in long-tailed data distributions where atypical data is prevalent. However, such harmful overfitting rarely happens in overparameterized neural networks. Further experimental results suggested that memorization is necessary for achieving near-optimal generalization error in long-tailed data distributions (Feldman & Zhang, 2020). We argue that this discrepancy between theoretical predictions and empirical observations arises because previous feature-noise data models overlook the heterogeneous nature of noise across different data classes. In this paper, we refine the feature-noise data model by incorporating class-dependent heterogeneous noise and re-examine the overfitting phenomenon in neural networks. Through a comprehensive analysis of the training dynamics, we establish test loss bounds for the refined model. Our findings reveal that neural networks can leverage \"data noise\" to learn implicit features that improve the classification accuracy for long-tailed data. Our analysis also provides a training-free metric for evaluating data influence on test performance. Experimental validation on both synthetic and real-world datasets supports our theoretical results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JD4eHocSPi": {
    "title": "Symmetry-Aware GFlowNets",
    "volume": "poster",
    "abstract": "Generative Flow Networks (GFlowNets) offer a powerful framework for sampling graphs in proportion to their rewards. However, existing approaches suffer from systematic biases due to inaccuracies in state transition probability computations. These biases, rooted in the inherent symmetries of graphs, impact both atom-based and fragment-based generation schemes. To address this challenge, we introduce Symmetry-Aware GFlowNets (SA-GFN), a method that incorporates symmetry corrections into the learning process through reward scaling. By integrating bias correction directly into the reward structure, SA-GFN eliminates the need for explicit state transition computations. Empirical results show that SA-GFN enables unbiased sampling while enhancing diversity and consistently generating high-reward graphs that closely match the target distribution",
    "checked": true,
    "id": "97eae068742785b1ea1ceb6e296769df5e4900d5",
    "semantic_title": "symmetry-aware gflownets",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7J1kwZY72h": {
    "title": "Kinetic Langevin Diffusion for Crystalline Materials Generation",
    "volume": "poster",
    "abstract": "Generative modeling of crystalline materials using diffusion models presents a series of challenges: the data distribution is characterized by inherent symmetries and involves multiple modalities, with some defined on specific manifolds. Notably, the treatment of fractional coordinates representing atomic positions in the unit cell requires careful consideration, as they lie on a hypertorus. In this work, we introduce Kinetic Langevin Diffusion for Materials (KLDM), a novel diffusion model for crystalline materials generation, where the key innovation resides in the modeling of the coordinates. Instead of resorting to Riemannian diffusion on the hypertorus directly, we generalize Trivialized Diffusion Model (TDM) to account for the symmetries inherent to crystals. By coupling coordinates with auxiliary Euclidean variables representing velocities, the diffusion process is now offset to a flat space. This allows us to effectively perform diffusion on the hypertorus while providing a training objective that accounts for the periodic translation symmetry of the true data distribution. We evaluate KLDM on both Crystal Structure Prediction (CSP) and De-novo Generation (DNG) tasks, demonstrating its competitive performance with current state-of-the-art models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FSlTEObdLl": {
    "title": "CombiMOTS: Combinatorial Multi-Objective Tree Search for Dual-Target Molecule Generation",
    "volume": "poster",
    "abstract": "Dual-target molecule generation, which focuses on discovering compounds capable of interacting with two target proteins, has garnered significant attention due to its potential for improving therapeutic efficiency, safety and resistance mitigation. Existing approaches face two critical challenges. First, by simplifying the complex dual-target optimization problem to scalarized combinations of individual objectives, they fail to capture important trade-offs between target engagement and molecular properties. Second, they typically do not integrate synthetic planning into the generative process. This highlights a need for more appropriate objective function design and synthesis-aware methodologies tailored to the dual-target molecule generation task. In this work, we propose CombiMOTS, a Pareto Monte Carlo Tree Search (PMCTS) framework that generates dual-target molecules. CombiMOTS is designed to explore a synthesizable fragment space while employing vectorized optimization constraints to encapsulate target affinity and physicochemical properties. Extensive experiments on real-world databases demonstrate that CombiMOTS produces novel dual-target molecules with high docking scores, enhanced diversity, and balanced pharmacological characteristics, showcasing its potential as a powerful tool for dual-target drug discovery. The code and data is accessible through \\url{https://github.com/Tibogoss/CombiMOTS}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NWKjVzkDzg": {
    "title": "Scalable Meta-Learning via Mixed-Mode Differentiation",
    "volume": "poster",
    "abstract": "Gradient-based bilevel optimisation is a powerful technique with applications in hyperparameter optimisation, task adaptation, algorithm discovery, meta-learning more broadly, and beyond. It often requires differentiating through the gradient-based optimisation process itself, leading to \"gradient-of-a-gradient\" calculations with computationally expensive second-order and mixed derivatives. While modern automatic differentiation libraries provide a convenient way to write programs for calculating these derivatives, they oftentimes cannot fully exploit the specific structure of these problems out-of-the-box, leading to suboptimal performance. In this paper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or MixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to construct more efficient and scalable computational graphs yielding over 10x memory and up to 25\\% wall-clock time improvements over standard implementations in modern meta-learning setups",
    "checked": true,
    "id": "ddd469ea8dc80a540a6dc2541dbea1334efc9a8f",
    "semantic_title": "scalable meta-learning via mixed-mode differentiation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=RO5OGOzs6M": {
    "title": "PINNsAgent: Automated PDE Surrogation with Large Language Models",
    "volume": "poster",
    "abstract": "Solving partial differential equations (PDEs) using neural methods has been a long-standing scientific and engineering research pursuit. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative to traditional numerical methods for solving PDEs. However, the gap between domain-specific knowledge and deep learning expertise often limits the practical application of PINNs. Previous works typically involve manually conducting extensive PINNs experiments and summarizing heuristic rules for hyperparameter tuning. In this work, we introduce PINNsAgent, a novel surrogation framework that leverages large language models (LLMs) to bridge the gap between domain-specific knowledge and deep learning. PINNsAgent integrates Physics-Guided Knowledge Replay (PGKR) for efficient knowledge transfer from solved PDEs to similar problems, and Memory Tree Reasoning for exploring the search space of optimal PINNs architectures. We evaluate PINNsAgent on 14 benchmark PDEs, demonstrating its effectiveness in automating the surrogation process and significantly improving the accuracy of PINNs-based solutions",
    "checked": true,
    "id": "925b7e67f2705c7188791dc4bc1c8721a8df2f5a",
    "semantic_title": "pinnsagent: automated pde surrogation with large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=rkHCHI5H5W": {
    "title": "Compositional Generalization via Forced Rendering of Disentangled Latents",
    "volume": "poster",
    "abstract": "Composition—the ability to generate myriad variations from finite means—is believed to underlie powerful generalization. However, compositional generalization remains a key challenge for deep learning. A widely held assumption is that learning disentangled (factorized) representations naturally supports this kind of extrapolation. Yet, empirical results are mixed, with many generative models failing to recognize and compose factors to generate out-of-distribution (OOD) samples. In this work, we investigate a controlled 2D Gaussian \"bump\" generation task with fully disentangled $(x,y)$ inputs, demonstrating that standard generative architectures still fail in OOD regions when training with partial data, by re-entangling latent representations in subsequent layers. By examining the model's learned kernels and manifold geometry, we show that this failure reflects a \"memorization\" strategy for generation via data superposition rather than via composition of the true factorized features. We show that when models are forced—through architectural modifications with regularization or curated training data—to render the disentangled latents into the full-dimensional representational (pixel) space, they can be highly data-efficient and effective at composing in OOD regions. These findings underscore that disentangled latents in an abstract representation are insufficient and show that if models can represent disentangled factors directly in the output representational space, it can achieve robust compositional generalization",
    "checked": true,
    "id": "54e1a7e340f91153002b5f424cbcee571e49e661",
    "semantic_title": "compositional generalization via forced rendering of disentangled latents",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aqZKgwf7Cc": {
    "title": "Time to Spike? Understanding the Representational Power of Spiking Neural Networks in Discrete Time",
    "volume": "poster",
    "abstract": "Recent years have seen significant progress in developing spiking neural networks (SNNs) as a potential solution to the energy challenges posed by conventional artificial neural networks (ANNs). However, our theoretical understanding of SNNs remains relatively limited compared to the ever-growing body of literature on ANNs. In this paper, we study a discrete-time model of SNNs based on leaky integrate-and-fire (LIF) neurons, referred to as discrete-time LIF-SNNs, a widely used framework that still lacks solid theoretical foundations. We demonstrate that discrete-time LIF-SNNs realize piecewise constant functions defined on polyhedral regions, and more importantly, we quantify the network size required to approximate continuous functions. Moreover, we investigate the impact of latency (number of time steps) and depth (number of layers) on the complexity of the input space partitioning induced by discrete-time LIF-SNNs. Our analysis highlights the importance of latency and contrasts these networks with ANNs that use piecewise linear activation functions. Finally, we present numerical experiments to support our theoretical findings",
    "checked": true,
    "id": "a62b50733083c4626f773e81c1a243ca45fe721e",
    "semantic_title": "time to spike? understanding the representational power of spiking neural networks in discrete time",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l5KpQ5MmaD": {
    "title": "Scalable Non-Equivariant 3D Molecule Generation via Rotational Alignment",
    "volume": "poster",
    "abstract": "Equivariant diffusion models have achieved impressive performance in 3D molecule generation. These models incorporate Euclidean symmetries of 3D molecules by utilizing an SE(3)-equivariant denoising network. However, specialized equivariant architectures limit the scalability and efficiency of diffusion models. In this paper, we propose an approach that relaxes such equivariance constraints. Specifically, our approach learns a sample-dependent SO(3) transformation for each molecule to construct an aligned latent space. A non-equivariant diffusion model is then trained over the aligned representations. Experimental results demonstrate that our approach performs significantly better than previously reported non-equivariant models. It yields sample quality comparable to state-of-the-art equivariant diffusion models and offers improved training and sampling efficiency. Our code is available at: https://github.com/skeletondyh/RADM",
    "checked": true,
    "id": "10fbe372ec5c72711ab3b86b282060494ae39c62",
    "semantic_title": "scalable non-equivariant 3d molecule generation via rotational alignment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SOwcmZ91Sl": {
    "title": "Learning Distances from Data with Normalizing Flows and Score Matching",
    "volume": "poster",
    "abstract": "Density-based distances (DBDs) provide a principled approach to metric learning by defining distances in terms of the underlying data distribution. By employing a Riemannian metric that increases in regions of low probability density, shortest paths naturally follow the data manifold. Fermat distances, a specific type of DBD, have attractive properties, but existing estimators based on nearest neighbor graphs suffer from poor convergence due to inaccurate density estimates. Moreover, graph-based methods scale poorly to high dimensions, as the proposed geodesics are often insufficiently smooth. We address these challenges in two key ways. First, we learn densities using normalizing flows. Second, we refine geodesics through relaxation, guided by a learned score model. Additionally, we introduce a dimension-adapted Fermat distance that scales intuitively to high dimensions and improves numerical stability. Our work paves the way for the practical use of density-based distances, especially in high-dimensional spaces",
    "checked": true,
    "id": "af4585da68fe51efd2bf43d78bffaaa2ecb21b8e",
    "semantic_title": "learning distances from data with normalizing flows and score matching",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=edhBkkYS8R": {
    "title": "The Importance of Being Lazy: Scaling Limits of Continual Learning",
    "volume": "poster",
    "abstract": "Despite recent efforts, neural networks still struggle to learn in non-stationary environments, and our understanding of catastrophic forgetting (CF) is far from complete. In this work, we perform a systematic study on the impact of model scale and the degree of feature learning in continual learning. We reconcile existing contradictory observations on scale in the literature, by differentiating between *lazy* and *rich* training regimes through a variable parameterization of the architecture. We show that increasing model width is only beneficial when it reduces the amount of *feature learning*, yielding more laziness. Using the framework of dynamical mean field theory, we then study the infinite width dynamics of the model in the feature learning regime and characterize CF, extending prior theoretical results limited to the lazy regime. We study the intricate relationship between feature learning, task non-stationarity, and forgetting, finding that high feature learning is only beneficial with highly similar tasks. We identify a transition modulated by task similarity where the model exits an effectively lazy regime with low forgetting to enter a rich regime with significant forgetting. Finally, our findings reveal that neural networks achieve optimal performance at a critical level of feature learning, which depends on task non-stationarity and *transfers across model scales*. This work provides a unified perspective on the role of scale and feature learning in continual learning",
    "checked": true,
    "id": "f07bcff4e67611e1872bc7b026e1c0a43c2bef28",
    "semantic_title": "the importance of being lazy: scaling limits of continual learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YUtJsxQjv3": {
    "title": "EcoMapper: Generative Modeling for Climate-Aware Satellite Imagery",
    "volume": "poster",
    "abstract": "Satellite imagery is essential for Earth observation, enabling applications like crop yield prediction, environmental monitoring, and climate change assessment. However, integrating satellite imagery with climate data remains a challenge, limiting its utility for forecasting and scenario analysis. We introduce a novel dataset of 2.9 million Sentinel-2 images spanning 15 land cover types with corresponding climate records, forming the foundation for two satellite image generation approaches using fine-tuned Stable Diffusion 3 models. The first is a text-to-image generation model that uses textual prompts with climate and land cover details to produce realistic synthetic imagery for specific regions. The second leverages ControlNet for multi-conditional image generation, preserving spatial structures while mapping climate data or generating time-series to simulate landscape evolution. By combining synthetic image generation with climate and land cover data, our work advances generative modeling in remote sensing, offering realistic inputs for environmental forecasting and new possibilities for climate adaptation and geospatial analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=umT6rMf1Rm": {
    "title": "DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts",
    "volume": "poster",
    "abstract": "The proliferation of disinformation demands reliable and scalable fact-checking solutions. We present **D**ynamic **E**vidence-based **FA**ct-checking with **M**ultimodal **E**xperts (DEFAME), a modular, zero-shot MLLM pipeline for open-domain, text-image claim verification. DEFAME operates in a six-stage process, dynamically selecting the tools and search depth to extract and evaluate textual and visual evidence. Unlike prior approaches that are text-only, lack explainability, or rely solely on parametric knowledge, DEFAME performs end-to-end verification, accounting for images in claims *and* evidence while generating structured, multimodal reports. Evaluation on the popular benchmarks VERITE, AVeriTeC, and MOCHEG shows that DEFAME surpasses all previous methods, establishing itself as the new general state-of-the-art fact-checking system for uni- and multimodal fact-checking. Moreover, we introduce a new multimodal benchmark, ClaimReview2024+, featuring claims after the knowledge cutoff of GPT-4o, avoiding data leakage. Here, DEFAME drastically outperforms the GPT-4o baselines, showing temporal generalizability and the potential for real-time fact-checking",
    "checked": true,
    "id": "5fcd69259de58e354e9927372c37053337e14225",
    "semantic_title": "defame: dynamic evidence-based fact-checking with multimodal experts",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=Au9rfI6Fjd": {
    "title": "Generalization of noisy SGD in unbounded non-convex settings",
    "volume": "poster",
    "abstract": "We study generalization of iterative noisy gradient schemes on smooth non-convex losses. Formally, we establish time-independent information theoretic generalization bounds for Stochastic Gradient Langevin Dynamics (SGLD) that do not diverge as the iteration count increases. Our bounds are obtained through a stability argument: we analyze the difference between two SGLD sequences ran in parallel on two datasets sampled from the same distribution. Our result only requires an isoperimetric inequality to hold, which is merely a restriction on the tails of the loss. Our work relaxes the assumptions of prior work to establish that the iterates stay within a bounded KL divergence from each other. Under an additional dissipativity assumption, we show that the stronger Renyi divergence also stays bounded by establishing a uniform log-Sobolev constant of the iterates. Without dissipativity, we sidestep the need for local log-Sobolev inequalities and instead exploit the regularizing properties of Gaussian convolution. These techniques allow us to show that strong convexity is not necessary for finite stability bounds. Our work shows that noisy SGD can have finite, iteration-independent, generalization and differential privacy bounds in unbounded non-convex settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ECayXPDoha": {
    "title": "Statistical Hypothesis Testing for Auditing Robustness in Language Models",
    "volume": "poster",
    "abstract": "Consider the problem of testing whether the outputs of a large language model (LLM) system change under an arbitrary intervention, such as an input perturbation or changing the model variant. We cannot simply compare two LLM outputs since they might differ due to the stochastic nature of the system, nor can we compare the entire output distribution due to computational intractability. While existing methods for analyzing text-based outputs exist, they focus on fundamentally different problems, such as measuring bias or fairness. To this end, we introduce distribution-based perturbation analysis, a framework that reformulates LLM perturbation analysis as a frequentist hypothesis testing problem. We construct empirical null and alternative output distributions within a low-dimensional semantic similarity space via Monte Carlo sampling, enabling tractable inference without restrictive distributional assumptions. The framework is (i) model-agnostic, (ii) supports the evaluation of arbitrary input perturbations on any black-box LLM, (iii) yields interpretable p-values; (iv) supports multiple perturbations via controlled error rates; and (v) provides scalar effect sizes. We demonstrate the usefulness of the framework across multiple case studies, showing how we can quantify response changes, measure true/false positive rates, and evaluate alignment with reference models. Above all, we see this as a reliable frequentist hypothesis testing framework for LLM auditing",
    "checked": true,
    "id": "902ff11715b0238a682c96c7cda60451cfab1c24",
    "semantic_title": "statistical hypothesis testing for auditing robustness in language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Hd1lh52Fi": {
    "title": "SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic Differential Equations",
    "volume": "poster",
    "abstract": "The Latent Stochastic Differential Equation (SDE) is a powerful tool for time series and sequence modeling. However, training Latent SDEs typically relies on adjoint sensitivity methods, which depend on simulation and backpropagation through approximate SDE solutions, which limit scalability. In this work, we propose SDE Matching, a new simulation-free method for training Latent SDEs. Inspired by modern Score- and Flow Matching algorithms for learning generative dynamics, we extend these ideas to the domain of stochastic dynamics for time series modeling, eliminating the need for costly numerical simulations. Our results demonstrate that SDE Matching achieves performance comparable to adjoint sensitivity methods while drastically reducing computational complexity",
    "checked": true,
    "id": "9365708b97a62f7ad0c0b1b2818e641392a37ff7",
    "semantic_title": "sde matching: scalable and simulation-free training of latent stochastic differential equations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DTdtM53iag": {
    "title": "An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability",
    "volume": "poster",
    "abstract": "The advancements in Multimodal Large Language Models (MLLMs) have enabled various multimodal tasks to be addressed under a zero-shot paradigm. This paradigm sidesteps the cost of model fine-tuning, emerging as a dominant trend in practical application. Nevertheless, Multimodal Sentiment Analysis (MSA), a pivotal challenge in the quest for general artificial intelligence, fails to accommodate this convenience. The zero-shot paradigm exhibits undesirable performance on MSA, casting doubt on whether MLLMs can perceive sentiments as competent as supervised models. By extending the zero-shot paradigm to In-Context Learning (ICL) and conducting an in-depth study on configuring demonstrations, we validate that MLLMs indeed possess such capability. Specifically, three key factors that cover demonstrations' retrieval, presentation, and distribution are comprehensively investigated and optimized. A sentimental predictive bias inherent in MLLMs is also discovered and later effectively counteracted. By complementing each other, the devised strategies for three factors result in average accuracy improvements of 15.9% on six MSA datasets against the zero-shot paradigm and 11.2% against the random ICL baseline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G80YGyxzv7": {
    "title": "Beyond Cropped Regions: New Benchmark and Corresponding Baseline for Chinese Scene Text Retrieval in Diverse Layouts",
    "volume": "poster",
    "abstract": "Chinese scene text retrieval is a practical task that aims to search for images containing visual instances of a Chinese query text. This task is extremely challenging because Chinese text often features complex and diverse layouts in real-world scenes. Current efforts tend to inherit the solution for English scene text retrieval, failing to achieve satisfactory performance. In this paper, we establish a Diversified Layout benchmark for Chinese Street View Text Retrieval (DL-CSVTR), which is specifically designed to evaluate retrieval performance across various text layouts, including vertical, cross-line, and partial alignments. To address the limitations in existing methods, we propose Chinese Scene Text Retrieval CLIP (CSTR-CLIP), a novel model that integrates global visual information with multi-granularity alignment training. CSTR-CLIP applies a two-stage training process to overcome previous limitations, such as the exclusion of visual features outside the text region and reliance on single-granularity alignment, thereby enabling the model to effectively handle diverse text layouts. Experiments on existing benchmark show that CSTR-CLIP outperforms the previous state-of-the-art model by 18.82% accuracy and also provides faster inference speed. Further analysis on DL-CSVTR confirms the superior performance of CSTR-CLIP in handling various text layouts. The dataset and code will be publicly available to facilitate research in Chinese scene text retrieval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cEKrGCFXPA": {
    "title": "Controlling Large Language Model with Latent Action",
    "volume": "poster",
    "abstract": "Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement Learning (RL) has proven to be an effective approach. However, LLMs do not inherently define the structure of an agent for RL training, particularly in terms of specifying the action space. This paper studies learning a compact latent action space to enhance the controllability and exploration of RL for LLMs. Inspired by reinforcement learning from observations, we propose **Co**ntrolling Large Language Models with **L**atent **A**ctions **CoLA**, a framework that integrates a latent action space into pre-trained LLMs. **CoLA** employs an \\emph{inverse dynamics model} to extract latent actions conditioned on future tokens, ensuring that the next token prediction is partially influenced by these actions. Simultaneously, **CoLA** fine-tunes the pre-trained LLM to function as a \\emph{language world model}, capable of incorporating latent actions as inputs. Additionally, **CoLA** trains a \\emph{policy model} to generate actions within this language world model. The policy model can be trained via behavior cloning to mimic a standard language model or through RL to maximize task-specific rewards. In this work, we apply **CoLA** to the Llama-3.1-8B model. Our experiments demonstrate that, compared to RL with token-level actions, **CoLA**'s latent actions enable greater semantic diversity. For enhancing downstream tasks, we show that **CoLA** with RL achieves a score of 42.4 on the \\emph{math500} benchmark, surpassing the baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo Tree Search variant. Furthermore, **CoLA** with RL consistently improves performance on agent-based tasks without degrading the pre-trained LLM's capabilities, unlike the baseline. Finally, **CoLA** reduces computation time by half in tasks involving enhanced thinking prompts for LLMs via RL. These results highlight **CoLA**'s potential to advance RL-based adaptation of LLMs for downstream applications. The CoLA model is available at \\url{https://huggingface.co/LAMDA-RL/Llama-3.1-CoLA-10B}",
    "checked": false,
    "id": "bc6016c22b3ccdc838adaf0963d04bdc4ec654b4",
    "semantic_title": "controlling large language model with latent actions",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=gmdElnwBxt": {
    "title": "Neutral residues: revisiting adapters for model extension",
    "volume": "poster",
    "abstract": "We address the problem of extending a pre-trained large language model to a new domain that was not seen during training. Standard techniques, such as fine-tuning or low-rank adaptation (LoRA) are successful at domain adaptation, but do not formally add capacity to the model. This often leads to a trade-off, between performing well on the new domain vs. degrading performance on the original domain. Here, we propose to revisit and improve adapters to extend LLMs. Our paper analyzes this extension problem from three angles: data, architecture and training procedure, which are advantageously considered jointly. The resulting method, called neutral residues, modifies adapters in a way that leads to each new residual block to output near-zeros on the original domain. This solution leads to strong results when adapting a state-of-the-art model originally trained on English to a new language. Neutral residues significantly outperforms competing approaches such as fine-tuning, LoRA or vanilla adapters in terms of the trade-off between learning the new language and not forgetting English",
    "checked": true,
    "id": "a34cff21849239689c125ff0ba876fad0a4c6dbe",
    "semantic_title": "neutral residues: revisiting adapters for model extension",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=viXwXCkA7N": {
    "title": "Certification for Differentially Private Prediction in Gradient-Based Training",
    "volume": "poster",
    "abstract": "We study private prediction where differential privacy is achieved by adding noise to the outputs of a non-private model. Existing methods rely on noise proportional to the global sensitivity of the model, often resulting in sub-optimal privacy-utility trade-offs compared to private training. We introduce a novel approach for computing dataset-specific upper bounds on prediction sensitivity by leveraging convex relaxation and bound propagation techniques. By combining these bounds with the smooth sensitivity mechanism, we significantly improve the privacy analysis of private prediction compared to global sensitivity-based approaches. Experimental results across real-world datasets in medical image classification and natural language processing demonstrate that our sensitivity bounds are can be orders of magnitude tighter than global sensitivity. Our approach provides a strong basis for the development of novel privacy preserving technologies",
    "checked": true,
    "id": "04131ccd6645c45ceb6814653a7c58f87f6d84fe",
    "semantic_title": "certification for differentially private prediction in gradient-based training",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=X21P8etjWL": {
    "title": "Measuring In-Context Computation Complexity via Hidden State Prediction",
    "volume": "poster",
    "abstract": "Detecting when a neural sequence model does \"interesting\" computation is an open problem. The next token prediction loss is a poor indicator: Low loss can stem from trivially predictable sequences that are uninteresting, while high loss may reflect unpredictable but also irrelevant information that can be ignored by the model. We propose a better metric: measuring the model's ability to predict its own future hidden states. We show empirically that this metric–in contrast to the next token prediction loss–correlates with the intuitive interestingness of the task. To measure predictability, we introduce the architecture-agnostic \"prediction of hidden states\" (PHi) layer that serves as an information bottleneck on the main pathway of the network (e.g., the residual stream in Transformers). We propose a novel learned predictive prior that enables us to measure the novel information gained in each computation step, which serves as our metric. We show empirically that our metric predicts the description length of formal languages learned in-context, the complexity of mathematical reasoning problems, and the correctness of self-generated reasoning chains",
    "checked": true,
    "id": "8164bf25f61512ad310ca6373f8c343187a57e06",
    "semantic_title": "measuring in-context computation complexity via hidden state prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zFR5aWGaUs": {
    "title": "Let LLM Tell What to Prune and How Much to Prune",
    "volume": "poster",
    "abstract": "Large language models (LLMs) have revolutionized various AI applications. However, their billions of parameters pose significant challenges for practical deployment. Structured pruning is a hardware-friendly compression technique and receives widespread attention. Nonetheless, existing literature typically targets a single structure of LLMs. We observe that the structure units of LLMs differ in terms of inference cost and functionality. Therefore, pruning a single structure unit in isolation often results in an imbalance between performance and efficiency. In addition, previous works mainly employ a prescribed pruning ratio. Since the significance of LLM modules may vary, it is ideal to distribute the pruning load to a specific structure unit according to its role within LLMs. To address the two issues, we propose a pruning method that targets multiple LLM modules with dynamic pruning ratios. Specifically, we find the intrinsic properties of LLMs can guide us to determine the importance of each module and thus distribute the pruning load on demand, i.e., what to prune and how much to prune. This is achieved by quantifying the complex interactions within LLMs. Extensive experiments on multiple benchmarks and LLM variants demonstrate that our method effectively balances the trade-off between efficiency and performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dd7Qo7TJpf": {
    "title": "Multilayer Matrix Factorization via Dimension-Reducing Diffusion Variational Inference",
    "volume": "poster",
    "abstract": "Multilayer matrix factorization (MMF) has recently emerged as a generalized model of, and potentially a more expressive approach than, the classic matrix factorization. This paper considers MMF under a probabilistic formulation, and our focus is on inference methods under variational inference. The challenge in this context lies in determining a variational process that leads to a computationally efficient and accurate approximation of the maximum likelihood inference. One well-known example is the variational autoencoder (VAE), which uses neural networks for the variational process. In this work, we take insight from variational diffusion models in the context of generative models to develop variational inference for MMF. We propose a dimension-reducing diffusion process that results in a new way to interact with the layered structures of the MMF model. Experimental results demonstrate that the proposed diffusion variational inference method leads to improved performance scores compared to several existing methods, including the VAE",
    "checked": false,
    "id": "6edb00fa41d5ad127adb1091c2e7c1a5df9925ea",
    "semantic_title": "hir-diff: unsupervised hyperspectral image restoration via improved diffusion models",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=9Klg7ce8D7": {
    "title": "Compressing tree ensembles through Level-wise Optimization and Pruning",
    "volume": "poster",
    "abstract": "Tree ensembles (e.g., gradient boosting decision trees) are often used in practice because they offer excellent predictive performance while still being easy and efficient to learn. In some contexts, it is important to additionally optimize their size: this is specifically the case when models need to have verifiable properties (verification of fairness, robustness, etc. is often exponential in the ensemble's size), or when models run on battery-powered devices (smaller ensembles consume less energy, increasing battery autonomy). For this reason, compression of tree ensembles is worth studying. This paper presents LOP, a method for compressing a given tree ensemble by pruning or entirely removing trees in it, while updating leaf predictions in such a way that predictive accuracy is mostly unaffected. Empirically, LOP achieves compression factors that are often 10 to 100 times better than that of competing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SaKPKyjDp6": {
    "title": "Time Series Representations with Hard-Coded Invariances",
    "volume": "poster",
    "abstract": "Automatically extracting robust representations from large and complex time series data is becoming imperative for several real-world applications. Unfortunately, the potential of common neural network architectures in capturing invariant properties of time series remains relatively underexplored. For instance, convolutional layers often fail to capture underlying patterns in time series inputs that encompass strong deformations, such as trends. Indeed, invariances to some deformations may be critical for solving complex time series tasks, such as classification, while guaranteeing good generalization performance. To address these challenges, we mathematically formulate and technically design efficient and hard-coded *invariant convolutions* for specific group actions applicable to the case of time series. We construct these convolutions by considering specific sets of deformations commonly observed in time series, including *scaling*, *offset shift*, and *trend*. We further combine the proposed invariant convolutions with standard convolutions in single embedding layers, and we showcase the layer capacity to capture complex invariant time series properties in several scenarios",
    "checked": false,
    "id": "d81732bbc63ea40c8dabe1a1b504ab00463b502c",
    "semantic_title": "timedrl: disentangled representation learning for multivariate time-series",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=hrBfufwMzg": {
    "title": "Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning",
    "volume": "poster",
    "abstract": "Multi-objective optimization (MOO) exists extensively in machine learning, and aims to find a set of Pareto-optimal solutions, called the Pareto front, e.g., it is fundamental for multiple avenues of research in federated learning (FL). Pareto-Front Learning (PFL) is a powerful method implemented using Hypernetworks (PHNs) to approximate the Pareto front. This method enables the acquisition of a mapping function from a given preference vector to the solutions on the Pareto front. However, most existing PFL approaches still face two challenges: (a) sampling rays in high-dimensional spaces; (b) failing to cover the entire Pareto Front which has a convex shape. Here, we introduce a novel PFL framework, called as PHN-HVVS, which decomposes the design space into Voronoi grids and deploys a genetic algorithm (GA) for Voronoi grid partitioning within high-dimensional space. We put forward a new loss function, which effectively contributes to more extensive coverage of the resultant Pareto front and maximizes the HV Indicator. Experimental results on multiple MOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines significantly in generating Pareto front. Also, we illustrate that PHN-HVVS advances the methodologies of several recent problems in the FL field. The code is available at https://github.com/buptcmm/phnhvvs",
    "checked": true,
    "id": "67cc5a526b7db2e37288a4ee845878ca7f5e2d25",
    "semantic_title": "voronoi-grid-based pareto front learning and its application to collaborative federated learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CAPNgWkEEk": {
    "title": "Optimal Sensor Scheduling and Selection for Continuous-Discrete Kalman Filtering with Auxiliary Dynamics",
    "volume": "poster",
    "abstract": "We study the Continuous-Discrete Kalman Filter (CD-KF) for State-Space Models (SSMs) where continuous-time dynamics are observed via multiple sensors with discrete, irregularly timed measurements. Our focus extends to scenarios in which the measurement process is coupled with the states of an auxiliary SSM. For instance, higher measurement rates may increase energy consumption or heat generation, while a sensor's accuracy can depend on its own spatial trajectory or that of the measured target. Each sensor thus carries distinct costs and constraints associated with its measurement rate and additional constraints and costs on the auxiliary state. We model measurement occurrences as independent Poisson processes with sensor-specific rates and derive an upper bound on the mean posterior covariance matrix of the CD-KF along the mean auxiliary state. The bound is continuously differentiable with respect to the measurement rates, which enables efficient gradient-based optimization. Exploiting this bound, we propose a finite-horizon optimal control framework to optimize measurement rates and auxiliary-state dynamics jointly. We further introduce a deterministic method for scheduling measurement times from the optimized rates. Empirical results in state-space filtering and dynamic temporal Gaussian process regression demonstrate that our approach achieves improved trade-offs between resource usage and estimation accuracy",
    "checked": true,
    "id": "054c07f0c20929497c4b7e8edc0169cc563cc268",
    "semantic_title": "optimal sensor scheduling and selection for continuous-discrete kalman filtering with auxiliary dynamics",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7jxa1o8rDW": {
    "title": "Fairness on Principal Stratum: A New Perspective on Counterfactual Fairness",
    "volume": "poster",
    "abstract": "Fairness in human and algorithmic decision-making is crucial in areas such as criminal justice, education, and social welfare. Recently, counterfactual fairness has drawn increasing research interest, suggesting that decision-making for individuals should remain the same when intervening with different values on protected attributes. Nevertheless, the question of \"which attributes and individuals should be protected\" is rarely discussed in the existing counterfactual fairness literature. For example, when considering leg disability as a protected attribute, the algorithms should not treat individuals with leg disabilities differently in college admissions, but one may naturally consider this factor when selecting runner athletes. In other words, when and how to enforce fairness is expected to depend on the causal relation between the protected attribute and the outcome of interest. Formally, this paper proposes principal counterfactual fairness using the concept of principal stratification from the causal inference literature, focusing on whether an algorithm is counterfactually fair for individuals whose protected attribute has no individual causal effect on the outcome of interest. To examine whether an algorithm satisfies principal counterfactual fairness, we derive the statistical bounds and propose a post-processing approach to achieving principal counterfactual fairness with minimal individual decision changes. Experiments are conducted using synthetic and real-world datasets to verify the effectiveness of our methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kjtvCSkSsy": {
    "title": "Exponential Family Variational Flow Matching for Tabular Data Generation",
    "volume": "poster",
    "abstract": "While denoising diffusion and flow matching have driven major advances in generative modeling, their application to tabular data remains limited, despite its ubiquity in real-world applications. To this end, we develop *TabbyFlow*, a variational Flow Matching (VFM) method for tabular data generation. To apply VFM to data with mixed continuous and discrete features, we introduce **Exponential Family Variational Flow Matching (EF-VFM)**, which represents heterogeneous data types using a general exponential family distribution. We hereby obtain an efficient, data-driven objective based on moment matching, enabling principled learning of probability paths over mixed continuous and discrete variables. We also establish a connection between variational flow matching and generalized flow matching objectives based on Bregman divergences. Evaluation on tabular data benchmarks demonstrates state-of-the-art performance compared to baselines",
    "checked": true,
    "id": "c4ca1b1b06904eeee7d502b682a0d17f60f3aa32",
    "semantic_title": "exponential family variational flow matching for tabular data generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uTv5rOPZr4": {
    "title": "LLMs Can Reason Faster Only If We Let Them",
    "volume": "poster",
    "abstract": "Large language models (LLMs) are making inroads into classical AI problems such as automated planning, yet key shortcomings continue to hamper their integration. Chain-of-Thought (CoT) struggles in complex multi-step reasoning, and Tree-of-Thoughts requires multiple queries that increase computational overhead. Recently, Algorithm-of-Thoughts (AoT) have shown promise using in-context examples, at the cost of significantly longer solutions compared to CoT. Aimed at bridging the solution length gap between CoT and AoT, this paper introduces AoT-O3, which combines supervised finetuning on AoT-style plans with a reinforcement learning (RL) framework designed to reduce solution length. The RL component uses a reward model that favors concise, valid solutions while maintaining planning accuracy. Empirical evaluations indicate that AoT-O3 shortens solution length by up to 80\\% compared to baseline AoT while maintaining or surpassing prior performance. These findings suggest a promising pathway for more efficient, scalable LLM-based planning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YSVSMV0lXQ": {
    "title": "Controlled Generation with Equivariant Variational Flow Matching",
    "volume": "poster",
    "abstract": "We derive a controlled generation objective within the framework of Variational Flow Matching (VFM), which casts flow matching as a variational inference problem. We demonstrate that controlled generation can be implemented two ways: (1) by way of end-to-end training of conditional generative models, or (2) as a Bayesian inference problem, enabling post hoc control of unconditional models without retraining. Furthermore, we establish the conditions required for equivariant generation and provide an equivariant formulation of VFM tailored for molecular generation, ensuring invariance to rotations, translations, and permutations. We evaluate our approach on both uncontrolled and controlled molecular generation, achieving state-of-the-art performance on uncontrolled generation and outperforming state-of-the-art models in controlled generation, both with end-to-end training and in the Bayesian inference setting. This work strengthens the connection between flow-based generative modeling and Bayesian inference, offering a scalable and principled framework for constraint-driven and symmetry-aware generation",
    "checked": true,
    "id": "bd39b33b0bc121b6ecd70dbb71d3b7dbfc6d3772",
    "semantic_title": "controlled generation with equivariant variational flow matching",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hRMAo5N66M": {
    "title": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces",
    "volume": "poster",
    "abstract": "Open-ended learning agents must efficiently prioritize goals in vast possibility spaces, focusing on those that maximize learning progress (LP). When such autotelic exploration is achieved by LLM agents trained with online RL in high-dimensional and evolving goal spaces, a key challenge for LP prediction is modeling one's own competence, a form of metacognitive monitoring. Traditional approaches either require extensive sampling or rely on brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive framework that lets LLM agents learn to predict their competence and learning progress online. By capturing semantic relationships between goals, MAGELLAN enables sample-efficient LP estimation and dynamic adaptation to evolving goal spaces through generalization. In an interactive learning environment, we show that MAGELLAN improves LP prediction efficiency and goal prioritization, being the only method allowing the agent to fully master a large and evolving goal space. These results demonstrate how augmenting LLM agents with a metacognitive ability for LP predictions can effectively scale curriculum learning to open-ended goal spaces",
    "checked": true,
    "id": "bd79f32356ff313139fe08c7335b8155fb4a67b2",
    "semantic_title": "magellan: metacognitive predictions of learning progress guide autotelic llm agents in large goal spaces",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OmQcPgq9RN": {
    "title": "The Harder Path: Last Iterate Convergence for Uncoupled Learning in Zero-Sum Games with Bandit Feedback",
    "volume": "poster",
    "abstract": "We study the problem of learning in zero-sum matrix games with repeated play and bandit feedback. Specifically, we focus on developing uncoupled algorithms that guarantee, without communication between players, convergence of the last-iterate to a Nash equilibrium. Although the non-bandit case has been studied extensively, this setting has only been explored recently, with a bound of $\\mathcal{O}(T^{-1/8})$ on the exploitability gap. We show that, for uncoupled algorithms, guaranteeing convergence of the policy profiles to a Nash equilibrium is detrimental to the performances, with the best attainable rate being $\\mathcal{O}(T^{-1/4})$ in contrast to the usual $\\mathcal{O}(T^{-1/2})$ rate for convergence of the average iterates. We then propose two algorithms that achieve this optimal rate. The first algorithm leverages a straightforward tradeoff between exploration and exploitation, while the second employs a regularization technique based on a two-step mirror descent approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H76PMm7hf2": {
    "title": "Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning",
    "volume": "poster",
    "abstract": "Online fine-tuning vision-language model (VLM) agents with reinforcement learning (RL) has shown promise for equipping agents with multi-step, goal-oriented capabilities in dynamic environments. However, their open-ended textual action space and non-end-to-end nature of action generation present significant challenges to effective online exploration in RL, e.g., explosion of the exploration space. We propose a novel online fine-tuning method, Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual output space of VLM agents. Compared to prior methods that assign uniform uncertainty to all tokens, CoSo leverages counterfactual reasoning to dynamically assess the causal influence of individual tokens on post-processed actions. By prioritizing the exploration of action-critical tokens while reducing the impact of semantically redundant or low-impact tokens, CoSo enables a more targeted and efficient online rollout process. We provide theoretical analysis proving CoSo's convergence and policy improvement guarantees, and extensive empirical evaluations supporting CoSo's effectiveness. Our results across a diverse set of agent tasks, including Android device control, card gaming, and embodied AI, highlight its remarkable ability to enhance exploration efficiency and deliver consistent performance gains. The code is available at https://github.com/langfengQ/CoSo",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h5TXCnnEyy": {
    "title": "Towards Attributions of Input Variables in a Coalition",
    "volume": "poster",
    "abstract": "This paper focuses on the fundamental challenge of partitioning input variables in attribution methods for Explainable AI, particularly in Shapley value-based approaches. Previous methods always compute attributions given a predefined partition but lack theoretical guidance on how to form meaningful variable partitions. We identify that attribution conflicts arise when the attribution of a coalition differs from the sum of its individual variables' attributions. To address this, we analyze the numerical effects of AND-OR interactions in AI models and extend the Shapley value to a new attribution metric for variable coalitions. Our theoretical findings reveal that specific interactions cause attribution conflicts, and we propose three metrics to evaluate coalition faithfulness. Experiments on synthetic data, NLP, image classification, and the game of Go validate our approach, demonstrating consistency with human intuition and practical applicability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4LClOWTAth": {
    "title": "Neural Guided Diffusion Bridges",
    "volume": "poster",
    "abstract": "We propose a novel method for simulating conditioned diffusion processes (diffusion bridges) in Euclidean spaces. By training a neural network to approximate bridge dynamics, our approach eliminates the need for computationally intensive Markov Chain Monte Carlo (MCMC) methods or reverse-process modeling. Compared to existing methods, it offers greater robustness across various diffusion specifications and conditioning scenarios. This applies in particular to rare events and multimodal distributions, which pose challenges for score-learning- and MCMC-based approaches. We propose a flexible variational family for approximating the diffusion bridge path measure which is partially specified by a neural network. Once trained, it enables efficient independent sampling at a cost comparable to sampling the unconditioned (forward) process",
    "checked": true,
    "id": "f08050068df877864ee10f62043e5ff2e27f8939",
    "semantic_title": "neural guided diffusion bridges",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d6CTIPrTTC": {
    "title": "ELMO : Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces",
    "volume": "poster",
    "abstract": "Large output spaces, also referred to as Extreme multilabel classification (XMC), is a setting that arises, e.g., in large-scale tagging and product-to-product recommendation, and is characterized by the number of labels ranging from hundreds of thousands to millions. This means that the linear classification head, usually only a tiny fraction of the overall model, turns into the main driver for compute and memory demand. Current state-of-the-art XMC methods predominantly rely on FP16-FP32 mixed-precision training, which we show can be unstable, and inefficient in terms of memory usage and computational overhead. Meanwhile, existing low-precision methods typically retain higher precision for the classification layer. In this work, we propose ELMO, a pure low-precision training framework for XMC models using BFloat16 and Float8 data types. By leveraging Kahan summation and stochastic rounding, we demonstrate that XMC models can be effectively trained entirely in Float8, without relying on single-precision master weights or tensor scaling. Low-precision training, combined with our proposed memory optimizations---gradient fusion and chunking---enables significant reductions in GPU memory usage. For example, we train a 3-million-label XMC model with only 6.6 GiB of GPU memory, compared to the 39.7GiB required by the optimized SOTA method, Renee without compromising accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yhgcRwJ9Dn": {
    "title": "Hyper-Transforming Latent Diffusion Models",
    "volume": "poster",
    "abstract": "We introduce a novel generative framework for functions by integrating Implicit Neural Representations (INRs) and Transformer-based hypernetworks into latent variable models. Unlike prior approaches that rely on MLP-based hypernetworks with scalability limitations, our method employs a Transformer-based decoder to generate INR parameters from latent variables, addressing both representation capacity and computational efficiency. Our framework extends latent diffusion models (LDMs) to INR generation by replacing standard decoders with a Transformer-based hypernetwork, which can be trained either from scratch or via hyper-transforming—a strategy that fine-tunes only the decoder while freezing the pre-trained latent space. This enables efficient adaptation of existing generative models to INR-based representations without requiring full retraining. We validate our approach across multiple modalities, demonstrating improved scalability, expressiveness, and generalization over existing INR-based generative models. Our findings establish a unified and flexible framework for learning structured function representations",
    "checked": true,
    "id": "753f54dd749521ede4b17d0e7282e6f47acab822",
    "semantic_title": "hyper-transforming latent diffusion models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Ce79P8ULPY": {
    "title": "Emergent Response Planning in LLMs",
    "volume": "poster",
    "abstract": "In this work, we argue that large language models (LLMs), though trained to predict only the next token, exhibit emergent planning behaviors: $\\textbf{their hidden representations encode future outputs beyond the next token}$. Through simple probing, we demonstrate that LLM prompt representations encode global attributes of their entire responses, including $\\textit{structure attributes}$ (e.g., response length, reasoning steps), $\\textit{content attributes}$ (e.g., character choices in storywriting, multiple-choice answers at the end of response), and $\\textit{behavior attributes}$ (e.g., answer confidence, factual consistency). In addition to identifying response planning, we explore how it scales with model size across tasks and how it evolves during generation. The findings that LLMs plan ahead for the future in their hidden representations suggest potential applications for improving transparency and generation control",
    "checked": true,
    "id": "5610f2cd40f3f0d3a804876fde0a7f5dc099955d",
    "semantic_title": "emergent response planning in llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iXvm0zvspb": {
    "title": "Explicit Preference Optimization: No Need for an Implicit Reward Model",
    "volume": "poster",
    "abstract": "The generated responses of large language models (LLMs) are often fine-tuned to human preferences through a process called reinforcement learning from human feedback (RLHF). As RLHF relies on a challenging training sequence, whereby a separate reward model is independently learned and then later applied to LLM policy updates, ongoing research effort has targeted more straightforward alternatives. In this regard, direct preference optimization (DPO) and its many offshoots circumvent the need for a separate reward training step. Instead, through the judicious use of a reparameterization trick that induces an implicit reward, DPO and related methods consolidate learning to the minimization of a single loss function. And yet despite demonstrable success in some real-world settings, we prove that DPO-based objectives are nonetheless subject to sub-optimal regularization and counter-intuitive interpolation behaviors, underappreciated artifacts of the reparameterizations upon which they are based. To this end, we introduce an explicit preference optimization framework termed EXPO that requires no analogous reparameterization to achieve an implicit reward. Quite differently, we merely posit intuitively-appealing regularization factors from scratch that transparently avoid the potential pitfalls of key DPO variants, provably satisfying regularization desiderata that prior methods do not. Empirical results serve to corroborate our analyses and showcase the efficacy of EXPO",
    "checked": true,
    "id": "372c106a2c806eb28a58dd18ccfcb403835d9331",
    "semantic_title": "explicit preference optimization: no need for an implicit reward model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CS4RyQuTig": {
    "title": "CaDA: Cross-Problem Routing Solver with Constraint-Aware Dual-Attention",
    "volume": "poster",
    "abstract": "Vehicle routing problems (VRPs) are significant combinatorial optimization problems (COPs) holding substantial practical importance. Recently, neural combinatorial optimization (NCO), which involves training deep learning models on extensive data to learn vehicle routing heuristics, has emerged as a promising approach due to its efficiency and the reduced need for manual algorithm design. However, applying NCO across diverse real-world scenarios with various constraints necessitates cross-problem capabilities. Current cross-problem NCO methods for VRPs typically employ a constraint-unaware model, limiting their cross-problem performance. Furthermore, they rely solely on global connectivity, which fails to focus on key nodes and leads to inefficient representation learning. This paper introduces a \\underline{C}onstraint-\\underline{A}ware \\underline{D}ual-\\underline{A}ttention Model (CaDA), designed to address these limitations. CaDA incorporates a constraint prompt that efficiently represents different problem variants. Additionally, it features a dual-attention mechanism with a global branch for capturing broader graph-wide information and a sparse branch that selectively focuses on the key node connections. We comprehensively evaluate our model on 16 different VRPs and compare its performance against existing cross-problem VRP solvers. CaDA achieves state-of-the-art results across all tested VRPs. Our ablation study confirms that each component contributes to its cross-problem learning performance. The source code for CaDA is publicly available at \\url{https://github.com/CIAM-Group/CaDA}",
    "checked": true,
    "id": "5ce14a6427f09468421cbdc88f450128974db850",
    "semantic_title": "cada: cross-problem routing solver with constraint-aware dual-attention",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=GDvO6viRCF": {
    "title": "Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory",
    "volume": "poster",
    "abstract": "While uncertainty estimation for graphs recently gained traction, most methods rely on homophily and deteriorate in heterophilic settings. We address this by analyzing message passing neural networks from an information-theoretic perspective and developing a suitable analog to data processing inequality to quantify information throughout the model's layers. In contrast to non-graph domains, information about the node-level prediction target can *increase* with model depth if a node's features are semantically different from its neighbors. Therefore, on heterophilic graphs, the latent embeddings of an MPNN each provide different information about the data distribution - different from homophilic settings. This reveals that considering all node representations simultaneously is a key design principle for epistemic uncertainty estimation on graphs beyond homophily. We empirically confirm this with a simple post-hoc density estimator on the joint node embedding space that provides state-of-the-art uncertainty on heterophilic graphs. At the same time, it matches prior work on homophilic graphs without explicitly exploiting homophily through post-processing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Jr5Al16MS": {
    "title": "Near Optimal Best Arm Identification for Clustered Bandits",
    "volume": "poster",
    "abstract": "This work investigates the problem of best arm identification for multi-agent multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where each cluster solves a stochastic bandit problem. The mapping between agents and bandits is \\textit{a priori} unknown. Each bandit is associated with $K$ arms, and the goal is to identify the best arm for each agent under a $\\delta$-probably correct ($\\delta$-PC) framework, while minimizing sample complexity and communication overhead. We propose two novel algorithms: \\emph{Clustering then Best Arm Identification} (\\texttt{Cl-BAI}) and \\emph{Best Arm Identification then Clustering} (\\texttt{BAI-Cl}). \\texttt{Cl-BAI} employs a two-phase approach that first clusters agents based on the bandit problems they are learning, followed by identifying the best arm for each cluster. \\texttt{BAI-Cl} reverses the sequence by identifying the best arms first and then clustering agents accordingly. Both algorithms exploit the successive elimination framework to ensure computational efficiency and high accuracy. Theoretical analysis establishes $\\delta$-PC guarantees for both methods, derives bounds on their sample complexity, and provides a lower bound for the problem class. Moreover, when $M$ is small (a constant), we show that the sample complexity of (a variant of) \\texttt{BAI-Cl} is (order-wise) minimax optimal. Experiments on synthetic and real-world (Movie Lens, Yelp) data demonstrates the superior performance of the proposed algorithms in terms of sample and communication efficiency, particularly in settings where $M \\ll N$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rTPq8VzhmZ": {
    "title": "High Probability Bound for Cross-Learning Contextual Bandits with Unknown Context Distributions",
    "volume": "poster",
    "abstract": "Motivated by applications in online bidding and sleeping bandits, we examine the problem of contextual bandits with cross learning, where the learner observes the loss associated with the action across all possible contexts, not just the current round's context. Our focus is on a setting where losses are chosen adversarially, and contexts are sampled i.i.d. from a specific distribution. This problem was first studied by Balseiro et al. (2019), who proposed an algorithm that achieves near-optimal regret under the assumption that the context distribution is known in advance. However, this assumption is often unrealistic. To address this issue, Schneider & Zimmert (2023) recently proposed a new algorithm that achieves nearly optimal expected regret. It is well-known that expected regret can be significantly weaker than high-probability bounds. In this paper, we present a novel, in-depth analysis of their algorithm and demonstrate that it actually achieves near-optimal regret with high probability. There are steps in the original analysis by Schneider & Zimmert (2023) that lead only to an expected bound by nature. In our analysis, we introduce several new insights. Specifically, we make extensive use of the weak dependency structure between different epochs, which was overlooked in previous analyses. Additionally, standard martingale inequalities are not directly applicable, so we refine martingale inequalities to complete our analysis",
    "checked": true,
    "id": "485c36d90082d5650f0003f2b0c31b904aad365c",
    "semantic_title": "high probability bound for cross-learning contextual bandits with unknown context distributions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UTT5OTyIWm": {
    "title": "FOUNDER: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making",
    "volume": "poster",
    "abstract": "Foundation Models (FMs) and World Models (WMs) offer complementary strengths in task generalization at different levels. In this work, we propose FOUNDER, a framework that integrates the generalizable knowledge embedded in FMs with the dynamic modeling capabilities of WMs to enable open-ended task solving in embodied environments in a reward-free manner. We learn a mapping function that grounds FM representations in the WM state space, effectively inferring the agent's physical states in the world simulator from external observations. This mapping enables the learning of a goal-conditioned policy through imagination during behavior learning, with the mapped task serving as the goal state. Our method leverages the predicted temporal distance to the goal state as an informative reward signal. FOUNDER demonstrates superior performance on various multi-task offline visual control benchmarks, excelling in capturing the deep-level semantics of tasks specified by text or videos, particularly in scenarios involving complex observations or domain gaps where prior methods struggle. The consistency of our learned reward function with the ground-truth reward is also empirically validated. Our project website is https://sites.google.com/view/founder-rl",
    "checked": true,
    "id": "b7b0d2411723fec46715f129b13fe39e34d0e686",
    "semantic_title": "founder: grounding foundation models in world models for open-ended embodied decision making",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=XfjrLEPOQV": {
    "title": "Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More",
    "volume": "poster",
    "abstract": "When training deep neural networks with gradient descent, sharpness often increases---a phenomenon known as *progressive sharpening*---before saturating at the *edge of stability*. Although commonly observed in practice, the underlying mechanisms behind progressive sharpening remain poorly understood. In this work, we study this phenomenon using a minimalist model: a deep linear network with a single neuron per layer. We show that this simple model effectively captures the sharpness dynamics observed in recent empirical studies, offering a simple testbed to better understand neural network training. Moreover, we theoretically analyze how dataset properties, network depth, stochasticity of optimizers, and step size affect the degree of progressive sharpening in the minimalist model. We then empirically demonstrate how these theoretical insights extend to practical scenarios. This study offers a deeper understanding of sharpness dynamics in neural network training, highlighting the interplay between depth, training data, and optimizers",
    "checked": true,
    "id": "2700b6b1e5c284b28563e1798c41e9ea65949d80",
    "semantic_title": "understanding sharpness dynamics in nn training with a minimalist example: the effects of dataset difficulty, depth, stochasticity, and more",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cUNfm13VUR": {
    "title": "Comparing Few to Rank Many: Active Human Preference Learning Using Randomized Frank-Wolfe Method",
    "volume": "poster",
    "abstract": "We study learning human preferences from limited comparison feedback, a core machine learning problem that is at the center of reinforcement learning from human feedback (RLHF). We formulate the problem as learning a Plackett-Luce (PL) model from a limited number of $K$-subset comparisons over a universe of $N$ items, where typically $K \\ll N$. Our objective is to select the $K$-subsets such that all items can be ranked with minimal mistakes within the budget. We solve the problem using the D-optimal design, which minimizes the worst-case ranking loss under the estimated PL model. All known algorithms for this problem are computationally infeasible in our setting because we consider exponentially many subsets in $K$. To address this challenge, we propose a randomized Frank-Wolfe algorithm with memoization and sparse updates that has a low $O(N^2 + K^2)$ per-iteration complexity. We analyze it and demonstrate its empirical superiority on synthetic and open-source NLP datasets",
    "checked": false,
    "id": "6f6726d59d9e78104680fb4548c85feb3d82e383",
    "semantic_title": "comparing few to rank many: active human preference learning using randomized frank-wolfe",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=xYtLsWiUli": {
    "title": "Vector Grimoire: Codebook-based Shape Generation under Raster Image Supervision",
    "volume": "poster",
    "abstract": "Scalable Vector Graphics (SVG) is a popular format on the web and in the design industry. However, despite the great strides made in generative modeling, SVG has remained underexplored due to the discrete and complex nature of such data. We introduce GRIMOIRE, a text-guided SVG generative model that is comprised of two modules: A Visual Shape Quantizer (VSQ) learns to map raster images onto a discrete codebook by reconstructing them as vector shapes, and an Auto-Regressive Transformer (ART) models the joint probability distribution over shape tokens, positions and textual descriptions, allowing us to generate vector graphics from natural language. Unlike existing models that require direct supervision from SVG data, GRIMOIRE learns shape image patches using only raster image supervision which opens up vector generative modeling to significantly more data. We demonstrate the effectiveness of our method by fitting GRIMOIRE for closed filled shapes on the MNIST and Emoji, and for outline strokes on icon and font data, surpassing previous image-supervised methods in generative quality and vector-supervised approach in flexibility",
    "checked": true,
    "id": "c079da246eedc0d58b665030f5ba1120e3424eb4",
    "semantic_title": "vector grimoire: codebook-based shape generation under raster image supervision",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2dqiqST8ZJ": {
    "title": "Score-Based Diffusion Policy Compatible with Reinforcement Learning via Optimal Transport",
    "volume": "poster",
    "abstract": "Diffusion policies have shown promise in learning complex behaviors from demonstrations, particularly for tasks requiring precise control and long-term planning. However, they face challenges in robustness when encountering distribution shifts. This paper explores improving diffusion-based imitation learning models through online interactions with the environment. We propose OTPR (Optimal Transport-guided score-based diffusion Policy for Reinforcement learning fine-tuning), a novel method that integrates diffusion policies with RL using optimal transport theory. OTPR leverages the Q-function as a transport cost and views the policy as an optimal transport map, enabling efficient and stable fine-tuning. Moreover, we introduce masked optimal transport to guide state-action matching using expert keypoints and a compatibility-based resampling strategy to enhance training stability. Experiments on three simulation tasks demonstrate OTPR's superior performance and robustness compared to existing methods, especially in complex and sparse-reward environments. In sum, OTPR provides an effective framework for combining IL and RL, achieving versatile and reliable policy learning",
    "checked": true,
    "id": "406af78910c3aadacf43ea3286d912e452815eb0",
    "semantic_title": "score-based diffusion policy compatible with reinforcement learning via optimal transport",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UNrfYfbLZ3": {
    "title": "Accelerating Linear Recurrent Neural Networks for the Edge with Unstructured Sparsity",
    "volume": "poster",
    "abstract": "Linear recurrent neural networks enable powerful long-range sequence modeling with constant memory usage and time-per-token during inference. These architectures hold promise for streaming applications at the edge, but deployment in resource-constrained environments requires hardware-aware optimizations to minimize latency and energy consumption. Unstructured sparsity offers a compelling solution, enabling substantial reductions in compute and memory requirements--when accelerated by compatible hardware platforms. In this paper, we conduct a scaling study to investigate the Pareto front of performance and efficiency across inference compute budgets. We find that highly sparse linear RNNs *consistently* achieve better efficiency-performance trade-offs than dense baselines, with $2\\times$ less compute and $36$\\% less memory at iso-accuracy. Our models achieve state-of-the-art results on a real-time streaming task for audio denoising. By quantizing our sparse models to fixed-point arithmetic and deploying them on the Intel Loihi 2 neuromorphic chip for real-time processing, we translate model compression into tangible gains of $42\\times$ lower latency and $149\\times$ lower energy consumption compared to a dense model on an edge GPU. Our findings showcase the transformative potential of unstructured sparsity, paving the way for highly efficient recurrent neural networks in real-world, resource-constrained environments",
    "checked": true,
    "id": "d5b4f5af3ceedf79b74ef4cdd4abc8f80a9a75e3",
    "semantic_title": "accelerating linear recurrent neural networks for the edge with unstructured sparsity",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=iwkCnlOa2A": {
    "title": "Complex Wavelet Mutual Information Loss: A Multi-Scale Loss Function for Semantic Segmentation",
    "volume": "poster",
    "abstract": "Recent advancements in deep neural networks have significantly enhanced the performance of semantic segmentation. However, class imbalance and instance imbalance remain persistent challenges, where smaller instances and thin boundaries are often overshadowed by larger structures. To address the multiscale nature of segmented objects, various models have incorporated mechanisms such as spatial attention and feature pyramid networks. Despite these advancements, most loss functions are still primarily pixel-wise, while regional and boundary-focused loss functions often incur high computational costs or are restricted to small-scale regions. To address this limitation, we propose the complex wavelet mutual information (CWMI) loss, a novel loss function that leverages mutual information from subband images decomposed by a complex steerable pyramid. The complex steerable pyramid captures features across multiple orientations and preserves structural similarity across scales. Meanwhile, mutual information is well-suited to capturing high-dimensional directional features and offers greater noise robustness. Extensive experiments on diverse segmentation datasets demonstrate that CWMI loss achieves significant improvements in both pixel-wise accuracy and topological metrics compared to state-of-the-art methods, while introducing minimal computational overhead. Our code is available at https://github.com/lurenhaothu/CWMI",
    "checked": true,
    "id": "e0f3eaccfb5d546b9c5d8b73876cd38a8fe63e78",
    "semantic_title": "complex wavelet mutual information loss: a multi-scale loss function for semantic segmentation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=1d1ssNedLv": {
    "title": "Balancing Model Efficiency and Performance: Adaptive Pruner for Long-tailed Data",
    "volume": "poster",
    "abstract": "Long-tailed distribution datasets are prevalent in many machine learning tasks, yet existing neural network models still face significant challenges when handling such data. This paper proposes a novel adaptive pruning strategy, LTAP (Long-Tailed Adaptive Pruner), aimed at balancing model efficiency and performance to better address the challenges posed by long-tailed data distributions. LTAP introduces multi-dimensional importance scoring criteria and designs a dynamic weight adjustment mechanism to adaptively determine the pruning priority of parameters for different classes. By focusing on protecting parameters critical for tail classes, LTAP significantly enhances computational efficiency while maintaining model performance. This method combines the strengths of long-tailed learning and neural network pruning, overcoming the limitations of existing approaches in handling imbalanced data. Extensive experiments demonstrate that LTAP outperforms existing methods on various long-tailed datasets, achieving a good balance between model compression rate, computational efficiency, and classification accuracy. This research provides new insights into solving model optimization problems in long-tailed learning and is significant for improving the performance of neural networks on imbalanced datasets. The code is available at https://github.com/DataLab-atom/LT-VOTE",
    "checked": false,
    "id": "aed589dc96affc44ded3dfadf6ef25bbf6f7367e",
    "semantic_title": "domain-specific load balancing for accelerating gradient synchronization communication in large model training",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ftR9OuiUJA": {
    "title": "Contour Integration Underlies Human-Like Vision",
    "volume": "poster",
    "abstract": "Despite the tremendous success of deep learning in computer vision, models still fall behind humans in generalizing to new input distributions. Existing benchmarks do not investigate the specific failure points of models by analyzing performance under many controlled conditions. Our study systematically dissects where and why models struggle with contour integration - a hallmark of human vision -- by designing an experiment that tests object recognition under various levels of object fragmentation. Humans (n=50) perform at high accuracy, even with few object contours present. This is in contrast to models which exhibit substantially lower sensitivity to increasing object contours, with most of the over 1,000 models we tested barely performing above chance. Only at very large scales ($\\sim5B$ training dataset size) do models begin to approach human performance. Importantly, humans exhibit an integration bias - a preference towards recognizing objects made up of directional fragments over directionless fragments. We find that not only do models that share this property perform better at our task, but that this bias also increases with model training dataset size, and training models to exhibit contour integration leads to high shape bias. Taken together, our results suggest that contour integration is a hallmark of object vision that underlies object recognition performance, and may be a mechanism learned from data at scale",
    "checked": true,
    "id": "baf5c6e29593a4a09a5bd5008a7870b24c804a3a",
    "semantic_title": "contour integration underlies human-like vision",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=NOV32X1Rq3": {
    "title": "Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings",
    "volume": "poster",
    "abstract": "The pursuit of universal black-box optimization (BBO) algorithms is a longstanding goal. However, unlike domains such as language or vision, where scaling structured data has driven generalization, progress in offline BBO remains hindered by the lack of unified representations for heterogeneous numerical spaces. Thus, existing offline BBO approaches are constrained to single-task and fixed-dimensional settings, failing to achieve cross-domain universal optimization. Recent advances in language models (LMs) offer a promising path forward: their embeddings capture latent relationships in a unifying way, enabling universal optimization across different data types possible. In this paper, we discuss multiple potential approaches, including an end-to-end learning framework in the form of next-token prediction, as well as prioritizing the learning of latent spaces with strong representational capabilities. To validate the effectiveness of these methods, we collect offline BBO tasks and data from open-source academic works for training. Experiments demonstrate the universality and effectiveness of our proposed methods. Our findings suggest that unifying language model priors and learning string embedding space can overcome traditional barriers in universal BBO, paving the way for general-purpose BBO algorithms. The code is provided at https://github.com/lamda-bbo/universal-offline-bbo",
    "checked": true,
    "id": "81d94696d3beba402fb81b9f3ddd33eac8568280",
    "semantic_title": "towards universal offline black-box optimization via learning language model embeddings",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xQTSvP57C3": {
    "title": "Nonlinear transformers can perform inference-time feature learning",
    "volume": "poster",
    "abstract": "Pretrained transformers have demonstrated the ability to implement various algorithms at inference time without parameter updates. While theoretical works have established this capability through constructions and approximation guarantees, the optimization and statistical efficiency aspects remain understudied. In this work, we investigate how transformers learn features in-context -- a key mechanism underlying their inference-time adaptivity. We focus on the in-context learning of single-index models $y=\\sigma_*(\\langle \\\\boldsymbol{x},\\\\boldsymbol{\\beta}\\rangle)$, which are low-dimensional nonlinear functions parameterized by feature vector $\\\\boldsymbol\\beta$. We prove that transformers pretrained by gradient-based optimization can perform *inference-time feature learning*, i.e., extract information of the target features $\\\\boldsymbol{\\beta}$ solely from test prompts (despite $\\\\boldsymbol {\\beta}$ varying across different prompts), hence achieving an in-context statistical efficiency that surpasses any non-adaptive (fixed-basis) algorithms such as kernel methods. Moreover, we show that the inference-time sample complexity surpasses the Correlational Statistical Query (CSQ) lower bound, owing to nonlinear label transformations naturally induced by the Softmax self-attention mechanism",
    "checked": false,
    "id": "bf6098f75d6329a6ca931e22389a25540bad99b2",
    "semantic_title": "automatic domain adaptation by transformers in in-context learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=R65zHNqND0": {
    "title": "Bongard in Wonderland: Visual Puzzles that Still Make AI Go Mad?",
    "volume": "poster",
    "abstract": "Recently, newly developed Vision-Language Models (VLMs), such as OpenAI's o1, have emerged, seemingly demonstrating advanced reasoning capabilities across text and image modalities. However, the depth of these advances in language-guided perception and abstract reasoning remains underexplored, and it is unclear whether these models can truly live up to their ambitious promises. To assess the progress and identify shortcomings, we enter the wonderland of Bongard problems, a set of classic visual reasoning puzzles that require human-like abilities of pattern recognition and abstract reasoning. With our extensive evaluation setup, we show that while VLMs occasionally succeed in identifying discriminative concepts and solving some of the problems, they frequently falter. Surprisingly, even elementary concepts that may seem trivial to humans, such as simple spirals, pose significant challenges. Moreover, when explicitly asked to recognize ground truth concepts, they continue to falter, suggesting not only a lack of understanding of these elementary visual concepts but also an inability to generalize to unseen concepts. We compare the results of VLMs to human performance and observe that a significant gap remains between human visual reasoning capabilities and machine cognition",
    "checked": true,
    "id": "a020ae590e9ffba46adcfdfaa263e5d7eb911eab",
    "semantic_title": "bongard in wonderland: visual puzzles that still make ai go mad?",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=A82tIFgJaK": {
    "title": "Harmonizing Geometry and Uncertainty: Diffusion with Hyperspheres",
    "volume": "poster",
    "abstract": "Do contemporary diffusion models preserve the class geometry of hyperspherical data? Standard diffusion models rely on isotropic Gaussian noise in the forward process, inherently favoring Euclidean spaces. However, many real-world problems involve non-Euclidean distributions, such as hyperspherical manifolds, where class-specific patterns are governed by angular geometry within hypercones. When modeled in Euclidean space, these angular subtleties are lost, leading to suboptimal generative performance. To address this limitation, we introduce \\textbf{HyperSphereDiff} to align hyperspherical structures with directional noise, preserving class geometry and effectively capturing angular uncertainty. We demonstrate both theoretically and empirically that this approach aligns the generative process with the intrinsic geometry of hyperspherical data, resulting in more accurate and geometry-aware generative models. We evaluate our framework on four object datasets and two face datasets, showing that incorporating angular uncertainty better preserves the underlying hyperspherical manifold",
    "checked": true,
    "id": "2e1ca3dbd3ce61d8a0f69fe67f966a831b2afd36",
    "semantic_title": "harmonizing geometry and uncertainty: diffusion with hyperspheres",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P0RkH1RT5z": {
    "title": "Subgroups Matter for Robust Bias Mitigation",
    "volume": "poster",
    "abstract": "Despite the constant development of new bias mitigation methods for machine learning, no method consistently succeeds, and a fundamental question remains unanswered: when and why do bias mitigation techniques fail? In this paper, we hypothesise that a key factor may be the often-overlooked but crucial step shared by many bias mitigation methods: the definition of subgroups. To investigate this, we conduct a comprehensive evaluation of state-of-the-art bias mitigation methods across multiple vision and language classification tasks, systematically varying subgroup definitions, including coarse, fine-grained, intersectional, and noisy subgroups. Our findings reveal that subgroup choice significantly impacts performance, with certain groupings paradoxically leading to worse outcomes than no mitigation at all. They suggest that observing a disparity between a set of subgroups is not a sufficient reason to use those subgroups for mitigation. Through theoretical analysis, we explain these phenomena and uncover a counter-intuitive insight that, in some cases, improving fairness with respect to a particular set of subgroups is best achieved by using a different set of subgroups for mitigation. Our work highlights the importance of careful subgroup definition in bias mitigation and presents it as an alternative lever for improving the robustness and fairness of machine learning models",
    "checked": true,
    "id": "c0ba431076e0a20a914495fcddd8acb105f49e30",
    "semantic_title": "subgroups matter for robust bias mitigation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UJXbcJ7qXB": {
    "title": "Hyperbolic-PDE GNN: Spectral Graph Neural Networks in the Perspective of A System of Hyperbolic Partial Differential Equations",
    "volume": "poster",
    "abstract": "Graph neural networks (GNNs) leverage message passing mechanisms to learn the topological features of graph data. Traditional GNNs learns node features in a spatial domain unrelated to the topology, which can hardly ensure topological features. In this paper, we formulates message passing as a system of hyperbolic partial differential equations (hyperbolic PDEs), constituting a dynamical system that explicitly maps node representations into a particular solution space. This solution space is spanned by a set of eigenvectors describing the topological structure of graphs. Within this system, for any moment in time, a node features can be decomposed into a superposition of the basis of eigenvectors. This not only enhances the interpretability of message passing but also enables the explicit extraction of fundamental characteristics about the topological structure. Furthermore, by solving this system of hyperbolic partial differential equations, we establish a connection with spectral graph neural networks (spectral GNNs), serving as a message passing enhancement paradigm for spectral GNNs.We further introduce polynomials to approximate arbitrary filter functions. Extensive experiments demonstrate that the paradigm of hyperbolic PDEs not only exhibits strong flexibility but also significantly enhances the performance of various spectral GNNs across diverse graph tasks",
    "checked": true,
    "id": "9b29face8388675d6fb346e98d37580d0cc5933f",
    "semantic_title": "hyperbolic-pde gnn: spectral graph neural networks in the perspective of a system of hyperbolic partial differential equations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TV17MLZGuA": {
    "title": "Mind the Gap: A Practical Attack on GGUF Quantization",
    "volume": "poster",
    "abstract": "With the increasing size of frontier LLMs, post-training quantization has become the standard for memory-efficient deployment. Recent work has shown that basic rounding-based quantization schemes pose security risks, as they can be exploited to inject malicious behaviors into quantized models that remain hidden in full precision. However, existing attacks cannot be applied to more complex quantization methods, such as the GGUF family used in the popular ollama and llama.cpp frameworks. In this work, we address this gap by introducing the first attack on GGUF. Our key insight is that the quantization error -- the difference between the full-precision weights and their (de-)quantized version -- provides sufficient flexibility to construct malicious quantized models that appear benign in full precision. Leveraging this, we develop an attack that trains the target malicious LLM while constraining its weights based on quantization errors. We demonstrate the effectiveness of our attack on three popular LLMs across nine GGUF quantization data types on three diverse attack scenarios: insecure code generation ($\\Delta$=$88.7\\%$), targeted content injection ($\\Delta$=$85.0\\%$), and benign instruction refusal ($\\Delta$=$30.1\\%$). Our attack highlights that (1) the most widely used post-training quantization method is susceptible to adversarial interferences, and (2) the complexity of quantization schemes alone is insufficient as a defense",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EIfCH9OgjR": {
    "title": "Elucidating the design space of language models for image generation",
    "volume": "poster",
    "abstract": "The success of large language models (LLMs) in text generation has inspired their application to image generation. However, existing methods either rely on specialized designs with inductive biases or adopt LLMs without fully exploring their potential in vision tasks. In this work, we systematically investigate the design space of LLMs for image generation and demonstrate that LLMs can achieve near state-of-the-art performance without domain-specific designs, simply by making proper choices in tokenization methods, modeling approaches, scan patterns, vocabulary design, and sampling strategies. We further analyze autoregressive models' learning and scaling behavior, revealing how larger models effectively capture more useful information than the smaller ones. Additionally, we explore the inherent differences between text and image modalities, highlighting the potential of LLMs across domains. The exploration provides valuable insights to inspire more effective designs when applying LLMs to other domains. With extensive experiments, our proposed model, **ELM** achieves an FID of 1.54 on 256$\\times$256 ImageNet and an FID of 3.29 on 512$\\times$512 ImageNet, demonstrating the powerful generative potential of LLMs in vision tasks",
    "checked": true,
    "id": "27cbd9f3c2ad568cb80731164c41424ed3232dda",
    "semantic_title": "elucidating the design space of language models for image generation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=aTQtGq7IyT": {
    "title": "Be a Goldfish: Forgetting Bad Conditioning in Sparse Linear Regression via Variational Autoencoders",
    "volume": "poster",
    "abstract": "Variational Autoencoders (VAEs), a class of latent-variable generative models, have seen extensive use in high-fidelity synthesis tasks, yet their loss landscape remains poorly understood. Prior theoretical works on VAE loss analysis have focused on their latent-space representational capabilities, both in the optimal and limiting cases. Although these insights have guided better VAE designs, they also often restrict VAEs to problem settings where classical algorithms, such as Principal Component Analysis (PCA), can trivially guarantee globally optimal solutions. In this work, we push the boundaries of our understanding of VAEs beyond these traditional regimes to tackle NP-hard sparse inverse problems, for which no classical algorithms exist. Specifically, we examine the nontrivial Sparse Linear Regression (SLR) problem of recovering optimal sparse inputs in the presence of an ill-conditioned design matrix having correlated features. We provably show that, under a linear encoder-decoder architecture incorporating the product of the SLR design matrix with a trainable, sparsity-promoting diagonal matrix, any minimum of VAE loss is guaranteed to be an optimal solution. This property is especially useful for identifying (a) a preconditioning factor that reduces the eigenvalue spread, and (b) the corresponding optimal sparse representation. Lastly, our empirical analysis with different types of design matrices validates these findings and even demonstrates a higher recovery rate at low sparsity where traditional algorithms fail. Overall, this work highlights the flexible nature of the VAE loss, which can be adapted to efficiently solve computationally hard problems under specific constraints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LiXD7mpjU0": {
    "title": "Incremental Gradient Descent with Small Epoch Counts is Surprisingly Slow on Ill-Conditioned Problems",
    "volume": "poster",
    "abstract": "Recent theoretical results demonstrate that the convergence rates of permutation-based SGD (e.g., random reshuffling SGD) are faster than uniform-sampling SGD; however, these studies focus mainly on the large epoch regime, where the number of epochs $K$ exceeds the condition number $\\kappa$. In contrast, little is known when $K$ is smaller than $\\kappa$, and it is still a challenging open question whether permutation-based SGD can converge faster in this small epoch regime (Safran and Shamir, 2021). As a step toward understanding this gap, we study the naive deterministic variant, Incremental Gradient Descent (IGD), on smooth and strongly convex functions. Our lower bounds reveal that for the small epoch regime, IGD can exhibit surprisingly slow convergence even when all component functions are strongly convex. Furthermore, when some component functions are allowed to be nonconvex, we prove that the optimality gap of IGD can be significantly worse throughout the small epoch regime. Our analyses reveal that the convergence properties of permutation-based SGD in the small epoch regime may vary drastically depending on the assumptions on component functions. Lastly, we supplement the paper with tight upper and lower bounds for IGD in the large epoch regime",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9rLxi2cnZC": {
    "title": "Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty",
    "volume": "poster",
    "abstract": "Recent advances in deep learning rely heavily on massive datasets, leading to substantial storage and training costs. Dataset pruning aims to alleviate this demand by discarding redundant examples. However, many existing methods require training a model with a full dataset over a large number of epochs before being able to prune the dataset, which ironically makes the pruning process more expensive than just training the model on the entire dataset. To overcome this limitation, we introduce the **Difficulty and Uncertainty-Aware Lightweight (DUAL)** score, which aims to identify important samples from the early training stage by considering both example difficulty and prediction uncertainty. To address a catastrophic accuracy drop at an extreme pruning ratio, we further propose a pruning ratio-adaptive sampling using Beta distribution. Experiments on various datasets and learning scenarios such as image classification with label noise and image corruption, and model architecture generalization demonstrate the superiority of our method over previous state-of-the-art (SOTA) approaches. Specifically, on ImageNet-1k, our method reduces the time cost for pruning to 66\\% compared to previous methods while achieving a SOTA 60\\% test accuracy at a 90\\% pruning ratio. On CIFAR datasets, the time cost is reduced to just 15\\% while maintaining SOTA performance",
    "checked": true,
    "id": "85747d3bf259c6b6344e9c2b306417f9718ad75c",
    "semantic_title": "lightweight dataset pruning without full training via example difficulty and prediction uncertainty",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=SY4owu5BK6": {
    "title": "The Case for Learned Provenance-based System Behavior Baseline",
    "volume": "poster",
    "abstract": "Provenance graphs describe data flows and causal dependencies of host activities, enabling to track the data propagation and manipulation throughout the systems, which provide a foundation for intrusion detection. However, these Provenance-based Intrusion Detection Systems (PIDSes) face significant challenges in storage, representation, and analysis, which impede the efficacy of machine learning models such as Graph Neural Networks (GNNs) in processing and learning from these graphs. This paper presents a novel learning-based anomaly detection method designed to efficiently embed and analyze large-scale provenance graphs. Our approach integrates dynamic graph processing with adaptive encoding, facilitating compact embeddings that effectively address out-of-vocabulary (OOV) elements and adapt to normality shifts in dynamic real-world environments. Subsequently, we incorporate this refined baseline into a tag-propagation framework for real-time detection. Our evaluation demonstrates the method's accuracy and adaptability in anomaly path mining, significantly advancing the state-of-the-art in handling and analyzing provenance graphs for anomaly detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ba3sSfEnj1": {
    "title": "Flexible, Efficient, and Stable Adversarial Attacks on Machine Unlearning",
    "volume": "poster",
    "abstract": "Machine unlearning (MU) aims to remove the influence of specific data points from trained models, enhancing compliance with privacy regulations. However, the vulnerability of basic MU models to malicious unlearning requests in adversarial learning environments has been largely overlooked. Existing adversarial MU attacks suffer from three key limitations: inflexibility due to pre-defined attack targets, inefficiency in handling multiple attack requests, and instability caused by non-convex loss functions. To address these challenges, we propose a Flexible, Efficient, and Stable Attack (DDPA). First, leveraging Carathéodory's theorem, we introduce a convex polyhedral approximation to identify points in the loss landscape where convexity approximately holds, ensuring stable attack performance. Second, inspired by simplex theory and John's theorem, we develop a regular simplex detection technique that maximizes coverage over the parameter space, improving attack flexibility and efficiency. We theoretically derive the proportion of the effective parameter space occupied by the constructed simplex. We evaluate the attack success rate of our DDPA method on real datasets against state-of-the-art machine unlearning attack methods. Our source code is available at https://github.com/zzz0134/DDPA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O0lxLP4ABD": {
    "title": "PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization",
    "volume": "poster",
    "abstract": "Pipeline parallelism (PP) is widely used for training large language models (LLMs), yet its scalability is often constrained by high activation memory consumption as the number of in-flight microbatches grows with the degree of PP. In this paper, we focus on addressing this challenge by leveraging the under-explored memory offload strategy in PP. With empirical study, we discover that in the majority of standard configurations, at least half, and potentially all, of the activations can be offloaded with negligible overhead. In the cases where full overload is not possible, we introduce a novel selective offload strategy that decreases peak activation memory in a better-than-linear manner. Furthermore, we integrate memory offload with other techniques to jointly consider overall throughput and memory limitation. Our experiments proves that the per-device activation memory effectively reduces with the total number of stages, making PP a stronger alternative than TP, offering up to a 19\\% acceleration with even lower memory consumption",
    "checked": true,
    "id": "3293cf9cf1f2682322a22ee359c112e1b567d625",
    "semantic_title": "pipeoffload: improving scalability of pipeline parallelism with memory optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z4XS0Ie391": {
    "title": "Unified Screening for Multiple Diseases",
    "volume": "poster",
    "abstract": "Current screening programs that focus on improving patient health while minimizing screening costs are tailored for individual diseases. Designing unified screening programs for multiple diseases requires carefully balancing competing disease risks, which is an open problem. In this work, we address this problem by casting unified screening as a referral problem, in which we choose to activate a subset of screening policies for individual diseases by accounting for competing risks that influence patient outcomes. We introduce a novel optimization framework that incorporates disease risks, budget constraints, and diagnostic error limits and characterize the structural properties of the optimal referral policy. For the unified screening of two diseases, we show that the optimal activation threshold for the screening of one disease depends on the risk of the other, resulting in decision boundaries with distinct risk-dependent profiles. We compare our unified model with independent screening programs that apply isolated activation thresholds for screening of each disease. Our approach optimizes screening decisions collectively, improving overall survival outcomes, particularly for patients with high disease risks",
    "checked": false,
    "id": "4474264ab36adf069f02f97de3d5dc7f3f73cef4",
    "semantic_title": "a self-supervised classification model for endometrial diseases",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=39JKH8k3FS": {
    "title": "Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants",
    "volume": "poster",
    "abstract": "This paper investigates causal effect identification in latent variable Linear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants, addressing two prominent setups that are challenging in the presence of latent confounding: (1) a single proxy variable that may causally influence the treatment and (2) underspecified instrumental variable cases where fewer instruments exist than treatments. We prove that causal effects are identifiable with a single proxy or instrument and provide corresponding estimation methods. Experimental results demonstrate the accuracy and robustness of our approaches compared to existing methods, advancing the theoretical and practical understanding of causal inference in linear systems with latent confounders",
    "checked": true,
    "id": "ba74db439661c1648950121df41adb3987d8eafe",
    "semantic_title": "causal effect identification in lvlingam from higher-order cumulants",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qF6mxani2X": {
    "title": "STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings",
    "volume": "poster",
    "abstract": "Given how large parts of publicly available text are crawled to pretrain large language models (LLMs), data creators increasingly worry about the inclusion of their proprietary data for model training without attribution or licensing. Their concerns are also shared by benchmark curators whose test-sets might be compromised. In this paper, we present STAMP, a framework for detecting dataset membership—i.e., determining the inclusion of a dataset in the pretraining corpora of LLMs. Given an original piece of content, our proposal involves first generating multiple rephrases, each embedding a watermark with a unique secret key. One version is to be released publicly, while others are to be kept private. Subsequently, creators can compare model likelihoods between public and private versions using paired statistical tests to prove membership. We show that our framework can successfully detect contamination across four benchmarks which appear only once in the training data and constitute less than 0.001% of the total tokens, outperforming several contamination detection and dataset inference baselines. We verify that STAMP preserves both the semantic meaning and utility of the original data. We apply STAMP to two real-world scenarios to confirm the inclusion of paper abstracts and blog articles in the pretraining corpora",
    "checked": true,
    "id": "d1006b6233f86e67ed2283c084e1dacfbdc3e71b",
    "semantic_title": "stamp your content: proving dataset membership via watermarked rephrasings",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jHLSnYNt1m": {
    "title": "Counterfactual Effect Decomposition in Multi-Agent Sequential Decision Making",
    "volume": "poster",
    "abstract": "We address the challenge of explaining counterfactual outcomes in multi-agent Markov decision processes. In particular, we aim to explain the total counterfactual effect of an agent's action on the outcome of a realized scenario through its influence on the environment dynamics and the agents' behavior. To achieve this, we introduce a novel causal explanation formula that decomposes the counterfactual effect by attributing to each agent and state variable a score reflecting their respective contributions to the effect. First, we show that the total counterfactual effect of an agent's action can be decomposed into two components: one measuring the effect that propagates through all subsequent agents' actions and another related to the effect that propagates through the state transitions. Building on recent advancements in causal contribution analysis, we further decompose these two effects as follows. For the former, we consider agent-specific effects -- a causal concept that quantifies the counterfactual effect of an agent's action that propagates through a subset of agents. Based on this notion, we use Shapley value to attribute the effect to individual agents. For the latter, we consider the concept of structure-preserving interventions and attribute the effect to state variables based on their \"intrinsic'' contributions. Through extensive experimentation, we demonstrate the interpretability of our approach in a Gridworld environment with LLM-assisted agents and a sepsis management simulator",
    "checked": true,
    "id": "5b62497f46bebc4073858a06e901406519c32257",
    "semantic_title": "counterfactual effect decomposition in multi-agent sequential decision making",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JsPyLqCgks": {
    "title": "A Mixed-Curvature based Pre-training Paradigm for Multi-Task Vehicle Routing Solver",
    "volume": "poster",
    "abstract": "Solving various types of vehicle routing problems (VRPs) using a unified neural solver has garnered significant attentions in recent years. Despite their effectiveness, existing neural multi-task solvers often fail to account for the geometric structures inherent in different tasks, which may result in suboptimal performance. To address this limitation, we propose a curvature-aware pre-training framework. Specifically, we leverage mixed-curvature spaces during the feature fusion stage, encouraging the model to capture the underlying geometric properties of each instance. Through extensive experiments, we evaluate the proposed pre-training strategy on existing neural multi-task solvers across a variety of testing scenarios. The results demonstrate that the curvature-aware pre-training approach not only enhances the generalization capabilities of existing neural VRP solvers on synthetic datasets but also improves solution quality on real-world benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rxKC8v2uHc": {
    "title": "GRAM: A Generative Foundation Reward Model for Reward Generalization",
    "volume": "poster",
    "abstract": "In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aTC2euLwnh": {
    "title": "Fine-Grained Captioning of Long Videos through Scene Graph Consolidation",
    "volume": "poster",
    "abstract": "Recent advances in vision-language models have led to impressive progress in caption generation for images and short video clips. However, these models remain constrained by their limited temporal receptive fields, making it difficult to produce coherent and comprehensive captions for long videos. While several methods have been proposed to aggregate information across video segments, they often rely on supervised fine-tuning or incur significant computational overhead. To address these challenges, we introduce a novel framework for long video captioning based on graph consolidation. Our approach first generates segment-level captions, corresponding to individual frames or short video intervals, using off-the-shelf visual captioning models. These captions are then parsed into individual scene graphs, which are subsequently consolidated into a unified graph representation that preserves both holistic context and fine-grained details throughout the video. A lightweight graph-to-text decoder then produces the final video-level caption. This framework effectively extends the temporal understanding capabilities of existing models without requiring any additional fine-tuning on long video datasets. Experimental results show that our method significantly outperforms existing LLM-based consolidation approaches, achieving strong zero-shot performance while substantially reducing computational costs",
    "checked": true,
    "id": "4c6757524f7272e43274fa8071c2e0682d53e848",
    "semantic_title": "fine-grained captioning of long videos through scene graph consolidation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=tYwKQMMjJA": {
    "title": "M3-JEPA: Multimodal Alignment via Multi-gate MoE based on the Joint-Embedding Predictive Architecture",
    "volume": "poster",
    "abstract": "Current multimodal learning strategies primarily optimize in the original token space. Such a framework is easy to incorporate with the backbone of pretrained language model, but might result in modality collapse. To alleviate such issues, we leverage the Joint-Embedding Predictive Architecture (JEPA) on the multimodal tasks, which converts the input embedding into the output embedding space by a predictor and then conducts the cross-modal alignment on the latent space. We implement this predictor by a Multi-Gate Mixture of Experts (MMoE) and name the framework as M3-JEPA, accordingly. The gating function disentangles the modality-specific and shared information and derives information-theoretic optimality. The framework is implemented with both contrastive and regularization loss, and solved by alternative gradient descent (AGD) between different multimodal tasks. By thoroughly designed experiments, we show that M3-JEPA can obtain state-of-the-art performance on different modalities and tasks, generalize to unseen datasets and domains, and is computationally efficient in both training and inference. Our observation suggests that M3-JEPA might become a new basis to self-supervised learning in the open world",
    "checked": true,
    "id": "2a0d86ea61f05fadb693730c3b06cbe850e491d0",
    "semantic_title": "m3-jepa: multimodal alignment via multi-gate moe based on the joint-embedding predictive architecture",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qzM37nOy3N": {
    "title": "Inverse problems with experiment-guided AlphaFold",
    "volume": "poster",
    "abstract": "Proteins exist as a dynamic ensemble of multiple conformations, and these motions are often crucial for their functions. However, current structure prediction methods predominantly yield a single conformation, overlooking the conformational heterogeneity revealed by diverse experimental modalities. Here, we present a framework for building experiment-grounded protein structure generative models that infer conformational ensembles consistent with measured experimental data. The key idea is to treat state-of-the-art protein structure predictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and cast ensemble modeling as posterior inference of protein structures given experimental measurements. Through extensive real-data experiments, we demonstrate the generality of our method to incorporate a variety of experimental measurements. In particular, our framework uncovers previously unmodeled conformational heterogeneity from crystallographic densities, generates high-accuracy NMR ensembles orders of magnitude faster than status quo, and incorporates pairwise cross-link constraints. Notably, we demonstrate that our ensembles outperform AlphaFold3 and sometimes better fit experimental data than publicly deposited structures to the protein database (PDB). We believe that this approach will unlock building predictive models that fully embrace experimentally observed conformational diversity",
    "checked": true,
    "id": "ea1d7a8b16668f20f9c7a85e27f2a18d83e213c2",
    "semantic_title": "inverse problems with experiment-guided alphafold",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=P9DQ2IExgS": {
    "title": "Synthesizing Software Engineering Data in a Test-Driven Manner",
    "volume": "poster",
    "abstract": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in Test-Driven Development (TDD). Unlike existing software engineering data that rely on human-submitted issues, **SWE-Flow** automatically infers incremental development steps directly from unit tests, which inherently encapsulate high-level requirements. The core of **SWE-Flow** is the construction of a Runtime Dependency Graph (RDG), which precisely captures function interactions, enabling the generation of a structured, step-by-step *development schedule*. At each step, **SWE-Flow** produces a partial codebase, the corresponding unit tests, and the necessary code modifications, resulting in fully verifiable TDD tasks. With this approach, we generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, creating the **SWE-Flow-Eval** benchmark. Our experiments show that fine-tuning open model on this dataset significantly improves performance in TDD-based coding. To facilitate further research, we release all code, datasets, models, and Docker images at [Github](https://github.com/Hambaobao/SWE-Flow)",
    "checked": false,
    "id": "1ad9d18a0ea4addea188e8f6b7b6c97834550c01",
    "semantic_title": "swe-flow: synthesizing software engineering data in a test-driven manner",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=mQE0EsrX1y": {
    "title": "AEQA-NAT : Adaptive End-to-end Quantization Alignment Training Framework for Non-autoregressive Machine Translation",
    "volume": "poster",
    "abstract": "Non-autoregressive Transformers (NATs) have garnered significant attention due to their efficient decoding compared to autoregressive methods. However, existing conditional dependency modeling schemes based on masked language modeling introduce a *training-inference gap* in NATs. For instance, while NATs sample target words during training to enhance input, this condition cannot be met during inference, and simply annealing the sampling rate to zero during training leads to model performance degradation. We demonstrate that this *training-inference gap* prevents NATs from fully realizing their potential. To address this, we propose an adaptive end-to-end quantization alignment training framework, which introduces a semantic consistency space to adaptively align NAT training, eliminating the need for target information and thereby bridging the *training-inference gap*.Experimental results demonstrate that our method outperforms most existing fully NAT models, delivering performance on par with Autoregressive Transformer (AT) while being 17.0 times more efficient in inference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cYNBsMTAVL": {
    "title": "Portable Reward Tuning: Towards Reusable Fine-Tuning across Different Pretrained Models",
    "volume": "poster",
    "abstract": "While foundation models have been exploited for various expert tasks with their fine-tuned parameters, any foundation model will be eventually outdated due to its old knowledge or limited capability, and thus should be replaced by a new foundation model. Subsequently, to benefit from its latest knowledge or improved capability, the new foundation model should be fine-tuned on each task again, which incurs not only the additional training cost but also the maintenance cost of the task-specific data. Existing work address this problem by inference-time tuning, i.e., modifying the output probability from the new foundation model by the outputs from the old foundation model and its fine-tuned model, which involves an additional inference cost by the latter two models. In this paper, we explore a new fine-tuning principle (which we call portable reward tuning; PRT) that reduces the inference cost by its nature, based on the reformulation of fine-tuning as the reward maximization with Kullback-Leibler regularization. Specifically, instead of fine-tuning parameters of the foundation models, PRT trains the reward model explicitly through the same loss as in fine-tuning. During inference, the reward model can be used with any foundation model (with the same set of vocabularies or labels) through the formulation of reward maximization. Experimental results, including both vision and language models, show that the PRT-trained model can achieve comparable accuracy with less inference cost, in comparison to the existing work of inference-time tuning",
    "checked": true,
    "id": "8375d8ebcebbcb9458b94ab5872a092c82e2f549",
    "semantic_title": "portable reward tuning: towards reusable fine-tuning across different pretrained models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CXPpYJpYXQ": {
    "title": "LOB-Bench: Benchmarking Generative AI for Finance - an Application to Limit Order Book Data",
    "volume": "poster",
    "abstract": "While financial data presents one of the most challenging and interesting sequence modelling tasks due to high noise, heavy tails, and strategic interactions, progress in this area has been hindered by the lack of consensus on quantitative evaluation paradigms. To address this, we present **LOB-Bench**, a benchmark, implemented in python, designed to evaluate the quality and realism of generative message-by-order data for limit order books (LOB) in the LOBSTER format. Our framework measures distributional differences in conditional and unconditional statistics between generated and real LOB data, supporting flexible multivariate statistical evaluation. The benchmark also includes features commonly used LOB statistics such as spread, order book volumes, order imbalance, and message inter-arrival times, along with scores from a trained discriminator network. Lastly, LOB-Bench contains \"market impact metrics\", i.e. the cross-correlations and price response functions for specific events in the data. We benchmark generative autoregressive state-space models, a (C)GAN, as well as a parametric LOB model and find that the autoregressive GenAI approach beats traditional model classes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cf8gsqWrua": {
    "title": "Comparing Comparisons: Informative and Easy Human Feedback with Distinguishability Queries",
    "volume": "poster",
    "abstract": "Learning human objectives from preference feedback has significantly advanced reinforcement learning (RL) in domains where objectives are hard to formalize. However, traditional methods based on pairwise trajectory comparisons face notable challenges, including the difficulty in comparing trajectories with subtle differences and the limitation of conveying only ordinal information, limiting direct inference of preference strength. In this paper, we introduce a novel *distinguishability query*, enabling humans to express preference strength by comparing two pairs of trajectories. Labelers first indicate which of two pairs is easier to distinguish, then provide preference feedback only on the easier pair. Our proposed query type directly captures preference strength and is expected to reduce the cognitive load on the labeler. We further connect this query to cardinal utility and difference relations and develop an efficient query selection scheme to achieve a better trade-off between query informativeness and easiness. Experimental results demonstrate the potential of our method for faster, data-efficient learning and improved user-friendliness in RLHF benchmarks, particularly in classical control settings where preference strength is critical for expected utility maximization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z9Xugry05b": {
    "title": "BounDr.E: Predicting Drug-likeness via Biomedical Knowledge Alignment and EM-like One-Class Boundary Optimization",
    "volume": "poster",
    "abstract": "The advent of generative AI now enables large-scale $\\textit{de novo}$ design of molecules, but identifying viable drug candidates among them remains an open problem. Existing drug-likeness prediction methods often rely on ambiguous negative sets or purely structural features, limiting their ability to accurately classify drugs from non-drugs. In this work, we introduce BounDr.E}: a novel modeling of drug-likeness as a compact space surrounding approved drugs through a dynamic one-class boundary approach. Specifically, we enrich the chemical space through biomedical knowledge alignment, and then iteratively tighten the drug-like boundary by pushing non-drug-like compounds outside via an Expectation-Maximization (EM)-like process. Empirically, BounDr.E achieves 10\\% F1-score improvement over the previous state-of-the-art and demonstrates robust cross-dataset performance, including zero-shot toxic compound filtering. Additionally, we showcase its effectiveness through comprehensive case studies in large-scale $\\textit{in silico}$ screening. Our codes and constructed benchmark data under various schemes are provided at: https://github.com/eugenebang/boundr_e",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZuaU2bYzlc": {
    "title": "Private Federated Learning using Preference-Optimized Synthetic Data",
    "volume": "poster",
    "abstract": "In practical settings, differentially private federated learning (DP-FL) is the dominant method for training models from private, on-device client data. Recent work has suggested that DP-FL may be enhanced or outperformed by methods that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary algorithms for generating DP synthetic data for FL applications require careful prompt engineering based on public information and/or iterative private client feedback. Our key insight is that the private client feedback collected by prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be viewed as a preference ranking. Our algorithm, Preference Optimization for Private Client Data (POPri) harnesses client feedback using preference optimization algorithms such as Direct Preference Optimization (DPO) to fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri, we release LargeFedBench, a new federated text benchmark for uncontaminated LLM evaluations on federated client data. POPri closes the gap in next-token prediction accuracy between the fully-private and non-private settings by up to 68%, compared to 52% for prior synthetic data methods, and 10% for state-of-the-art DP federated learning methods. The code and data are available at https://github.com/meiyuw/POPri",
    "checked": true,
    "id": "3973be966aab31a0150d6dd90604377ba6c5f01b",
    "semantic_title": "private federated learning using preference-optimized synthetic data",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=KBUSuiLBMq": {
    "title": "Provable Benefit of Random Permutations over Uniform Sampling in Stochastic Coordinate Descent",
    "volume": "poster",
    "abstract": "We analyze the convergence rates of two popular variants of coordinate descent (CD): random CD (RCD), in which the coordinates are sampled uniformly at random, and random-permutation CD (RPCD), in which random permutations are used to select the update indices. Despite abundant empirical evidence that RPCD outperforms RCD in various tasks, the theoretical gap between the two algorithms' performance has remained elusive. Even for the benign case of positive-definite quadratic functions with permutation-invariant Hessians, previous efforts have failed to demonstrate a provable performance gap between RCD and RPCD. To this end, we present novel results showing that, for a class of quadratics with permutation-invariant structures, the contraction rate upper bound for RPCD is always strictly smaller than the contraction rate lower bound for RCD for every individual problem instance. Furthermore, we conjecture that this function class contains the worst-case examples of RPCD among all positive-definite quadratics. Combined with our RCD lower bound, this conjecture extends our results to the general class of positive-definite quadratic functions",
    "checked": true,
    "id": "edb3208d10fa0bc06934e53319f6c45488375d64",
    "semantic_title": "provable benefit of random permutations over uniform sampling in stochastic coordinate descent",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WcFLasjwXs": {
    "title": "Optimal and Practical Batched Linear Bandit Algorithm",
    "volume": "poster",
    "abstract": "We study the linear bandit problem under limited adaptivity, known as the batched linear bandit. While existing approaches can achieve near-optimal regret in theory, they are often computationally prohibitive or underperform in practice. We propose BLAE, a novel batched algorithm that integrates arm elimination with regularized G-optimal design, achieving the minimax optimal regret (up to logarithmic factors in $T$) in both large-$K$ and small-$K$ regimes for the first time, while using only $O(\\log\\log T)$ batches. Our analysis introduces new techniques for batch-wise optimal design and refined concentration bounds. Crucially, BLAE demonstrates low computational overhead and strong empirical performance, outperforming state-of-the-art methods in extensive numerical evaluations. Thus, BLAE is the first algorithm to combine provable minimax-optimality in all regimes and practical superiority in batched linear bandits",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S4HPn5Bo6k": {
    "title": "SAE-V: Interpreting Multimodal Models for Enhanced Alignment",
    "volume": "poster",
    "abstract": "With the integration of image modality, the semantic space of multimodal large language models (MLLMs) is more complex than text-only models, making their interpretability more challenging and their alignment less stable, particularly susceptible to low-quality data, which can lead to inconsistencies between modalities, hallucinations, and biased outputs. As a result, developing interpretability methods for MLLMs is crucial for improving alignment quality and efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained attention for their ability to interpret latent representations. However, extending SAEs to multimodal settings presents new challenges due to modality fusion and the difficulty of isolating cross-modal representations. To address these challenges, we introduce SAE-V, a mechanistic interpretability framework that extends the SAE paradigm to MLLMs. By identifying and analyzing interpretable features along with their corresponding data, SAE-V enables fine-grained interpretation of both model behavior and data quality, facilitating a deeper understanding of cross-modal interactions and alignment dynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides an intrinsic data filtering mechanism to enhance model alignment without requiring additional models. Specifically, when applied to the alignment process of MLLMs, SAE-V-based data filtering methods could achieve more than 110% performance with less than 50% data. Our results highlight SAE-V's ability to enhance interpretability and alignment in MLLMs, providing insights into their internal mechanisms",
    "checked": true,
    "id": "bd965bc125f6dadc4aaf86655613af30286430aa",
    "semantic_title": "sae-v: interpreting multimodal models for enhanced alignment",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=CdFnEu0JZV": {
    "title": "OR-Bench: An Over-Refusal Benchmark for Large Language Models",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs. While significant research focuses on mitigating harmful content generation, the enhanced safety often come with the side effect of over-refusal, where LLMs may reject innocuous prompts and become less helpful. Although the issue of over-refusal has been empirically observed, a systematic measurement is challenging due to the difficulty of crafting prompts that can elicit the over-refusal behaviors of LLMs. This study proposes a novel method for automatically generating large-scale over-refusal datasets. Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises 80,000 over-refusal prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses. We then conduct a comprehensive study to measure the over-refusal of 32 popular LLMs across 8 model families. Our datasets are publicly available at https://huggingface.co/bench-llms and our codebase is open-sourced at https://github.com/justincui03/or-bench. We hope this benchmark can help the community develop better safety aligned models",
    "checked": true,
    "id": "d30e8ac45470dcc9208edb7a518d69088ae925e8",
    "semantic_title": "or-bench: an over-refusal benchmark for large language models",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=LhkSfpfRXW": {
    "title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation",
    "volume": "poster",
    "abstract": "As large language models (LMs) advance, there is an increasing need to control their outputs to align with human values (e.g., detoxification) or desired attributes (e.g., personalization, topic). However, autoregressive models focus on next-token predictions and struggle with global properties that require looking ahead. Existing solutions either post-train LMs for each new attribute—expensive and inflexible—or approximate the Expected Attribute Probability (EAP) of future sequences by sampling or training, which is slow and unreliable for rare attributes. We introduce **TRACE** (Tractable Probabilistic Reasoning for Adaptable Controllable gEneration), a novel framework that efficiently computes EAP and adapts to new attributes through tractable *probabilistic* reasoning and lightweight *control*. TRACE distills a Hidden Markov Model (HMM) from an LM and pairs it with a small classifier to estimate attribute probabilities, enabling exact EAP computation over the HMM's predicted futures. This EAP is then used to reweigh the LM's next-token probabilities for globally compliant continuations. Empirically, TRACE achieves state-of-the-art detoxification results with only 20% decoding overhead, yields 76 low-resource personalized LMs within seconds, and seamlessly extends to composite attributes",
    "checked": true,
    "id": "84d02dc0b708438f452d8b07bcf74d5b1a1b72eb",
    "semantic_title": "trace back from the future: a probabilistic reasoning approach to controllable language generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aLDAu7QDw0": {
    "title": "AtlasD: Automatic Local Symmetry Discovery",
    "volume": "poster",
    "abstract": "Existing symmetry discovery methods predominantly focus on global transformations across the entire system or space, but they fail to consider the symmetries in local neighborhoods. This may result in the reported symmetry group being a misrepresentation of the true symmetry. In this paper, we formalize the notion of local symmetry as atlas equivariance. Our proposed pipeline, automatic local symmetry discovery (AtlasD), recovers the local symmetries of a function by training local predictor networks and then learning a Lie group basis to which the predictors are equivariant. We demonstrate AtlasD is capable of discovering local symmetry groups with multiple connected components in top-quark tagging and partial differential equation experiments. The discovered local symmetry is shown to be a useful inductive bias that improves the performance of downstream tasks in climate segmentation and vision tasks. Our code is publicly available at https://github.com/Rose-STL-Lab/AtlasD",
    "checked": true,
    "id": "d522235f525c9609bf4de5e6368bdeb8a2d4d679",
    "semantic_title": "atlasd: automatic local symmetry discovery",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kWSRVtuIuH": {
    "title": "Generalization Analysis for Supervised Contrastive Representation Learning under Non-IID Settings",
    "volume": "poster",
    "abstract": "Contrastive Representation Learning (CRL) has achieved impressive success in various domains in recent years. Nevertheless, the theoretical understanding of the generalization behavior of CRL has remained limited. Moreover, to the best of our knowledge, the current literature only analyzes generalization bounds under the assumption that the data tuples used for contrastive learning are independently and identically distributed. However, in practice, we are often limited to a fixed pool of reusable labeled data points, making it inevitable to recycle data across tuples to create sufficiently large datasets. Therefore, the tuple-wise independence condition imposed by previous works is invalidated. In this paper, we provide a generalization analysis for the CRL framework under non-$i.i.d.$ settings that adheres to practice more realistically. Drawing inspiration from the literature on U-statistics, we derive generalization bounds which indicate that the required number of samples in each class scales as the logarithm of the covering number of the class of learnable feature representations associated to that class. Next, we apply our main results to derive excess risk bounds for common function classes such as linear maps and neural networks",
    "checked": true,
    "id": "b3d753758f1bca5d947dcba59350e23c3ab07e0f",
    "semantic_title": "generalization analysis for supervised contrastive representation learning under non-iid settings",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F1yANMCnAn": {
    "title": "A Theoretical Justification for Asymmetric Actor-Critic Algorithms",
    "volume": "poster",
    "abstract": "In reinforcement learning for partially observable environments, many successful algorithms have been developed within the asymmetric learning paradigm. This paradigm leverages additional state information available at training time for faster learning. Although the proposed learning objectives are usually theoretically sound, these methods still lack a precise theoretical justification for their potential benefits. We propose such a justification for asymmetric actor-critic algorithms with linear function approximators by adapting a finite-time convergence analysis to this setting. The resulting finite-time bound reveals that the asymmetric critic eliminates error terms arising from aliasing in the agent state",
    "checked": true,
    "id": "1053a3c5bced66e4ce07aec4e50b47539cd836b9",
    "semantic_title": "a theoretical justification for asymmetric actor-critic algorithms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fd7ddFBNmP": {
    "title": "Reducing Confounding Bias without Data Splitting for Causal Inference via Optimal Transport",
    "volume": "poster",
    "abstract": "Causal inference seeks to estimate the effect given a treatment such as a medicine or the dosage of a medication. To reduce the confounding bias caused by the non-randomized treatment assignment, most existing methods reduce the shift between subpopulations receiving different treatments. However, these methods split limited training samples into smaller groups, which cuts down the number of samples in each group, while precise distribution estimation and alignment highly rely on a sufficient number of training samples. In this paper, we propose a distribution alignment paradigm without data splitting, which can be naturally applied in the settings of binary and continuous treatments. To this end, we characterize the confounding bias by considering different probability measures of the same set including all the training samples, and exploit the optimal transport theory to analyze the confounding bias and outcome estimation error. Based on this, we propose to learn balanced representations by reducing the bias between the marginal distribution and the conditional distribution of a treatment. As a result, data reduction caused by splitting is avoided, and the outcome prediction model trained on one treatment group can be generalized to the entire population. The experiments on both binary and continuous treatment settings demonstrate the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6znPjYn11w": {
    "title": "Provably Near-Optimal Federated Ensemble Distillation with Negligible Overhead",
    "volume": "poster",
    "abstract": "Federated ensemble distillation addresses client heterogeneity by generating pseudo-labels for an unlabeled server dataset based on client predictions and training the server model using the pseudo-labeled dataset. The unlabeled server dataset can either be pre-existing or generated through a data-free approach. The effectiveness of this approach critically depends on the method of assigning weights to client predictions when creating pseudo-labels, especially in highly heterogeneous settings. Inspired by theoretical results from GANs, we propose a provably near-optimal weighting method that leverages client discriminators trained with a server-distributed generator and local datasets. Our experiments on various image classification tasks demonstrate that the proposed method significantly outperforms baselines. Furthermore, we show that the additional communication cost, client-side privacy leakage, and client-side computational overhead introduced by our method are negligible, both in scenarios with and without a pre-existing server dataset",
    "checked": true,
    "id": "a9373cd9d8f782061e53e46668d8a5969154e8c9",
    "semantic_title": "provably near-optimal federated ensemble distillation with negligible overhead",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=waeJHU2oeI": {
    "title": "Ensemble Distribution Distillation via Flow Matching",
    "volume": "poster",
    "abstract": "Neural network ensembles have proven effective in improving performance across a range of tasks; however, their high computational cost limits their applicability in resource-constrained environments or for large models. Ensemble distillation, the process of transferring knowledge from an ensemble teacher to a smaller student model, offers a promising solution to this challenge. The key is to ensure that the student model is both cost-efficient and achieves performance comparable to the ensemble teacher. With this in mind, we propose a novel ensemble distribution distillation method, which leverages flow matching to effectively transfer the diversity from the ensemble teacher to the student model. Our extensive experiments demonstrate the effectiveness of our proposed method compared to existing ensemble distillation approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JKsxKPXXUd": {
    "title": "Deep Ridgelet Transform and Unified Universality Theorem for Deep and Shallow Joint-Group-Equivariant Machines",
    "volume": "poster",
    "abstract": "We present a constructive universal approximation theorem for learning machines equipped with joint-group-equivariant feature maps, called the joint-equivariant machines, based on the group representation theory. ``Constructive'' here indicates that the distribution of parameters is given in a closed-form expression known as the ridgelet transform. Joint-group-equivariance encompasses a broad class of feature maps that generalize classical group-equivariance. Particularly, fully-connected networks are *not* group-equivariant *but* are joint-group-equivariant. Our main theorem also unifies the universal approximation theorems for both shallow and deep networks. Until this study, the universality of deep networks has been shown in a different manner from the universality of shallow networks, but our results discuss them on common ground. Now we can understand the approximation schemes of various learning machines in a unified manner. As applications, we show the constructive universal approximation properties of four examples: depth-$n$ joint-equivariant machine, depth-$n$ fully-connected network, depth-$n$ group-convolutional network, and a new depth-$2$ network with quadratic forms whose universality has not been known",
    "checked": true,
    "id": "70496190812a582f378886b9153f631f7ce2677d",
    "semantic_title": "deep ridgelet transform and unified universality theorem for deep and shallow joint-group-equivariant machines",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Ofa1cspTrv": {
    "title": "Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors",
    "volume": "poster",
    "abstract": "Interpretability research now offers a variety of techniques for identifying abstract internal mechanisms in neural networks. Can such techniques be used to predict how models will behave on out-of-distribution examples? In this work, we provide a positive answer to this question. Through a diverse set of language modeling tasks—including symbol manipulation, knowledge retrieval, and instruction following—we show that the most robust features for correctness prediction are those that play a distinctive causal role in the model's behavior. Specifically, we propose two methods that leverage causal mechanisms to predict the correctness of model outputs: counterfactual simulation (checking whether key causal variables are realized) and value probing (using the values of those variables to make predictions). Both achieve high AUC-ROC in distribution and outperform methods that rely on causal-agnostic features in out-of-distribution settings, where predicting model behaviors is more crucial. Our work thus highlights a novel and significant application for internal causal analysis of language models",
    "checked": true,
    "id": "05877e76088d0f7d1363f287f44e359708667579",
    "semantic_title": "internal causal mechanisms robustly predict language model out-of-distribution behaviors",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vISiVCssVg": {
    "title": "Adversarial Robustness via Deformable Convolution with Stochasticity",
    "volume": "poster",
    "abstract": "Random defense represents a promising strategy to protect neural networks from adversarial attacks. Most of these methods enhance robustness by injecting randomness into the data, increasing uncertainty for attackers. However, this randomness could reduce the generalization capacity of defense, as defense performance could be sensitive to the hyperparameters of noise added to the data, making it difficult to generalize across different datasets. Additionally, the involvement of randomness always comes with a reduction of natural accuracy, which leads to a delicate trade-off between them, which is seldom studied in random defense. In this work, we propose incorporating randomness into the network structure instead of data input by designing stochastic deformable convolution, where a random mask replaces the convolutional offset. This process promotes data independence, enhancing generalization across datasets. To study the trade-off, we conduct a theoretical analysis of both robust and clean accuracy, from a perspective of gradient cosine similarity and natural inference. Based on the analysis, we reformulate the adversarial training in our random defense framework. Extensive experiments show that our method achieves SOTA adversarial robustness and clean accuracy compared with other random defense methods",
    "checked": false,
    "id": "9e9ed5b08218f24ca6e11afaf9a1dce823eba163",
    "semantic_title": "scalable gaussian process regression enables accurate prediction of protein and small molecule properties with uncertainty quantitation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=pRmxQHgjb1": {
    "title": "UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning",
    "volume": "poster",
    "abstract": "Large Language Model (LLM) agents equipped with external tools have become increasingly powerful for complex tasks such as web shopping, automated email replies, and financial trading. However, these advancements amplify the risks of adversarial attacks, especially when agents can access sensitive external functionalities. Nevertheless, manipulating LLM agents into performing targeted malicious actions or invoking specific tools remains challenging, as these agents extensively reason or plan before executing final actions. In this work, we present UDora, a unified red teaming framework designed for LLM agents that dynamically hijacks the agent's reasoning processes to compel malicious behavior. Specifically, UDora first generates the model's reasoning trace for the given task, then automatically identifies optimal points within this trace to insert targeted perturbations. The resulting perturbed reasoning is then used as a surrogate response for optimization. By iteratively applying this process, the LLM agent will then be induced to undertake designated malicious actions or to invoke specific malicious tools. Our approach demonstrates superior effectiveness compared to existing methods across three LLM agent datasets. The code is available at https://github.com/AI-secure/UDora",
    "checked": true,
    "id": "d7615286b342dd800881a3a6c9ce70397a0012bd",
    "semantic_title": "udora: a unified red teaming framework against llm agents by dynamically hijacking their own reasoning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=WtD8EIzkmm": {
    "title": "Learning Initial Basis Selection for Linear Programming via Duality-Inspired Tripartite Graph Representation and Comprehensive Supervision",
    "volume": "poster",
    "abstract": "For the fundamental linear programming (LP) problems, the simplex method remains popular, which usually requires an appropriate initial basis as a warm start to accelerate the solving process. Predicting an initial basis close to an optimal one can often accelerate the solver, but a closer initial basis does not always result in greater acceleration. To achieve better acceleration, we propose a GNN model based on a tripartite graph representation inspired by LP duality. This approach enables more effective feature extraction for general LP problems and enhances the expressiveness of GNNs. Additionally, we introduce novel loss functions targeting basic variable selection and basis feasibility, along with data preprocessing schemes, to further improve learning capability. In addition to achieving high prediction accuracy, we enhance the quality of the initial basis for practical use. Experimental results show that our approach greatly surpasses the state-of-the-art method in predicting initial basis with greater accuracy and in reducing the number of iterations and solving time of the LP solver",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K0Vg8b7nyI": {
    "title": "PDUDT: Provable Decentralized Unlearning under Dynamic Topologies",
    "volume": "poster",
    "abstract": "This paper investigates decentralized unlearning, aiming to eliminate the impact of a specific client on the whole decentralized system. However, decentralized communication characterizations pose new challenges for effective unlearning: the indirect connections make it difficult to trace the specific client's impact, while the dynamic topology limits the scalability of retraining-based unlearning methods. In this paper, we propose the first **P**rovable **D**ecentralized **U**nlearning algorithm under **D**ynamic **T**opologies called PDUDT. It allows clients to eliminate the influence of a specific client without additional communication or retraining. We provide rigorous theoretical guarantees for PDUDT, showing it is statistically indistinguishable from perturbed retraining. Additionally, it achieves an efficient convergence rate of $\\mathcal{O}(\\frac{1}{T})$ in subsequent learning, where $T$ is the total communication rounds. This rate matches state-of-the-art results. Experimental results show that compared with the Retrain method, PDUDT saves more than 99\\% of unlearning time while achieving comparable unlearning performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1dZgzGTZEO": {
    "title": "Rethinking Score Distilling Sampling for 3D Editing and Generation",
    "volume": "poster",
    "abstract": "Score Distillation Sampling (SDS) has emerged as a prominent method for text-to-3D generation by leveraging the strengths of 2D diffusion models. However, SDS is limited to generation tasks and lacks the capability to edit existing 3D assets. Conversely, variants of SDS that introduce editing capabilities often can not generate new 3D assets effectively. In this work, we observe that the processes of generation and editing within SDS and its variants have unified underlying gradient terms. Building on this insight, we propose Unified Distillation Sampling (UDS), a method that seamlessly integrates both the generation and editing of 3D assets. Essentially, UDS refines the gradient terms used in vanilla SDS methods, unifying them to support both tasks. Extensive experiments demonstrate that UDS not only outperforms baseline methods in generating 3D assets with richer details but also excels in editing tasks, thereby bridging the gap between 3D generation and editing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2gcEQCT7QW": {
    "title": "Latent Action Learning Requires Supervision in the Presence of Distractors",
    "volume": "poster",
    "abstract": "Recently, latent action learning, pioneered by Latent Action Policies (LAPO), have shown remarkable pre-training efficiency on observation-only data, offering potential for leveraging vast amounts of video available on the web for embodied AI. However, prior work has focused on distractor-free data, where changes between observations are primarily explained by ground-truth actions. Unfortunately, real-world videos contain action-correlated distractors that may hinder latent action learning. Using Distracting Control Suite (DCS) we empirically investigate the effect of distractors on latent action learning and demonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO modification that improves the quality of latent actions by **8x**, as measured by linear probing. Importantly, we show that providing supervision with ground-truth actions, as few as 2.5% of the full dataset, during latent action learning improves downstream performance by **4.2x** on average. Our findings suggest that integrating supervision during Latent Action Models (LAM) training is critical in the presence of distractors, challenging the conventional pipeline of first learning LAM and only then decoding from latent to ground-truth actions",
    "checked": true,
    "id": "d4dee189fd92a8ab065123645d67383fd7438e5c",
    "semantic_title": "latent action learning requires supervision in the presence of distractors",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=nKJGjovmZz": {
    "title": "SafeAuto: Knowledge-Enhanced Safe Autonomous Driving with Multimodal Foundation Models",
    "volume": "poster",
    "abstract": "Traditional autonomous driving systems often struggle to connect high-level reasoning with low-level control, leading to suboptimal and sometimes unsafe behaviors. Recent advances in multimodal large language models (MLLMs), which process both visual and textual data, offer an opportunity to unify perception and reasoning. However, effectively embedding precise safety knowledge into MLLMs for autonomous driving remains a significant challenge. To address this, we propose SafeAuto, a framework that enhances MLLM-based autonomous driving by incorporating both unstructured and structured knowledge. First, we introduce a Position-Dependent Cross-Entropy (PDCE) loss to improve low-level control signal predictions when values are represented as text. Second, to explicitly integrate safety knowledge, we develop a reasoning component that translates traffic rules into first-order logic (e.g., \"red light => stop\") and embeds them into a probabilistic graphical model (e.g., Markov Logic Network) to verify predicted actions using recognized environmental attributes. Additionally, our Multimodal Retrieval-Augmented Generation (RAG) model leverages video, control signals, and environmental attributes to learn from past driving experiences. Integrating PDCE, MLN, and Multimodal RAG, SafeAuto outperforms existing baselines across multiple datasets, enabling more accurate, reliable, and safer autonomous driving. The code is available at https://github.com/AI-secure/SafeAuto",
    "checked": true,
    "id": "d271eb81d2d717934b773797d2b6e44b4e848a7f",
    "semantic_title": "safeauto: knowledge-enhanced safe autonomous driving with multimodal foundation models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=qUTiOeM57J": {
    "title": "Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift",
    "volume": "poster",
    "abstract": "Subpopulation shift, characterized by a disparity in subpopulation distribution between the training and target datasets, can significantly degrade the performance of machine learning models. Current solutions to subpopulation shift involve modifying empirical risk minimization with re-weighting strategies to improve generalization. This strategy relies on assumptions about the number and nature of subpopulations and annotations on group membership, which are unavailable for many real-world datasets. Instead, we propose using an ensemble of diverse classifiers to adaptively capture risk associated with subpopulations. Given a feature extractor network, we replace its standard linear classification layer with a mixture of prototypical classifiers, where each member is trained to classify the data while focusing on different features and samples from other members. In empirical evaluation on nine real-world datasets, covering diverse domains and kinds of subpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often outperforms the prior state-of-the-art in worst-group accuracy. The code is available at https://github.com/minhto2802/dpe4subpop",
    "checked": true,
    "id": "03ba7286bc9053c3441e8ab03538447b06bc6ecc",
    "semantic_title": "diverse prototypical ensembles improve robustness to subpopulation shift",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=olzs3zVsE7": {
    "title": "Privacy-Shielded Image Compression: Defending Against Exploitation from Vision-Language Pretrained Models",
    "volume": "poster",
    "abstract": "The improved semantic understanding of vision-language pretrained (VLP) models has made it increasingly difficult to protect publicly posted images from being exploited by search engines and other similar tools. In this context, this paper seeks to protect users' privacy by implementing defenses at the image compression stage to prevent exploitation. Specifically, we propose a flexible coding method, termed Privacy-Shielded Image Compression (PSIC), that can produce bitstreams with multiple decoding options. By default, the bitstream is decoded to preserve satisfactory perceptual quality while preventing interpretation by VLP models. Our method also retains the original image compression functionality. With a customizable input condition, the proposed scheme can reconstruct the image that preserves its full semantic information. A Conditional Latent Trigger Generation (CLTG) module is proposed to produce bias information based on customizable conditions to guide the decoding process into different reconstructed versions, and an Uncertainty-Aware Encryption-Oriented (UAEO) optimization function is designed to leverage the soft labels inferred from the target VLP model's uncertainty on the training data. This paper further incorporates an adaptive multi-objective optimization strategy to obtain improved encrypting performance and perceptual quality simultaneously within a unified training process. The proposed scheme is plug-and-play and can be seamlessly integrated into most existing Learned Image Compression (LIC) models. Extensive experiments across multiple downstream tasks have demonstrated the effectiveness of our design",
    "checked": true,
    "id": "13ccb99157474e15cdddd48b508a290e4905054f",
    "semantic_title": "privacy-shielded image compression: defending against exploitation from vision-language pretrained models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mgJkeqc685": {
    "title": "HYGMA: Hypergraph Coordination Networks with Dynamic Grouping for Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "Cooperative multi-agent reinforcement learning faces significant challenges in effectively organizing agent relationships and facilitating information exchange, particularly when agents need to adapt their coordination patterns dynamically. This paper presents a novel framework that integrates dynamic spectral clustering with hypergraph neural networks to enable adaptive group formation and efficient information processing in multi-agent systems. The proposed framework dynamically constructs and updates hypergraph structures through spectral clustering on agents' state histories, enabling higher-order relationships to emerge naturally from agent interactions. The hypergraph structure is enhanced with attention mechanisms for selective information processing, providing an expressive and efficient way to model complex agent relationships. This architecture can be implemented in both value-based and policy-based paradigms through a unified objective combining task performance with structural regularization. Extensive experiments on challenging cooperative tasks demonstrate that our method significantly outperforms state-of-the-art approaches in both sample efficiency and final performance. The code is available at: https://github.com/mysteryelder/HYGMA",
    "checked": true,
    "id": "fe39b7cc88c5bc76c637f995f8f7a3caf3fd58db",
    "semantic_title": "hygma: hypergraph coordination networks with dynamic grouping for multi-agent reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dzGtPrqORu": {
    "title": "How Distributed Collaboration Influences the Diffusion Model Training? A Theoretical Perspective",
    "volume": "poster",
    "abstract": "This paper examines the theoretical performance of distributed diffusion models in environments where computational resources and data availability vary significantly among workers. Traditional models centered on single-worker scenarios fall short in such distributed settings, particularly when some workers are resource-constrained. This discrepancy in resources and data diversity challenges the assumption of accurate score function estimation foundational to single-worker models. We establish the inaugural generation error bound for distributed diffusion models in resource-limited settings, establishing a linear relationship with the data dimension $d$ and consistency with established single-worker results. Our analysis highlights the critical role of hyperparameter selection in influencing the training dynamics, which are key to the performance of model generation. This study provides a streamlined theoretical approach to optimizing distributed diffusion models, paving the way for future research in this area",
    "checked": false,
    "id": "3603300ba7c4db999b4c26ef018ad2d0e419ee7c",
    "semantic_title": "rethinking organisational communication through an ecological perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LCFPWXymVt": {
    "title": "A Near Linear Query Lower Bound for Submodular Maximization",
    "volume": "poster",
    "abstract": "We revisit the problem of selecting $k$-out-of-$n$ elements with the goal of optimizing an objective function, and ask whether it can be solved approximately with sublinear query complexity. For objective functions that are monotone submodular, [Li, Feldman, Kazemi, Karbasi, NeurIPS'22; Kuhnle, AISTATS'21] gave an $\\Omega(n/k)$ query lower bound for approximating to within any constant factor. We strengthen their lower bound to a nearly tight $\\tilde{\\Omega}(n)$. This lower bound holds even for estimating the value of the optimal subset. When the objective function is additive, we prove that finding an approximately optimal subset still requires near-linear query complexity, but we can estimate the value of the optimal subset in $\\tilde{O}(n/k)$ queries, and that this is tight up to polylog factors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M18dhHTFf8": {
    "title": "Local Pan-privacy for Federated Analytics",
    "volume": "poster",
    "abstract": "Pan-privacy was proposed by Dwork et al. (2010) as an approach to designing a private analytics system that retains its privacy properties in the face of intrusions that expose the system's internal state. Motivated by Federated telemetry applications, we study {\\em local pan-privacy}, where privacy should be retained under repeated unannounced intrusions {\\em on the local state}. We consider the problem of monitoring the count of an event in a federated system, where event occurrences on a local device should be hidden even from an intruder on that device. We show that under reasonable constraints, the goal of providing information-theoretic differential privacy under intrusion is incompatible with collecting telemetry information. We then show that this problem can be solved in a scalable way using standard cryptographic primitives",
    "checked": true,
    "id": "5bdc338f372a649f93a4075e52107c1c2911aec7",
    "semantic_title": "local pan-privacy for federated analytics",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l1sx5KiM7Z": {
    "title": "Curvature Enhanced Data Augmentation for Regression",
    "volume": "poster",
    "abstract": "Deep learning models with a large number of parameters, often referred to as over-parameterized models, have achieved exceptional performance across various tasks. Despite concerns about overfitting, these models frequently generalize well to unseen data, thanks to effective regularization techniques, with data augmentation being among the most widely used. While data augmentation has shown great success in classification tasks using label-preserving transformations, its application in regression problems has received less attention. Recently, a novel manifold learning approach for generating synthetic data was proposed, utilizing a first-order approximation of the data manifold. Building on this foundation, we present a theoretical framework and practical tools for approximating and sampling general data manifolds. Furthermore, we introduce the Curvature-Enhanced Manifold Sampling (CEMS) method for regression tasks. CEMS leverages a second-order representation of the data manifold to enable efficient sampling and reconstruction of new data points. Extensive evaluations across multiple datasets and comparisons with state-of-the-art methods demonstrate that CEMS delivers superior performance in both in-distribution and out-of-distribution scenarios, while introducing only minimal computational overhead. Code is available at https://github.com/azencot-group/CEMS",
    "checked": true,
    "id": "b356dbb38d030b64721531f4dab65b66c0e4f110",
    "semantic_title": "curvature enhanced data augmentation for regression",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bwidSkOyWF": {
    "title": "AdvAgent: Controllable Blackbox Red-teaming on Web Agents",
    "volume": "poster",
    "abstract": "Foundation model-based agents are increasingly used to automate complex tasks, enhancing efficiency and productivity. However, their access to sensitive resources and autonomous decision-making also introduce significant security risks, where successful attacks could lead to severe consequences. To systematically uncover these vulnerabilities, we propose AdvAgent, a black-box red-teaming framework for attacking web agents. Unlike existing approaches, AdvAgent employs a reinforcement learning-based pipeline to train an adversarial prompter model that optimizes adversarial prompts using feedback from the black-box agent. With careful attack design, these prompts effectively exploit agent weaknesses while maintaining stealthiness and controllability. Extensive evaluations demonstrate that AdvAgent achieves high success rates against state-of-the-art GPT-4-based web agents across diverse web tasks. Furthermore, we find that existing prompt-based defenses provide only limited protection, leaving agents vulnerable to our framework. These findings highlight critical vulnerabilities in current web agents and emphasize the urgent need for stronger defense mechanisms. We release code at https://ai-secure.github.io/AdvAgent/",
    "checked": true,
    "id": "9ee3c8c68d4aa28d179fef38b29643be6954ea41",
    "semantic_title": "advagent: controllable blackbox red-teaming on web agents",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=asgBo3FNdg": {
    "title": "The Surprising Effectiveness of Test-Time Training for Few-Shot Learning",
    "volume": "poster",
    "abstract": "Language models (LMs) have shown impressive performance on tasks within their training distribution, but often struggle with structurally novel tasks even when given a small number of in-context task examples. We investigate the effectiveness of test-time training (TTT)—temporarily updating model parameters during inference using a loss derived from input data—as a mechanism for improving LMs' reasoning and few-shot learning capabilities. On the Abstraction and Reasoning Corpus (ARC), performing TTT with in-context examples yields up to $6\\times$ higher accuracy compared to fine-tuned baselines—reaching $53.0\\%$ on the public validation set with an 8B-parameter LM and $61.9\\%$ when ensembled with program-synthesis methods, matching average human performance. On BIG-Bench Hard (BBH), TTT on in-context examples surpasses standard few-shot prompting in the $10$-shot setting by $7.3$ percentage points ($50.5\\%$ to $57.8\\%$). Our findings highlight the limitations of in-context learning for novel tasks and demonstrate the potential of test-time training to enhance language model adaptability",
    "checked": true,
    "id": "90febba94f09c45ba94ed153b894afaf2a0ab50a",
    "semantic_title": "the surprising effectiveness of test-time training for few-shot learning",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=id2CfAgEAk": {
    "title": "On the Local Complexity of Linear Regions in Deep ReLU Networks",
    "volume": "poster",
    "abstract": "We define the *local complexity* of a neural network with continuous piecewise linear activations as a measure of the density of linear regions over an input data distribution. We show theoretically that ReLU networks that learn low-dimensional feature representations have a lower local complexity. This allows us to connect recent empirical observations on feature learning at the level of the weight matrices with concrete properties of the learned functions. In particular, we show that the local complexity serves as an upper bound on the total variation of the function over the input data distribution and thus that feature learning can be related to adversarial robustness. Lastly, we consider how optimization drives ReLU networks towards solutions with lower local complexity. Overall, this work contributes a theoretical framework towards relating geometric properties of ReLU networks to different aspects of learning such as feature learning and representation cost",
    "checked": true,
    "id": "18edf06b3df19a66cf6edbc884f27a46c40f891e",
    "semantic_title": "on the local complexity of linear regions in deep relu networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=8PJmKfeDdp": {
    "title": "TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable Inference",
    "volume": "poster",
    "abstract": "Large language models (LLMs) have proven to be very capable, but access to frontier models currently relies on inference providers. This introduces trust challenges: how can we be sure that the provider is using the model configuration they claim? We propose TOPLOC, a novel method for verifiable inference that addresses this problem. TOPLOC leverages a compact locality-sensitive hashing mechanism for intermediate activations, which can detect unauthorized modifications to models, prompts, or precision with 100\\% accuracy, achieving no false positives or negatives in our empirical evaluations. Our approach is robust across diverse hardware configurations, GPU types, and algebraic reorderings, which allows for validation speeds significantly faster than the original inference. By introducing a polynomial encoding scheme, TOPLOC minimizes the memory overhead of the generated proofs by $1000\\times$, requiring only 258 bytes of storage per 32 new tokens, compared to the 262 KB requirement of storing the token embeddings directly for Llama 3.1-8B-Instruct. Our method empowers users to verify LLM inference computations efficiently, fostering greater trust and transparency in open ecosystems and laying a foundation for decentralized, verifiable and trustless AI services",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HdogAuhlD5": {
    "title": "Exploring Invariance in Images through One-way Wave Equations",
    "volume": "poster",
    "abstract": "In this paper, we empirically demonstrate that natural images can be reconstructed with high fidelity from compressed representations using a simple first-order norm-plus-linear autoregressive (FINOLA) process—without relying on explicit positional information. Through systematic analysis, we observe that the learned coefficient matrices ($\\mathbf{A}$ and $\\mathbf{B}$) in FINOLA are typically invertible, and their product, $\\mathbf{AB}^{-1}$, is diagonalizable across training runs. This structure enables a striking interpretation: FINOLA's latent dynamics resemble a system of one-way wave equations evolving in a compressed latent space. Under this framework, each image corresponds to a unique solution of these equations. This offers a new perspective on image invariance, suggesting that the underlying structure of images may be governed by simple, invariant dynamic laws. Our findings shed light on a novel avenue for understanding and modeling visual data through the lens of latent-space dynamics and wave propagation",
    "checked": true,
    "id": "49b0c368bb5061c80d8216d682ed19407bc1f5c5",
    "semantic_title": "exploring invariance in images through one-way wave equations",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=uPVynwZxch": {
    "title": "Unifying Knowledge from Diverse Datasets to Enhance Spatial-Temporal Modeling: A Granularity-Adaptive Geographical Embedding Approach",
    "volume": "poster",
    "abstract": "Spatio-temporal forecasting provides potential for discovering evolutionary patterns in geographical scientific data. However, geographical scientific datasets are often manually collected across studies, resulting in limited time spans and data scales. This hinders existing methods that rely on rich historical data for individual entities. In this paper, we argue that heterogeneous datasets from different studies can provide complementary insights into the same underlying system, helping improve predictions for geographical entities with limited historical data. To this end, we propose a Segment Quadtree Geographical Embedding Framework (SQGEF). SQGEF integrates knowledge from datasets with varied target entities, time spans, and observation variables to learn unified representations for multi-granularity entities—including those absent during training. Specifically, we propose a novel data structure, Segment Quadtree, that flexibly accommodates entities of varying granularities. SQGEF not only captures multi-level interactions from grid data but also extracts nested relationships and human-defined boundaries from diverse entities, enabling a comprehensive understanding of complex geographical structures. Experiments on real-world datasets demonstrate that SQGEF effectively represents unseen geographical entities and enhances performance for various models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m2EfTrbv4o": {
    "title": "DiLQR: Differentiable Iterative Linear Quadratic Regulator via Implicit Differentiation",
    "volume": "poster",
    "abstract": "While differentiable control has emerged as a powerful paradigm combining model-free flexibility with model-based efficiency, the iterative Linear Quadratic Regulator (iLQR) remains underexplored as a differentiable component. The scalability of differentiating through extended iterations and horizons poses significant challenges, hindering iLQR from being an effective differentiable controller. This paper introduces DiLQR, a framework that facilitates differentiation through iLQR, allowing it to serve as a trainable and differentiable module, either as or within a neural network. A novel aspect of this framework is the analytical solution that it provides for the gradient of an iLQR controller through implicit differentiation, which ensures a constant backward cost regardless of iteration, while producing an accurate gradient. We evaluate our framework on imitation tasks on famous control benchmarks. Our analytical method demonstrates superior computational performance, achieving up to $\\textbf{128x}$ speedup and a minimum of $\\textbf{21x}$ speedup compared to automatic differentiation. Our method also demonstrates superior learning performance ($\\mathbf{10^6x}$) compared to traditional neural network policies and better model loss with differentiable controllers that lack exact analytical gradients. Furthermore, we integrate our module into a larger network with visual inputs to demonstrate the capacity of our method for high-dimensional, fully end-to-end tasks. Codes can be found on the project homepage~\\url{https://sites.google.com/view/dilqr/}",
    "checked": true,
    "id": "6a884a61d8bc52f7ed786abd675cbc9757afa091",
    "semantic_title": "dilqr: differentiable iterative linear quadratic regulator via implicit differentiation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tjPxZiqeHB": {
    "title": "The Disparate Benefits of Deep Ensembles",
    "volume": "poster",
    "abstract": "Ensembles of Deep Neural Networks, Deep Ensembles, are widely used as a simple way to boost predictive performance. However, their impact on algorithmic fairness is not well understood yet. Algorithmic fairness examines how a model's performance varies across socially relevant groups defined by protected attributes such as age, gender, or race. In this work, we explore the interplay between the performance gains from Deep Ensembles and fairness. Our analysis reveals that they unevenly favor different groups, a phenomenon that we term the disparate benefits effect. We empirically investigate this effect using popular facial analysis and medical imaging datasets with protected group attributes and find that it affects multiple established group fairness metrics, including statistical parity and equal opportunity. Furthermore, we identify that the per-group differences in predictive diversity of ensemble members can explain this effect. Finally, we demonstrate that the classical Hardt post-processing method is particularly effective at mitigating the disparate benefits effect of Deep Ensembles by leveraging their better-calibrated predictive distributions",
    "checked": true,
    "id": "62aa722c22e0e8da91019715b0f8bd7486593c58",
    "semantic_title": "the disparate benefits of deep ensembles",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=LL8R2QUEvB": {
    "title": "Power Mean Estimation in Stochastic Continuous Monte-Carlo Tree Search",
    "volume": "poster",
    "abstract": "Monte-Carlo Tree Search (MCTS) has demonstrated success in online planning for deterministic environments, yet significant challenges remain in adapting it to stochastic Markov Decision Processes (MDPs), particularly in continuous state-action spaces. Existing methods, such as HOOT, which combines MCTS with the Hierarchical Optimistic Optimization (HOO) bandit strategy, address continuous spaces but rely on a logarithmic exploration bonus that lacks theoretical guarantees in non-stationary, stochastic settings. Recent advancements, such as Poly-HOOT, introduced a polynomial bonus term to achieve convergence in deterministic MDPs, though a similar theory for stochastic MDPs remains undeveloped. In this paper, we propose a novel MCTS algorithm, Stochastic-Power-HOOT, designed for continuous, stochastic MDPs. Stochastic-Power-HOOT integrates a power mean as a value backup operator, alongside a polynomial exploration bonus to address the non-stationarity inherent in continuous action spaces. Our theoretical analysis establishes that Stochastic-Power-HOOT converges at a polynomial rate of $\\mathcal{O}(n^{-1/2})$, where \\( n \\) is the number of visited trajectories, thereby extending the non-asymptotic convergence guarantees of Poly-HOOT to stochastic environments. Experimental results on synthetic and stochastic tasks validate our theoretical findings, demonstrating the effectiveness of Stochastic-Power-HOOT in continuous, stochastic domains",
    "checked": false,
    "id": "8ae78bbe8745c6cdc136de4f4b40902c79316f16",
    "semantic_title": "power mean estimation in stochastic monte-carlo tree_search",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DbUmeNnNpt": {
    "title": "Provable In-Context Vector Arithmetic via Retrieving Task Concepts",
    "volume": "poster",
    "abstract": "In-context learning (ICL) has garnered significant attention for its ability to grasp functions/tasks from demonstrations. Recent studies suggest the presence of a latent **task/function vector** in LLMs during ICL. Merullo et al. (2024) showed that LLMs leverage this vector alongside the residual stream for Word2Vec-like vector arithmetic, solving factual-recall ICL tasks. Additionally, recent work empirically highlighted the key role of Question-Answer data in enhancing factual-recall capabilities. Despite these insights, a theoretical explanation remains elusive. To move one step forward, we propose a theoretical framework building on empirically grounded *hierarchical* concept modeling. We develop an optimization theory, showing how nonlinear residual transformers trained via gradient descent on cross-entropy loss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss convergence and show the strong generalization, including robustness to concept recombination and distribution shifts. These results elucidate the advantages of transformers over static embedding predecessors. Empirical simulations corroborate our theoretical insights",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gGY9TNVYs3": {
    "title": "Contextual Bandits for Unbounded Context Distributions",
    "volume": "poster",
    "abstract": "Nonparametric contextual bandit is an important model of sequential decision making problems. Under $\\alpha$-Tsybakov margin condition, existing research has established a regret bound of $\\tilde{O}\\left(T^{1-\\frac{\\alpha+1}{d+2}}\\right)$ for bounded supports. However, the optimal regret with unbounded contexts has not been analyzed. The challenge of solving contextual bandit problems with unbounded support is to achieve both exploration-exploitation tradeoff and bias-variance tradeoff simultaneously. In this paper, we solve the nonparametric contextual bandit problem with unbounded contexts. We propose two nearest neighbor methods combined with UCB exploration. The first method uses a fixed $k$. Our analysis shows that this method achieves minimax optimal regret under a weak margin condition and relatively light-tailed context distributions. The second method uses adaptive $k$. By a proper data-driven selection of $k$, this method achieves an expected regret of $\\tilde{O}\\left(T^{1-\\frac{(\\alpha+1)\\beta}{\\alpha+(d+2)\\beta}}+T^{1-\\beta}\\right)$, in which $\\beta$ is a parameter describing the tail strength. This bound matches the minimax lower bound up to logarithm factors, indicating that the second method is approximately optimal",
    "checked": true,
    "id": "8d92ef85c21a852bc0e88067250024437d236d3e",
    "semantic_title": "contextual bandits for unbounded context distributions",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=gye2zYytx6": {
    "title": "PiD: Generalized AI-Generated Images Detection with Pixelwise Decomposition Residuals",
    "volume": "poster",
    "abstract": "Fake images, created by recently advanced generative models, have become increasingly indistinguishable from real ones, making their detection crucial, urgent, and challenging. This paper introduces PiD (Pixelwise Decomposition Residuals), a novel detection method that focuses on residual signals within images. Generative models are designed to optimize high-level semantic content (principal components), often overlooking low-level signals (residual components). PiD leverages this observation by disentangling residual components from images, encouraging the model to uncover more underlying and general forgery clues independent of semantic content. Compared to prior approaches that rely on reconstruction techniques or high-frequency information, PiD is computationally efficient and does not rely on any generative models for reconstruction. Specifically, PiD operates at the pixel level, mapping the pixel vector to another color space (e.g., YUV) and then quantizing the vector. The pixel vector is mapped back to the RGB space and the quantization loss is taken as the residual for AIGC detection. Our experiment results are striking and highly surprising: PiD achieves 98% accuracy on the widely used GenImage benchmark, highlighting the effectiveness and generalization performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5of0l7eUau": {
    "title": "CAT: Contrastive Adversarial Training for Evaluating the Robustness of Protective Perturbations in Latent Diffusion Models",
    "volume": "poster",
    "abstract": "Latent diffusion models have recently demonstrated superior capabilities in many downstream image synthesis tasks. However, customization of latent diffusion models using unauthorized data can severely compromise the privacy and intellectual property rights of data owners. Adversarial examples as protective perturbations have been developed to defend against unauthorized data usage by introducing imperceptible noise to customization samples, preventing diffusion models from effectively learning them. In this paper, we first reveal that the primary reason adversarial examples are effective as protective perturbations in latent diffusion models is the distortion of their latent representations, as demonstrated through qualitative and quantitative experiments. We then propose the Contrastive Adversarial Training (CAT) utilizing lightweight adapters as an adaptive attack against these protection methods, highlighting their lack of robustness. Extensive experiments demonstrate that our CAT method significantly reduces the effectiveness of protective perturbations in customization, urging the community to reconsider and improve the robustness of existing protective perturbations. The code is available at \\url{https://github.com/senp98/CAT}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m25ma7O7Ec": {
    "title": "Online Robust Reinforcement Learning Through Monte-Carlo Planning",
    "volume": "poster",
    "abstract": "Monte Carlo Tree Search (MCTS) is a powerful framework for solving complex decision-making problems, yet it often relies on the assumption that the simulator and the real-world dynamics are identical. Although this assumption helps achieve the success of MCTS in games like Chess, Go, and Shogi, the real-world scenarios incur ambiguity due to their modeling mismatches in low-fidelity simulators. In this work, we present a new robust variant of MCTS that mitigates dynamical model ambiguities. Our algorithm addresses transition dynamics and reward distribution ambiguities to bridge the gap between simulation-based planning and real-world deployment. We incorporate a robust power mean backup operator and carefully designed exploration bonuses to ensure finite-sample convergence at every node in the search tree. We show that our algorithm achieves a convergence rate of $\\mathcal{O}(n^{-1/2})$ for the value estimation at the root node, comparable to that of standard MCTS. Finally, we provide empirical evidence that our method achieves robust performance in planning problems even under significant ambiguity in the underlying reward distribution and transition dynamics",
    "checked": false,
    "id": "ca7ec6803fc5f2ff976b0c448962da36869fc1ee",
    "semantic_title": "speeding up path planning via reinforcement learning in mcts for automated parking",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=sqjQ6p56GR": {
    "title": "Learning Condensed Graph via Differentiable Atom Mapping for Reaction Yield Prediction",
    "volume": "poster",
    "abstract": "Yield of chemical reactions generally depends on the activation barrier, i.e., the energy difference between the reactant and the transition state. Computing the transition state from the reactant and product graphs requires prior knowledge of the correct node alignment (i.e., atom mapping), which is not available in yield prediction datasets. In this work, we propose YieldNet, a neural yield prediction model, which tackles these challenges. Here, we first approximate the atom mapping between the reactants and products using a differentiable node alignment network. We then use this approximate atom mapping to obtain a noisy realization of the condensed graph of reaction (CGR), which is a supergraph encompassing both the reactants and products. This CGR serves as a surrogate for the transition state graph structure. The CGR embeddings of different steps in a multi-step reaction are then passed into a transformer-guided reaction path encoder. Our experiments show that YieldNet can predict the yield more accurately than the baselines. Furthermore, the model is trained only under the distant supervision of yield values, without requiring fine-grained supervision of atom mapping",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kPqvx2mvec": {
    "title": "What Limits Bidirectional Model's Generative Capabilities? A Uni-Bi-Directional Mixture-of-Expert Method For Bidirectional Fine-tuning",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) excel in generation tasks, yet their causal attention mechanisms limit performance in embedding tasks. While bidirectional modeling may enhance embeddings, naively fine-tuning unidirectional models bidirectionally severely degrades generative performance. To investigate this trade-off, we analyze attention weights as dependence indicators and find that bidirectional fine-tuning increases subsequent dependence, impairing unidirectional generation. Through systematic Transformer module evaluations, we discover the FFN layer is least affected by such dependence. Leveraging this discovery, we propose UBMoE-LLM, a novel Uni-Bi-directional Mixture-of-Experts LLM, which integrates the original unidirectional FFN with a bidirectionally fine-tuned FFN via unsupervised contrastive learning. This MoE-based approach enhances embedding performance while preserving robust generation. Extensive experiments across diverse datasets and model scales validate our attention dependence metric and demonstrate UBMoE-LLM's superior generative quality and reduced hallucination. Code is available at: https://github.com/heiyonghua/ubmoe_llm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GK6q2SFNHm": {
    "title": "The Noisy Laplacian: a Threshold Phenomenon for Non-Linear Dimension Reduction",
    "volume": "poster",
    "abstract": "In this paper, we clarify the effect of noise on common spectrally motivated algorithms such as Diffusion Maps (DM) for dimension reduction. Empirically, these methods are much more robust to noise than current work suggests. Specifically, existing consistency results require that either the noise amplitude or dimensionality must vary with the sample size $n$. We provide new theoretical results demonstrating that low-frequency eigenpairs reliably capture the geometry of the underlying manifold under a constant noise level, up to a dimension independent threshold $O(r^{-2})$, where $r$ is the noise amplitude. Our results rely on a decomposition of the manifold Laplacian in the Sasaki metric, a technique not used before in this area, to our knowledge. We experimentally validate our theoretical predictions. Additionally, we observe similar robust behavior for other manifold learning algorithms which are not based on computing the Laplacian, namely LTSA and VAE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ehcWKZ2nEn": {
    "title": "GTR: A General, Multi-View, and Dynamic Framework for Trajectory Representation Learning",
    "volume": "poster",
    "abstract": "Trajectory representation learning aims to transform raw trajectory data into compact and low-dimensional vectors that are suitable for downstream analysis. However, most existing methods adopt either a free-space view or a road-network view during the learning process, which limits their ability to capture the complex, multi-view spatiotemporal features inherent in trajectory data. Moreover, these approaches rely on task-specific model training, restricting their generalizability and effectiveness for diverse analysis tasks. To this end, we propose GTR, a general, multi-view, and dynamic Trajectory Representation framework built on a pre-train and fine-tune architecture. Specifically, GTR introduces a multi-view encoder that captures the intrinsic multi-view spatiotemporal features. Based on the pre-train and fine-tune architecture, we provide the spatio-temporal fusion pre-training with a spatio-temporal mixture of experts to dynamically combine spatial and temporal features, enabling seamless adaptation to diverse trajectory analysis tasks. Furthermore, we propose an online frozen-hot updating strategy to efficiently update the representation model, accommodating the dynamic nature of trajectory data. Extensive experiments on two real-world datasets demonstrate that GTR consistently outperforms 15 state-of-the-art methods across 6 mainstream trajectory analysis tasks. All source code and data are available at https://github.com/ZJU-DAILY/GTR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qOLjAhxZgm": {
    "title": "µnit Scaling: Simple and Scalable FP8 LLM Training",
    "volume": "poster",
    "abstract": "Large language model training with 8-bit floating point (FP8) formats promises significant efficiency improvements, but reduced numerical precision makes training challenging. It is currently possible to train in FP8 only if one is willing to tune various hyperparameters, reduce model scale, or accept the overhead of computing dynamic scale factors. We demonstrate simple, scalable FP8 training that requires no dynamic scaling factors or special hyperparameters, even at large model sizes. Our method, \\textit{µnit Scaling (µS)}, also enables simple hyperparameter transfer across model widths, matched numerics across training and inference, and other desirable properties. µnit Scaling is straightforward to implement, consisting of a set of minimal interventions based on a first-principles analysis of transformer operations. We validate our method by training models with parameters ranging from 1B to 13B, performing all hidden linear layer computations in FP8. We achieve quality equal to higher-precision baselines while also training up to 33% faster",
    "checked": false,
    "id": "0a0c87df6ce243b7dcc3a692b8c495f6c08e0b6b",
    "semantic_title": "μnit scaling: simple and scalable fp8 llm training",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=e3NNvqD7wA": {
    "title": "Contextual Optimization Under Model Misspecification: A Tractable and Generalizable Approach",
    "volume": "poster",
    "abstract": "Contextual optimization problems are prevalent in decision-making applications where historical data and contextual features are used to learn predictive models that inform optimal actions. However, practical applications often suffer from model misspecification due to incomplete knowledge of the underlying data-generating process, leading to suboptimal decisions. Existing approaches primarily address the well-specified case, leaving a critical gap in handling misspecified models. In this paper, we propose a novel Integrated Learning and Optimization (ILO) framework that explicitly accounts for model misspecification by introducing a tractable surrogate loss function with strong theoretical guarantees on generalizability, tractability, and optimality. Our surrogate loss aligns with the true decision performance objective, ensuring robustness to misspecification without imposing restrictive assumptions. The proposed approach effectively mitigates the challenges of non-convexity and non-smoothness in the target loss function, leading to efficient optimization procedures. We provide rigorous theoretical analysis and experimental validation, demonstrating superior performance compared to state-of-the-art methods. Our work offers a principled solution to the practically relevant challenge of model misspecification in contextual optimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SxJUV9mnyt": {
    "title": "Linear Transformers as VAR Models: Aligning Autoregressive Attention Mechanisms with Autoregressive Forecasting",
    "volume": "poster",
    "abstract": "Autoregressive attention-based time series forecasting (TSF) has drawn increasing interest, with mechanisms like linear attention often outperforming vanilla attention. However, deeper Transformer architectures frequently misalign with autoregressive objectives, obscuring the underlying VAR structure embedded within linear attention and hindering their ability to capture the data generative processes in TSF. In this work, we first show that a single linear attention layer can be interpreted as a dynamic vector autoregressive (VAR) structure. We then explain that existing multi-layer Transformers have structural mismatches with the autoregressive forecasting objective, which impair interpretability and generalization ability. To address this, we show that by rearranging the MLP, attention, and input-output flow, multi-layer linear attention can also be aligned as a VAR model. Then, we propose Structural Aligned Mixture of VAR (SAMoVAR), a linear Transformer variant that integrates interpretable dynamic VAR weights for multivariate TSF. By aligning the Transformer architecture with autoregressive objectives, SAMoVAR delivers improved performance, interpretability, and computational efficiency, comparing to SOTA TSF models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3B6fF1PxYD": {
    "title": "NextCoder: Robust Adaptation of Code LMs to Diverse Code Edits",
    "volume": "poster",
    "abstract": "Software engineering activities frequently involve edits to existing code. However, contemporary code language models (LMs) lack the ability to handle diverse types of code-edit requirements. In this work, we attempt to overcome this shortcoming through (1) a novel synthetic data generation pipeline and (2) a robust model adaptation algorithm. Starting with seed code examples and diverse editing criteria, our pipeline generates high-quality samples comprising original and modified code, along with natural language instructions in different styles and verbosity. Today's code LMs come bundled with strong abilities, such as code generation and instruction following, which should not be lost due to fine-tuning. To ensure this, we propose a novel adaptation algorithm, SeleKT, that (a) leverages a dense gradient-based step to identify the weights that are most important for code editing, and (b) does a sparse projection onto the base model to avoid overfitting. Using our approach, we obtain a new series of models NextCoder (adapted from QwenCoder-2.5) that achieves strong results on five code-editing benchmarks, outperforming comparable size models and even several larger ones. We show the generality of our approach on two model families DeepSeekCoder and QwenCoder), compare against other fine-tuning approaches, and demonstrate robustness by showing retention of code generation and general problem-solving abilities post adaptation. We opensource the models, synthetic dataset, and implementation at http://aka.ms/nextcoder",
    "checked": false,
    "id": "0476d2f5f992a9062f8346bbdf0a59e351c2aa9e",
    "semantic_title": "nextcoder : robust adaptation of code lms to diverse code edits",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FYvrNKYu6H": {
    "title": "O-MAPL: Offline Multi-agent Preference Learning",
    "volume": "poster",
    "abstract": "Inferring reward functions from demonstrations is a key challenge in reinforcement learning (RL), particularly in multi-agent RL (MARL). The large joint state-action spaces and intricate inter-agent interactions in MARL make inferring the joint reward function especially challenging. While prior studies in single-agent settings have explored ways to recover reward functions and expert policies from human preference feedback, such studies in MARL remain limited. Existing methods typically combine two separate stages, supervised reward learning, and standard MARL algorithms, leading to unstable training processes. In this work, we exploit the inherent connection between reward functions and Q functions in cooperative MARL to introduce a novel end-to-end preference-based learning framework. Our framework is supported by a carefully designed multi-agent value decomposition strategy that enhances training efficiency. Extensive experiments on two state-of-the-art benchmarks, SMAC and MAMuJoCo, using preference data generated by both rule-based and large language model approaches demonstrate that our algorithm consistently outperforms existing methods across various tasks",
    "checked": true,
    "id": "e3ab8e807a03e7895fa7016204c299bbe94dee83",
    "semantic_title": "o-mapl: offline multi-agent preference learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=h30EzoI3s0": {
    "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models",
    "volume": "poster",
    "abstract": "Supervised fine-tuning is a standard method for adapting pre-trained large language models (LLMs) to downstream tasks. Quantization has been recently studied as a post-training technique for efficient LLM deployment. To obtain quantized fine-tuned LLMs, conventional pipelines would first fine-tune the pre-trained models, followed by post-training quantization. This often yields suboptimal performance as it fails to leverage the synergy between fine-tuning and quantization. To effectively realize low-bit quantization of weights, activations and KV caches in LLMs, we propose an algorithm named Rotated Straight-Through-Estimator (RoSTE), which combines quantization-aware supervised fine-tuning (QA-SFT) with an adaptive rotation strategy that identifies an effective rotation configuration to reduce activation outliers. We provide theoretical insights on RoSTE by analyzing its prediction error when applied to an overparameterized least square quantized training problem. Our findings reveal that the prediction error is directly proportional to the quantization error of the converged weights, which can be effectively managed through an optimized rotation configuration. Experiments on Pythia, Qwen and Llama models of different sizes demonstrate the effectiveness of RoSTE. Compared to existing post-SFT quantization baselines, our method consistently achieves superior performances across various tasks and different LLM architectures. Our code is available at https://github.com/OptimAI-Lab/RoSTE",
    "checked": true,
    "id": "e91d4c1ac468acb67aa1288f8734128647d20bed",
    "semantic_title": "roste: an efficient quantization-aware supervised fine-tuning approach for large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=eff38SdyvN": {
    "title": "Reflection-Bench: Evaluating Epistemic Agency in Large Language Models",
    "volume": "poster",
    "abstract": "With large language models (LLMs) increasingly deployed as cognitive engines for AI agents, the reliability and effectiveness critically hinge on their intrinsic epistemic agency, which remains understudied. Epistemic agency, the ability to flexibly construct, adapt, and monitor beliefs about dynamic environments, represents a base-model-level capacity independent of specific tools, modules, or applications. We characterize the holistic process underlying epistemic agency, which unfolds in seven interrelated dimensions: prediction, decision-making, perception, memory, counterfactual thinking, belief updating, and meta-reflection. Correspondingly, we propose Reflection-Bench, a cognitive-psychology-inspired benchmark consisting of seven tasks with long-term relevance and minimization of data leakage. Through a comprehensive evaluation of 16 models using three prompting strategies, we identify a clear three-tier performance hierarchy and significant limitations of current LLMs, particularly in meta-reflection capabilities. While state-of-the-art LLMs demonstrate rudimentary signs of epistemic agency, our findings suggest several promising research directions, including enhancing core cognitive functions, improving cross-functional coordination, and developing adaptive processing mechanisms. Our code and data are available at https://github.com/AI45Lab/ReflectionBench",
    "checked": true,
    "id": "6875e36ecb05f73d9e5a98729af0b927bb4f94d6",
    "semantic_title": "reflection-bench: evaluating epistemic agency in large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=EV0itGFjmm": {
    "title": "COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning",
    "volume": "poster",
    "abstract": "Sparse tensor programs are essential in deep learning and graph analytics, driving the need for optimized processing. To meet this demand, specialized hardware accelerators are being developed. Optimizing these programs for accelerators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffective for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general-purpose hardware (e.g., CPUs) to train cost models, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5% of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE outperforms existing techniques, achieving average speedups of 1.47× (up to 5.46×) for SpMM and 1.39× (up to 4.22×) for SDDMM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TC1sQg5z0T": {
    "title": "Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism",
    "volume": "poster",
    "abstract": "Interactive Imitation Learning (IIL) allows agents to acquire desired behaviors through human interventions, but current methods impose high cognitive demands on human supervisors. We propose the Adaptive Intervention Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive criterion for requesting human demonstrations. AIM utilizes a proxy Q-function to mimic the human intervention rule and adjusts intervention requests based on the alignment between agent and human actions. By assigning high Q-values when the agent deviates from the expert and decreasing these values as the agent becomes proficient, the proxy Q-function enables the agent to assess the real-time alignment with the expert and request assistance when needed. Our expert-in-the-loop experiments reveal that AIM significantly reduces expert monitoring efforts in both continuous and discrete control tasks. Compared to the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40% improvement in terms of human take-over cost and learning efficiency. Furthermore, AIM effectively identifies safety-critical states for expert assistance, thereby collecting higher-quality expert demonstrations and reducing overall expert data and environment interactions needed. Code and demo video are available at https://github.com/metadriverse/AIM",
    "checked": true,
    "id": "d42f55af51fc631f3a0637af966b52c698693626",
    "semantic_title": "robot-gated interactive imitation learning with adaptive intervention mechanism",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kBTgizDiCq": {
    "title": "A Reasoning-Based Approach to Cryptic Crossword Clue Solving",
    "volume": "poster",
    "abstract": "Cryptic crossword clues are challenging language tasks for which new test sets are released daily by major newspapers on a global basis. Each cryptic clue contains both the definition of the answer to be placed in the crossword grid (in common with regular crosswords), and ‘wordplay' that *proves* that the answer is correct (i.e. a human solver can be confident that an answer is correct without needing crossing words as confirmation). This work describes an LLM-based reasoning system built from open-licensed components that solves cryptic clues by (i) hypothesising answers; (ii) proposing wordplay explanations; and (iii) using a verifier system that operates on codified reasoning steps. Overall, this system establishes a new state-of-the-art performance on the challenging Cryptonite dataset of clues from The Times and The Telegraph newspapers in the UK. Because each proved solution is expressed in Python, interpretable wordplay reasoning for proven answers is available for inspection",
    "checked": true,
    "id": "61c02f6d2a7bbcf3aee8581fc2001ffd13741a96",
    "semantic_title": "a reasoning-based approach to cryptic crossword clue solving",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fHt4Nau7FW": {
    "title": "Maximal Update Parametrization and Zero-Shot Hyperparameter Transfer for Fourier Neural Operators",
    "volume": "poster",
    "abstract": "Fourier Neural Operators (FNOs) offer a principled approach for solving complex partial differential equations (PDEs). However, scaling them to handle more complex PDEs requires increasing the number of Fourier modes, which significantly expands the number of model parameters and makes hyperparameter tuning computationally impractical. To address this, we introduce $\\mu$**Transfer-FNO**, a zero-shot hyperparameter transfer technique that enables optimal configurations, tuned on smaller FNOs, to be directly applied to billion-parameter FNOs _without_ additional tuning. Building on the Maximal Update Parametrization ($\\mu$P) framework, we mathematically derive a parametrization scheme that facilitates the transfer of optimal hyperparameters across models with different numbers of Fourier modes in FNOs, which is validated through extensive experiments on various PDEs. Our empirical study shows that $\\mu$Transfer-FNO reduces computational cost for tuning hyperparameters on large FNOs while maintaining or improving accuracy",
    "checked": true,
    "id": "a8ed18e73bdb42b27690729ce2fd3a79d04b1165",
    "semantic_title": "maximal update parametrization and zero-shot hyperparameter transfer for fourier neural operators",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pNZ3pioKRN": {
    "title": "Semantics-aware Test-time Adaptation for 3D Human Pose Estimation",
    "volume": "poster",
    "abstract": "This work highlights a semantics misalignment in 3D human pose estimation. For the task of test-time adaptation, the misalignment manifests as overly smoothed and unguided predictions. The smoothing settles predictions towards some average pose. Furthermore, when there are occlusions or truncations, the adaptation becomes fully unguided. To this end, we pioneer the integration of a semantics-aware motion prior for the test-time adaptation of 3D pose estimation. We leverage video understanding and a well-structured motion-text space to adapt the model motion prediction to adhere to video semantics during test time. Additionally, we incorporate a missing 2D pose completion based on the motion-text similarity. The pose completion strengthens the motion prior's guidance for occlusions and truncations. Our method significantly improves state-of-the-art 3D human pose estimation TTA techniques, with more than 12% decrease in PA-MPJPE on 3DPW and 3DHP",
    "checked": true,
    "id": "91f07d935dc725fdb2afb053b2ce326aa755c33c",
    "semantic_title": "semantics-aware test-time adaptation for 3d human pose estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JPkJAyutW0": {
    "title": "Revisiting Cooperative Off-Policy Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "Cooperative Multi-Agent Reinforcement Learning (MARL) has become a critical tool for addressing complex real-world problems. However, off-policy MARL methods, which rely on joint Q-functions, face significant scalability challenges due to the exponentially growing joint action space. In this work, we highlight a critical yet often overlooked issue: erroneous Q-target estimation, primarily caused by extrapolation error. Our analysis reveals that this error becomes increasingly severe as the number of agents grows, leading to unique challenges in MARL due to its expansive joint action space and the decentralized execution paradigm. To address these challenges, we propose a suite of techniques tailored for off-policy MARL, including annealed multi-step bootstrapping, averaged Q-targets, and restricted action representation. Experimental results demonstrate that these methods effectively mitigate erroneous estimations, yielding substantial performance improvements in challenging benchmarks such as SMAC, SMACv2, and Google Research Football",
    "checked": false,
    "id": "2716f9744913eca6153f1003457e082055d117a9",
    "semantic_title": "mean-field sampling for cooperative multi-agent reinforcement learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Kjivk5OPtL": {
    "title": "Improving LLM Safety Alignment with Dual-Objective Optimization",
    "volume": "poster",
    "abstract": "Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research also suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment. The code is available at https://github.com/wicai24/DOOR-Alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2nBcjCZrrP": {
    "title": "GuardAgent: Safeguard LLM Agents via Knowledge-Enabled Reasoning",
    "volume": "poster",
    "abstract": "The rapid advancement of large language model (LLM) agents has raised new concerns regarding their safety and security. In this paper, we propose GuardAgent, the first guardrail agent to protect target agents by dynamically checking whether their actions satisfy given safety guard requests. Specifically, GuardAgent first analyzes the safety guard requests to generate a task plan, and then maps this plan into guardrail code for execution. By performing the code execution, GuardAgent can deterministically follow the safety guard request and safeguard target agents. In both steps, an LLM is utilized as the reasoning component, supplemented by in-context demonstrations retrieved from a memory module storing experiences from previous tasks. In addition, we propose two novel benchmarks: EICU-AC benchmark to assess the access control for healthcare agents and Mind2Web-SC benchmark to evaluate the safety policies for web agents. We show that GuardAgent effectively moderates the violation actions for different types of agents on these two benchmarks with over 98% and 83% guardrail accuracies, respectively. Project page: https://guardagent.github.io/",
    "checked": false,
    "id": "2fd69fa8ebbcbdf23f3ca6dff00706df5719a156",
    "semantic_title": "guardagent: safeguard llm agents by a guard agent via knowledge-enabled reasoning",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=ERU7QgD6gc": {
    "title": "Can Diffusion Models Learn Hidden Inter-Feature Rules Behind Images?",
    "volume": "poster",
    "abstract": "Despite the remarkable success of diffusion models (DMs) in data generation, they exhibit specific failure cases with unsatisfactory outputs. We focus on one such limitation: the ability of DMs to learn hidden rules between image features. Specifically, for image data with dependent features ($\\mathbf{x}$) and ($\\mathbf{y}$) (e.g., the height of the sun ($\\mathbf{x}$) and the length of the shadow ($\\mathbf{y}$)), we investigate whether DMs can accurately capture the inter-feature rule ($p(\\mathbf{y}|\\mathbf{x})$). Empirical evaluations on mainstream DMs (e.g., Stable Diffusion 3.5) reveal consistent failures, such as inconsistent lighting-shadow relationships and mismatched object-mirror reflections. Inspired by these findings, we design four synthetic tasks with strongly correlated features to assess DMs' rule-learning abilities. Extensive experiments show that while DMs can identify coarse-grained rules, they struggle with fine-grained ones. Our theoretical analysis demonstrates that DMs trained via denoising score matching (DSM) exhibit constant errors in learning hidden rules, as the DSM objective is not compatible with rule conformity. To mitigate this, we introduce a common technique - incorporating additional classifier guidance during sampling, which achieves (limited) improvements. Our analysis reveals that the subtle signals of fine-grained rules are challenging for the classifier to capture, providing insights for future exploration",
    "checked": true,
    "id": "3c50bdb648bc77f813150d5009f81f610c046009",
    "semantic_title": "can diffusion models learn hidden inter-feature rules behind images?",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=kzYq2hfyHB": {
    "title": "Correlated Errors in Large Language Models",
    "volume": "poster",
    "abstract": "Diversity in training data, architecture, and providers is assumed to mitigate homogeneity in LLMs. However, we lack empirical evidence on whether different LLMs differ \\textit{meaningfully}. We conduct a large-scale empirical evaluation on over 350 LLMs overall, using two popular leaderboards and a resume-screening task. We find substantial correlation in model errors---on one leaderboard dataset, models agree 60\\% of the time when both models err. We identify factors driving model correlation, including shared architectures and providers. Crucially, however, larger and more accurate models have highly correlated errors, even with distinct architectures and providers. Finally, we show the effects of correlation in two downstream tasks: LLM-as-judge evaluation and hiring---the latter reflecting theoretical predictions regarding algorithmic monoculture",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qqn5ktBUxH": {
    "title": "WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting",
    "volume": "poster",
    "abstract": "We propose a Weighted Autoregressive Varying gatE (WAVE) attention mechanism equipped with both Autoregressive (AR) and Moving-average (MA) components. It can adapt to various attention mechanisms, enhancing and decoupling their ability to capture long-range and local temporal patterns in time series data. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that WAVE attention that incorporates the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results",
    "checked": true,
    "id": "920a936e3e081722a4aef33527e64cb86eb73d54",
    "semantic_title": "wave: weighted autoregressive varying gate for time series forecasting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dwjwvTwV3V": {
    "title": "Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts",
    "volume": "poster",
    "abstract": "Domain-Incremental Learning (DIL) focuses on continual learning in non-stationary environments, requiring models to adjust to evolving domains while preserving historical knowledge. DIL faces two critical challenges in the context of imbalanced data: intra-domain class imbalance and cross-domain class distribution shifts. These challenges significantly hinder model performance, as intra-domain imbalance leads to underfitting of few-shot classes, while cross-domain shifts require maintaining well-learned many-shot classes and transferring knowledge to improve few-shot class performance in old domains. To overcome these challenges, we introduce the Dual-Balance Collaborative Experts (DCE) framework. DCE employs a frequency-aware expert group, where each expert is guided by specialized loss functions to learn features for specific frequency groups, effectively addressing intra-domain class imbalance. Subsequently, a dynamic expert selector is learned by synthesizing pseudo-features through balanced Gaussian sampling from historical class statistics. This mechanism navigates the trade-off between preserving many-shot knowledge of previous domains and leveraging new data to improve few-shot class performance in earlier tasks. Extensive experimental results on four benchmark datasets demonstrate DCE's state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cBtsxtJqEK": {
    "title": "ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification",
    "volume": "poster",
    "abstract": "Self-awareness, i.e., the ability to assess and correct one's generation, is a fundamental aspect of human intelligence, making its replication in large language models (LLMs) an important yet challenging task. Previous works tackle this by employing extensive reinforcement learning or relying on large external verifiers. In this work, we propose Refine via Intrinsic Self-Verification (ReVISE), an efficient and effective framework that enables LLMs to self-correct their outputs through self-verification. The core idea of ReVISE is to enable LLMs to verify their reasoning processes and continually rethink reasoning trajectories based on its verification. To implement this efficiently, we introduce a structured curriculum based on preference learning. Specifically, as ReVISE involves two challenging tasks (i.e., self-verification and reasoning correction), we tackle each task sequentially using curriculum learning, collecting both failed and successful reasoning paths to construct preference pairs for efficient training. During inference, our approach enjoys natural test-time scaling by integrating self-verification and correction capabilities, further enhanced by our proposed confidence-aware decoding mechanism. Our experiments on various reasoning tasks demonstrate that ReVISE achieves efficient self-correction and significantly improves the reasoning performance of LLMs",
    "checked": true,
    "id": "a0ba3a62fc6761b1c0528b554d9ff335d26893e8",
    "semantic_title": "revise: learning to refine at test-time via intrinsic self-verification",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=rMCyR6VSOM": {
    "title": "Large Displacement Motion Transfer with Unsupervised Anytime Interpolation",
    "volume": "poster",
    "abstract": "Motion transfer is to transfer pose in driving video to object of source image, so that object of source image moves. Although great progress has been made recently in unsupervised motion transfer, many unsupervised methods still struggle to accurately model large displacement motions when large motion differences occur between source and driving images. To solve the problem, we propose an unsupervised anytime interpolation based large displacement motion transfer method, which can generate a series of anytime interpolated images between source and driving images. By decomposing large displacement motion into many small displacement motions, difficulty of large displacement motion estimation is reduced. In the process, we design a selector that can select optimal interpolated image from generated interpolated images for downstream tasks. Since there are no real images as labels in the interpolation process, we propose a bidirectional training strategy. Some constraints are added to optimal interpolated image to generate a reasonable interpolated image. To encourage network to generate high-quality images, a pre-trained Vision Transformer model is used to design constraint losses. Finally, experiments show that compared with the large displacement motion between source and driving images, small displacement motion between interpolated and driving images is easier to realize motion transfer. Compared with existing state-of-art methods, our method has significant improvements in motion-related metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A31Ep22iQ7": {
    "title": "One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs",
    "volume": "poster",
    "abstract": "Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of \"proof by counterexamples\" commonly used in human mathematics education, our work aims to enhance LLMs' ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, COUNTERMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that COUNTERMATH is challenging, indicating that LLMs, such as OpenAI o1, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs",
    "checked": true,
    "id": "15eee39bb4d976b31d34beac789e7077fc4794e7",
    "semantic_title": "one example shown, many concepts known! counterexample-driven conceptual reasoning in mathematical llms",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Hq2RniQAET": {
    "title": "LEAPS: A discrete neural sampler via locally equivariant networks",
    "volume": "poster",
    "abstract": "We propose *LEAPS*, an algorithm to sample from discrete distributions known up to normalization by learning a rate matrix of a continuous-time Markov chain (CTMC). LEAPS can be seen as a continuous-time formulation of annealed importance sampling and sequential Monte Carlo methods, extended so that the variance of the importance weights is offset by the inclusion of the CTMC. To derive these importance weights, we introduce a set of Radon-Nikodym derivatives of CTMCs over their path measures. Because the computation of these weights is intractable with standard neural network parameterizations of rate matrices, we devise a new compact representation for rate matrices via what we call \\textit{locally equivariant} functions. To parameterize them, we introduce a family of locally equivariant multilayer perceptrons, attention layers, and convolutional networks, and provide an approach to make deep networks that preserve the local equivariance. This property allows us to propose a scalable training algorithm for the rate matrix such that the variance of the importance weights associated to the CTMC are minimal. We demonstrate the efficacy of LEAPS on problems in statistical physics. We provide code in https://github.com/malbergo/leaps/",
    "checked": true,
    "id": "89745561b90e40661876cfcc4d7a438db9aca159",
    "semantic_title": "leaps: a discrete neural sampler via locally equivariant networks",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=JvRoF9FRga": {
    "title": "Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery",
    "volume": "poster",
    "abstract": "Attention mechanisms have emerged as transformative tools in core AI domains such as natural language processing and computer vision. Yet, their largely untapped potential for modeling intricate physical systems presents a compelling frontier. Learning such systems often entails discovering operators that map between functional spaces using limited instances of function pairs---a task commonly framed as a severely ill-posed inverse PDE problem. In this work, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator architecture that builds upon and enhances Nonlocal Attention Operators (NAO) in both predictive accuracy and computational efficiency. NIPS employs a linear attention mechanism to enable scalable learning and integrates a learnable kernel network that acts as a channel-independent convolution in Fourier space. As a consequence, NIPS eliminates the need to explicitly compute and store large pairwise interactions, effectively amortizing the cost of handling spatial interactions into the Fourier transform. Empirical evaluations demonstrate that NIPS consistently surpasses NAO and other baselines across diverse benchmarks, heralding a substantial leap in scalable, interpretable, and efficient physics learning. Our code and data accompanying this paper are available at https://github.com/fishmoon1234/Nonlocal-Attention-Operator",
    "checked": true,
    "id": "8e03c8265b4f38c7ea23e8120ee8d354a7badabf",
    "semantic_title": "neural interpretable pdes: harmonizing fourier insights with attention for scalable and interpretable physics discovery",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=VK47MdCjBH": {
    "title": "MuseControlLite: Multifunctional Music Generation with Lightweight Conditioners",
    "volume": "poster",
    "abstract": "We propose MuseControlLite, a lightweight mechanism designed to fine-tune text-to-music generation models for precise conditioning using various time-varying musical attributes and reference audio signals. The key finding is that positional embeddings, which have been seldom used by text-to-music generation models in the conditioner for text conditions, are critical when the condition of interest is a function of time. Using melody control as an example, our experiments show that simply adding rotary positional embeddings to the decoupled cross-attention layers increases control accuracy from 56.6% to 61.1%, while requiring 6.75 times fewer trainable parameters than state-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion Transformer model of Stable Audio Open. We evaluate various forms of musical attribute control, audio inpainting, and audio outpainting, demonstrating improved controllability over MusicGen-Large and Stable Audio Open ControlNet at a significantly lower fine-tuning cost, with only 85M trainable parameters. Source code, model checkpoints, and demo examples are available at: https://MuseControlLite.github.io/web/",
    "checked": true,
    "id": "037a9564910265d951d8633b5e40b5f9ae4d9e03",
    "semantic_title": "musecontrollite: multifunctional music generation with lightweight conditioners",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=n08niE37ku": {
    "title": "Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Auto Speculation",
    "volume": "poster",
    "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful tools for generative modeling. However, their sequential computation requirements lead to significant inference-time bottlenecks. In this work, we utilize the connection between DDPMs and Stochastic Localization to prove that, under an appropriate reparametrization, the increments of DDPM satisfy an exchangeability property. This general insight enables near-black-box adaptation of various performance optimization techniques from autoregressive models to the diffusion setting. To demonstrate this, we introduce _Autospeculative Decoding_ (ASD), an extension of the widely used speculative decoding algorithm to DDPMs that does not require any auxiliary draft models. Our theoretical analysis shows that ASD achieves a $\\tilde{O}(K^{\\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM. We also demonstrate that a practical implementation of autospeculative decoding accelerates DDPM inference significantly in various domains",
    "checked": false,
    "id": "d31afa9013658209ad3b2b6b1f360efa27aee4c6",
    "semantic_title": "diffusion models are secretly exchangeable: parallelizing ddpms via autospeculation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=M2WMUuwoh5": {
    "title": "Does Data Scaling Lead to Visual Compositional Generalization?",
    "volume": "poster",
    "abstract": "Compositional understanding is crucial for human intelligence, yet it remains unclear whether contemporary vision models exhibit it. The dominant machine learning paradigm is built on the premise that scaling data and model sizes will improve out-of-distribution performance, including compositional generalization. We test this premise through controlled experiments that systematically vary data scale, concept diversity, and combination coverage. We find that compositional generalization is driven by data diversity, not mere data scale. Increased combinatorial coverage forces models to discover a linearly factored representational structure, where concepts decompose into additive components. We prove this structure is key to efficiency, enabling perfect generalization from few observed combinations. Evaluating pretrained models (DINO, CLIP), we find above-random yet imperfect performance, suggesting partial presence of this structure. Our work motivates stronger emphasis on constructing diverse datasets for compositional generalization, and considering the importance of representational structure that enables efficient compositional learning",
    "checked": true,
    "id": "a62acff2ae06c50eb52fc2c610368ce95c3f2c62",
    "semantic_title": "does data scaling lead to visual compositional generalization?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yXcY4wKAG7": {
    "title": "NEAR: Neural Electromagnetic Array Response",
    "volume": "poster",
    "abstract": "We address the challenge of achieving angular super-resolution in multi-antenna radar systems that are widely used for localization, navigation, and automotive perception. A multi-antenna radar achieves very high resolution by computationally creating a large virtual sensing system using very few physical antennas. However, practical constraints imposed by hardware, noise, and a limited number of antennas can impede its performance. Conventional supervised learning models that rely on extensive pre-training with large datasets, often exhibit poor generalization in unseen environments. To overcome these limitations, we propose NEAR, an untrained implicit neural representation (INR) framework that predicts radar responses at unseen locations from sparse measurements, by leveraging latent harmonic structures inherent in radar wave propagation. We establish new theoretical results linking antenna array response to expressive power of INR architectures, and develop a novel physics-informed and latent geometry-aware regularizer. Our approach integrates classical signal representation with modern implicit neural learning, enabling high-resolution radar sensing that is both interpretable and generalizable. Extensive simulations and real-world experiments using radar platforms demonstrate NEAR's effectiveness and its ability to adapt to unseen environments",
    "checked": false,
    "id": "d8ff1f5d2a353ee0c0e28cde264421706d54cb19",
    "semantic_title": "deep learning for electromagnetic metamaterial inverse design",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ybBuwgOPOd": {
    "title": "Dynamical phases of short-term memory mechanisms in RNNs",
    "volume": "poster",
    "abstract": "Short-term memory is essential for cognitive processing, yet our understanding of its neural mechanisms remains unclear. Neuroscience has long focused on how sequential activity patterns, where neurons fire one after another within large networks, can explain how information is maintained. While recurrent connections were shown to drive sequential dynamics, a mechanistic understanding of this process still remains unknown. In this work, we introduce two unique mechanisms that can support this form of short-term memory: slow-point manifolds generating direct sequences or limit cycles providing temporally localized approximations. Using analytical models, we identify fundamental properties that govern the selection of each mechanism. Precisely, on short-term memory tasks (delayed cue-discrimination tasks), we derive theoretical scaling laws for critical learning rates as a function of the delay period length, beyond which no learning is possible. We empirically verify these results by training and evaluating approximately 80,000 recurrent neural networks (RNNs), which are publicly available for further analysis. Overall, our work provides new insights into short-term memory mechanisms and proposes experimentally testable predictions for systems neuroscience",
    "checked": true,
    "id": "5b68873fc4332d0346b82873d8d63881096ded63",
    "semantic_title": "dynamical phases of short-term memory mechanisms in rnns",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=5QMJZiHuGn": {
    "title": "Improved Approximations for Hard Graph Problems using Predictions",
    "volume": "poster",
    "abstract": "We design improved approximation algorithms for NP-hard graph problems by incorporating predictions (e.g., learned from past data). Our prediction model builds upon and extends the $\\varepsilon$-prediction framework by Cohen-Addad, d'Orsi, Gupta, Lee, and Panigrahi (NeurIPS 2024). We consider an edge-based version of this model, where each edge provides two bits of information, corresponding to predictions about whether each of its endpoints belong to an optimal solution. Even with weak predictions where each bit is only $\\varepsilon$-correlated with the true solution, this information allows us to break approximation barriers in the standard setting. We develop algorithms with improved approximation ratios for MaxCut, Vertex Cover, Set Cover, and Maximum Independent Set problems (among others). Across these problems, our algorithms share a unifying theme, where we separately satisfy constraints related to high degree vertices (using predictions) and low-degree vertices (without using predictions) and carefully combine the answers",
    "checked": true,
    "id": "ed5edf57efb14bef17f62ed986813c4c2ecebcbe",
    "semantic_title": "improved approximations for hard graph problems using predictions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I1NtlLvJal": {
    "title": "Why Has Predicting Downstream Capabilities of Frontier AI Models with Scale Remained Elusive?",
    "volume": "poster",
    "abstract": "Predictable behavior from scaling advanced AI systems is an extremely desirable property for engineers, companies, economists and governments alike, and while a well-established literature exists on how pretraining performance scales, predictable scaling behavior on downstream capabilities remains elusive. While many factors are certainly responsible, this paper shines a light on a significant factor that makes predicting scaling behavior on widely used multiple-choice question answering benchmarks challenging and illuminates a path towards making such downstream evaluations predictable with scale. Using five model families and twelve well-established multiple-choice benchmarks, we show that downstream performance is computed from negative log likelihoods via a sequence of transformations that progressively degrades the statistical relationship between performance and scale. We then reveal the mechanism causing this degradation: downstream metrics require comparing the correct choice against a small number of specific incorrect choices, meaning accurately predicting downstream capabilities requires predicting not just how probability mass concentrates on the correct choice with scale, but also how probability mass fluctuates on specific incorrect choices with scale. We empirically study how probability mass on the correct choice co-varies with probability mass on incorrect choices with increasing compute, suggesting that scaling laws for \\textit{incorrect} choices might be achievable. Our work also explains why pretraining scaling laws are commonly regarded as more predictable than downstream capabilities and contributes towards establishing scaling-predictable evaluations of frontier AI models",
    "checked": true,
    "id": "34c95589cdad119f5309b12a55c5cca8c778ff0c",
    "semantic_title": "why has predicting downstream capabilities of frontier ai models with scale remained elusive?",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=ATNEHkXFrW": {
    "title": "Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback",
    "volume": "poster",
    "abstract": "Learning from human feedback plays an important role in aligning generative models, such as large language models (LLM). However, the effectiveness of this approach can be influenced by adversaries, who may intentionally provide misleading preferences to manipulate the output in an undesirable or harmful direction. To tackle this challenge, we study a specific model within this problem domain--contextual dueling bandits with adversarial feedback, where the true preference label can be flipped by an adversary. We propose an algorithm namely robust contextual dueling bandits ($\\texttt{RCDB}$), which is based on uncertainty-weighted maximum likelihood estimation. Our algorithm achieves an $\\tilde O(d\\sqrt{T}/\\kappa+dC/\\kappa)$ regret bound, where $T$ is the number of rounds, $d$ is the dimension of the context, $\\kappa$ is the lower bound of the derivative of the link function, and $ 0 \\le C \\le T$ is the total number of adversarial feedback. We also prove a lower bound to show that our regret bound is nearly optimal, both in scenarios with and without ($C=0$) adversarial feedback. Our work is the first to achieve nearly minimax optimal regret for dueling bandits in the presence of adversarial preference feedback. Additionally, for the sigmoid link function, we develop a novel algorithm that takes into account the effect of local derivatives into maximum likelihood estimation (MLE) analysis through a refined method for estimating the link function's derivative. This method helps us to eliminate the $\\kappa$ dependence in the leading term with respect to $T$, which reduces the exponential dependence on the parameter radius $B$ to a polynomial dependence. We conduct experiments to evaluate our proposed algorithm $\\texttt{RCDB}$ against various types of adversarial feedback. Experimental results demonstrate its superiority over the state-of-the-art dueling bandit algorithms in the presence of adversarial feedback",
    "checked": true,
    "id": "2297b5174db69eac8c885c5680fe0c4cf8eedac7",
    "semantic_title": "nearly optimal algorithms for contextual dueling bandits from adversarial feedback",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=VzFXb6Au58": {
    "title": "Contradiction Retrieval via Contrastive Learning with Sparsity",
    "volume": "poster",
    "abstract": "Contradiction retrieval refers to identifying and extracting documents that explicitly disagree with or refute the content of a query, which is important to many downstream applications like fact checking and data cleaning. To retrieve contradiction argument to the query from large document corpora, existing methods such as similarity search and cross-encoder models exhibit different limitations. To address these challenges, we introduce a novel approach: SparseCL that leverages specially trained sentence embeddings designed to preserve subtle, contradictory nuances between sentences. Our method utilizes a combined metric of cosine similarity and a sparsity function to efficiently identify and retrieve documents that contradict a given query. This approach dramatically enhances the speed of contradiction detection by reducing the need for exhaustive document comparisons to simple vector calculations. We conduct contradiction retrieval experiments on Arguana, MSMARCO, and HotpotQA, where our method produces an average improvement of $11.0\\%$ across different models. We also validate our method on downstream tasks like natural language inference and cleaning corrupted corpora. This paper outlines a promising direction for non-similarity-based information retrieval which is currently underexplored",
    "checked": false,
    "id": "8381c580ad975ce6855790edbdb05f8ec37b3ba6",
    "semantic_title": "semantic retrieval augmented contrastive learning for sequential recommendation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=6KeALGcu2j": {
    "title": "Sounding that Object: Interactive Object-Aware Image to Audio Generation",
    "volume": "poster",
    "abstract": "Generating accurate sounds for complex audio-visual scenes is challenging, especially in the presence of multiple objects and sound sources. In this paper, we propose an interactive object-aware audio generation model that grounds sound generation in user-selected visual objects within images. Our method integrates object-centric learning into a conditional latent diffusion model, which learns to associate image regions with their corresponding sounds through multi-modal attention. At test time, our model employs image segmentation to allow users to interactively generate sounds at the object level. We theoretically validate that our attention mechanism functionally approximates test-time segmentation masks, ensuring the generated audio aligns with selected objects. Quantitative and qualitative evaluations show that our model outperforms baselines, achieving better alignment between objects and their associated sounds",
    "checked": true,
    "id": "8a2bcaf63b6ed01f4beacc4e64ad5971ea51ff32",
    "semantic_title": "sounding that object: interactive object-aware image to audio generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OzQLuoKMQZ": {
    "title": "Correlation Clustering Beyond the Pivot Algorithm",
    "volume": "poster",
    "abstract": "We study the classic correlation clustering problem. Given $n$ objects and a complete labeling of the object-pairs as either \"similar\" or \"dissimilar\", the goal is to partition the objects into arbitrarily many clusters while minimizing disagreements with the labels. A classic Pivot algorithm for this problem, due to [Ailon et al STOC'05], obtains a 3-approximation for this problem. Over the years, this algorithm has been successfully implemented in various settings. The downside of the Pivot algorithm is that the approximation analysis of 3 is tight for it. While better approximations have been achieved in some settings, these algorithms are often hard to implement in various settings. For example, [Behnezhad et al FOCS19] showed that the output of Pivot can be maintained in polylog time per update in a dynamic setting, a bound that was improved to constant by [Dalirrooyfard et al ICML'24]. But obtaining a better approximation remains open. In this paper, we present Modified Pivot, an algorithm that locally improves the output of Pivot. Our Modified Pivot algorithm can be implemented just as efficiently as Pivot in various settings. Our experiments show that the output of Modified Pivot on average makes less than 77\\% of the mistakes made by Pivot. More surprisingly, we prove theoretically that Modified Pivot has approximation ratio $3-\\epsilon_0$ for some absolute constant $\\epsilon_0 > 0$. This, e.g., leads to a better than 3 approximation in the dynamic setting in polylog time, improving the 3-approximation obtained by [Behnezhad et al FOCS'19] and [Dalirrooyfard et al ICML'24]",
    "checked": true,
    "id": "565e74bba4ed3c5d99d81949e51682e6fa1d1388",
    "semantic_title": "correlation clustering beyond the pivot algorithm",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=0dualJz9OI": {
    "title": "Test-time Correlation Alignment",
    "volume": "poster",
    "abstract": "Deep neural networks often degrade under distribution shifts. Although domain adaptation offers a solution, privacy constraints often prevent access to source data, making Test-Time Adaptation (TTA)—which adapts using only unlabeled test data—increasingly attractive. However, current TTA methods still face practical challenges: (1) a primary focus on instance-wise alignment, overlooking CORrelation ALignment (CORAL) due to missing source correlations; (2) complex backpropagation operations for model updating, resulting in overhead computation and (3) domain forgetting. To address these challenges, we provide a theoretical analysis to investigate the feasibility of **T**est-time **C**orrelation **A**lignment (**TCA**), demonstrating that correlation alignment between high-certainty instances and test instances can enhance test performances with a theoretical guarantee. Based on this, we propose two simple yet effective algorithms: LinearTCA and LinearTCA+. LinearTCA applies a simple linear transformation to achieve both instance and correlation alignment without additional model updates, while LinearTCA+ serves as a plug-and-play module that can easily boost existing TTA methods. Extensive experiments validate our theoretical insights and show that TCA methods significantly outperforms baselines across various tasks, benchmarks and backbones. Notably, LinearTCA achieves higher accuracy with only 4\\% GPU memory and 0.6\\% computation time compared to the best TTA baseline. It also outperforms existing methods on CLIP over 1.86\\%. Code: https://github.com/youlj109/TCA",
    "checked": true,
    "id": "9db230a9544bd67dac1c871beb95eaa24a37980b",
    "semantic_title": "test-time correlation alignment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JWtcAlXkMN": {
    "title": "BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in Multi-modal Model Editing",
    "volume": "poster",
    "abstract": "Large multi-modal models inevitably decay over time as facts update and previously learned information becomes outdated. Traditional approaches such as fine-tuning are often impractical for updating these models due to their size and complexity. Instead, direct knowledge editing within the models presents a more viable solution. Current model editing techniques, however, typically overlook the unique influence ranges of different facts, leading to compromised model performance in terms of both generality and locality. To address this issue, we introduce the concept of the generality-locality trade-off in multi-modal model editing. We develop a new model editing dataset named OKEDIT, specifically designed to effectively evaluate this trade-off. Building on this foundation, we propose \\textbf{BalancEdit}, a novel method for balanced model editing that dynamically achieves an optimal balance between generality and locality. BalancEdit utilizes a unique mechanism that generates both positive and negative samples for each fact to accurately determine its influence scope and incorporates these insights into the model's latent space using a discrete, localized codebook of edits, without modifying the underlying model weights. To our knowledge, this is the first approach explicitly addressing the generality-locality trade-off in multi-modal model editing. Our comprehensive results confirm the effectiveness of BalancEdit, demonstrating minimal trade-offs while maintaining robust editing capabilities. Our code and dataset are available at https://github.com/donglgcn/BalancEdit/tree/MMOKVQA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3BZyQqbytZ": {
    "title": "DyCodeEval: Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination",
    "volume": "poster",
    "abstract": "The rapid advancement of code large language models (Code LLMs) underscores the critical need for effective and transparent benchmarking methods. However, current benchmarking predominantly relies on publicly available, human-created datasets. The widespread use of these static benchmark datasets makes the evaluation process particularly susceptible to data contamination—an unavoidable consequence of the extensive data collection processes employed during LLM training. Existing methods for addressing data contamination typically face significant limitations, including reliance on substantial human effort and difficulty in managing class imbalances. To overcome these challenges, we propose DyCodeEval, a novel benchmarking suite specifically designed to evaluate Code LLMs under realistic contamination scenarios. Given an initial seed programming problem, DyCodeEval utilizes multiple agents to systematically extract and modify contextual information without changing the core logic, generating semantically equivalent variations. We introduce a dynamic data generation method and conduct extensive empirical studies on two seed datasets involving 18 Code LLMs. The results demonstrate that DyCodeEval effectively assesses the reasoning capabilities of Code LLMs under contamination conditions while producing diverse problem variants, thereby ensuring robust and consistent benchmarking outcomes",
    "checked": false,
    "id": "ce2cda821b4c6a28c9477aac03ca918f7319b9cb",
    "semantic_title": "dynamic benchmarking of reasoning capabilities in code large language models under data contamination",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Svk7jjhlSu": {
    "title": "ResKoopNet: Learning Koopman Representations for Complex Dynamics with Spectral Residuals",
    "volume": "poster",
    "abstract": "Analyzing the long-term behavior of high-dimensional nonlinear dynamical systems remains a significant challenge. While the Koopman operator framework provides a powerful global linearization tool, current methods for approximating its spectral components often face theoretical limitations and depend on predefined dictionaries. Residual Dynamic Mode Decomposition (ResDMD) advanced the field by introducing the \\emph{spectral residual} to assess Koopman operator approximation accuracy; however, its approach of only filtering precomputed spectra prevents the discovery of the operator's complete spectral information, a limitation known as the `spectral inclusion' problem. We introduce ResKoopNet (Residual-based Koopman-learning Network), a novel method that directly addresses this by explicitly minimizing the \\emph{spectral residual} to compute Koopman eigenpairs. This enables the identification of a more precise and complete Koopman operator spectrum. Using neural networks, our approach provides theoretical guarantees while maintaining computational adaptability. Experiments on a variety of physical and biological systems show that ResKoopNet achieves more accurate spectral approximations than existing methods, particularly for high-dimensional systems and those with continuous spectra, which demonstrates its effectiveness as a tool for analyzing complex dynamical systems",
    "checked": true,
    "id": "aad007e14353e9409418e07f99675b3ef32a75e3",
    "semantic_title": "reskoopnet: learning koopman representations for complex dynamics with spectral residuals",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4aXfSLfM0Z": {
    "title": "Compositional Flows for 3D Molecule and Synthesis Pathway Co-design",
    "volume": "poster",
    "abstract": "Many generative applications, such as synthesis-based 3D molecular design, involve constructing compositional objects with continuous features. Here, we introduce Compositional Generative Flows (CGFlow), a novel framework that extends flow matching to generate objects in compositional steps while modeling continuous states. Our key insight is that modeling compositional state transitions can be formulated as a straightforward extension of the flow matching interpolation process. We further build upon the theoretical foundations of generative flow networks (GFlowNets), enabling reward-guided sampling of compositional structures. We apply CGFlow to synthesizable drug design by jointly designing the molecule's synthetic pathway with its 3D binding pose. Our approach achieves state-of-the-art binding affinity and synthesizability on all 15 targets from the LIT-PCBA benchmark, and 4.2x improvement in sampling efficiency compared to 2D synthesis-based baseline. To our best knowledge, our method is also the first to achieve state of-art-performance in both Vina Dock (-9.42) and AiZynth success rate (36.1\\%) on the CrossDocked2020 benchmark",
    "checked": true,
    "id": "7535f6505302f55ca8e3eccfbdff6f4e299d72be",
    "semantic_title": "compositional flows for 3d molecule and synthesis pathway co-design",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Asr955jcuZ": {
    "title": "Aligning Protein Conformation Ensemble Generation with Physical Feedback",
    "volume": "poster",
    "abstract": "Protein dynamics play a crucial role in protein biological functions and properties, and their traditional study typically relies on time-consuming molecular dynamics (MD) simulations conducted in silico. Recent advances in generative modeling, particularly denoising diffusion models, have enabled efficient accurate protein structure prediction and conformation sampling by learning distributions over crystallographic structures. However, effectively integrating physical supervision into these data-driven approaches remains challenging, as standard energy-based objectives often lead to intractable optimization. In this paper, we introduce Energy-based Alignment (EBA), a method that aligns generative models with feedback from physical models, efficiently calibrating them to appropriately balance conformational states based on their energy differences. Experimental results on the MD ensemble benchmark demonstrate that EBA achieves state-of-the-art performance in generating high-quality protein ensembles. By improving the physical plausibility of generated structures, our approach enhances model predictions and holds promise for applications in structural biology and drug discovery",
    "checked": true,
    "id": "07c117eb9b00fb9432d1784811dbd7a97a404e90",
    "semantic_title": "aligning protein conformation ensemble generation with physical feedback",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=lNVHg9npif": {
    "title": "Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models",
    "volume": "poster",
    "abstract": "Generalist robots that can perform a range of different tasks in open-world settings must be able to not only reason about the steps needed to accomplish their goals, but also process complex instructions, prompts, and even feedback during task execution. Intricate instructions (e.g., \"Could you make me a vegetarian sandwich?\" or \"I don't like that one\") require not just the ability to physically perform the individual steps, but the ability to situate complex commands and feedback in the physical world. In this work, we describe a system that uses vision-language models in a hierarchical structure, first reasoning over complex prompts and user feedback to deduce the most appropriate next step to fulfill the task, and then performing that step with low-level actions. In contrast to direct instruction following methods that can fulfill simple commands (\"pick up the cup\"), our system can reason through complex prompts and incorporate situated feedback during task execution (\"that's not trash\"). We evaluate our system across three robotic platforms, including single-arm, dual-arm, and dual-arm mobile robots, demonstrating its ability to handle tasks such as cleaning messy tables, making sandwiches, and grocery shopping. Videos are available at https://www.pi.website/research/hirobot",
    "checked": true,
    "id": "69afd10d387423a03617d461c3f47a5f3033cc54",
    "semantic_title": "hi robot: open-ended instruction following with hierarchical vision-language-action models",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=Y0hjl4L1ve": {
    "title": "Activation by Interval-wise Dropout: A Simple Way to Prevent Neural Networks from Plasticity Loss",
    "volume": "poster",
    "abstract": "Plasticity loss, a critical challenge in neural network training, limits a model's ability to adapt to new tasks or shifts in data distribution. While widely used techniques like L2 regularization and Layer Normalization have proven effective in mitigating this issue, Dropout remains notably ineffective. This paper introduces AID (Activation by Interval-wise Dropout), a novel method inspired by Dropout, designed to address plasticity loss. Unlike Dropout, AID generates subnetworks by applying Dropout with different probabilities on each preactivation interval. Theoretical analysis reveals that AID regularizes the network, promoting behavior analogous to that of deep linear networks, which do not suffer from plasticity loss. We validate the effectiveness of AID in maintaining plasticity across various benchmarks, including continual learning tasks on standard image classification datasets such as CIFAR10, CIFAR100, and TinyImageNet. Furthermore, we show that AID enhances reinforcement learning performance in the Arcade Learning Environment benchmark",
    "checked": true,
    "id": "2dc9299d699bb7702b61837e5b774abe7c9b41d7",
    "semantic_title": "activation by interval-wise dropout: a simple way to prevent neural networks from plasticity loss",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=MPlcU7Sxzs": {
    "title": "Pointwise Information Measures as Confidence Estimators in Deep Neural Networks: A Comparative Study",
    "volume": "poster",
    "abstract": "Estimating the confidence of deep neural network predictions is crucial for safe deployment in high-stakes applications. While softmax probabilities are commonly used, they are often poorly calibrated, and existing calibration methods have been shown to be detrimental to failure prediction. In this paper, we propose using information-theoretic measures to estimate prediction confidence in a post-hoc manner, without modifying network architecture or training. Specifically, we compare three pointwise information (PI) measures: pointwise mutual information (PMI), pointwise V-information (PVI), and the recently proposed pointwise sliced mutual information (PSI). These measures are theoretically grounded in their relevance to predictive uncertainty, with properties such as invariance, convergence rates, and sensitivity to geometric attributes like margin and intrinsic dimensionality. Through extensive experiments on benchmark computer vision models and datasets, we find that PVI consistently outperforms or matches existing baselines in post-hoc confidence estimation, excelling over PMI and PSI in both failure prediction and calibration. This aligns with our theoretical insights, which suggest that PVI offers the most balanced trade-offs. Finally, we show that replacing softmax outputs with PVI in existing confidence estimation methods can further enhance performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e02oLEbehE": {
    "title": "FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain",
    "volume": "poster",
    "abstract": "Supervised fine-tuning (SFT) is the most common way of adapting large language models (LLMs) to a new domain. In this paper, we improve the efficiency of SFT by selecting an informative subset of training examples. Specifically, for a fixed budget of training examples, which determines the computational cost of fine-tuning, we select those that maximize information gain, as measured by the Fisher information matrix of the SFT objective. We approximate it efficiently by linearization at the last layer of the LLM. Our approach is computationally efficient, analyzable, and performs well empirically. We demonstrate this on several problems, with both quantitative results and LLM-as-a-judge evaluations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d3PjjtGc07": {
    "title": "Ranking with Multiple Oracles: From Weak to Strong Stochastic Transitivity",
    "volume": "poster",
    "abstract": "We study the problem of efficiently aggregating the preferences of items from multiple information sources (oracles) and infer the ranking under both the weak stochastic transitivity (WST) and the strong stochastic transitivity (SST) conditions. When the underlying preference model satisfies the WST condition, we propose an algorithm named RMO-WST, which has a bi-level design: at the higher level, it actively allocates comparison budgets to all undetermined pairs until the full ranking is recovered; at the lower level, it attempts to compare the pair of items and selects the more accurate oracles simultaneously. We prove that the sample complexity of RMO-WST is $ \\tilde O( N\\sum_{i=2}^{N}H_{\\sigma^{-1}(i),{\\sigma^{-1}(i-1)}} )$, where $N$ is the number of items to rank, $H$ is a problem-dependent hardness factor, and $\\sigma^{-1}(i)$ represents the $i$-th best item. We also provide a tight lower bound that matches the upper bound of approximate ranking under the WST condition, answering a previously open problem. In addition, when the SST condition is satisfied, we propose an algorithm named RMO-SST, which can achieve an $\\tilde{O}(\\sum_{i=1}^{N} H_i \\log(N))$ sample complexity. This outperforms the best-known sample complexity by a factor of $\\log(N)$. The theoretical advantages of our algorithms are verified by empirical experiments in a simulated environment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9UExQpH078": {
    "title": "RZ-NAS: Enhancing LLM-guided Neural Architecture Search via Reflective Zero-Cost Strategy",
    "volume": "poster",
    "abstract": "LLM-to-NAS is a promising field at the intersection of Large Language Models (LLMs) and Neural Architecture Search (NAS), as recent research has explored the potential of architecture generation leveraging LLMs on multiple search spaces. However, the existing LLM-to-NAS methods face the challenges of limited search spaces, time-cost search efficiency, and uncompetitive performance across standard NAS benchmarks and multiple downstream tasks. In this work, we propose the Reflective Zero-cost NAS (RZ-NAS) method that can search NAS architectures with humanoid reflections and training-free metrics to elicit the power of LLMs. We rethink LLMs' roles in NAS in current work and design a structured, prompt-based to comprehensively understand the search tasks and architectures from both text and code levels. By integrating LLM reflection modules, we use LLM-generated feedback to provide linguistic guidance within architecture optimization. RZ-NAS enables effective search within both micro and macro search spaces without extensive time cost, achieving SOTA performance across multiple downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3TM3fxwTps": {
    "title": "In-Context Linear Regression Demystified: Training Dynamics and Mechanistic Interpretability of Multi-Head Softmax Attention",
    "volume": "poster",
    "abstract": "We study how multi-head softmax attention models are trained to perform in-context learning on linear data. Through extensive empirical experiments and rigorous theoretical analysis, we demystify the emergence of elegant attention patterns: a diagonal and homogeneous pattern in the key-query weights, and a last-entry-only and zero-sum pattern in the output-value weights. Remarkably, these patterns consistently appear from gradient-based training starting from random initialization. Our analysis reveals that such emergent structures enable multi-head attention to approximately implement a debiased gradient descent predictor --- one that outperforms single-head attention and nearly achieves Bayesian optimality up to proportional factor. We also extend our study to scenarios with anisotropic covariates and multi-task linear regression. Our results reveal that in-context learning ability emerges from the trained transformer as an aggregated effect of its architecture and the underlying data distribution, paving the way for deeper understanding and broader applications of in-context learning",
    "checked": true,
    "id": "180d89a6fe2ce8521935c6c00cca8c59a35345da",
    "semantic_title": "in-context linear regression demystified: training dynamics and mechanistic interpretability of multi-head softmax attention",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=NhJ4cCifqF": {
    "title": "Triple-Optimistic Learning for Stochastic Contextual Bandits with General Constraints",
    "volume": "poster",
    "abstract": "We study contextual bandits with general constraints, where a learner observes contexts and aims to maximize cumulative rewards while satisfying a wide range of general constraints. We introduce the Optimistic$^3$ framework, a novel learning and decision-making approach that integrates optimistic design into parameter learning, primal decision, and dual violation adaptation (i.e., triple-optimism), combined with an efficient primal-dual architecture. Optimistic$^3$ achieves $\\tilde{O}(\\sqrt{T})$ regret and constraint violation for contextual bandits with general constraints. This framework not only outperforms the state-of-the-art results that achieve $\\tilde{O}(T^{\\frac{3}{4}})$ guarantees when Slater's condition does not hold but also improves on previous results that achieve $\\tilde{O}(\\sqrt{T}/\\delta)$ when Slater's condition holds ($\\delta$ denotes the Slater's condition parameter), offering a $O(1/\\delta)$ improvement. Note this improvement is significant because $\\delta$ can be arbitrarily small when constraints are particularly challenging. Moreover, we show that Optimistic$^3$ can be extended to classical multi-armed bandits with both stochastic and adversarial constraints, recovering the best-of-both-worlds guarantee established in the state-of-the-art works, but with significantly less computational overhead",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TUk7gCqtmf": {
    "title": "Federated In-Context Learning: Iterative Refinement for Improved Answer Quality",
    "volume": "poster",
    "abstract": "For question-answering (QA) tasks, in-context learning (ICL) enables language models (LMs) to generate responses without modifying their parameters by leveraging examples provided in the input. However, the effectiveness of ICL heavily depends on the availability of high-quality examples, which are often scarce due to data privacy constraints, annotation costs, and distribution disparities. A natural solution is to utilize examples stored on client devices, but existing approaches either require transmitting model parameters—incurring significant communication overhead—or fail to fully exploit local datasets, limiting their effectiveness. To address these challenges, we propose Federated In-Context Learning (Fed-ICL), a general framework that enhances ICL through an iterative, collaborative process. Fed-ICL progressively refines responses by leveraging multi-round interactions between clients and a central server, improving answer quality without the need to transmit model parameters. We establish theoretical guarantees for the convergence of Fed-ICL and conduct extensive experiments on standard QA benchmarks, demonstrating that our proposed approach achieves strong performance while maintaining low communication costs",
    "checked": true,
    "id": "6d0e40228118db3aea1b25bc95ba3556fa9ff644",
    "semantic_title": "federated in-context learning: iterative refinement for improved answer quality",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jbafwTkVUn": {
    "title": "Fast, Accurate Manifold Denoising by Tunneling Riemannian Optimization",
    "volume": "poster",
    "abstract": "Learned denoisers play a fundamental role in various signal generation (e.g., diffusion models) and reconstruction (e.g., compressed sensing) architectures, whose success derives from their ability to leverage low-dimensional structure in data. Existing denoising methods, however, either rely on local approximations that require a linear scan of the entire dataset or treat denoising as generic function approximation problems, sacrificing efficiency and interpretability. We consider the problem of efficiently denoising a new noisy data point sampled from an unknown manifold $\\mathcal M \\in \\mathbb{R}^D$, using only noisy samples. This work proposes a framework for test-time efficient manifold denoising, by framing the concept of \"learning-to-denoise\" as *\"learning-to-optimize\"*. We have two technical innovations: (i) *online learning* methods which learn to optimize over the manifold of clean signals using only noisy data, effectively \"growing\" an optimizer one sample at a time. (ii) *mixed-order* methods which guarantee that the learned optimizers achieve global optimality, ensuring both efficiency and near-optimal denoising performance. We corroborate these claims with theoretical analyses of both the complexity and denoising performance of mixed-order traversal. Our experiments on scientific manifolds demonstrate significantly improved complexity-performance tradeoffs compared to nearest neighbor search, which underpins existing provable denoising approaches based on exhaustive search",
    "checked": true,
    "id": "526246bca15a661ad41eb228fd6115374688f609",
    "semantic_title": "fast, accurate manifold denoising by tunneling riemannian optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=xjTrTlBbrc": {
    "title": "Graph World Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W9s817KqYf": {
    "title": "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E8dMQGsKZv": {
    "title": "Understanding Mode Connectivity via Parameter Space Symmetry",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TiYOHdK35L": {
    "title": "Just Enough Shifts: Mitigating Over-Refusal in Aligned Language Models with Targeted Representation Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bma2FB5MNs": {
    "title": "Test-Time Training Provably Improves Transformers as In-context Learners",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k4KVhQd19x": {
    "title": "Dynamical Modeling of Behaviorally Relevant Spatiotemporal Patterns in Neural Imaging Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hcoxm3Vwgy": {
    "title": "ELoRA: Low-Rank Adaptation for Equivariant GNNs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CP2eFH3Ex0": {
    "title": "Tracking Most Significant Shifts in Infinite-Armed Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J3gzdbYZxS": {
    "title": "Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NBtgS3OJh4": {
    "title": "Floating-Point Neural Networks Can Represent Almost All Floating-Point Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4umRQdvuW5": {
    "title": "Geometry Informed Tokenization of Molecules for Language Model Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nY1Ge2wxtP": {
    "title": "Uncertainty Quantification for LLM-Based Survey Simulations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=asDx9sPAUN": {
    "title": "(How) Can Transformers Predict Pseudo-Random Numbers?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WDybFnCPaB": {
    "title": "Causal Abstraction Inference under Lossy Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x10ryC8F0C": {
    "title": "Safety-Polarized and Prioritized Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xl9sv9vEDy": {
    "title": "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wHaL4GkvOP": {
    "title": "On Exact Bit-level Reversible Transformers Without Changing Architecture",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yAdcCADXqH": {
    "title": "Reinforcement Learning Control of a Physical Robot Device for Assisted Human Walking without a Simulator",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t9RKslSC00": {
    "title": "Heterogeneous Sufficient Dimension Reduction and Subspace Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5g6LPR0Dlx": {
    "title": "Prune 'n Predict: Optimizing LLM Decision-making with Conformal Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=45he3Ri6JP": {
    "title": "HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NME3HKUHLX": {
    "title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=skoBTs4ke4": {
    "title": "Delay-DSGN: A Dynamic Spiking Graph Neural Network with Delay Mechanisms for Evolving Graph",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9sNiCqi2RD": {
    "title": "An End-to-End Model for Logits-Based Large Language Models Watermarking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XkEp70qckE": {
    "title": "Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6QH9IB53uy": {
    "title": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CxQPqcxRnx": {
    "title": "EditLord: Learning Code Transformation Rules for Code Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WSUO2uRDPc": {
    "title": "MVA: Linear Attention with High-order Query-Keys Integration and Multi-level Vocabulary Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H8DkMvWnSQ": {
    "title": "Conformal Tail Risk Control for Large Language Model Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EscpGI2XAx": {
    "title": "Online Clustering of Dueling Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UchFXOIwvA": {
    "title": "Distributed Retraction-Free and Communication-Efficient Optimization on the Stiefel Manifold",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I6UAeNdvFe": {
    "title": "Contrastive Learning with Simplicial Convolutional Networks for Short-Text Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DRvtabzN0n": {
    "title": "Zero-Inflated Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z83rodY0Pw": {
    "title": "SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AjbiIcRt6q": {
    "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oq0t5BXilT": {
    "title": "Accelerating Large Language Model Reasoning via Speculative Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L1Bm396P0X": {
    "title": "The Butterfly Effect: Neural Network Training Trajectories Are Highly Sensitive to Initial Conditions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=859NdHQv0Z": {
    "title": "OmiAD: One-Step Adaptive Masked Diffusion Model for Multi-class Anomaly Detection via Adversarial Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MOlihFnYNU": {
    "title": "MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cTsYaXSm9O": {
    "title": "The Limits of Tractable Marginalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pedm1880A2": {
    "title": "e-GAI: e-value-based Generalized α -Investing for Online False Discovery Rate Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QqGw9StPbQ": {
    "title": "NETS: A Non-equilibrium Transport Sampler",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hmGhP5DO2W": {
    "title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gHGrzxFujU": {
    "title": "Open Materials Generation with Stochastic Interpolants",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hk7CBybb6x": {
    "title": "Bipartite Ranking From Multiple Labels: On Loss Versus Label Aggregation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yeoN1iQT1x": {
    "title": "KernelBench: Can LLMs Write Efficient GPU Kernels?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NhkNX8jYld": {
    "title": "LLM-Augmented Chemical Synthesis and Design Decision Programs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HWN9CAfcav": {
    "title": "Efficient Bisection Projection to Ensure Neural-Network Solution Feasibility for Optimization over General Set",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LWH8yn4HS2": {
    "title": "From RAG to Memory: Non-Parametric Continual Learning for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zy15E0X3Dq": {
    "title": "Pretraining Generative Flow Networks with Inexpensive Rewards for Molecular Graph Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l7ZmdeFyM1": {
    "title": "Training High Performance Spiking Neural Network by Temporal Model Calibration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0E5rZOGA13": {
    "title": "Unbiased Recommender Learning from Implicit Feedback via Weakly Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3zWSmhNSa7": {
    "title": "Scalable First-order Method for Certifying Optimal k-Sparse GLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p9YlQPF8fE": {
    "title": "DataDecide: How to Predict Best Pretraining Data with Small Experiments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9oIjvaDhoN": {
    "title": "On the Query Complexity of Verifier-Assisted Language Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zpuKPuSYru": {
    "title": "Dimension-Free Adaptive Subgradient Methods with Frequent Directions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xg1BGlybfq": {
    "title": "SKOLR: Structured Koopman Operator Linear RNN for Time-Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Km3QvYPmK4": {
    "title": "Stochastic Deep Restoration Priors for Imaging Inverse Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3YTjTQhX8B": {
    "title": "Multi-Armed Bandits with Interference: Bridging Causal Inference and Adversarial Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0A4Y9qRnu9": {
    "title": "Leveraging Per-Instance Privacy for Machine Unlearning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EMHED4WTHT": {
    "title": "Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yTfQt7vK6M": {
    "title": "Representation Preserving Multiclass Agnostic to Realizable Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TGcXwWdQQj": {
    "title": "Optimal Fair Learning Robust to Adversarial Distribution Shift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dNnA8ahuTY": {
    "title": "Understanding Synthetic Context Extension via Retrieval Heads",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CAurIUGjkb": {
    "title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ihRwpPYoKM": {
    "title": "Safely Learning Optimal Auctions: A Testable Learning Framework for Mechanism Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sezgRffNiS": {
    "title": "TraceGrad: a Framework Learning Expressive SO(3)-equivariant Non-linear Representations for Electronic-Structure Hamiltonian Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2xjUkU7FDb": {
    "title": "SlimLLM: Accurate Structured Pruning for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gT8JSEFqaS": {
    "title": "Structure-Guided Large Language Models for Text-to-SQL Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5rVcKWyvnt": {
    "title": "EGPlace: An Efficient Macro Placement Method via Evolutionary Search with Greedy Repositioning Guided Mutation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4gUVnk2Hyo": {
    "title": "A Trichotomy for List Transductive Online Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9biCmI3Mnd": {
    "title": "Improving Memory Efficiency for Training KANs via Meta Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cufJbug7du": {
    "title": "Gradient Descent Converges Arbitrarily Fast for Logistic Regression via Large and Adaptive Stepsizes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gVnGcm8E3d": {
    "title": "How does Labeling Error Impact Contrastive Learning? A Perspective from Data Dimensionality Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xsyrolw1Q1": {
    "title": "Bayesian Neural Scaling Law Extrapolation with Prior-Data Fitted Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=biMO2gWsWS": {
    "title": "Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KwGc2JUIDK": {
    "title": "Online Conformal Prediction via Online Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SsLGTZKXf1": {
    "title": "When Bad Data Leads to Good Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=51tMpvPNSm": {
    "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oDPtv1RveE": {
    "title": "Rethinking Point Cloud Data Augmentation: Topologically Consistent Deformation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CgJEHynkJt": {
    "title": "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dfOqiHuklY": {
    "title": "Generalization Principles for Inference over Text-Attributed Graphs with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hcLeFe7idT": {
    "title": "Sample-Optimal Agnostic Boosting with Unlabeled Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zvyb3WAg03": {
    "title": "BiMark: Unbiased Multilayer Watermarking for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JiFfij5iv0": {
    "title": "MedRAX: Medical Reasoning Agent for Chest X-ray",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xHhURhYmgK": {
    "title": "Projection Optimization: A General Framework for Multi-Objective and Multi-Group RLHF",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HMequFD3Uz": {
    "title": "Fixed-Confidence Multiple Change Point Identification under Bandit Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QhCb3FAQi2": {
    "title": "Implicit Subgraph Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CAvnZQgrLu": {
    "title": "Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=khFk7sdv9o": {
    "title": "Online Detection of LLM-Generated Texts via Sequential Hypothesis Testing by Betting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BdQKHcrBxT": {
    "title": "Improved Last-Iterate Convergence of Shuffling Gradient Methods for Nonsmooth Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nSldadGGY5": {
    "title": "EFDTR: Learnable Elliptical Fourier Descriptor Transformer for Instance Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TKHWvyzR1t": {
    "title": "TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wl3eI4wiE5": {
    "title": "Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MM6ZWF7gl9": {
    "title": "Understanding the Statistical Accuracy-Communication Trade-off in Personalized Federated Learning with Minimax Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DE6dqmcmQ9": {
    "title": "Scaling Laws for Differentially Private Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mvbWw0w7pG": {
    "title": "Pareto-Optimal Fronts for Benchmarking Symbolic Regression Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FeZoO7mfG7": {
    "title": "Faster Global Minimum Cut with Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ApVH0G4l6P": {
    "title": "FEAT-KD: Learning Concise Representations for Single and Multi-Target Regression via TabNet Knowledge Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QwoGfQzuMa": {
    "title": "Action-Minimization Meets Generative Modeling: Efficient Transition Path Sampling with the Onsager-Machlup Functional",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aINERD9MzJ": {
    "title": "Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Eg1OrHmg2": {
    "title": "Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7zrS5hHlfY": {
    "title": "Confounder-Free Continual Learning via Recursive Feature Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HDbWrsgkB9": {
    "title": "Reliable and Efficient Amortized Model-based Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WZKcJZWG3P": {
    "title": "Hybrid Spiking Vision Transformer for Object Detection with Event Cameras",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7epYTVsWEI": {
    "title": "PDE-Controller: LLMs for Autoformalization and Reasoning of PDEs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=le8hVvWi6Q": {
    "title": "Making Hard Problems Easier with Custom Data Distributions and Loss Regularization: A Case Study in Modular Arithmetic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4OszSYdsgO": {
    "title": "Attention-Level Speculation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkegUc8d0c": {
    "title": "Prediction-Powered E-Values",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4ufjBV6S4I": {
    "title": "RAGGED: Towards Informed Design of Scalable and Stable RAG Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RLu1QIPiVr": {
    "title": "H-Tuning: Toward Low-Cost and Efficient ECG-based Cardiovascular Disease Detection with Pre-Trained Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kVtyv7bpnw": {
    "title": "How Do Transformers Learn Variable Binding in Symbolic Programs?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pb4om8rWRQ": {
    "title": "One Stone, Two Birds: Enhancing Adversarial Defense Through the Lens of Distributional Discrepancy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UaTrcei5Ba": {
    "title": "Multimodal Medical Code Tokenizer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=grlezgVg4s": {
    "title": "ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language Models for Reward Design in Robotics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rcqVuXU2Gm": {
    "title": "Tight and Fast Bounds for Multi-Label Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SDjZtxDo35": {
    "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7hEZd8Rtlh": {
    "title": "CSG-ODE: ControlSynth Graph ODE For Modeling Complex Evolution of Dynamic Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p6nhzZ9ilZ": {
    "title": "Unlocking the Power of Rehearsal in Continual Learning: A Theoretical Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OUzDIhgiqr": {
    "title": "Weak-to-Strong Generalization Even in Random Feature Networks, Provably",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WH3ZRH2jno": {
    "title": "Identification of Latent Confounders via Investigating the Tensor Ranks of the Nonlinear Observations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lbrqeIipJr": {
    "title": "Disentangling and Integrating Relational and Sensory Information in Transformer Architectures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6nbcwJVZNy": {
    "title": "Sample-specific Noise Injection for Diffusion-based Adversarial Purification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4OWGON33HE": {
    "title": "NestQuant: nested lattice quantization for matrix products and LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xfWYB81p5O": {
    "title": "Interpolating Neural Network-Tensor Decomposition (INN-TD): a scalable and interpretable approach for large-scale physics-based problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H78W6bTkuZ": {
    "title": "MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aYUjVw9zcO": {
    "title": "Calibrating Video Watch-time Predictions with Credible Prototype Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gUj2fxQcLZ": {
    "title": "Auditing Prompt Caching in Language Model APIs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ci3nWnys6T": {
    "title": "Tensor Product Neural Networks for Functional ANOVA Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=juARG7yu4P": {
    "title": "Instruction-Following Pruning for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xWu5qpDK6U": {
    "title": "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8bGEHOTvmq": {
    "title": "It's My Data Too: Private ML for Datasets with Multi-User Training Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2qTwKMDAsD": {
    "title": "Understanding the Kronecker Matrix-Vector Complexity of Linear Algebra",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jImlK83NmV": {
    "title": "Fair Clustering via Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VFI7HottBp": {
    "title": "Computing Voting Rules with Improvement Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=19kqoNoc2N": {
    "title": "Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GZ7gwOZ6Or": {
    "title": "Protriever: End-to-End Differentiable Protein Homology Search for Fitness Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z4I9cf7WY6": {
    "title": "Settling the Maximin Share Fairness for Scheduling among Groups of Machines",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YKyza9lrv4": {
    "title": "When can in-context learning generalize out of task distribution?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sw2pUzbTf1": {
    "title": "Bayesian Inference for Correlated Human Experts and Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=teJdFzLnKh": {
    "title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GzS6b5Xvvu": {
    "title": "Structured Preconditioners in Adaptive Optimization: A Unified Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FCm4laCLiH": {
    "title": "Synthesizing Privacy-Preserving Text Data via Finetuning *without* Finetuning Billion-Scale LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HnXElKZdEh": {
    "title": "CEGA: A Cost-Effective Approach for Graph-Based Model Extraction and Acquisition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rjZ2SWjwwB": {
    "title": "Observation Interference in Partially Observable Assistance Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GrF14Q0DNW": {
    "title": "Feasible Action Search for Bandit Linear Programs via Thompson Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WUGrleBcYP": {
    "title": "SafetyAnalyst: Interpretable, Transparent, and Steerable Safety Moderation for AI Behavior",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UHVk08XFkX": {
    "title": "Introducing 3D Representation for Dense Volume-to-Volume Translation via Score Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fm0nDMKBwC": {
    "title": "LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJkpCMeIxu": {
    "title": "Breaking the Barrier of Hard Samples: A Data-Centric Approach to Synthetic Data for Medical Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YV05KZt7v2": {
    "title": "Imitation Learning from a Single Temporally Misaligned Video",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E7c9Jf1KjV": {
    "title": "The Logical Implication Steering Method for Conditional Interventions on Transformer Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s1WJSRaJuy": {
    "title": "Variational Phylogenetic Inference with Products over Bipartitions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=emkdmORaj4": {
    "title": "Shifting Time: Time-series Forecasting with Khatri-Rao Neural Operators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EAAjvpE7sp": {
    "title": "Constrained Online Convex Optimization with Polyak Feasibility Steps",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dmN2fQ3woH": {
    "title": "Graph-Based Algorithms for Diverse Similarity Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zZXOXhxO6I": {
    "title": "Sketch to Adapt: Fine-Tunable Sketches for Efficient LLM Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NbjrGgxLPi": {
    "title": "RestoreGrad: Signal Restoration Using Conditional Denoising Diffusion Models with Jointly Learned Prior",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aWd7mL5U9Q": {
    "title": "Adversarial Reasoning at Jailbreaking Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nAv5ketrHq": {
    "title": "Hierarchical Equivariant Policy via Frame Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GGPM0z3dhU": {
    "title": "Does Generation Require Memorization? Creative Diffusion Models using Ambient Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SuYO70ZxZX": {
    "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Response in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=824TCt6CkE": {
    "title": "Test-Time Graph Neural Dataset Search With Generative Projection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i9npQatSev": {
    "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LmdZ0pSWtG": {
    "title": "AlphaPO: Reward Shape Matters for LLM Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sQ6lqdjGBX": {
    "title": "Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bKsZomnmqn": {
    "title": "Near-optimal Sketchy Natural Gradients for Physics-Informed Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WBN0Mz3VAC": {
    "title": "KinDEL: DNA-Encoded Library Dataset for Kinase Inhibitors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Rtj4mYH1C": {
    "title": "UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NXtoNstR96": {
    "title": "Online Laplacian-Based Representation Learning in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y8hMadAgrz": {
    "title": "Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sRJrMPu5Uu": {
    "title": "Memorization Sinks: Isolating Memorization during LLM Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2gpjvMEAMm": {
    "title": "Skip the Equations: Learning Behavior of Personalized Dynamical Systems Directly From Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ebf2IYBrZO": {
    "title": "Return Capping: Sample Efficient CVaR Policy Gradient Optimisation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=om0CcjvEQh": {
    "title": "LensLLM: Unveiling Fine-Tuning Dynamics for LLM Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NRVdvg7VMn": {
    "title": "A New Concentration Inequality for Sampling Without Replacement and Its Application for Transductive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RsyMfsqzeG": {
    "title": "Improving Diversity in Language Models: When Temperature Fails, Change the Loss",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6wglsDXIei": {
    "title": "PENCIL: Long Thoughts with Short Memory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t10fde8tQ7": {
    "title": "Universal Neural Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eZ5QyZV7zi": {
    "title": "Policy-Regret Minimization in Markov Games with Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OZy70UggXr": {
    "title": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j3JBfFnGYh": {
    "title": "DeepCrossAttention: Supercharging Transformer Residual Connections",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ax550Vokon": {
    "title": "Steerable Transformers for Volumetric Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FLPFPYJeVU": {
    "title": "Value-Based Deep RL Scales Predictably",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kdmjVF1iDO": {
    "title": "Hardware and Software Platform Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kqj2Cn3Sxr": {
    "title": "Putnam-AXIOM: A Functional & Static Benchmark for Measuring Higher Level Mathematical Reasoning in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6uPcJtMgWN": {
    "title": "Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZPQU4uGMBA": {
    "title": "Do Vision-Language Models Really Understand Visual Language?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f6SFHNfuMu": {
    "title": "What can large language models do for sustainable food?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z9GgK3CK39": {
    "title": "No Free Lunch from Random Feature Ensembles: Scaling Laws and Near-Optimality Conditions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n52yyvEwPa": {
    "title": "GSM- ∞ : How Do your LLMs Behave over Infinitely Increasing Reasoning Complexity and Context Length?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K4N9UvsuNB": {
    "title": "Improving Model Alignment Through Collective Intelligence of Open-Source Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pL87Z7YTJS": {
    "title": "Robust Reward Alignment via Hypothesis Space Batch Cutting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vsJsR3ieCx": {
    "title": "Convergence of Consistency Model with Multistep Sampling under General Data Assumptions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oAKe7MG9GM": {
    "title": "Wasserstein Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=shTZSlk0HQ": {
    "title": "Simplifying DINO via Coding Rate Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QbtBIE36Fd": {
    "title": "Boosting Protein Graph Representations through Static-Dynamic Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YjBrt82S3v": {
    "title": "Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lllg9YjAFX": {
    "title": "Joint Localization and Activation Editing for Low-Resource Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8nelB0aMry": {
    "title": "Multiple-policy Evaluation via Density Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y1SnRPDWx4": {
    "title": "Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QgSfbgzgbH": {
    "title": "Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h3EEUJaMLu": {
    "title": "CoastalBench: A Decade-Long High-Resolution Dataset to Emulate Complex Coastal Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rm2WHra1fB": {
    "title": "Improved Algorithm for Deep Active Learning under Imbalance via Optimal Separation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tFBIbCVXkG": {
    "title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nv6mOSqUVA": {
    "title": "DMM: Distributed Matrix Mechanism for Differentially-Private Federated Learning Based on Constant-Overhead Linear Secret Resharing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C7XmEByCFv": {
    "title": "Which Attention Heads Matter for In-Context Learning?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iBOMvaa2aN": {
    "title": "Byzantine-Resilient Federated Alternating Gradient Descent and Minimization for Partly-Decoupled Low Rank Matrix Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3BmllnhGpm": {
    "title": "Constrained Pareto Set Identification with Bandit Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SDC3D4vdpb": {
    "title": "A Theoretical Framework For Overfitting In Energy-based Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dg24PyeWsI": {
    "title": "Auto-reconfiguration for Latency Minimization in CPU-based DNN Serving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mE1M626qOo": {
    "title": "The Lock-in Hypothesis: Stagnation by Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MRmI68k3gd": {
    "title": "Wasserstein Flow Matching: Generative Modeling Over Families of Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XAFfOEFJG1": {
    "title": "Non-Asymptotic and Non-Lipschitzian Bounds on Optimal Values in Stochastic Optimization Under Heavy Tails",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1xsW6tvMb3": {
    "title": "Understanding Overadaptation in Supervised Fine-Tuning: The Role of Ensemble Methods",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0VvD1PmNzM": {
    "title": "TabICL: A Tabular Foundation Model for In-Context Learning on Large Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rgd7poMTDp": {
    "title": "SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=49g4c8MWHy": {
    "title": "Preference Controllable Reinforcement Learning with Advanced Multi-Objective Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R9iBZen9aY": {
    "title": "Zero-Shot Generalization of GNNs over Distinct Attribute Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mHySkOp46b": {
    "title": "Synthetic Text Generation for Training Large Language Models via Gradient Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x5RQnF7Vw9": {
    "title": "Do Bayesian Neural Networks Actually Behave Like Bayesian Models?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NvYwrQbzOb": {
    "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hntp7s2YfF": {
    "title": "What If We Recaption Billions of Web Images with LLaMA-3?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lzzPAQ1TxA": {
    "title": "A Bregman Proximal Viewpoint on Neural Operators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tf4lRAOGkj": {
    "title": "On the Robustness of Reward Models for Language Model Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fsf7LhbYdf": {
    "title": "From Low Rank Gradient Subspace Stabilization to Low-Rank Weights: Observations, Theories, and Applications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EIEpGfixF6": {
    "title": "QUTE: Quantifying Uncertainty in TinyML models with Early-exit-assisted ensembles for model-monitoring",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qWgAAVhoXb": {
    "title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=edN2rEemj6": {
    "title": "FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for Enabling Fair LLM-Based Recommender Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rA2P4H7Dep": {
    "title": "Adaptive Partitioning Schemes for Optimistic Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=osNUbJ4vlX": {
    "title": "Zero-Shot Adaptation of Parameter-Efficient Fine-Tuning in Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IQN6ID0snT": {
    "title": "Self-Supervised Transformers as Iterative Solution Improvers for Constraint Satisfaction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sSrOwve6vb": {
    "title": "MIB: A Mechanistic Interpretability Benchmark",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XEYoTQv00G": {
    "title": "Convergence Analysis of Policy Gradient Methods with Dynamic Stochasticity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jk8vYZLty7": {
    "title": "Enforcing Idempotency in Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z63Ot0pcM3": {
    "title": "Editable Noise Map Inversion: Encoding Target-image into Noise For High-Fidelity Image Manipulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9P9Y8FOSOk": {
    "title": "The Diffusion Duality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vZhdXRnfPU": {
    "title": "Active Learning of Deep Neural Networks via Gradient-Free Cutting Planes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AulTigiaMv": {
    "title": "Eliciting Language Model Behaviors with Investigator Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ywj1B3DuO": {
    "title": "Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bWeLpOgqGp": {
    "title": "A Machine Learning Approach to Duality in Statistical Physics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aPm6SfcMWQ": {
    "title": "Resolving Lexical Bias in Model Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I7N6vtUChM": {
    "title": "Adaptive Elicitation of Latent Information Using Natural Language",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rcivp36KzO": {
    "title": "Randomized Dimensionality Reduction for Euclidean Maximization and Diversity Measures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q0P4rrDImq": {
    "title": "DIS-CO: Discovering Copyrighted Content in VLMs Training Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vOxaD3hhPt": {
    "title": "MetaAgent: Automatically Constructing Multi-Agent Systems Based on Finite State Machines",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uqpML2nbIz": {
    "title": "RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cZi1njoRT6": {
    "title": "Leveraging Online Olympiad-Level Math Problems for LLMs Training and Contamination-Resistant Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wKs9fHYxCV": {
    "title": "CRANE: Reasoning with constrained LLM generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0m4VsLwj5s": {
    "title": "sciLaMA: A Single-Cell Representation Learning Framework to Leverage Prior Knowledge from Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jkyUbkNJyH": {
    "title": "Domain-Adapted Diffusion Model for PROTAC Linker Design Through the Lens of Density Ratio in Chemical Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CdqBQwFG9i": {
    "title": "Leveraging Predictive Equivalence in Decision Trees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=890gHX7ieS": {
    "title": "Flexibility-conditioned protein structure design with flow matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EHG5Iv1mmb": {
    "title": "Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dWuN4jCQo3": {
    "title": "Spatial Reasoning with Denoising Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7TDnfx5s14": {
    "title": "TruthFlow: Truthful LLM Generation via Representation Flow Correction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BdO4R6XxUH": {
    "title": "DCBM: Data-Efficient Visual Concept Bottleneck Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pRlKbAwczl": {
    "title": "SPEX: Scaling Feature Interaction Explanations for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9OaZCNbV2w": {
    "title": "S4S: Solving for a Fast Diffusion Model Solver",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jYmGi1175R": {
    "title": "Aggregation of Dependent Expert Distributions in Multimodal Variational Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u4LlYWJHUF": {
    "title": "TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yI24Wy5YaN": {
    "title": "Principal-Agent Bandit Games with Self-Interested and Exploratory Learning Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eCVrfVDNSY": {
    "title": "Language Models over Canonical Byte-Pair Encodings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1jx6bgemqg": {
    "title": "Provable Zero-Shot Generalization in Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bzbuZ0ItBq": {
    "title": "Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VCjPjexvpM": {
    "title": "Improved Learning via k-DTW: A Novel Dissimilarity Measure for Curves",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZLyb8DwXXE": {
    "title": "Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yaI2ZYFmeD": {
    "title": "EEG-Language Pretraining for Highly Label-Efficient Clinical Phenotyping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JXCiQteuOv": {
    "title": "DUNIA: Pixel-Sized Embeddings via Cross-Modal Alignment for Earth Observation Applications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dqcl5SNbc8": {
    "title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TqODUDsU4u": {
    "title": "Optimizing Test-Time Compute via Meta Reinforcement Finetuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oetxkccLoq": {
    "title": "Potemkin Understanding in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PGNff6H1TV": {
    "title": "EncryptedLLM: Privacy-Preserving Large Language Model Inference via GPU-Accelerated Fully Homomorphic Encryption",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fGbGF0kl5L": {
    "title": "Active Learning for Efficient Discovery of Optimal Combinatorial Perturbations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OrkMLgWoiG": {
    "title": "Heterogeneous Treatment Effect in Time-to-Event Outcomes: Harnessing Censored Data with Recursively Imputed Trees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G2mAZTpJcz": {
    "title": "On Explaining Equivariant Graph Networks via Improved Relevance Propagation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PNRznmmWP7": {
    "title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qQBzVFwtPN": {
    "title": "Adaptive Sample Sharing for Multi Agent Linear Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v2nQ1e78Rc": {
    "title": "Geometric Contact Flows: Contactomorphisms for Dynamics and Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kRKsUOJdp5": {
    "title": "Regression for the Mean: Auto-Evaluation and Inference with Few Labels through Post-hoc Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SUX6Wzy9t1": {
    "title": "Stochastic Poisson Surface Reconstruction with One Solve using Geometric Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W6RPXUUFic": {
    "title": "Communicating Activations Between Language Model Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iIPAdNq9cq": {
    "title": "Extracting Rare Dependence Patterns via Adaptive Sample Reweighting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5s7D7FPuTc": {
    "title": "Online Learning in Risk Sensitive constrained MDP",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q6zwUeWkZL": {
    "title": "Self-Play Q -Learners Can Provably Collude in the Iterated Prisoner's Dilemma",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vfqdJy12Kk": {
    "title": "Hierarchical Planning for Complex Tasks with Knowledge Graph-RAG and Symbolic Verification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xB9eROwBCB": {
    "title": "Diffusion on Language Model Encodings for Protein Sequence Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TaqwI9qF5Q": {
    "title": "Stacey: Promoting Stochastic Steepest Descent via Accelerated ℓ p -Smooth Nonconvex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aAkq9mMviY": {
    "title": "Verification Learning: Make Unsupervised Neuro-Symbolic System Feasible",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6k3oFS3Lbl": {
    "title": "Reinforce LLM Reasoning through Multi-Agent Reflection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JwZVPTTEwO": {
    "title": "QuEst: Enhancing Estimates of Quantile-Based Distributional Measures Using Model Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9UN0E5DO5M": {
    "title": "Generalization Analysis for Controllable Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t3zwUqibMq": {
    "title": "Simple Path Structural Encoding for Graph Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AEHtq4xHSU": {
    "title": "Towards characterizing the value of edge embeddings in Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ERu2ZiAnR7": {
    "title": "Designing Cyclic Peptides via Harmonic SDE with Atom-Bond Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4R0pugRyN5": {
    "title": "Sparse Autoencoders for Hypothesis Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ruSU7xtH6v": {
    "title": "Implicit Bias of Gradient Descent for Non-Homogeneous Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5zsBvPOIUQ": {
    "title": "To Each Metric Its Decoding: Post-Hoc Optimal Decision Rules of Probabilistic Hierarchical Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bcHMa96Dv6": {
    "title": "Polynomial-Time Approximability of Constrained Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZtX0MBT6mf": {
    "title": "Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QxZfMpsFn3": {
    "title": "Set Valued Predictions For Robust Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QfD9P9IIoz": {
    "title": "Hidden No More: Attacking and Defending Private Third-Party LLM Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OJ3dQNRnsx": {
    "title": "Compositional Causal Reasoning Evaluation in Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vutMcZl50l": {
    "title": "Mahalanobis++: Improving OOD Detection via Feature Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NsU6MKwbis": {
    "title": "Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S22CMkkQzY": {
    "title": "Selective Preference Aggregation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yTAR011mOF": {
    "title": "How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BuN4FX0iBr": {
    "title": "A General Representation-Based Approach to Multi-Source Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4BfaPHfhJ0": {
    "title": "Do We Need to Verify Step by Step? Rethinking Process Supervision from a Theoretical Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zFB0ujQ1R5": {
    "title": "Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lZ4UQ6SzlX": {
    "title": "Benefits of Early Stopping in Gradient Descent for Overparameterized Logistic Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kxFu9rQ0Mu": {
    "title": "Aligning Spoken Dialogue Models from User Interactions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2nQyYo71ih": {
    "title": "Since Faithfulness Fails: The Performance Limits of Neural Causal Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HnwcrtoDd4": {
    "title": "FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EJ2CQwMTci": {
    "title": "Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs Between Privacy and Regret",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EGrSMMj37o": {
    "title": "An Architecture Search Framework for Inference-Time Techniques",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cve4NOiyVp": {
    "title": "Tuning LLM Judge Design Decisions for 1/1000 of the Cost",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gV01DWTFTc": {
    "title": "Distinguishing Cause from Effect with Causal Velocity Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=skNzqUi4wk": {
    "title": "Nearly Optimal Sample Complexity for Learning with Label Proportions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l76CyYCctL": {
    "title": "Limitations of measure-first protocols in quantum machine learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1laMy7jPux": {
    "title": "Actor-Critics Can Achieve Optimal Sample Efficiency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wjZcCbTvrU": {
    "title": "Learning Mixtures of Experts with EM: A Mirror Descent Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qpi7NiaCYj": {
    "title": "Direct Motion Models for Assessing Generated Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Um2YotdbD": {
    "title": "Topology-aware Neural Flux Prediction Guided by Physics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AMB11zS6kg": {
    "title": "Safety Certificate against Latent Variables with Partially Unidentifiable Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q9lITFNKds": {
    "title": "Zero-shot Meta-learning for Tabular Prediction Tasks with Adversarially Pre-trained Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vx7YaE7vJU": {
    "title": "Batch List-Decodable Linear Regression via Higher Moments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BCJPAmlfxv": {
    "title": "Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KfTf9vFvSn": {
    "title": "From Crowdsourced Data to High-quality Benchmarks: Arena-Hard and Benchbuilder Pipeline",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DtVVltU1ak": {
    "title": "Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8SXosAVIFH": {
    "title": "(How) Do Language Models Track State?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uxzgGLWPj2": {
    "title": "In-Context Fine-Tuning for Time-Series Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ZwOiEGxCe": {
    "title": "Understanding Fixed Predictions via Confined Regions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qut63YypaD": {
    "title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fBn6om49Ur": {
    "title": "Towards scientific discovery with dictionary learning: Extracting biological concepts from microscopy foundation models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vlF9bZHrJg": {
    "title": "Sliding Puzzles Gym: A Scalable Benchmark for State Representation in Visual Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J16AIOkjjY": {
    "title": "Causal Abstraction Learning based on the Semantic Embedding Principle",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nv70EgUAA7": {
    "title": "Scalable Gaussian Processes with Latent Kronecker Structure",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AAl89VNNy1": {
    "title": "A Unified Approach to Routing and Cascading for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i6va3yHwN9": {
    "title": "On Mitigating Affinity Bias through Bandits with Evolving Biased Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vVTgnjpaLp": {
    "title": "On Linear Convergence in Smooth Convex-Concave Bilinearly-Coupled Saddle-Point Optimization: Lower Bounds and Optimal Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j24YaWZENk": {
    "title": "BARNN: A Bayesian Autoregressive and Recurrent Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hxUGmRusz5": {
    "title": "Weakly Supervised Anomaly Detection via Dual-Tailed Kernel",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X7uVxeFS9A": {
    "title": "UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AyYjRvrbDx": {
    "title": "Sortformer: A Novel Approach for Permutation-Resolved Speaker Supervision in Speech-to-Text Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EY6pXIDi3G": {
    "title": "Learning-Order Autoregressive Models with Application to Molecular Graph Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Va5jZARDcx": {
    "title": "The Hidden Joules: Evaluating the Energy Consumption of Vision Backbones for Progress Towards More Efficient Model Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=efWv8jOBkb": {
    "title": "Global Convergence and Rich Feature Learning in L -Layer Infinite-Width Neural Networks under μ Parametrization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VY74pP1w93": {
    "title": "Attributes Shape the Embedding Space of Face Recognition Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uq70mJuUB8": {
    "title": "ESPFormer: Doubly-Stochastic Attention with Expected Sliced Transport Plans",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E2VsqgKNlr": {
    "title": "One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tU8QKX4dMI": {
    "title": "AlphaVerus: Bootstrapping Formally Verified Code Generation through Self-Improving Translation and Treefinement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ThMQfsBnje": {
    "title": "SADA: Stability-guided Adaptive Diffusion Acceleration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4QcFfTu6UT": {
    "title": "Winner-takes-all for Multivariate Probabilistic Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WaJB9V2fIy": {
    "title": "Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cPMhMoJLAx": {
    "title": "Riemann Tensor Neural Networks: Learning Conservative Systems with Physics-Constrained Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BDfBKk9CbE": {
    "title": "Quantum Speedups in Regret Analysis of Infinite Horizon Average-Reward Markov Decision Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hEUgRW0qYX": {
    "title": "From Kernels to Features: A Multi-Scale Adaptive Theory of Feature Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=29eZ8pWc8E": {
    "title": "KoopSTD: Reliable Similarity Analysis between Dynamical Systems via Approximating Koopman Spectrum with Timescale Decoupling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q670PBd4p4": {
    "title": "Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NBi8mjNebT": {
    "title": "Identifying biological perturbation targets through causal differential networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=glvtfXlzCS": {
    "title": "Permutation-Free High-Order Interaction Tests",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZG7bkp6ScT": {
    "title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f4qxkR6GQK": {
    "title": "Categorical Distributional Reinforcement Learning with Kullback-Leibler Divergence: Convergence and Asymptotics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LD0qNRusFo": {
    "title": "Accelerating Quantum Reinforcement Learning with a Quantum Natural Policy Gradient Based Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dSJo5X56KQ": {
    "title": "Solving Zero-Sum Convex Markov Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z0jnz149L1": {
    "title": "Isolated Causal Effects of Natural Language",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VnO2GEpmlb": {
    "title": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0A4JSAU3FD": {
    "title": "Conditioning Diffusions Using Malliavin Calculus",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AXJnqocQpm": {
    "title": "Quantifying Prediction Consistency Under Fine-tuning Multiplicity in Tabular LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EiAQrilPYP": {
    "title": "MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-text Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2fBcAOi8lO": {
    "title": "On Measuring Long-Range Interactions in Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rLM4qDBJKA": {
    "title": "Features are fate: a theory of transfer learning in high-dimensional regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QfKrcgyase": {
    "title": "Propagate and Inject: Revisiting Propagation-Based Feature Imputation for Graphs with Partially Observed Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sp7jclUwkV": {
    "title": "Simultaneous Multi-Robot Motion Planning with Projected Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XHHIZNgrho": {
    "title": "LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oh9sG5ae2b": {
    "title": "Lower Bounds for Chain-of-Thought Reasoning in Hard-Attention Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4JyULSyPl6": {
    "title": "Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hu7hUjEMiW": {
    "title": "Automatic Reward Shaping from Confounded Offline Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WD2CKUrxmx": {
    "title": "When Diffusion Models Memorize: Inductive Biases in Probability Flow of Minimum-Norm Shallow Neural Nets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JCKkum1Qye": {
    "title": "Graph Neural Network Generalization With Gaussian Mixture Model Based Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H8JTsbG4KW": {
    "title": "Hessian Geometry of Latent Space in Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HLC9eJ8RMg": {
    "title": "A Variational Framework for Improving Naturalness in Generative Spoken Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NLiwENWuaJ": {
    "title": "Adaptive kernel predictors from feature-learning infinite limits of neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=22lwBrVUkU": {
    "title": "Doubly Robust Fusion of Many Treatments for Policy Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lLnAua5poB": {
    "title": "Differential Privacy Guarantees of Markov Chain Monte Carlo Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RAa8muWVhW": {
    "title": "Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f3iBgm2Zi0": {
    "title": "Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g45SHBmZLz": {
    "title": "QoS-Efficient Serving of Multiple Mixture-of-Expert LLMs Using Partial Runtime Reconfiguration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vYMuAcbdIE": {
    "title": "Avoiding spurious sharpness minimization broadens applicability of SAM",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YgfpMhNYnW": {
    "title": "Large Language Models are Demonstration Pre-Selectors for Themselves",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=73zrUyI5kB": {
    "title": "Thermalizer: Stable autoregressive neural emulation of spatiotemporal chaos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fxmJHFscQz": {
    "title": "Learning Gaussian DAG Models without Condition Number Bounds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HV8vZDDoYc": {
    "title": "PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ib9drlZllP": {
    "title": "Minerva: A Programmable Memory Test Benchmark for Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y0Kxvmjkmh": {
    "title": "From Jack of All Trades to Master of One: Specializing LLM-based Autoraters to a Test Set",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o7mxIi8jRv": {
    "title": "Dimension-Independent Rates for Structured Neural Density Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iuD649wPAw": {
    "title": "Preference Learning for AI Alignment: a Causal Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YW6edSufht": {
    "title": "Overtrained Language Models Are Harder to Fine-Tune",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5KszXnnkG5": {
    "title": "Cowpox: Towards the Immunity of VLM-based Multi-Agent Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JfLgvNe1tj": {
    "title": "FlowAR: Scale-wise Autoregressive Image Generation Meets Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4UfRP8MopP": {
    "title": "SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for LLM Weight Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S4JmmpnSPy": {
    "title": "Softmax is not Enough (for Sharp Size Generalisation)",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EznrK7QWgK": {
    "title": "TAROT: Targeted Data Selection via Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LmT1of1LUJ": {
    "title": "Efficient Multivariate Robust Mean Estimation Under Mean-Shift Contamination",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JgUBM5hwcM": {
    "title": "Accelerating Spectral Clustering under Fairness Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MpjtvkvXDo": {
    "title": "Overestimation in LLM Evaluation: A Controlled Large-Scale Study on Data Contamination's Impact on Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G0My9EEJbw": {
    "title": "PEAKS: Selecting Key Training Examples Incrementally via Prediction Error Anchored by Kernel Similarity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6f1Hx1eneF": {
    "title": "Geometric and Physical Constraints Synergistically Enhance Neural PDE Surrogates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9qzpNSTUYp": {
    "title": "Reward-Guided Iterative Refinement in Diffusion Models at Test-Time with Applications to Protein and DNA Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZTOVlPC5Vf": {
    "title": "Pareto-Optimality, Smoothness, and Stochasticity in Learning-Augmented One-Max-Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2B11W1Z6ID": {
    "title": "Looking Beyond the Top-1: Transformers Determine Top Tokens in Order",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SBUxQakoJJ": {
    "title": "DriveGPT: Scaling Autoregressive Behavior Models for Driving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sGny74zx2V": {
    "title": "Multiaccuracy and Multicalibration via Proxy Groups",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KVt0TeQ5Ne": {
    "title": "ADIOS: Antibody Development via Opponent Shaping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vs1u8WKlB5": {
    "title": "Universal Approximation of Mean-Field Models via Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w9HPYVpfvY": {
    "title": "What Makes a Good Feedforward Computational Graph?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qC5FZs34Xr": {
    "title": "NeuronTune: Towards Self-Guided Spurious Bias Mitigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PPsiS5nSlv": {
    "title": "Improving Rationality in the Reasoning Process of Language Models through Self-playing Game",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GdsbEOwAE7": {
    "title": "Model-Based Exploration in Monitored Markov Decision Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=idPoj6ZeDs": {
    "title": "LASER: Attention with Exponential Transformation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wIfl8PK6Op": {
    "title": "Collapse-Proof Non-Contrastive Self-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fHD76uOB4t": {
    "title": "Censor Dependent Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j4FXxMiDjL": {
    "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3xznpzabYQ": {
    "title": "Deep Neural Cellular Potts Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EiSXjzgBK4": {
    "title": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NMdWQXosFs": {
    "title": "Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jwjvkWsePB": {
    "title": "Federated Oriented Learning: A Practical One-Shot Personalized Federated Learning Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u08HAb5WyC": {
    "title": "Feedforward Few-shot Species Range Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PuVmGAggkU": {
    "title": "Improving Multi-Class Calibration through Normalization-Aware Isotonic Techniques",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L47M0km5dq": {
    "title": "Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jSxU7ZGe3B": {
    "title": "Testing the Limits of Fine-Tuning for Improving Visual Cognition in Vision Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OQXpFh0hqf": {
    "title": "Aligned Multi Objective Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=boSqwdvJVC": {
    "title": "Organize the Web: Constructing Domains Enhances Pre-Training Data Curation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zn6hmmBnAa": {
    "title": "Harnessing Heterogeneous Statistical Strength for Personalized Federated Learning via Hierarchical Bayesian Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9dHilxylvC": {
    "title": "Field Matching: an Electrostatic Paradigm to Generate and Transfer Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qZMLrURRr9": {
    "title": "R*: Efficient Reward Design via Reward Structure Evolution and Parameter Alignment Optimization with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MFNIka7nx0": {
    "title": "Soup-of-Experts: Pretraining Specialist Models via Parameters Averaging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IVUjRWnU6c": {
    "title": "LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CQZXGmw5vO": {
    "title": "SWAN: SGD with Normalization and Whitening Enables Stateless LLM Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1YOYA2zN1j": {
    "title": "Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4T7gFdhsPJ": {
    "title": "Understanding the Logic of Direct Preference Alignment through Logic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vWMij23BmQ": {
    "title": "Scaling Laws for Forgetting during Finetuning with Pretraining Data Injection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=27tMzmzDjO": {
    "title": "A Geometric Approach to Personalized Recommendation with Set-Theoretic Constraints Using Box Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sOUZXKReSN": {
    "title": "SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kip4avTjth": {
    "title": "Temporal Misalignment in ANN-SNN Conversion and its Mitigation via Probabilistic Spiking Neurons",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GSyX4amBFR": {
    "title": "Active Reward Modeling: Adaptive Preference Labeling for Large Language Model Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qu3jbojB5Q": {
    "title": "Improved and Oracle-Efficient Online ℓ 1 -Multicalibration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c4zVRwxjDD": {
    "title": "Partially Observable Reinforcement Learning with Memory Traces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QbOoz74GNO": {
    "title": "AutoCATE: End-to-End, Automated Treatment Effect Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AVFUoC53Lm": {
    "title": "Provable Length Generalization in Sequence Prediction via Spectral Filtering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fk8yB6uSJy": {
    "title": "Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Aw6dBR7Vxj": {
    "title": "DIME: Diffusion-Based Maximum Entropy Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1nEBAkpfb9": {
    "title": "Distillation Scaling Laws",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DpPUIOPY7C": {
    "title": "Inverse Flow and Consistency Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UcnL2Xd3Qm": {
    "title": "A Sub-Problem Quantum Alternating Operator Ansatz for Correlation Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hErdffTsLu": {
    "title": "Modular Duality in Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OxBWTFSGcv": {
    "title": "Differential Coding for Training-Free ANN-to-SNN Conversion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ETLKYVMVLt": {
    "title": "AlphaDPO: Adaptive Reward Margin for Direct Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7VPRrzFEN8": {
    "title": "Prompt-to-Leaderboard: Prompt-Adaptive LLM Evaluations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=afDHwQ1ZDO": {
    "title": "∞ -Video: A Training-Free Approach to Long Video Understanding via Continuous-Time Memory Consolidation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FFJFT93oa7": {
    "title": "Physics-Informed Generative Modeling of Wireless Channels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vOu5K93z4f": {
    "title": "Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mcae98J1wq": {
    "title": "Improved Theoretically-Grounded Evolutionary Algorithms for Subset Selection with a Linear Cost Constraint",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hw1kGPcSZ5": {
    "title": "Private Model Personalization Revisited",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ncTwQagrj8": {
    "title": "Larger or Smaller Reward Margins to Select Preferences for LLM Alignment?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HZdK1aj22X": {
    "title": "Learning Robust Neural Processes with Risk-Averse Stochastic Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t04D9bkKUq": {
    "title": "PertEval-scFM: Benchmarking Single-Cell Foundation Models for Perturbation Effect Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gi9MOXNfw2": {
    "title": "Vintix: Action Model via In-Context Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BuTbVl9abf": {
    "title": "MetricEmbedding: Accelerate Metric Nearness by Tropical Inner Product",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OOYPeymDz2": {
    "title": "Unified Breakdown Analysis for Byzantine Robust Gossip",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d7v2iUSa9s": {
    "title": "SkipGPT: Each Token is One of a Kind",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pQYEMwHd6c": {
    "title": "Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WFIMSlNS7C": {
    "title": "LEVIS: Large Exact Verifiable Input Spaces for Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J8YRdm39jn": {
    "title": "Active feature acquisition via explainability-driven ranking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kR5ZAP7F9b": {
    "title": "What makes an Ensemble (Un) Interpretable?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YF17x9e3J2": {
    "title": "Theoretical Performance Guarantees for Partial Domain Adaptation via Partial Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkYBAXPUBw": {
    "title": "The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oNDhnGrD51": {
    "title": "Volume Optimality in Conformal Prediction with Structured Prediction Sets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DnuuFjCsDo": {
    "title": "System-Aware Unlearning Algorithms: Use Lesser, Forget Faster",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1v3XEcRMyP": {
    "title": "Iterative Vectors: In-Context Gradient Steering without Backpropagation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MjVmVakGdx": {
    "title": "Stochastic Encodings for Active Feature Acquisition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t0x2VnBskT": {
    "title": "From Logits to Hierarchies: Hierarchical Clustering made Simple",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zltxOTEtfm": {
    "title": "Aligning Multimodal Representations through an Information Bottleneck",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DdMMzlI5YE": {
    "title": "Metadata Conditioning Accelerates Language Model Pre-training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j68NJfjMVH": {
    "title": "Online Sparsification of Bipartite-Like Clusters in Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C7dmhyTDrx": {
    "title": "Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b0qRSUcQP7": {
    "title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GfWucMJt1S": {
    "title": "Upcycling Text-to-Image Diffusion Models for Multi-Task Capabilities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D7qKAO34tp": {
    "title": "Provably Cost-Sensitive Adversarial Defense via Randomized Smoothing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o4ANDWaomX": {
    "title": "Protein Structure Tokenization: Benchmarking and New Recipe",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rUDRWP9WvZ": {
    "title": "LAuReL: Learned Augmented Residual Layer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SqnViOBHP0": {
    "title": "Towards Theoretical Understanding of Sequential Decision Making with Preference Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eWQQPtfJjV": {
    "title": "Square χ PO: Differentially Private and Robust χ 2 -Preference Optimization in Offline Direct Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7BKcLeHQsm": {
    "title": "The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models Via Visual Information Steering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=focYle7CfD": {
    "title": "MathConstruct: Challenging LLM Reasoning with Constructive Proofs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=80IwJqlXs8": {
    "title": "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GaCo82yC7z": {
    "title": "Guarantees of a Preconditioned Subgradient Algorithm for Overparameterized Asymmetric Low-rank Matrix Recovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fyi34BxCwq": {
    "title": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p7IdHkKMiJ": {
    "title": "Fast and Provable Algorithms for Sparse PCA with Improved Sample Complexity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JN8O01IZYR": {
    "title": "When, Where and Why to Average Weights?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rnMH9njZxb": {
    "title": "Understanding Input Selectivity in Mamba: Impact on Approximation Power, Memorization, and Associative Recall Capacity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uBMnbCBEtZ": {
    "title": "Progressive Tempering Sampler with Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DoaqUv7YQy": {
    "title": "Provable Maximum Entropy Manifold Exploration via Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rQK6IWHdzA": {
    "title": "A Sample Efficient Conditional Independence Test in the Presence of Discretization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RKbanvzycr": {
    "title": "Schwarz–Schur Involution: Lightspeed Differentiable Sparse Linear Solvers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qUcUyqP1UA": {
    "title": "Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0jm6gkAuYH": {
    "title": "MITIGATING OVER-EXPLORATION IN LATENT SPACE OPTIMIZATION USING LES",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VFM6BcxCd2": {
    "title": "Online Linear Classification with Massart Noise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7DVmEc7c1u": {
    "title": "A New Approach to Backtracking Counterfactual Explanations: A Unified Causal Framework for Efficient Model Interpretability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ymlwqfxuUc": {
    "title": "ABNet: Adaptive explicit-Barrier Net for Safe and Scalable Robot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Spoe53kbj9": {
    "title": "LIVS: A Pluralistic Alignment Dataset for Inclusive Public Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cEhLObwvvu": {
    "title": "Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l6mkb1LBVP": {
    "title": "LDMol: A Text-to-Molecule Diffusion Model with Structurally Informative Latent Space Surpasses AR Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ULJ4gJJYFp": {
    "title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RUip3cD66H": {
    "title": "Adapting to Evolving Adversaries with Regularized Continual Robust Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yeICCRy3lE": {
    "title": "AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1N4Y0Yj8th": {
    "title": "Causal Logistic Bandits with Counterfactual Fairness Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bikq2MsV0C": {
    "title": "On the Dynamic Regret of Following the Regularized Leader: Optimism with History Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mMasOShOVt": {
    "title": "DEALing with Image Reconstruction: Deep Attentive Least Squares",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fgxobShivL": {
    "title": "Cover learning for large-scale topology representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TkUxFAbpfQ": {
    "title": "AutoAL: Automated Active Learning with Differentiable Query Strategy Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ULZHqJU4ZC": {
    "title": "Privacy-Preserving Federated Convex Optimization: Balancing Partial-Participation and Efficiency via Noise Cancellation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mDxarRaTY9": {
    "title": "Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NngoETL9IK": {
    "title": "Pixel-level Certified Explanations via Randomized Smoothing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1WfWvpiEPE": {
    "title": "Optimal Auction Design in the Joint Advertising",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SP43jVv7fJ": {
    "title": "Robust Multimodal Large Language Models Against Modality Conflict",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zDieh7VWfN": {
    "title": "Can Large Language Models Understand Intermediate Representations in Compilers?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pWrqgY7rB1": {
    "title": "On the Duality between Gradient Transformations and Adapters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5YXzrcchMu": {
    "title": "Scalable Approximation Algorithms for p -Wasserstein Distance and Its Variants",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=saBXnGIDSj": {
    "title": "Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JAcOYCqNo9": {
    "title": "The Relationship Between No-Regret Learning and Online Conformal Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kHEVCfES4Q": {
    "title": "A Closer Look at Transformers for Time Series Forecasting: Understanding Why They Work and Where They Struggle",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s0AwKb1dAW": {
    "title": "Learning Adversarial MDPs with Stochastic Hard Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=StaRAs9n49": {
    "title": "Tree-Sliced Wasserstein Distance: A Geometric Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xkgQWEj9F2": {
    "title": "Strategic Planning: A Top-Down Approach to Option Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HlRUGC5ork": {
    "title": "Measuring Variable Importance in Heterogeneous Treatment Effects with Confidence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Si0HHbjBfU": {
    "title": "Boosting Multi-Domain Fine-Tuning of Large Language Models through Evolving Interactions between Samples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZDPNmihkMR": {
    "title": "SENSEI: Semantic Exploration Guided by Foundation Models to Learn Versatile World Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kcUNMKqrCg": {
    "title": "Conditional Diffusion Model with Nonlinear Data Transformation for Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sxL3irchez": {
    "title": "Long-Short Alignment for Effective Long-Context Modeling in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I0Ux2nAN6u": {
    "title": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SOMDiaGoil": {
    "title": "On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fwQH7M5DRM": {
    "title": "Predicting High-precision Depth on Low-Precision Devices Using 2D Hilbert Curves",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RNSd6G3lcD": {
    "title": "Universal Length Generalization with Turing Programs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zDwipF6h06": {
    "title": "KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DYUZ55CqGw": {
    "title": "Contract Design Under Approximate Best Responses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SwXYV7CIw9": {
    "title": "Tree-Sliced Wasserstein Distance with Nonlinear Projection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5WEmyTooVV": {
    "title": "ROPO: Robust Preference Optimization for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SjT6JK4KZv": {
    "title": "Propagation of Chaos for Mean-Field Langevin Dynamics and its Application to Model Ensemble",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z4IG090qt2": {
    "title": "Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=epDkt44mkq": {
    "title": "A Cognac Shot To Forget Bad Memories: Corrective Unlearning for Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x9LGowGzZu": {
    "title": "GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uJ3JqtBYWk": {
    "title": "Navigating Conflicting Views: Harnessing Trust for Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ne5brB1tKN": {
    "title": "Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hFvp9NYfY9": {
    "title": "Redundancy Undermines the Trustworthiness of Self-Interpretable GNNs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ViRFgwVjk0": {
    "title": "AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89QPmZjIhv": {
    "title": "All-atom Diffusion Transformers: Unified generative modelling of molecules and materials",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nayOhK5DCg": {
    "title": "Concept Reachability in Diffusion Models: Beyond Dataset Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dMYL47aQwb": {
    "title": "Towards flexible perception with visual memory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nyXXE3EA1b": {
    "title": "Diversity By Design: Leveraging Distribution Matching for Offline Model-Based Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v68wPjgEQ8": {
    "title": "Modified K-means Algorithm with Local Optimality Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fa0aFZ9LZi": {
    "title": "From Pixels to Perception: Interpretable Predictions via Instance-wise Grouped Feature Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1d2fpvyKvJ": {
    "title": "Understanding High-Dimensional Bayesian Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QlEx8f3S61": {
    "title": "Federated Causal Structure Learning with Non-identical Variable Sets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nLW7e7KjN0": {
    "title": "You Always Recognize Me (YARM): Robust Texture Synthesis Against Multi-View Corruption",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HexoeKd0g2": {
    "title": "LAION-C: An Out-of-Distribution Benchmark for Web-Scale Vision Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X3tspPU5nF": {
    "title": "Feature Shift Localization Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wQvR1LHboD": {
    "title": "A Theoretical Study of (Hyper) Self-Attention through the Lens of Interactions: Representation, Training, Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=21kAulloDG": {
    "title": "PoisonBench: Assessing Language Model Vulnerability to Poisoned Preference Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3aC94m0wbF": {
    "title": "Extreme Value Policy Optimization for Safe Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zjG9GRG462": {
    "title": "Surrogate Prompt Learning: Towards Efficient and Diverse Prompt Learning for Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=thCqMz1ZXw": {
    "title": "Noisy SIGNSGD Is More Differentially Private Than You (Might) Think",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZGtcgeCpWB": {
    "title": "CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JRBctqPV8U": {
    "title": "Variational Learning of Fractional Posteriors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MR9VQLWUFI": {
    "title": "BackSlash: Rate Constrained Optimized Training of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fgjN8B6xVX": {
    "title": "High-Fidelity Simultaneous Speech-To-Speech Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZYkFTSEZ6k": {
    "title": "Decoupled SGDA for Games with Intermittent Strategy Communication",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=irY40jSwCH": {
    "title": "ML 2 -GCL: Manifold Learning Inspired Lightweight Graph Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=stfnyxnhAm": {
    "title": "A Sharper Global Convergence Analysis for Average Reward Reinforcement Learning via an Actor-Critic Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EgfsB1aWaw": {
    "title": "Universal Approximation Theorem of Deep Q-Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NE6Px91RkQ": {
    "title": "Stability and Generalization Capability of Subgraph Reasoning Models for Inductive Knowledge Graph Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gmFeso9sXJ": {
    "title": "Enhancing Ligand Validity and Affinity in Structure-Based Drug Design with Multi-Reward Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YNh77OLRid": {
    "title": "Fixing the Double Penalty in Data-Driven Weather Forecasting Through a Modified Spherical Harmonic Loss Function",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FzulAfAJxE": {
    "title": "Robust Offline Reinforcement Learning with Linearly Structured f -Divergence Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sncnyt4CzJ": {
    "title": "Policy Optimization for CMDPs with Bandit Feedback: Learning Stochastic and Adversarial Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YWUYqN2laV": {
    "title": "Beyond Low-rank Decomposition: A Shortcut Approach for Efficient On-Device Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DYuTV3ESCQ": {
    "title": "Transfer Q-Learning with Composite MDP Structures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gJ7cU9cdZB": {
    "title": "Implicit degree bias in the link prediction task",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J6LYjEOxbz": {
    "title": "Layer-wise Quantization for Quantized Optimistic Dual Averaging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0YZHfUmsJv": {
    "title": "Joint Metric Space Embedding by Unbalanced Optimal Transport with Gromov–Wasserstein Marginal Penalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o5D8i2zZ1l": {
    "title": "Dataflow-Guided Neuro-Symbolic Language Models for Type Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ez7LqHsP5": {
    "title": "Tool Unlearning for Tool-Augmented LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aPMB4uwBmK": {
    "title": "Learning Minimum-Size BDDs: Towards Efficient Exact Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b0jYs6JOZu": {
    "title": "Otter: Generating Tests from Issues to Validate SWE Patches",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UvwWrUV1JV": {
    "title": "Maximizing Intermediate Checkpoint Value in LLM Pretraining with Bayesian Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z9SRjXPf8T": {
    "title": "Survival Analysis via Density Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ooUlq9jA2K": {
    "title": "Equivariant Neural Tangent Kernels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5dFJukfj4y": {
    "title": "Sorbet: A Neuromorphic Hardware-Compatible Transformer-Based Spiking Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qnE2m3pIAb": {
    "title": "Automated Benchmark Generation for Repository-Level Coding Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zVBYbjjlMX": {
    "title": "GCAL: Adapting Graph Models to Evolving Domain Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jv7bF50spq": {
    "title": "Unnatural Languages Are Not Bugs but Features for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nOfSWmPYL5": {
    "title": "Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TOn1rhgdeD": {
    "title": "Privacy Amplification Through Synthetic Data: Insights from Linear Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VRGc8KrBdP": {
    "title": "Enhancing Performance of Explainable AI Models with Constrained Concept Refinement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0d0L3U3MAM": {
    "title": "Analytical Construction on Geometric Architectures: Transitioning from Static to Temporal Link Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1VqxIgyQlp": {
    "title": "Kona: An Efficient Privacy-Preservation Framework for KNN Classification by Communication Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uXYZZv6fiE": {
    "title": "Counting atoms faster: policy-based nuclear magnetic resonance pulse sequencing for atomic abundance measurement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EHqQaBYYlE": {
    "title": "Active Evaluation Acquisition for Efficient LLM Benchmarking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qi9rPcdiTY": {
    "title": "Large Language-Geometry Model: When LLM meets Equivariance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pw5ySjY11s": {
    "title": "Deep Reinforcement Learning from Hierarchical Preference Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BGpVHG1wiE": {
    "title": "Predictive Performance of Deep Quantum Data Re-uploading Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rS3ufabhQr": {
    "title": "ViTally Consistent: Scaling Biological Representation Learning for Cell Microscopy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hYxZJycvrz": {
    "title": "Integration-free Kernels for Equivariant Gaussian Process Modelling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q4cNdrwKAk": {
    "title": "Simple Randomized Rounding for Max-Min Eigenvalue Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z2uLBBck2X": {
    "title": "Calibrated Physics-Informed Uncertainty Quantification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AJN5btaqNk": {
    "title": "Score-based Pullback Riemannian Geometry: Extracting the Data Manifold Geometry using Anisotropic Flows",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=leh2A9fIhY": {
    "title": "Private Lossless Multiple Release",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2wt8m5HUBs": {
    "title": "NICE Data Selection for Instruction Tuning in LLMs with Non-differentiable Evaluation Metric",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eUMGCipgtE": {
    "title": "Aligning LLMs by Predicting Preferences from User Writing Samples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pgrJPhsk2w": {
    "title": "Learning Representations of Instruments for Partial Identification of Treatment Effects",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YJZFAtuQWX": {
    "title": "Exact Upper and Lower Bounds for the Output Distribution of Neural Networks with Random Inputs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sgHza4bz3n": {
    "title": "Strong and Weak Identifiability of Optimization-based Causal Discovery in Non-linear Additive Noise Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=czqxB524oo": {
    "title": "Revisiting Non-Acyclic GFlowNets in Discrete Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I8jMuUDOxK": {
    "title": "Quantifying Treatment Effects: Estimating Risk Ratios via Observational Studies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GekXB58ZS7": {
    "title": "AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive Modelling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HqmXiuFaOr": {
    "title": "Improving Generalization in Federated Learning with Highly Heterogeneous Data via Momentum-Based Stochastic Controlled Weight Averaging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zkGfPYAM5D": {
    "title": "Point-Level Topological Representation Learning on Point Clouds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KugSHTH0c8": {
    "title": "A Hitchhiker's Guide to Scaling Law Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BYakLzKJDz": {
    "title": "iN2V: Bringing Transductive Node Embeddings to Inductive Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7fQohcFrxG": {
    "title": "FLAM: Frame-Wise Language-Audio Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H4jB8GBqaO": {
    "title": "Probabilistic Group Mask Guided Discrete Optimization for Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rCJNbDXkvC": {
    "title": "Improved Coresets for Vertical Federated Learning: Regularized Linear and Logistic Regressions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w7lm8AjzH6": {
    "title": "A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1IyPRv1A0r": {
    "title": "A Likelihood Based Approach to Distribution Regression Using Conditional Deep Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o62ZzfCEwZ": {
    "title": "Investigating the Overlooked Hessian Structure: From CNNs to LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BFU7QLDku5": {
    "title": "PAC Learning with Improvements",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DYQW7QrOyq": {
    "title": "Logits are All We Need to Adapt Closed Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WCkMkMcqpb": {
    "title": "The Sparse-Plus-Low-Rank Quasi-Newton Method for Entropic-Regularized Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8prLgZ0vmm": {
    "title": "Progressively Label Enhancement for Large Language Model Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3VN8FxSzDa": {
    "title": "Efficient LiDAR Reflectance Compression via Scanning Serialization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dqYO5LVyYh": {
    "title": "Circumventing Backdoor Space via Weight Symmetry",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xKMMGugUgy": {
    "title": "Dimensionality Reduction on Complex Vector Spaces for Euclidean Distance with Dynamic Weights",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7OxAdd8BUo": {
    "title": "Breaking the n 1.5 Additive Error Barrier for Private and Efficient Graph Sparsification via Private Expander Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XIyrotmBSJ": {
    "title": "KEA: Keeping Exploration Alive by Proactively Coordinating Exploration Strategies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y8lfuSoqQz": {
    "title": "OV-MER: Towards Open-Vocabulary Multimodal Emotion Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PHhW4PEm2D": {
    "title": "Model Uncertainty Quantification by Conformal Prediction in Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BrLuZ0HOnb": {
    "title": "Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pXoUfvNRRu": {
    "title": "Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hDH3KfZSsF": {
    "title": "Improving Multimodal Learning Balance and Sufficiency through Data Remixing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qxSFIigPug": {
    "title": "On Teacher Hacking in Language Model Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xw01vF13aV": {
    "title": "Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fWv0aGD1Xu": {
    "title": "Hierarchical Reinforcement Learning with Targeted Causal Interventions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=99zsyZpUqp": {
    "title": "Clustering Items through Bandit Feedback: Finding the Right Feature out of Many",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ETIsFhZwhJ": {
    "title": "Unbiased Evaluation of Large Language Models from a Causal Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ek5a5WC4TW": {
    "title": "Online Curvature-Aware Replay: Leveraging 2 nd Order Information for Online Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qAHnSkHvsm": {
    "title": "Persistent Topological Features in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ialr09SfeJ": {
    "title": "Synonymous Variational Inference for Perceptual Image Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DidTLeezyp": {
    "title": "Angle Domain Guidance: Latent Diffusion Requires Rotation Rather Than Extrapolation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hcJWWC82KW": {
    "title": "Energy-Based Flow Matching for Generating 3D Molecular Structure",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1RS4cPFNZ6": {
    "title": "Optimistic Algorithms for Adaptive Estimation of the Average Treatment Effect",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iUDsgI8z1T": {
    "title": "Random Feature Representation Boosting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wSZeQoJ1Vk": {
    "title": "Revisiting Chain-of-Thought in Code Generation: Do Language Models Need to Learn Reasoning before Coding?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JZmL3SjSag": {
    "title": "Tackling Dimensional Collapse toward Comprehensive Universal Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bp975dIAjt": {
    "title": "An in depth look at the Procrustes-Wasserstein distance: properties and barycenters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BMxcJwaKhr": {
    "title": "When Can Proxies Improve the Sample Complexity of Preference Learning?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lQWTRVrArk": {
    "title": "Branches: Efficiently Seeking Optimal Sparse Decision Trees via AO*",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q0rJmpLat9": {
    "title": "Zero-Shot Cyclic Peptide Design via Composable Geometric Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t2xZSQTArz": {
    "title": "Deep Principal Support Vector Machines for Nonlinear Sufficient Dimension Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IjOWms0hrf": {
    "title": "Understanding Chain-of-Thought in LLMs through Information Theory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YbtH1aoE1V": {
    "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=auwNRkhkN4": {
    "title": "Learning-Augmented Algorithms for MTS with Bandit Access to Multiple Predictors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HLsZQdHaoG": {
    "title": "Laplace Transform Based Low-Complexity Learning of Continuous Markov Semigroups",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZvkyeUrpsA": {
    "title": "Efficient Logit-based Knowledge Distillation of Deep Spiking Neural Networks for Full-Range Timestep Deployment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UCJSF6Vt0C": {
    "title": "Inverse Bridge Matching Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pFqUNiwC7Z": {
    "title": "The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i17GUNGzVq": {
    "title": "Counterfactual Voting Adjustment for Quality Assessment and Fairer Voting in Online Platforms with Helpfulness Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KFMuaSG7eB": {
    "title": "GradPS: Resolving Futile Neurons in Parameter Sharing Network for Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aCBd1FeE5Z": {
    "title": "Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QxOgS8WwCr": {
    "title": "Synthetic Face Datasets Generation via Latent Space Exploration from Brownian Identity Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lNyaQIJ5Z7": {
    "title": "SDMG: Smoothing Your Diffusion Models for Powerful Graph Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j88QAtutwW": {
    "title": "Causal Discovery from Conditionally Stationary Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzTb1h2nsk": {
    "title": "FairICP: Encouraging Equalized Odds via Inverse Conditional Permutation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PdBEggnDIl": {
    "title": "Adaptive Median Smoothing: Adversarial Defense for Unlearned Text-to-Image Diffusion Models at Inference Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=htP5YRXcS9": {
    "title": "Craftium: Bridging Flexibility and Efficiency for Rich 3D Single- and Multi-Agent Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=22kNOkkokU": {
    "title": "Zebra: In-Context Generative Pretraining for Solving Parametric PDEs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V1YfPJDliw": {
    "title": "Loss Functions and Operators Generated by f-Divergences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z3qwOvWbC8": {
    "title": "Learnware Specification via Dual Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aXU48nrA2v": {
    "title": "Compositional Condition Question Answering in Tabular Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tbH0td0deH": {
    "title": "The Power of Random Features and the Limits of Distribution-Free Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uxA0GI240s": {
    "title": "MOGIC: Metadata-infused Oracle Guidance for Improved Extreme Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x4yTgv2WkJ": {
    "title": "Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DLlVjZQ7vD": {
    "title": "How Far Is Video Generation from World Model: A Physical Law Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uPgr7MzPKI": {
    "title": "Joint Learning of Energy-based Models and their Partition Function",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H0ySAzwu8k": {
    "title": "GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FXQ09DpwXt": {
    "title": "Improved Online Confidence Bounds for Multinomial Logistic Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5DSj3MfWrB": {
    "title": "VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Snksn3U47": {
    "title": "Distributed Differentially Private Data Analytics via Secure Sketching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lw0kC75dY0": {
    "title": "Learning curves theory for hierarchically compositional data with power-law distributed features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aEsIW59zDm": {
    "title": "Overcoming the Curse of Dimensionality in Reinforcement Learning Through Approximate Factorization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qib0e91UcC": {
    "title": "Combinatorial Reinforcement Learning with Preference Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dPyA4ZYs7n": {
    "title": "Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C4F42Ho7IM": {
    "title": "Defending LVLMs Against Vision Attacks Through Partial-Perception Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fINjgBMnTS": {
    "title": "A Variational Perspective on Generative Protein Fitness Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hTrSxX3kiV": {
    "title": "Stay Hungry, Keep Learning: Sustainable Plasticity for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TMRh3ScSCb": {
    "title": "Learning Latent Graph Structures and their Uncertainty",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bb0zKbPE0L": {
    "title": "X-Hacking: The Threat of Misguided AutoML",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sgrJs7dbWC": {
    "title": "Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=44Wq2xeRF0": {
    "title": "Multi-Domain Graph Foundation Models: Robust Knowledge Transfer via Topology Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=li59703WbA": {
    "title": "LBI-FL: Low-Bit Integerized Federated Learning with Temporally Dynamic Bit-Width Allocation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RHAWcjIyl2": {
    "title": "AAAR-1.0: Assessing AI's Potential to Assist Research",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SrEOUSyJcR": {
    "title": "Moirai-MoE: Empowering Time Series Foundation Models with Sparse Mixture of Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lZ18hxItYI": {
    "title": "Cooperation of Experts: Fusing Heterogeneous Information with Large Margin",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rTcK6oq0On": {
    "title": "N2GON: Neural Networks for Graph-of-Net with Position Awareness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9NVm1Bf7CS": {
    "title": "WikiBigEdit: Understanding the Limits of Lifelong Knowledge Editing in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iUQORXdrCG": {
    "title": "The Batch Complexity of Bandit Pure Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fvmnx3OxTI": {
    "title": "Learning Cascade Ranking as One Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Dp5jdNs2s": {
    "title": "Convergence of Policy Mirror Descent Beyond Compatible Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f4CPc211U1": {
    "title": "Provable Policy Gradient for Robust Average-Reward MDPs Beyond Rectangularity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WbfbT2BH6F": {
    "title": "TabNAT: A Continuous-Discrete Joint Generative Framework for Tabular Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CpjKXe9rY7": {
    "title": "Maximum Entropy Reinforcement Learning with Diffusion Policy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RBly0nOr2h": {
    "title": "Categorical Schrödinger Bridge Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o4KpjiCdrk": {
    "title": "Geometry-Informed Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hz9LN310jZ": {
    "title": "The impact of uncertainty on regularized learning in games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XjbJR9374o": {
    "title": "FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IEyNrmICas": {
    "title": "AffinityFlow: Guided Flows for Antibody Affinity Maturation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pI4AbQ7pg1": {
    "title": "Federated Learning for Feature Generalization with Convex Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sDGafRLNQa": {
    "title": "Multi-Objective Causal Bayesian Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xf0tiH1e4u": {
    "title": "Continuous Semi-Implicit Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G8R3ni0MI4": {
    "title": "A Unified Comparative Study with Generalized Conformity Scores for Multi-Output Conformal Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WGejWCgrpD": {
    "title": "Learning the Electronic Hamiltonian of Large Atomic Structures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4A0ZaS9kj2": {
    "title": "Topology-Aware Dynamic Reweighting for Distribution Shifts on Graph",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H4UMsoQrdI": {
    "title": "Benign Overfitting in Token Selection of Attention Mechanism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X7X4YLMneu": {
    "title": "When to retrain a machine learning model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kT0EVqL77E": {
    "title": "The Elicitation Game: Evaluating Capability Elicitation Techniques",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lflqQWP1jy": {
    "title": "WILTing Trees: Interpreting the Distance Between MPNN Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r4XsTcuiqc": {
    "title": "Models of Heavy-Tailed Mechanistic Universality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EL61NlfSa1": {
    "title": "Explaining the role of Intrinsic Dimensionality in Adversarial Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JTOOU5SsXT": {
    "title": "Fast Estimation of Partial Dependence Functions using Trees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SqhhqMbqV3": {
    "title": "Regret-Free Reinforcement Learning for Temporal Logic Specifications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VD4rLMrHXZ": {
    "title": "Multidimensional Adaptive Coefficient for Inference Trajectory Optimization in Flow and Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GNTmqRTpzr": {
    "title": "EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OjXXqktkPz": {
    "title": "Hypothesis Testing for Generalized Thurstone Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WeOLZmDXyA": {
    "title": "Reducing Tool Hallucination via Reliability Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2pdFMgv54m": {
    "title": "Measuring Diversity: Axioms and Challenges",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9uPCP3j62m": {
    "title": "Clustering via Self-Supervised Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u8wRbX2r2V": {
    "title": "HyperIMTS: Hypergraph Neural Network for Irregular Multivariate Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dKn5mBPmkZ": {
    "title": "Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sUVOXjOglX": {
    "title": "Aggregation Buffer: Revisiting DropEdge with a New Parameter Block",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JnXbUKtLzz": {
    "title": "Sort Before You Prune: Improved Worst-Case Guarantees of the DiskANN Family of Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6CAgbrjHTc": {
    "title": "Cradle: Empowering Foundation Agents towards General Computer Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eHs8vIEhtE": {
    "title": "From Theory to Practice: Rethinking Green and Martin Kernels for Unleashing Graph Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eNKEToqt1E": {
    "title": "Function-Space Learning Rates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bxYbxzCI2R": {
    "title": "Diffusion models for Gaussian distributions: Exact solutions and Wasserstein errors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bXilZCSueG": {
    "title": "Tensor-Var: Efficient Four-Dimensional Variational Data Assimilation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8mScy0IDRl": {
    "title": "Maximum Total Correlation Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wwYDQ1vXcZ": {
    "title": "Relational Conformal Prediction for Correlated Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v4mcWhN0Sf": {
    "title": "Beyond Communication Overhead: A Multilevel Monte Carlo Approach for Mitigating Compression Bias in Distributed Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1BaC3AdG1i": {
    "title": "ATA: Adaptive Task Allocation for Efficient Resource Management in Distributed Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=stgw28KnaX": {
    "title": "Dynamic Similarity Graph Construction with Kernel Density Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TPuFRuNano": {
    "title": "Jacobian Sparse Autoencoders: Sparsify Computations, Not Just Activations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sUBuOCquHX": {
    "title": "Predicting the Susceptibility of Examples to Catastrophic Forgetting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oty1LQrnFc": {
    "title": "Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J5SbLoq7Uv": {
    "title": "Are Large Brainwave Foundation Models Capable Yet ? Insights from Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6udKBHc0Mr": {
    "title": "Exploiting Presentative Feature Distributions for Parameter-Efficient Continual Learning of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RfqTvlo9XQ": {
    "title": "An Efficient Search-and-Score Algorithm for Ancestral Graphs using Multivariate Information Scores for Complex Non-linear and Categorical Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qFXDv0X4yc": {
    "title": "MERGE 3 : Efficient Evolutionary Merging on Consumer-grade GPUs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l7QzcZpjc5": {
    "title": "EvoPress: Accurate Dynamic Model Compression via Evolutionary Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VhEpf2HFr0": {
    "title": "Scalable Sobolev IPM for Probability Measures on a Graph",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kB6mJY0MTV": {
    "title": "Learning Single Index Models with Diffusion Priors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cx5aNPycdO": {
    "title": "Learning Utilities from Demonstrations in Markov Decision Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sa9DluzlXJ": {
    "title": "Finding Wasserstein Ball Center: Efficient Algorithm and The Applications in Fairness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XbmBNwrfG5": {
    "title": "No Metric to Rule Them All: Toward Principled Evaluations of Graph-Learning Datasets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=clLERWKNja": {
    "title": "Bootstrapping Self-Improvement of Language Model Programs for Zero-Shot Schema Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f3mQ0xYA1I": {
    "title": "The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7exLwkfcGL": {
    "title": "Event-Customized Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2MPI9KSBV5": {
    "title": "Interchangeable Token Embeddings for Extendable Vocabulary and Alpha-Equivalence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AhebPqDOMI": {
    "title": "Teaching Transformers Causal Reasoning through Axiomatic Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4vb9BDTIDh": {
    "title": "Revolve: Optimizing AI Systems by Tracking Response Evolution in Textual Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dsBjxI6l8W": {
    "title": "Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tpbtodnI1p": {
    "title": "World Model Implanting for Test-time Adaptation of Embodied Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uocLJOnKBv": {
    "title": "COSDA: Counterfactual-based Susceptibility Risk Framework for Open-Set Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eFHfRQRjJo": {
    "title": "Wyckoff Transformer: Generation of Symmetric Crystals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=imkFoKwFwd": {
    "title": "Subobject-level Image Tokenization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hlyBdwHBeC": {
    "title": "Active Fine-Tuning of Multi-Task Policies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5tyvHfhRFZ": {
    "title": "Riemannian Diffusion Adaptation for Distributed Optimization on Manifolds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V18WOxHRMq": {
    "title": "FedSMU: Communication-Efficient and Generalization-Enhanced Federated Learning through Symbolic Model Updates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lHaGLJ65J9": {
    "title": "Ranked Entropy Minimization for Continual Test-Time Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZVWJO5YTz4": {
    "title": "Optimizing Language Models for Inference Time Objectives using Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hw6MDsaor9": {
    "title": "CoMemo: LVLMs Need Image Context with Image Memory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vg9pb5Le63": {
    "title": "Expressive Score-Based Priors for Distribution Matching with Geometry-Preserving Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IiVM70BgSV": {
    "title": "Feature Importance Metrics in the Presence of Missing Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fNixzmprun": {
    "title": "Revisiting Neural Networks for Few-Shot Learning: A Zero-Cost NAS Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5DD3RCcVcT": {
    "title": "TabPFN Unleashed: A Scalable and Effective Solution to Tabular Classification Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QWpuqidr53": {
    "title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5anr1H5vR5": {
    "title": "Online Pre-Training for Offline-to-Online Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5MiSZuBLmq": {
    "title": "Understanding the Limits of Deep Tabular Methods with Temporal Shift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NYi9B34E1e": {
    "title": "Action-Constrained Imitation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oWkRmgJgMJ": {
    "title": "On the Private Estimation of Smooth Transport Maps",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U0mI6M7lvI": {
    "title": "ARS: Adaptive Reward Scaling for Multi-Task Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AYxiZfJN9V": {
    "title": "GenZSL: Generative Zero-Shot Learning Via Inductive Variational Autoencoder",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1jd25AlvHS": {
    "title": "Test-Time Multimodal Backdoor Detection by Contrastive Prompting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mzz4BhdIFb": {
    "title": "ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y34a82DptF": {
    "title": "Learning Mean Field Control on Sparse Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AVXApuBCvN": {
    "title": "How Transformers Learn Structured Data: Insights From Hierarchical Filtering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DJiouYdH19": {
    "title": "Efficient Robotic Policy Learning via Latent Space Backward Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oF1OPyMw1m": {
    "title": "Physics-Informed DeepONets for drift-diffusion on metric graphs: simulation and parameter identification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NniXePXVXw": {
    "title": "MDDM: Practical Message-Driven Generative Image Steganography Based on Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z1GNg9Jwqd": {
    "title": "CursorCore: Assist Programming through Aligning Anything",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W598uW37zt": {
    "title": "UniSim: A Unified Simulator for Time-Coarsened Dynamics of Biomolecules",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bd9JlrqZhN": {
    "title": "EAGLES: Towards Effective, Efficient, and Economical Federated Graph Learning via Unified Sparsification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H4BuhRezCV": {
    "title": "On Expressive Power of Looped Transformers: Theoretical Analysis and Enhancement via Timestep Encoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6vk6Xg24ZC": {
    "title": "Imagine While Reasoning in Space: Multimodal Visualization-of-Thought",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FCHIGDCoow": {
    "title": "CASE-Bench: Context-Aware SafEty Benchmark for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ci1S6wmXfO": {
    "title": "Peri-LN: Revisiting Normalization Layer in the Transformer Architecture",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W9YdVrSJIh": {
    "title": "Latent Variable Causal Discovery under Selection Bias",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7w1YGBPDMw": {
    "title": "Mechanistic PDE Networks for Discovery of Governing Equations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9hiq7LaV4G": {
    "title": "Zero-Shot Offline Imitation Learning via Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=crcxt5uy8x": {
    "title": "Reinforcement Learning with Random Time Horizons",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=swkgSw3IzR": {
    "title": "Hierarchical Masked Autoregressive Models with Low-Resolution Token Pivots",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LCr6CIAEye": {
    "title": "Preference Adaptive and Sequential Text-to-Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YKAnIvY5hf": {
    "title": "Quantifying Memory Utilization with Effective State-Size",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hj8hAfft9V": {
    "title": "De-AntiFake: Rethinking the Protective Perturbations Against Voice Cloning Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=33YrT1j0O0": {
    "title": "Autoformulation of Mathematical Optimization Models Using LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QwHUnp8Dt4": {
    "title": "Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jDdaysR6bz": {
    "title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AIx21InAn2": {
    "title": "Closed-form Solutions: A New Perspective on Solving Differential Equations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ak0vlKTkpx": {
    "title": "Programming Every Example: Lifting Pre-training Data Quality Like Experts at Scale",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ruj5ILBUuK": {
    "title": "Enhancing Visual Localization with Cross-Domain Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kEAyffH3tn": {
    "title": "Optimal transport-based conformal prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2tH2vexW1Z": {
    "title": "Structure Is All You Need: Structural Representation Learning on Hyper-Relational Knowledge Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kGg1ndttmI": {
    "title": "Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bnhFueOeav": {
    "title": "Robust Multi-Agent Reinforcement Learning with Stochastic Adversary",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KCDoaDlzUB": {
    "title": "Graph-Supported Dynamic Algorithm Configuration for Multi-Objective Combinatorial Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XuCf87V8OF": {
    "title": "A Memory Efficient Randomized Subspace Optimization Method for Training Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FLy6yXdrlW": {
    "title": "BOPO: Neural Combinatorial Optimization via Best-anchored and Objective-guided Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZzuaeYvLsJ": {
    "title": "Deep Fuzzy Multi-view Learning for Reliable Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2GmDdhBdDk": {
    "title": "The Berkeley Function Calling Leaderboard (BFCL): From Tool Use to Agentic Evaluation of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pT4gJ0PgGD": {
    "title": "Learning Efficient Robotic Garment Manipulation with Standardization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ysRvf0tjVP": {
    "title": "TANGO: Clustering with Typicality-Aware Nonlocal Mode-Seeking and Graph-Cut Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZawsPjlIGu": {
    "title": "GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A6RjIi2ONN": {
    "title": "TMetaNet: Topological Meta-Learning Framework for Dynamic Link Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NE9ajrddxX": {
    "title": "Finite-Sample Convergence Bounds for Trust Region Policy Optimization in Mean Field Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QJLGj57MfZ": {
    "title": "Improving the Effective Receptive Field of Message-Passing Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VzC3BAd9gf": {
    "title": "The Courage to Stop: Overcoming Sunk Cost Fallacy in Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iCTybKNnqb": {
    "title": "Tractable Transformers for Flexible Conditional Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ekez0DZZPe": {
    "title": "Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G2PujeWPAL": {
    "title": "Unveiling Markov heads in Pretrained Language Models for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wxU2LuTE74": {
    "title": "Sparse Autoencoders, Again?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Xvkpaikt4": {
    "title": "The Missing Alignment Link of In-context Learning on Sequences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CLF25dahgA": {
    "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs – No Silver Bullet for LC or RAG Routing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CMoX0BEsDs": {
    "title": "VCT: Training Consistency Models with Variational Noise Coupling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z6GEZ2ogct": {
    "title": "DRAG: Data Reconstruction Attack using Guided Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OgfvSDn73E": {
    "title": "CALM: Consensus-Aware Localized Merging for Multi-Task Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TCsdlqzZNL": {
    "title": "Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OxzPgnkbB1": {
    "title": "BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WFhQ9owxOH": {
    "title": "LightningDrag: Lightning Fast and Accurate Drag-based Image Editing Emerging from Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6YUdCt7rUR": {
    "title": "FDGen: A Fairness-Aware Graph Generation Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DWCDyGl6k8": {
    "title": "A Closer Look at Backdoor Attacks on CLIP",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SrfwiloGQF": {
    "title": "A Physics-Informed Machine Learning Framework for Safe and Optimal Control of Autonomous Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GCjsjnl8yA": {
    "title": "ProofAug: Efficient Neural Theorem Proving via Fine-grained Proof Structure Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l6uwlGANSz": {
    "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SVl9tIADWV": {
    "title": "DocKS-RAG: Optimizing Document-Level Relation Extraction through LLM-Enhanced Hybrid Prompt Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nh9mBCYeF7": {
    "title": "An Efficient Pruner for Large Language Model with Theoretical Guarantee",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUCYJ9JJuZ": {
    "title": "Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O3dsbpAcqJ": {
    "title": "CurvGAD: Leveraging Curvature for Enhanced Graph Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pLQtovjXiw": {
    "title": "Reasoning Through Execution: Unifying Process and Outcome Rewards for Code Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=me6PfbATWM": {
    "title": "MMInference: Accelerating Pre-filling for Long-Context Visual Language Models via Modality-Aware Permutation Sparse Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pf0PaYS9KG": {
    "title": "How Much Can We Forget about Data Contamination?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=URm4JsuHaf": {
    "title": "The Generalized Skew Spectrum of Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=csB1njlpjM": {
    "title": "On the Provable Separation of Scales in Maximal Update Parameterization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nnO4TpeUnH": {
    "title": "Latent Score-Based Reweighting for Robust Classification on Imbalanced Tabular Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=47ghX0qpYW": {
    "title": "Low-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EXds2NBOoq": {
    "title": "Posterior Inference with Diffusion Models for High-dimensional Black-box Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dlshKV0S9t": {
    "title": "Physics-Informed Weakly Supervised Learning For Interatomic Potentials",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9P5e6iE4WK": {
    "title": "OptMATH: A Scalable Bidirectional Data Synthesis Framework for Optimization Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xcnbtxns0V": {
    "title": "Non-Stationary Predictions May Be More Informative: Exploring Pseudo-Labels with a Two-Phase Pattern of Training Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QnjfkhrbYK": {
    "title": "Is Best-of-N the Best of Them? Coverage, Scaling, and Optimality in Inference-Time Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X3ikghfWwD": {
    "title": "Diving into Self-Evolving Training for Multimodal Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ucU1o3PNB0": {
    "title": "SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i0m3hToUwf": {
    "title": "Learning Joint Interventional Effects from Single-Variable Interventions in Additive Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pWWUJw2qew": {
    "title": "Causality Inspired Federated Learning for OOD Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yiZtLtvW9S": {
    "title": "Policy Gradient with Tree Expansion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sw1Sm72dmV": {
    "title": "The Ripple Effect: On Unforeseen Complications of Backdoor Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6BJFle1f23": {
    "title": "Average Sensitivity of Hierarchical k -Median Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9B5pBbzCwQ": {
    "title": "Heavy-Tailed Linear Bandits: Huber Regression with One-Pass Update",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9aEYGpSV6v": {
    "title": "One-dimensional Path Convolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GMjkK2CKx5": {
    "title": "WATCH: Adaptive Monitoring for AI Deployments via Weighted-Conformal Martingales",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Inrv8EXylW": {
    "title": "Discrete Neural Algorithmic Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w5fONAEwra": {
    "title": "Outlier-Aware Post-Training Quantization for Discrete Graph Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cerqDkPLx7": {
    "title": "Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CAbuWU44ky": {
    "title": "Sable: a Performant, Efficient and Scalable Sequence Model for MARL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4BARFrWnTI": {
    "title": "Subspace Optimization for Large Language Models with Convergence Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pvfd7NiUS6": {
    "title": "Conformity Score Averaging for Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b3xzkfd0G1": {
    "title": "Efficiently Access Diffusion Fisher: Within the Outer Product Span Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Qj3xSwN2I": {
    "title": "Flat-LoRA: Low-Rank Adaptation over a Flat Loss Landscape",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UeGo9RmpRY": {
    "title": "Efficient Core-set Selection for Deep Learning Through Squared Loss Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k6p8UKRdH7": {
    "title": "Nemotron-CORTEXA: Enhancing LLM Agents for Software Engineering Tasks via Improved Localization and Solution Diversity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jnpgx8OzfD": {
    "title": "Optimization Proxies using Limited Labeled Data and Training Time -- A Semi-Supervised Bayesian Neural Network Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4UF0zeLwyE": {
    "title": "Banyan: Improved Representation Learning with Explicit Structure",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XzZC4gs1mf": {
    "title": "LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mMgSxbO4H0": {
    "title": "Reasoning-as-Logic-Units: Scaling Test-Time Reasoning in Large Language Models Through Logic Unit Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HZCx5EToh9": {
    "title": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1k4dKH1XOz": {
    "title": "End-to-End Learning Framework for Solving Non-Markovian Optimal Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uZvQMfA6Rh": {
    "title": "FourierMamba: Fourier Learning Integration with State Space Models for Image Deraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RTHTyTsRT3": {
    "title": "Multi-objective Linear Reinforcement Learning with Lexicographic Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eYBA77XorT": {
    "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OYUG5SCg6k": {
    "title": "Smooth Interpolation for Improved Discrete Graph Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TeHF8YjJaw": {
    "title": "Non-stationary Online Learning for Curved Losses: Improved Dynamic Regret via Mixability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HdS6tZwwa7": {
    "title": "Janus: Dual-Server Multi-Round Secure Aggregation with Verifiability for Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RR05ym7DNF": {
    "title": "Residual Matrix Transformers: Scaling the Size of the Residual Stream",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=toZOqONu9x": {
    "title": "One Leaf Reveals the Season: Occlusion-Based Contrastive Learning with Semantic-Aware Views for Efficient Visual Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IXYBuwCOMl": {
    "title": "BSLoRA: Enhancing the Parameter Efficiency of LoRA with Intra-Layer and Inter-Layer Sharing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lvvMwGUam6": {
    "title": "Code-Generated Graph Representations Using Multiple LLM Agents for Material Properties Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sfQJrVb4XM": {
    "title": "SPHINX: Structural Prediction using Hypergraph Inference Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u5yUsOm7qv": {
    "title": "Cross-City Latent Space Alignment for Consistency Region Embedding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KUphSx7PAC": {
    "title": "Learning Along the Arrow of Time: Hyperbolic Geometry for Backward-Compatible Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pXoZLGMNDm": {
    "title": "MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1wq7uK5de9": {
    "title": "Positional Encoding meets Persistent Homology on Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8PCxOlwbIn": {
    "title": "PARQ: Piecewise-Affine Regularized Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wleRTUQj07": {
    "title": "Less is More: Federated Graph Learning with Alleviating Topology Heterogeneity from A Causal Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9LSx4IS9Mx": {
    "title": "Learning from Suboptimal Data in Continuous Control via Auto-Regressive Soft Q-Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=crCPLUtIuU": {
    "title": "Chip Placement with Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pNyodFNPhv": {
    "title": "Preconditioned Riemannian Gradient Descent Algorithm for Low-Multilinear-Rank Tensor Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aPhRysevbu": {
    "title": "Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ENSibPAjks": {
    "title": "MuLan: Adapting Multilingual Diffusion Models for Hundreds of Languages with Negligible Cost",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QW6f72tutX": {
    "title": "Generative Human Trajectory Recovery via Embedding-Space Conditional Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5QU8dAnLgO": {
    "title": "Prior Knowledge Guided Neural Architecture Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zvyHCOcwsw": {
    "title": "Dequantified Diffusion-Schrödinger Bridge for Density Ratio Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wITGhlMPXz": {
    "title": "Relating Misfit to Gain in Weak-to-Strong Generalization Beyond the Squared Loss",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7SHbJENgHX": {
    "title": "QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0hrkN07DuO": {
    "title": "Linear convergence of Sinkhorn's algorithm for generalized static Schrödinger bridge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aJIoBur0Ef": {
    "title": "AnyEdit: Edit Any Knowledge Encoded in Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jeJGH6UDOL": {
    "title": "Global Context-aware Representation Learning for Spatially Resolved Transcriptomics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uDreZphNky": {
    "title": "Taming Rectified Flow for Inversion and Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t5QNCIltAn": {
    "title": "Energy-Based Preference Model Offers Better Offline Alignment than the Bradley-Terry Preference Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mqY0X3Yoy9": {
    "title": "CogMath: Assessing LLMs' Authentic Mathematical Ability from a Human Cognitive Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mqDgNdiTai": {
    "title": "PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware Selection of Generative Models and LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UfLJqcEle6": {
    "title": "Token Signature: Predicting Chain-of-Thought Gains with Token Decoding Feature in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fFgiXamW8E": {
    "title": "An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=upSPbQoViI": {
    "title": "What Makes In-context Learning Effective for Mathematical Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PO9bBEPNWy": {
    "title": "SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LB5F02kwAv": {
    "title": "LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZfDNDkg7Dh": {
    "title": "Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UTvdB2WPSp": {
    "title": "A General Graph Spectral Wavelet Convolution via Chebyshev Order Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gqKz6uCbI1": {
    "title": "Flow Matching for Denoised Social Recommendation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BnfJSwtHLu": {
    "title": "Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WPeAraSKAK": {
    "title": "Discovering Latent Causal Graphs from Spatiotemporal Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0OshX1hiSa": {
    "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bPJo5uSkOJ": {
    "title": "Pixel2Feature Attack (P2FA): Rethinking the Perturbed Space to Enhance Adversarial Transferability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pSdWTED0ZZ": {
    "title": "Efficient Heterogeneity-Aware Federated Active Data Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2QaqxseJYT": {
    "title": "The Polynomial Stein Discrepancy for Assessing Moment Convergence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S9LkB0UBKb": {
    "title": "The Role of Sparsity for Length Generalization in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qGDlzt3dKz": {
    "title": "Cost-efficient Collaboration between On-device and Cloud Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HfefR1baKA": {
    "title": "Overcoming Non-monotonicity in Transducer-based Streaming Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e0OFWfvLCO": {
    "title": "Neural Solver Selection for Combinatorial Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=46n3izUNiv": {
    "title": "Origin Identification for Text-Guided Image-to-Image Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cq1BNvHx74": {
    "title": "Training Software Engineering Agents and Verifiers with SWE-Gym",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kjz03pmyW0": {
    "title": "SPMC: Self-Purifying Federated Backdoor Defense via Margin Contribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qaJuLzY6iL": {
    "title": "Linear Mode Connectivity between Multiple Models modulo Permutation Symmetries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=unUW6MC7Su": {
    "title": "Constrained Exploitability Descent: An Offline Reinforcement Learning Method for Finding Mixed-Strategy Nash Equilibrium",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jywq7qJLt5": {
    "title": "Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KooDdcx6Hq": {
    "title": "Subgoal-Guided Policy Heuristic Search with Learned Subgoals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sfdFOs68Ia": {
    "title": "Diversifying Policy Behaviors with Extrinsic Behavioral Curiosity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iA8lD3oG5G": {
    "title": "Strengthen Out-of-Distribution Detection Capability with Progressive Self-Knowledge Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F06FPb0Mtu": {
    "title": "Demeaned Sparse: Efficient Anomaly Detection by Residual Estimate",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=erbRHt0XgI": {
    "title": "Offline Model-based Optimization for Real-World Molecular Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ypEW077kle": {
    "title": "Representations Shape Weak-to-Strong Generalization: Theoretical Insights and Empirical Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p411a7WHox": {
    "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dktpDfUTtj": {
    "title": "StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U3RtBk95d1": {
    "title": "Learning Curves of Stochastic Gradient Descent in Kernel Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HHwGfLOKxq": {
    "title": "Scaling Laws for Pre-training Agents and World Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2OEb20dy7B": {
    "title": "Understanding and Improving Length Generalization in Recurrent Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1dd7q3Ktkz": {
    "title": "Towards a Unified Framework of Clustering-based Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aRUUFFycNh": {
    "title": "On The Concurrence of Layer-wise Preconditioning Methods and Provable Feature Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5CnnxNPtuE": {
    "title": "OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AZT4EiONRQ": {
    "title": "Learning Imbalanced Data with Beneficial Label Noise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6hDNXCdTsE": {
    "title": "Earley-Driven Dynamic Pruning for Efficient Structured Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1EgFJDjodU": {
    "title": "Do Not Mimic My Voice : Speaker Identity Unlearning for Zero-Shot Text-to-Speech",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j8Vr3E3vhy": {
    "title": "History-Guided Video Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sTAJ9QyA6l": {
    "title": "ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u8CA3qIS0V": {
    "title": "Sparse Video-Gen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zUk00sasl6": {
    "title": "QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZbWXovStjD": {
    "title": "NegMerge: Sign-Consensual Weight Merging for Machine Unlearning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zV5pkTMHPP": {
    "title": "Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HXOicJsmMQ": {
    "title": "Activation Space Interventions Can Be Transferred Between Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JpoDVFYx2w": {
    "title": "SKIM: Any-bit Quantization Pushing The Limits of Post-Training Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ruvzyT9HqJ": {
    "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ezna4V4zHs": {
    "title": "CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FU527ycGgW": {
    "title": "Learning-Augmented Hierarchical Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pu2aw2zwMx": {
    "title": "Steering Protein Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Anv3KB9lz": {
    "title": "Efficient Online Reinforcement Learning for Diffusion Policy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iZdGZSWe1A": {
    "title": "Task Generalization with Autoregressive Compositional Structure: Can Learning from D Tasks Generalize to D T Tasks?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=70YgwpIxbc": {
    "title": "Relative Error Fair Clustering in the Weak-Strong Oracle Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nCoaJYNCcg": {
    "title": "The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CZTcRSxkfe": {
    "title": "Improved Discretization Complexity Analysis of Consistency Models: Variance Exploding Forward Process and Decay Discretization Scheme",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ybA4EcMmUZ": {
    "title": "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VWjkpro9gv": {
    "title": "Learning Monotonic Probabilities with a Generative Cost Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cuqvlLBQK6": {
    "title": "Sample Efficient Demonstration Selection for In-Context Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s7HUJamWqX": {
    "title": "Agent Reviewers: Domain-specific Multimodal Agents with Shared Memory for Paper Review",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S8kbmk12Oo": {
    "title": "AutoEval Done Right: Using Synthetic Data for Model Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8mEhwcebmz": {
    "title": "Anytime-Constrained Equilibria in Polynomial Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7SG4s8d8AQ": {
    "title": "Decomposition of Graphic Design with Unified Multimodal Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JblpkLmvPg": {
    "title": "Efficient Graph Continual Learning via Lightweight Graph Neural Tangent Kernels-based Dataset Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WZlq625BWD": {
    "title": "Non-Asymptotic Length Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F1BCivEK57": {
    "title": "Ranked from Within: Ranking Large Multimodal Models Without Labels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AzF9xAMrBK": {
    "title": "MA-LoT: Model-Collaboration Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0qRROKHVdZ": {
    "title": "Overcoming Spurious Solutions in Semi-Dual Neural Optimal Transport: A Smoothing Approach for Learning the Optimal Transport Plan",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vSzcsYucCW": {
    "title": "Watch Out Your Album! On the Inadvertent Privacy Memorization in Multi-Modal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XUkJGDfuXu": {
    "title": "EARL-BO: Reinforcement Learning for Multi-Step Lookahead, High-Dimensional Bayesian Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9UxEydnM1N": {
    "title": "Pareto-frontier Entropy Search with Variational Lower Bound Maximization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6EZ3MDDf6p": {
    "title": "DISCO: learning to DISCover an evolution Operator for multi-physics-agnostic prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tAwNFWTXSr": {
    "title": "Deep Unsupervised Hashing via External Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kQ42hGjrKn": {
    "title": "Zero Shot Generalization of Vision-Based RL Without Data Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PSNce503is": {
    "title": "QEM-Bench: Benchmarking Learning-based Quantum Error Mitigation and QEMFormer as a Multi-ranged Context Learning Baseline",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2HJcVtuovs": {
    "title": "Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of Search, RL and Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=44gnGhurnZ": {
    "title": "A Dynamical Systems-Inspired Pruning Strategy for Addressing Oversmoothing in Graph Attention Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VfoKOD65Zq": {
    "title": "LangTime: A Language-Guided Unified Model for Time Series Forecasting with Proximal Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aMXOX8VrNq": {
    "title": "EgoPrivacy: What Your First-Person Camera Says About You?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUWYuwUkqE": {
    "title": "POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pbkwh7QivE": {
    "title": "Random Policy Evaluation Uncovers Policies of Generative Flow Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FeZimuj6SG": {
    "title": "Improving Flow Matching by Aligning Flow Divergence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ef1UHxznNy": {
    "title": "Active Learning with Selective Time-Step Acquisition for PDEs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f6uZasBASY": {
    "title": "Exact Recovery of Sparse Binary Vectors from Generalized Linear Measurements",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OtxLhobhwb": {
    "title": "ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision Transformers under Domain Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jjGT1Fifdb": {
    "title": "HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QRms4lx0Fp": {
    "title": "Evolving Minds: Logic-Informed Inference from Temporal Action Patterns",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D8xx4Gl3MJ": {
    "title": "Distilling the Knowledge in Data Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=28CwvrlpAR": {
    "title": "AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zy7Jw91tdh": {
    "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DF2JV03T6q": {
    "title": "Fixing the Loose Brake: Exponential-Tailed Stopping Time in Best Arm Identification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n7qKt6gjl9": {
    "title": "Identifying Neural Dynamics Using Interventional State Space Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k7vcuqLK4X": {
    "title": "Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dYur3yabMj": {
    "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7UqNM85dD6": {
    "title": "SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nDHEkgVs8V": {
    "title": "Maximum Coverage in Turnstile Streams with Applications to Fingerprinting Measures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6AOnQ0KSUT": {
    "title": "Pruning for GNNs: Lower Complexity with Comparable Expressiveness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9lw5HYPT4Y": {
    "title": "Efficient ANN-SNN Conversion with Error Compensation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yHpWI6a2xT": {
    "title": "Geometric Algebra Planes: Convex Implicit Neural Volumes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CZPOIZqWwd": {
    "title": "ResearchTown: Simulator of Human Research Community",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rcjgzbYvhF": {
    "title": "Highly Compressed Tokenizer Can Generate Without Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rcMeab1QVn": {
    "title": "Expressive Power of Graph Neural Networks for (Mixed-Integer) Quadratic Programs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qrU3yNfX0d": {
    "title": "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WsApckAb2B": {
    "title": "Towards Escaping from Class Dependency Modeling for Multi-Dimensional Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LkgwdSzlb6": {
    "title": "Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DW8oTCk2nF": {
    "title": "Text-to-CAD Generation Through Infusing Visual Feedback in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2xmOEbpYv1": {
    "title": "Mixture of Experts Provably Detect and Learn the Latent Cluster Structure in Gradient-Based Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9rG2oAWj0s": {
    "title": "ProDiff: Prototype-Guided Diffusion for Minimal Information Trajectory Imputation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=94G4eL3RWi": {
    "title": "Self-Consistency Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yO95ALeoGw": {
    "title": "Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TiEsk0mf7r": {
    "title": "TSP: A Two-Sided Smoothed Primal-Dual Method for Nonconvex Bilevel Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R7gCixl2xR": {
    "title": "Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cwpf8S4f5C": {
    "title": "Faster Approximation Algorithms for k-Center via Data Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qE4e9ouzoQ": {
    "title": "Self-cross Feature based Spiking Neural Networks for Efficient Few-shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EuJaF5QsMP": {
    "title": "I 2 MoE : Interpretable Multimodal Interaction-aware Mixture-of-Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J5YGi2LzI7": {
    "title": "Enhancing Decision-Making of Large Language Models via Actor-Critic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eKXicUVKCs": {
    "title": "Binary Hypothesis Testing for Softmax Models and Leverage Score Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ybODpT8ydV": {
    "title": "PatchPilot: A Cost-Efficient Software Engineering Agent with Early Attempts on Formal Verification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s1EImzs5Id": {
    "title": "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zL8O4wWQps": {
    "title": "Contextual Linear Bandits with Delay as Payoff",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WMHNs2Necq": {
    "title": "Dueling Convex Optimization with General Preferences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ab8yOxtKWj": {
    "title": "TabFSBench: Tabular Benchmark for Feature Shifts in Open Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JVouJNs8Vr": {
    "title": "A Parameter-Free and Near-Optimal Zeroth-Order Algorithm for Stochastic Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vDB2oX3Wl3": {
    "title": "Unlocking the Capabilities of Large Vision-Language Models for Generalizable and Explainable Deepfake Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8eQKjsVnN3": {
    "title": "Decision-aware Training of Spatiotemporal Forecasting Models to Select a Top-K Subset of Sites for Intervention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PgjUFjuxH9": {
    "title": "Explanatory Instructions: Towards Unified Vision Tasks Understanding and Zero-shot Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=laUd1q5iWW": {
    "title": "Directed Graph Grammars for Sequence-based Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fDzzshfELg": {
    "title": "MATS: An Audio Language Model under Text-only Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9NrUIaH1sx": {
    "title": "A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EU5lci90fF": {
    "title": "Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pqSLcEbk5X": {
    "title": "Information Bottleneck-guided MLPs for Robust Spatial-temporal Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DZ6iFdVDrx": {
    "title": "The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RyOpooIxDF": {
    "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e5yAhjSJ4j": {
    "title": "The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aoLFIUlyPE": {
    "title": "BCE vs. CE in Deep Feature Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IOVzGtXiNr": {
    "title": "Foundation Molecular Grammar: Multi-Modal Foundation Models Induce Interpretable Molecular Graph Languages",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VhmTXbsdtx": {
    "title": "A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nvf4jFsbv9": {
    "title": "An Efficient Matrix Multiplication Algorithm for Accelerating Inference in Binary and Ternary Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZdmMDz33Io": {
    "title": "You Get What You Give: Reciprocally Fair Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=12oD7HIckz": {
    "title": "Revisiting Differentially Private Algorithms for Decentralized Online Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TXIWOhUTtx": {
    "title": "Distributionally Robust Policy Learning under Concept Drifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R6ORNPrIdv": {
    "title": "Generalizable Multi-Camera 3D Object Detection from a Single Source via Fourier Cross-View Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gdcwm4LLfb": {
    "title": "Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3lsEeqmvpz": {
    "title": "HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NQyqpK6d72": {
    "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching in Diffusion Transformer Acceleration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JrxJUMqqz4": {
    "title": "polybasic Speculative Decoding Through a Theoretical Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eerZAppd4T": {
    "title": "In-Context Reinforcement Learning From Suboptimal Historical Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MzQed6QEmS": {
    "title": "Modalities Contribute Unequally: Enhancing Medical Multi-modal Learning through Adaptive Modality Token Re-balancing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rdeDanrYEj": {
    "title": "MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rn3thNKvzw": {
    "title": "Learning Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KU7jlB2MPN": {
    "title": "An Expressive and Self-Adaptive Dynamical System for Efficient Function Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CDillQjA7N": {
    "title": "From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hFnM9AqT5A": {
    "title": "Contextual Online Decision Making with Infinite-Dimensional Functional Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lvkVhZ776k": {
    "title": "CAN: Leveraging Clients As Navigators for Generative Replay in Federated Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ojuu0ewXC1": {
    "title": "Linear Bandits with Partially Observable Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IRQ0n961nn": {
    "title": "Learngene Tells You How to Customize: Task-Aware Parameter Initialization at Flexible Scales",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0p86Mhg014": {
    "title": "Federated Disentangled Tuning with Textual Prior Decoupling and Visual Dynamic Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5TUa2UXSpp": {
    "title": "Learning Optimal Multimodal Information Bottleneck Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p1UBWkOvZm": {
    "title": "AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rmWpE3FrHW": {
    "title": "Optimizing Temperature for Language Models with Multi-Sample Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7TrOBcxSvy": {
    "title": "SafeArena: Evaluating the Safety of Autonomous Web Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k0hAQhl87M": {
    "title": "Revisiting Instance-Optimal Cluster Recovery in the Labeled Stochastic Block Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6DJEaz1cCj": {
    "title": "SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kPL1cVv0v8": {
    "title": "Controllable Data Generation with Hierarchical Neural Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6UIer20oYA": {
    "title": "Understanding the Forgetting of (Replay-based) Continual Learning via Feature Learning: Angle Matters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OaC01wTE44": {
    "title": "One Image is Worth a Thousand Words: A Usability Preservable Text-Image Collaborative Erasing Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CXJV23a8nm": {
    "title": "Generalization Performance of Ensemble Clustering: From Theory to Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JsmfjEEKqX": {
    "title": "Understanding and Mitigating Memorization in Diffusion Models for Tabular Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WkqZ6Qmmq2": {
    "title": "Towards Understanding Catastrophic Forgetting in Two-layer Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BSwxSqqWfN": {
    "title": "A Model of Place Field Reorganization During Reward Maximization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yPhAbeQxbJ": {
    "title": "Diffusion Sampling Correction via Approximately 10 Parameters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6FwwtwJ89I": {
    "title": "Peripheral Memory for LLMs: Integration of Sequential Memory Banks with Adaptive Querying",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5d6Y7xxRMr": {
    "title": "Learning Extrapolative Sequence Transformations from Markov Chains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w4c5bLkhsz": {
    "title": "Rethinking Confidence Scores and Thresholds in Pseudolabeling-based SSL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GGgnmOlnRY": {
    "title": "Ergodic Generative Flows",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RcJCuma3mo": {
    "title": "Haste Makes Waste: A Simple Approach for Scaling Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iSyB2yYaMx": {
    "title": "On the Training Convergence of Transformers for In-Context Classification of Gaussian Mixtures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EoxpGWgeCH": {
    "title": "Learnable Spatial-Temporal Positional Encoding for Link Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=klw8Ko4ENe": {
    "title": "SPRI: Aligning Large Language Models with Context-Situated Principles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mJAa823xKu": {
    "title": "TTFSFormer: A TTFS-based Lossless Conversion of Spiking Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ck7dvZFbRW": {
    "title": "EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uTz2Utym5n": {
    "title": "FlatQuant: Flatness Matters for LLM Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yqyEUcGreT": {
    "title": "A Stronger Mixture of Low-Rank Experts for Fine-Tuning Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BCKSxOFX85": {
    "title": "Constrain Alignment with Sparse Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9gyJJw8ZUj": {
    "title": "Ab Initio Nonparametric Variable Selection for Scalable Symbolic Regression with Large p",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LUFPNGiCUw": {
    "title": "Oscillation-Reduced MXFP4 Training for Vision Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FB2e8PV6qg": {
    "title": "Graph4MM: Weaving Multimodal Learning with Structural Information",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=38Nh0TebXZ": {
    "title": "Federated Node-Level Clustering Network with Cross-Subgraph Link Mending",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JgbrkAJHDZ": {
    "title": "Adaptive Estimation and Learning under Temporal Distribution Shift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3D16aFxblb": {
    "title": "Optimal Survey Design for Private Mean Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rKi8eyJBoB": {
    "title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A6zDim0rQf": {
    "title": "DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nkV9PPp8R8": {
    "title": "Determinant Estimation under Memory Constraints and Neural Scaling Laws",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t46uezeQH8": {
    "title": "MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3HEyISL2qy": {
    "title": "Design Considerations in Offline Preference-based RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jyxwWQjU4J": {
    "title": "Textual Unlearning Gives a False Sense of Unlearning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FE9QN8d536": {
    "title": "CoDy: Counterfactual Explainers for Dynamic Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nBgQ66iEUu": {
    "title": "Hi-Patch: Hierarchical Patch GNN for Irregular Multivariate Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvzArsKUww": {
    "title": "DTZO: Distributed Trilevel Zeroth Order Learning with Provable Non-Asymptotic Convergence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q4yzASDktN": {
    "title": "Revisiting the Predictability of Performative, Social Events",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzPArjGqrs": {
    "title": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZCgzSqZ7gD": {
    "title": "A Near-Optimal Single-Loop Stochastic Algorithm for Convex Finite-Sum Coupled Compositional Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iNWFA3yOqR": {
    "title": "RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TpP47EH1xG": {
    "title": "LAST SToP for Modeling Asynchronous Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v2nV83Q849": {
    "title": "Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hslOzRxzXL": {
    "title": "Retraining-free Merging of Sparse MoE via Hierarchical Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ag3uveGZCb": {
    "title": "Lightweight-Mark: Rethinking Deep Learning-Based Watermarking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NpIIbrg361": {
    "title": "Refining Adaptive Zeroth-Order Optimization at Ease",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KM7pXWG1xj": {
    "title": "GenMol: A Drug Discovery Generalist with Discrete Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SG8Yx1FyeU": {
    "title": "Simple Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LftsDSnwH8": {
    "title": "Deep Bayesian Filter for Bayes-Faithful Data Assimilation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n3IkEjDq4V": {
    "title": "EasyInv: Toward Fast and Better DDIM Inversion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LJvtuXhcIs": {
    "title": "Learning to Stop: Deep Learning for Mean Field Optimal Stopping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OqutBNEEjz": {
    "title": "Learn Singularly Perturbed Solutions via Homotopy Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q6f9Fg9LHe": {
    "title": "OmniBal: Towards Fast Instruction-Tuning for Vision-Language Models via Omniverse Computation Balance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nKJEAQ6JCY": {
    "title": "Flow Matching for Few-Trial Neural Adaptation with Stable Latent Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9CCJJFiutB": {
    "title": "Nesterov Method for Asynchronous Pipeline Parallel Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=br7fTbnd16": {
    "title": "Efficient Noise Calculation in Deep Learning-based MRI Reconstructions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=323GZNnGqe": {
    "title": "Agent-Centric Actor-Critic for Asynchronous Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a7qFlPOTix": {
    "title": "ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0K4H3TBIIV": {
    "title": "Understanding Nonlinear Implicit Bias via Region Counts in Input Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WhHHQXVBdi": {
    "title": "Tracking The Best Expert Privately",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UlHHz2XYP7": {
    "title": "Towards Black-Box Membership Inference Attack for Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=En0vq4ICkW": {
    "title": "Predicting mutational effects on protein binding from folding energy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5dF4mqVVqK": {
    "title": "De-mark: Watermark Removal in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UCn7dyYa4h": {
    "title": "Super Deep Contrastive Information Bottleneck for Multi-modal Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gTDUSrjQLy": {
    "title": "Learning to Steer Learners in Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n7VLFYNoCb": {
    "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X6nIBckemw": {
    "title": "Learning Safe Control via On-the-Fly Bandit Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=perpuTFEF7": {
    "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6p2wsBeYSs": {
    "title": "CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m4Q3CkuUyV": {
    "title": "An Instrumental Value for Data Production and its Application to Data Pricing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pzbr7MGzEH": {
    "title": "Improving Generalization with Flat Hilbert Bayesian Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K0sv5T2usb": {
    "title": "WeGeFT: Weight‑Generative Fine‑Tuning for Multi‑Faceted Efficient Adaptation of Large Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TfB7AHM2bJ": {
    "title": "Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b1Bae7TKmw": {
    "title": "Splitting & Integrating: Out-of-Distribution Detection via Adversarial Gradient Attribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=32IBDeRFgZ": {
    "title": "Adaptive Exploration for Multi-Reward Multi-Policy Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TJHhXzTcQe": {
    "title": "Retrieval-Augmented Language Model for Knowledge-aware Protein Encoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cngMks94dR": {
    "title": "Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yMJAYbGcCc": {
    "title": "LieRE: Lie Rotational Positional Encodings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8S5rzd08FI": {
    "title": "BILBO: BILevel Bayesian Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hYfOPXrbUr": {
    "title": "Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=flxvawr9N2": {
    "title": "Quadratic Upper Bound for Boosting Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=do5vVfKEXZ": {
    "title": "Towards Global-level Mechanistic Interpretability: A Perspective of Modular Circuits of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sGQEOXlezg": {
    "title": "Contrastive Localized Language-Image Pre-Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MhYnnDBAJ8": {
    "title": "Concurrent Reinforcement Learning with Aggregated States via Randomized Least Squares Value Iteration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9stlHtljkj": {
    "title": "SEAD: Unsupervised Ensemble of Streaming Anomaly Detectors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lTBq5LOUKC": {
    "title": "WOMD-Reasoning: A Large-Scale Dataset for Interaction Reasoning in Driving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AUtO02LArY": {
    "title": "Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LwXXnqM4uP": {
    "title": "An Asymptotically Optimal Approximation Algorithm for Multiobjective Submodular Maximization at Scale",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=koGlzcbXt3": {
    "title": "Explainable Concept Generation through Vision-Language Preference Learning for Understanding Neural Networks' Internal Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jMNQaNbjQl": {
    "title": "Leveraging Offline Data in Linear Latent Contextual Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iTevNo8PzG": {
    "title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gt1MmGaKdZ": {
    "title": "MELON: Provable Defense Against Indirect Prompt Injection Attacks in AI Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ypDMPB0qY2": {
    "title": "Improving the Variance of Differentially Private Randomized Experiments through Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FKqmIAnkrb": {
    "title": "Learn from Downstream and Be Yourself in Multimodal Large Language Models Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x2lm33kdrZ": {
    "title": "Invariance Makes LLM Unlearning Resilient Even to Unanticipated Downstream Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zZjLv6F0Ks": {
    "title": "Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n8fJdB6GMZ": {
    "title": "Adaptive Data Collection for Robust Learning Across Multiple Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A9XePhy4Z3": {
    "title": "Can We Predict Performance of Large Models across Vision-Language Tasks?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V66xc5KxgY": {
    "title": "Regress, Don't Guess: A Regression-like Loss on Number Tokens for Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9JpzL95oYE": {
    "title": "Efficient Optimization with Orthogonality Constraint: a Randomized Riemannian Submanifold Method",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mwZzyF7J3D": {
    "title": "Towards Cost-Effective Reward Guided Text Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NO7egHvBHa": {
    "title": "Learning State-Based Node Representations from a Class Hierarchy for Fine-Grained Open-Set Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KWzddk0Ejx": {
    "title": "Hybrid Quantum-Classical Multi-Agent Pathfinding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LyUfPOvM6I": {
    "title": "OrcaLoca: An LLM Agent Framework for Software Issue Localization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q2pjlx1OeX": {
    "title": "Of Mice and Machines: A Comparison of Learning Between Real World Mice and RL Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3hYrORJndz": {
    "title": "SHARP-Distill: A 68× Faster Recommender System with Hypergraph Neural Networks and Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bDBnd9T2Cz": {
    "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CxnKXLDCZM": {
    "title": "SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=egqOSr5gGw": {
    "title": "Instance-Optimal Pure Exploration for Linear Bandits on Continuous Arms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sdFRCRk1pP": {
    "title": "Breaking Silos: Adaptive Model Fusion Unlocks Better Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j07qnZjYIU": {
    "title": "Unpaired Point Cloud Completion via Unbalanced Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3qL4LRUaJ8": {
    "title": "A First-order Generative Bilevel Optimization Framework for Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fr7kH2SFq7": {
    "title": "Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gujuGnbhZr": {
    "title": "FedECADO: A Dynamical System Model of Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=brn95athTo": {
    "title": "Intersectional Fairness in Reinforcement Learning with Large State and Constraint Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dkcraXnIIL": {
    "title": "Exploiting Curvature in Online Convex Optimization with Delayed Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NDH7AzljFD": {
    "title": "Training Diffusion-based Generative Models with Limited Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lEV0x6aDKc": {
    "title": "Inverse Optimization via Learning Feasible Regions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9buvSnaiMp": {
    "title": "GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzNVZEsqTi": {
    "title": "Fast Exact Unlearning for In-Context Learning Data for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O1scMa6d1d": {
    "title": "SafeMap: Robust HD Map Construction from Incomplete Observations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r4XfIgD77g": {
    "title": "Bridging Protein Sequences and Microscopy Images with Unified Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O37Pg2cm7y": {
    "title": "Differentially Private Space-Efficient Algorithms for Counting Distinct Elements in the Turnstile Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wptlUkP48t": {
    "title": "Mixed-curvature decision trees and random forests",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XFz3EJS8RW": {
    "title": "PIPA: Preference Alignment as Prior-Informed Statistical Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wpaxYGgp2n": {
    "title": "MixMin: Finding Data Mixtures via Convex Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oo6ucNx10s": {
    "title": "Efficient Fine-Grained Guidance for Diffusion Model Based Symbolic Music Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mSCjZg0Ob0": {
    "title": "RISE: Radius of Influence based Subgraph Extraction for 3D Molecular Graph Explanation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CQp36039EM": {
    "title": "Reward-Guided Prompt Evolving in Reinforcement Learning for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ywmoIu5t9i": {
    "title": "From Individual Experience to Collective Evidence: A Reporting-Based Framework for Identifying Systemic Harms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w0QxS5arnp": {
    "title": "Adaptive Sensitivity Analysis for Robust Augmentation against Natural Corruptions in Image Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pxfpGbeVx2": {
    "title": "Geometric Median (GM) Matching for Robust k-Subset Selection from Noisy Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kFB4qBpayS": {
    "title": "Learning With Multi-Group Guarantees For Clusterable Subpopulations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=It1AkQ6xEJ": {
    "title": "AdvI2I: Adversarial Image Attack on Image-to-Image Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QUOwuRtf61": {
    "title": "AutoStep: Locally adaptive involutive MCMC",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vNpq41EXrM": {
    "title": "Doubly Protected Estimation for Survival Outcomes Utilizing External Controls for Randomized Clinical Trials",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DiqeZY27XK": {
    "title": "Knowledge Retention in Continual Model-Based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5KICQlFN4s": {
    "title": "Compelling ReLU Networks to Exhibit Exponentially Many Linear Regions at Initialization and During Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RwHNnO6Jvv": {
    "title": "Adapting to Linear Separable Subsets with Large-Margin in Differentially Private Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aEnkBIhYvO": {
    "title": "Selective Prompt Anchoring for Code Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vfRmp9RPV7": {
    "title": "AKORN: Adaptive Knots generated Online for RegressioN splines",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xTuWXFH8nQ": {
    "title": "Variance-Reduced Forward-Reflected-Backward Splitting Methods for Nonmonotone Generalized Equations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bbJ0QCujU4": {
    "title": "Regularized Langevin Dynamics for Combinatorial Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4ESwnU0kCA": {
    "title": "Sampling Binary Data by Denoising through Score Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CXPhqbWTEo": {
    "title": "Near-Optimal Sample Complexity for MDPs via Anchoring",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kBX0jaABBr": {
    "title": "How to Evaluate and Mitigate IP Infringement in Visual Generative AI?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y6aT5rlrlv": {
    "title": "Gradient Flow Provably Learns Robust Classifiers for Orthonormal GMMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AjpoFnPzLd": {
    "title": "David and Goliath: Small One-step Model Beats Large Diffusion with Score Post-training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MQpAF3Z1YD": {
    "title": "Exact risk curves of signSGD in High-Dimensions: quantifying preconditioning and noise-compression effects",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f5czhqYK3H": {
    "title": "Elucidating Flow Matching ODE Dynamics via Data Geometry and Denoisers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DIZItj8ueN": {
    "title": "PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fpdx6GtAqM": {
    "title": "Near-optimal Regret Using Policy Optimization in Online MDPs with Aggregate Bandit Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QtbyoRxyNx": {
    "title": "Who experiences large model decay and why?\" A Hierarchical Framework for Diagnosing Heterogeneous Performance Drift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JmOCquEAqW": {
    "title": "Compositional Scene Understanding through Inverse Generative Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BiXwdXZxs7": {
    "title": "Representative Language Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IF9VXY4YRl": {
    "title": "Beyond Confidence: Exploiting Homogeneous Pattern for Semi-Supervised Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rL3uxe5a0c": {
    "title": "RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oRT6H6We48": {
    "title": "Data-driven Design of Randomized Control Trials with Guaranteed Treatment Effects",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mLysCaKFg7": {
    "title": "Optimization for Neural Operators can Benefit from Width",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LKQIS65fgd": {
    "title": "Armijo Line-search Can Make (Stochastic) Gradient Descent Provably Faster",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JL4MRb1bKH": {
    "title": "Direct Prediction Set Minimization via Bilevel Conformal Classifier Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RvYO10B5ss": {
    "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cnqyzuZhSo": {
    "title": "The Impact of On-Policy Parallelized Data Collection on Deep Reinforcement Learning Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4uOEiitySn": {
    "title": "A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rrSMo793Wx": {
    "title": "Constant Stepsize Local GD for Logistic Regression: Acceleration by Instability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YwltsnSUFQ": {
    "title": "Scaling Value Iteration Networks to 5000 Layers for Extreme Long-Term Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OI3mAAY8eB": {
    "title": "A Versatile Influence Function for Data Attribution with Non-Decomposable Loss",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YJ1My9ttEN": {
    "title": "Adaptive Flow Matching for Resolving Small-Scale Physics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qBtomgvLwn": {
    "title": "Validating Mechanistic Interpretations: An Axiomatic Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ciUHD7jcT9": {
    "title": "Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c2xhvZA8Xq": {
    "title": "Right Now, Wrong Then: Non-Stationary Direct Preference Optimization under Preference Drift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gt138OTYzY": {
    "title": "Diagonal Symmetrization of Neural Network Solvers for the Many-Electron Schrödinger Equation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rCxB0tVHp1": {
    "title": "Securing Equal Share: A Principled Approach for Learning Multiplayer Symmetric Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzfGuKazvf": {
    "title": "Understanding the difficulties of posterior predictive estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mp8Og8Rpop": {
    "title": "Scaling Sparse Feature Circuits For Studying In-Context Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hC7zCFk5Dp": {
    "title": "NTK-DFL: Enhancing Decentralized Federated Learning in Heterogeneous Settings via Neural Tangent Kernel",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TFXxarWZzv": {
    "title": "Tokenized Bandit for LLM Decoding and Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2dlTi4S4JN": {
    "title": "NMA-tune: Generating Highly Designable and Dynamics Aware Protein Backbones",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BDaMEdNejx": {
    "title": "Learning from others' mistakes: Finetuning machine translation models with span-level error annotations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jSYBqtOJS4": {
    "title": "Black-Box Adversarial Attacks on LLM-Based Code Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O14GjxDAt3": {
    "title": "Accurate Identification of Communication Between Multiple Interacting Neural Populations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hPn8eX3LVv": {
    "title": "Fully Dynamic Embedding into ℓ p Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yxoxw0IUTR": {
    "title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sElAqKsJrQ": {
    "title": "Provably Efficient RL for Linear MDPs under Instantaneous Safety Constraints in Non-Convex Feature Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VBTHduhm4K": {
    "title": "Permutation-based Rank Test in the Presence of Discretization and Application in Causal Discovery with Mixed Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oOAICE3sY7": {
    "title": "Minimax Optimal Regret Bound for Reinforcement Learning with Trajectory Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QY7Au9nZwp": {
    "title": "Star Attention: Efficient LLM Inference over Long Sequences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d2qpEYzyyy": {
    "title": "Grokking at the Edge of Linear Separability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8lt5776GLB": {
    "title": "A Certified Unlearning Approach without Access to Source Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m25T5rAy43": {
    "title": "Learning Multi-Level Features with Matryoshka Sparse Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fEjktWQ5Am": {
    "title": "A Bayesian Model Selection Criterion for Selecting Pretraining Checkpoints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KpWmOJAjr7": {
    "title": "Optimal Transfer Learning for Missing Not-at-Random Matrix Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z2rrB4S3hg": {
    "title": "On the Importance of Gaussianizing Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BQ4KbrUCC2": {
    "title": "Wait-Less Offline Tuning and Re-solving for Online Decision Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PIqtU1SgUR": {
    "title": "Integer Programming for Generalized Causal Bootstrap Designs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MumOAOs9HY": {
    "title": "Learnings from Scaling Visual Tokenizers for Reconstruction and Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5BGC2I2fxx": {
    "title": "Can Transformers Reason Logically? A Study in SAT Solving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lZ4HiOwpBO": {
    "title": "SING: Spatial Context in Large Language Model for Next-Gen Wearables",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GAmmzu6GYS": {
    "title": "Diss-l-ECT: Dissecting Graph Data with Local Euler Characteristic Transforms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rcDYnkG1F8": {
    "title": "Symmetry-Robust 3D Orientation Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q6aopfT2d5": {
    "title": "Finite-Time Convergence Rates in Stochastic Stackelberg Games with Smooth Algorithmic Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=goVzfYtj58": {
    "title": "Exploring Representations and Interventions in Time Series Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B6WalMoQJW": {
    "title": "Enhancing Foundation Models for Time Series Forecasting via Wavelet-based Tokenization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g2tr7nA4pS": {
    "title": "Multivariate Conformal Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oj9hnQpA9M": {
    "title": "On Temperature Scaling and Conformal Prediction of Deep Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QGUju9B68Z": {
    "title": "The Double-Ellipsoid Geometry of CLIP",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uCkwqAG0uS": {
    "title": "Proposer-Agent-Evaluator (PAE): Autonomous Skill Discovery For Foundation Model Internet Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RFCp1QzzHQ": {
    "title": "PISA Experiments: Exploring Physics Post-Training for Video Diffusion Models by Watching Stuff Drop",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bJnSplWSCL": {
    "title": "Ladder-Residual: Parallelism-Aware Architecture for Accelerating Large Model Inference with Communication Overlapping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LRIkWWxUZI": {
    "title": "Context-Informed Neural ODEs Unexpectedly Identify Broken Symmetries: Insights from the Poincaré–Hopf Theorem",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z7lApwS4nV": {
    "title": "CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mM65b81LdM": {
    "title": "Boosting Masked ECG-Text Auto-Encoders as Discriminative Learners",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OLodUbcWjB": {
    "title": "Demystifying Long Chain-of-Thought Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lIdcR7vU76": {
    "title": "LGDM: Latent Guidance in Diffusion Models for Perceptual Evaluations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s55Af9Emyq": {
    "title": "Robust Conformal Outlier Detection under Contaminated Reference Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p71VhJHAtq": {
    "title": "Reflection-Window Decoding: Text Generation with Selective Refinement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BSqf2k01ag": {
    "title": "Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IdRrKDsTZ8": {
    "title": "Stochastic Online Conformal Prediction with Semi-Bandit Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4lfmCCYKDY": {
    "title": "Learning Likelihood-Free Reference Priors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6GFPnVHEKB": {
    "title": "Understanding Complexity in VideoQA via Visual Program Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qg9p1I5lmp": {
    "title": "Stream-level Flow Matching with Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZJzZt7NmLM": {
    "title": "Improved Sample Complexity for Private Nonsmooth Nonconvex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=23zxLtvder": {
    "title": "SPD: Sync-Point Drop for Efficient Tensor Parallelism of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mkqcUWBykZ": {
    "title": "Beyond Topological Self-Explainable GNNs: A Formal Explainability Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4tYckHNVXV": {
    "title": "Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f6lio2CZIM": {
    "title": "QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1jutKQ5R8T": {
    "title": "Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tuYmhlu62K": {
    "title": "Does learning the right latent variables necessarily improve in-context learning?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z9BaaWDE0r": {
    "title": "ETTA: Elucidating the Design Space of Text-to-Audio Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x2xZidAK4J": {
    "title": "Competitively Consistent Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZfX43ZZRZR": {
    "title": "Learning to Keep a Promise: Scaling Language Model Decoding Parallelism with Learned Asynchronous Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQYJMq39gI": {
    "title": "MiraGe: Editable 2D Images using Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sTc83mG2H9": {
    "title": "A Multi-Region Brain Model to Elucidate the Role of Hippocampus in Spatially Embedded Decision-Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hInfvt7c4p": {
    "title": "InfAlign: Inference-aware language model alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hUHRTaTfvZ": {
    "title": "Learning Vision and Language Concepts for Controllable Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YC6ItZfdVk": {
    "title": "Improving Compositional Generation with Diffusion Models Using Lift Scores",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t0ZDijoZbD": {
    "title": "In-Context Learning and Occam's Razor",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATqGm1WyDj": {
    "title": "Memory Layers at Scale",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y861SAkeCQ": {
    "title": "Gandalf the Red: Adaptive Security for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o877aFqlvK": {
    "title": "Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for Neural Similarity? E-prop as an Example",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w16xG4A7W4": {
    "title": "MTL-UE: Learning to Learn Nothing for Multi-Task Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kuIwMEHXMT": {
    "title": "LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wCBuHDe7Ud": {
    "title": "Optimizing Noise Distributions for Differential Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lvGmEDrIN9": {
    "title": "UltraTWD: Optimizing Ultrametric Trees for Tree-Wasserstein Distance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PvkO6rIixC": {
    "title": "G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0VSDl40xMv": {
    "title": "DOLPHIN: A Programmable Framework for Scalable Neurosymbolic Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QyG0ilz5ju": {
    "title": "GEFA: A General Feature Attribution Framework Using Proxy Gradient Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aPgRQIXmdE": {
    "title": "Differentiable Quadratic Optimization For the Maximum Independent Set Problem",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XAckVo0iNj": {
    "title": "Shielded Diffusion: Generating Novel and Diverse Images using Sparse Repellency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0REM9ydeLZ": {
    "title": "Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u3U8qzFV7w": {
    "title": "Theoretical guarantees on the best-of-n alignment policy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y0zXGw0GUk": {
    "title": "LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g5CijB2ERy": {
    "title": "Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MHiTGWDbIb": {
    "title": "IT 3 : Idempotent Test-Time Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cdjfN2kNtB": {
    "title": "Improving Soft Unification with Knowledge Graph Embedding Methods",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=28Essvtvkw": {
    "title": "SITCOM: Step-wise Triple-Consistent Diffusion Sampling For Inverse Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3WLpiPeJbk": {
    "title": "Generalized additive models via direct optimization of regularized decision stump forests",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tlLkY9E2bZ": {
    "title": "Behavioral Exploration: Learning to Explore via In-Context Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z6SrgYCdey": {
    "title": "Thinking LLMs: General Instruction Following with Thought Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJeFULIiot": {
    "title": "LLMs can see and hear without any training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=axN83wi3NH": {
    "title": "Compositional Risk Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EemtbhJOXc": {
    "title": "Automatically Interpreting Millions of Features in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b90EKQbL7B": {
    "title": "Learning Survival Distributions with the Asymmetric Laplace Distribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CbEp1ubnQy": {
    "title": "Sample Complexity of Branch-length Estimation by Maximum Likelihood",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bivU8fTTDo": {
    "title": "What Do Learning Dynamics Reveal About Generalization in LLM Mathematical Reasoning?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iQQ2zuWhFM": {
    "title": "Understanding Generalization in Quantum Machine Learning with Margins",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GJKe8WYHxq": {
    "title": "Best of Both Worlds: Advantages of Hybrid Graph Sequence Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rYiSEOEE0p": {
    "title": "Towards the Efficient Inference by Incorporating Automated Computational Phenotypes under Covariate Shift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qbwR36zHsB": {
    "title": "MemFreezing: A Novel Adversarial Attack on Temporal Graph Neural Networks under Limited Future Knowledge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S5njonQdBf": {
    "title": "TopoTune: A Framework for Generalized Combinatorial Complex Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Li4rieeClO": {
    "title": "Self-Bootstrapping for Versatile Test-Time Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VSIjdKPGp8": {
    "title": "R3DM: Enabling Role Discovery and Diversity Through Dynamics Models in Multi-agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mUDnPzopZF": {
    "title": "Proto Successor Measure: Representing the Behavior Space of an RL Agent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XGJ33p4qwt": {
    "title": "From Uncertain to Safe: Conformal Adaptation of Diffusion Models for Safe PDE Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Etc912C6AR": {
    "title": "Reinforcement Learning for Quantum Control under Physical Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gMf347JT3R": {
    "title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=urbvnjSGbE": {
    "title": "Global-Local Dirichlet Processes for Clustering Grouped Data in the Presence of Group-Specific Idiosyncratic Variables",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NGC7wdMFao": {
    "title": "Self-Organizing Visual Prototypes for Non-Parametric Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y7V6yqSZl2": {
    "title": "Breaking Barriers: Combinatorial Algorithms for Non-Monotone Submodular Maximization with Sublinear Adaptivity and 1 / e Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=flKhxGTBj2": {
    "title": "Are Large Language Models Ready for Multi-Turn Tabular Data Analysis?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZD3VMCvxvM": {
    "title": "Prediction via Shapley Value Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fXCfT7ErvL": {
    "title": "Towards a Formal Theory of Representational Compositionality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gscscNNiPN": {
    "title": "Balancing the Scales: A Theoretical and Algorithmic Framework for Learning from Imbalanced Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tDomx6S4F5": {
    "title": "Action-Dependent Optimality-Preserving Reward Shaping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VdwdU81Uzy": {
    "title": "DiffusionVLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J2JxJ0P1LI": {
    "title": "A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Cxk63lTTm": {
    "title": "A Manifold Perspective on the Statistical Generalization of Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2KlxjR6lsd": {
    "title": "Mastering Multiple-Expert Routing: Realizable H -Consistency and Strong Guarantees for Learning to Defer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WeMpvGxXMn": {
    "title": "Beyond CVaR: Leveraging Static Spectral Risk Measures for Enhanced Decision-Making in Distributional Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SvrLHOF0V5": {
    "title": "Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pTSWi6RTtJ": {
    "title": "Is Noise Conditioning Necessary for Denoising Generative Models?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1XXqIIsgT3": {
    "title": "Reinforcement Learning with Segment Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5t2TWcPCvS": {
    "title": "Learn to Vaccinate: Combining Structure Learning and Effective Vaccination for Epidemic and Outbreak Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hSCxEZLvxI": {
    "title": "Discovering Spoofing Attempts on Language Model Watermarks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mMwuRYIPFm": {
    "title": "Boosting Adversarial Robustness with CLAT: Criticality Leveraged Adversarial Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Rvs8jluQP": {
    "title": "Efficient Multi-modal Long Context Learning for Training-free Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NzoZXju2bL": {
    "title": "GRAIL: Graph Edit Distance and Node Alignment using LLM-Generated Code",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v8ipdsBPEx": {
    "title": "Latent Variable Estimation in Bayesian Black-Litterman Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kAWtGZIHzm": {
    "title": "Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SibkcjNnsC": {
    "title": "Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D5RNACOZEI": {
    "title": "DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YngQelHz1X": {
    "title": "Principled Algorithms for Optimizing Generalized Metrics in Binary Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sHvImzN9pL": {
    "title": "Update Your Transformer to the Latest Release: Re-Basin of Task Vectors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Ofb0cGXb5": {
    "title": "Targeted Unlearning with Single Layer Unlearning Gradient",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ckcXwR8cQS": {
    "title": "On the Alignment between Fairness and Accuracy: from the Perspective of Adversarial Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kPTW6hGrJy": {
    "title": "Simple and Critical Iterative Denoising: A Recasting of Discrete Diffusion in Graph Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wgRGArdaKo": {
    "title": "Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u8kFBce69J": {
    "title": "Neural Genetic Search in Discrete Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wW1gRFTqhx": {
    "title": "Automatic Differentiation of Optimization Algorithms with Time-Varying Updates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Em2oaXd8Dc": {
    "title": "HashAttention: Semantic Sparsity for Faster Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y62fhuA69I": {
    "title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2XvOJvUlKc": {
    "title": "Scaffold with Stochastic Gradients: New Analysis with Linear Speed-Up",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C0xvseVIPU": {
    "title": "Revisiting Noise Resilience Strategies in Gesture Recognition: Short-Term Enhancement in sEMG Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0TY5lhhdZm": {
    "title": "Improved Off-policy Reinforcement Learning in Biological Sequence Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cBukWQKWvQ": {
    "title": "Learning Policy Committees for Effective Personalization in MDPs with Diverse Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qIzYWidWZH": {
    "title": "INRFlow: Flow Matching for INRs in Ambient Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zm0Kper4yx": {
    "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A5l37HL8vX": {
    "title": "Test-Time Adaptation with Binary Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=biJiSMLGOV": {
    "title": "Discrete Markov Probabilistic Models: An Improved Discrete Score-Based Framework with sharp convergence bounds under minimal assumptions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GUDnecJdJU": {
    "title": "Retrieval Augmented Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GHyvvWu1XC": {
    "title": "Trajectory Inference with Smooth Schrödinger Bridges",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Eog0kXX7hW": {
    "title": "On-Device Collaborative Language Modeling via a Mixture of Generalists and Specialists",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jNGTAyaRNY": {
    "title": "Two Tickets are Better than One: Fair and Accurate Hiring Under Strategic LLM Manipulations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yUxVZBYaQA": {
    "title": "Inverse Reinforcement Learning with Switching Rewards and History Dependency for Characterizing Animal Behaviors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IH8OwjOGzM": {
    "title": "Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b9hVMJi0t2": {
    "title": "AssistanceZero: Scalably Solving Assistance Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7QH48TtFZX": {
    "title": "SeedLoRA: A Fusion Approach to Efficient LLM Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qyMxunrR2j": {
    "title": "Flow of Reasoning: Training LLMs for Divergent Reasoning with Minimal Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lvR39kEqpZ": {
    "title": "On the Clean Generalization and Robust Overfitting in Adversarial Training from Two Theoretical Views: Representation Complexity and Training Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IQ4V1yRCJv": {
    "title": "FlipAttack: Jailbreak LLMs via Flipping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Myx2kJFzAn": {
    "title": "VinePPO: Refining Credit Assignment in RL Training of LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6TDSDdgP7Z": {
    "title": "SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7DXaCYUvDN": {
    "title": "Weak-to-Strong Jailbreaking on Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kJ5i29FejW": {
    "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d7v7RVXbNH": {
    "title": "Multi-Timescale Dynamics Model Bayesian Optimization for Plasma Stabilization in Tokamaks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fUvuEfZSEE": {
    "title": "Textural or Textual: How Vision-Language Models Read Text in Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qAarsvflTa": {
    "title": "Simplicity Bias and Optimization Threshold in Two-Layer ReLU Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xnEv5pq4cB": {
    "title": "Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pJdMOKqdSV": {
    "title": "Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TJfBwirYTH": {
    "title": "ROS: A GNN-based Relax-Optimize-and-Sample Framework for Max- k -Cut Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lvrn4vnNdd": {
    "title": "Faster Rates for Private Adversarial Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UA8DESerfC": {
    "title": "Preference learning made easy: Everything should be understood through win rate",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JpbqiD7n9r": {
    "title": "Inference-Time Alignment of Diffusion Models with Direct Noise Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SZCdoPvpls": {
    "title": "Average Certified Radius is a Poor Metric for Randomized Smoothing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lm9DXFrcHD": {
    "title": "Hyperband-based Bayesian Optimization for Black-box Prompt Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GkVirCa8wA": {
    "title": "Deep Streaming View Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gedP1zQzX4": {
    "title": "QuanONet: Quantum Neural Operator with Application to Differential Equation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B0fdg3vqqW": {
    "title": "CTBench: A Library and Benchmark for Certified Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GQ6epWZqdS": {
    "title": "Prediction-Aware Learning in Multi-Agent Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk8NjQ4yhT": {
    "title": "Does Graph Prompt Work? A Data Operation Perspective with Theoretical Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C8pGYyfhoF": {
    "title": "Devil is in the Details: Density Guidance for Detail-Aware Generation with Flow Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XCBYIfu9Fs": {
    "title": "Speculate, then Collaborate: Fusing Knowledge of Language Models during Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k77bq8AJVy": {
    "title": "Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hgM387MPm1": {
    "title": "Secant Line Search for Frank-Wolfe Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VjI1NnsW4t": {
    "title": "PF3plat: Pose-Free Feed-Forward 3D Gaussian Splatting for Novel View Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=51l8tvuIxo": {
    "title": "ToMA: Token Merge with Attention for Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZwcMZ443BF": {
    "title": "Unraveling the Interplay between Carryover Effects and Reward Autocorrelations in Switchback Experiments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c5R0lKLIr3": {
    "title": "Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yQBZZeWPdQ": {
    "title": "Importance Corrected Neural JKO Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qkeYxpB9w0": {
    "title": "Neurosymbolic World Models for Sequential Decision Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kOttDCDYJp": {
    "title": "Generalization in Federated Learning: A Conditional Mutual Information Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bwfiFv8KG2": {
    "title": "Tilted Sharpness-Aware Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XStA4AX4dK": {
    "title": "When Dynamic Data Selection Meets Data Augmentation: Achieving Enhanced Training Acceleration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Z27aGSgJ4": {
    "title": "Graph Inverse Style Transfer for Counterfactual Explainability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yTsbZQTsAu": {
    "title": "Adversarial Inception Backdoor Attacks against Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CY9MlORQs5": {
    "title": "Rethinking Aleatoric and Epistemic Uncertainty",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S0V9MX9jKe": {
    "title": "Stable Offline Value Function Learning with Bisimulation-based Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zLHn677EMS": {
    "title": "Alberta Wells Dataset: Pinpointing Oil and Gas Wells from Satellite Imagery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DYeVXcPsN6": {
    "title": "BARK: A Fully Bayesian Tree Kernel for Black-box Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gHzx2apaYD": {
    "title": "Pre-Training Graph Contrastive Masked Autoencoders are Strong Distillers for EEG",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oYzDcCZdR9": {
    "title": "Unifying Specialized Visual Encoders for Video Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hHYjiOJFum": {
    "title": "CFPT: Empowering Time Series Forecasting through Cross-Frequency Interaction and Periodic-Aware Timestamp Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eRdXrPJFLM": {
    "title": "Differentially Private Boxplots",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OpLJ5wE1aX": {
    "title": "Towards Understanding Gradient Dynamics of the Sliced-Wasserstein Distance via Critical Point Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rGOl3duXnm": {
    "title": "Lightspeed Geometric Dataset Distance via Sliced Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bPJVWvyII5": {
    "title": "In-Context Deep Learning via Transformer Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dZAQxNFKGg": {
    "title": "Interaction-Aware Gaussian Weighting for Clustered Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OJnoRkfq2Q": {
    "title": "Fundamental limits of learning in sequence multi-index models and deep attention networks: high-dimensional asymptotics and sharp thresholds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L6U7nYc4ah": {
    "title": "Provably Improving Generalization of Few-shot models with Synthetic Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kRmfzTfIGe": {
    "title": "FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GGlqxnfGjl": {
    "title": "A Variational Information Theoretic Approach to Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7vdg7VICsX": {
    "title": "Continuous-Time Analysis of Heavy Ball Momentum in Min-Max Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bSrRCmx10g": {
    "title": "InfoSEM: A Deep Generative Model with Informative Priors for Gene Regulatory Network Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fuE1UMH99F": {
    "title": "HEAP: Hyper Extended A-PDHG Operator for Constrained High-dim PDEs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sRKtbGsebH": {
    "title": "Mind the Gap: a Spectral Analysis of Rank Collapse and Signal Propagation in Attention Layers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=51SFypI0J8": {
    "title": "Enhancing Diversity In Parallel Agents: A Maximum State Entropy Exploration Story",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pkKQGJ5d99": {
    "title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mxNEWzqXFH": {
    "title": "Improving the Statistical Efficiency of Cross-Conformal Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fgO1R1iVEi": {
    "title": "Calibrated Value-Aware Model Learning with Probabilistic Environment Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lWcM04ExOD": {
    "title": "Learning to Match Unpaired Data with Minimum Entropy Coupling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6XQOarhYF8": {
    "title": "Multinoulli Extension: A Lossless Yet Effective Probabilistic Framework for Subset Selection over Partition Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H9vSxIhsuh": {
    "title": "False Coverage Proportion Control for Conformal Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yOXoJpG6Qy": {
    "title": "Robust Autonomy Emerges from Self-Play",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CmVApdsdVx": {
    "title": "Spurious Correlations in High Dimensional Regression: The Roles of Regularization, Simplicity Bias and Over-Parameterization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mWKCajTUUu": {
    "title": "Inverse Problem Sampling in Latent Space Using Sequential Monte Carlo",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GKly3FkxN4": {
    "title": "Shortcut-connected Expert Parallelism for Accelerating Mixture of Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D1gs8QT74m": {
    "title": "MultiPDENet: PDE-embedded Learning with Multi-time-stepping for Accelerated Flow Simulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d8xnwqslqq": {
    "title": "Online Episodic Convex Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gpizm0I3lp": {
    "title": "The underlying structures of self-attention: symmetry, directionality, and emergent dynamics in Transformer training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RqtRSrCbNu": {
    "title": "Temperature-Annealed Boltzmann Generators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Tj02fPeYH": {
    "title": "MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jdBWtGZnYd": {
    "title": "Provably Efficient Algorithm for Best Scoring Rule Identification in Online Principal-Agent Information Acquisition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4zRb89SbzG": {
    "title": "Embedding Safety into RL: A New Take on Trust Region Methods",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pm8LUCx6Mb": {
    "title": "PROTOCOL: Partial Optimal Transport-enhanced Contrastive Learning for Imbalanced Multi-view Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OKDN1Hg3im": {
    "title": "Physics-informed Temporal Alignment for Auto-regressive PDE Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6srcNB5kCC": {
    "title": "Flex3D: Feed-Forward 3D Generation with Flexible Reconstruction Model and Input View Curation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Daf4TMtX9": {
    "title": "On Understanding Attention-Based In-Context Learning for Categorical Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7dlzDUaOsY": {
    "title": "Confidence Difference Reflects Various Supervised Signals in Confidence-Difference Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hvaPL2DMpQ": {
    "title": "PRIME: Deep Imbalanced Regression with Proxies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LV3DpKD08B": {
    "title": "xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6vP20U55bA": {
    "title": "Stochastic Layer-Wise Shuffle for Improving Vision Mamba Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b74kufhhsk": {
    "title": "DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image Editing in Diffusion Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1Agoo9f41i": {
    "title": "Navigating the Social Welfare Frontier: Portfolios for Multi-objective Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yFF7hy8Mbh": {
    "title": "A Non-isotropic Time Series Diffusion Model with Moving Average Transitions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pD5oklKrDV": {
    "title": "Improved Expressivity of Hypergraph Neural Networks through High-Dimensional Generalized Weisfeiler-Leman Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dlIoumNiXt": {
    "title": "General agents need world models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EZLGupYjlb": {
    "title": "Discrete and Continuous Difference of Submodular Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gZ5N3TLjwv": {
    "title": "Time-Aware World Model for Adaptive Prediction and Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=njZ5oVPObS": {
    "title": "One Wave To Explain Them All: A Unifying Perspective On Feature Attribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XKuTFM93mG": {
    "title": "K 2 IE: Kernel Method-based Kernel Intensity Estimators for Inhomogeneous Poisson Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PqJFVbJAMR": {
    "title": "One-Shot Heterogeneous Federated Learning with Local Model-Guided Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j4IELrBhoG": {
    "title": "Gradient Inversion of Multimodal Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JKzS7zJRep": {
    "title": "Bi-perspective Splitting Defense: Achieving Clean-Seed-Free Backdoor Security",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kl7SbPfBsB": {
    "title": "B-score: Detecting biases in large language models using response history",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d8GDXPcoW8": {
    "title": "BAnG: Bidirectional Anchored Generation for Conditional RNA Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pWtgTbiJTO": {
    "title": "Compact Matrix Quantum Group Equivariant Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HGnMNUTdUz": {
    "title": "Leveraging Model Guidance to Extract Training Data from Personalized Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YIO9ritzWV": {
    "title": "LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets and Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R0WHut6jWo": {
    "title": "Adversarial Inputs for Linear Algebra Backends",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q2s4DLsegO": {
    "title": "DiffAdvMAP: Flexible Diffusion-Based Framework for Generating Natural Unrestricted Adversarial Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lyUJH51URt": {
    "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=soLNj4l2EL": {
    "title": "Calibrated Language Models and How to Find Them with Label Smoothing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mEEiiUXQ7O": {
    "title": "BOOD: Boundary-based Out-Of-Distribution Data Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oTCiv1bkjG": {
    "title": "FedBEns: One-Shot Federated Learning based on Bayesian Ensemble",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tTVYR82Iz6": {
    "title": "Predictive Data Selection: The Data That Predicts Is the Data That Teaches",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NCZOxhBTrz": {
    "title": "Reasoning Limitations of Multimodal Large Language Models. A case study of Bongard Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=djVlAgkRFO": {
    "title": "Neural Event-Triggered Control with Optimal Scheduling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oMkmNTBb0l": {
    "title": "Learning Safe Strategies for Value Maximizing Buyers in Uniform Price Auctions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V364YjXm3T": {
    "title": "MODULI: Unlocking Preference Generalization via Diffusion Models for Offline Multi-Objective Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D2cDJzotb8": {
    "title": "Adjustment for Confounding using Pre-Trained Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j7H4mbeOI1": {
    "title": "SecEmb: Sparsity-Aware Secure Federated Learning of On-Device Recommender System with Large Embedding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dewZTXKwli": {
    "title": "Bayesian Basis Function Approximation for Scalable Gaussian Process Priors in Deep Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AVeskAAETB": {
    "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IyVcxU0RKI": {
    "title": "MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nyDBxn5PFQ": {
    "title": "Sample Complexity of Correlation Detection in the Gaussian Wigner Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jwMjzGpzi4": {
    "title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zha2m39ZoM": {
    "title": "Come Together, But Not Right Now: A Progressive Strategy to Boost Low-Rank Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5MQQsenQBm": {
    "title": "Interpreting CLIP with Hierarchical Sparse Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f2inwmDR4g": {
    "title": "On Volume Minimization in Conformal Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wvc6d6926j": {
    "title": "Generalization and Robustness of the Tilted Empirical Risk",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NNWSNy4YB4": {
    "title": "Policy Guided Tree Search for Enhanced LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i5aHAkkhJH": {
    "title": "FloE: On-the-Fly MoE Inference on Memory-constrained GPU",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UWhW5YYLo6": {
    "title": "EQ-VAE: Equivariance Regularized Latent Space for Improved Generative Image Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TO6jrwuxi4": {
    "title": "Scaling Large Motion Models with Million-Level Human Motions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jnJLZXSOin": {
    "title": "Parametric Scaling Law of Tuning Bias in Conformal Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Odvol3BbvC": {
    "title": "Reinforced Learning Explicit Circuit Representations for Quantum State Characterization from Local Measurements",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iUsHLRTp3t": {
    "title": "Optimal Decision Tree Pruning Revisited: Algorithms and Complexity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ghkWIlliZ8": {
    "title": "BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yIL0WEiGNo": {
    "title": "On the Impact of Performative Risk Minimization for Binary Random Variables",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wdy227OCCr": {
    "title": "Broadband Ground Motion Synthesis by Diffusion Model with Minimal Condition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ctDkRyNXrm": {
    "title": "Faster Stochastic Optimization with Arbitrary Delays via Adaptive Asynchronous Mini-Batching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RbsfEuYNTA": {
    "title": "Learning Compact Semantic Information for Incomplete Multi-View Missing Multi-Label Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9LqXn0Izwk": {
    "title": "Kernel Quantile Embeddings and Associated Probability Metrics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tI04pBsIbq": {
    "title": "Reinforcement Learning with Adaptive Reward Modeling for Expensive-to-Evaluate Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NEfIaE4BI5": {
    "title": "Scalable Private Partition Selection via Adaptive Weighting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i6uxIAAMje": {
    "title": "Human-Aligned Image Models Improve Visual Decoding from the Brain",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bFB0N8ABIr": {
    "title": "Multi-Session Budget Optimization for Forward Auction-based Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lWvj3jifuO": {
    "title": "Tensor Decomposition Based Memory-Efficient Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ADQZAfcUC": {
    "title": "Balanced Learning for Domain Adaptive Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8NN0ZEBuQk": {
    "title": "CodeSync: Synchronizing Large Language Models with Dynamic Code Evolution at Scale",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hsoyRfvMGu": {
    "title": "Q-Supervised Contrastive Representation: A State Decoupling Framework for Safe Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LAIwq7aWNv": {
    "title": "On the Diversity of Adversarial Ensemble Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pwr2LznsQc": {
    "title": "M2PDE: Compositional Generative Multiphysics and Multi-component PDE Simulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p1KkW2kgDp": {
    "title": "Patch-wise Structural Loss for Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PsSJrGDOJR": {
    "title": "Human Cognition-Inspired Hierarchical Fuzzy Learning Machine",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uEUIeIrRPv": {
    "title": "When to Forget? Complexity Trade-offs in Machine Unlearning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nshtqLv4r4": {
    "title": "TINED: GNNs-to-MLPs by Teacher Injection and Dirichlet Energy Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZkybWzxvw9": {
    "title": "MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uWAGlqVkAv": {
    "title": "Balancing Interference and Correlation in Spatial Experimental Designs: A Causal Graph Cut Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3BaJMRaPSx": {
    "title": "PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6EZMWeV5sH": {
    "title": "Test-Time Selective Adaptation for Uni-Modal Distribution Shift in Multi-Modal Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Td2JZVLitV": {
    "title": "Exploiting Similarity for Computation and Communication-Efficient Decentralized Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I9DSsCBwG0": {
    "title": "DiMa: Understanding the Hardness of Online Matching Problems via Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SFAcjdJdXz": {
    "title": "Selective Response Strategies for GenAI",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7aAGBuexh9": {
    "title": "FAB-PPI: Frequentist, Assisted by Bayes, Prediction-Powered Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lHzLxYiJVF": {
    "title": "Best of Both Worlds: Regret Minimization versus Minimax Play",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qA3xHJzF6B": {
    "title": "A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SgIg3cZjuN": {
    "title": "Differential Privacy Under Class Imbalance: Methods and Empirical Insights",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gn2izAiYzZ": {
    "title": "Density Ratio Estimation with Conditional Probability Paths",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bpyh6H9Xr1": {
    "title": "ADDQ: Adaptive distributional double Q-learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dmZQrojdVU": {
    "title": "Online Differentially Private Conformal Prediction for Uncertainty Quantification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kzoNjAtwby": {
    "title": "A Reductions Approach to Risk-Sensitive Reinforcement Learning with Optimized Certainty Equivalents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkwXYSDKso": {
    "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4kF2ZZcePc": {
    "title": "FedClean: A General Robust Label Noise Correction for Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R0R3MRD8vh": {
    "title": "Editable Concept Bottleneck Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NCYBdRCpw1": {
    "title": "DA-KD: Difficulty-Aware Knowledge Distillation for Efficient Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XOUpHJPYRX": {
    "title": "SOLD: Slot Object-Centric Latent Dynamics Models for Relational Manipulation Learning from Pixels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8tQdwSCJmA": {
    "title": "All-atom inverse protein folding through discrete flow matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pSnBPLYOBZ": {
    "title": "Effective and Efficient Masked Image Generation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nGjw3UxRf2": {
    "title": "TextCenGen: Attention-Guided Text-Centric Background Adaptation for Text-to-Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kkSvncjUOq": {
    "title": "LIMEFLDL: A Local Interpretable Model-Agnostic Explanations Approach for Label Distribution Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=STEhUnCmdm": {
    "title": "Rectifying Conformity Scores for Better Conditional Coverage",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KUN7A7Okb6": {
    "title": "UniMoMo: Unified Generative Modeling of 3D Molecules for De Novo Binder Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gtVW8CO6Wf": {
    "title": "Learning from Sample Stability for Deep Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RmZZ4AeNsl": {
    "title": "Almost Optimal Fully Dynamic k -Center Clustering with Recourse",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f375uEmYDf": {
    "title": "ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eYtgs9k75o": {
    "title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JAWKe4vg0l": {
    "title": "EvoControl: Multi-Frequency Bi-Level Control for High-Frequency Continuous Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ooAub9jwPF": {
    "title": "Concentration Distribution Learning from Label Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V2bw5SmpF6": {
    "title": "BSO: Binary Spiking Online Optimization Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jh0Fss7LA0": {
    "title": "Enabling Optimal Decisions in Rehearsal Learning under CARE Condition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jy5Lz5xNUy": {
    "title": "Learning Changes in Graphon Attachment Network Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YVU391OrrK": {
    "title": "Transformer-Based Spatial-Temporal Counterfactual Outcomes Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hOxq91N1Sb": {
    "title": "Optimal Algorithm for Max-Min Fair Bandit",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qkhgzNiEdj": {
    "title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FvBYG5jA7k": {
    "title": "Identifying and Understanding Cross-Class Features in Adversarial Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvIwwGYTLc": {
    "title": "The Price of Freedom: Exploring Expressivity and Runtime Tradeoffs in Equivariant Tensor Products",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nF8NxPUd0q": {
    "title": "GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zgeoOFyIyb": {
    "title": "Enhancing Parallelism in Decentralized Stochastic Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OMwdvGDeHL": {
    "title": "Learning In-context n -grams with Transformers: Sub- n -grams Are Near-Stationary Points",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nofe0rzyhY": {
    "title": "Sparse Spectral Training and Inference on Euclidean and Hyperbolic Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5OLRHkzTYk": {
    "title": "Pivoting Factorization: A Compact Meta Low-Rank Representation of Sparsity for Efficient Inference in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=saqVVehEOM": {
    "title": "Conformal Prediction with Cellwise Outliers: A Detect-then-Impute Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9uBY2PBfcf": {
    "title": "Robust Consensus Anchor Learning for Efficient Multi-view Subspace Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f21sRSRb1E": {
    "title": "Global curvature for second-order optimization of neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wGq6WpfhFl": {
    "title": "Clipped SGD Algorithms for Performative Prediction: Tight Bounds for Stochastic Bias and Remedies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Doi0G4UNgt": {
    "title": "Towards an Explainable Comparison and Alignment of Feature Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eJzZryJfri": {
    "title": "PAC-Bayes Analysis for Recalibration in Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=auc4Ng6h4R": {
    "title": "On Zero-Initialized Attention: Optimal Prompt and Gating Factor Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=29bt3kWj2J": {
    "title": "Polynomial Time Learning Augmented Algorithms for NP-hard Permutation Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k9gfog88GK": {
    "title": "On Efficient Estimation of Distributional Treatment Effects under Covariate-Adaptive Randomization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zWskCdu3QA": {
    "title": "Text-to-LoRA: Instant Transformer Adaption",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ue1ptN4kSS": {
    "title": "High-Dimensional Tensor Regression With Oracle Properties",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tStRKJKZEI": {
    "title": "Double-Filter: Efficient Fine-tuning of Pre-trained Vision-Language Models via Patch&Layer Filtering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AdWRA8faAO": {
    "title": "3D-LMVIC: Learning-based Multi-View Image Compression with 3D Gaussian Geometric Priors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B5IrPGOl9p": {
    "title": "The Global Convergence Time of Stochastic Gradient Descent in Non-Convex Landscapes: Sharp Estimates via Large Deviations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j9pVnmulQm": {
    "title": "WorldSimBench: Towards Video Generation Models as World Simulators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vC9GJr3m83": {
    "title": "Understanding the Unfairness in Network Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZRGZ4OcfXV": {
    "title": "Don't Restart, Just Reuse: Reoptimizing MILPs with Dynamic Parameters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ihUi76a4u7": {
    "title": "How to Synthesize Text Data without Model Collapse?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=INg866tEaT": {
    "title": "Unifews: You Need Fewer Operations for Efficient Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LBBUJkqkOM": {
    "title": "A Novel Characterization of the Population Area Under the Risk Coverage Curve (AURC) and Rates of Finite Sample Estimators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1b6NNpFYI4": {
    "title": "La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rzzy69QL6Q": {
    "title": "Dual Feature Reduction for the Sparse-group Lasso and its Adaptive Variant",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vcNJgiEGdz": {
    "title": "Recommendations with Sparse Comparison Data: Provably Fast Convergence for Nonconvex Matrix Factorization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xyVevDUsb6": {
    "title": "ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KOGHdQlLcv": {
    "title": "Towards Understanding Parametric Generalized Category Discovery on Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V6fBMFduGS": {
    "title": "Reaction Graph: Towards Reaction-Level Modeling for Chemical Reactions with 3D Structures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NxxHkScf8z": {
    "title": "When Model Knowledge meets Diffusion Model: Diffusion-assisted Data-free Image Synthesis with Alignment of Domain and Class",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YqtgKdW9dD": {
    "title": "Catching Two Birds with One Stone: Reward Shaping with Dual Random Networks for Balancing Exploration and Exploitation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ss5JNmJDkW": {
    "title": "Distributed Parallel Gradient Stacking(DPGS): Solving Whole Slide Image Stacking Challenge in Multi-Instance Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dtmTQawQN2": {
    "title": "Modularized Self-Reflected Video Reasoner for Multimodal LLM with Application to Video Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h0ZeiDRN8A": {
    "title": "A Square Peg in a Square Hole: Meta-Expert for Long-Tailed Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6J9tJKK4YI": {
    "title": "Towards a General Time Series Forecasting Model with Unified Representation and Adaptive Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EIK6xxIoCB": {
    "title": "Core Knowledge Deficits in Multi-Modal Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zDZkyvZqFR": {
    "title": "Beyond One-Hot Labels: Semantic Mixing for Model Calibration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uvU29AfoNT": {
    "title": "Few-Shot Learner Generalizes Across AI-Generated Image Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Wk0L4vjhV": {
    "title": "Graph Attention is Not Always Beneficial: A Theoretical Analysis of Graph Attention Mechanisms via Contextual Stochastic Block Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CuASYs6XZW": {
    "title": "Geometric Feature Embedding for Effective 3D Few-Shot Class Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vVEBtDDSA6": {
    "title": "LOGO --- Long cOntext aliGnment via efficient preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=73FyDmYsdn": {
    "title": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B0hE61qHMp": {
    "title": "Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HZKCXym5cS": {
    "title": "Efficient Parallel Training Methods for Spiking Neural Networks with Constant Time Complexity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UKX0k1cWfL": {
    "title": "Can DBNNs Robust to Environmental Noise for Resource-constrained Scenarios?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jIci8mGVeh": {
    "title": "BoxLM: Unifying Structures and Semantics of Medical Concepts for Diagnosis Prediction in Healthcare",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bvrsrvo0Mt": {
    "title": "Vision-Language Model Selection and Reuse for Downstream Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G6DmP9wxeB": {
    "title": "LRA-QViT: Integrating Low-Rank Approximation and Quantization for Robust and Efficient Vision Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sVNNlzjZVN": {
    "title": "SNS-Bench: Defining, Building, and Assessing Capabilities of Large Language Models in Social Networking Services",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2GJkMGMACH": {
    "title": "Supervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=msPfUNX9NF": {
    "title": "Enhancing Logits Distillation with Plug&Play Kendall's τ Ranking Loss",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IHAnkPkoiX": {
    "title": "Kandinsky Conformal Prediction: Beyond Class- and Covariate-Conditional Coverage",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oZM5g4IvmS": {
    "title": "LoRA-Gen: Specializing Large Language Model via Online LoRA Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9cOtYlD5UA": {
    "title": "TCP-Diffusion: A Multi-modal Diffusion Model for Global Tropical Cyclone Precipitation Forecasting with Change Awareness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aTjEIAAYmg": {
    "title": "Unveiling AI's Blind Spots: An Oracle for In-Domain, Out-of-Domain, and Adversarial Errors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MPhyCn8BcU": {
    "title": "Retrieval Augmented Zero-Shot Enzyme Generation for Specified Substrate",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3H7qAT9Qow": {
    "title": "Improving LLM Video Understanding with 16 Frames Per Second",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m7Mx14cxv8": {
    "title": "GaussMarker: Robust Dual-Domain Watermark for Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WuP4hwvLzo": {
    "title": "Right Time to Learn: Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=80mK2Mqaph": {
    "title": "One Arrow, Two Hawks: Sharpness-aware Minimization for Federated Learning via Global Model Trajectory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tVtgnSCTTS": {
    "title": "Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W0GrWqqTJo": {
    "title": "Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6BHa4AJVSM": {
    "title": "Approximately Correct Label Distribution Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i2N7gCWcQG": {
    "title": "Divide and Conquer: Learning Label Distribution with Subtasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6pNp7to8kW": {
    "title": "Cross-Modal Alignment via Variational Copula Modelling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SGrJ8a9a5U": {
    "title": "Fast Large Language Model Collaborative Decoding via Speculation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fMAihjfJij": {
    "title": "Primphormer: Efficient Graph Transformers with Primal Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZSXHilmAgR": {
    "title": "Low-Dimension-to-High-Dimension Generalization and Its Implications for Length Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m74x7brnd6": {
    "title": "FicGCN: Unveiling the Homomorphic Encryption Efficiency from Irregular Graph Convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=29Leye951l": {
    "title": "Commute Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BiM4ewH9zD": {
    "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F7BOaYmWl7": {
    "title": "How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FyiTRWgBR3": {
    "title": "Stabilizing Sample Similarity in Representation via Mitigating Random Consistency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ezp2elh9Yk": {
    "title": "Benchmarking Quantum Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=owEhpoKBKC": {
    "title": "Reward-free World Models for Online Imitation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jSoNlHD9qA": {
    "title": "Rethinking the Bias of Foundation Model under Long-tailed Distribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pYcn9ilTSf": {
    "title": "Generalized Category Discovery via Reciprocal Learning and Class-Wise Distribution Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R07oAGxwhG": {
    "title": "RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PlihOwfx4r": {
    "title": "Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7v2loOdcLH": {
    "title": "A Lens into Interpretable Transformer Mistakes via Semantic Dependency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0p04srg7uf": {
    "title": "Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2FGpL5Nd4C": {
    "title": "Improving Reward Model Generalization from Adversarial Process Enhanced Preferences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q6ba0bZfpl": {
    "title": "Robust Spatio-Temporal Centralized Interaction for OOD Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bHCio4Pg7e": {
    "title": "Generalizing Causal Effects from Randomized Controlled Trials to Target Populations across Diverse Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yx0bl5OFvc": {
    "title": "Parrot: Multilingual Visual Instruction Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6kGTxbn4Qf": {
    "title": "Beyond Minimax Rates in Group Distributionally Robust Optimization via a Novel Notion of Sparsity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZZxVwVUYg3": {
    "title": "CABS: Conflict-Aware and Balanced Sparsification for Enhancing Model Merging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=09WRNhHYkM": {
    "title": "MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schrödinger Bridges",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rab1LXJKLp": {
    "title": "Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j2oXVBJ6YZ": {
    "title": "One-Pass Feature Evolvable Learning with Theoretical Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gM6aboVgTO": {
    "title": "Visual Generation Without Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GCkhEPE1FG": {
    "title": "Adversaries Can Misuse Combinations of Safe Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BiyGtuM20U": {
    "title": "OpenworldAUC: Towards Unified Evaluation and Optimization for Open-world Prompt Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dIjMswSzgF": {
    "title": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dikIlmW4f8": {
    "title": "Identifiable Object Representations under Spatial Ambiguities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tcMWVgYf8f": {
    "title": "RuleAdapter: Dynamic Rules for training Safety Reward Models in RLHF",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zuQROb7YBK": {
    "title": "A Mathematical Framework for AI-Human Integration in Work",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BKnssDRh7d": {
    "title": "A Selective Learning Method for Temporal Graph Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ckbo9yutGO": {
    "title": "NeuroTree: Hierarchical Functional Brain Pathway Decoding for Mental Health Disorders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v3niP1j6Rg": {
    "title": "Knowledge-Guided Wasserstein Distributionally Robust Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uvj6XcSJ5d": {
    "title": "BoA: Attention-aware Post-training Quantization without Backpropagation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v0Ui6SJ1d4": {
    "title": "AGAV-Rater: Adapting Large Multimodal Model for AI-Generated Audio-Visual Quality Assessment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=So6DMbeAak": {
    "title": "Off-Policy Evaluation under Nonignorable Missing Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TLR036ADaA": {
    "title": "Homophily Enhanced Graph Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9CX7oxgjJl": {
    "title": "SERENA: A Unified Stochastic Recursive Variance Reduced Gradient Framework for Riemannian Non-Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iuDS9qDzJG": {
    "title": "Meta Optimality for Demographic Parity Constrained Regression via Post-Processing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vPRWeQjLtG": {
    "title": "Sampling from Binary Quadratic Distributions via Stochastic Localization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gdE3HbHkIL": {
    "title": "GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QwxjjaGBUo": {
    "title": "Beyond Entropy: Region Confidence Proxy for Wild Test-Time Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=huu5JErrT1": {
    "title": "Low-Rank Tensor Transitions (LoRT) for Transferable Tensor Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ntCAP6tMoX": {
    "title": "Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ino4CdB6w": {
    "title": "NBDI: A Simple and Effective Termination Condition for Skill Extraction from Task-Agnostic Demonstrations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=biKDr4cNMQ": {
    "title": "Runtime Analysis of Evolutionary NAS for Multiclass Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UjLxG9k4B6": {
    "title": "Improving Out-of-Distribution Detection via Dynamic Covariance Calibration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JjhnSLb2wO": {
    "title": "Reconstructing Cell Lineage Trees from Phenotypic Features with Metric Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DVjkling5x": {
    "title": "Robust Multi-bit Text Watermark with LLM-based Paraphrasers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fFBnsYRsPg": {
    "title": "Breaking the Curse of Multiagency in Robust Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UWWNxyIT1h": {
    "title": "Self-Consuming Generative Models with Adversarially Curated Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D7qRwx6BOS": {
    "title": "Pareto Merging: Multi-Objective Optimization for Preference-Aware Model Merging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fY7KjywLK1": {
    "title": "Importance Sampling for Nonlinear Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EGpueKe6TP": {
    "title": "Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HjVhSL76GM": {
    "title": "WMarkGPT: Watermarked Image Understanding via Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wd9KPQCKwq": {
    "title": "Beyond Atoms: Enhancing Molecular Pretrained Representations with 3D Space Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1NsWgKCM3Q": {
    "title": "E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zpGK1bOlHt": {
    "title": "Dynamic Mixture of Curriculum LoRA Experts for Continual Multimodal Instruction Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FRjRuSWF3e": {
    "title": "Grokking Beyond the Euclidean Norm of Model Parameters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fmlol78Qqf": {
    "title": "BSemiFL: Semi-supervised Federated Learning via a Bayesian Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dpbvg1mFBq": {
    "title": "RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OwYVyOPrVU": {
    "title": "Staged and Physics-Grounded Learning Framework with Hyperintensity Prior for Pre-Contrast MRI Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1jUXprrfcb": {
    "title": "Reinforced Lifelong Editing for Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bxPRPLPyqT": {
    "title": "A Meta-learner for Heterogeneous Effects in Difference-in-Differences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LVqWNYhPXR": {
    "title": "Generalized Smooth Bilevel Optimization with Nonconvex Lower-Level",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YufVk7I6Ii": {
    "title": "On the Emergence of Position Bias in Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=37NsgZkyWH": {
    "title": "On the Statistical Mechanisms of Distributional Compositional Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uzVShD7wHO": {
    "title": "Scaling Probabilistic Circuits via Monarch Matrices",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cN9GkuBo4w": {
    "title": "An Augmentation-Aware Theory for Self-Supervised Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=55ysNwbOTI": {
    "title": "Efficient Personalized Adaptation for Physiological Signal Foundation Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8forr1FkvC": {
    "title": "Compute Optimal Inference and Provable Amortisation Gap in Sparse Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NkVCB1Cpgl": {
    "title": "Provable and Practical Online Learning Rate Adaptation with Hypergradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yoaErYlGE9": {
    "title": "Generative Point Cloud Registration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V5HEX6stS2": {
    "title": "Score as Action: Fine Tuning Diffusion Generative Models by Continuous-time Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rNfzT8YkgO": {
    "title": "Are Sparse Autoencoders Useful? A Case Study in Sparse Probing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ThnPFhGm8": {
    "title": "Free Process Rewards without Process Labels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F4HByzTu69": {
    "title": "Eigen Analysis of Conjugate Kernel and Neural Tangent Kernel",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ojF1rXmUdy": {
    "title": "DMOSpeech: Direct Metric Optimization via Distilled Diffusion Model in Zero-Shot Speech Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l9FVZ7NXmm": {
    "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zvZTIXkzDe": {
    "title": "DAMA: Data- and Model-aware Alignment of Multi-modal LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvILcv2v8L": {
    "title": "DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=846O8wcn8K": {
    "title": "Disentangled Graph Spectral Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C3gAIlPhjV": {
    "title": "Arrow: Accelerator for Time Series Causal Discovery with Time Weaving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vF5F7cL76i": {
    "title": "unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2JRrmzPQSc": {
    "title": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JqLKV0L5hM": {
    "title": "SHE: Streaming-media Hashing Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bKkCyPzD5P": {
    "title": "Beyond Message Passing: Neural Graph Pattern Machine",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HT0PSgajyN": {
    "title": "CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IajCvMJw41": {
    "title": "Improving Transformer World Models for Data-Efficient RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZU9QV0aT7o": {
    "title": "Volume-Aware Distance for Robust Similarity Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VmXgkCmLFD": {
    "title": "Counterfactual Contrastive Learning with Normalizing Flows for Robust Treatment Effect Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UOw6Qt0qYU": {
    "title": "RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=60q5yK7yjW": {
    "title": "EduLLM: Leveraging Large Language Models and Framelet-Based Signed Hypergraph Neural Networks for Student Performance Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1bnH3Q3zL6": {
    "title": "Gradient Aligned Regression via Pairwise Losses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oktw116wt2": {
    "title": "POQD: Performance-Oriented Query Decomposer for Multi-vector retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lAjj22UxZy": {
    "title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pZbv8pEkfe": {
    "title": "ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=06UlFIly8J": {
    "title": "MindCustomer: Multi-Context Image Generation Blended with Brain Signal",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P0zvNhHGG9": {
    "title": "Diversified Flow Matching with Translation Identifiability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FRFuvBRueA": {
    "title": "When Maximum Entropy Misleads Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EE8Z8Se5Te": {
    "title": "Sidechain conditioning and modeling for full-atom protein sequence design with FAMPNN",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yWDvVl9Wtp": {
    "title": "MTSTRec: Multimodal Time-Aligned Shared Token Recommender",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qdKzBrYhiu": {
    "title": "Telling Peer Direct Effects from Indirect Effects in Observational Network Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iYmV2xRSNW": {
    "title": "FrameBridge: Improving Image-to-Video Generation with Bridge Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MRD19y4Zjm": {
    "title": "Risk-Sensitive Theory of Mind: Coordinating with Agents of Unknown Bias using Cumulative Prospect Theory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FzvKazljRc": {
    "title": "Cross-regularization: Adaptive Model Complexity through Validation Gradients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kWyov6XrXs": {
    "title": "Poly2Vec: Polymorphic Fourier-Based Encoding of Geospatial Objects for GeoAI Applications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rRQ8hvILhb": {
    "title": "Bayesian Weight Enhancement with Steady-State Adaptation for Test-time Adaptation in Dynamic Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r9Yp0v8toA": {
    "title": "Approximate Forest Completion and Learning-Augmented Algorithms for Metric Minimum Spanning Trees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5RYSqSKz9b": {
    "title": "TimeStacker: A Novel Framework with Multilevel Observation for Capturing Nonstationary Patterns in Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZLC4B9oQWX": {
    "title": "Flow-field inference from neural data using deep recurrent networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LiXbEXRH2L": {
    "title": "Discovering Global False Negatives On the Fly for Self-supervised Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2hEDcA7xy4": {
    "title": "Improving the Diffusability of Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wgGC1N4rKy": {
    "title": "Rethinking Addressing in Language Models via Contextualized Equivariant Positional Encoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jaCD2nEpyr": {
    "title": "EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QJPl0DWajD": {
    "title": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eCMycCrQp6": {
    "title": "Unified Analysis of Continuous Weak Features Learning with Applications to Learning from Missing Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kcE0TdWKji": {
    "title": "A Unified Framework for Generalization Error Analysis of Learning with Arbitrary Discrete Weak Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZeetWz8zbG": {
    "title": "Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4JbQK1qGpA": {
    "title": "Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3xvnJS8H1v": {
    "title": "Prediction-Powered Adaptive Shrinkage Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=145So0OrGC": {
    "title": "Optimal Task Order for Continual Learning of Multiple Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3xdFvqsVnM": {
    "title": "MF-LAL: Drug Compound Generation Using Multi-Fidelity Latent Space Active Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fm1K8tMlaf": {
    "title": "Revisiting Unbiased Implicit Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HQEPgICjBS": {
    "title": "Positive-unlabeled AUC Maximization under Covariate Shift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wslytZ28Ms": {
    "title": "Scaling Inference-Efficient Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3K6BkFZ7ka": {
    "title": "Leveraging Randomness in Model and Data Partitioning for Privacy Amplification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SwMjcC5owE": {
    "title": "Tightening Causal Bounds via Covariate-Aware Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GmqZ3WvkeV": {
    "title": "AuPair: Golden Example Pairs for Code Repair",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PfyyEeVzQW": {
    "title": "De-coupled NeuroGF for Shortest Path Distance Approximations on Large Terrain Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yzNX6Tf3EF": {
    "title": "SGD Jittering: A Training Strategy for Robust and Accurate Model-Based Architectures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wqrqcc8O2v": {
    "title": "Diffusion Counterfactual Generation with Semantic Abduction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xpm298DHxn": {
    "title": "Arbitrarily-Conditioned Multi-Functional Diffusion for Multi-Physics Emulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TamwbDvbdX": {
    "title": "A Physics-Augmented Deep Learning Framework for Classifying Single Molecule Force Spectroscopy Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d1xbZ8X6O5": {
    "title": "Linear Q -Learning Does Not Diverge in L 2 : Convergence Rates to a Bounded Set",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5nvaGPhSFo": {
    "title": "Differentially Private Analysis for Binary Response Models: Optimality, Estimation, and Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U7c1DqiPTl": {
    "title": "Sleeping Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NHTqYrp5YL": {
    "title": "MissScore: High-Order Score Estimation in the Presence of Missing Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TXcifVbFpG": {
    "title": "RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SEj9uopOWP": {
    "title": "Deep Linear Network Training Dynamics from Random Initialization: Data, Width, Depth, and Hyperparameter Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5cDc71jLc1": {
    "title": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tXkOUS3vLS": {
    "title": "Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OUr945QeUb": {
    "title": "Collaborative Mean Estimation Among Heterogeneous Strategic Agents: Individual Rationality, Fairness, and Truthful Contribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nxv7d5GjcY": {
    "title": "Bridging Fairness and Efficiency in Conformal Inference: A Surrogate-Assisted Group-Clustered Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Yw6yDZ4RW": {
    "title": "Statistical and Computational Guarantees of Kernel Max-Sliced Wasserstein Distances",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oEvbe7vtOm": {
    "title": "Empirical Privacy Variance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9bYOqwtAud": {
    "title": "Copilot Arena: A Platform for Code LLM Evaluation in the Wild",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mFMGldHdOo": {
    "title": "Reliable Algorithm Selection for Machine Learning-Guided Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zkxe5vASi8": {
    "title": "PROXSPARSE: REGULARIZED LEARNING OF SEMI-STRUCTURED SPARSITY MASKS FOR PRETRAINED LLMS",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AAgzsnhc28": {
    "title": "Diffusion Adversarial Post-Training for One-Step Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4GZwFPzLgW": {
    "title": "Contextures: Representations from Contexts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mXEdUcLtaK": {
    "title": "How Expressive are Knowledge Graph Foundation Models?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xPzwJznH9S": {
    "title": "Unconstrained Robust Online Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tuKwODJ08b": {
    "title": "FactTest: Factuality Testing in Large Language Models with Finite-Sample and Distribution-Free Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FHzkr8FRie": {
    "title": "Identifying Metric Structures of Deep Latent Variable Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ppSGIfpzq": {
    "title": "Accelerating Unbiased LLM Evaluation via Synthetic Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yfoi5O68rf": {
    "title": "TUMTraf VideoQA: Dataset and Benchmark for Unified Spatio-Temporal Video Understanding in Traffic Scenes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WMIueIRcAm": {
    "title": "Computing Optimal Transport Maps and Wasserstein Barycenters Using Conditional Normalizing Flows",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vBYRG3p21R": {
    "title": "Improved Lower Bounds for First-order Stochastic Non-convex Optimization under Markov Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2wUQttiab3": {
    "title": "WGFormer: An SE(3)-Transformer Driven by Wasserstein Gradient Flows for Molecular Ground-State Conformation Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LxYGqJuPnl": {
    "title": "Function Encoders: A Principled Approach to Transfer Learning in Hilbert Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UHF0km7R5M": {
    "title": "OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v14jD1ankb": {
    "title": "Quantum Optimization via Gradient-Based Hamiltonian Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ri1cs3vtXX": {
    "title": "Capturing Temporal Dynamics in Large-Scale Canopy Tree Height Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VPHFKbhJIJ": {
    "title": "Symmetry-Driven Discovery of Dynamical Variables in Molecular Simulations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BGDSSSJWot": {
    "title": "DyPolySeg: Taylor Series-Inspired Dynamic Polynomial Fitting Network for Few-shot Point Cloud Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XROl6J4wjX": {
    "title": "Consensus Is All You Get: The Role of Attention in Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9v1eW8HgMU": {
    "title": "Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept Extraction in Large Vision Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BUhYurycps": {
    "title": "Topological Signatures of Adversaries in Multimodal Alignments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mz4J6GRZso": {
    "title": "Implicit Riemannian Optimism with Applications to Min-Max Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ih2WuBT1Fn": {
    "title": "Context is Key: A Benchmark for Forecasting with Essential Textual Information",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U7eMoRDIGi": {
    "title": "Scalable Equilibrium Sampling with Sequential Boltzmann Generators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XKeCy8dUgm": {
    "title": "Fast Tensor Completion via Approximate Richardson Iteration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xd3J3QJg0b": {
    "title": "Speeding up Policy Simulation in Supply Chain RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qr4a4uS82y": {
    "title": "Policy Design for Two-sided Platforms with Participation Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AxqgpcL90a": {
    "title": "Explicit Exploration for High-Welfare Equilibria in Game-Theoretic Multiagent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=44WiPvzDbu": {
    "title": "Rank-One Modified Value Iteration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o4L9y4Jetm": {
    "title": "SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oOtdWiLb1e": {
    "title": "Training Flexible Models of Genetic Variant Effects from Functional Annotations using Accelerated Linear Algebra",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eU8vAuMlpH": {
    "title": "QPRL : Learning Optimal Policies with Quasi-Potential Functions for Asymmetric Traversal",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AVVXX0erKT": {
    "title": "DexScale: Automating Data Scaling for Sim2Real Generalizable Robot Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Of3wZhVv1R": {
    "title": "EnIGMA: Interactive Tools Substantially Assist LM Agents in Finding Security Vulnerabilities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AhumZOmTDf": {
    "title": "Near-Optimal Consistency-Robustness Trade-Offs for Learning-Augmented Online Knapsack Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fbRbrtSvKm": {
    "title": "Linear Contextual Bandits With Interference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fRoRsmPHUM": {
    "title": "LEMoN: Label Error Detection using Multimodal Neighbors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WVth3Webet": {
    "title": "Interpreting the Repeated Token Phenomenon in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DqlN1yFyxN": {
    "title": "Distributionally Robust Multi-Agent Reinforcement Learning for Dynamic Chute Mapping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i9jUoxSM7Q": {
    "title": "Deep Electromagnetic Structure Design Under Limited Evaluation Budgets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jf8MPTOpLc": {
    "title": "Disparate Conditional Prediction in Multiclass Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MaOYl3P84E": {
    "title": "Supercharging Graph Transformers with Advective Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ya2ksKuNMh": {
    "title": "Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XAZKPGUcQm": {
    "title": "BAME: Block-Aware Mask Evolution for Efficient N:M Sparse Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iAkg2nVmvN": {
    "title": "Low-Rank Thinning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z6RsbHAJk5": {
    "title": "Flexible Tails for Normalizing Flows",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rbI5mOUA8Z": {
    "title": "Focus On This, Not That! Steering LLMs with Adaptive Feature Specification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cnfogmxymj": {
    "title": "EARTH: Epidemiology-Aware Neural ODE with Continuous Disease Transmission Graph",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oyoiHf51es": {
    "title": "FSTLLM: Spatio-Temporal LLM for Few Shot Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9R47dBSG3x": {
    "title": "Cut out and Replay: A Simple yet Versatile Strategy for Multi-Label Online Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H9CDAY3DPW": {
    "title": "Mixture of Hidden-Dimensions: Not All Hidden-States' Dimensions are Needed in Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RQwexjUCxm": {
    "title": "From Debate to Equilibrium: Belief‑Driven Multi‑Agent LLM Reasoning via Bayesian Nash Equilibrium",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N2Dey442PJ": {
    "title": "Rethinking Latent Redundancy in Behavior Cloning: An Information Bottleneck Approach for Robot Manipulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UKHlXpiFMy": {
    "title": "Probably Approximately Global Robustness Certification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q9DAI0AYv1": {
    "title": "NExtLong: Toward Effective Long-Context Training without Long Documents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tnxONP8zTE": {
    "title": "T1: Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xF5PuTLPbn": {
    "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4RdzeucFmW": {
    "title": "GraphGPT: Generative Pre-trained Graph Eulerian Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nqQcAhXGSy": {
    "title": "Online Learning with Unknown Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8EUR45XguK": {
    "title": "HetSSNet: Spatial-Spectral Heterogeneous Graph Learning Network for Panchromatic and Multispectral Images Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fkrEgiR165": {
    "title": "Double Machine Learning for Causal Inference under Shared-State Interference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RdiEl7LF7a": {
    "title": "Bivariate Causal Discovery with Proxy Variables: Integral Solving and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gy3l3r2JP9": {
    "title": "Unisoma: A Unified Transformer-based Solver for Multi-Solid Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0pUSBi1BjC": {
    "title": "Enhancing Graph Invariant Learning from a Negative Inference Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wFBBh8bcoC": {
    "title": "GrokFormer: Graph Fourier Kolmogorov-Arnold Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vWYLQ0VPJx": {
    "title": "Curvature-aware Graph Attention for PDEs on Manifolds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hEaiFYEx3a": {
    "title": "Data-Driven Selection of Instrumental Variables for Additive Nonlinear, Constant Effects Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ty9OcrL86v": {
    "title": "PieClam: A Universal Graph Autoencoder Based on Overlapping Inclusive and Exclusive Communities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rvZv7sDPV9": {
    "title": "Generalized Interpolating Discrete Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hE0uIz07cv": {
    "title": "R.I.P.: Better Models by Survival of the Fittest Prompts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kwHvs1UdTM": {
    "title": "On the Role of Label Noise in the Feature Learning Process",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9rg8Tl9KB3": {
    "title": "Phase and Amplitude-aware Prompting for Enhancing Adversarial Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RuMfpz8bTw": {
    "title": "Fast Inference with Kronecker-Sparse Matrices",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1Iny6XlON0": {
    "title": "Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YtQCoUtWQ9": {
    "title": "Incorporating Arbitrary Matrix Group Equivariance into KANs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kzY6nbbBYq": {
    "title": "Explicit Discovery of Nonlinear Symmetries from Dynamic Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3KVHR1b9UZ": {
    "title": "Learning without Isolation: Pathway Protection for Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3XMA8RDJu2": {
    "title": "Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2v91xhNdsz": {
    "title": "I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DaOdkXgLvE": {
    "title": "RobustZero: Enhancing MuZero Reinforcement Learning Robustness to State Perturbations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pV7hSmGJXP": {
    "title": "On the Convergence of Continuous Single-timescale Actor-critic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oah1fwo6Wv": {
    "title": "Efficient Curvature-Aware Hypergradient Approximation for Bilevel Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=45EIiFd6Oa": {
    "title": "Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fUjkoGUre0": {
    "title": "From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=umWKZCfzQs": {
    "title": "Non-asymptotic Error Bounds in W 2 -Distance with Sqrt(d) Dimension Dependence and First Order Convergence for Langevin Monte Carlo beyond Log-Concavity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y2FyXF5hRe": {
    "title": "Efficient Time Series Processing for Transformers and State-Space Models through Token Merging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JRMoeNZgNl": {
    "title": "Trustworthy Machine Learning through Data-Specific Indistinguishability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kyUoOI0KNJ": {
    "title": "A Generic Family of Graphical Models: Diversity, Efficiency, and Heterogeneity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KRk0WTII0I": {
    "title": "AnalogGenie-Lite: Enhancing Scalability and Precision in Circuit Topology Discovery through Lightweight Graph Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bLng6Z10Vx": {
    "title": "Function-to-Style Guidance of LLMs for Code Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NTAhi2JEEE": {
    "title": "Agent Workflow Memory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ny0m8YEUzH": {
    "title": "Splitting with Importance-aware Updating for Heterogeneous Federated Learning with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wazV91p0oU": {
    "title": "Bayesian Active Learning for Bivariate Causal Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ThK6o74QLc": {
    "title": "Adapting Precomputed Features for Efficient Graph Condensation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zbFiEmkFNP": {
    "title": "Transfer Learning for Nonparametric Contextual Dynamic Pricing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yqcKvMW1sL": {
    "title": "Learning to Quantize for Training Vector-Quantized Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=583klsIjNx": {
    "title": "ELITE: Enhanced Language-Image Toxicity Evaluation for Safety",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aq3YxKPZBk": {
    "title": "Discriminative Policy Optimization for Token-Level Reward Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7mxDGiF01U": {
    "title": "Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y4BDcJmb8t": {
    "title": "Latent Mamba Operator for Partial Differential Equations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P9gwpJDo8i": {
    "title": "PlaySlot: Learning Inverse Latent Dynamics for Controllable Object-Centric Video Prediction and Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fzmtDDOcJ3": {
    "title": "WildChat-50M: A Deep Dive Into the Role of Synthetic Data in Post-Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vhTPfOdwyQ": {
    "title": "How to Train Your Multi-Exit Model? Analyzing the Impact of Training Strategies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SENVTfjHPr": {
    "title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=70DGIxEHiB": {
    "title": "Joint MoE Scaling Laws: Mixture of Experts Can Be Memory Efficient",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3C1s1aEICC": {
    "title": "LLM Data Selection and Utilization via Dynamic Bi-level Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8bgaOg1TlZ": {
    "title": "EffiCoder: Enhancing Code Generation in Large Language Models through Efficiency-Aware Fine-tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PAjCdkkNaU": {
    "title": "From Complex to Atomic: Enhancing Augmented Generation via Knowledge-Aware Dual Rewriting and Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EFLPHl5RGJ": {
    "title": "Differentially Private Federated k -Means Clustering with Server-Side Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wex0vL4c2Y": {
    "title": "DocVXQA: Context-Aware Visual Explanations for Document Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xAGDSXAJgf": {
    "title": "On the Adversarial Robustness of Multi-Kernel Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8mkVTiU1jG": {
    "title": "GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=axaoljXRQd": {
    "title": "Geometric Resampling in Nearly Linear Time for Follow-the-Perturbed-Leader with Best-of-Both-Worlds Guarantee in Bandit Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sKYdVKE1tS": {
    "title": "Are High-Quality AI-Generated Images More Difficult for Models to Detect?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1R84O2Xr1k": {
    "title": "Learning to Reuse Policies in State Evolvable Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WYlerYGDPL": {
    "title": "Approximating Latent Manifolds in Neural Networks via Vanishing Ideals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cerbjnKoGt": {
    "title": "Balancing Efficiency and Expressiveness: Subgraph GNNs with Walk-Based Centrality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y7pDvbi9xz": {
    "title": "FedPHA: Federated Prompt Learning for Heterogeneous Client Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZH7YgIZ3DF": {
    "title": "Can Classic GNNs Be Strong Baselines for Graph-level Tasks? Simple Architectures Meet Excellence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fUCPq5RvmH": {
    "title": "To Steer or Not to Steer? Mechanistic Error Reduction with Abstention for Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ryn8tYWHL": {
    "title": "Conservative Offline Goal-Conditioned Implicit V-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vhktpw6Vid": {
    "title": "LLM-Assisted Semantically Diverse Teammate Generation for Efficient Multi-agent Coordination",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zm53HtGiXN": {
    "title": "PARM: Multi-Objective Test-Time Alignment via Preference-Aware Autoregressive Reward Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MgNeJO0PcF": {
    "title": "Projection Pursuit Density Ratio Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xv1jY6U0pT": {
    "title": "Test-time Adapted Reinforcement Learning with Action Entropy Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gqZO3eSZRy": {
    "title": "Galileo: Learning Global & Local Features of Many Remote Sensing Modalities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QsCDgFKErb": {
    "title": "Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ae5qnQxAxQ": {
    "title": "Multi-View Graph Clustering via Node-Guided Contrastive Encoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g2hucDbOJt": {
    "title": "Enhancing Graph Contrastive Learning for Protein Graphs from Perspective of Invariance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zVJGILkCRZ": {
    "title": "ENAHPool: The Edge-Node Attention-based Hierarchical Pooling for Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u2jH71U9T6": {
    "title": "LADA: Scalable Label-Specific CLIP Adapter for Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZBBo19jldX": {
    "title": "Scaling Laws for Upcycling Mixture-of-Experts Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2veJwf07RN": {
    "title": "L-Diffusion: Laplace Diffusion for Efficient Pathology Image Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oji8jIBHgo": {
    "title": "From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y3EQLjoYdQ": {
    "title": "Guardians of Image Quality: Benchmarking Defenses Against Adversarial Attacks on Image Quality Metrics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OTDdeygJTP": {
    "title": "AMPO: Active Multi Preference Optimization for Self-play Preference Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7qCkf5OT3x": {
    "title": "Meta-Black-Box-Optimization through Offline Q-function Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=magOSIm8UT": {
    "title": "Fundamental Limits of Visual Autoregressive Transformers: Universal Approximation Abilities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jkVH7nLzUR": {
    "title": "Three-Dimensional Trajectory Prediction with 3DMoTraj Dataset",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wDKlybjm7T": {
    "title": "Dissecting Submission Limit in Desk-Rejections: A Mathematical Analysis of Fairness in AI Conference Policies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nAk0ENu8LS": {
    "title": "GHOST: Generalizable One-Shot Federated Graph Learning with Proxy-Based Topology Knowledge Retention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1PfZs0xC2v": {
    "title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iJdjDM6Odd": {
    "title": "Rethinking Causal Ranking: A Balanced Perspective on Uplift Model Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cgHfR7bt0V": {
    "title": "Momentum-Driven Adaptivity: Towards Tuning-Free Asynchronous Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZKISVpQ0bj": {
    "title": "Refined generalization analysis of the Deep Ritz Method and Physics-Informed Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V9d9YUXg6s": {
    "title": "Finite-Time Analysis of Discrete-Time Stochastic Interpolants",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yMhkpJ1K01": {
    "title": "Self-supervised Adversarial Purification for Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9mPhtzB9XP": {
    "title": "TimeStep Master: Asymmetrical Mixture of Timestep LoRA Experts for Versatile and Efficient Diffusion Models in Vision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LCaTpVuvpj": {
    "title": "From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hRxHF1xPYB": {
    "title": "Prompt-based Depth Pruning of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v0HLf7b1rH": {
    "title": "A-PSRO: A Unified Strategy Learning Method with Advantage Metric for Normal-form Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h4lZMgGyAz": {
    "title": "Masked Generative Nested Transformers with Decode Time Scaling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=avGZE46gL6": {
    "title": "Continuous Visual Autoregressive Generation via Score Maximization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y7GpMDrWG4": {
    "title": "Maintaining Proportional Committees with Dynamic Candidate Sets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cumipBkkAR": {
    "title": "InfoCons: Identifying Interpretable Critical Concepts in Point Clouds via Information Theory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f9jVsRGpDG": {
    "title": "DragSolver: A Multi-Scale Transformer for Real-World Automotive Drag Coefficient Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q2av1PZmfT": {
    "title": "GraphCL: Graph-based Clustering for Semi-Supervised Medical Image Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NSxKNNFni0": {
    "title": "MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cx9AnB8rzi": {
    "title": "CERTAIN: Context Uncertainty-aware One-Shot Adaptation for Context-based Offline Meta Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7VLYI1naAr": {
    "title": "Omni-Angle Assault: An Invisible and Powerful Physical Adversarial Attack on Face Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ERw2196o1": {
    "title": "Think Smarter not Harder: Adaptive Reasoning with Inference Aware Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ziDKPXJBYL": {
    "title": "DynaMind: Reasoning over Abstract Video Dynamics for Embodied Decision-Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QuecSemZIy": {
    "title": "KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5jlvLwoO1n": {
    "title": "SpikF: Spiking Fourier Network for Efficient Long-term Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R7sAZDq0M9": {
    "title": "Distributed Conformal Prediction via Message Passing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sg8ZqQ9J6W": {
    "title": "RepLoRA: Reparameterizing Low-rank Adaptation via the Perspective of Mixture of Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UtlFqRDJKl": {
    "title": "TabSDS: a Lightweight, Fully Non-Parametric, and Model Free Approach for Generating Synthetic Tabular Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vwIjp2751u": {
    "title": "Feature out! Let Raw Image as Your Condition for Blind Face Restoration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G5xO7oD5G7": {
    "title": "Learning Imperfect Information Extensive-form Games with Last-iterate Convergence under Bandit Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=97N3XNtFwy": {
    "title": "C2IQL: Constraint-Conditioned Implicit Q-learning for Safe Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LnoTEuVhud": {
    "title": "Learning to Generate Projections for Reducing Dimensionality of Heterogeneous Linear Programming Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ysVDe6JGGs": {
    "title": "Understanding Model Ensemble in Transferable Adversarial Attack",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xiVuqZZ59O": {
    "title": "Ultra Lowrate Image Compression with Semantic Residual Coding and Compression-aware Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4qMJ8Ignmp": {
    "title": "Efficient Skill Discovery via Regret-Aware Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qR4HCCAIf3": {
    "title": "COMRECGC: Global Graph Counterfactual Explainer through Common Recourse",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pyKO0ZZ5lz": {
    "title": "TimeBridge: Non-Stationarity Matters for Long-term Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hhhcwCgyM1": {
    "title": "Olica: Efficient Structured Pruning of Large Language Models without Retraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B3zlIHdnER": {
    "title": "Knowledge Swapping via Learning and Unlearning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iilSR3cKTx": {
    "title": "Learn Beneficial Noise as Graph Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q7PAIE0z5p": {
    "title": "Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of Intrinsic Dimension",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SGnjO72L95": {
    "title": "STD-FD: Spatio-Temporal Distribution Fitting Deviation for AIGC Forgery Identification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lDPtsCYTwr": {
    "title": "Low-Rank Adapting Models for Sparse Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xPMDwGL1TT": {
    "title": "Improving the Continuity of Goal-Achievement Ability via Policy Self-Regularization for Goal-Conditioned Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oyFAFpqaca": {
    "title": "Separating Knowledge and Perception with Procedural Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ULAQ9GmJlo": {
    "title": "Natural Perturbations for Black-box Training of Neural Networks by Zeroth-Order Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tJmhOPkWCj": {
    "title": "any4: Learned 4-bit Numeric Representation for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=30RWdxmJV1": {
    "title": "Geometric Generative Modeling with Noise-Conditioned Graph Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eyjrms4HHm": {
    "title": "CMoS: Rethinking Time Series Prediction Through the Lens of Chunk-wise Spatial Correlations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ratfqvU5GF": {
    "title": "Breaking the Quadratic Barrier: Robust Cardinality Sketches for Adaptive Queries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=80faIPZ67S": {
    "title": "SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hlpwAmQ4wr": {
    "title": "C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dr8msCnFYw": {
    "title": "FIC-TSC: Learning Time Series Classification with Fisher Information Constraint",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XlRIub1r5s": {
    "title": "Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable Structured Latent Modelling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n1CVVzBSjQ": {
    "title": "Hyper: Hyperparameter Robust Efficient Exploration in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xnPW7yYomF": {
    "title": "OWLS: Scaling Laws for Multilingual Speech Recognition and Translation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LXILcnYTSl": {
    "title": "The Underlying Universal Statistical Structure of Natural Datasets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x8S61xscqh": {
    "title": "Phase transitions for the existence of unregularized M-estimators in single index models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0IJQD8zRXT": {
    "title": "Positional Attention: Expressivity and Learnability of Algorithmic Computation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gTYzm0Qchr": {
    "title": "PPDiff: Diffusing in Hybrid Sequence-Structure Space for Protein-Protein Complex Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R6aLcBGBXf": {
    "title": "Sharp Optimality of Simple, Plug-in Estimation of the Fisher Information of a Smoothed Density",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L854DKcgHo": {
    "title": "Should Decision-Makers Reveal Classifiers in Online Strategic Classification?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rqOlUTnKWh": {
    "title": "Analytical Lyapunov Function Discovery: An RL-based Generative Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8unyWZ14mf": {
    "title": "Evaluating Neuron Explanations: A Unified Framework with Sanity Checks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZcvGJH4ps7": {
    "title": "Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yNpYb376zf": {
    "title": "Fleet of Agents: Coordinated Problem Solving with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L6CYAzpO1k": {
    "title": "Flexible and Efficient Grammar-Constrained Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6SIVFmjIm4": {
    "title": "Enhancing Foundation Models with Federated Domain Knowledge Infusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9JQXuyzdGL": {
    "title": "Flow-based Domain Randomization for Learning and Sequencing Robotic Skills",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ypeehAYK7W": {
    "title": "Approximate Differential Privacy of the ℓ 2 Mechanism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q4tpr9bnXU": {
    "title": "Optimizing Social Network Interventions via Hypergradient-Based Recommender System Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vw4f8M67jE": {
    "title": "Gravity-Bench-v1: A Benchmark on Gravitational Physics Discovery for Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJd7IklNpZ": {
    "title": "Generative Data Mining with Longtail-Guided Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6N0GxaKdX9": {
    "title": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UgKcpPE0ZX": {
    "title": "Beyond Bradley-Terry Models: A General Preference Model for Language Model Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zydNWJzoVd": {
    "title": "Continuous Bayesian Model Selection for Multivariate Causal Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=owulFly8oQ": {
    "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with Tool Usage Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zWArMedNuW": {
    "title": "STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=40gBawg6LX": {
    "title": "Rejecting Hallucinated State Targets during Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CXN1Myzsp4": {
    "title": "LapSum - One Method to Differentiate Them All: Ranking, Sorting and Top-k Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fi6p2dBDNd": {
    "title": "Autoencoder-Based Hybrid Replay for Class-Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1jjXooEooD": {
    "title": "Mutual Learning for SAM Adaptation: A Dual Collaborative Network Framework for Source-Free Domain Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HIOA3bScFB": {
    "title": "A Mixture-Based Framework for Guiding Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fIf2xt4GXZ": {
    "title": "Machines and Mathematical Mutations: Using GNNs to Characterize Quiver Mutation Classes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0n2nXmOxZS": {
    "title": "Gap-Dependent Bounds for Federated Q -Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KG6aBfGi6e": {
    "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K1LcmG2jQN": {
    "title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s015m2jUGg": {
    "title": "Efficient Diffusion Models for Symmetric Manifolds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2XvF67vbCK": {
    "title": "Does One-shot Give the Best Shot? Mitigating Model Inconsistency in One-shot Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kl2SA1N03E": {
    "title": "Generalized Venn and Venn-Abers Calibration with Applications in Conformal Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NNr8DHb0L7": {
    "title": "IL-SOAR : Imitation Learning with Soft Optimistic Actor cRitic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mqljEW2NQl": {
    "title": "Fusing Reward and Dueling Feedback in Stochastic Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6MZz9eJUnD": {
    "title": "Efficient Distributed Optimization under Heavy-Tailed Noise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YDVROyJbgF": {
    "title": "Rethink the Role of Deep Learning towards Large-scale Quantum Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jhsyra8NpQ": {
    "title": "Continual Generalized Category Discovery: Learning and Forgetting from a Bayesian Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8m2zWI6OJv": {
    "title": "Improving Out-of-Distribution Detection with Markov Logic Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJzkPnSVRM": {
    "title": "Toward Efficient Kernel-Based Solvers for Nonlinear PDEs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=85Yiqs0zxT": {
    "title": "Fully Heteroscedastic Count Regression with Deep Double Poisson Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K3lykWhXON": {
    "title": "Efficient and Scalable Density Functional Theory Hamiltonian Prediction through Adaptive Sparsity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YG84SWm7gn": {
    "title": "Robust and Conjugate Spatio-Temporal Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w8MCYYAvQD": {
    "title": "From Thousands to Billions: 3D Visual Language Grounding via Render-Supervised Distillation from 2D VLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BwYQ1MTrCR": {
    "title": "Strategic A/B testing via Maximum Probability-driven Two-armed Bandit",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C5Jj3QKQav": {
    "title": "Detecting Strategic Deception with Linear Probes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YDgAsTe6r6": {
    "title": "Solving Satisfiability Modulo Counting Exactly with Probabilistic Circuits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=36oullXtuS": {
    "title": "The Empirical Mean is Minimax Optimal for Local Glivenko-Cantelli",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cq7XU5tmP8": {
    "title": "Conformal Anomaly Detection in Event Sequences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eTDgECpQ2I": {
    "title": "Equivariant Polynomial Functional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mtk8tTKWs0": {
    "title": "PIGDreamer: Privileged Information Guided World Models for Safe Partially Observable Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x2Dw9aNbvw": {
    "title": "Heads up! Large Language Models Can Perform Tasks Without Your Instruction via Selective Attention Head Masking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vxM49M5B4s": {
    "title": "No-Regret is not enough! Bandits with General Constraints through Adaptive Regret Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CKQI3Nksxm": {
    "title": "Learning from True-False Labels via Multi-modal Prompt Retrieving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jMKaATBEKb": {
    "title": "Behavior-agnostic Task Inference for Robust Offline In-context Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YLTfcWxVVj": {
    "title": "Reward Translation via Reward Machine in Semi-Alignable MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5hqeKn6U66": {
    "title": "Uniform Mean Estimation for Heavy-Tailed Distributions via Median-of-Means",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G3j3kq7rSC": {
    "title": "UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DgdOkUUBzf": {
    "title": "FlexTok: Resampling Images into 1D Token Sequences of Flexible Length",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OXIIRxwJwx": {
    "title": "OW-VAP: Visual Attribute Parsing for Open World Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uK7JArZEJM": {
    "title": "Optimizing Large Language Model Training Using FP4 Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MhVJCxYEEi": {
    "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o36quGYoQX": {
    "title": "Heterogeneous Data Game: Characterizing the Model Competition Across Multiple Data Sources",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N2mOBiSqhc": {
    "title": "Learning Strategic Language Agents in the Werewolf Game with Iterative Latent Space Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HemldRNffY": {
    "title": "UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a3swNuXTxI": {
    "title": "The Limits of Predicting Agents from Behaviour",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DzP9N6oO0N": {
    "title": "Perceptually Constrained Precipitation Nowcasting Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XSOJx4hj0y": {
    "title": "Large Continual Instruction Assistant",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SAIZ9qAMmg": {
    "title": "Provable Efficiency of Guidance in Diffusion Models for General Data Distribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8tRtweTTwv": {
    "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xX8NJShgny": {
    "title": "Scalable Model Merging with Progressive Layer-wise Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f6ibJCQfH4": {
    "title": "TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wpbNczwAwV": {
    "title": "Hierarchical Graph Tokenization for Molecule-Language Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OjIMyKYviK": {
    "title": "Enhancing Spectral GNNs: From Topology and Perturbation Perspectives",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9bzgpYtQZn": {
    "title": "Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AUkBFMtyUs": {
    "title": "Residual TPP: A Unified Lightweight Approach for Event Stream Data Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PX29zF9wRb": {
    "title": "AI for Global Climate Cooperation: Modeling Global Climate Negotiations, Agreements, and Long-Term Cooperation in RICE-N",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L8YhRgEcZp": {
    "title": "On the Importance of Embedding Norms in Self-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tl3FlgWScA": {
    "title": "Ad Hoc Teamwork via Offline Goal-Based Decision Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDWvebg8mL": {
    "title": "Active Treatment Effect Estimation via Limited Samples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7bgqx5OoVe": {
    "title": "Sassha: Sharpness-aware Adaptive Second-order Optimization with Stable Hessian Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cQHwUckohW": {
    "title": "Compressed Image Generation with Denoising Diffusion Codebook Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1fUJk8viuw": {
    "title": "Learning Configurations for Data-Driven Multi-Objective Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bqQVa6VRvm": {
    "title": "NeuralCohort: Cohort-aware Neural Representation Learning for Healthcare Analytics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1W2WlYRq0K": {
    "title": "MindAligner: Explicit Brain Functional Alignment for Cross-Subject Visual Decoding from Limited fMRI Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PNy6UmfzgS": {
    "title": "Inducing, Detecting and Characterising Neural Modules: A Pipeline for Functional Interpretability in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JRg8P2bX8P": {
    "title": "Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bkiM54QftZ": {
    "title": "On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sguPQWr44L": {
    "title": "Task-Agnostic Pre-training and Task-Guided Fine-tuning for Versatile Diffusion Planner",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BOrR7YqKUt": {
    "title": "CoSER: Coordinating LLM-Based Persona Simulation of Established Roles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6SIymOqJlc": {
    "title": "PoisonedEye: Knowledge Poisoning Attack on Retrieval-Augmented Generation based Large Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=51x0dfsD8A": {
    "title": "Hierarchical Overlapping Clustering on Graphs: Cost Function, Algorithm and Scalability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eFgtUFYe6v": {
    "title": "TS-SNN: Temporal Shift Module for Spiking Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fE3kgW7kMp": {
    "title": "Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M9keJ0Jy3J": {
    "title": "LLMScan: Causal Scan for LLM Misbehavior Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sylDbssCU9": {
    "title": "Efficient Federated Incomplete Multi-View Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NiMu23k0Ym": {
    "title": "Mitigating Over-Squashing in Graph Neural Networks by Spectrum-Preserving Sparsification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xAhUoyb5eU": {
    "title": "Ex-VAD: Explainable Fine-grained Video Anomaly Detection Based on Visual-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u1yVbMzZLm": {
    "title": "In-Context Adaptation to Concept Drift for Learned Database Operations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jk3TjZAHem": {
    "title": "Continuously Updating Digital Twins using Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4vLFFkFBwi": {
    "title": "Adversarial Robust Generalization of Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=992yMPvMqV": {
    "title": "BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural Speech Synthesis with Flow Matching Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rxa6fWKces": {
    "title": "DSBRouter: End-to-end Global Routing via Diffusion Schr\\\"{o}dinger Bridge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tVDZlyVMIZ": {
    "title": "Distributionally Robust Active Learning for Gaussian Process Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wn6WHREK9k": {
    "title": "Oracle-MoE: Locality-preserving Routing in the Oracle Space for Memory-constrained Large Language Model Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J1xVXyWv2T": {
    "title": "On Fine-Grained Distinct Element Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KwSaa1Ykdf": {
    "title": "A Recipe for Causal Graph Regression: Confounding Effects Revisited",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v3B79m7t8Z": {
    "title": "RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZRhi2hZWG4": {
    "title": "Drug-TTA: Test-Time Adaptation for Drug Virtual Screening via Multi-task Meta-Auxiliary Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oUzxhnnGVM": {
    "title": "When do neural networks learn world models?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KaukiEQcbk": {
    "title": "VerbalTS: Generating Time Series from Texts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6F0L4HW8a8": {
    "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BHF0KOOllW": {
    "title": "Minimum Width for Universal Approximation using Squashable Activation Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VNLmfMJi3w": {
    "title": "Stay-Positive: A Case for Ignoring Real Image Features in Fake Image Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nrs6csi52N": {
    "title": "Balancing Preservation and Modification: A Region and Semantic Aware Metric for Instruction-Based Image Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xCWAcX4pNa": {
    "title": "Latent Thought Models with Variational Bayes Inference-Time Computation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PFMVVaPCn5": {
    "title": "Meta-Reinforcement Learning with Adaptation from Human Feedback via Preference-Order-Preserving Task Embedding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DmPW0pO3F3": {
    "title": "Spectral-Aware Reservoir Computing for Fast and Accurate Time Series Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SznX4yic20": {
    "title": "PTTA: Purifying Malicious Samples for Test-Time Model Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=APElRzkmGY": {
    "title": "IBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1ryQoI9iZ": {
    "title": "Unisolver: PDE-Conditional Transformers Towards Universal Neural PDE Solvers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oB5a6yIAmF": {
    "title": "Holistic Physics Solver: Learning PDEs in a Unified Spectral-Physical Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0YXxh8WALf": {
    "title": "MimicMotion: High-Quality Human Motion Video Generation with Confidence-aware Pose Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Um7XmQEWu5": {
    "title": "Towards Robust Influence Functions with Flat Validation Minima",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IszVnczhfz": {
    "title": "How to set AdamW's weight decay as you scale model and dataset size",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S09B5wNa6J": {
    "title": "ZipAR: Parallel Autoregressive Image Generation through Spatial Locality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Us8v5tDOFd": {
    "title": "Point Cloud Dataset Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cx80t5FAQJ": {
    "title": "Learning Input Encodings for Kernel-Optimal Implicit Neural Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RfQNtVTY7Q": {
    "title": "Efficient Network Automatic Relevance Determination",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8n6CY3zsJo": {
    "title": "Label Distribution Propagation-based Label Completion for Crowdsourcing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zis7Mgk2Sf": {
    "title": "On the Out-of-Distribution Generalization of Self-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ITMu1pZTFo": {
    "title": "Attention-Only Transformers via Unrolled Subspace Denoising",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r6y9TEdLMh": {
    "title": "Sum-of-Parts: Self-Attributing Neural Networks with End-to-End Learning of Feature Groups",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dKfq3JbjnE": {
    "title": "Certifiably Robust Model Evaluation in Federated Learning under Meta-Distributional Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FMg9mEKH17": {
    "title": "SBGD: Improving Graph Diffusion Generative Model via Stochastic Block Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OOVYGgm1J7": {
    "title": "On the Interplay between Graph Structure and Learning Algorithms in Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XGZeCEzeRT": {
    "title": "A Non-Asymptotic Convergent Analysis for Scored-Based Graph Generative Model via a System of Stochastic Differential Equations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zcOCXlvsk1": {
    "title": "Adversarial Combinatorial Semi-bandits with Graph Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hWYisuBbp7": {
    "title": "Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ckZbP606Bt": {
    "title": "An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LMPJnZSNC8": {
    "title": "HALoS: Hierarchical Asynchronous Local SGD over Slow Networks for Geo-Distributed Large Language Model Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1Dq4rW1Oy4": {
    "title": "ArrayDPS: Unsupervised Blind Speech Separation with a Diffusion Prior",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=malQrLFGO3": {
    "title": "Permutation Equivariant Neural Networks for Symmetric Tensors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KVf2SFL1pi": {
    "title": "Flow Q-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0p0FukRmzE": {
    "title": "Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OcFsPBXREI": {
    "title": "Task-Gated Multi-Expert Collaboration Network for Degraded Multi-Modal Image Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=82A81az3V5": {
    "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=drP7QMlkHh": {
    "title": "Preserving AUC Fairness in Learning with Noisy Protected Groups",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bezL796MKt": {
    "title": "DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZWZLYVFgDL": {
    "title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qou3A0Bjzt": {
    "title": "Latent Imputation before Prediction: A New Computational Paradigm for De Novo Peptide Sequencing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gb2dwmPXhs": {
    "title": "Robust Secure Swap: Responsible Face Swap With Persons of Interest Redaction and Provenance Traceability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HtSdgubxsJ": {
    "title": "Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SbyrpBNNs4": {
    "title": "SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C9tD7ZLew4": {
    "title": "Best Subset Selection: Optimal Pursuit for Feature Selection and Elimination",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j4HpE61Zcv": {
    "title": "Lean and Mean Adaptive Optimization via Subset-Norm and Subspace-Momentum with Convergence Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SLEeGAg5ee": {
    "title": "DANCE: Dual Unbiased Expansion with Group-acquired Alignment for Out-of-distribution Graph Fairness Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dc9vh0NlBR": {
    "title": "TRUST-VLM: Thorough Red-Teaming for Uncovering Safety Threats in Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u7dlwgKstN": {
    "title": "CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BnPaSXSmz1": {
    "title": "An Online Statistical Framework for Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yTpn3QY9Ff": {
    "title": "Divide and Conquer: Exploring Language-centric Tree Reasoning for Video Question-Answering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uRD6wkqulN": {
    "title": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cr9qfD3qRc": {
    "title": "Not All Tokens Matter All The Time: Dynamic Token Aggregation Towards Efficient Detection Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jnhkY0yCIW": {
    "title": "SEMU: Singular Value Decomposition for Efficient Machine Unlearning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PRa4e2BHIf": {
    "title": "FlexControl: Computation-Aware Conditional Control with Differentiable Router for Text-to-Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FO2fu3daSL": {
    "title": "Generative Modeling Reinvents Supervised Learning: Label Repurposing with Predictive Consistency Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PFdWf0H4V2": {
    "title": "Demystifying Catastrophic Forgetting in Two-Stage Incremental Object Detector",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ij0vj0BC72": {
    "title": "Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zy7VeNtSLM": {
    "title": "MP-Nav: Enhancing Data Poisoning Attacks against Multimodal Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zNUOZcAUxz": {
    "title": "Accurate and Efficient World Modeling with Masked Latent Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vUDWRMB3tX": {
    "title": "Reidentify: Context-Aware Identity Generation for Contextual Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GzFKZctIzj": {
    "title": "KoNODE: Koopman-Driven Neural Ordinary Differential Equations with Evolving Parameters for Time Series Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=va3zmBXPat": {
    "title": "Objective drives the consistency of representational similarity across datasets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G3grccIXIg": {
    "title": "Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tcK4PV3VN4": {
    "title": "Adaptive Localization of Knowledge Negation for Continual LLM Unlearning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1CK1kuH1he": {
    "title": "Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3vjsUgCsZ4": {
    "title": "Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cnogN1gvbu": {
    "title": "Unsupervised Learning for Class Distribution Mismatch",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MrphqqwnKv": {
    "title": "Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1QZMWVrgsU": {
    "title": "PEINR: A Physics-enhanced Implicit Neural Representation for High-Fidelity Flow Field Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Afmi28vgIf": {
    "title": "An All-Atom Generative Model for Designing Protein Complexes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gwlJgoq5QZ": {
    "title": "Slimming the Fat-Tail: Morphing-Flow for Adaptive Time Series Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0uoKBSxRjh": {
    "title": "Complete-Tree Space Favors Data-Efficient Link Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Gke1dfRVA": {
    "title": "NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UVoxPlv5E1": {
    "title": "Teaching Language Models to Critique via Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ksdPdnwKGi": {
    "title": "Rethinking Chain-of-Thought from the Perspective of Self-Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AM7iAh0krx": {
    "title": "Transolver++: An Accurate Neural Solver for PDEs on Million-Scale Geometries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bInH58kyxp": {
    "title": "The Price of Linear Time: Error Analysis of Structured Kernel Interpolation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F3Cbhb611p": {
    "title": "Causal Invariance-aware Augmentation for Brain Graph Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TxeCxVb3cL": {
    "title": "Griffin: Towards a Graph-Centric Relational Database Foundation Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wGFEzfhFae": {
    "title": "The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z7ms8FJfCM": {
    "title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nC8XliUxeg": {
    "title": "SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=74c3Wwk8Tc": {
    "title": "SpargeAttention: Accurate and Training-free Sparse Attention Accelerating Any Model Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g4eTrS2U8o": {
    "title": "Stability and Generalization Analysis of Decentralized SGD: Sharper Bounds Beyond Lipschitzness and Smoothness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eLkkXaPFEP": {
    "title": "An Effective and Secure Federated Multi-View Clustering Method with Information-Theoretic Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oeo53q93iP": {
    "title": "Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nymm7XlXvZ": {
    "title": "Near Optimal Non-asymptotic Sample Complexity of 1-Identification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WbLXcMt2eG": {
    "title": "Curriculum Learning for Biological Sequence Prediction: The Case of De Novo Peptide Sequencing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dEhemezhTu": {
    "title": "Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LSnpEbs0nA": {
    "title": "Enhancing Treatment Effect Estimation via Active Learning: A Counterfactual Covering Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6qNbVtKGY2": {
    "title": "Probabilistic Interactive 3D Segmentation with Hierarchical Neural Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TcvjOSePic": {
    "title": "CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l9DJGAtoAj": {
    "title": "Self-Discriminative Modeling for Anomalous Graph Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=16oBNUlWyB": {
    "title": "Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DvRuQ6mObK": {
    "title": "The Complexity of Learning Sparse Superposed Features with Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iA3kafgHGi": {
    "title": "Understanding Multimodal LLMs Under Distribution Shifts: An Information-Theoretic Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4NJCI4Q3Za": {
    "title": "Understanding the Emergence of Multimodal Representation Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=diFvAHoHry": {
    "title": "Testing Conditional Mean Independence Using Generative Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FcTeo26AfZ": {
    "title": "Unifying 2D and 3D Vision-Language Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATUfSZayVo": {
    "title": "RLTHF: Targeted Human Feedback for LLM Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WrWqv3mpQx": {
    "title": "Stochastic Forward–Backward Deconvolution: Training Diffusion Models with Finite Noisy Datasets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hLGJ1qZPdu": {
    "title": "On the Generalization Ability of Next-Token-Prediction Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1jGQUjAag": {
    "title": "Fragments to Facts: Partial-Information Fragment Inference from LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ggt3iu0Zni": {
    "title": "Efficient Quantification of Multimodal Interaction at Sample Level",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BsTLUx38qV": {
    "title": "PolyConf: Unlocking Polymer Conformation Generation through Hierarchical Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f6Hl60FBFU": {
    "title": "Constrained Belief Updates Explain Geometric Structures in Transformer Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oRvWspa6Uu": {
    "title": "A Classification View on Meta Learning Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=onumui0nHi": {
    "title": "Empowering World Models with Reflection for Embodied Video Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vDoAA8xKXL": {
    "title": "DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4WMZqAoYi4": {
    "title": "Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vOCPctm3nb": {
    "title": "CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9D5aM5LQ3Y": {
    "title": "Safe-EF: Error Feedback for Non-smooth Constrained Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=riYSkLG0vt": {
    "title": "One Diffusion Step to Real-World Super-Resolution via Flow Trajectory Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w5Y0415tGt": {
    "title": "ReverB-SNN: Reversing Bit of the Weight and Activation for Spiking Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OKbECHtO4S": {
    "title": "Nested Expectations with Kernel Quadrature",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v2G9HML7ep": {
    "title": "TimeDART: A Diffusion Autoregressive Transformer for Self-Supervised Time Series Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qjd3ZUiHRT": {
    "title": "EPIC: Efficient Position-Independent Caching for Serving Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p6PElhIM4E": {
    "title": "Stealix: Model Stealing via Prompt Evolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UYUqCPCZCw": {
    "title": "Noise Conditional Variational Score Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3GOaybsKfJ": {
    "title": "A Peer-review Look on Multi-modal Clustering: An Information Bottleneck Realization Method",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V3KXsUFw8D": {
    "title": "Beyond The Rainbow: High Performance Deep Reinforcement Learning on a Desktop PC",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=94ZmvjqMWu": {
    "title": "Ehrenfeucht-Haussler Rank and Chain of Thought",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2mnZ7AjMim": {
    "title": "IMTS is Worth Time × Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P8qRHKKiAx": {
    "title": "A Closer Look at Generalized BH Algorithm for Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mHZhbRnwJI": {
    "title": "Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fXqUGWCdjf": {
    "title": "Multi-band Frequency Reconstruction for Neural Psychoacoustic Coding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b3bJR1quJ3": {
    "title": "IRBridge: Solving Image Restoration Bridge with Pre-trained Generative Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aV9uE5i4Tu": {
    "title": "Learning Adaptive Lighting via Channel-Aware Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=svFtPVdD9t": {
    "title": "On the Power of Learning-Augmented Search Trees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y0yXuQtPn8": {
    "title": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for Energy-Efficient LLM Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hfPaOxDWfI": {
    "title": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4x7H7nwTZW": {
    "title": "Human Body Restoration with One-Step Diffusion Model and A New Benchmark",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KTZinkSMTG": {
    "title": "SMART-PC: Skeletal Model Adaptation for Robust Test-Time Training in Point Clouds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RmQnijtu6h": {
    "title": "Instruct2See: Learning to Remove Any Obstructions Across Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=buwLCdOHxO": {
    "title": "Collapse or Thrive: Perils and Promises of Synthetic Data in a Self-Generating World",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WVlEwFiDGH": {
    "title": "D-Fusion: Direct Preference Optimization for Aligning Diffusion Models with Visually Consistent Samples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nq3oz7vn3j": {
    "title": "Improving Value Estimation Critically Enhances Vanilla Policy Gradient",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vh2Dt4sT67": {
    "title": "Occult: Optimizing Collaborative Communications across Experts for Accelerated Parallel MoE Training and Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VdEG08ZJCH": {
    "title": "Towards Lifelong Model Editing via Simulating Ideal Editor",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uEsWuHra1Y": {
    "title": "Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7cNLsEAcmL": {
    "title": "Offline Learning for Combinatorial Multi-armed Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gAxYbvoOQz": {
    "title": "Lightweight Online Adaption for Time Series Foundation Model Forecasts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sw7ML1IOkk": {
    "title": "Concept-Centric Token Interpretation for Vector-Quantized Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hzYHxtIn23": {
    "title": "Discovering Physics Laws of Dynamical Systems via Invariant Function Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pj3vkXJvKg": {
    "title": "Tuning Sequential Monte Carlo Samplers via Greedy Incremental Divergence Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U08mUogGDM": {
    "title": "Learning to Route LLMs with Confidence Tokens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bcheYCitFy": {
    "title": "Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ym19zWky7W": {
    "title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iouCP8zZQS": {
    "title": "Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot Transfer Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B5p7bjTI8v": {
    "title": "A Reduction Framework for Distributionally Robust Reinforcement Learning under Average Reward",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FCZ3jVzmTZ": {
    "title": "Idiosyncrasies in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nx8QhIlZh8": {
    "title": "Neural Graph Matching Improves Retrieval Augmented Generation in Molecular Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eFjv7NPOn1": {
    "title": "ExpProof : Operationalizing Explanations for Confidential Models with ZKPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8BIDrYWCeg": {
    "title": "Autonomy-of-Experts Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rxg8vCZSee": {
    "title": "Chaos Meets Attention: Transformers for Large-Scale Dynamical Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gVenpttGec": {
    "title": "Beyond Log-Concavity and Score Regularity: Improved Convergence Bounds for Score-Based Generative Models in W2-distance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8aChcUzAhI": {
    "title": "LMAct: A Benchmark for In-Context Imitation Learning with Long Multimodal Demonstrations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O2jukIZR50": {
    "title": "V i s t a DPO : Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=94c9hu6Fsv": {
    "title": "Outsourced Diffusion Sampling: Efficient Posterior Inference in Latent Spaces of Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0rDn6BDNiF": {
    "title": "OOD-Chameleon: Is Algorithm Selection for OOD Generalization Learnable?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A5vUx8S2yB": {
    "title": "IMPACT: Iterative Mask-based Parallel Decoding for Text-to-Audio Generation with Diffusion Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tH4Zka30JZ": {
    "title": "Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KgaZqX8YQ7": {
    "title": "Counting in Small Transformers: The Delicate Interplay between Attention and Feed-Forward Layers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a5Kgv47d2e": {
    "title": "Unlocking Post-hoc Dataset Inference with Synthetic Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WnTGpncrxK": {
    "title": "Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7SXXczJCWP": {
    "title": "Privacy Attacks on Image AutoRegressive Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RPPBhhRddB": {
    "title": "A Market for Accuracy: Classification Under Competition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=COowwJOAZi": {
    "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lioemOcq3H": {
    "title": "FeatSharp: Your Vision Model Features, Sharper",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2FDsh5D2Th": {
    "title": "Pre-training Auto-regressive Robotic Models with 4D Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=490VcNtjh7": {
    "title": "TimeFilter: Patch-Specific Spatial-Temporal Graph Filtration for Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BTJSkPpH1t": {
    "title": "Accelerated Diffusion Models via Speculative Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N82967FcVK": {
    "title": "Distributional Diffusion Models with Scoring Rules",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ArifAHrEVD": {
    "title": "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MSbU3L7V00": {
    "title": "Do NOT Think That Much for 2+3=? On the Overthinking of Long Reasoning Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ii0DX4U439": {
    "title": "EmoGrowth: Incremental Multi-label Emotion Decoding with Augmented Emotional Relation Graph",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hyPWP38j5k": {
    "title": "Graph Minimum Factorization Distance and Its Application to Large-Scale Graph Data Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fnz1g18EdI": {
    "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7EtP9u7JNw": {
    "title": "Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EAnuqHF0tx": {
    "title": "Algorithms and Hardness for Active Learning on Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lLGqtDcPag": {
    "title": "An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YD9ZoqUDAY": {
    "title": "Unified K-Means Clustering with Label-Guided Manifold Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nxCWKWok8N": {
    "title": "Leveraging Sparsity for Sample-Efficient Preference Learning: A Theoretical Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FgNYqzjVLg": {
    "title": "Generative Intervention Models for Causal Perturbation Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=afhPCaIRrh": {
    "title": "Enhancing the Influence of Labels on Unlabeled Nodes in Graph Convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CG4QPoJ7ST": {
    "title": "Playmate: Flexible Control of Portrait Animation via 3D-Implicit Space Guided Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jwe5FJ8QGx": {
    "title": "Preference Optimization for Combinatorial Optimization Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bBPnGYbypF": {
    "title": "L3A: Label-Augmented Analytic Adaptation for Multi-Label Class Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NrcKQ3ASLZ": {
    "title": "MARS: Unleashing the Power of Variance Reduction for Training Large Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WHqSAEmvai": {
    "title": "Online Learning in the Random-Order Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O3WqAhxuc7": {
    "title": "When Do LLMs Help With Node Classification? A Comprehensive Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jXZR3XinPg": {
    "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kz1zCJRr1r": {
    "title": "Measuring Representational Shifts in Continual Learning: A Linear Transformation Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zQS9YryNHs": {
    "title": "MIRROR: Make Your Object-Level Multi-View Generation More Consistent with Training-Free Rectification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3CiSpY3QdZ": {
    "title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ChmJZ9V2o1": {
    "title": "On the Similarities of Embeddings in Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jCEl0aJpF6": {
    "title": "Distillation of Discrete Diffusion through Dimensional Correlations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qmeNQpLiG5": {
    "title": "A Two-Stage Learning-to-Defer Approach for Multi-Task Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a02CH43z1G": {
    "title": "Representation Surgery in Model Merging with Probabilistic Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6HVcoIbZoC": {
    "title": "Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zvzCdHbDBP": {
    "title": "SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and O ( T ) Complexity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KYiynifZUk": {
    "title": "FuseUNet: A Multi-Scale Feature Fusion Method for U-like Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gmLD0DHaoZ": {
    "title": "Algorithmic Recourse for Long-Term Improvement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s69Ei2VrIW": {
    "title": "TimePro: Efficient Multivariate Long-term Time Series Forecasting with Variable- and Time-Aware Hyper-state",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QwTDQXllam": {
    "title": "FedOne: Query-Efficient Federated Learning for Black-box Discrete Prompt Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7qvYLnJDRd": {
    "title": "Federated Incomplete Multi-view Clustering with Globally Fused Graph Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qMgxXXMMSE": {
    "title": "Toward Data-centric Directed Graph Learning: An Entropy-driven Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qsYHqLFCH5": {
    "title": "How Effective Can Dropout Be in Multiple Instance Learning ?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O2fUpyeRWs": {
    "title": "Optimal Transport Barycenter via Nonconvex-Concave Minimax Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3q6T4hQ4Dy": {
    "title": "Adversarial Perturbations Are Formed by Iteratively Learning Linear Combinations of the Right Singular Vectors of the Adversarial Jacobian",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0AUltNAxjP": {
    "title": "TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hm9FNEZZ6z": {
    "title": "ExtPose: Robust and Coherent Pose Estimation by Extending ViTs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z0ffRRtOim": {
    "title": "Variational Control for Guidance in Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rkgn9KLHhd": {
    "title": "Ringmaster ASGD: The First Asynchronous SGD with Optimal Time Complexity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=reuPtSzCU9": {
    "title": "Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wVDR2qmE28": {
    "title": "How Contaminated Is Your Benchmark? Measuring Dataset Leakage in Large Language Models with Kernel Divergence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DnNV3Ea09e": {
    "title": "Stochastic Control for Fine-tuning Diffusion Models: Optimality, Regularity, and Convergence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pirv9O749u": {
    "title": "Representative Ranking for Deliberation in the Public Sphere",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qap9pHIkI8": {
    "title": "PILAF: Optimal Human Preference Sampling for Reward Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zt05jXhqXx": {
    "title": "Stealing That Free Lunch: Exposing the Limits of Dyna-Style Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bAUVnNc0Ky": {
    "title": "Dynamic Sparse Training of Diagonally Sparse Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uvFE58mSnR": {
    "title": "Multi-Modal Object Re-identification via Sparse Mixture-of-Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oBCw6PZ0fX": {
    "title": "Minimalist Concept Erasure in Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YG3DbpAQBf": {
    "title": "GaussMark: A Practical Approach for Structural Watermarking of Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cW9Ttnm1aC": {
    "title": "Nonparametric Identification of Latent Concepts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RAxe7nF4Oz": {
    "title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RY5MMBHRqo": {
    "title": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GA7JfZyJMw": {
    "title": "Learning Classifiers That Induce Markets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KZlQEoEtiu": {
    "title": "Towards Memorization Estimation: Fast, Formal and Free",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VK8SuRaJfX": {
    "title": "The Four Color Theorem for Cell Instance Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8u5bzM2XfI": {
    "title": "Can RLHF be More Efficient with Imperfect Reward Models? A Policy Coverage Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CeTXRRAdFi": {
    "title": "SAN: Hypothesizing Long-Term Synaptic Development and Neural Engram Mechanism in Scalable Model's Parameter-Efficient Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KhCKypSaqx": {
    "title": "Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CYmaCpGDDG": {
    "title": "Curse of High Dimensionality Issue in Transformer for Long Context Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DVW16DW1Cn": {
    "title": "Diffusion Instruction Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rzn2OgflOK": {
    "title": "Efficient Long Context Fine-tuning with Chunk Flow",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SH4FFEQ3Yv": {
    "title": "KIND: Knowledge Integration and Diversion for Training Decomposable Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GPSmbdTZBm": {
    "title": "Lego Sketch: A Scalable Memory-augmented Neural Network for Sketching Data Streams",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Do1OdZzYHr": {
    "title": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based Automatic Heuristic Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FIME06SV71": {
    "title": "Whitened CLIP as a Likelihood Surrogate of Images and Captions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4oE8vTw5IU": {
    "title": "Decision Mixer: Integrating Long-term and Local Dependencies via Dynamic Token Selection for Decision-Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PqpPrlAQqa": {
    "title": "Channel Normalization for Time Series Channel Identification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=77ziPGdQct": {
    "title": "Vision-Language Models Create Cross-Modal Task Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WjCpaV6N8q": {
    "title": "Task-Aware Virtual Training: Enhancing Generalization in Meta-Reinforcement Learning for Out-of-Distribution Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3h80HyStMH": {
    "title": "Improving Parallel Program Performance with LLM Optimizers via Agent-System Interfaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S0ncZdwcLt": {
    "title": "Targeted Low-rank Refinement: Enhancing Sparse Language Models with Precision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ggERt5kcpa": {
    "title": "Matrix Completion with Incomplete Side Information via Orthogonal Complement Projection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mgbFOJpKY4": {
    "title": "Primitive Vision: Improving Diagram Understanding in MLLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bif35if4n3": {
    "title": "Be Confident: Uncovering Overfitting in MLLM Multi-Task Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HFtcZrh8jF": {
    "title": "Multiobjective distribution matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qEVxub0h2a": {
    "title": "OmniAudio: Generating Spatial Audio from 360-Degree Video",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z5FJsp1U3Z": {
    "title": "LightGTS: A Lightweight General Time Series Forecasting Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LYBiatN3aJ": {
    "title": "KV Shifting Attention Enhances Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XAlUoJFFQR": {
    "title": "DeepLayout: Learning Neural Representations of Circuit Placement Layout",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xraEFvFHR4": {
    "title": "Fluctuations of the largest eigenvalues of transformed spiked Wigner matrices",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NXroN0sD8V": {
    "title": "CogReact: A Reinforced Framework to Model Human Cognitive Reaction Modulated by Dynamic Intervention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hRWcT3AH7Y": {
    "title": "Joker: Joint Optimization Framework for Lightweight Kernel Machines",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QAkh7c3r7c": {
    "title": "Edge-Colored Clustering in Hypergraphs: Beyond Minimizing Unsatisfied Edges",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SBUc5wirM8": {
    "title": "Sparsing Law: Towards Large Language Models with Greater Activation Sparsity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DMJ3b19RAJ": {
    "title": "Measuring Diversity in Synthetic Datasets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=skAjaAEuA2": {
    "title": "Plausible Token Amplification for Improving Accuracy of Differentially Private In-Context Learning Based on Implicit Bayesian Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ySWrJer7mW": {
    "title": "Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qbIcZLSvmH": {
    "title": "Retraining with Predicted Hard Labels Provably Increases Model Accuracy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ku2fvCEpwx": {
    "title": "Self-Supervised Learning of Intertwined Content and Positional Features for Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fbmj0EoeFk": {
    "title": "Modeling All-Atom Glycan Structures via Hierarchical Message Passing and Multi-Scale Pre-training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Xnqm4f71y": {
    "title": "DVI:A Derivative-based Vision Network for INR",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rk18ZikrFI": {
    "title": "Variational Rectified Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=drBVowFvqf": {
    "title": "Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ggyHPOXLGH": {
    "title": "Variational Counterfactual Intervention Planning to Achieve Target Outcomes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ouaiJlE1hJ": {
    "title": "Generation from Noisy Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9MJ8FpUvjD": {
    "title": "Feature-Mapping Topology Optimization with Neural Heaviside Signed Distance Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=27hOkXzy9e": {
    "title": "SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wwj6jjxZet": {
    "title": "A Unified View on Learning Unnormalized Distributions via Noise-Contrastive Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZEPWk1Q6ww": {
    "title": "Backdoor Attacks in Token Selection of Attention Mechanism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xXYGBmpMAj": {
    "title": "WMAdapter: Adding WaterMark Control to Latent Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZM5usuLxur": {
    "title": "MARGE: Improving Math Reasoning with Guided Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bv7LUUYOiq": {
    "title": "Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vbw9hSlOaG": {
    "title": "Clone-Robust AI Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YeQ8SGjarH": {
    "title": "Disentangling Invariant Subgraph via Variance Contrastive Estimation under Distribution Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kONwjsPKcI": {
    "title": "Falsification of Unconfoundedness by Testing Independence of Causal Mechanisms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iCYbIaGKSR": {
    "title": "Test-Time Learning for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Al6qG8BlKg": {
    "title": "Learning Progress Driven Multi-Agent Curriculum",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QRNpmG8XGd": {
    "title": "Solving Linear-Gaussian Bayesian Inverse Problems with Decoupled Diffusion Sequential Monte Carlo",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OS2ZVeHI4U": {
    "title": "QT-DoG: Quantization-Aware Training for Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9CpeZ8BzPO": {
    "title": "MaskTwins: Dual-form Complementary Masking for Domain-Adaptive Image Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EiM163eZyg": {
    "title": "CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qLwQuLQHwL": {
    "title": "Distributed Event-Based Learning via ADMM",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XWC7JXHXvo": {
    "title": "SUICA: Learning Super-high Dimensional Sparse Implicit Neural Representations for Spatial Transcriptomics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3sXMHlhBSs": {
    "title": "AlphaQCM: Alpha Discovery in Finance with Distributional Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XIxcK2Jzpi": {
    "title": "Habitizing Diffusion Planning for Efficient and Effective Decision Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=suZ1pNdKrV": {
    "title": "Solving Probabilistic Verification Problems of Neural Networks using Branch and Bound",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BY3SSEVcjV": {
    "title": "Heterogeneous Label Shift: Theory and Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1b94nzUAvt": {
    "title": "Mitigating Local Cohesion and Global Sparseness in Graph Contrastive Learning with Fuzzy Boundaries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JZIJxr9KsO": {
    "title": "CSTrack: Enhancing RGB-X Tracking via Compact Spatiotemporal Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tkDtiOeLtx": {
    "title": "Noise-Guided Predicate Representation Extraction and Diffusion-Enhanced Discretization for Scene Graph Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ikZH6D9IGt": {
    "title": "Ensemble Learned Bloom Filters: Two Oracles are Better than One",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DatAXrGzlc": {
    "title": "From Feature Interaction to Feature Generation: A Generative Paradigm of CTR Prediction Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h3KHwZCnxH": {
    "title": "Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WIIfdh0GJu": {
    "title": "DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QJKTAcLf1H": {
    "title": "Nonconvex Theory of M -estimators with Decomposable Regularizers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2hbx3wG2I1": {
    "title": "Weight matrices compression based on PDB model in deep neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=djAtMRB1Y4": {
    "title": "Learning Bayesian Nash Equilibrium in Auction Games via Approximate Best Response",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0WQJ6DFSKp": {
    "title": "Socialized Coevolution: Advancing a Better World through Cross-Task Collaboration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QV0PcBbfTd": {
    "title": "Mechanisms of Projective Composition of Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U74MOXPEJd": {
    "title": "Fast Video Generation with Sliding Tile Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h1O10vT4aW": {
    "title": "On the Learnability of Distribution Classes with Adaptive Adversaries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cRmuEY7jhb": {
    "title": "Guided Zeroth-Order Methods for Stochastic Non-convex Problems with Decision-Dependent Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vhjuemZuRU": {
    "title": "SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uNK4ftGdnq": {
    "title": "REG: Rectified Gradient Guidance for Conditional Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vWPzKn6usZ": {
    "title": "Towards Learning to Complete Anything in Lidar",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gdsZ3uMPsY": {
    "title": "Adaptive Self-improvement LLM Agentic System for ML Library Development",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZMrdvSm7xi": {
    "title": "Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UoaxRN88oR": {
    "title": "Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BKOeyZal0x": {
    "title": "Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nn9POI9Ekt": {
    "title": "Agent-as-a-Judge: Evaluate Agents with Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ibX0A4agKU": {
    "title": "Fraud-Proof Revenue Division on Subscription Platforms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MwBxNUl9AV": {
    "title": "Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QdELyl0FST": {
    "title": "GPTAQ: Efficient Finetuning-Free Quantization for Asymmetric Calibration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qKHLzCELGP": {
    "title": "Robust Sparsification via Sensitivity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rWUV8Pti9x": {
    "title": "TeDS: Joint Learning of Diachronic and Synchronic Perspectives in Quaternion Space for Temporal Knowledge Graph Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=11id5ppGZ8": {
    "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tw81RElDpe": {
    "title": "Aequa: Fair Model Rewards in Collaborative Learning via Slimmable Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eIm0PQVu55": {
    "title": "QMamba: On First Exploration of Vision Mamba for Image Quality Assessment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W7phL2sNif": {
    "title": "Advancing Personalized Learning with Neural Collapse for Long-Tail Challenge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qwy6JIA8Os": {
    "title": "Scalable Attribute-Missing Graph Clustering via Neighborhood Differentiation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wEtNrpA8QH": {
    "title": "TtBA: Two-third Bridge Approach for Decision-Based Adversarial Attack",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Dt3nH5TZq": {
    "title": "CSV-Occ: Fusing Multi-frame Alignment for Occupancy Prediction with Temporal Cross State Space Model and Central Voting Mechanism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MNSW6U5zUA": {
    "title": "Impossible Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lNTHwNF8IH": {
    "title": "Hgformer: Hyperbolic Graph Transformer for Collaborative Filtering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sq5eL4jfsn": {
    "title": "Improving Continual Learning Performance and Efficiency with Auxiliary Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6QERrXMLP2": {
    "title": "Mixture of Experts Made Intrinsically Interpretable",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EkoFXfSauv": {
    "title": "Mitigating Plasticity Loss in Continual Reinforcement Learning by Reducing Churn",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b6VYI1Bvo2": {
    "title": "Piloting Structure-Based Drug Design via Modality-Specific Optimal Schedule",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m5EIuQxt0x": {
    "title": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YWLWUTtVF3": {
    "title": "Adaptive Message Passing: A General Framework to Mitigate Oversmoothing, Oversquashing, and Underreaching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3rWQlV3s1I": {
    "title": "Certified Unlearning for Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p2smPMRQae": {
    "title": "A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7tWpRX4Dxk": {
    "title": "LineFlow: A Framework to Learn Active Control of Production Lines",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fh9XlJnXb5": {
    "title": "Variance as a Catalyst: Efficient and Transferable Semantic Erasure Adversarial Attack for Customized Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OYbZWmNHwn": {
    "title": "SAH-Drive: A Scenario-Aware Hybrid Planner for Closed-Loop Vehicle Trajectory Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=URmGywyt59": {
    "title": "Gradient Boosting Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pFQ3MnyIT6": {
    "title": "S 2 FGL: Spatial Spectral Federated Graph Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eih5Gy3Pjt": {
    "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lTZEAqL1R1": {
    "title": "Rhomboid Tiling for Geometric Graph Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cKnzZrIJBR": {
    "title": "Learning Fused State Representations for Control from Multi-View Observations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BHwWLeXDYF": {
    "title": "Safety Reasoning with Guidelines",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ObGn4e1IS": {
    "title": "Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UuvcoHGivw": {
    "title": "ConText: Driving In-context Learning for Text Removal and Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G7fZ0GTUb6": {
    "title": "Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9W3CSuHoFA": {
    "title": "Expert Race: A Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e6vlZdLlIK": {
    "title": "MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hLvWwRZkok": {
    "title": "HPS: Hard Preference Sampling for Human Preference Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=038rEwbChh": {
    "title": "Semi-Supervised Blind Quality Assessment with Confidence-quantifiable Pseudo-label Learning for Authentic Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tiYNbKwL7T": {
    "title": "RUN: Reversible Unfolding Network for Concealed Object Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lC40m2jjUO": {
    "title": "Test-time Adaptation on Graphs via Adaptive Subgraph-based Selection and Regularized Prototypes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gbeZKej40m": {
    "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WSGpXQRUtH": {
    "title": "Fast Incomplete Multi-view Clustering by Flexible Anchor Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xR9msNaREW": {
    "title": "Whoever Started the interference Should End It: Guiding Data-Free Model Merging via Task Vectors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OKWlVPHeW1": {
    "title": "Boosting Virtual Agent Learning and Reasoning: A Step-Wise, Multi-Dimensional, and Generalist Reward Model with Benchmark",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cFNgLub5tQ": {
    "title": "A Computationally Efficient Algorithm for Infinite-Horizon Average-Reward Linear MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=agtwOsnLUB": {
    "title": "ROME is Forged in Adversity: Robust Distilled Datasets via Information Bottleneck",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WfMMQLop6M": {
    "title": "Open-Det: An Efficient Learning Framework for Open-Ended Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FmHnhDLlOX": {
    "title": "Graph Generative Pre-trained Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D4Y71nbGRg": {
    "title": "CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for Text-to-Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U5nRMOs8Ed": {
    "title": "SE(3)-Equivariant Diffusion Policy in Spherical Fourier Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bhTBirS0qi": {
    "title": "Probing Visual Language Priors in VLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8AGdUCdDyI": {
    "title": "Controlling Neural Collapse Enhances Out-of-Distribution Detection and Transfer Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Sl6Ex7Vmo": {
    "title": "M³HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9vwkmxLZVL": {
    "title": "Learning Distribution-wise Control in Representation Space for Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qn6yZb5iLC": {
    "title": "Generalization Bounds via Meta-Learned Model Representations: PAC-Bayes and Sample Compression Hypernetworks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pqHWzviKKN": {
    "title": "On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention for Long-Context LLM Serving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yTWqL3XHCC": {
    "title": "Promoting Ensemble Diversity with Interactive Bayesian Distributional Robustness for Fine-tuning Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9xGSeVolcN": {
    "title": "OneForecast: A Universal Framework for Global and Regional Weather Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CzSNEvCckO": {
    "title": "Deep Sturm–Liouville: From Sample-Based to 1D Regularization with Learnable Orthogonal Basis Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XfuNPjDjoG": {
    "title": "Quadruple Attention in Many-body Systems for Accurate Molecular Property Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CIoBEB17FT": {
    "title": "Empower Structure-Based Molecule Optimization with Gradient Guided Bayesian Flow Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S9unJQditt": {
    "title": "Directly Forecasting Belief for Reinforcement Learning with Delays",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iMp2vzs4f8": {
    "title": "High Dynamic Range Novel View Synthesis with Single Exposure",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f8uPzuMNDq": {
    "title": "Efficient Generative Modeling with Residual Vector Quantization-Based Tokens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LBE7HKLQDa": {
    "title": "FreeMesh: Boosting Mesh Generation with Coordinates Merging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZDsbK7Xx4n": {
    "title": "ERICT: Enhancing Robustness by Identifying Concept Tokens in Zero-Shot Vision Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kUagmiIN5x": {
    "title": "Stray Intrusive Outliers-Based Feature Selection on Intra-Class Asymmetric Instance Distribution or Multiple High-Density Clusters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CiKWAofp7n": {
    "title": "A Comprehensive Framework for Analyzing the Convergence of Adam: Bridging the Gap with SGD",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w6Ds32QkQd": {
    "title": "Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u6k5y3FDW1": {
    "title": "Optimal Information Retention for Time-Series Explanations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zuXCQRhbl4": {
    "title": "Visual Attention Never Fades: Selective Progressive Attention ReCalibration for Detailed Image Captioning in Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=On7Ffq7Qde": {
    "title": "An Efficient Private GPT Never Autoregressively Decodes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ulCAfxJzc": {
    "title": "A Simple Model of Inference Scaling Laws",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=REnIf3dCsI": {
    "title": "Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4y8H1sGK4Z": {
    "title": "Open Your Eyes: Vision Enhances Message Passing Neural Networks in Link Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OHPBPveXdg": {
    "title": "WyckoffDiff -- A Generative Diffusion Model for Crystal Symmetry",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cLxLgpMd2v": {
    "title": "Cape: Context-Aware Prompt Perturbation Mechanism with Differential Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VLVS8c53vZ": {
    "title": "Info-Coevolution: An Efficient Framework for Data Model Coevolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=muZEsBQzlA": {
    "title": "Tensorized Multi-View Multi-Label Classification via Laplace Tensor Rank",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QIL44dSUPo": {
    "title": "Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJCROTFMEM": {
    "title": "EvFocus: Learning to Reconstruct Sharp Images from Out-of-Focus Event Streams",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JVDFkVf4QY": {
    "title": "Spherical-Nested Diffusion Model for Panoramic Image Outpainting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pdNtji3ktF": {
    "title": "Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents via Offline Hierarchical Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YGjd2xw98G": {
    "title": "RobustLight: Improving Robustness via Diffusion Reinforcement Learning for Traffic Signal Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wP8meX6uJC": {
    "title": "EnsLoss: Stochastic Calibrated Loss Ensembles for Preventing Overfitting in Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zNQnx4H03e": {
    "title": "Quantum Speedup for Hypergraph Sparsification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Py2KmXaRmi": {
    "title": "Trajectory World Models for Heterogeneous Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZWsjNcUDIs": {
    "title": "Concept-Based Unsupervised Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SUxq4HeIAd": {
    "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jp988ELppQ": {
    "title": "A General Framework for Inference-time Scaling and Steering of Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JCUsWrwkKw": {
    "title": "Diff-MoE: Diffusion Transformer with Time-Aware and Space-Adaptive Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UMqNQEPNT3": {
    "title": "Steer LLM Latents for Hallucination Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YZvefQVLJI": {
    "title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e24CueVty2": {
    "title": "Temporal Query Network for Efficient Multivariate Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5UzT2VfPws": {
    "title": "Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6ojzpDczIY": {
    "title": "Global Optimization with a Power-Transformed Objective and Gaussian Smoothing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LFF7kUQ5Rp": {
    "title": "Accelerating PDE-Constrained Optimization by the Derivative of Neural Operators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8V6MEtSnlR": {
    "title": "Beyond Zero Initialization: Investigating the Impact of Non-Zero Initialization on LoRA Fine-Tuning Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aooq3tQIX9": {
    "title": "The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bEqMmGu6qg": {
    "title": "Language Models as Implicit Tree Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K3QoqobsiR": {
    "title": "IntLoRA: Integral Low-rank Adaptation of Quantized Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MCqlamfhAy": {
    "title": "Closed-Loop Long-Horizon Robotic Planning via Equilibrium Sequence Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9c4YYoBS4N": {
    "title": "Towards the Causal Complete Cause of Multi-Modal Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ccUNMIbpcf": {
    "title": "VIP: Vision Instructed Pre-training for Robotic Manipulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zcx7jqUZg5": {
    "title": "LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8zsMorEU8U": {
    "title": "X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sbbyCB39HN": {
    "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WOyOtaO6lQ": {
    "title": "Compute or Load KV Cache? Why Not Both?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fyLsLTi1rE": {
    "title": "Differentiable Structure Learning with Ancestral Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tbAA3eXKWq": {
    "title": "Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OcqbkROe8J": {
    "title": "M+: Extending MemoryLLM with Scalable Long-Term Memory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ekGV1VLuwB": {
    "title": "Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PMKAu92gMT": {
    "title": "SSHR: More Secure Generative Steganography with High-Quality Revealed Secret Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TO4q3FJEG1": {
    "title": "Locality Preserving Markovian Transition for Instance Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzle2Jnt72": {
    "title": "Toward a Unified Theory of Gradient Descent under Generalized Smoothness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XCLZgbm99O": {
    "title": "FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QtMcq04xXd": {
    "title": "ALMTokenizer: A Low-bitrate and Semantic-rich Audio Codec Tokenizer for Audio Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sugs65XoGg": {
    "title": "Demonstration Selection for In-Context Learning via Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0fLoGPTLo1": {
    "title": "Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MLiR9LS5PW": {
    "title": "Mirror, Mirror of the Flow: How Does Regularization Shape Implicit Bias?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DTL79Vl0qy": {
    "title": "Hypo3D: Exploring Hypothetical Reasoning in 3D",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Toy3nwPhk4": {
    "title": "HyperNear: Unnoticeable Node Injection Attacks on Hypergraph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NxVuIhW2v1": {
    "title": "TinyMIG: Transferring Generalization from Vision Foundation Models to Single-Domain Medical Imaging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=beBNOZP5Tk": {
    "title": "Preference-CFR: Beyond Nash Equilibrium for Better Game Strategies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oUApObDC1e": {
    "title": "Differentiable Solver Search for Fast Diffusion Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Gs48lcx49": {
    "title": "Control and Realism: Best of Both Worlds in Layout-to-Image without Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQegZj99sk": {
    "title": "AdaWorld: Learning Adaptable World Models with Latent Actions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vkmi3jZtYG": {
    "title": "VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UlprLwWYKP": {
    "title": "OmniArch: Building Foundation Model for Scientific Computing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LNTepdaSLC": {
    "title": "Clipping Improves Adam-Norm and AdaGrad-Norm when the Noise Is Heavy-Tailed",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pyC5idZvjT": {
    "title": "Learning with Selectively Labeled Data from Multiple Decision-makers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dewR2augg2": {
    "title": "FlexiReID: Adaptive Mixture of Expert for Multi-Modal Person Re-Identification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NEBa0bs5LR": {
    "title": "DS-VLM: Diffusion Supervision Vision Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lwn9T8Yoti": {
    "title": "Avoiding Catastrophe in Online Learning by Asking for Help",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4yBnUokU2v": {
    "title": "Demystifying Singular Defects in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bjDKZ3Roax": {
    "title": "How Much Can Transfer? BRIDGE: Bounded Multi-Domain Graph Foundation Model with Generalization Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lCk4PZto8T": {
    "title": "Focal-SAM: Focal Sharpness-Aware Minimization for Long-Tailed Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K4GaB4fdIq": {
    "title": "Test-Time Adaptation for Online Vision-Language Navigation with Feedback-based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IkhJApkJQ3": {
    "title": "3D Question Answering via only 2D Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mGOugCZlAq": {
    "title": "Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EAjhGr1Oeo": {
    "title": "GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xtxCM4XZ82": {
    "title": "FlexiClip: Locality-Preserving Free-Form Character Animation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B4TyAILcE4": {
    "title": "FRUGAL: Memory-Efficient Optimization by Reducing State Overhead for Scalable Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WvanLeuEAC": {
    "title": "Enhancing Statistical Validity and Power in Hybrid Controlled Trials: A Randomization Inference Approach with Conformal Selective Borrowing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iLm1cNnjzk": {
    "title": "ENSUR: Equitable and Statistically Unbiased Recommendation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0BsEMZDzWr": {
    "title": "COKE: Core Kernel for More Efficient Approximation of Kernel Weights in Multiple Kernel Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EFMAjY8dVZ": {
    "title": "Bifurcate then Alienate: Incomplete Multi-view Clustering via Coupled Distribution Learning with Linear Overhead",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6WBca6adox": {
    "title": "Large Language Models to Diffusion Finetuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JaNKGPkDpw": {
    "title": "Learning Event Completeness for Weakly Supervised Video Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aTyQjpsaB4": {
    "title": "SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fjXcRSfyIV": {
    "title": "BDC-CLIP: Brownian Distance Covariance for Adapting CLIP to Action Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eLTPkGGHum": {
    "title": "Provably Efficient Exploration in Inverse Constrained Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZZvTc92dYQ": {
    "title": "EvoMesh: Adaptive Physical Simulation with Hierarchical Graph Evolutions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CoDoOe6Xab": {
    "title": "Gaussian Mixture Flow Matching Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iT8qIzNSAl": {
    "title": "Annealing Flow Generative Models Towards Sampling High-Dimensional and Multi-Modal Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zup6F3MwQO": {
    "title": "In-Context Learning as Conditioned Associative Memory Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xkV3uCQtJm": {
    "title": "Nonparametric Modern Hopfield Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oyUiJmkD7H": {
    "title": "Fast and Low-Cost Genomic Foundation Models via Outlier Removal",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oRdfFS7xO5": {
    "title": "Contrastive Private Data Synthesis via Weighted Multi-PLM Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=64mHSb9DlQ": {
    "title": "Parameter-Efficient Fine-Tuning of State Space Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=enwylUTeuP": {
    "title": "Deterministic Sparse Fourier Transform for Continuous Signals with Frequency Gap",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GvV9MJs268": {
    "title": "Structure-informed Risk Minimization for Robust Ensemble Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LLk1qYQatJ": {
    "title": "LETS Forecast: Learning Embedology for Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Roc5O1ECEt": {
    "title": "Customizing the Inductive Biases of Softmax Attention using Structured Matrices",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mNxjoMiFFf": {
    "title": "Competing Bandits in Matching Markets via Super Stability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e9YosoGkYg": {
    "title": "Safety Alignment Can Be Not Superficial With Explicit Safety Signals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T8VLY1KuOz": {
    "title": "A Rescaling-Invariant Lipschitz Bound Based on Path-Metrics for Modern ReLU Network Parameterizations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6mQv4fnsj0": {
    "title": "HybridGS: High-Efficiency Gaussian Splatting Data Compression using Dual-Channel Sparse Representation and Point Cloud Encoder",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bUGdGaNFhi": {
    "title": "TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AKvy9a4jho": {
    "title": "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YqerkYZfvu": {
    "title": "Perception in Reflection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uDTyhcOd9l": {
    "title": "Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=73EwiOrN8W": {
    "title": "Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZPqj44Knpd": {
    "title": "Physics Aware Neural Networks for Unsupervised Binding Energy Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yIfCq03hsM": {
    "title": "Convex Markov Games: A New Frontier for Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LNfiFWccxn": {
    "title": "Offline Opponent Modeling with Truncated Q-driven Instant Policy Refinement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vCtyqomphj": {
    "title": "A Chaotic Dynamics Framework Inspired by Dorsal Stream for Event Signal Processing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I8DVh2jnEA": {
    "title": "FairPFN: A Tabular Foundation Model for Causal Fairness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FSq95c0wb3": {
    "title": "Pfeife: Automatic Pipeline Parallelism for PyTorch",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pYMZQtkp3F": {
    "title": "Inductive Gradient Adjustment for Spectral Bias in Implicit Neural Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m7kvl1es0S": {
    "title": "SAND: One-Shot Feature Selection with Additive Noise Distortion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Arepl4R86m": {
    "title": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning Attack",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g6XtPA0DQp": {
    "title": "Guided Structural Inference: Leveraging Priors with Soft Gating Mechanisms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MAHPZNduS4": {
    "title": "Core Context Aware Transformers for Long Context Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UWTz4ai3FZ": {
    "title": "LLM Enhancers for GNNs: An Analysis from the Perspective of Causal Mechanism Identification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DDvInyGVuz": {
    "title": "Visual Abstraction: A Plug-and-Play Approach for Text-Visual Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PjadKnUson": {
    "title": "Towards Trustworthy Federated Learning with Untrusted Participants",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y9JV6VANYp": {
    "title": "Contrastive Visual Data Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0epuNvt5Dj": {
    "title": "MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dUq579lefp": {
    "title": "Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TQtUTC3eKv": {
    "title": "Unlocking the Power of SAM 2 for Few-Shot Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZWTb5UBdpG": {
    "title": "Approximation to Smooth Functions by Low-Rank Swish Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQIrsaIQdn": {
    "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f9xsNQ8oSd": {
    "title": "Rethinking the Temperature for Federated Heterogeneous Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Ip6fihKbc": {
    "title": "Can Transformers Learn Full Bayesian Inference in Context?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GB9XiKIwfp": {
    "title": "Learning Invariant Causal Mechanism from Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yh9vxlxnjA": {
    "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal Dictionaries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vHr9cdeFfu": {
    "title": "S2-Track: A Simple yet Strong Approach for End-to-End 3D Multi-Object Tracking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b5h60xQnzM": {
    "title": "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q0JaH6Ukqb": {
    "title": "Beyond Self-Interest: How Group Strategies Reshape Content Creation in Recommendation Platforms?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zuOKPHHfEr": {
    "title": "XAttnMark: Learning Robust Audio Watermarking with Cross-Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oJQWvsStNh": {
    "title": "Stable Fair Graph Representation Learning with Lipschitz Constraint",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AyHefRhiZR": {
    "title": "Distributed Nonparametric Estimation: from Sparse to Dense Samples per Terminal",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0OVCcpLNGI": {
    "title": "Learning multivariate Gaussians with imperfect advice",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r3ZLefVUMO": {
    "title": "Why Is There a Tumor?\": Tell Me the Reason, Show Me the Evidence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wz2T778EKJ": {
    "title": "Faster and Stronger: When ANN-SNN Conversion Meets Parallel Spiking Calculation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aDVzd958YY": {
    "title": "Efficient Motion Prompt Learning for Robust Visual Tracking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1SMcxxQiSL": {
    "title": "Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ifnxXCCEiM": {
    "title": "Radio: Rate–Distortion Optimization for Large Language Model Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fiv2M4P5vk": {
    "title": "Falcon: Fast Visuomotor Policies via Partial Denoising",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v7ebdKPTnG": {
    "title": "CUPS: Improving Human Pose-Shape Estimators with Conformalized Deep Uncertainty",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fwIJJGhnBD": {
    "title": "MIPT: Multilevel Informed Prompt Tuning for Robust Molecular Property Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RP8K523PyW": {
    "title": "Neural Representational Consistency Emerges from Probabilistic Neural-Behavioral Representation Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LWQ4zu9SdQ": {
    "title": "KAN-AD: Time Series Anomaly Detection with Kolmogorov–Arnold Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=toTYcYf4qI": {
    "title": "Quantum Algorithms for Finite-horizon Markov Decision Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BMJ3pyYxu2": {
    "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=phVWcUSGYP": {
    "title": "Matryoshka Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JFafMSAjUm": {
    "title": "FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XXh3zmw2Uy": {
    "title": "RelGNN: Composite Message Passing for Relational Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oqPcOMafOF": {
    "title": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pekYL20W3n": {
    "title": "When Data-Free Knowledge Distillation Meets Non-Transferable Teacher: Escaping Out-of-Distribution Trap is All You Need",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IaUJl5RCOu": {
    "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JMZ7mr19AK": {
    "title": "Test-Time Canonicalization by Foundation Models for Robust Perception",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GA7RDBGs6S": {
    "title": "A Parametric Contextual Online Learning Theory of Brokerage",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RBZpAa27ls": {
    "title": "No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3NLNmdheIi": {
    "title": "CellFlux: Simulating Cellular Morphology Changes via Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P1RMiAn4Tr": {
    "title": "Taming Diffusion for Dataset Distillation with High Representativeness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CVQwQp9V1D": {
    "title": "More Than Meets the Eye: Enhancing Multi-Object Tracking Even with Prolonged Occlusions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0MpGi6IwZr": {
    "title": "CPCF: A Cross-Prompt Contrastive Framework for Referring Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9NmdppWbXi": {
    "title": "TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FQoy1Y1Hd8": {
    "title": "PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bf8862KaL1": {
    "title": "Asymmetric Decision-Making in Online Knowledge Distillation: Unifying Consensus and Divergence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vexHifrbJg": {
    "title": "Canonical Rank Adaptation: An Efficient Fine-Tuning Strategy for Vision Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hcjXy7kLFd": {
    "title": "iDPA: Instance Decoupled Prompt Attention for Incremental Medical Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EuDlvBqcSm": {
    "title": "Emotional Face-to-Speech",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W7dN3SQKkH": {
    "title": "Consensus Based Stochastic Optimal Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EqoKRSR5Pa": {
    "title": "Modeling Multi-Task Model Merging as Adaptive Projective Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GMwKpJ9TiR": {
    "title": "Risk and cross validation in ridge regression with correlated samples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vvBAZJh2nQ": {
    "title": "EraseAnything: Enabling Concept Erasure in Rectified Flow Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0gX9f5xTrD": {
    "title": "Exploring Vision Semantic Prompt for Efficient Point Cloud Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N0jZKugEuS": {
    "title": "Trust-Region Twisted Policy Improvement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQx66EJUu0": {
    "title": "SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o9jtS7HupI": {
    "title": "HyperIV: Real-time Implied Volatility Smoothing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V7JPraxi5j": {
    "title": "UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EmLiyZGvrR": {
    "title": "UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BeoXADC9PW": {
    "title": "RBench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c4EEnWu9FE": {
    "title": "LLaVA-ReID: Selective Multi-image Questioner for Interactive Person Re-Identification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IYOPfJwduh": {
    "title": "ExLM: Rethinking the Impact of [MASK] Tokens in Masked Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b836TGkRSw": {
    "title": "The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OqOzcGBsqB": {
    "title": "A Theory for Conditional Generative Modeling on Multiple Data Sources",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4d2dwJN4v1": {
    "title": "Random Registers for Cross-Domain Few-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j7tKsPQotr": {
    "title": "Clustering Properties of Self-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5PiZevq9fY": {
    "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric Reduction and Restoration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oK4ot2wAJU": {
    "title": "Epsilon-VAE: Denoising as Visual Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iPDw3O6u3T": {
    "title": "ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HBa4FcegJY": {
    "title": "Dendritic Localized Learning: Toward Biologically Plausible Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AgAn6xtwWO": {
    "title": "MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4skvLszxZ1": {
    "title": "Ultra-Resolution Adaptation with Ease",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8tE4DkYUex": {
    "title": "From Spectrum-free towards Baseline-view-free: Double-track Proximity Driven Multi-view Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qfgtBeBLsD": {
    "title": "Exactly Tight Information-theoretic Generalization Bounds via Binary Jensen-Shannon Divergence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yV9MbLCt30": {
    "title": "BiMaCoSR: Binary One-Step Diffusion Model Leveraging Flexible Matrix Compression for Real Super-Resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EhStXG4dCS": {
    "title": "Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YrCvW1Hx7g": {
    "title": "BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9OjTHu4sea": {
    "title": "Action Dubber: Timing Audible Actions via Inflectional Flow",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JPTpUGJDGY": {
    "title": "Visual Autoregressive Modeling for Image Super-Resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=si1iynl5y7": {
    "title": "Gamma Distribution PCA-Enhanced Feature Learning for Angle-Robust SAR Target Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=83VDYpSd8R": {
    "title": "Scaling Laws for Floating–Point Quantization Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JD45c59Q09": {
    "title": "Enhancing Target-unspecific Tasks through a Features Matrix",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oaMDLJezM4": {
    "title": "Tackling View-Dependent Semantics in 3D Language Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pnZq5FojHH": {
    "title": "Rethinking Time Encoding via Learnable Transformation Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ziezViPoN1": {
    "title": "Delta Decompression for MoE-based LLMs Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=acJ3vdFljk": {
    "title": "MoE-SVD: Structured Mixture-of-Experts LLMs Compression via Singular Value Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=anSWDvJm8v": {
    "title": "SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KMaBXMWsBM": {
    "title": "COExpander: Adaptive Solution Expansion for Combinatorial Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gU0MwTihsn": {
    "title": "BECAME: Bayesian Continual Learning with Adaptive Model Merging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6PVUY0FhbG": {
    "title": "Targeted control of fast prototyping through domain-specific interface",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bxHgOYSQNe": {
    "title": "CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rsegDeiPt9": {
    "title": "Vision Graph Prompting via Semantic Low-Rank Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4SsNofUQf1": {
    "title": "GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OhgLhXgr27": {
    "title": "Token Coordinated Prompt Attention is Needed for Visual Prompting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QWCdBzLOsk": {
    "title": "Componential Prompt-Knowledge Alignment for Domain Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TuvDxubEfE": {
    "title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o2waQ5S2ty": {
    "title": "How to Move Your Dragon: Text-to-Motion Synthesis for Large-Vocabulary Objects",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Aycl60Rhbt": {
    "title": "Teaching Physical Awareness to LLMs through Sounds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BtRn4eayfE": {
    "title": "Context Matters: Query-aware Dynamic Long Sequence Modeling of Gigapixel Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V7VnjJxBlg": {
    "title": "Sub-Sequential Physics-Informed Learning with State Space Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L8hYdTQVcs": {
    "title": "Policy Filtration for RLHF to Mitigate Noise in Reward Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  }
}