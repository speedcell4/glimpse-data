{
  "https://www.isca-speech.org/archive/interspeech_2019/tokuda19_interspeech.html": {
    "title": "Statistical Approach to Speech Synthesis: Past, Present and Future",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fa87441f84e6fe211195648b103707bbe9bd2367",
    "semantic_title": "statistical approach to speech synthesis: past, present and future",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19_interspeech.html": {
    "title": "Advances in Automatic Speech Recognition for Child Speech Using Factored Time Delay Neural Network",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) has shown huge advances in adult speech; however, when the models are tested on child speech, the performance does not achieve satisfactory word error rates (WER). This is mainly due to the high variance in acoustic features of child speech and the lack of clean, labeled corpora. We apply the factored time delay neural network (TDNN-F) to the child speech domain, finding that it yields better performance. To enable our models to handle the different noise conditions and extremely small corpora, we augment the original training data by adding noise and reverberation. Compared with conventional GMM-HMM and TDNN systems, TDNN-F does better on two widely accessible corpora: CMU Kids and CSLU Kids, and on the combination of these two. Our system achieves a 26% relative improvement in WER",
    "checked": true,
    "id": "beeaa7417e7818f737c2958550757735982fc49b",
    "semantic_title": "advances in automatic speech recognition for child speech using factored time delay neural network",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yeung19_interspeech.html": {
    "title": "A Frequency Normalization Technique for Kindergarten Speech Recognition Inspired by the Role of fo in Vowel Perception",
    "volume": "main",
    "abstract": "Accurate automatic speech recognition (ASR) of kindergarten speech is particularly important as this age group may benefit the most from voice-based educational tools. Due to the lack of young child speech data, kindergarten ASR systems often are trained using older child or adult speech. This study proposes a fundamental frequency (f )-based normalization technique to reduce the spectral mismatch between kindergarten and older child speech. The technique is based on the tonotopic distances between formants and f developed to model vowel perception. This proposed procedure only relies on the computation of median f across an utterance. Tonotopic distances for vowel perception were reformulated as a linear relationship between formants and f to provide an effective approach for frequency normalization. This reformulation was verified by examining the formants and f of child vowel productions. A 208-word ASR experiment using older child speech for training and kindergarten speech for testing was performed to examine the effectiveness of the proposed technique against piecewise vocal tract length, F3-based, and subglottal resonance normalization techniques. Results suggest that the proposed technique either has performance advantages or requires the computation of fewer parameters",
    "checked": true,
    "id": "b521d1c626053e5252bc0fcd3e16d63efde5ba3c",
    "semantic_title": "a frequency normalization technique for kindergarten speech recognition inspired by the role of fo in vowel perception",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gale19_interspeech.html": {
    "title": "Improving ASR Systems for Children with Autism and Language Impairment Using Domain-Focused DNN Transfer Techniques",
    "volume": "main",
    "abstract": "This study explores building and improving an automatic speech recognition (ASR) system for children aged 6–9 years and diagnosed with autism spectrum disorder (ASD), language impairment (LI), or both. Working with only 1.5 hours of target data in which children perform the Clinical Evaluation of Language Fundamentals Recalling Sentences task, we apply deep neural network (DNN) weight transfer techniques to adapt a large DNN model trained on the LibriSpeech corpus of adult speech. To begin, we aim to find the best proportional training rates of the DNN layers. Our best configuration yields a 29.38% word error rate (WER). Using this configuration, we explore the effects of quantity and similarity of data augmentation in transfer learning. We augment our training with portions of the OGI Kids' Corpus, adding 4.6 hours of typically developing speakers aged kindergarten through 3 grade. We find that 2 grade data alone — approximately the mean age of the target data — outperforms other grades and all the sets combined. Doubling the data for 1 , 2 , and 3 grade, we again compare each grade as well as pairs of grades. We find the combination of 1 and 2 grade performs best at a 26.21% WER",
    "checked": true,
    "id": "9826fb6c64b76d5d0809a7c6ae5914342c782930",
    "semantic_title": "improving asr systems for children with autism and language impairment using domain-focused dnn transfer techniques",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ribeiro19_interspeech.html": {
    "title": "Ultrasound Tongue Imaging for Diarization and Alignment of Child Speech Therapy Sessions",
    "volume": "main",
    "abstract": "We investigate the automatic processing of child speech therapy sessions using ultrasound visual biofeedback, with a specific focus on complementing acoustic features with ultrasound images of the tongue for the tasks of speaker diarization and time-alignment of target words. For speaker diarization, we propose an ultrasound-based time-domain signal which we call estimated tongue activity. For word-alignment, we augment an acoustic model with low-dimensional representations of ultrasound images of the tongue, learned by a convolutional neural network. We conduct our experiments using the Ultrasuite repository of ultrasound and speech recordings for child speech therapy sessions. For both tasks, we observe that systems augmented with ultrasound data outperform corresponding systems using only the audio signal",
    "checked": true,
    "id": "e940bdd66421ab3d0d26434798344f846553c35f",
    "semantic_title": "ultrasound tongue imaging for diarization and alignment of child speech therapy sessions",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/loukina19_interspeech.html": {
    "title": "Automated Estimation of Oral Reading Fluency During Summer Camp e-Book Reading with MyTurnToRead",
    "volume": "main",
    "abstract": "Use of speech technologies in the classroom is often limited by the inferior acoustic conditions as well as other factors that might affect the quality of the recordings. We describe MyTurnToRead, an e-book-based app designed to support an interleaved listening and reading experience, where the child takes turns reading aloud with a virtual partner. The child's reading turns are recorded, and processed by an automated speech analysis system in order to provide feedback or track improvement in reading skill. We describe the architecture of the speech processing back-end and evaluate system performance on the data collected in several summer camps where children used the app on consumer-grade devices as part of the camp programming. We show that while the quality of the audio recordings varies greatly, our estimates of student oral reading fluency are very good: for example, the correlation between ASR-based and transcription-based estimates of reading fluency at the speaker level is r=0.93. These are also highly correlated with an external measure of reading comprehension",
    "checked": true,
    "id": "4b6866be7c111007cf4cb949b36c2123cc7f285e",
    "semantic_title": "automated estimation of oral reading fluency during summer camp e-book reading with myturntoread",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lopes19_interspeech.html": {
    "title": "Sustained Vowel Game: A Computer Therapy Game for Children with Dysphonia",
    "volume": "main",
    "abstract": "Problems in vocal quality are common in 4 to 12-year-old children, which may affect their health as well as their social interactions and development process. The sustained vowel exercise is widely used by speech and language pathologists for the child's voice recovery and vocal re-education. Nonetheless, despite being an important voice exercise, it can be a monotonous and tedious activity for children. Here, we propose a computer therapy game that uses the sustained vowel exercise to motivate children on doing this exercise often. In addition, the game gives visual feedback on the child's performance, which helps the child understand how to improve the voice production. The game uses a vowel classification model learned with a support vector machine and Mel frequency cepstral coefficients. A user test with 14 children showed that when using the game, children achieve longer phonation times than without the game. Also, it shows that the visual feedback helps and motivates children on improving their sustained vowel productions",
    "checked": true,
    "id": "594c6449ddd016ae5ac10079546d39b518cee779",
    "semantic_title": "sustained vowel game: a computer therapy game for children with dysphonia",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/esposito19_interspeech.html": {
    "title": "The Dependability of Voice on Elders' Acceptance of Humanoid Agents",
    "volume": "main",
    "abstract": "The research on ambient assistive technology is concerned with features humanoid agents should show in order to gain user acceptance. However, differently aged groups may have different requirements. This paper is particularly focused on agent's voice preferences among elders, young adults, and adolescents To this aim 316 users organized in groups of 45/46 subjects of which 3 groups of elders (65+ years old), 2 of young adults (aged between 22–35 years), and 2 of adolescents (aged between 14–16 years) were recruited and administered the Virtual Agent Acceptance Questionnaire (VAAQ), after watching video-clips of mute and speaking agents, in order to test their preferences in terms of willingness to interact, pragmatic and hedonic qualities, and attractiveness, of proposed speaking and mute agents. In addition, the elders were also tested on listening only the agent's. The results suggest that voice is primary for getting elder's acceptance of virtual humanoid agents in contrast to young adults and adolescents which accept equally well either mute or speaking agents",
    "checked": true,
    "id": "7e956e27098fe50c55e709ed2ba4247ac24f8f5e",
    "semantic_title": "the dependability of voice on elders' acceptance of humanoid agents",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niebuhr19_interspeech.html": {
    "title": "God as Interlocutor — Real or Imaginary? Prosodic Markers of Dialogue Speech and Expected Efficacy in Spoken Prayer",
    "volume": "main",
    "abstract": "We analyze the phonetic correlates of petitionary prayer in 22 Christian practitioners. Our aim is to examine if praying is characterized by prosodic markers of dialogue speech and expected efficacy. Three similar conditions are compared; 1) requests to God, 2) requests to a human recipient, 3) requests to an imaginary person. We find that making requests to God is clearly distinguishable from making requests to both human and imaginary interlocutors. Requests to God are, unlike requests to an imaginary person, characterized by markers of dialogue speech (as opposed to monologue speech), including, a higher f0 level, a larger f0 range, and a slower speaking rate. In addition, requests to God differ from those made to both human and imaginary persons in markers of expected efficacy on the part of the speaker. These markers are related to a more careful speech production, including almost complete lack of hesitations, more pauses, and a much longer speaking time",
    "checked": false,
    "id": "8608531f9219f0f0b8ed895c26a9dfa3bd95cb17",
    "semantic_title": "god as interlocutor - real or imaginary? prosodic markers of dialogue speech and expected efficacy in spoken prayer",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cohn19_interspeech.html": {
    "title": "Expressiveness Influences Human Vocal Alignment Toward voice-AI",
    "volume": "main",
    "abstract": "This study explores whether people align to expressive speech spoken by a voice-activated artificially intelligent device (voice-AI), specifically Amazon's Alexa. Participants shadowed words produced by the Alexa voice in two acoustically distinct conditions: \"regular\" and \"expressive\", containing more exaggerated pitch contours and longer word durations. Another group of participants rated the shadowed items, in an AXB perceptual similarity task, as an assessment of overall degree of vocal alignment. Results show greater vocal alignment toward expressive speech produced by the Alexa voice and, furthermore, systematic variation based on speaker gender. Overall, these findings have applications to the field of affective computing in understanding human responses to synthesized emotional expressiveness",
    "checked": true,
    "id": "7499570fd0ddc4fe1da060aeb8a5fcec15f33755",
    "semantic_title": "expressiveness influences human vocal alignment toward voice-ai",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lai19_interspeech.html": {
    "title": "Detecting Topic-Oriented Speaker Stance in Conversational Speech",
    "volume": "main",
    "abstract": "Being able to detect topics and speaker stances in conversations is a key requirement for developing spoken language understanding systems that are personalized and adaptive. In this work, we explore how topic-oriented speaker stance is expressed in conversational speech. To do this, we present a new set of topic and stance annotations of the CallHome corpus of spontaneous dialogues. Specifically, we focus on six stances — positivity, certainty, surprise, amusement, interest, and comfort — which are useful for characterizing important aspects of a conversation, such as whether a conversation is going well or not. Based on this, we investigate the use of neural network models for automatically detecting speaker stance from speech in multi-turn, multi-speaker contexts. In particular, we examine how performance changes depending on how input feature representations are constructed and how this is related to dialogue structure. Our experiments show that incorporating both lexical and acoustic features is beneficial for stance detection. However, we observe variation in whether using hierarchical models for encoding lexical and acoustic information improves performance, suggesting that some aspects of speaker stance are expressed more locally than others. Overall, our findings highlight the importance of modelling interaction dynamics and non-lexical content for stance detection",
    "checked": true,
    "id": "ab76b65badae8872ce191ea6173d0fdf1f1c3536",
    "semantic_title": "detecting topic-oriented speaker stance in conversational speech",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sebastian19_interspeech.html": {
    "title": "Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts",
    "volume": "main",
    "abstract": "In human perception and understanding, a number of different and complementary cues are adopted according to different modalities. Various emotional states in communication between humans reflect this variety of cues across modalities. Recent developments in multi-modal emotion recognition utilize deep-learning techniques to achieve remarkable performances, with models based on different features suitable for text, audio and vision. This work focuses on cross-modal fusion techniques over deep learning models for emotion detection from spoken audio and corresponding transcripts We investigate the use of long short-term memory (LSTM) recurrent neural network (RNN) with pre-trained word embedding for text-based emotion recognition and convolutional neural network (CNN) with utterance-level descriptors for emotion recognition from speech. Various fusion strategies are adopted on these models to yield an overall score for each of the emotional categories. Intra-modality dynamics for each emotion is captured in the neural network designed for the specific modality. Fusion techniques are employed to obtain the inter-modality dynamics. Speaker and session-independent experiments on IEMOCAP multi-modal emotion detection dataset show the effectiveness of the proposed approaches. This method yields state-of-the-art results for utterance-level emotion recognition based on speech and text",
    "checked": true,
    "id": "7b43738839277ba7123e8df056df983b79c14530",
    "semantic_title": "fusion techniques for utterance-level emotion recognition combining speech and transcripts",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rajwadi19_interspeech.html": {
    "title": "Explaining Sentiment Classification",
    "volume": "main",
    "abstract": "This paper presents a novel 1-D sentiment classifier trained on the benchmark IMDB dataset. The classifier is a 1-D convolutional neural network with repeated convolution and max pooling layers. The main contribution of this work is the demonstration of a deconvolution technique for 1-D convolutional neural networks that is agnostic to specific architecture types. This deconvolution technique enables text classification to be explained, a feature that is important for NLP-based decision support systems, as well as being an invaluable diagnostic tool",
    "checked": true,
    "id": "03849bdecfc720717d3953c8fe9f4ded437f1d1b",
    "semantic_title": "explaining sentiment classification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kleinlein19_interspeech.html": {
    "title": "Predicting Group-Level Skin Attention to Short Movies from Audio-Based LSTM-Mixture of Experts Models",
    "volume": "main",
    "abstract": "Electrodermal activity (EDA) is a psychophysiological indicator that can be considered a somatic marker of the emotional and attentional reaction of subjects towards stimuli like audiovisual content. EDA measurements are not biased by the cognitive process of giving an opinion or a score to characterize the subjective perception, and group-level EDA recordings integrate the reaction of an audience, thus reducing the signal noise. This paper contributes to the field of audience's attention prediction to video content, extending previous novel work on the use of EDA as ground truth for prediction algorithms. Videos are segmented into shorter clips attending to the audience's increasing or decreasing attention, and we process videos' audio waveform to extract meaningful aural embeddings from a VGGish model pretrained on the Audioset database. While previous similar work on attention level prediction using only audio accomplished 69.83% accuracy, we propose a Mixture of Experts approach to train a binary classifier that outperforms the main existing state-of-the-art approaches predicting increasing and decreasing attention levels with 81.76% accuracy. These results confirm the usefulness of providing acoustic features with a semantic significance, and the convenience of considering experts over partitions of the dataset in order to predict group-level attention from audio",
    "checked": true,
    "id": "13c30a3e0888f4d1af79c05fddaae06895380a4f",
    "semantic_title": "predicting group-level skin attention to short movies from audio-based lstm-mixture of experts models",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schluter19_interspeech.html": {
    "title": "Survey Talk: Modeling in Automatic Speech Recognition: Beyond Hidden Markov Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "17e5123e8954a077ab2e2e933b812eec5878a0ce",
    "semantic_title": "survey talk: modeling in automatic speech recognition: beyond hidden markov models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pham19_interspeech.html": {
    "title": "Very Deep Self-Attention Networks for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recently, end-to-end sequence-to-sequence models for speech recognition have gained significant interest in the research community. While previous architecture choices revolve around time-delay neural networks (TDNN) and long short-term memory (LSTM) recurrent neural networks, we propose to use self-attention via the Transformer architecture as an alternative. Our analysis shows that deep Transformer networks with high learning capacity are able to exceed performance from previous end-to-end approaches and even match the conventional hybrid systems. Moreover, we trained very deep models with up to 48 Transformer layers for both encoder and decoders combined with stochastic residual connections, which greatly improve generalizability and training efficiency. The resulting models outperform all previous end-to-end ASR approaches on the Switchboard benchmark. An ensemble of these models achieve 9.9% and 17.7% WER on Switchboard and CallHome test sets respectively. This finding brings our end-to-end models to competitive levels with previous hybrid systems. Further, with model ensembling the Transformers can outperform certain hybrid systems, which are more complicated in terms of both structure and training procedure",
    "checked": true,
    "id": "f2bb7e2f5a1afad5370159c15760c44df93c0438",
    "semantic_title": "very deep self-attention networks for end-to-end speech recognition",
    "citation_count": 143
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19_interspeech.html": {
    "title": "Jasper: An End-to-End Convolutional Neural Acoustic Model",
    "volume": "main",
    "abstract": "In this paper we report state-of-the-art results on LibriSpeech among end-to-end speech recognition models without any external training data. Our model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout, and residual connections. To improve training, we further introduce a new layer-wise optimizer called NovoGrad. Through experiments, we demonstrate that the proposed deep architecture performs as well or better than more complex choices. Our deepest Jasper variant uses 54 convolutional layers. With this architecture, we achieve 2.95% WER using a beam-search decoder with an external neural language model and 3.86% WER with a greedy decoder on LibriSpeech test-clean. We also report competitive results on Wall Street Journal and the Hub5'00 conversational evaluation datasets",
    "checked": true,
    "id": "d85b2af4f163383bbfa62b73d5f0b179868cc9a8",
    "semantic_title": "jasper: an end-to-end convolutional neural acoustic model",
    "citation_count": 203
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moritz19_interspeech.html": {
    "title": "Unidirectional Neural Network Architectures for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "In hybrid automatic speech recognition (ASR) systems, neural networks are used as acoustic models (AMs) to recognize phonemes that are composed to words and sentences using pronunciation dictionaries, hidden Markov models, and language models, which can be jointly represented by a weighted finite state transducer (WFST). The importance of capturing temporal context by an AM has been studied and discussed in prior work. In an end-to-end ASR system, however, all components are merged into a single neural network, i.e., the breakdown into an AM and the different parts of the WFST model is no longer possible. This implies that end-to-end neural network architectures have even stronger requirements for processing long contextual information. Bidirectional long short-term memory (BLSTM) neural networks have demonstrated state-of-the-art results in end-to-end ASR but are unsuitable for streaming applications. Latency-controlled BLSTMs account for this by limiting the future context seen by the backward directed recurrence using chunk-wise processing. In this paper, we propose two new unidirectional neural network architectures, the time-delay LSTM (TDLSTM) and the parallel time-delayed LSTM (PTDLSTM) streams, which both limit the processing latency to a fixed size and demonstrate significant improvements compared to prior art on a variety of ASR tasks",
    "checked": true,
    "id": "6aed9eedd5cb712c3d902152cb94f2f18ba4c729",
    "semantic_title": "unidirectional neural network architectures for end-to-end automatic speech recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/belinkov19_interspeech.html": {
    "title": "Analyzing Phonetic and Graphemic Representations in End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end neural network systems for automatic speech recognition (ASR) are trained from acoustic features to text transcriptions. In contrast to modular ASR systems, which contain separately-trained components for acoustic modeling, pronunciation lexicon, and language modeling, the end-to-end paradigm is both conceptually simpler and has the potential benefit of training the entire system on the end task. However, such neural network models are more opaque: it is not clear how to interpret the role of different parts of the network and what information it learns during training. In this paper, we analyze the learned internal representations in an end-to-end ASR model. We evaluate the representation quality in terms of several classification tasks, comparing phonemes and graphemes, as well as different articulatory features. We study two languages (English and Arabic) and three datasets, finding remarkable consistency in how different properties are represented in different layers of the deep neural network",
    "checked": true,
    "id": "facccce4f8d059d6156cf3ce536786eb43016939",
    "semantic_title": "analyzing phonetic and graphemic representations in end-to-end automatic speech recognition",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tawara19_interspeech.html": {
    "title": "Multi-Channel Speech Enhancement Using Time-Domain Convolutional Denoising Autoencoder",
    "volume": "main",
    "abstract": "This paper investigates the use of time-domain convolutional denoising autoencoders (TCDAEs) with multiple channels as a method of speech enhancement. In general, denoising autoencoders (DAEs), deep learning systems that map noise-corrupted into clean waveforms, have been shown to generate high-quality signals while working in the time domain without the intermediate stage of phase modeling. Convolutional DAEs are one of the popular structures which learns a mapping between noise-corrupted and clean waveforms with convolutional denoising autoencoder. Multi-channel signals for TCDAEs are promising because the different times of arrival of a signal can be directly processed with their convolutional structure, Up to this time, TCDAEs have only been applied to single-channel signals. This paper explorers the effectiveness of TCDAEs in a multi-channel configuration. A multi-channel TCDAEs are evaluated on multi-channel speech enhancement experiments, yielding significant improvement over single-channel DAEs in terms of signal-to-distortion ratio, perceptual evaluation of speech quality (PESQ), and word error rate",
    "checked": true,
    "id": "69161fc1b301054f7b4423085530e2dc6083255d",
    "semantic_title": "multi-channel speech enhancement using time-domain convolutional denoising autoencoder",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tesch19_interspeech.html": {
    "title": "On Nonlinear Spatial Filtering in Multichannel Speech Enhancement",
    "volume": "main",
    "abstract": "Using multiple microphones for speech enhancement allows for exploiting spatial information for improved performance. In most cases, the spatial filter is selected to be a linear function of the input as, for example, the minimum variance distortionless response (MVDR) beamformer. For non-Gaussian distributed noise, however, the minimum mean square error (MMSE) optimal spatial filter may be nonlinear Potentially, such nonlinear functional relationships could be learned by deep neural networks. However, the performance would depend on many parameters and the architecture of the neural network. Therefore, in this paper, we more generally analyze the potential benefit of nonlinear spatial filters as a function of the multivariate kurtosis of the noise distribution The results imply that using a nonlinear spatial filter is only worth the effort if the noise data follows a distribution with a multivariate kurtosis that is considerably higher than for a Gaussian. In this case, we report a performance difference of up to 2.6 dB segmental signal-to-noise ratio (SNR) improvement for artificial stationary noise. We observe an advantage of 1.2dB for the nonlinear spatial filter over the linear one even for real-world noise data from the CHiME-3 dataset given oracle data for parameter estimation",
    "checked": false,
    "id": "8945e5c963f77d663cf9832c31dcd2da0469ada9",
    "semantic_title": "nonlinear spatial filtering in multichannel speech enhancement",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/martindonas19_interspeech.html": {
    "title": "Multi-Channel Block-Online Source Extraction Based on Utterance Adaptation",
    "volume": "main",
    "abstract": "This paper deals with multi-channel speech recognition in scenarios with multiple speakers. Recently, the spectral characteristics of a target speaker, extracted from an adaptation utterance, have been used to guide a neural network mask estimator to focus on that speaker. In this work we present two variants of speaker-aware neural networks, which exploit both spectral and spatial information to allow better discrimination between target and interfering speakers. Thus, we introduce either a spatial pre-processing prior to the mask estimation or a spatial plus spectral speaker characterization block whose output is directly fed into the neural mask estimator. The target speaker's spectral and spatial signature is extracted from an adaptation utterance recorded at the beginning of a session. We further adapt the architecture for low-latency processing by means of block-online beamforming that recursively updates the signal statistics. Experimental results show that the additional spatial information clearly improves source extraction, in particular in the same-gender case, and that our proposal achieves state-of-the-art performance in terms of distortion reduction and recognition accuracy",
    "checked": true,
    "id": "396c65bbf4098d767586ea0210a8bfa0b829405b",
    "semantic_title": "multi-channel block-online source extraction based on utterance adaptation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bagheri19_interspeech.html": {
    "title": "Exploiting Multi-Channel Speech Presence Probability in Parametric Multi-Channel Wiener Filter",
    "volume": "main",
    "abstract": "In this paper, we present a practical implementation of the parametric multi-channel Wiener filter (PMWF) noise reduction algorithm. In particular, we extend on methods that incorporate the multi-channel speech presence probability (MC-SPP) in the PMWF derivation and its output. The use of the MC-SPP brings several advantages. Firstly, the MC-SPP allows for better estimates of noise and speech statistics, for which we derive a direct update of the inverse of the noise power spectral density (PSD). Secondly, the MC-SPP is used to control the trade-off parameter in PMWF which, with proper tuning, outperforms the traditional approach with a fixed trade-off parameter. Thirdly, the MC-SPP for each frequency-band is used to obtain the MMSE estimate of the desired speech signal at the output, where we control the maximum amount of noise reduction based on our application. Experimental results on a large number of simulated scenarios show significant benefits of employing MC-SPP in terms of SNR improvements and speech distortion",
    "checked": true,
    "id": "7eb423a18481d37e7d766e9b083a190effecc6a9",
    "semantic_title": "exploiting multi-channel speech presence probability in parametric multi-channel wiener filter",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/togami19_interspeech.html": {
    "title": "Variational Bayesian Multi-Channel Speech Dereverberation Under Noisy Environments with Probabilistic Convolutive Transfer Function",
    "volume": "main",
    "abstract": "In this paper, we propose a multi-channel speech dereverberation method which can reduce reverberation even when acoustic transfer functions (ATFs) are time varying under noisy environments. The microphone input signal is modeled as a convolutive mixture in a time-frequency domain so as to incorporate late reverberation whose tap length is longer than frame size of short term Fourier transform. To reduce reverberation effectively under the time-varying ATF conditions, the proposed method extends the deterministic convolutive transfer function (D-CTF) into a probabilistic convolutive transfer function (P-CTF). A variational Bayesian framework was applied to approximation of a joint posterior probability density functions of a speech source signal and the ATFs. Variational posterior probability density functions and the other parameters are iteratively updated so as to maximize an evidence lower bound (ELBO). Experimental results when the ATFs are time-varying and there is background noise showed that the proposed method can reduce reverberation more accurately than the Weighted Prediction error (WPE) and the Kalman-EM for dereverberation (KEMD)",
    "checked": true,
    "id": "96273401679e1d2da953e3367712c11fd17ee7f1",
    "semantic_title": "variational bayesian multi-channel speech dereverberation under noisy environments with probabilistic convolutive transfer function",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nakatani19_interspeech.html": {
    "title": "Simultaneous Denoising and Dereverberation for Low-Latency Applications Using Frame-by-Frame Online Unified Convolutional Beamformer",
    "volume": "main",
    "abstract": "This article presents frame-by-frame online processing algorithms for a Weighted Power minimization Distortionless response convolutional beamformer (WPD). The WPD unifies widely-used multichannel dereverberation and denoising methods, namely a weighted prediction error based dereverberation method (WPE) and a minimum power distortionless response beamformer (MPDR) into a single convolutional beamformer, and achieves simultaneous dereverberation and denoising based on maximum likelihood estimation. We derive two different online algorithms, one based on frame-by-frame recursive updating of the spatio-temporal covariance matrix of the captured signal, and the other on recursive least square estimation of the convolutional beamformer. In addition, for both algorithms, the desired signal's relative transfer function (RTF) is estimated by online processing using a neural network based online mask estimation. Experiments using the REVERB challenge dataset show the effectiveness of both algorithms in terms of objective speech enhancement measures and automatic speech recognition (ASR) performance",
    "checked": true,
    "id": "460ecb228570a9fbbb12d0cdfa4670a0110e3870",
    "semantic_title": "simultaneous denoising and dereverberation for low-latency applications using frame-by-frame online unified convolutional beamformer",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/snyder19_interspeech.html": {
    "title": "Individual Variation in Cognitive Processing Style Predicts Differences in Phonetic Imitation of Device and Human Voices",
    "volume": "main",
    "abstract": "Phonetic imitation, or implicitly matching the acoustic-phonetic patterns of another speaker, has been empirically associated with natural tendencies to promote successful social communication, as well as individual differences in personality and cognitive processing style. The present study explores whether individual differences in cognitive processing style, as indexed by self-reported scored from the Autism-Spectrum Quotient (AQ) questionnaire, are linked to the way people imitate the vocal productions by two digital device voices (i.e., Apple's Siri) and two human voices. Subjects first performed a word shadowing task of human and device voices and then completed the self-administered AQ. We assessed imitation of two acoustic properties: f0 and vowel duration. We find that the attention to detail and the imagination subscale scores on the AQ mediated degree of imitation of f0 and vowel duration, respectively. The findings yield new insight to speech production and perception mechanisms and how it interacts with individual cognitive processing style differences",
    "checked": true,
    "id": "b3516010239a9e4fd3061581793a335fe4615682",
    "semantic_title": "individual variation in cognitive processing style predicts differences in phonetic imitation of device and human voices",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/illa19_interspeech.html": {
    "title": "An Investigation on Speaker Specific Articulatory Synthesis with Speaker Independent Articulatory Inversion",
    "volume": "main",
    "abstract": "Estimating speech representations from articulatory movements is known as articulatory-to-acoustic forward (AAF) mapping. Typically this mapping is learned using directly measured articulatory movement in a subject-specific manner. Such AAF mapping has been shown to benefit the speech synthesis applications. In this work, we investigate the speaker similarity and naturalness of utterances generated by AAF which is driven by the articulatory movements from a subject (referred to as cross speaker) different from the speaker (target speaker) used for training AAF mapping. Experiments are performed with directly measured articulatory data from 9 speakers (8 target speakers and 1 cross speaker), which are recorded using Electromagnetic articulograph AG501. Experiments are also performed with articulatory features estimated using speaker independent acoustic-to-articulatory inversion (SI-AAI) model trained on 26 reference speakers. Objective evaluation on target speakers reveal that the articulatory features estimated from SI-AAI result in a lower Mel-cepstrum distortion compared to that using directly measured articulatory features. Further, listening tests reveal that the directly measured articulatory movements preserve the speaker similarity better than estimated ones. Although, for naturalness, articulatory movements predicted by SI-AAI perform better than the direct measurements",
    "checked": true,
    "id": "9e3b686c1443d8e9d263de95cd46fcd9c825edd8",
    "semantic_title": "an investigation on speaker specific articulatory synthesis with speaker independent articulatory inversion",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19_interspeech.html": {
    "title": "Individual Difference of Relative Tongue Size and its Acoustic Effects",
    "volume": "main",
    "abstract": "This study examines how the speaker's tongue size contributes to generating dynamic characteristics of speaker individuality. The relative tongue size (RTS) has been proposed as an index for the tongue area within the oropharyngeal cavity on the midsagittal magnetic resonance imaging (MRI). Our earlier studies have shown that the smaller the RTS, the faster the tongue movement. In this study, acoustic consequences of individual RTS values were analyzed by comparing tongue movement velocity and formant transition rate. The materials used were cine-MRI data and acoustic signals during production of a sentence and two words produced by two female speakers with contrasting RTS values. The results indicate that the speaker with the small RTS value exhibited the faster changes of tongue positions and formant transitions than the speakers with the large RTS values. Since the tongue size is uncontrollable by a speaker's intention, the RTS can be regarded as one of the causal factors of dynamic individual characteristics in the lower frequency region of speech signals",
    "checked": true,
    "id": "68d0c775c0c976117da4b914321e89e5cfa9a717",
    "semantic_title": "individual difference of relative tongue size and its acoustic effects",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yoshinaga19_interspeech.html": {
    "title": "Individual Differences of Airflow and Sound Generation in the Vocal Tract of Sibilant /s/",
    "volume": "main",
    "abstract": "To clarify the individual differences of flow and sound characteristics of sibilant /s/, the large eddy simulation of compressible flow was applied to vocal tract geometries of five subjects pronouncing /s/. The vocal tract geometry was extracted by separately collecting images of digital dental casts and the vocal tract of /s/. The computational grids were constructed for each geometry, and flow and acoustic fields were predicted by the simulation. Results of the simulation showed that jet flow in the vocal tract was disturbed and fluctuated, and the sound source of /s/ was generated in different place for each subject. With an increment of the jet velocity, not only the overall sound amplitude but also the spectral mean was increased, indicating that the increment of the jet velocity contributes to the increase of amplitudes in a higher frequency range among different vocal tract geometries",
    "checked": true,
    "id": "7b273a1c00e585e0677ed99968cf1c18ecbe706f",
    "semantic_title": "individual differences of airflow and sound generation in the vocal tract of sibilant /s/",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/uttam19_interspeech.html": {
    "title": "Hush-Hush Speak: Speech Reconstruction Using Silent Videos",
    "volume": "main",
    "abstract": "Speech Reconstruction is the task of recreation of speech using silent videos as input. In the literature, it is also referred to as lipreading. In this paper, we design an encoder-decoder architecture which takes silent videos as input and outputs an audio spectrogram of the reconstructed speech. The model, despite being a speaker-independent model, achieves comparable results on speech reconstruction to the current state-of-the-art speaker-dependent model. We also perform user studies to infer speech intelligibility. Additionally, we test the usability of the trained model using bilingual speech",
    "checked": true,
    "id": "ec9fda5db1ced6058824ce0c8fe9cb6e1d20c24e",
    "semantic_title": "hush-hush speak: speech reconstruction using silent videos",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/saha19_interspeech.html": {
    "title": "SPEAK YOUR MIND! Towards Imagined Speech Recognition with Hierarchical Deep Learning",
    "volume": "main",
    "abstract": "Speech-related Brain Computer Interface (BCI) technologies provide effective vocal communication strategies for controlling devices through speech commands interpreted from brain signals. In order to infer imagined speech from active thoughts, we propose a novel hierarchical deep learning BCI system for subject-independent classification of 11 speech tokens including phonemes and words. Our novel approach exploits predicted articulatory information of six phonological categories (e.g., nasal, bilabial) as an intermediate step for classifying the phonemes and words, thereby finding discriminative signal responsible for natural speech synthesis. The proposed network is composed of hierarchical combination of spatial and temporal CNN cascaded with a deep autoencoder. Our best models on the KARA database achieve an average accuracy of 83.42% across the six different binary phonological classification tasks, and 53.36% for the individual token identification task, significantly outperforming our baselines. Ultimately, our work suggests the possible existence of a brain imagery footprint for the underlying articulatory movement related to different sounds that can be used to aid imagined speech decoding",
    "checked": true,
    "id": "fb2029d6587f3099b9b62b3abc601c64bd48fefc",
    "semantic_title": "speak your mind! towards imagined speech recognition with hierarchical deep learning",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chung19_interspeech.html": {
    "title": "An Unsupervised Autoregressive Model for Speech Representation Learning",
    "volume": "main",
    "abstract": "This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks. In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data. Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches. Further analysis shows that different levels of speech information are captured by our model at different layers. In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content",
    "checked": true,
    "id": "2f803165d054ee89bec2401368ceb9e75bad8b60",
    "semantic_title": "an unsupervised autoregressive model for speech representation learning",
    "citation_count": 322
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19_interspeech.html": {
    "title": "Harmonic-Aligned Frame Mask Based on Non-Stationary Gabor Transform with Application to Content-Dependent Speaker Comparison",
    "volume": "main",
    "abstract": "We propose harmonic-aligned frame mask for speech signals using non-stationary Gabor transform (NSGT). A frame mask operates on the transfer coefficients of a signal and consequently converts the signal into a counterpart signal. It depicts the difference between the two signals. In preceding studies, frame masks based on regular Gabor transform were applied to single-note instrumental sound analysis. This study extends the frame mask approach to speech signals. For voiced speech, the fundamental frequency is usually changing consecutively over time. We employ NSGT with pitch-dependent and therefore time-varying frequency resolution to attain harmonic alignment in the transform domain and hence yield harmonic-aligned frame masks for speech signals. We propose to apply the harmonic-aligned frame mask to content-dependent speaker comparison. Frame masks, computed from voiced signals of a same vowel but from different speakers, were utilized as similarity measures to compare and distinguish the speaker identities (SID). Results obtained with deep neural networks demonstrate that the proposed frame mask is valid in representing speaker characteristics and shows a potential for SID applications in limited data scenarios",
    "checked": true,
    "id": "7cdcd85e2e9e44c5b1040079313d48886ce27553",
    "semantic_title": "harmonic-aligned frame mask based on non-stationary gabor transform with application to content-dependent speaker comparison",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/m19_interspeech.html": {
    "title": "Glottal Closure Instants Detection from Speech Signal by Deep Features Extracted from Raw Speech and Linear Prediction Residual",
    "volume": "main",
    "abstract": "Glottal closure instants (GCI) also called as instants of significant excitation occur during abrupt closure of vocal folds is a well-studied problem for its many potential applications in speech processing. Speech signal or its transformed linear prediction residual (LPR) is the most popular signal representations for GCI detection. In this paper, we propose a supervised classification based GCI detection method, in which, we train multiple convolution neural networks to determine the suitable feature representation for efficient GCI detection. Also, we show that the combined model trained with joint acoustic-residual deep features and the model trained with low pass filtered speech significantly increases the detection accuracy. We have manually annotated the speech signal for ground truth GCI using electroglottograph (EGG) as a reference signal. The evaluation results showed that the proposed model trained with very small and less diverse data performs significantly better than the traditional signal processing and most recent data-driven approaches",
    "checked": true,
    "id": "bf70044aefbf7c8433e93116f1ed9a4e85ca669c",
    "semantic_title": "glottal closure instants detection from speech signal by deep features extracted from raw speech and linear prediction residual",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pascual19_interspeech.html": {
    "title": "Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks",
    "volume": "main",
    "abstract": "Learning good representations without supervision is still an open issue in machine learning, and is particularly challenging for speech signals, which are often characterized by long sequences with a complex hierarchical structure. Some recent works, however, have shown that it is possible to derive useful speech representations by employing a self-supervised encoder-discriminator approach. This paper proposes an improved self-supervised method, where a single neural encoder is followed by multiple workers that jointly solve different self-supervised tasks. The needed consensus across different tasks naturally imposes meaningful constraints to the encoder, contributing to discover general representations and to minimize the risk of learning superficial ones. Experiments show that the proposed approach can learn transferable, robust, and problem-agnostic features that carry on relevant information from the speech signal, such as speaker identity, phonemes, and even higher-level features such as emotional cues. In addition, a number of design choices make the encoder easily exportable, facilitating its direct usage or adaptation to different problems",
    "checked": true,
    "id": "b3beb9bc7395a8a489b9c64c46329a84d45968bd",
    "semantic_title": "learning problem-agnostic speech representations from multiple self-supervised tasks",
    "citation_count": 197
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nellore19_interspeech.html": {
    "title": "Excitation Source and Vocal Tract System Based Acoustic Features for Detection of Nasals in Continuous Speech",
    "volume": "main",
    "abstract": "The aim of the current study is to propose acoustic features for detection of nasals in continuous speech. Acoustic features that represent certain characteristics of speech production are extracted. Features representing excitation source characteristics are extracted using zero frequency filtering method. Features representing vocal tract system characteristics are extracted using zero time windowing method Feature sets are formed by combining certain subsets of the features mentioned above. These feature sets are evaluated for their representativeness of nasals in continuous speech in three different languages, namely, English, Hindi and Telugu. Results show that nasal detection is reliable and consistent across all the languages mentioned above",
    "checked": true,
    "id": "997642aa05d29b542130ef90653e306fae7c8260",
    "semantic_title": "excitation source and vocal tract system based acoustic features for detection of nasals in continuous speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chatziagapi19_interspeech.html": {
    "title": "Data Augmentation Using GANs for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In this work, we address the problem of data imbalance for the task of Speech Emotion Recognition (SER). We investigate conditioned data augmentation using Generative Adversarial Networks (GANs), in order to generate samples for underrepresented emotions. We adapt and improve a conditional GAN architecture to generate synthetic spectrograms for the minority class. For comparison purposes, we implement a series of signal-based data augmentation methods. The proposed GAN-based approach is evaluated on two datasets, namely IEMOCAP and FEEL-25k, a large multi-domain dataset. Results demonstrate a 10% relative performance improvement in IEMOCAP and 5% in FEEL-25k, when augmenting the minority classes",
    "checked": true,
    "id": "f3c25b8aa1f7e2a00b947c38d52dcaa6b3da31bd",
    "semantic_title": "data augmentation using gans for speech emotion recognition",
    "citation_count": 82
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kons19_interspeech.html": {
    "title": "High Quality, Lightweight and Adaptable TTS Using LPCNet",
    "volume": "main",
    "abstract": "We present a lightweight adaptable neural TTS system with high quality output. The system is composed of three separate neural network blocks: prosody prediction, acoustic feature prediction and Linear Prediction Coding Net as a neural vocoder. This system can synthesize speech with close to natural quality while running 3 times faster than real-time on a standard CPU The modular setup of the system allows for simple adaptation to new voices with a small amount of data We first demonstrate the ability of the system to produce high quality speech when trained on large, high quality datasets. Following that, we demonstrate its adaptability by mimicking unseen voices using 5 to 20 minutes long datasets with lower recording quality. Large scale Mean Opinion Score quality and similarity tests are presented, showing that the system can adapt to unseen voices with quality gap of 0.12 and similarity gap of 3% compared to natural speech for male voices and quality gap of 0.35 and similarity of gap of 9% for female voices",
    "checked": true,
    "id": "52daae0ff7d09d4b11ab447fa0cb57e2ba1d12b6",
    "semantic_title": "high quality, lightweight and adaptable tts using lpcnet",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lorenzotrueba19_interspeech.html": {
    "title": "Towards Achieving Robust Universal Neural Vocoding",
    "volume": "main",
    "abstract": "This paper explores the potential universality of neural vocoders. We train a WaveRNN-based vocoder on 74 speakers coming from 17 languages. This vocoder is shown to be capable of generating speech of consistently good quality (98% relative mean MUSHRA when compared to natural speech) regardless of whether the input spectrogram comes from a speaker or style seen during training or from an out-of-domain scenario when the recording conditions are studio-quality. When the recordings show significant changes in quality, or when moving towards non-speech vocalizations or singing, the vocoder still significantly outperforms speaker-dependent vocoders, but operates at a lower average relative MUSHRA of 75%. These results are shown to be consistent across languages, regardless of them being seen during training (e.g. English or Japanese) or unseen (e.g. Wolof, Swahili, Ahmaric)",
    "checked": true,
    "id": "93d857da76fdeeece7ed641f4d48e1e9770e8315",
    "semantic_title": "towards achieving robust universal neural vocoding",
    "citation_count": 78
  },
  "https://www.isca-speech.org/archive/interspeech_2019/neekhara19_interspeech.html": {
    "title": "Expediting TTS Synthesis with Adversarial Vocoding",
    "volume": "main",
    "abstract": "Recent approaches in text-to-speech (TTS) synthesis employ neural network strategies to vocode perceptually-informed spectrogram representations directly into listenable waveforms. Such vocoding procedures create a computational bottleneck in modern TTS pipelines. We propose an alternative approach which utilizes generative adversarial networks (GANs) to learn mappings from perceptually-informed spectrograms to simple magnitude spectrograms which can be heuristically vocoded. Through a user study, we show that our approach significantly outperforms naïve vocoding strategies while being hundreds of times faster than neural network vocoders used in state-of-the-art TTS systems. We also show that our method can be used to achieve state-of-the-art results in unsupervised synthesis of individual words of speech",
    "checked": true,
    "id": "3b978703968c2e3f8a41b0d34f870bfc2228677f",
    "semantic_title": "expediting tts synthesis with adversarial vocoding",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mustafa19_interspeech.html": {
    "title": "Analysis by Adversarial Synthesis — A Novel Approach for Speech Vocoding",
    "volume": "main",
    "abstract": "Classical parametric speech coding techniques provide a compact representation for speech signals. This affords a very low transmission rate but with a reduced perceptual quality of the reconstructed signals. Recently, autoregressive deep generative models such as WaveNet and SampleRNN have been used as speech vocoders to scale up the perceptual quality of the reconstructed signals without increasing the coding rate. However, such models suffer from a very slow signal generation mechanism due to their sample-by-sample modelling approach. In this work, we introduce a new methodology for neural speech vocoding based on generative adversarial networks (GANs). A fake speech signal is generated from a very compressed representation of the glottal excitation using conditional GANs as a deep generative model. This fake speech is then refined using the LPC parameters of the original speech signal to obtain a natural reconstruction. The reconstructed speech waveforms based on this approach show a higher perceptual quality than the classical vocoder counterparts according to subjective and objective evaluation scores for a dataset of 30 male and female speakers. Moreover, the usage of GANs enables to generate signals in one-shot compared to autoregressive generative models. This makes GANs promising for exploration to implement high-quality neural vocoders",
    "checked": false,
    "id": "07a9dc45559f8c4cc2a8740e36e18f29db8e7a3d",
    "semantic_title": "analysis by adversarial synthesis - a novel approach for speech vocoding",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19b_interspeech.html": {
    "title": "Quasi-Periodic WaveNet Vocoder: A Pitch Dependent Dilated Convolution Model for Parametric Speech Generation",
    "volume": "main",
    "abstract": "In this paper, we propose a quasi-periodic neural network (QPNet) vocoder with a novel network architecture named pitch-dependent dilated convolution (PDCNN) to improve the pitch controllability of WaveNet (WN) vocoder. The effectiveness of the WN vocoder to generate high-fidelity speech samples from given acoustic features has been proved recently. However, because of the fixed dilated convolution and generic network architecture, the WN vocoder hardly generates speech with given F values which are outside the range observed in training data. Consequently, the WN vocoder lacks the pitch controllability which is one of the essential capabilities of conventional vocoders. To address this limitation, we propose the PDCNN component which has the time-variant adaptive dilation size related to the given F values and a cascade network structure of the QPNet vocoder to generate quasi-periodic signals such as speech. Both objective and subjective tests are conducted, and the experimental results demonstrate the better pitch controllability of the QPNet vocoder compared to the same and double sized WN vocoders while attaining comparable speech qualities",
    "checked": true,
    "id": "ff01789535aa1535f610739176083f17c6239d2f",
    "semantic_title": "quasi-periodic wavenet vocoder: a pitch dependent dilated convolution model for parametric speech generation",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tian19_interspeech.html": {
    "title": "A Speaker-Dependent WaveNet for Voice Conversion with Non-Parallel Data",
    "volume": "main",
    "abstract": "In a typical voice conversion system, vocoder is commonly used for speech-to-features analysis and features-to-speech synthesis. However, vocoder can be a source of speech quality degradation. This paper presents a novel approach to voice conversion using WaveNet for non-parallel training data. Instead of reconstructing speech with intermediate features, the proposed approach utilizes the WaveNet to map the Phonetic PosteriorGrams (PPGs) to the waveform samples directly. In this way, we avoid the estimation errors arising from vocoding and feature conversion. Additionally, as PPG is assumed to be speaker independent, the proposed approach also reduces the feature mismatch problem in WaveNet vocoder based solutions. Experimental results conducted on the CMU-ARCTIC database show that the proposed approach significantly outperforms the traditional vocoder and WaveNet Vocoder baselines in terms of speech quality",
    "checked": true,
    "id": "b8360a4ec8d54f5bbc4b8081730a47b9c7fe026e",
    "semantic_title": "a speaker-dependent wavenet for voice conversion with non-parallel data",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/han19_interspeech.html": {
    "title": "Survey Talk: When Attention Meets Speech Applications: Speech & Speaker Recognition Perspective",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "984a38f397b3211963b36e2049b945afaadaebbf",
    "semantic_title": "survey talk: when attention meets speech applications: speech & speaker recognition perspective",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19_interspeech.html": {
    "title": "Attention-Enhanced Connectionist Temporal Classification for Discrete Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Discrete speech emotion recognition (SER), the assignment of a single emotion label to an entire speech utterance, is typically performed as a sequence-to-label task. This approach, however, is limited, in that it can result in models that do not capture temporal changes in the speech signal, including those indicative of a particular emotion. One potential solution to overcome this limitation is to model SER as a sequence-to-sequence task instead. In this regard, we have developed an attention-based bidirectional long short-term memory (BLSTM) neural network in combination with a connectionist temporal classification (CTC) objective function (Attention-BLSTM-CTC) for SER. We also assessed the benefits of incorporating two contemporary attention mechanisms, namely component attention and quantum attention, into the CTC framework. To the best of the authors' knowledge, this is the first time that such a hybrid architecture has been employed for SER.We demonstrated the effectiveness of our approach on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) and FAU-Aibo Emotion corpora. The experimental results demonstrate that our proposed model outperforms current state-of-the-art approaches",
    "checked": true,
    "id": "920f779bef257922d7685244f1a7afc1e4d6ad86",
    "semantic_title": "attention-enhanced connectionist temporal classification for discrete speech emotion recognition",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19b_interspeech.html": {
    "title": "Attentive to Individual: A Multimodal Emotion Recognition Network with Personalized Attention Profile",
    "volume": "main",
    "abstract": "A growing number of human-centered applications benefit from continuous advancements in the emotion recognition technology. Many emotion recognition algorithms have been designed to model multimodal behavior cues to achieve high performances. However, most of them do not consider the modulating factors of an individual's personal attributes in his/her expressive behaviors. In this work, we propose a Personalized Attributes-Aware Attention Network (PAaAN) with a novel personalized attention mechanism to perform emotion recognition using speech and language cues. The attention profile is learned from embeddings of an individual's profile, acoustic, and lexical behavior data. The profile embedding is derived using linguistics inquiry word count computed between the target speaker and a large set of movie scripts. Our method achieves the state-of-the-art 70.3% unweighted accuracy in a four class emotion recognition task on the IEMOCAP. Further analysis reveals that affect-related semantic categories are emphasized differently for each speaker in the corpus showing the effectiveness of our attention mechanism for personalization",
    "checked": true,
    "id": "1515d8597ef03ff52cda5bc3551fbdee6350c2bd",
    "semantic_title": "attentive to individual: a multimodal emotion recognition network with personalized attention profile",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gallardoantolin19_interspeech.html": {
    "title": "A Saliency-Based Attention LSTM Model for Cognitive Load Classification from Speech",
    "volume": "main",
    "abstract": "Cognitive Load (CL) refers to the amount of mental demand that a given task imposes on an individual's cognitive system and it can affect his/her productivity in very high load situations. In this paper, we propose an automatic system capable of classifying the CL level of a speaker by analyzing his/her voice. Our research on this topic goes into two main directions. In the first one, we focus on the use of Long Short-Term Memory (LSTM) networks with different weighted pooling strategies for CL level classification. In the second contribution, for overcoming the need of a large amount of training data, we propose a novel attention mechanism that uses the Kalinli's auditory saliency model. Experiments show that our proposal outperforms significantly both, a baseline system based on Support Vector Machines (SVM) and a LSTM-based system with logistic regression attention model",
    "checked": true,
    "id": "af4f0d9acf3841703855891c12ce24f6b41da608",
    "semantic_title": "a saliency-based attention lstm model for cognitive load classification from speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mallolragolta19_interspeech.html": {
    "title": "A Hierarchical Attention Network-Based Approach for Depression Detection from Transcribed Clinical Interviews",
    "volume": "main",
    "abstract": "The high prevalence of depression in society has given rise to a need for new digital tools that can aid its early detection. Among other effects, depression impacts the use of language. Seeking to exploit this, this work focuses on the detection of depressed and non-depressed individuals through the analysis of linguistic information extracted from transcripts of clinical interviews with a virtual agent. Specifically, we investigated the advantages of employing hierarchical attention-based networks for this task. Using Global Vectors (GloVe) pretrained word embedding models to extract low-level representations of the words, we compared hierarchical local-global attention networks and hierarchical contextual attention networks. We performed our experiments on the Distress Analysis Interview Corpus - Wizard of Oz (DAIC-WoZ) dataset, which contains audio, visual, and linguistic information acquired from participants during a clinical session. Our results using the DAIC-WoZ test set indicate that hierarchical contextual attention networks are the most suitable configuration to detect depression from transcripts. The configuration achieves an Unweighted Average Recall (UAR) of .66 using the test set, surpassing our baseline, a Recurrent Neural Network that does not use attention",
    "checked": true,
    "id": "8db761dc173e30b0882390892fe92af7acd11208",
    "semantic_title": "a hierarchical attention network-based approach for depression detection from transcribed clinical interviews",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2019/carmantini19_interspeech.html": {
    "title": "Untranscribed Web Audio for Low Resource Speech Recognition",
    "volume": "main",
    "abstract": "Speech recognition models are highly susceptible to mismatch in the acoustic and language domains between the training and the evaluation data. For low resource languages, it is difficult to obtain transcribed speech for target domains, while untranscribed data can be collected with minimal effort. Recently, a method applying lattice-free maximum mutual information (LF-MMI) to untranscribed data has been found to be effective for semi-supervised training. However, weaker initial models and domain mismatch can result in high deletion rates for the semi-supervised model. Therefore, we propose a method to force the base model to overgenerate possible transcriptions, relying on the ability of LF-MMI to deal with uncertainty. On data from the IARPA MATERIAL programme, our new semi-supervised method outperforms the standard semi-supervised method, yielding significant gains when adapting for mismatched bandwidth and domain",
    "checked": true,
    "id": "07a873b8a6c00dc72978ef7e98160b3e245c4bca",
    "semantic_title": "untranscribed web audio for low resource speech recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luscher19_interspeech.html": {
    "title": "RWTH ASR Systems for LibriSpeech: Hybrid vs Attention",
    "volume": "main",
    "abstract": "We present state-of-the-art automatic speech recognition (ASR) systems employing a standard hybrid DNN/HMM architecture compared to an attention-based encoder-decoder design for the LibriSpeech task. Detailed descriptions of the system development, including model design, pretraining schemes, training schedules, and optimization approaches are provided for both system architectures. Both hybrid DNN/HMM and attention-based systems employ bi-directional LSTMs for acoustic modeling/encoding. For language modeling, we employ both LSTM and Transformer based architectures. All our systems are built using RWTH's open-source toolkits RASR and RETURNN. To the best knowledge of the authors, the results obtained when training on the full LibriSpeech training set, are the best published currently, both for the hybrid DNN/HMM and the attention-based systems. Our single hybrid system even outperforms previous results obtained from combining eight single systems. Our comparison shows that on the LibriSpeech 960h task, the hybrid DNN/HMM system outperforms the attention-based system by 15% relative on the clean and 40% relative on the other test sets in terms of word error rate. Moreover, experiments on a reduced 100h-subset of the LibriSpeech training corpus even show a more pronounced margin between the hybrid DNN/HMM and attention-based architectures",
    "checked": false,
    "id": "744196b6cb5091c0760d05ef068a92a6cd531587",
    "semantic_title": "rwth asr systems for librispeech: hybrid vs attention - w/o data augmentation",
    "citation_count": 211
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kanda19_interspeech.html": {
    "title": "Auxiliary Interference Speaker Loss for Target-Speaker Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a novel auxiliary loss function for target-speaker automatic speech recognition (ASR). Our method automatically extracts and transcribes target speaker's utterances from a monaural mixture of multiple speakers speech given a short sample of the target speaker. The proposed auxiliary loss function attempts to additionally maximize interference speaker ASR accuracy during training. This will regularize the network to achieve a better representation for speaker separation, thus achieving better accuracy on the target-speaker ASR. We evaluated our proposed method using two-speaker-mixed speech in various signal-to-interference-ratio conditions. We first built a strong target-speaker ASR baseline based on the state-of-the-art lattice-free maximum mutual information. This baseline achieved a word error rate (WER) of 18.06% on the test set while a normal ASR trained with clean data produced a completely corrupted result (WER of 84.71%). Then, our proposed loss further reduced the WER by 6.6% relative to this strong baseline, achieving a WER of 16.87%. In addition to the accuracy improvement, we also showed that the auxiliary output branch for the proposed loss can even be used for a secondary ASR for interference speakers' speech",
    "checked": true,
    "id": "388d41b99c9c0867301f345c65877a2796225ead",
    "semantic_title": "auxiliary interference speaker loss for target-speaker speech recognition",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/meng19_interspeech.html": {
    "title": "Speaker Adaptation for Attention-Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "We propose three regularization-based speaker adaptation approaches to adapt the attention-based encoder-decoder (AED) model with very limited adaptation data from target speakers for end-to-end automatic speech recognition. The first method is Kullback-Leibler divergence (KLD) regularization, in which the output distribution of a speaker-dependent (SD) AED is forced to be close to that of the speaker-independent (SI) model by adding a KLD regularization to the adaptation criterion. To compensate for the asymmetric deficiency in KLD regularization, an adversarial speaker adaptation (ASA) method is proposed to regularize the deep-feature distribution of the SD AED through the adversarial learning of an auxiliary discriminator and the SD AED. The third approach is the multi-task learning, in which an SD AED is trained to jointly perform the primary task of predicting a large number of output units and an auxiliary task of predicting a small number of output units to alleviate the target sparsity issue. Evaluated on a Microsoft short message dictation task, all three methods are highly effective in adapting the AED model, achieving up to 12.2% and 3.0% word error rate improvement over an SI AED trained from 3400 hours data for supervised and unsupervised adaptation, respectively",
    "checked": true,
    "id": "ce77b9212751e1afd10c7fccd5271a806dfbb445",
    "semantic_title": "speaker adaptation for attention-based end-to-end speech recognition",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19_interspeech.html": {
    "title": "Large Margin Training for Attention Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end speech recognition systems are typically evaluated using the maximum a posterior criterion. Since only one hypothesis is involved during evaluation, the ideal number of hypotheses for training should also be one. In this study, we propose a large margin training scheme for attention based end-to-end speech recognition. Using only one training hypothesis, the large margin training strategy achieves the same performance as the minimum word error rate criterion using four hypotheses. The theoretical derivation in this study is widely applicable to other sequence discriminative criteria such as maximum mutual information. In addition, this paper provides a more succinct formulation of the large margin concept, paving the road towards a better combination of support vector machine and deep neural network",
    "checked": true,
    "id": "72abad6cd58731dafa8e7e35b2ce7192f8f6fc72",
    "semantic_title": "large margin training for attention based end-to-end speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mac19_interspeech.html": {
    "title": "Large-Scale Mixed-Bandwidth Deep Neural Network Acoustic Modeling for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "In automatic speech recognition (ASR), wideband (WB) and narrowband (NB) speech signals with different sampling rates typically use separate acoustic models. Therefore mixed-bandwidth (MB) acoustic modeling has important practical values for ASR system deployment. In this paper, we extensively investigate large-scale MB deep neural network acoustic modeling for ASR using 1,150 hours of WB data and 2,300 hours of NB data. We study various MB strategies including downsampling, upsampling and bandwidth extension for MB acoustic modeling and evaluate their performance on 8 diverse WB and NB test sets from various application domains. To deal with the large amounts of training data, distributed training is carried out on multiple GPUs using synchronous data parallelism",
    "checked": true,
    "id": "2804c86dc045206d9759f4f9813e5b66cdbb2771",
    "semantic_title": "large-scale mixed-bandwidth deep neural network acoustic modeling for automatic speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/milde19_interspeech.html": {
    "title": "SparseSpeech: Unsupervised Acoustic Unit Discovery with Memory-Augmented Sequence Autoencoders",
    "volume": "main",
    "abstract": "We propose a sparse sequence autoencoder model for unsupervised acoustic unit discovery, based on bidirectional LSTM encoders/decoders with a sparsity-inducing bottleneck. The sparsity layer is based on memory-augmented neural networks, with a differentiable embedding memory bank addressed from the encoder. The decoder reconstructs the encoded input feature sequence from an utterance-level context embedding and the bottleneck representation. At some time steps, the input to the decoder is randomly omitted by applying sequence dropout, forcing the decoder to learn about the temporal structure of the sequence. We propose a bootstrapping training procedure, after which the network can be trained end-to-end with standard back-propagation. Sparsity of the generated representation can be controlled with a parameter in the proposed loss function. We evaluate the units with the ABX discriminability on minimal triphone pairs and also on entire words. Forcing the network to favor highly sparse memory addressings in the memory component yields symbolic-like representations of speech that are very compact and still offer better ABX discriminability than MFCC",
    "checked": true,
    "id": "a98badb3e7503d7be09c01a43f85429505d4c907",
    "semantic_title": "sparsespeech: unsupervised acoustic unit discovery with memory-augmented sequence autoencoders",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ondel19_interspeech.html": {
    "title": "Bayesian Subspace Hidden Markov Model for Acoustic Unit Discovery",
    "volume": "main",
    "abstract": "This work tackles the problem of learning a set of language specific acoustic units from unlabeled speech recordings given a set of labeled recordings from other languages. Our approach may be described by the following two steps procedure: first the model learns the notion of acoustic units from the labelled data and then the model uses its knowledge to find new acoustic units on the target language. We implement this process with the Bayesian Subspace Hidden Markov Model (SHMM), a model akin to the Subspace Gaussian Mixture Model (SGMM) where each low dimensional embedding represents an acoustic unit rather than just a HMM's state. The subspace is trained on 3 languages from the GlobalPhone corpus (German, Polish and Spanish) and the AUs are discovered on the TIMIT corpus. Results, measured in equivalent Phone Error Rate, show that this approach significantly outperforms previous HMM based acoustic units discovery systems and compares favorably with the Variational Auto Encoder-HMM",
    "checked": true,
    "id": "57d1734db27c6ce1aae420d56182a4dab9f4fa5c",
    "semantic_title": "bayesian subspace hidden markov model for acoustic unit discovery",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/higuchi19_interspeech.html": {
    "title": "Speaker Adversarial Training of DPGMM-Based Feature Extractor for Zero-Resource Languages",
    "volume": "main",
    "abstract": "We propose a novel framework for extracting speaker-invariant features for zero-resource languages. A deep neural network (DNN)-based acoustic model is normalized against speakers via adversarial training: a multi-task learning process trains a shared bottleneck feature to be discriminative to phonemes and independent of speakers. However, owing to the absence of phoneme labels, zero-resource languages cannot employ adversarial multi-task (AMT) learning for speaker normalization. In this work, we obtain a posteriorgram from a Dirichlet process Gaussian mixture model (DPGMM) and utilize the posterior vector for supervision of the phoneme estimation in the AMT training. The AMT network is designed so that the DPGMM posteriorgram itself is embedded in a speaker-invariant feature space. The proposed network is expected to resolve the potential problem that the posteriorgram may lack reliability as a phoneme representation if the DPGMM components are intermingled with phoneme and speaker information. Based on the Zero Resource Speech Challenges, we conduct phoneme discriminant experiments on the extracted features. The results of the experiments show that the proposed framework extracts discriminative features, suppressing the variety in speakers",
    "checked": true,
    "id": "a62a5322c25a9074fd2499c88a46690afd176755",
    "semantic_title": "speaker adversarial training of dpgmm-based feature extractor for zero-resource languages",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/prasad19_interspeech.html": {
    "title": "Building Large-Vocabulary ASR Systems for Languages Without Any Audio Training Data",
    "volume": "main",
    "abstract": "When building automatic speech recognition (ASR) systems, typically some amount of audio and text data in the target language is needed. While text data can be obtained relatively easily across many languages, transcribed audio data is challenging to obtain. This presents a barrier to making voice technologies available in more languages of the world. In this paper, we present a way to build an ASR system system for a language even in the absence of any audio training data in that language at all. We do this by simply re-using an existing acoustic model from a phonologically similar language, without any kind of modification or adaptation towards the target language. The basic insight is that, if two languages are sufficiently similar in terms of their phonological system, an acoustic model should hold up relatively well when used for another language. We describe how we tailor our pronunciation models to enable such re-use, and show experimental results across a number of languages from various language families. We also provide a theoretical analysis of situations in which this approach is likely to work. Our results show that it is possible to achieve less than 20% word error rate (WER) using this method",
    "checked": true,
    "id": "6a3ea42e8cd381e3ed95a0d4d965409172728aa1",
    "semantic_title": "building large-vocabulary asr systems for languages without any audio training data",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/azuh19_interspeech.html": {
    "title": "Towards Bilingual Lexicon Discovery From Visually Grounded Speech Audio",
    "volume": "main",
    "abstract": "In this paper, we present a method for the discovery of word-like units and their approximate translations from visually grounded speech across multiple languages. We first train a neural network model to map images and their spoken audio captions in both English and Hindi to a shared, multimodal embedding space. Next, we use this model to segment and cluster regions of the spoken captions which approximately correspond to words. Finally, we exploit between-cluster similarities in the embedding space to associate English pseudo-word clusters with Hindi pseudo-word clusters, and show that many of these cluster pairings capture semantic translations between English and Hindi words. We present quantitative cross-lingual clustering results, as well as qualitative results in the form of a bilingual picture dictionary",
    "checked": true,
    "id": "7ab9392167bdbaa272c95178d50fb08bcfba7148",
    "semantic_title": "towards bilingual lexicon discovery from visually grounded speech audio",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/feng19_interspeech.html": {
    "title": "Improving Unsupervised Subword Modeling via Disentangled Speech Representation Learning and Transformation",
    "volume": "main",
    "abstract": "This study tackles unsupervised subword modeling in the zero-resource scenario, learning frame-level speech representation that is phonetically discriminative and speaker-invariant, using only untranscribed speech for target languages. Frame label acquisition is an essential step in solving this problem. High quality frame labels should be in good consistency with golden transcriptions and robust to speaker variation. We propose to improve frame label acquisition in our previously adopted deep neural network-bottleneck feature (DNN-BNF) architecture by applying the factorized hierarchical variational autoencoder (FHVAE). FHVAEs learn to disentangle linguistic content and speaker identity information encoded in speech. By discarding or unifying speaker information, speaker-invariant features are learned and fed as inputs to DPGMM frame clustering and DNN-BNF training. Experiments conducted on ZeroSpeech 2017 show that our proposed approaches achieve 2.4% and 0.6% absolute ABX error rate reductions in across- and within-speaker conditions, comparing to the baseline DNN-BNF system without applying FHVAEs. Our proposed approaches significantly outperform vocal tract length normalization in improving frame labeling and subword modeling",
    "checked": true,
    "id": "3bad6b7a7d17eaa0027bee97d6b92cbcb40a33d1",
    "semantic_title": "improving unsupervised subword modeling via disentangled speech representation learning and transformation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nissen19_interspeech.html": {
    "title": "Listeners' Ability to Identify the Gender of Preadolescent Children in Different Linguistic Contexts",
    "volume": "main",
    "abstract": "This study evaluated listeners' ability to identify the gender of preadolescent children from speech samples of varying length and linguistic context. The listeners were presented with a total of 190 speech samples in four different categories of linguistic context: segments, words, sentences, and discourse. The listeners were instructed to evaluate each speech sample and decide whether the speaker was a male or female and rate their level of confidence in their decision. Results showed listeners identified the gender of the speakers with a high degree of accuracy, ranging from 86% to 95%. Significant differences in listener judgments were found across the four levels of linguistic context, with segments having the lowest accuracy (83%) and discourse the highest accuracy (99%). At the segmental level, the listeners' identification of each speaker's gender was greater for vowels than for fricatives, with both types of phoneme being identified at a rate well above chance. Significant differences in identification were found between the /s/ and /ʃ/ fricatives, but not between the four corner vowels. The perception of gender is likely multifactorial, with listeners possibly using phonetic, prosodic, or stylistic speech cues to determine a speaker's gender",
    "checked": true,
    "id": "f32d178eaffa86fefa8ea014fb67e5ca1b521184",
    "semantic_title": "listeners' ability to identify the gender of preadolescent children in different linguistic contexts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ahlers19_interspeech.html": {
    "title": "Sibilant Variation in New Englishes: A Comparative Sociophonetic Study of Trinidadian and American English /s(tr)/-Retraction",
    "volume": "main",
    "abstract": "The retraction of /s/, particularly in /str/ clusters, toward [ʃ] has been investigated in British, Australian, and American English and shown to be conditioned phonetically and sociolinguistically. To date, however, no research exists on the retraction of /s/ in New Englishes, the nativized Englishes spoken in postcolonial territories like the Caribbean. We take up this research gap and present the results of a large-scale comparative acoustic analysis of /s/-retraction in Trinidadian English (TrinE) and American English (AmE), using Center of Gravity measurements of more than 23,500 sibilants produced by 181 speakers from two speech corpora The results show that, in TrinE, /str/ is considerably retracted toward [ʃtɹ], while all other /sC(r)/ clusters are non-retracted and acoustically close to singleton /s/; less retracted realizations of /str/ occur across word boundaries. Although a statistically significant contrast is overall maintained between /ʃ/ and the sibilant in /str/, there is considerable overlap across many speakers. The comparison between TrinE and AmE indicates that, while sibilants in TrinE overall show acoustically lower values, both varieties have in common that retraction is limited to /str/ contexts and significantly larger in younger speakers. The degree of /str/-retraction, however, is overall larger in TrinE than AmE",
    "checked": true,
    "id": "2ff5f37edd3cec2c2ec4405cb345ee89bf34027a",
    "semantic_title": "sibilant variation in new englishes: a comparative sociophonetic study of trinidadian and american english /s(tr)/-retraction",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gubian19_interspeech.html": {
    "title": "Tracking the New Zealand English NEAR/SQUARE Merger Using Functional Principal Components Analysis",
    "volume": "main",
    "abstract": "The focus of the study is the application of functional principal components analysis (FPCA) to a sound change in progress in which the square and near falling diphthongs are merging in New Zealand English. FPCA approximated the trajectory shapes of the first two formant frequencies (F1/F2) in a large acoustic database of read New Zealand English speech spanning three different age groups and two regions. The derived FPCA parameters showed a greater degree of centralisation and monophthongisation in square than in near. Compatibly with the evidence of an ongoing sound change in which square is shifting towards near, these shape differences were more marked for older than for younger/mid-age speakers. There was no effect of region nor of place of articulation of the preceding consonant; there was a trend for the merger to be more advanced in low frequency words. The study underlines the benefits of FPCA for quantifying the many types of sound changes involving subtle shifts in speech dynamics. In particular, multi-dimensional trajectory shape differences can be quantified without the need for vowel targets nor for determining the influence of the parameters — in this case of the first two formant frequencies — independently of each other",
    "checked": true,
    "id": "731f20b2a2efc4f4ea684a387f972a33390237d4",
    "semantic_title": "tracking the new zealand english near/square merger using functional principal components analysis",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gessinger19_interspeech.html": {
    "title": "Phonetic Accommodation in a Wizard-of-Oz Experiment: Intonation and Segments",
    "volume": "main",
    "abstract": "This paper discusses phonetic accommodation of 20 native German speakers interacting with the simulated spoken dialogue system Mirabella in a Wizard-of-Oz experiment. The study examines intonation of wh-questions and pronunciation of allophonic contrasts in German. In a question-and-answer exchange with the system, the users produce predominantly falling intonation patterns for wh-questions when the system does so as well. The number of rising patterns on the part of the users increases significantly when Mirabella produces questions with rising intonation. In a map task, Mirabella provides information about hidden items while producing variants of two allophonic contrasts which are dispreferred by the users. For the [ɪç] vs. [ɪk] contrast in the suffix ⟨-ig⟩, the number of dispreferred variants on the part of the users increases significantly during the map task. For the [εː] vs. [eː] contrast as a realization of stressed ⟨-ä-⟩, such a convergence effect is not found on the group level, yet still occurs for some individual users. Almost every user converges to the system to a substantial degree for a subset of the examined features, but we also find maintenance of preferred variants and even occasional divergence. This individual variation is in line with previous findings in accommodation research",
    "checked": true,
    "id": "3cc2acd1c36c2a4a4482cfb215f63a3cbfd374cc",
    "semantic_title": "phonetic accommodation in a wizard-of-oz experiment: intonation and segments",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niebuhr19b_interspeech.html": {
    "title": "PASCAL and DPA: A Pilot Study on Using Prosodic Competence Scores to Predict Communicative Skills for Team Working and Public Speaking",
    "volume": "main",
    "abstract": "Strong communication skills in public-speaking and team-working exercises are associated with specific acoustic-prosodic profiles and strategies. We hypothesize that analyzing and assessing these profiles and strategies allows us to predict communicative skills. To that end, we used two analysis methods, one for charismatic and persuasive public speaking (PASCAL), and one for cooperative communication (DPA). PASCAL and DPA competency scores are determined on an acoustic basis for speech recordings of 21 students whose task was to co-create, in 7 teams of 3 students, a fully functioning weather station over 14 weeks in an Electrical Engineering project course — and to jointly write a development report about it afterwards. Results show that the students' PASCAL scores are significantly correlated with both the grade in their final oral project presentation and the grade of their written report as assessed by an independent lecturer group. The DPA scores correlate with better time-management and team working as well as with the quality and functionality of the designed product. Explanations for the links between student performance and acoustic competence scores are discussed",
    "checked": true,
    "id": "b6db8d80d41454d6a26e0ab861ea960fe73c8f51",
    "semantic_title": "pascal and dpa: a pilot study on using prosodic competence scores to predict communicative skills for team working and public speaking",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/michalsky19_interspeech.html": {
    "title": "Towards the Prosody of Persuasion in Competitive Negotiation. The Relationship Between f0 and Negotiation Success in Same Sex Sales Tasks",
    "volume": "main",
    "abstract": "Prosodic features play a key role in a speaker's persuasive power. However, previous studies on persuasion have been focused on public speaking and the signaling of leadership, while acoustic studies on negotiation have been primarily concerned with cooperative interactions. In this study we are taking a first step into investigating the role of acoustic-prosodic cues in competitive negotiation, focusing on f0 in same-sex negotiations. Specifically, we ask whether the prosodic correlates of persuasive speech are comparable for public speaking and negotiation. Sixty-two speakers (44f/18m) in 31 same-sex pairs participated in a competitive task to bargain over the selling price of a fictional company. We find a significant correlation between a speaker's f0 features and his/her interlocutor's concession range. In line with findings from public speaking, greater f0 excursions and higher f0 minima correlate with negotiation success. However, while the female speakers also show an expected elevated f0 mean, the opposite is the case for male speakers. We propose that in competitive negotiation, displaying dominance may overrule showing passion in contrast to public speaking, but only for male speakers",
    "checked": true,
    "id": "2a75a198f1b396effd78e8bcccd78c75dc62c5c4",
    "semantic_title": "towards the prosody of persuasion in competitive negotiation. the relationship between f0 and negotiation success in same sex sales tasks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sager19_interspeech.html": {
    "title": "VESUS: A Crowd-Annotated Database to Study Emotion Production and Perception in Spoken English",
    "volume": "main",
    "abstract": "We introduce the Varied Emotion in Syntactically Uniform Speech (VESUS) repository as a new resource for the speech community. VESUS is a lexically controlled database, in which a semantically neutral script is portrayed with different emotional inflections. In total, VESUS contains over 250 distinct phrases, each read by ten actors in five emotional states. We use crowd sourcing to obtain ten human ratings for the perceived emotional content of each utterance. Our unique database construction enables a multitude of scientific and technical explorations. To jumpstart this effort, we provide benchmark performance on three distinct emotion recognition tasks using VESUS: longitudinal speaker analysis, extrapolating across syntactical complexity, and generalization to a new speaker",
    "checked": true,
    "id": "88fbd781267892f6c50d9bcdb5cfdfe7558fac9f",
    "semantic_title": "vesus: a crowd-annotated database to study emotion production and perception in spoken english",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/koh19_interspeech.html": {
    "title": "Building the Singapore English National Speech Corpus",
    "volume": "main",
    "abstract": "The National Speech Corpus (NSC) is the first large-scale Singapore English corpus spearheaded by the Info-communications and Media Development Authority of Singapore. It aims to become an important source of open speech data for automatic speech recognition (ASR) research and speech-related applications. The first release of the corpus features more than 2000 hours of orthographically transcribed read speech data designed with the inclusion of locally relevant words. It is available for public and commercial use upon request at \"www.imda.gov.sg/nationalspeechcorpus\", under the Singapore Open Data License. An accompanying lexicon is currently in the works and will be published soon. In addition, another 1000 hours of conversational speech data will be made available in the near future under the second release of NSC. This paper reports on the development and collection process of the read speech and conversational speech corpora",
    "checked": true,
    "id": "5d86886822404c7398edfad18b01b922eabccec2",
    "semantic_title": "building the singapore english national speech corpus",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2019/picheny19_interspeech.html": {
    "title": "Challenging the Boundaries of Speech Recognition: The MALACH Corpus",
    "volume": "main",
    "abstract": "There has been huge progress in speech recognition over the last several years. Tasks once thought extremely difficult, such as SWITCHBOARD, now approach levels of human performance. The MALACH corpus (LDC catalog LDC2012S05), a 375-Hour subset of a large archive of Holocaust testimonies collected by the Survivors of the Shoah Visual History Foundation, presents significant challenges to the speech community. The collection consists of unconstrained, natural speech filled with disfluencies, heavy accents, age-related coarticulations, un-cued speaker and language switching, and emotional speech - all still open problems for speech recognition systems. Transcription is challenging even for skilled human annotators. This paper proposes that the community place focus on the MALACH corpus to develop speech recognition systems that are more robust with respect to accents, disfluencies and emotional speech. To reduce the barrier for entry, a lexicon and training and testing setups have been created and baseline results using current deep learning technologies are presented. The metadata has just been released by LDC (LDC2019S11). It is hoped that this resource will enable the community to build on top of these baselines so that the extremely important information in these and related oral histories becomes accessible to a wider audience",
    "checked": true,
    "id": "8bcbcaa3f507d1d3c26f9a378a7c1d488caacf11",
    "semantic_title": "challenging the boundaries of speech recognition: the malach corpus",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ramteke19_interspeech.html": {
    "title": "NITK Kids' Speech Corpus",
    "volume": "main",
    "abstract": "This paper introduces speech database for analyzing children's speech. The proposed database of children is recorded in Kannada language (one of the South Indian languages) from children between age 2.5 to 6.5 years. The database is named as National Institute of Technology Karnataka Kids' Speech Corpus (NITK Kids' Speech Corpus). The relevant design considerations for the database collection are discussed in detail. It is divided into four age groups with an interval of 1 year between each age group. The speech corpus includes nearly 10 hours of speech recordings from 160 children. For each age range, the data is recorded from 40 children (20 male and 20 female). Further, the effect of developmental changes on the speech from 2.5 to 6.5 years are analyzed using pitch and formant analysis. Some of the potential applications, of the NITK Kids' Speech Corpus, such as, systematic study on the language learning ability of children, phonological process analysis and children speech recognition are discussed",
    "checked": true,
    "id": "057e9df4772bc756051edaad4bcfa1068908e493",
    "semantic_title": "nitk kids' speech corpus",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ali19_interspeech.html": {
    "title": "Towards Variability Resistant Dialectal Speech Evaluation",
    "volume": "main",
    "abstract": "We study the problem of evaluating automatic speech recognition (ASR) systems that target dialectal speech input. A major challenge in this case is that the orthography of dialects is typically not standardized. From an ASR evaluation perspective, this means that there is no clear gold standard for the expected output, and several possible outputs could be considered correct according to different human annotators, which makes standard word error rate (WER) inadequate as an evaluation metric. Specifically targeting the case of Arabic dialects, which are also morphologically rich and complex, we propose a number of alternative WER-based metrics that vary in terms of text representation, including different degrees of morphological abstraction and spelling normalization.We evaluate the efficacy of these metrics by comparing their correlation with human judgments on a validation set of 1,000 utterances. Our results show that the use of morphological abstractions and spelling normalization produces systems with higher correlation with human judgment. We released the code and the datasets to the research community",
    "checked": true,
    "id": "438a0502b893be669eb80e4043bbed52835a0843",
    "semantic_title": "towards variability resistant dialectal speech evaluation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fallgren19_interspeech.html": {
    "title": "How to Annotate 100 Hours in 45 Minutes",
    "volume": "main",
    "abstract": "Speech data found in the wild hold many advantages over artificially constructed speech corpora in terms of ecological validity and cultural worth. Perhaps most importantly, there is a lot of it. However, the combination of great quantity, noisiness and variation poses a challenge for its access and processing. Generally speaking, automatic approaches to tackle the problem require good labels for training, while manual approaches require time. In this study, we provide further evidence for a semi-supervised, human-in-the-loop framework that previously has shown promising results for browsing and annotating large quantities of found audio data quickly. The findings of this study show that a 100-hour long subset of the Fearless Steps corpus can be annotated for speech activity in less than 45 minutes, a fraction of the time it would take traditional annotation methods, without a loss in performance",
    "checked": true,
    "id": "7eae889eed59ac7813412a96b00e6a9990c8c47d",
    "semantic_title": "how to annotate 100 hours in 45 minutes",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/diez19_interspeech.html": {
    "title": "Bayesian HMM Based x-Vector Clustering for Speaker Diarization",
    "volume": "main",
    "abstract": "This paper presents a simplified version of the previously proposed diarization algorithm based on Bayesian Hidden Markov Models, which uses Variational Bayesian inference for very fast and robust clustering of x-vector (neural network based speaker embeddings). The presented results show that this clustering algorithm provides significant improvements in diarization performance as compared to the previously used Agglomerative Hierarchical Clustering. The output of this system can be further employed as an initialization for a second stage VB diarization system, using frame-wise MFCC features as input, to obtain optimal results",
    "checked": true,
    "id": "55277df8e04cc75d46470318d9ffbffe365527ee",
    "semantic_title": "bayesian hmm based x-vector clustering for speaker diarization",
    "citation_count": 60
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vestman19_interspeech.html": {
    "title": "Unleashing the Unused Potential of i-Vectors Enabled by GPU Acceleration",
    "volume": "main",
    "abstract": "Speaker embeddings are continuous-value vector representations that allow easy comparison between voices of speakers with simple geometric operations. Among others, i-vector and x-vector have emerged as the mainstream methods for speaker embedding. In this paper, we illustrate the use of modern computation platform to harness the benefit of GPU acceleration for i-vector extraction. In particular, we achieve an acceleration of 3000 times in frame posterior computation compared to real time and 25 times in training the i-vector extractor compared to the CPU baseline from Kaldi toolkit. This significant speed-up allows the exploration of ideas that were hitherto impossible. In particular, we show that it is beneficial to update the universal background model (UBM) and re-compute frame alignments while training the i-vector extractor. Additionally, we are able to study different variations of i-vector extractors more rigorously than before. In this process, we reveal some undocumented details of Kaldi's i-vector extractor and show that it outperforms the standard formulation by a margin of 1 to 2% when tested with VoxCeleb speaker verification protocol. All of our findings are asserted by ensemble averaging the results from multiple runs with random start",
    "checked": true,
    "id": "cccc0dc0167dc5a919922d5fb44c431c545e9e1f",
    "semantic_title": "unleashing the unused potential of i-vectors enabled by gpu acceleration",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shon19_interspeech.html": {
    "title": "MCE 2018: The 1st Multi-Target Speaker Detection and Identification Challenge Evaluation",
    "volume": "main",
    "abstract": "The Multi-target Challenge aims to assess how well current speech technology is able to determine whether or not a recorded utterance was spoken by one of a large number of blacklisted speakers. It is a form of multi-target speaker detection based on real-world telephone conversations. Data recordings are generated from call center customer-agent conversations. The task is to measure how accurately one can detect 1) whether a test recording is spoken by a blacklisted speaker, and 2) which specific blacklisted speaker was talking. This paper outlines the challenge and provides its baselines, results, and discussions",
    "checked": true,
    "id": "b3918fab36f106e83e016a3e33d260ad656191c4",
    "semantic_title": "mce 2018: the 1st multi-target speaker detection and identification challenge evaluation",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19_interspeech.html": {
    "title": "Improving Aggregation and Loss Function for Better Embedding Learning in End-to-End Speaker Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19_interspeech.html": {
    "title": "LSTM Based Similarity Measurement with Spectral Clustering for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chung19b_interspeech.html": {
    "title": "Who Said That?: Audio-Visual Speaker Diarisation of Real-World Meetings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xie19_interspeech.html": {
    "title": "Multi-PLDA Diarization on Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mccree19_interspeech.html": {
    "title": "Speaker Diarization Using Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ghahabi19_interspeech.html": {
    "title": "Speaker-Corrupted Embeddings for Online Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19_interspeech.html": {
    "title": "Speaker Diarization with Lexical Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shafey19_interspeech.html": {
    "title": "Joint Speech Recognition and Speaker Diarization via Sequence Transduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cumani19_interspeech.html": {
    "title": "Normal Variance-Mean Mixtures for Unsupervised Score Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yamamoto19_interspeech.html": {
    "title": "Speaker Augmentation and Bandwidth Extension for Deep Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ylmaz19_interspeech.html": {
    "title": "Large-Scale Speaker Diarization of Radio Broadcast Archives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dubey19_interspeech.html": {
    "title": "Toeplitz Inverse Covariance Based Robust Speaker Clustering for Naturalistic Audio Streams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kovacs19_interspeech.html": {
    "title": "Examining the Combination of Multi-Band Processing and Channel Dropout for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soni19_interspeech.html": {
    "title": "Label Driven Time-Frequency Masking for Robust Continuous Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19c_interspeech.html": {
    "title": "Speaker-Invariant Feature-Mapping for Distant Speech Recognition via Adversarial Teacher-Student Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ming19_interspeech.html": {
    "title": "Full-Sentence Correlation: A Method to Handle Unpredictable Noise for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soni19b_interspeech.html": {
    "title": "Generative Noise Modeling and Channel Simulation for Robust Speech Recognition in Unseen Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kumar19_interspeech.html": {
    "title": "Far-Field Speech Enhancement Using Heteroscedastic Autoencoder for Improved Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/delcroix19_interspeech.html": {
    "title": "End-to-End SpeakerBeam for Single Channel Target Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsu19_interspeech.html": {
    "title": "NIESR: Nuisance Invariant End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/suzuki19_interspeech.html": {
    "title": "Knowledge Distillation for Throat Microphone Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19d_interspeech.html": {
    "title": "Improved Speaker-Dependent Separation for CHiME-5 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19b_interspeech.html": {
    "title": "Bridging the Gap Between Monaural Speech Enhancement and Recognition with Distortion-Independent Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19c_interspeech.html": {
    "title": "Enhanced Spectral Features for Distortion-Independent Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/neekhara19b_interspeech.html": {
    "title": "Universal Adversarial Perturbations for Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fujimoto19_interspeech.html": {
    "title": "One-Pass Single-Channel Noisy Speech Recognition Using a Combination of Noisy and Enhanced Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19_interspeech.html": {
    "title": "Jointly Adversarial Enhancement Training for Robust End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19_interspeech.html": {
    "title": "Predicting Humor by Learning from Time-Aligned Comments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dinkov19_interspeech.html": {
    "title": "Predicting the Leading Political Ideology of YouTube Channels Using Acoustic, Textual, and Metadata Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/an19_interspeech.html": {
    "title": "Mitigating Gender and L1 Differences to Improve State and Trait Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/weninger19_interspeech.html": {
    "title": "Deep Learning Based Mandarin Accent Identification for Accent Robust ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gosztolya19_interspeech.html": {
    "title": "Calibrating DNN Posterior Probability Estimates of HMM/DNN Models to Improve Social Signal Detection from Audio Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mori19_interspeech.html": {
    "title": "Conversational and Social Laughter Synthesis with WaveNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ludusan19_interspeech.html": {
    "title": "Laughter Dynamics in Dyadic Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/truong19_interspeech.html": {
    "title": "Towards an Annotation Scheme for Complex Laughter in Speech Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/baird19_interspeech.html": {
    "title": "Using Speech to Predict Sequentially Measured Cortisol Levels During a Trier Social Stress Test",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/baird19b_interspeech.html": {
    "title": "Sincerity in Acted Speech: Presenting the Sincere Apology Corpus and Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niebuhr19c_interspeech.html": {
    "title": "Do not Hesitate! — Unless You Do it Shortly or Nasally: How the Phonetics of Filled Pauses Determine Their Subjective Frequency and Perceived Speaker Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vasquezcorrea19_interspeech.html": {
    "title": "Phonet: A Tool Based on Gated Recurrent Neural Networks to Extract Phonological Posteriors from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chang19_interspeech.html": {
    "title": "Code-Switching Sentence Generation by Generative Adversarial Networks and its Application to Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/meier19_interspeech.html": {
    "title": "Comparative Analysis of Think-Aloud Methods for Everyday Activities in the Context of Cognitive Robotics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/beeferman19_interspeech.html": {
    "title": "RadioTalk: A Large-Scale Corpus of Talk Radio Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mdhaffar19_interspeech.html": {
    "title": "Qualitative Evaluation of ASR Adaptation in a Lecture Context: Application to the PASTEL Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marinelli19_interspeech.html": {
    "title": "Active Annotation: Bootstrapping Annotation Lexicon and Guidelines for Supervised NLU Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dabike19_interspeech.html": {
    "title": "Automatic Lyric Transcription from Karaoke Vocal Tracks: Resources and a Baseline System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19b_interspeech.html": {
    "title": "Detecting Mismatch Between Speech and Transcription Using Cross-Modal Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vidal19_interspeech.html": {
    "title": "EpaDB: A Database for Development of Pronunciation Assessment Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/angerbauer19_interspeech.html": {
    "title": "Automatic Compression of Subtitles with Neural Networks and its Effect on User Experience",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luo19_interspeech.html": {
    "title": "Integrating Video Retrieval and Moment Detection in a Unified Corpus for Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gutz19_interspeech.html": {
    "title": "Early Identification of Speech Changes Due to Amyotrophic Lateral Sclerosis Using Machine Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/k19_interspeech.html": {
    "title": "Automatic Detection of Breath Using Voice Activity Detection and SVM Classifier with Application on News Reports",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heo19_interspeech.html": {
    "title": "Acoustic Scene Classification Using Teacher-Student Learning with Soft-Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19_interspeech.html": {
    "title": "Rare Sound Event Detection Using Deep Learning and Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19_interspeech.html": {
    "title": "A Combination of Model-Based and Feature-Based Strategy for Speech-to-Singing Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shrem19_interspeech.html": {
    "title": "Dr.VOT: Measuring Positive and Negative Voice Onset Time in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hui19_interspeech.html": {
    "title": "Effects of Base-Frequency and Spectral Envelope on Deep-Learning Speech Separation and Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shah19_interspeech.html": {
    "title": "Phone Aware Nearest Neighbor Technique Using Spectral Transition Measure for Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shankar19_interspeech.html": {
    "title": "Weakly Supervised Syllable Segmentation by Vowel-Consonant Peak Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mateju19_interspeech.html": {
    "title": "An Approach to Online Speaker Change Point Detection Using DNNs and WFSTs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tang19_interspeech.html": {
    "title": "Regression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paul19_interspeech.html": {
    "title": "Non-Parallel Voice Conversion Using Weighted Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chou19_interspeech.html": {
    "title": "One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19_interspeech.html": {
    "title": "One-Shot Voice Conversion with Global Speaker Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tobing19_interspeech.html": {
    "title": "Non-Parallel Voice Conversion with Cyclic Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaneko19_interspeech.html": {
    "title": "StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kurita19_interspeech.html": {
    "title": "Robustness of Statistical Voice Conversion Based on Direct Waveform Modification Against Background Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19b_interspeech.html": {
    "title": "Fast Learning for Non-Parallel Many-to-Many Voice Conversion with Residual Star Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/juvela19_interspeech.html": {
    "title": "GELP: GAN-Excited Linear Prediction for Speech Synthesis from Mel-Spectrogram",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yamamoto19b_interspeech.html": {
    "title": "Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mohammadi19_interspeech.html": {
    "title": "One-Shot Voice Conversion with Disentangled Representations by Leveraging Phonetic Posteriorgrams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19c_interspeech.html": {
    "title": "Investigation of F0 Conditioning and Fully Convolutional Networks in Variational Autoencoder Based Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19b_interspeech.html": {
    "title": "Jointly Trained Conversion Model and WaveNet Vocoder for Non-Parallel Voice Conversion Using Mel-Spectrograms and Phonetic Posteriorgrams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19b_interspeech.html": {
    "title": "Generative Adversarial Networks for Unpaired Voice Transformation on Impaired Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ding19_interspeech.html": {
    "title": "Group Latent Embedding for Vector Quantized Variational Autoencoder in Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/stephenson19_interspeech.html": {
    "title": "Semi-Supervised Voice Conversion with Amortized Variational Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dey19_interspeech.html": {
    "title": "Exploiting Semi-Supervised Training Through a Dropout Regularization in End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kim19_interspeech.html": {
    "title": "Improved Vocal Tract Length Perturbation for a State-of-the-Art End-to-End Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhu19_interspeech.html": {
    "title": "Multi-Accent Adaptation Based on Gate Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19_interspeech.html": {
    "title": "Unsupervised Adaptation with Adversarial Dropout Regularization for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kitza19_interspeech.html": {
    "title": "Cumulative Adaptation for BLSTM Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xie19b_interspeech.html": {
    "title": "Fast DNN Acoustic Model Speaker Adaptation by Learning Hidden Unit Contribution Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tsunoo19_interspeech.html": {
    "title": "End-to-End Adaptation with Backpropagation Through WFST for On-Device Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sar19_interspeech.html": {
    "title": "Learning Speaker Aware Offsets for Speaker Adaptation of Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sim19_interspeech.html": {
    "title": "An Investigation into On-Device Personalization of End-to-End Automatic Speech Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jain19_interspeech.html": {
    "title": "A Multi-Accent Acoustic Model Using Mixture of Experts for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shor19_interspeech.html": {
    "title": "Personalizing ASR for Dysarthric and Accented Speech with Limited Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peskov19_interspeech.html": {
    "title": "Mitigating Noisy Inputs for Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19_interspeech.html": {
    "title": "One-vs-All Models for Asynchronous Training: An Empirical Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marzinotto19_interspeech.html": {
    "title": "Adapting a FrameNet Semantic Parser for Spoken Language Understanding Using Adversarial Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parcollet19_interspeech.html": {
    "title": "M2H-GAN: A GAN-Based Mapping from Machine to Human Transcripts for Speech Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/georges19_interspeech.html": {
    "title": "Ultra-Compact NLU: Neuronal Network Binarization as Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lugosch19_interspeech.html": {
    "title": "Speech Model Pre-Training for End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shivakumar19_interspeech.html": {
    "title": "Spoken Language Intent Detection Using Confusion2Vec",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tomashenko19_interspeech.html": {
    "title": "Investigating Adaptation and Transfer Learning for End-to-End Spoken Language Understanding from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/song19_interspeech.html": {
    "title": "Topic-Aware Dialogue Speech Recognition with Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/masumura19_interspeech.html": {
    "title": "Improving Conversation-Context Language Models with Multiple Spoken Language Understanding Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chien19_interspeech.html": {
    "title": "Meta Learning for Hyperparameter Optimization in Dialogue System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/williams19_interspeech.html": {
    "title": "Zero Shot Intent Classification Using Long-Short Term Memory Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/korpusik19_interspeech.html": {
    "title": "A Comparison of Deep Learning Methods for Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kobayashi19_interspeech.html": {
    "title": "Slot Filling with Weighted Multi-Encoders for Out-of-Domain Values",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seneviratne19_interspeech.html": {
    "title": "Multi-Corpus Acoustic-to-Articulatory Speech Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dash19_interspeech.html": {
    "title": "Towards a Speaker Independent Speech-BCI Using Speaker Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sheth19_interspeech.html": {
    "title": "Identifying Input Features for Development of Real-Time Translation of Neural Signals to Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/silva19_interspeech.html": {
    "title": "Exploring Critical Articulator Identification from 50Hz RT-MRI Data of the Vocal Tract",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/douros19_interspeech.html": {
    "title": "Towards a Method of Dynamic Vocal Tract Shapes Generation by Combining Static 3D and Dynamic 2D MRI Speech Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rasskazova19_interspeech.html": {
    "title": "Temporal Coordination of Articulatory and Respiratory Events Prior to Speech Initiation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gubian19b_interspeech.html": {
    "title": "Zooming in on Spatiotemporal V-to-C Coarticulation with Functional PCA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/csapo19_interspeech.html": {
    "title": "Ultrasound-Based Silent Speech Interface Built on a Continuous Vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/klein19_interspeech.html": {
    "title": "Assessing Acoustic and Articulatory Dimensions of Speech Motor Adaptation with Random Forests",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/takemoto19_interspeech.html": {
    "title": "Speech Organ Contour Extraction Using Real-Time MRI and Machine Learning Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/leeuwen19_interspeech.html": {
    "title": "CNN-Based Phoneme Classifier from Vocal Tract MRI Learns Embedding Consistent with Articulatory Topology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mucke19_interspeech.html": {
    "title": "Strength and Structure: Coupling Tones with Oral Constriction Gestures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kleijn19_interspeech.html": {
    "title": "Salient Speech Representations Based on Cloned Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ramanathi19_interspeech.html": {
    "title": "ASR Inspired Syllable Stress Detection for Pronunciation Evaluation Without Using a Supervised Classifier and Syllable Level Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mannem19_interspeech.html": {
    "title": "Acoustic and Articulatory Feature Based Speech Rate Estimation Using a Convolutional Dense Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/springenberg19_interspeech.html": {
    "title": "Predictive Auxiliary Variational Autoencoder for Representation Learning of Global Speech Characteristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paraskevopoulos19_interspeech.html": {
    "title": "Unsupervised Low-Rank Representations for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dhiman19_interspeech.html": {
    "title": "On the Suitability of the Riesz Spectro-Temporal Envelope for WaveNet Based Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xu19_interspeech.html": {
    "title": "Autonomous Emotion Learning in Speech: A View of Zero-Shot Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sudhakara19_interspeech.html": {
    "title": "An Improved Goodness of Pronunciation (GoP) Measure for Pronunciation Evaluation with DNN-HMM System Considering HMM Transition Probabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/saha19b_interspeech.html": {
    "title": "Low Resource Automatic Intonation Classification Using Gated Recurrent Unit (GRU) Networks Pre-Trained with Synthesized Pitch Patterns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vasquezcorrea19b_interspeech.html": {
    "title": "Apkinson: A Mobile Solution for Multimodal Assessment of Patients with Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kiss19_interspeech.html": {
    "title": "Depression State Assessment: Application for Detection of Depression by Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yarra19_interspeech.html": {
    "title": "SPIRE-fluent: A Self-Learning App for Tutoring Oral Fluency to Second Language English Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nissen19b_interspeech.html": {
    "title": "Using Real-Time Visual Biofeedback for Second Language Instruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/miwardelli19_interspeech.html": {
    "title": "Splash: Speech and Language Assessment in Schools and Homes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/annand19_interspeech.html": {
    "title": "Using Ultrasound Imaging to Create Augmented Visual Biofeedback for Articulatory Practice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/radostev19_interspeech.html": {
    "title": "Speech-Based Web Navigation for Limited Mobility Users",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schultz19_interspeech.html": {
    "title": "Biosignal Processing for Human-Machine Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ryant19_interspeech.html": {
    "title": "The Second DIHARD Diarization Challenge: Dataset, Task, and Baselines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/singh19_interspeech.html": {
    "title": "LEAP Diarization System for the Second DIHARD Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vinals19_interspeech.html": {
    "title": "ViVoLAB Speaker Diarization System for the DIHARD 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zajic19_interspeech.html": {
    "title": "UWB-NTIS Speaker Diarization System for the DIHARD II 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19b_interspeech.html": {
    "title": "The Second DIHARD Challenge: System Description for USC-SAIL Team",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novoselov19_interspeech.html": {
    "title": "Speaker Diarization with Deep Speaker Embeddings for DIHARD Challenge II",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/todisco19_interspeech.html": {
    "title": "ASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lai19b_interspeech.html": {
    "title": "ASSERT: Anti-Spoofing with Squeeze-Excitation and Residual Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chettri19_interspeech.html": {
    "title": "Ensemble Models for Spoofing Detection in Automatic Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19_interspeech.html": {
    "title": "The DKU Replay Detection System for the ASVspoof 2019 Challenge: On Data Augmentation, Feature Representation, Classification, and Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biaobrzeski19_interspeech.html": {
    "title": "Robust Bayesian and Light Neural Networks for Voice Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lavrentyeva19_interspeech.html": {
    "title": "STC Antispoofing Systems for the ASVspoof2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19b_interspeech.html": {
    "title": "The SJTU Robust Anti-Spoofing System for the ASVspoof 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/alluri19_interspeech.html": {
    "title": "IIIT-H Spoofing Countermeasures for Automatic Speaker Verification Spoofing and Countermeasures Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19c_interspeech.html": {
    "title": "Anti-Spoofing Speaker Verification System with Multi-Feature Integration and Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/williams19b_interspeech.html": {
    "title": "Speech Replay Detection with x-Vector Attack Embeddings and Spectral Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/das19_interspeech.html": {
    "title": "Long Range Acoustic Features for Spoofed Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chang19b_interspeech.html": {
    "title": "Transfer-Representation Learning for Detecting Spoofing Attacks with Converted and Synthesized Speech in Automatic Speaker Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gomezalanis19_interspeech.html": {
    "title": "A Light Convolutional GRU-RNN Deep Feature Extractor for ASV Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zeinali19_interspeech.html": {
    "title": "Detecting Spoofing Attacks Using VGG and SincNet: BUT-Omilia Submission to ASVspoof 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/alzantot19_interspeech.html": {
    "title": "Deep Residual Neural Networks for Audio Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jung19_interspeech.html": {
    "title": "Replay Attack Detection with Complementary High-Resolution Information Using End-to-End DNN for the ASVspoof 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dunbar19_interspeech.html": {
    "title": "The Zero Resource Speech Challenge 2019: TTS Without T",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/feng19b_interspeech.html": {
    "title": "Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yusuf19_interspeech.html": {
    "title": "Temporally-Aware Acoustic Unit Discovery for Zerospeech 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eloff19_interspeech.html": {
    "title": "Unsupervised Acoustic Unit Discovery for Speech Synthesis Using Discrete Latent-Variable Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19c_interspeech.html": {
    "title": "Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/s19_interspeech.html": {
    "title": "Zero Resource Speech Synthesis Using Transcripts Derived from Perceptual Acoustic Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tjandra19_interspeech.html": {
    "title": "VQVAE Unsupervised Unit Discovery and Multi-Scale Code2Spec Inverter for Zerospeech Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niehues19_interspeech.html": {
    "title": "Survey Talk: A Survey on Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jia19_interspeech.html": {
    "title": "Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19d_interspeech.html": {
    "title": "End-to-End Speech Translation with Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gangi19_interspeech.html": {
    "title": "Adapting Transformer to End-to-End Spoken Language Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hillis19_interspeech.html": {
    "title": "Unsupervised Phonetic and Word Level Discovery for Speech to Speech Translation for Unwritten Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhattacharya19_interspeech.html": {
    "title": "Deep Speaker Recognition: Modular or Monolithic?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19d_interspeech.html": {
    "title": "On the Usage of Phonetic Information for Text-Independent Speaker Embedding Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ravanelli19_interspeech.html": {
    "title": "Learning Speaker Representations with Mutual Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/you19_interspeech.html": {
    "title": "Multi-Task Learning with High-Order Statistics for x-Vector Based Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19e_interspeech.html": {
    "title": "Data Augmentation Using Variational Autoencoder for Embedding Based Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/you19b_interspeech.html": {
    "title": "Deep Neural Network Embeddings with Gating Mechanisms for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhat19_interspeech.html": {
    "title": "Neural Transition Systems for Modeling Hierarchical Semantic Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vukotic19_interspeech.html": {
    "title": "Mining Polysemous Triplets with Recurrent Neural Networks for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ray19_interspeech.html": {
    "title": "Iterative Delexicalization for Improved Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhosale19_interspeech.html": {
    "title": "End-to-End Spoken Language Understanding: Bootstrapping in Low Resource Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/takatsu19_interspeech.html": {
    "title": "Recognition of Intentions of Users' Short Responses for Conversational News Delivery System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/caubriere19_interspeech.html": {
    "title": "Curriculum-Based Transfer Learning for an Effective End-to-End Spoken Language Understanding and Domain Portability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dash19b_interspeech.html": {
    "title": "Spatial and Spectral Fingerprint in the Brain: Speaker Identification from Single Trial MEG Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nijveld19_interspeech.html": {
    "title": "ERP Signal Analysis with Temporal Resolution Using a Time Window Bank",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bosch19_interspeech.html": {
    "title": "Phase Synchronization Between EEG Signals as a Function of Differences Between Stimuli Characteristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kharaman19_interspeech.html": {
    "title": "The Processing of Prosodic Cues to Rhetorical Question Interpretation: Psycholinguistic and Neurolinguistics Evidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/scharenborg19_interspeech.html": {
    "title": "The Neural Correlates Underlying Lexically-Guided Perceptual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parmonangan19_interspeech.html": {
    "title": "Speech Quality Evaluation of Synthesized Japanese Speech Using EEG",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19d_interspeech.html": {
    "title": "Multi-Microphone Adaptive Noise Cancellation for Robust Hotword Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19c_interspeech.html": {
    "title": "Multi-Task Multi-Network Joint-Learning of Deep Residual Networks and Cycle-Consistency Generative Adversarial Networks for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khokhlov19_interspeech.html": {
    "title": "R-Vectors: New Technique for Adaptation to Room Acoustics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kanda19b_interspeech.html": {
    "title": "Guided Source Separation Meets a Strong ASR Backend: Hitachi/Paderborn University Joint Investigation for Dinner Party ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/drude19_interspeech.html": {
    "title": "Unsupervised Training of Neural Mask-Based Beamforming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ma19_interspeech.html": {
    "title": "Acoustic Model Ensembling Using Effective Data Augmentation for CHiME-5 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19d_interspeech.html": {
    "title": "Survey Talk: End-to-End Deep Neural Network Based Speaker and Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/padi19_interspeech.html": {
    "title": "Attention Based Hybrid i-Vector BLSTM Model for Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jung19b_interspeech.html": {
    "title": "RawNet: Advanced End-to-End Deep Neural Network Using Raw Waveforms for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rao19_interspeech.html": {
    "title": "Target Speaker Extraction for Multi-Talker Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mazzawi19_interspeech.html": {
    "title": "Improving Keyword Spotting and Language Identification via Neural Architecture Search at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zheng19_interspeech.html": {
    "title": "Forward-Backward Decoding for Regularizing End-to-End TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19b_interspeech.html": {
    "title": "A New GAN-Based End-to-End TTS Training Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/he19_interspeech.html": {
    "title": "Robust Sequence-to-Sequence Acoustic Modeling with Stepwise Monotonic Attention for Neural TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19b_interspeech.html": {
    "title": "Joint Training Framework for Text-to-Speech and Voice Conversion Using Multi-Source Tacotron and WaveNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luong19_interspeech.html": {
    "title": "Training Multi-Speaker Neural Text-to-Speech Systems Using Speaker-Imbalanced Speech Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/okamoto19_interspeech.html": {
    "title": "Real-Time Neural Text-to-Speech with Sequence-to-Sequence Acoustic Model and WaveGlow or Single Gaussian WaveRNN Vocoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kafle19_interspeech.html": {
    "title": "Fusion Strategy for Prosodic and Lexical Representations of Word Importance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chien19b_interspeech.html": {
    "title": "Self Attention in Variational Sequential Learning for Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19_interspeech.html": {
    "title": "Multi-Modal Sentiment Analysis Using Deep Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shen19_interspeech.html": {
    "title": "Interpreting and Improving Deep Neural SLU Models via Vocabulary Importance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tundik19_interspeech.html": {
    "title": "Assessing the Semantic Space Bias Caused by ASR Error Propagation and its Effect on Spoken Document Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19e_interspeech.html": {
    "title": "Latent Topic Attention for Domain Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/narisetty19_interspeech.html": {
    "title": "A Unified Bayesian Source Modelling for Determined Blind Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/takahashi19_interspeech.html": {
    "title": "Recursive Speech Separation for Unknown Number of Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/appeltans19_interspeech.html": {
    "title": "Practical Applicability of Deep Neural Networks for Overlapping Speaker Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gu19_interspeech.html": {
    "title": "Speech Separation Using Independent Vector Analysis with an Amplitude Variable Gaussian Mixture Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19c_interspeech.html": {
    "title": "Improved Speech Separation with Time-and-Frequency Cross-Domain Joint Embedding and Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wichern19_interspeech.html": {
    "title": "WHAM!: Extending Speech Separation to Noisy Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nautsch19_interspeech.html": {
    "title": "Survey Talk: Preserving Privacy in Speaker and Speech Characterisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chermaz19_interspeech.html": {
    "title": "Evaluating Near End Listening Enhancement Algorithms in Realistic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/edraki19_interspeech.html": {
    "title": "Improvement and Assessment of Spectro-Temporal Modulation Analysis for Speech Intelligibility Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19c_interspeech.html": {
    "title": "Listener Preference on the Local Criterion for Ideal Binary-Masked Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dinh19_interspeech.html": {
    "title": "Using a Manifold Vocoder for Spectral Voice and Style Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/platen19_interspeech.html": {
    "title": "Multi-Span Acoustic Modelling Using Raw Waveform Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/merboldt19_interspeech.html": {
    "title": "An Analysis of Local Monotonic Attention Variants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19b_interspeech.html": {
    "title": "Layer Trajectory BLSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karita19_interspeech.html": {
    "title": "Improving Transformer-Based End-to-End Speech Recognition with Connectionist Temporal Classification and Language Model Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19d_interspeech.html": {
    "title": "Trainable Dynamic Subsampling for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19d_interspeech.html": {
    "title": "Shallow-Fusion End-to-End Contextual Biasing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nasir19_interspeech.html": {
    "title": "Modeling Interpersonal Linguistic Coordination in Conversations Using Word Mover's Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/du19_interspeech.html": {
    "title": "Bag-of-Acoustic-Words for Mental Health Assessment: A Deep Autoencoding Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/voleti19_interspeech.html": {
    "title": "Objective Assessment of Social Skills Using Automated Language Analysis for Identification of Schizophrenia and Bipolar Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/matton19_interspeech.html": {
    "title": "Into the Wild: Transitioning from Recognizing Mood in Clinical Interactions to Personal Conversations for Individuals with Bipolar Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rohanian19_interspeech.html": {
    "title": "Detecting Depression with Word-Level Multimodal Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/espywilson19_interspeech.html": {
    "title": "Assessing Neuromotor Coordination in Depression Using Inverted Vocal Tract Variables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paul19b_interspeech.html": {
    "title": "Towards Universal Dialogue Act Tagging for Task-Oriented Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/goel19_interspeech.html": {
    "title": "HyST: A Hybrid Approach for Flexible and Accurate Dialogue State Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/martinek19_interspeech.html": {
    "title": "Multi-Lingual Dialogue Act Recognition with Deep Learning Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chao19_interspeech.html": {
    "title": "BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/griol19_interspeech.html": {
    "title": "Discovering Dialog Rules by Means of an Evolutionary Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19c_interspeech.html": {
    "title": "Active Learning for Domain Classification in a Commercial Spoken Personal Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sadjadi19_interspeech.html": {
    "title": "The 2018 NIST Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/villalba19_interspeech.html": {
    "title": "State-of-the-Art Speaker Recognition for Telephone and Video Speech: The JHU-MIT Submission for NIST SRE18",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/garciaromero19_interspeech.html": {
    "title": "x-Vector DNN Refinement with Full-Length Recordings for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19_interspeech.html": {
    "title": "I4U Submission to NIST SRE 2018: Leveraging from a Decade of Shared Experiences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khoury19_interspeech.html": {
    "title": "Pindrop Labs' Submission to the First Multi-Target Speaker Detection and Identification Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/garciaromero19b_interspeech.html": {
    "title": "Speaker Recognition Benchmark Using the CHiME-5 Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ayllon19_interspeech.html": {
    "title": "Investigating the Effects of Noisy and Reverberant Speech in Text-to-Speech Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kuo19_interspeech.html": {
    "title": "Selection and Training Schemes for Improving TTS Voice Built on Found Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/braude19_interspeech.html": {
    "title": "All Together Now: The Living Audio Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zen19_interspeech.html": {
    "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shamsi19_interspeech.html": {
    "title": "Corpus Design Using Convolutional Auto-Encoder Embeddings for Audio-Book Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hojo19_interspeech.html": {
    "title": "Evaluating Intention Communication by TTS Using Explicit Definitions of Illocutionary Act Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lo19_interspeech.html": {
    "title": "MOSNet: Deep Learning-Based Objective Assessment for Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fong19_interspeech.html": {
    "title": "Investigating the Robustness of Sequence-to-Sequence Text-to-Speech Models to Imperfectly-Transcribed Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/govender19_interspeech.html": {
    "title": "Using Pupil Dilation to Measure Cognitive Load When Listening to Text-to-Speech in Quiet and in Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/douros19b_interspeech.html": {
    "title": "A Multimodal Real-Time MRI Articulatory Corpus of French for Speech Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19d_interspeech.html": {
    "title": "A Chinese Dataset for Identifying Speakers in Novels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19c_interspeech.html": {
    "title": "CSS10: A Collection of Single Speaker Speech Datasets for 10 Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karaulov19_interspeech.html": {
    "title": "Attention Model for Articulatory Features Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tong19_interspeech.html": {
    "title": "Unbiased Semi-Supervised LF-MMI Training Using Dropout",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cui19_interspeech.html": {
    "title": "Acoustic Model Optimization Based on Evolutionary Stochastic Gradient Descent with Anchors for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shah19b_interspeech.html": {
    "title": "Whether to Pretrain DNN or not?: An Empirical Analysis for Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/goyal19_interspeech.html": {
    "title": "Detection of Glottal Closure Instants from Raw Speech Using Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fainberg19_interspeech.html": {
    "title": "Lattice-Based Lightly-Supervised Acoustic Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/michel19_interspeech.html": {
    "title": "Comparison of Lattice-Free and Lattice-Based Sequence Discriminative Training Criteria for LVCSR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/masumura19b_interspeech.html": {
    "title": "End-to-End Automatic Speech Recognition with a Reconstruction Criterion Using Speech-to-Text and Text-to-Speech Encoder-Decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heba19_interspeech.html": {
    "title": "Char+CV-CTC: Combining Graphemes and Consonant/Vowel Units for CTC-Based ASR Using Multitask Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kurata19_interspeech.html": {
    "title": "Guiding CTC Posterior Spike Timings for Improved Posterior Fusion and Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fukuda19_interspeech.html": {
    "title": "Direct Neuron-Wise Fusion of Cognate Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ladkat19_interspeech.html": {
    "title": "Two Tiered Distributed Training Algorithm for Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19f_interspeech.html": {
    "title": "Exploring the Encoder Layers of Discriminative Autoencoders for LVCSR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kurata19b_interspeech.html": {
    "title": "Multi-Task CTC Training with Auxiliary Feature Reconstruction for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19e_interspeech.html": {
    "title": "Framewise Supervised Training Towards End-to-End Speech Recognition Models: First Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/georgiou19_interspeech.html": {
    "title": "Deep Hierarchical Fusion with Application in Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mitra19_interspeech.html": {
    "title": "Leveraging Acoustic Cues and Paralinguistic Embeddings to Detect Expression from Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parry19_interspeech.html": {
    "title": "Analysis of Deep Learning Architectures for Cross-Corpus Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19e_interspeech.html": {
    "title": "A Path Signature Approach for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/egorow19_interspeech.html": {
    "title": "Employing Bottleneck and Convolutional Features for Speech-Based Physical Load Detection on Limited Data Amounts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19e_interspeech.html": {
    "title": "Speech Emotion Recognition in Dyadic Dialogues with Attentive Interaction Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhong19_interspeech.html": {
    "title": "Predicting Group Performances Using a Personality Composite-Network Architecture During Collaborative Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chao19b_interspeech.html": {
    "title": "Enforcing Semantic Consistency for Cross Corpus Valence Regression from Speech Using Adversarial Discrepancy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mao19_interspeech.html": {
    "title": "Deep Learning of Segment-Level Feature Representation with Multiple Instance Learning for Utterance-Level Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/triantafyllopoulos19_interspeech.html": {
    "title": "Towards Robust Speech Emotion Recognition Using Deep Residual Networks for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19f_interspeech.html": {
    "title": "Towards Discriminative Representations and Unbiased Predictions: Class-Specific Angular Softmax for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jalal19_interspeech.html": {
    "title": "Learning Temporal Clusters Using Capsule Routing for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dapolito19_interspeech.html": {
    "title": "L2 Pronunciation Accuracy and Context: A Pilot Study on the Realization of Geminates in Italian as L2 by French Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jamakovic19_interspeech.html": {
    "title": "The Monophthongs of Formal Nigerian English: An Acoustic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/arantes19_interspeech.html": {
    "title": "Quantifying Fundamental Frequency Modulation as a Function of Language, Speaking Style and Speaker",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kelly19_interspeech.html": {
    "title": "The Voicing Contrast in Stops and Affricates in the Western Armenian of Lebanon",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jatteau19_interspeech.html": {
    "title": "Gra[f] e!\" Word-Final Devoicing of Obstruents in Standard French: An Acoustic Study Based on Large Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19g_interspeech.html": {
    "title": "Acoustic Indicators of Deception in Mandarin Daily Conversations Recorded from an Interactive Game",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schuppler19_interspeech.html": {
    "title": "Prosodic Effects on Plosive Duration in German and Austrian German",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/johny19_interspeech.html": {
    "title": "Cross-Lingual Consistency of Phonological Features: An Empirical Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guitardivent19_interspeech.html": {
    "title": "Are IP Initial Vowels Acoustically More Distinct? Results from LDA and CNN Classifications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wei19_interspeech.html": {
    "title": "Neural Network-Based Modeling of Phonetic Durations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moczanow19_interspeech.html": {
    "title": "An Acoustic Study of Vowel Undershoot in a System with Several Degrees of Prominence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/berger19_interspeech.html": {
    "title": "A Preliminary Study of Charismatic Speech on YouTube: Correlating Prosodic Variation with Counts of Subscribers, Views and Likes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luo19b_interspeech.html": {
    "title": "Phonetic Detail Encoding in Explaining the Size of Speech Planning Window",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zarka19_interspeech.html": {
    "title": "Acoustic Cues to Topic and Narrow Focus in Egyptian Arabic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/alowonou19_interspeech.html": {
    "title": "Acoustic and Articulatory Study of Ewe Vowels: A Comparative Study of Male and Female",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19c_interspeech.html": {
    "title": "Speech Augmentation via Speaker-Specific Noise in Unseen Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hao19_interspeech.html": {
    "title": "UNetGAN: A Robust Speech Enhancement Approach in Time Domain for Extremely Low Signal-to-Noise Ratio Condition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pascual19b_interspeech.html": {
    "title": "Towards Generalized Speech Enhancement with Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19g_interspeech.html": {
    "title": "A Convolutional Neural Network with Non-Local Module for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19b_interspeech.html": {
    "title": "IA-NET: Acceleration and Compression of Speech Enhancement Using Integer-Adder Deep Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chai19_interspeech.html": {
    "title": "KL-Divergence Regularized Deep Neural Network Adaptation for Low-Resource Speaker-Dependent Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/llombart19_interspeech.html": {
    "title": "Speech Enhancement with Wide Residual Networks in Reverberant Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/reddy19_interspeech.html": {
    "title": "A Scalable Noisy Speech Dataset and Online Subjective Test Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/adiga19_interspeech.html": {
    "title": "Speech Enhancement for Noise-Robust Speech Synthesis Using Wasserstein GAN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pv19_interspeech.html": {
    "title": "A Non-Causal FFTNet Architecture for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/braithwaite19_interspeech.html": {
    "title": "Speech Enhancement with Variance Constrained Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kyriakopoulos19_interspeech.html": {
    "title": "A Deep Learning Approach to Automatic Characterisation of Rhythm in Non-Native English Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/merkx19_interspeech.html": {
    "title": "Language Learning Using Speech to Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/skidmore19_interspeech.html": {
    "title": "Using Alexa for Flashcard-Based Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hansen19_interspeech.html": {
    "title": "The 2019 Inaugural Fearless Steps Challenge: A Giant Leap for Naturalistic Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19e_interspeech.html": {
    "title": "Completely Unsupervised Phoneme Recognition by a Generative Adversarial Network Harmonized with Iteratively Refined Hidden Markov Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/trisitichoke19_interspeech.html": {
    "title": "Analysis of Native Listeners' Facial Microexpressions While Shadowing Non-Native Speech — Potential of Shadowers' Facial Expressions for Comprehensibility Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karhila19_interspeech.html": {
    "title": "Transparent Pronunciation Scoring Using Articulatorily Weighted Phoneme Edit Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yoon19_interspeech.html": {
    "title": "Development of Robust Automated Scoring Models Using Adversarial Input for Oral Proficiency Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19b_interspeech.html": {
    "title": "Impact of ASR Performance on Spoken Grammatical Error Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19d_interspeech.html": {
    "title": "Self-Imitating Feedback Generation Using GAN for Computer-Assisted Pronunciation Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hori19_interspeech.html": {
    "title": "Joint Student-Teacher Learning for Audio-Visual Scene-Aware Dialog",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gopalakrishnan19_interspeech.html": {
    "title": "Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kubasova19_interspeech.html": {
    "title": "Analyzing Verbal and Nonverbal Features for Predicting Group Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/martinez19_interspeech.html": {
    "title": "Identifying Therapist and Client Personae for Therapeutic Alliance Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/haake19_interspeech.html": {
    "title": "Do Hesitations Facilitate Processing of Partially Defective System Utterances? An Exploratory Eye Tracking Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19h_interspeech.html": {
    "title": "Influence of Contextuality on Prosodic Realization of Information Structure in Chinese Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gjoreski19_interspeech.html": {
    "title": "Cross-Lingual Transfer Learning for Affective Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yu19_interspeech.html": {
    "title": "Identifying Personality Traits Using Overlap Dynamics in Multiparty Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/aldeneh19_interspeech.html": {
    "title": "Identifying Mood Episodes Using Dialogue Features from Clinical Interviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lubold19_interspeech.html": {
    "title": "Do Conversational Partners Entrain on Articulatory Precision?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lian19_interspeech.html": {
    "title": "Conversational Emotion Analysis via Attention Mechanisms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oneill19_interspeech.html": {
    "title": "The Effect of Phoneme Distribution on Perceptual Similarity in English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kakouros19_interspeech.html": {
    "title": "Prosodic Representations of Prominence Classification Neural Networks and Autoencoders Using Bottleneck Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peperkamp19_interspeech.html": {
    "title": "Compensation for French Liquid Deletion During Auditory Sentence Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kocharov19_interspeech.html": {
    "title": "Prosodic Factors Influencing Vowel Reduction in Russian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gobl19_interspeech.html": {
    "title": "Time to Frequency Domain Mapping of the Voice Source: The Influence of Open Quotient and Glottal Skew on the Low End of the Source Spectrum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chodroff19_interspeech.html": {
    "title": "Testing the Distinctiveness of Intonational Tunes: Evidence from Imitative Productions in American English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19d_interspeech.html": {
    "title": "A Study of a Cross-Language Perception Based on Cortical Analysis Using Biomimetic STRFs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sturm19_interspeech.html": {
    "title": "Perceptual Evaluation of Early versus Late F0 Peaks in the Intonation Structure of Czech Question-Word Questions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kelterer19_interspeech.html": {
    "title": "Acoustic Correlates of Phonation Type in Chichimec",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chien19c_interspeech.html": {
    "title": "F0 Variability Measures Based on Glottal Closure Instants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tavi19_interspeech.html": {
    "title": "Recognition of Creaky Voice from Emergency Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xu19b_interspeech.html": {
    "title": "Direct F0 Estimation with Neural-Network-Based Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19b_interspeech.html": {
    "title": "Real Time Online Visual End Point Detection Using Unidirectional LSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ardaillon19_interspeech.html": {
    "title": "Fully-Convolutional Network for Pitch Estimation of Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dong19_interspeech.html": {
    "title": "Vocal Pitch Extraction in Polyphonic Music Using Convolutional Residual Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19c_interspeech.html": {
    "title": "Multi-Level Adaptive Speech Activity Detector for Speech in Naturalistic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19d_interspeech.html": {
    "title": "On the Importance of Audio-Source Separation for Singer Identification in Polyphonic Music",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/terasawa19_interspeech.html": {
    "title": "Investigating the Physiological and Acoustic Contrasts Between Choral and Operatic Singing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19c_interspeech.html": {
    "title": "Optimizing Voice Activity Detection for Noisy Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yamamoto19c_interspeech.html": {
    "title": "Small-Footprint Magic Word Detection Method Using Convolutional LSTM Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19b_interspeech.html": {
    "title": "Acoustic Modeling for Automatic Lyrics-to-Audio Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vafeiadis19_interspeech.html": {
    "title": "Two-Dimensional Convolutional Recurrent Neural Networks for Speech Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaburagi19_interspeech.html": {
    "title": "A Study of Soprano Singing in Light of the Source-Filter Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zou19_interspeech.html": {
    "title": "Boosting Character-Based Chinese Speech Synthesis via Multi-Task Learning and Dictionary Tutoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xue19_interspeech.html": {
    "title": "Building a Mixed-Lingual Neural TTS System with Only Monolingual Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sokolov19_interspeech.html": {
    "title": "Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/taylor19_interspeech.html": {
    "title": "Analysis of Pronunciation Learning in End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19f_interspeech.html": {
    "title": "End-to-End Text-to-Speech for Low-Resource Languages by Cross-Lingual Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19e_interspeech.html": {
    "title": "Learning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice Cloning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/juzova19_interspeech.html": {
    "title": "Unified Language-Independent DNN-Based G2P Converter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dai19_interspeech.html": {
    "title": "Disambiguation of Chinese Polyphones in an End-to-End Framework with Semantic Features Extracted by Pre-Trained BERT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yolchuyeva19_interspeech.html": {
    "title": "Transformer Based Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bleyan19_interspeech.html": {
    "title": "Developing Pronunciation Models in New Languages Faster by Exploiting Common Grapheme-to-Phoneme Correspondences Across Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19g_interspeech.html": {
    "title": "Cross-Lingual, Multi-Speaker Text-To-Speech Synthesis Using Neural Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19b_interspeech.html": {
    "title": "Polyphone Disambiguation for Mandarin Chinese Using Conditional Neural Network with Multi-Level Embedding Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19c_interspeech.html": {
    "title": "Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19i_interspeech.html": {
    "title": "Multilingual Speech Recognition with Corpus Relatedness Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/arsikere19_interspeech.html": {
    "title": "Multi-Dialect Acoustic Modeling Using Phone Mapping and Online i-Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kannan19_interspeech.html": {
    "title": "Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mendes19_interspeech.html": {
    "title": "Recognition of Latin American Spanish Using Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/viglino19_interspeech.html": {
    "title": "End-to-End Accented Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19j_interspeech.html": {
    "title": "End-to-End Articulatory Attribute Modeling for Low-Resource Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/taneja19_interspeech.html": {
    "title": "Exploiting Monolingual Speech Corpora for Code-Mixed Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19_interspeech.html": {
    "title": "Phoneme-Based Contextualization for Cross-Lingual Speech Recognition in End-to-End Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khassanov19_interspeech.html": {
    "title": "Constrained Output Embeddings for End-to-End Code-Switching Speech Recognition with Only Monolingual Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zeng19_interspeech.html": {
    "title": "On the End-to-End Solution to Mandarin-English Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19f_interspeech.html": {
    "title": "Towards Language-Universal Mandarin-English Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/swarup19_interspeech.html": {
    "title": "Improving ASR Confidence Scores for Alexa Using Acoustic and Hypothesis Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19g_interspeech.html": {
    "title": "Investigation of Transformer Based Spelling Correction Model for CTC-Based End-to-End Mandarin Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peyser19_interspeech.html": {
    "title": "Improving Performance of End-to-End ASR on Numeric Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bai19_interspeech.html": {
    "title": "A Time Delay Neural Network with Shared Weight Self-Attention for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kao19_interspeech.html": {
    "title": "Sub-Band Convolutional Neural Networks for Small-Footprint Spoken Term Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19k_interspeech.html": {
    "title": "Investigating Radical-Based End-to-End Speech Recognition Systems for Chinese Dialects and Japanese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19d_interspeech.html": {
    "title": "Joint Decoding of CTC Based Systems for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tanaka19_interspeech.html": {
    "title": "A Joint End-to-End and DNN-HMM Hybrid Automatic Speech Recognition System with Transferring Sharable Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/malhotra19_interspeech.html": {
    "title": "Active Learning Methods for Low Resource End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karafiat19_interspeech.html": {
    "title": "Analysis of Multilingual Sequence-to-Sequence Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zapotoczny19_interspeech.html": {
    "title": "Lattice Generation in Attention-Based Speech Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jansche19_interspeech.html": {
    "title": "Sampling from Stochastic Finite Automata with Applications to CTC Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dudziak19_interspeech.html": {
    "title": "ShrinkML: End-to-End ASR Model Compression Using Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gaur19_interspeech.html": {
    "title": "Acoustic-to-Phrase Models for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19l_interspeech.html": {
    "title": "Performance Monitoring for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cohn19b_interspeech.html": {
    "title": "The Role of Musical Experience in the Perceptual Weighting of Acoustic Cues for the Obstruent Coda Voicing Contrast in American English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lewandowski19_interspeech.html": {
    "title": "Individual Differences in Implicit Attention to Phonetic Detail in Speech Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lalonde19_interspeech.html": {
    "title": "Effects of Natural Variability in Cross-Modal Temporal Correlations on Audiovisual Speech Recognition Benefit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bentum19_interspeech.html": {
    "title": "Listening with Great Expectations: An Investigation of Word Form Anticipations in Naturalistic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bentum19b_interspeech.html": {
    "title": "Quantifying Expectation Modulation in Human Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/turner19_interspeech.html": {
    "title": "Perception of Pitch Contours in Speech and Nonspeech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bosch19b_interspeech.html": {
    "title": "Analyzing Reaction Time and Error Sequences in Lexical Decision Experiments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19e_interspeech.html": {
    "title": "Automatic Detection of the Temporal Segmentation of Hand Movements in British English Cued Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yokoe19_interspeech.html": {
    "title": "Place Shift as an Autonomous Process: Evidence from Japanese Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/meyer19_interspeech.html": {
    "title": "A Perceptual Study of CV Syllables in Both Spoken and Whistled Speech: A Tashlhiyt Berber Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsieh19_interspeech.html": {
    "title": "Consonant Classification in Mandarin Based on the Depth Image Feature: A Pilot Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/levari19_interspeech.html": {
    "title": "The Different Roles of Expectations in Phonetic and Lexical Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/segedin19_interspeech.html": {
    "title": "Perceptual Adaptation to Device and Human Voices: Learning and Generalization of a Phonetic Shift Across Real and Voice-AI Talkers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/papadimitriou19_interspeech.html": {
    "title": "End-to-End Convolutional Sequence Learning for ASL Fingerspelling Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/somandepalli19_interspeech.html": {
    "title": "Multiview Shared Subspace Learning Across Speakers and Speech Commands",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/belitz19_interspeech.html": {
    "title": "A Machine Learning Based Clustering Protocol for Determining Hearing Aid Initial Configurations from Pure-Tone Audiograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nguyen19_interspeech.html": {
    "title": "Acoustic Scene Classification with Mismatched Devices Using CliqueNets and Mixup Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ahmed19_interspeech.html": {
    "title": "DeepLung: Smartphone Convolutional Neural Network-Based Inference of Lung Anomalies for Pulmonary Patients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moore19_interspeech.html": {
    "title": "On the Use/Misuse of the Term ‘Phoneme",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/muckenhirn19_interspeech.html": {
    "title": "Understanding and Visualizing Raw Waveform-Based CNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kilgour19_interspeech.html": {
    "title": "Fréchet Audio Distance: A Reference-Free Metric for Evaluating Music Enhancement Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gong19_interspeech.html": {
    "title": "ReMASC: Realistic Replay Attack Corpus for Voice Controlled Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bt19_interspeech.html": {
    "title": "Analyzing Intra-Speaker and Inter-Speaker Vocal Tract Impedance Characteristics in a Low-Dimensional Feature Space Using t-SNE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19b_interspeech.html": {
    "title": "Directional Audio Rendering Using a Neural Network Based Personalized HRTF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pienaar19_interspeech.html": {
    "title": "Online Speech Processing and Analysis Suite",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/maurer19_interspeech.html": {
    "title": "Formant Pattern and Spectral Shape Ambiguity of Vowel Sounds, and Related Phenomena of Vowel Acoustics — Exemplary Evidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/noll19_interspeech.html": {
    "title": "Sound Tools eXtended (STx) 5.0 — A Powerful Sound Analysis Tool Optimized for Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eldesouki19_interspeech.html": {
    "title": "FarSpeech: Arabic Natural Language Processing for Live Arabic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/haider19_interspeech.html": {
    "title": "A System for Real-Time Privacy Preserving Data Collection for Ambient Assisted Living",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19c_interspeech.html": {
    "title": "NUS Speak-to-Sing: A Web Platform for Personalized Speech-to-Singing Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaltenbacher19_interspeech.html": {
    "title": "Physiology and Physics of Voice Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schuller19_interspeech.html": {
    "title": "The INTERSPEECH 2019 Computational Paralinguistics Challenge: Styrian Dialects, Continuous Sleepiness, Baby Sounds & Orca Activity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dubagunta19_interspeech.html": {
    "title": "Using Speech Production Knowledge for Raw Waveform Modelling Based Styrian Dialect Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/elsner19_interspeech.html": {
    "title": "Deep Neural Baselines for Computational Paralinguistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kisler19_interspeech.html": {
    "title": "Styrian Dialect Classification: Comparing and Fusing Classifiers Based on a Feature Selection Using a Genetic Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yeh19_interspeech.html": {
    "title": "Using Attention Networks and Adversarial Augmentation for Styrian Dialect Continuous Sleepiness and Baby Sound Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19f_interspeech.html": {
    "title": "Ordinal Triplet Loss: Investigating Sleepiness Detection from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ravi19_interspeech.html": {
    "title": "Voice Quality and Between-Frame Entropy for Sleepiness Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gosztolya19b_interspeech.html": {
    "title": "Using Fisher Vector and Bag-of-Audio-Words Representations to Identify Styrian Dialects, Sleepiness, Baby & Orca Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/das19b_interspeech.html": {
    "title": "Instantaneous Phase and Long-Term Acoustic Cues for Orca Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schiller19_interspeech.html": {
    "title": "Relevance-Based Feature Masking: Improving Neural Network Based Whale Classification Through Explainable Artificial Intelligence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/caraty19_interspeech.html": {
    "title": "Spatial, Temporal and Spectral Multiresolution Analysis for the INTERSPEECH 2019 ComParE Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19g_interspeech.html": {
    "title": "The DKU-LENOVO Systems for the INTERSPEECH 2019 Computational Paralinguistic Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nandwana19_interspeech.html": {
    "title": "The VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novoselov19b_interspeech.html": {
    "title": "STC Speaker Recognition Systems for the VOiCES from a Distance Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/matejka19_interspeech.html": {
    "title": "Analysis of BUT Submission in Far-Field Scenarios of VOiCES 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/medennikov19_interspeech.html": {
    "title": "The STC ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chong19_interspeech.html": {
    "title": "The I2R's ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nandwana19b_interspeech.html": {
    "title": "The VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novoselov19c_interspeech.html": {
    "title": "STC Speaker Recognition Systems for the VOiCES from a Distance Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/matejka19b_interspeech.html": {
    "title": "Analysis of BUT Submission in Far-Field Scenarios of VOiCES 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/medennikov19b_interspeech.html": {
    "title": "The STC ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chong19b_interspeech.html": {
    "title": "The I2R's ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jati19_interspeech.html": {
    "title": "Multi-Task Discriminative Training of Hybrid DNN-TVM Model for Speaker Verification with Noisy and Far-Field Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/snyder19b_interspeech.html": {
    "title": "The JHU Speaker Recognition System for the VOiCES 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19h_interspeech.html": {
    "title": "Intel Far-Field Speaker Recognition System for VOiCES Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19d_interspeech.html": {
    "title": "The I2R's Submission to VOiCES Distance Speaker Recognition Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liang19_interspeech.html": {
    "title": "The LeVoice Far-Field Speech Recognition System for VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19f_interspeech.html": {
    "title": "The JHU ASR System for VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19c_interspeech.html": {
    "title": "The DKU System for the Speaker Recognition Task of the 2019 VOiCES from a Distance Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hauptman19_interspeech.html": {
    "title": "Identifying Distinctive Acoustic and Spectral Features in Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/drioli19_interspeech.html": {
    "title": "Aerodynamics and Lumped-Masses Combined with Delay Lines for Modeling Vertical and Anterior-Posterior Phase Differences in Pathological Vocal Fold Vibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kadiri19_interspeech.html": {
    "title": "Mel-Frequency Cepstral Coefficients of Voice Source Waveforms for Classification of Phonation Types in Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cho19_interspeech.html": {
    "title": "Automatic Detection of Autism Spectrum Disorder in Children Using Acoustic and Text Features from Brief Natural Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schoentgen19_interspeech.html": {
    "title": "Analysis and Synthesis of Vocal Flutter and Vocal Jitter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schaeffler19_interspeech.html": {
    "title": "Reliability of Clinical Voice Parameters Captured with Smartphones — Measurements of Added Noise and Spectral Tilt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moore19b_interspeech.html": {
    "title": "Say What? A Dataset for Exploring the Error Patterns That Two ASR Engines Make",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ward19_interspeech.html": {
    "title": "Survey Talk: Prosody Research and Applications: The State of the Art",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/roessig19_interspeech.html": {
    "title": "Dimensions of Prosodic Prominence in an Attractor Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/suni19_interspeech.html": {
    "title": "Comparative Analysis of Prosodic Characteristics Using WaveNet Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/murphy19_interspeech.html": {
    "title": "The Role of Voice Quality in the Perception of Prominence in Synthetic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/albar19_interspeech.html": {
    "title": "Phonological Awareness of French Rising Contours in Japanese Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/okawa19_interspeech.html": {
    "title": "Audio Classification of Bit-Representation Waveform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mulimani19_interspeech.html": {
    "title": "Locality-Constrained Linear Coding Based Fused Visual Features for Robust Acoustic Event Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shen19b_interspeech.html": {
    "title": "Learning How to Listen: A Temporal-Frequential Attention Model for Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ford19_interspeech.html": {
    "title": "A Deep Residual Network for Large-Scale Acoustic Scene Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/reddy19b_interspeech.html": {
    "title": "Supervised Classifiers for Audio Impairments with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tarantino19_interspeech.html": {
    "title": "Self-Attention for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nachmani19_interspeech.html": {
    "title": "Unsupervised Singing Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19c_interspeech.html": {
    "title": "Adversarially Trained End-to-End Korean Singing Voice Synthesis System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yi19_interspeech.html": {
    "title": "Singing Voice Synthesis Using Deep Autoregressive Neural Networks for Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dahmani19_interspeech.html": {
    "title": "Conditional Variational Auto-Encoder for Text-Driven Expressive AudioVisual Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ayllon19b_interspeech.html": {
    "title": "A Strategy for Improved Phone-Level Lyrics-to-Audio Alignment for Speech-to-Singing Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biasuttolervat19_interspeech.html": {
    "title": "Modeling Labial Coarticulation with Bidirectional Gated Recurrent Networks and Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19e_interspeech.html": {
    "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/audhkhasi19_interspeech.html": {
    "title": "Forget a Bit to Learn Better: Soft Forgetting for CTC-Based Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/miao19_interspeech.html": {
    "title": "Online Hybrid CTC/Attention Architecture for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19h_interspeech.html": {
    "title": "A Highly Efficient Distributed Deep Learning System for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19i_interspeech.html": {
    "title": "Knowledge Distillation for End-to-End Monaural Multi-Talker ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/menne19_interspeech.html": {
    "title": "Analysis of Deep Clustering as Preprocessing for Automatic Speech Recognition of Sparsely Overlapping Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bradlow19_interspeech.html": {
    "title": "Survey Talk: Recognition of Foreign-Accented Speech: Challenges and Opportunities for Human and Computer Speech Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novak19_interspeech.html": {
    "title": "The Effects of Time Expansion on English as a Second Language Individuals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19_interspeech.html": {
    "title": "Capturing L1 Influence on L2 Pronunciation by Simulating Perceptual Space Using Acoustic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19h_interspeech.html": {
    "title": "Cognitive Factors in Thai-Naïve Mandarin Speakers' Imitation of Thai Lexical Tones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tremblay19_interspeech.html": {
    "title": "Foreign-Language Knowledge Enhances Artificial-Language Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/abujabal19_interspeech.html": {
    "title": "Neural Named Entity Recognition from Subword Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhati19_interspeech.html": {
    "title": "Unsupervised Acoustic Segmentation and Clustering Using Siamese Network Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yusuf19b_interspeech.html": {
    "title": "An Empirical Evaluation of DTW Subsampling Methods for Keyword Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19e_interspeech.html": {
    "title": "Linguistically-Informed Training of Acoustic Word Embeddings for Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19g_interspeech.html": {
    "title": "Multimodal Word Discovery and Retrieval with Phone Sequence and Image Concepts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/boito19_interspeech.html": {
    "title": "Empirical Evaluation of Sequence-to-Sequence Models for Word Discovery in Low-Resource Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xue19b_interspeech.html": {
    "title": "Direct-Path Signal Cross-Correlation Estimation for Sound Source Localization in Reverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/grondin19_interspeech.html": {
    "title": "Multiple Sound Source Localization with SVD-PHAT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19j_interspeech.html": {
    "title": "Robust DOA Estimation Based on Convolutional Neural Network and Time-Frequency Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/masuyama19_interspeech.html": {
    "title": "Multichannel Loss Function for Supervised Speech Source Separation by Mask-Based Beamforming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19m_interspeech.html": {
    "title": "Direction-Aware Speaker Beam for Multi-Channel Speaker Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ochiai19_interspeech.html": {
    "title": "Multimodal SpeakerBeam: Single Channel Target Speech Extraction with Audio-Visual Speaker Clues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/germain19_interspeech.html": {
    "title": "Speech Denoising with Deep Feature Losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19h_interspeech.html": {
    "title": "VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liao19_interspeech.html": {
    "title": "Incorporating Symbolic Sequential Modeling for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mowlaee19_interspeech.html": {
    "title": "Maximum a posteriori Speech Enhancement Based on Double Spectrum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yao19_interspeech.html": {
    "title": "Coarse-to-Fine Optimization for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hui19b_interspeech.html": {
    "title": "Kernel Machines Beat Deep Neural Networks on Mask-Based Single-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/metze19_interspeech.html": {
    "title": "Survey Talk: Multimodal Processing of Speech and Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shrivastava19_interspeech.html": {
    "title": "MobiVSR : Efficient and Light-Weight Neural Network for Visual Speech Recognition on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kandala19_interspeech.html": {
    "title": "Speaker Adaptation for Lip-Reading Using Visual Identity Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/koumparoulis19_interspeech.html": {
    "title": "MobiLipNet: Resource-Efficient Deep Learning Based Lipreading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qu19_interspeech.html": {
    "title": "LipSound: Neural Mel-Spectrogram Reconstruction for Lip Reading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sainath19_interspeech.html": {
    "title": "Two-Pass End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lam19_interspeech.html": {
    "title": "Extract, Adapt and Recognize: An End-to-End Neural Network for Corrupted Monaural Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gowda19_interspeech.html": {
    "title": "Multi-Task Multi-Resolution Char-to-BPE Cross-Attention Decoder for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/han19b_interspeech.html": {
    "title": "Multi-Stride Self-Attention for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19b_interspeech.html": {
    "title": "LF-MMI Training of Bayesian and Gaussian Process Time Delay Neural Networks for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19c_interspeech.html": {
    "title": "Self-Teaching Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19n_interspeech.html": {
    "title": "Improved End-to-End Speech Emotion Recognition Using Self Attention Mechanism and Multitask Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schmitt19_interspeech.html": {
    "title": "Continuous Emotion Recognition in Speech — Do We Need Recurrence?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ouyang19_interspeech.html": {
    "title": "Speech Based Emotion Prediction: Can a Linear Model Work?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ando19_interspeech.html": {
    "title": "Speech Emotion Recognition Based on Multi-Label Emotion Existence Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gorrostieta19_interspeech.html": {
    "title": "Gender De-Biasing in Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bao19_interspeech.html": {
    "title": "CycleGAN-Based Emotion Style Transfer as Data Augmentation for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bollepalli19_interspeech.html": {
    "title": "Lombard Speech Synthesis Using Transfer Learning in a Tacotron Text-to-Speech System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seshadri19_interspeech.html": {
    "title": "Augmented CycleGANs for Continuous Scale Normal-to-Lombard Speaking Style Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19f_interspeech.html": {
    "title": "Foreign Accent Conversion by Synthesizing Speech from Phonetic Posteriorgrams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shankar19b_interspeech.html": {
    "title": "A Multi-Speaker Emotion Morphing Model Using Highway Networks and Maximum Likelihood Objective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lapidot19_interspeech.html": {
    "title": "Effects of Waveform PMF on Anti-Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19b_interspeech.html": {
    "title": "Nonparallel Emotional Speech Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/stafylakis19_interspeech.html": {
    "title": "Self-Supervised Speaker Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nautsch19b_interspeech.html": {
    "title": "Privacy-Preserving Speaker Recognition with Cohort Score Normalisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19f_interspeech.html": {
    "title": "Large Margin Softmax Loss for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hajavi19_interspeech.html": {
    "title": "A Deep Neural Network for Short-Segment Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhou19_interspeech.html": {
    "title": "Deep Speaker Embedding Extraction with Channel-Wise Feature Responses and Additive Supervision Softmax Loss Function",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shon19b_interspeech.html": {
    "title": "VoiceID Loss: Speech Enhancement for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/avila19_interspeech.html": {
    "title": "Blind Channel Response Estimation for Replay Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/patil19_interspeech.html": {
    "title": "Energy Separation-Based Instantaneous Frequency Estimation for Cochlear Cepstral Feature for Replay Spoof Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mingote19_interspeech.html": {
    "title": "Optimization of False Acceptance/Rejection Rates and Decision Threshold for End-to-End Text-Dependent Speaker Verification Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fan19_interspeech.html": {
    "title": "Deep Hashing for Speaker Identification and Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marras19_interspeech.html": {
    "title": "Adversarial Optimization for Dictionary Attacks on Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gunendradasan19_interspeech.html": {
    "title": "An Adaptive-Q Cochlear Model for Replay Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yun19_interspeech.html": {
    "title": "An End-to-End Text-Independent Speaker Verification Framework with a Keyword Adversarial Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seo19_interspeech.html": {
    "title": "Shortcut Connections Based Deep Speaker Embeddings for End-to-End Speaker Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/you19c_interspeech.html": {
    "title": "Device Feature Extractor for Replay Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19i_interspeech.html": {
    "title": "Cross-Domain Replay Spoofing Attack Detection Using Domain Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kanagasundaram19_interspeech.html": {
    "title": "A Study of x-Vector Based Speaker Recognition on Short Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19i_interspeech.html": {
    "title": "Tied Mixture of Factor Analyzers Layer to Combine Frame Level Representations in Neural Speaker Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wickramasinghe19_interspeech.html": {
    "title": "Biologically Inspired Adaptive-Q Filterbanks for Replay Spoofing Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bousquet19_interspeech.html": {
    "title": "On Robustness of Unsupervised Domain Adaptation for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shon19c_interspeech.html": {
    "title": "Large-Scale Speaker Retrieval on Random Speaker Variability Subspace",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yoshioka19_interspeech.html": {
    "title": "Meeting Transcription Using Asynchronous Distant Microphones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/thomas19_interspeech.html": {
    "title": "Detection and Recovery of OOVs for Improved English Broadcast News Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/farooq19_interspeech.html": {
    "title": "Improving Large Vocabulary Urdu Speech Recognition System Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tang19b_interspeech.html": {
    "title": "Hybrid Arbitration Using Raw ASR String and NLU Information — Taking the Best of Both Embedded World and Cloud World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/szaszak19_interspeech.html": {
    "title": "Leveraging a Character, Word and Prosody Triplet for an ASR Error Robust and Agglutination Friendly Punctuation Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pellegrini19_interspeech.html": {
    "title": "The Airbus Air Traffic Control Speech Recognition 2018 Challenge: Towards ATC Automatic Transcription and Call Sign Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oneata19_interspeech.html": {
    "title": "Kite: Automatic Speech Recognition for Unmanned Aerial Vehicles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19j_interspeech.html": {
    "title": "Exploring Methods for the Automatic Detection of Errors in Manual Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biswas19_interspeech.html": {
    "title": "Improved Low-Resource Somali Speech Recognition by Semi-Supervised Acoustic and Language Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/helgadottir19_interspeech.html": {
    "title": "The Althingi ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19d_interspeech.html": {
    "title": "CRIM's Speech Transcription and Call Sign Detection System for the ATC Airbus Challenge Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rutowski19_interspeech.html": {
    "title": "Optimizing Speech-Input Length for Speaker-Independent Depression Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pietrowicz19_interspeech.html": {
    "title": "A New Approach for Automating Analysis of Responses on Verbal Fluency Tests from Subjects At-Risk for Schizophrenia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jeancolas19_interspeech.html": {
    "title": "Comparison of Telephone Recordings and Professional Microphone Recordings for Early Detection of Parkinson's Disease, Using Mel-Frequency Cepstral Coefficients with Gaussian Mixture Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/janbakhshi19_interspeech.html": {
    "title": "Spectral Subspace Analysis for Automatic Assessment of Pathological Speech Intelligibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pasquale19_interspeech.html": {
    "title": "An Investigation of Therapeutic Rapport Through Prosody in Brief Psychodynamic Psychotherapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rueda19_interspeech.html": {
    "title": "Feature Representation of Pathophysiology of Parkinsonian Dysarthria",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/onu19_interspeech.html": {
    "title": "Neural Transfer Learning for Cry-Based Diagnosis of Perinatal Asphyxia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hong19_interspeech.html": {
    "title": "Investigating the Variability of Voice Quality and Pain Levels as a Function of Multiple Clinical Parameters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lopez19_interspeech.html": {
    "title": "Assessing Parkinson's Disease from Speech Using Fisher Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/klumpp19_interspeech.html": {
    "title": "Feature Space Visualization with Spatial Similarity Maps for Pathological Speech Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chakravarthula19_interspeech.html": {
    "title": "Predicting Behavior in Cancer-Afflicted Patient and Spouse Interactions Using Speech and Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qin19_interspeech.html": {
    "title": "Automatic Assessment of Language Impairment Based on Raw ASR Output",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fu19_interspeech.html": {
    "title": "Effects of Spectral and Temporal Cues to Mandarin Concurrent-Vowels Identification for Normal-Hearing and Hearing-Impaired Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zayats19_interspeech.html": {
    "title": "Disfluencies and Human Speech Transcription Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parhammer19_interspeech.html": {
    "title": "The Influence of Distraction on Speech Processing: How Selective is Selective Attention?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hazan19_interspeech.html": {
    "title": "Subjective Evaluation of Communicative Effort for Younger and Older Adults in Interactive Tasks with Energetic and Informational Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/davis19_interspeech.html": {
    "title": "Perceiving Older Adults Producing Clear and Lombard Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ariasvergara19_interspeech.html": {
    "title": "Phone-Attribute Posteriors to Evaluate the Speech of Cochlear Implant Users",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hodoshima19_interspeech.html": {
    "title": "Effects of Urgent Speech and Congruent/Incongruent Text on Speech Intelligibility in Noise and Reverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mamun19_interspeech.html": {
    "title": "Quantifying Cochlear Implant Users' Ability for Speaker Identification Using CI Auditory Stimuli",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/felker19_interspeech.html": {
    "title": "Lexically Guided Perceptual Learning of a Vowel Shift in an Interactive L2 Listening Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paulus19_interspeech.html": {
    "title": "Talker Intelligibility and Listening Effort with Temporally Modified Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ward19b_interspeech.html": {
    "title": "R2SPIN: Re-Recording the Revised Speech Perception in Noise Test",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19j_interspeech.html": {
    "title": "Contributions of Consonant-Vowel Transitions to Mandarin Tone Identification in Simulated Electric-Acoustic Hearing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pirhosseinloo19_interspeech.html": {
    "title": "Monaural Speech Enhancement with Dilated Convolutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liao19b_interspeech.html": {
    "title": "Noise Adaptive Speech Enhancement Using Domain Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ge19_interspeech.html": {
    "title": "Environment-Dependent Attention-Driven Recurrent Convolutional Neural Network for Robust Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pariente19_interspeech.html": {
    "title": "A Statistically Principled and Computationally Efficient Approach to Speech Enhancement Using Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19d_interspeech.html": {
    "title": "Speech Enhancement Using Forked Generative Adversarial Networks with Spectral Subtraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zezario19_interspeech.html": {
    "title": "Specialized Speech Enhancement Model Selection Based on Learned Non-Intrusive Quality Assessment Metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chuang19_interspeech.html": {
    "title": "Speaker-Aware Deep Denoising Autoencoder with Embedded Speaker Identity for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19g_interspeech.html": {
    "title": "Investigation of Cost Function for Supervised Monaural Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19b_interspeech.html": {
    "title": "Deep Attention Gated Dilated Temporal Convolutional Networks with Intra-Parallel Convolutional Modules for End-to-End Monaural Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19k_interspeech.html": {
    "title": "Masking Estimation with Phase Restoration of Clean Speech for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/llombart19b_interspeech.html": {
    "title": "Progressive Speech Enhancement with Residual Connections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19k_interspeech.html": {
    "title": "Acoustic Model Bootstrapping Using Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mantena19_interspeech.html": {
    "title": "Bandwidth Embeddings for Mixed-Bandwidth Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khare19_interspeech.html": {
    "title": "Adversarial Black-Box Attacks on Automatic Speech Recognition Systems Using Multi-Objective Evolutionary Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soomro19_interspeech.html": {
    "title": "Towards Debugging Deep Neural Networks by Generating Speech Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ding19b_interspeech.html": {
    "title": "Compression of CTC-Trained Acoustic Models by Dynamic Frame-Wise Distillation or Segment-Wise N-Best Hypotheses Imitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lopezespejo19_interspeech.html": {
    "title": "Keyword Spotting for Hearing Assistive Devices Robust to External Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/doulaty19_interspeech.html": {
    "title": "Latent Dirichlet Allocation Based Acoustic Data Selection for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19o_interspeech.html": {
    "title": "Target Speaker Recovery and Recognition Network with Average x-Vector and Global Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/suzuki19b_interspeech.html": {
    "title": "Lyrics Recognition from Singing Voice Focused on Correspondence Between Voice and Notes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsu19b_interspeech.html": {
    "title": "Transfer Learning from Audio-Visual Grounding to Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luo19c_interspeech.html": {
    "title": "Cross-Corpus Speech Emotion Recognition Using Semi-Supervised Transfer Non-Negative Matrix Factorization with Adaptation Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tammewar19_interspeech.html": {
    "title": "Modeling User Context for Valence Prediction from Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chakraborty19_interspeech.html": {
    "title": "Front-End Feature Compensation and Denoising for Noise Robust Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19p_interspeech.html": {
    "title": "The Contribution of Acoustic Features Analysis to Model Emotion Perceptual Process for Language Diversity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rajan19_interspeech.html": {
    "title": "Design and Development of a Multi-Lingual Speech Corpora (TaMaR-EmoDB) for Emotion Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sridhar19_interspeech.html": {
    "title": "Speech Emotion Recognition with a Reject Option",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jin19_interspeech.html": {
    "title": "Development of Emotion Rankers Based on Intended and Perceived Emotion Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gideon19_interspeech.html": {
    "title": "Emotion Recognition from Natural Phone Conversations in Individuals with and without Recent Suicidal Ideation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nazareth19_interspeech.html": {
    "title": "An Acoustic and Lexical Analysis of Emotional Valence in Spontaneous Speech: Autobiographical Memory Recall in Older Adults",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19g_interspeech.html": {
    "title": "Does the Lombard Effect Improve Emotional Communication in Noise? — Analysis of Emotional Speech Acted in Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gharsellaoui19_interspeech.html": {
    "title": "Linear Discriminant Differential Evolution for Feature Selection in Emotional Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sahu19_interspeech.html": {
    "title": "Multi-Modal Learning for Speech Emotion Recognition: An Analysis and Comparison of ASR Outputs with Ground Truth Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/spinu19_interspeech.html": {
    "title": "Articulatory Characteristics of Secondary Palatalization in Romanian Fricatives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ratko19_interspeech.html": {
    "title": "Articulation of Vowel Length Contrasts in Australian English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/deme19_interspeech.html": {
    "title": "V-to-V Coarticulation Induced Acoustic and Articulatory Variability of Vowels: The Effect of Pitch-Accent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/king19_interspeech.html": {
    "title": "The Contribution of Lip Protrusion to Anglo-English /r/: Evidence from Hyper- and Non-Hyperarticulated Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marko19_interspeech.html": {
    "title": "Articulatory Analysis of Transparent Vowel /iː/ in Harmonic and Antiharmonic Hungarian Stems: Is There a Difference?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cunha19_interspeech.html": {
    "title": "On the Role of Oral Configurations in European Portuguese Nasal Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xiong19_interspeech.html": {
    "title": "Residual + Capsule Networks (ResCap) for Simultaneous Single-Channel Overlapped Keyword Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19i_interspeech.html": {
    "title": "A Study for Improving Device-Directed Speech Detection Toward Frictionless Human-Machine Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/su19_interspeech.html": {
    "title": "Unsupervised Methods for Audio Classification from Lecture Discussion Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ashihara19_interspeech.html": {
    "title": "Neural Whispered Speech Detection with Imbalanced Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bergler19_interspeech.html": {
    "title": "Deep Learning for Orca Call Type Identification — A Fully Unsupervised Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sacchi19_interspeech.html": {
    "title": "Open-Vocabulary Keyword Spotting with Audio and Text Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19c_interspeech.html": {
    "title": "ToneNet: A CNN Model of Tone Classification of Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/choi19_interspeech.html": {
    "title": "Temporal Convolution for Real-Time Keyword Spotting on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19j_interspeech.html": {
    "title": "Audio Tagging with Compact Feedforward Sequential Memory Network and Audio-to-Audio Ratio Based Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19f_interspeech.html": {
    "title": "Music Genre Classification Using Duplicated Convolutional Layers in Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/carmi19_interspeech.html": {
    "title": "A Storyteller's Tale: Literature Audiobooks Genre Classification Using CNN and RNN Architectures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hwang19_interspeech.html": {
    "title": "Parameter Enhancement for MELP Speech Codec in Noisy Communication Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhen19_interspeech.html": {
    "title": "Cascaded Cross-Module Residual Learning Towards Lightweight End-to-End Speech Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/backstrom19_interspeech.html": {
    "title": "End-to-End Optimization of Source Models for Speech and Audio Coding Using a Machine Learning Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/valin19_interspeech.html": {
    "title": "A Real-Time Wideband Neural Vocoder at 1.6kb/s Using LPCNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fuchs19_interspeech.html": {
    "title": "Super-Wideband Spectral Envelope Modeling for Speech Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19q_interspeech.html": {
    "title": "Speech Audio Super-Resolution for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19e_interspeech.html": {
    "title": "Artificial Bandwidth Extension Using H∞ Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mittag19_interspeech.html": {
    "title": "Quality Degradation Diagnosis for Voice Networks — Estimating the Perceived Noisiness, Coloration, and Discontinuity of Transmitted Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chai19b_interspeech.html": {
    "title": "A Cross-Entropy-Guided (CEG) Measure for Speech Enhancement Front-End Assessing Performances of Back-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moller19_interspeech.html": {
    "title": "Extending the E-Model Towards Super-Wideband and Fullband Speech Communication Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sadhu19_interspeech.html": {
    "title": "Modulation Vectors as Robust Feature Representation for ASR in Domain Mismatched Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19r_interspeech.html": {
    "title": "Prosody Usage Optimization for Children Speech Recognition with Zero Resource Children Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/agrawal19_interspeech.html": {
    "title": "Unsupervised Raw Waveform Representation Learning for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ramsay19_interspeech.html": {
    "title": "Low-Dimensional Bottleneck Features for On-Device Continuous Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/riviello19_interspeech.html": {
    "title": "Binary Speech Features for Keyword Spotting Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schneider19_interspeech.html": {
    "title": "wav2vec: Unsupervised Pre-Training for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cho19b_interspeech.html": {
    "title": "Automatic Detection of Prosodic Focus in American English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/menon19_interspeech.html": {
    "title": "Feature Exploration for Almost Zero-Resource ASR-Free Keyword Spotting Using a Multilingual Bottleneck Extractor and Correspondence Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/loweimi19_interspeech.html": {
    "title": "On Learning Interpretable CNNs with Parametric Modulated Kernel-Based Filters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/verwimp19_interspeech.html": {
    "title": "Reverse Transfer Learning: Can Word Embeddings Trained for Different NLP Tasks Improve Neural Language Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19l_interspeech.html": {
    "title": "Joint Grapheme and Phoneme Embeddings for Contextual End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19h_interspeech.html": {
    "title": "Character-Aware Sub-Word Level Language Modeling for Uyghur and Turkish ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pusateri19_interspeech.html": {
    "title": "Connecting and Comparing Language Model Interpolation Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khassanov19b_interspeech.html": {
    "title": "Enriching Rare Word Representations in Neural Language Models by Embedding Matrix Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yu19b_interspeech.html": {
    "title": "Comparative Study of Parametric and Representation Uncertainty Modeling for Recurrent Neural Network Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/agenbag19_interspeech.html": {
    "title": "Improving Automatically Induced Lexicons for Highly Agglutinating Languages Using Data-Driven Morphological Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/coucheirolimeres19_interspeech.html": {
    "title": "Attention-Based Word Vector Prediction with LSTMs and its Application to the OOV Problem in ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19d_interspeech.html": {
    "title": "Code-Switching Sentence Generation by Bert and Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ritchie19_interspeech.html": {
    "title": "Unified Verbalization for Speech Recognition & Synthesis Across Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19e_interspeech.html": {
    "title": "Better Morphology Prediction for Better Speech Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sennema19_interspeech.html": {
    "title": "Vietnamese Learners Tackling the German /ʃt/ in Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lewis19_interspeech.html": {
    "title": "An Articulatory-Acoustic Investigation into GOOSE-Fronting in German-English Bilinguals Residing in London, UK",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jenne19_interspeech.html": {
    "title": "Multimodal Articulation-Based Pronunciation Error Detection with Spectrogram and Acoustic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/foltz19_interspeech.html": {
    "title": "Using Prosody to Discover Word Order Alternations in a Novel Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bradlow19b_interspeech.html": {
    "title": "Speaking Rate, Information Density, and Information Rate in First-Language and Second-Language Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/graham19_interspeech.html": {
    "title": "Articulation Rate as a Metric in Spoken Language Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xu19c_interspeech.html": {
    "title": "Learning Alignment for Multimodal Emotion Recognition from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peperkamp19b_interspeech.html": {
    "title": "Liquid Deletion in French Child-Directed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seidl19_interspeech.html": {
    "title": "Towards Detection of Canonical Babbling by Citizen Scientists: Performance as a Function of Clip Length",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ludusan19b_interspeech.html": {
    "title": "Nasal Consonant Discrimination in Infant- and Adult-Directed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marklund19_interspeech.html": {
    "title": "No Distributional Learning in Adults from Attended Listening to Non-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rasanen19_interspeech.html": {
    "title": "A Computational Model of Early Language Acquisition from Audiovisual Experiences of Young Infants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/du19b_interspeech.html": {
    "title": "The Production of Chinese Affricates /ts/ and /tsh/ by Native Urdu Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19s_interspeech.html": {
    "title": "Multi-Stream Network with Temporal Attention for Environmental Sound Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cerutti19_interspeech.html": {
    "title": "Neural Network Distillation on IoT Platforms for Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19d_interspeech.html": {
    "title": "Class-Wise Centroid Distance Metric Learning for Acoustic Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bai19b_interspeech.html": {
    "title": "A Hybrid Approach to Acoustic Scene Classification Based on Universal Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/he19b_interspeech.html": {
    "title": "Hierarchical Pooling Structure for Weakly Labeled Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xia19_interspeech.html": {
    "title": "Sound Event Detection in Multichannel Audio Using Convolutional Time-Frequency-Channel Squeeze and Excitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pham19b_interspeech.html": {
    "title": "A Robust Framework for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19c_interspeech.html": {
    "title": "Compression of Acoustic Event Detection Models with Quantized Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19m_interspeech.html": {
    "title": "An End-to-End Audio Classification System Based on Raw Waveforms and Mix-Training Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19k_interspeech.html": {
    "title": "Few-Shot Audio Classification with Attentional Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19e_interspeech.html": {
    "title": "Semi-Supervised Audio Classification with Consistency-Based Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mizgajski19_interspeech.html": {
    "title": "Avaya Conversational Intelligence: A Real-Time System for Spoken Language Understanding in Human-Human Call Center Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/an19b_interspeech.html": {
    "title": "Robust Keyword Spotting via Recycle-Pooling for Mobile Game",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chylek19_interspeech.html": {
    "title": "Multimodal Dialog with the MALACH Audiovisual Archive",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jelil19_interspeech.html": {
    "title": "SpeechMarker: A Voice Based Multi-Level Attendance Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19h_interspeech.html": {
    "title": "Robust Sound Recognition: A Neuromorphic Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19c_interspeech.html": {
    "title": "The CUHK Dysarthric Speech Recognition Systems for English and Cantonese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schiel19_interspeech.html": {
    "title": "BAS Web Services for Automatic Subtitle Creation and Anonymization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/voe19_interspeech.html": {
    "title": "A User-Friendly and Adaptable Re-Implementation of an Acoustic Prominence Detection and Annotation Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dominguez19_interspeech.html": {
    "title": "PyToBI: A Toolkit for ToBI Labeling Under Python",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/levy19_interspeech.html": {
    "title": "GECKO — A Tool for Effective Annotation of Human Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lo19b_interspeech.html": {
    "title": "SLP-AA: Tools for Sign Language Phonetic and Phonological Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19t_interspeech.html": {
    "title": "SANTLR: Speech Annotation Toolkit for Low Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gruber19_interspeech.html": {
    "title": "Web-Based Speech Synthesis Editor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/perrotin19_interspeech.html": {
    "title": "GFM-Voc: A Real-Time Voice Quality Modification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/szekely19_interspeech.html": {
    "title": "Off the Cuff: Exploring Extemporaneous Speech Delivery with TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kessler19_interspeech.html": {
    "title": "Synthesized Spoken Names: Biases Impacting Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bernardo19_interspeech.html": {
    "title": "Unbabel Talk — Human Verified Translations for Voice Instant Messaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rabiee19_interspeech.html": {
    "title": "Adjusting Pleasure-Arousal-Dominance for Continuous Emotional Text-to-Speech Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lapata19_interspeech.html": {
    "title": "Learning Natural Language Interfaces with Neural Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nautsch19c_interspeech.html": {
    "title": "The GDPR & Speech Data: Reflections of Legal and Technology Communities, First Steps Towards a Common Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/srivastava19_interspeech.html": {
    "title": "Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nelus19_interspeech.html": {
    "title": "Privacy-Preserving Siamese Feature Extraction for Gender Recognition versus Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nelus19b_interspeech.html": {
    "title": "Privacy-Preserving Variational Information Feature Extraction for Domestic Activity Monitoring versus Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/thaine19_interspeech.html": {
    "title": "Extracting Mel-Frequency and Bark-Frequency Cepstral Coefficients from Encrypted Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zarazaga19_interspeech.html": {
    "title": "Sound Privacy: A Conversational Speech Corpus for Quantifying the Experience of Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soto19_interspeech.html": {
    "title": "Improving Code-Switched Language Modeling Performance Using Cognate Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19d_interspeech.html": {
    "title": "Linguistically Motivated Parallel Data Augmentation for Code-Switch Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rallabandi19_interspeech.html": {
    "title": "Variational Attention Using Articulatory Priors for Generating Code Mixed Speech Using Monolingual Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19l_interspeech.html": {
    "title": "Code-Switching Detection Using ASR-Generated Language Posteriors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biswas19b_interspeech.html": {
    "title": "Semi-Supervised Acoustic Model Training for Five-Lingual Code-Switched ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ylmaz19b_interspeech.html": {
    "title": "Multi-Graph Decoding for Code-Switching ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seki19_interspeech.html": {
    "title": "End-to-End Multilingual Multi-Speaker Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guasch19_interspeech.html": {
    "title": "Survey Talk: Realistic Physics-Based Computational Voice Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mohapatra19_interspeech.html": {
    "title": "An Extended Two-Dimensional Vocal Tract Model for Fast Acoustic Simulation of Single-Axis Symmetric Three-Dimensional Tubes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/birkholz19_interspeech.html": {
    "title": "Perceptual Optimization of an Enhanced Geometric Vocal Fold Model for Articulatory Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19e_interspeech.html": {
    "title": "Articulatory Copy Synthesis Based on a Genetic Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shahrebabaki19_interspeech.html": {
    "title": "A Phonetic-Level Analysis of Different Input Features for Articulatory Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tuske19_interspeech.html": {
    "title": "Advancing Sequence-to-Sequence Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hannun19_interspeech.html": {
    "title": "Sequence-to-Sequence Speech Recognition with Time-Depth Separable Convolutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/baskar19_interspeech.html": {
    "title": "Semi-Supervised Sequence-to-Sequence ASR Using Unpaired Speech and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bai19c_interspeech.html": {
    "title": "Learn Spelling from Teachers: Transferring Knowledge from Language Models to Sequence-to-Sequence Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/irie19_interspeech.html": {
    "title": "On the Choice of Modeling Unit for Sequence-to-Sequence Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/weninger19b_interspeech.html": {
    "title": "Listen, Attend, Spell and Adapt: Speaker Adapted Sequence-to-Sequence ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/runarsdottir19_interspeech.html": {
    "title": "Lattice Re-Scoring During Manual Editing for Automatic Error Correction of ASR Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fukunaga19_interspeech.html": {
    "title": "GPU-Based WFST Decoding with Extra Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jorge19_interspeech.html": {
    "title": "Real-Time One-Pass Decoder for Speech Recognition Using LSTM Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seki19b_interspeech.html": {
    "title": "Vectorized Beam Search for CTC-Attention-Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/serrino19_interspeech.html": {
    "title": "Contextual Recovery of Out-of-Lattice Named Entities in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novitasari19_interspeech.html": {
    "title": "Sequence-to-Sequence Learning via Attention Transfer for Incremental Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lian19b_interspeech.html": {
    "title": "Unsupervised Representation Learning with Future Observation Prediction for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/phan19_interspeech.html": {
    "title": "Spatio-Temporal Attention Pooling for Audio Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19d_interspeech.html": {
    "title": "Subspace Pooling Based Temporal Features Extraction for Audio Event Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19l_interspeech.html": {
    "title": "Multi-Scale Time-Frequency Attention for Acoustic Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/song19b_interspeech.html": {
    "title": "Acoustic Scene Classification by Implicitly Identifying Distinct Sound Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qi19_interspeech.html": {
    "title": "Parameter-Transfer Learning for Low-Resource Individualization of Head-Related Transfer Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19i_interspeech.html": {
    "title": "Prosodic Characteristics of Mandarin Declarative and Interrogative Utterances in Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/morovelazquez19_interspeech.html": {
    "title": "Study of the Performance of Automatic Speech Recognition Systems in Speakers with Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19m_interspeech.html": {
    "title": "Towards the Speech Features of Mild Cognitive Impairment: Universal Evidence from Structured and Unstructured Connected Speech of Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19n_interspeech.html": {
    "title": "Child Speech Disorder Detection with Siamese Recurrent Network Using Speech Attribute Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/korzekwa19_interspeech.html": {
    "title": "Interpretable Deep Learning Model for the Detection and Reconstruction of Dysarthric Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/noufi19_interspeech.html": {
    "title": "Vocal Biomarker Assessment Following Pediatric Traumatic Brain Injury: A Retrospective Cohort Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/scharenborg19b_interspeech.html": {
    "title": "Survey Talk: Reaching Over the Gap: Cross- and Interdisciplinary Research on Human and Automatic Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ogawa19_interspeech.html": {
    "title": "Improved Deep Duel Model for Rescoring N-Best Speech Recognition List Using Backward LSTMLM and Ensemble Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/irie19b_interspeech.html": {
    "title": "Language Modeling with Deep Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/raju19_interspeech.html": {
    "title": "Scalable Multi Corpora Neural Language Models for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/likhomanenko19_interspeech.html": {
    "title": "Who Needs Words? Lexicon-Free Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/latif19_interspeech.html": {
    "title": "Direct Modelling of Speech Emotion from Raw Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sarma19_interspeech.html": {
    "title": "Improving Emotion Identification Using Phone Posteriors in Raw Speech Waveform Based DNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cao19_interspeech.html": {
    "title": "Pyramid Memory Block and Timestep Attention for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oates19_interspeech.html": {
    "title": "Robust Speech Emotion Recognition Under Different Encoding Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gosztolya19c_interspeech.html": {
    "title": "Using the Bag-of-Audio-Word Feature Representation of ASR DNN Posteriors for Paralinguistic Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/williams19c_interspeech.html": {
    "title": "Disentangling Style Factors from Speaker Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsu19c_interspeech.html": {
    "title": "Sentence Prosody and Wh-Indeterminates in Taiwan Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19d_interspeech.html": {
    "title": "Frication as a Vowel Feature? — Evidence from the Rui'an Wu Chinese Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19m_interspeech.html": {
    "title": "Vowels and Diphthongs in the Xupu Xiang Chinese Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/albuquerque19_interspeech.html": {
    "title": "Age-Related Changes in European Portuguese Vowel Acoustics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lalhminghlui19_interspeech.html": {
    "title": "Vowel-Tone Interaction in Two Tibeto-Burman Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rodriguez19_interspeech.html": {
    "title": "The Vowel System of Korebaju",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ibrahim19_interspeech.html": {
    "title": "Fundamental Frequency Accommodation in Multi-Party Human-Robot Game Interactions: The Effect of Winning or Losing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wagner19_interspeech.html": {
    "title": "Pitch Accent Trajectories Across Different Conditions of Visibility and Information Structure — Evidence from Spontaneous Dyadic Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/betz19_interspeech.html": {
    "title": "The Greennn Tree — Lengthening Position Influences Uncertainty Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/si19_interspeech.html": {
    "title": "CNN-BLSTM Based Question Detection from Dialogs Considering Phase and Context Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/metcalf19_interspeech.html": {
    "title": "Mirroring to Build Trust in Digital Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/raveh19_interspeech.html": {
    "title": "Three's a Crowd? Effects of a Second Human on Vocal Accommodation with a Voice Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19o_interspeech.html": {
    "title": "Adversarial Regularization for End-to-End Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/monteiro19_interspeech.html": {
    "title": "Combining Speaker Recognition and Metric Learning for Speaker-Dependent Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19n_interspeech.html": {
    "title": "VAE-Based Regularization for Deep Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mingote19b_interspeech.html": {
    "title": "Language Recognition Using Triplet Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jung19c_interspeech.html": {
    "title": "Spatial Pyramid Encoding with Convex Length Normalization for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heo19b_interspeech.html": {
    "title": "End-to-End Losses Based on Speaker Basis Vectors and All-Speaker Hard Negative Mining for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jiang19_interspeech.html": {
    "title": "An Effective Deep Embedding Learning Architecture for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qin19b_interspeech.html": {
    "title": "Far-Field End-to-End Text-Dependent Speaker Verification Based on Mixed Training Data with Transfer Learning and Enrollment Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ren19_interspeech.html": {
    "title": "Two-Stage Training for Chinese Dialect Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaminishi19_interspeech.html": {
    "title": "Investigation on Blind Bandwidth Extension with a Non-Linear Function and its Evaluation of x-Vector-Based Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khan19_interspeech.html": {
    "title": "Auto-Encoding Nearest Neighbor i-Vectors for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zheng19b_interspeech.html": {
    "title": "Towards a Fault-Tolerant Speaker Verification System: A Regularization Approach to Reduce the Condition Number",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/taherian19_interspeech.html": {
    "title": "Deep Learning Based Multi-Channel Speaker Recognition in Noisy and Reverberant Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19g_interspeech.html": {
    "title": "Joint Optimization of Neural Acoustic Beamforming and Dereverberation with x-Vectors for Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/miao19b_interspeech.html": {
    "title": "A New Time-Frequency Attention Mechanism for TDNN and CNN-LSTM-TDNN, with Application to Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19n_interspeech.html": {
    "title": "An Attention-Based Hybrid Network for Automatic Detection of Alzheimer's Disease from Narrative Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ma19b_interspeech.html": {
    "title": "Investigating the Lombard Effect Influence on End-to-End Audio-Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ooster19_interspeech.html": {
    "title": "Computer, Test My Hearing\": Accurate Speech Audiometry with Smart Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eshky19_interspeech.html": {
    "title": "Synchronising Audio and Ultrasound by Learning Cross-Modal Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pan19_interspeech.html": {
    "title": "Automatic Hierarchical Attention Neural Network for Detecting AD",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nallanthighal19_interspeech.html": {
    "title": "Deep Sensing of Breathing Signal During Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biadsy19_interspeech.html": {
    "title": "Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19j_interspeech.html": {
    "title": "Exploiting Visual Features Using Bayesian Gated Neural Networks for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vougioukas19_interspeech.html": {
    "title": "Video-Driven Speech Reconstruction Using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19k_interspeech.html": {
    "title": "On the Use of Pitch Features for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shillingford19_interspeech.html": {
    "title": "Large-Scale Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/razavi19_interspeech.html": {
    "title": "Investigating Linguistic and Semantic Features for Turn-Taking Prediction in Open-Domain Human-Computer Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bechet19_interspeech.html": {
    "title": "Benchmarking Benchmarks: Introducing New Automatic Indicators for Benchmarking Spoken Language Understanding Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19l_interspeech.html": {
    "title": "A Neural Turn-Taking Model without RNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/coman19_interspeech.html": {
    "title": "An Incremental Turn-Taking Model for Task-Oriented Dialog Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/su19b_interspeech.html": {
    "title": "Personalized Dialogue Response Generation Learned from Monologues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heldner19_interspeech.html": {
    "title": "Voice Quality as a Turn-Taking Cue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hara19_interspeech.html": {
    "title": "Turn-Taking Prediction Based on Detection of Transition Relevance Place",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lala19_interspeech.html": {
    "title": "Analysis of Effect and Timing of Fillers in Natural Turn-Taking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/horiguchi19_interspeech.html": {
    "title": "Multimodal Response Obligation Detection with Unsupervised Online Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/su19c_interspeech.html": {
    "title": "Follow-Up Question Generation Using Neural Tensor Network-Based Domain Ontology Population in an Interview Coaching System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tran19_interspeech.html": {
    "title": "On the Role of Style in Parsing Speech with Neural Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pasad19_interspeech.html": {
    "title": "On the Contributions of Visual and Textual Supervision in Low-Resource Semantic Speech Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19p_interspeech.html": {
    "title": "Automatic Detection of Off-Topic Spoken Responses Using Very Deep Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/piunova19_interspeech.html": {
    "title": "Rescoring Keyword Search Confidence Estimates with Graph-Based Re-Ranking Using Acoustic Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/segal19_interspeech.html": {
    "title": "SpeechYOLO: Detection and Localization of Speech Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oktem19_interspeech.html": {
    "title": "Prosodic Phrase Alignment for Machine Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tannander19_interspeech.html": {
    "title": "Spot the Pleasant People! Navigating the Cocktail Party Buzz",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19o_interspeech.html": {
    "title": "Neural Text Clustering with Document-Level Attention Based on Dynamic Soft Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bach19_interspeech.html": {
    "title": "Noisy BiLSTM-Based Models for Disfluency Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/singh19b_interspeech.html": {
    "title": "Subword RNNLM Approximations for Out-Of-Vocabulary Keyword Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/maekaku19_interspeech.html": {
    "title": "Simultaneous Detection and Localization of a Wake-Up Word Using Multi-Task Learning of the Duration and Endpoint",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19e_interspeech.html": {
    "title": "On Mitigating Acoustic Feedback in Hearing Aids with Frequency Warping by All-Pass Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fazel19_interspeech.html": {
    "title": "Deep Multitask Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19o_interspeech.html": {
    "title": "Deep Learning for Joint Acoustic Echo and Noise Cancellation with Nonlinear Distortions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/srensen19_interspeech.html": {
    "title": "Harmonic Beamformers for Non-Intrusive Speech Intelligibility Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mamun19b_interspeech.html": {
    "title": "Convolutional Neural Network-Based Speech Enhancement for Cochlear Implant Recipients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/srensen19b_interspeech.html": {
    "title": "Validation of the Non-Intrusive Codebook-Based Short Time Objective Intelligibility Metric for Processed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/arai19_interspeech.html": {
    "title": "Predicting Speech Intelligibility of Enhanced Speech Using Phone Accuracy of DNN-Based ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bu19_interspeech.html": {
    "title": "A Novel Method to Correct Steering Vectors in MVDR Beamformer for Noise Robust ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19f_interspeech.html": {
    "title": "End-to-End Multi-Channel Speech Enhancement Using Inter-Channel Time-Restricted Attention on Raw Waveform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gu19b_interspeech.html": {
    "title": "Neural Spatial Filter: Target Speaker Speech Separation Assisted with Directional Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/afouras19_interspeech.html": {
    "title": "My Lips Are Concealed: Audio-Visual Speech Enhancement Through Obstructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fujita19_interspeech.html": {
    "title": "End-to-End Neural Speaker Diarization with Permutation-Free Objectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/india19_interspeech.html": {
    "title": "Self Multi-Head Attention for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vinals19b_interspeech.html": {
    "title": "Phonetically-Aware Embeddings, Wide Residual Networks with Time-Delay Neural Networks and Self Attention Models for the 2018 NIST Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tu19_interspeech.html": {
    "title": "Variational Domain Adversarial Learning for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19m_interspeech.html": {
    "title": "A Unified Framework for Speaker and Utterance Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nandwana19c_interspeech.html": {
    "title": "Analysis of Critical Metadata Factors for the Calibration of Speaker Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novotny19_interspeech.html": {
    "title": "Factorization of Discriminatively Trained i-Vector Extractor for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/salvati19_interspeech.html": {
    "title": "End-to-End Speaker Identification in Noisy and Reverberant Environments Using Raw Waveform Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/naini19_interspeech.html": {
    "title": "Whisper to Neutral Mapping Using Cosine Similarity Maximization in i-Vector Space for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhu19b_interspeech.html": {
    "title": "Mixup Learning Strategies for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ferrer19_interspeech.html": {
    "title": "Optimizing a Speaker Embedding Extractor Through Backend-Driven Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19g_interspeech.html": {
    "title": "The NEC-TT 2018 Speaker Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zheng19c_interspeech.html": {
    "title": "Autoencoder-Based Semi-Supervised Curriculum Learning for Out-of-Domain Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19d_interspeech.html": {
    "title": "Multi-Channel Training for End-to-End Speaker Recognition Under Reverberant and Noisy Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19e_interspeech.html": {
    "title": "The DKU-SMIIP System for NIST 2018 Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wiesner19_interspeech.html": {
    "title": "Pretraining by Backtranslation for End-to-End ASR in Low-Resource Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kim19b_interspeech.html": {
    "title": "Cross-Attention End-to-End ASR for Two-Party Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chorowski19_interspeech.html": {
    "title": "Towards Using Context-Dependent Symbols in CTC Without State-Tying Decision Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fan19b_interspeech.html": {
    "title": "An Online Attention-Based Model for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tian19b_interspeech.html": {
    "title": "Self-Attention Transducers for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19u_interspeech.html": {
    "title": "Improving Transformer-Based Speech Recognition Systems with Compressed Structure and Speech Attributes Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bang19_interspeech.html": {
    "title": "Extending an Acoustic Data-Driven Phone Set for Spontaneous Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moriya19_interspeech.html": {
    "title": "Joint Maximization Decoder with Neural Converters for Fully Neural Network-Based Japanese Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parcollet19b_interspeech.html": {
    "title": "Real to H-Space Encoder for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yi19b_interspeech.html": {
    "title": "Ectc-Docd: An End-to-End Structure with CTC Encoder and OCD Decoder for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/denisov19_interspeech.html": {
    "title": "End-to-End Multi-Speaker Speech Recognition Using Speaker Embeddings and Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hayashi19_interspeech.html": {
    "title": "Pre-Trained Text Embeddings for Enhanced Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/szekely19b_interspeech.html": {
    "title": "Spontaneous Conversational Speech Synthesis from Found Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/klimkov19_interspeech.html": {
    "title": "Fine-Grained Robust Prosody Transfer for Single-Speaker Neural Text-To-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hussain19_interspeech.html": {
    "title": "Speech Driven Backchannel Generation Using Deep Q-Network for Enhancing Engagement in Human-Robot Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/koriyama19_interspeech.html": {
    "title": "Semi-Supervised Prosody Modeling Using Deep Gaussian Process Latent Variable Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nikulasdottir19_interspeech.html": {
    "title": "Bootstrapping a Text Normalization System for an Inflected Language. Numbers as a Test Case",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19e_interspeech.html": {
    "title": "Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ni19_interspeech.html": {
    "title": "Duration Modeling with Global Phoneme-Duration Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/aubin19_interspeech.html": {
    "title": "Improving Speech Synthesis with Discourse Relations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tits19_interspeech.html": {
    "title": "Visualization and Interpretation of Latent Spaces for Controlling Expressive Speech Synthesis Through Audio Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19h_interspeech.html": {
    "title": "Pre-Trained Text Representations for Improving Front-End Text Processing in Mandarin Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pan19b_interspeech.html": {
    "title": "A Mandarin Prosodic Boundary Prediction Model Based on Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gokcen19_interspeech.html": {
    "title": "Dual Encoder Classifier Models as Constraints in Neural Text Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19v_interspeech.html": {
    "title": "Knowledge-Based Linguistic Encoding for End-to-End Mandarin Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shankar19c_interspeech.html": {
    "title": "Automated Emotion Morphing in Speech Based on Diffeomorphic Curve Registration and Highway Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/connaghan19_interspeech.html": {
    "title": "Use of Beiwe Smartphone App to Identify and Track Speech Decline in Amyotrophic Lateral Sclerosis (ALS)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rowe19_interspeech.html": {
    "title": "Profiling Speech Motor Impairments in Persons with Amyotrophic Lateral Sclerosis: An Acoustic-Based Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mayle19_interspeech.html": {
    "title": "Diagnosing Dysarthria with Long Short-Term Memory Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sudro19_interspeech.html": {
    "title": "Modification of Devoicing Error in Cleft Lip and Palate Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eshghi19_interspeech.html": {
    "title": "Reduced Task Adaptation in Alternating Motion Rate Tasks as an Early Marker of Bulbar Involvement in Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19q_interspeech.html": {
    "title": "Towards the Speech Features of Early-Stage Dementia: Design and Application of the Mandarin Elderly Cognitive Speech Database",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19p_interspeech.html": {
    "title": "Acoustic Characteristics of Lexical Tone Disruption in Mandarin Speakers After Brain Damage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hermes19_interspeech.html": {
    "title": "Intragestural Variation in Natural Sentence Production: Essential Tremor Patients Treated with DBS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kalita19_interspeech.html": {
    "title": "Nasal Air Emission in Sibilant Fricatives of Cleft Lip and Palate Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/serrano19_interspeech.html": {
    "title": "Parallel vs. Non-Parallel Voice Conversion for Esophageal Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dubey19b_interspeech.html": {
    "title": "Hypernasality Severity Detection Using Constant Q Cepstral Coefficients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niu19_interspeech.html": {
    "title": "Automatic Depression Level Detection via ℓp-Norm Pooling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bn19_interspeech.html": {
    "title": "Comparison of Speech Tasks and Recording Devices for Voice Based Automatic Classification of Healthy Subjects and Patients with Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19r_interspeech.html": {
    "title": "A Modified Algorithm for Multiple Input Spectrogram Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bahmaninezhad19_interspeech.html": {
    "title": "A Comprehensive Study of Speech Separation: Spectrogram vs Waveform Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/inan19_interspeech.html": {
    "title": "Evaluating Audiovisual Source Separation in the Context of Video Conferencing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ditter19_interspeech.html": {
    "title": "Influence of Speaker-Specific Parameters on Speech Separation Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zegers19_interspeech.html": {
    "title": "CNN-LSTM Models for Multi-Speaker Source Separation Using Bayesian Hyper Parameter Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bear19_interspeech.html": {
    "title": "Towards Joint Sound Scene and Polyphonic Sound Event Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fan19c_interspeech.html": {
    "title": "Discriminative Learning for Monaural Speech Separation Using Deep Embedding Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yousefi19_interspeech.html": {
    "title": "Probabilistic Permutation Invariant Training for Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19e_interspeech.html": {
    "title": "Which Ones Are Speaking? Speaker-Inferred Model for Multi-Talker Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19f_interspeech.html": {
    "title": "End-to-End Monaural Speech Separation with Multi-Scale Dynamic Weighted Gated Dilated Convolutional Pyramid Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lluis19_interspeech.html": {
    "title": "End-to-End Music Source Separation: Is it Possible in the Waveform Domain?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/foley19_interspeech.html": {
    "title": "Elpis, an Accessible Speech-to-Text Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gruber19b_interspeech.html": {
    "title": "Framework for Conducting Tasks Requiring Human Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19k_interspeech.html": {
    "title": "Multimedia Simultaneous Translation System for Minority Language Communication with Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dikici19_interspeech.html": {
    "title": "The SAIL LABS Media Mining Indexer and the CAVA Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/goel19b_interspeech.html": {
    "title": "CaptionAI: A Real-Time Multilingual Captioning Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}